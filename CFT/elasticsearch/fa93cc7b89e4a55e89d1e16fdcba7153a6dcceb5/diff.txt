diff --git a/core/pom.xml b/core/pom.xml
index 4b55f93..4004209 100644
--- a/core/pom.xml
+++ b/core/pom.xml
@@ -6,7 +6,7 @@
     <parent>
         <groupId>org.elasticsearch</groupId>
         <artifactId>parent</artifactId>
-        <version>2.1.0-SNAPSHOT</version>
+        <version>3.0.0-SNAPSHOT</version>
     </parent>
 
     <groupId>org.elasticsearch</groupId>
diff --git a/core/src/main/java/org/apache/lucene/queryparser/classic/ExistsFieldQueryExtension.java b/core/src/main/java/org/apache/lucene/queryparser/classic/ExistsFieldQueryExtension.java
index cb4bee3..6cac629 100644
--- a/core/src/main/java/org/apache/lucene/queryparser/classic/ExistsFieldQueryExtension.java
+++ b/core/src/main/java/org/apache/lucene/queryparser/classic/ExistsFieldQueryExtension.java
@@ -21,8 +21,8 @@ package org.apache.lucene.queryparser.classic;
 
 import org.apache.lucene.search.ConstantScoreQuery;
 import org.apache.lucene.search.Query;
-import org.elasticsearch.index.query.ExistsQueryBuilder;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.ExistsQueryParser;
+import org.elasticsearch.index.query.QueryParseContext;
 
 /**
  *
@@ -32,7 +32,7 @@ public class ExistsFieldQueryExtension implements FieldQueryExtension {
     public static final String NAME = "_exists_";
 
     @Override
-    public Query query(QueryShardContext context, String queryText) {
-        return new ConstantScoreQuery(ExistsQueryBuilder.newFilter(context, queryText));
+    public Query query(QueryParseContext parseContext, String queryText) {
+        return new ConstantScoreQuery(ExistsQueryParser.newFilter(parseContext, queryText, null));
     }
 }
diff --git a/core/src/main/java/org/apache/lucene/queryparser/classic/FieldQueryExtension.java b/core/src/main/java/org/apache/lucene/queryparser/classic/FieldQueryExtension.java
index 299a37a..003ff18 100644
--- a/core/src/main/java/org/apache/lucene/queryparser/classic/FieldQueryExtension.java
+++ b/core/src/main/java/org/apache/lucene/queryparser/classic/FieldQueryExtension.java
@@ -20,12 +20,12 @@
 package org.apache.lucene.queryparser.classic;
 
 import org.apache.lucene.search.Query;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 
 /**
  *
  */
 public interface FieldQueryExtension {
 
-    Query query(QueryShardContext context, String queryText);
+    Query query(QueryParseContext parseContext, String queryText);
 }
diff --git a/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java b/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java
index 493423c..5304c3d 100644
--- a/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java
+++ b/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java
@@ -39,7 +39,7 @@ import org.elasticsearch.common.unit.Fuzziness;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.core.DateFieldMapper;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.support.QueryParsers;
 
 import com.google.common.base.Objects;
@@ -70,7 +70,7 @@ public class MapperQueryParser extends QueryParser {
                 .build();
     }
 
-    private final QueryShardContext context;
+    private final QueryParseContext parseContext;
 
     private QueryParserSettings settings;
 
@@ -85,9 +85,9 @@ public class MapperQueryParser extends QueryParser {
 
     private String quoteFieldSuffix;
 
-    public MapperQueryParser(QueryShardContext context) {
+    public MapperQueryParser(QueryParseContext parseContext) {
         super(null, null);
-        this.context = context;
+        this.parseContext = parseContext;
     }
 
     public void reset(QueryParserSettings settings) {
@@ -162,7 +162,7 @@ public class MapperQueryParser extends QueryParser {
     public Query getFieldQuery(String field, String queryText, boolean quoted) throws ParseException {
         FieldQueryExtension fieldQueryExtension = fieldQueryExtensions.get(field);
         if (fieldQueryExtension != null) {
-            return fieldQueryExtension.query(context, queryText);
+            return fieldQueryExtension.query(parseContext, queryText);
         }
         Collection<String> fields = extractMultiFields(field);
         if (fields != null) {
@@ -226,27 +226,27 @@ public class MapperQueryParser extends QueryParser {
             if (quoted) {
                 setAnalyzer(quoteAnalyzer);
                 if (quoteFieldSuffix != null) {
-                    currentFieldType = context.fieldMapper(field + quoteFieldSuffix);
+                    currentFieldType = parseContext.fieldMapper(field + quoteFieldSuffix);
                 }
             }
             if (currentFieldType == null) {
-                currentFieldType = context.fieldMapper(field);
+                currentFieldType = parseContext.fieldMapper(field);
             }
             if (currentFieldType != null) {
                 if (quoted) {
                     if (!forcedQuoteAnalyzer) {
-                        setAnalyzer(context.getSearchQuoteAnalyzer(currentFieldType));
+                        setAnalyzer(parseContext.getSearchQuoteAnalyzer(currentFieldType));
                     }
                 } else {
                     if (!forcedAnalyzer) {
-                        setAnalyzer(context.getSearchAnalyzer(currentFieldType));
+                        setAnalyzer(parseContext.getSearchAnalyzer(currentFieldType));
                     }
                 }
                 if (currentFieldType != null) {
                     Query query = null;
                     if (currentFieldType.useTermQueryWithQueryString()) {
                         try {
-                            query = currentFieldType.termQuery(queryText, context);
+                            query = currentFieldType.termQuery(queryText, parseContext);
                         } catch (RuntimeException e) {
                             if (settings.lenient()) {
                                 return null;
@@ -357,7 +357,7 @@ public class MapperQueryParser extends QueryParser {
     }
 
     private Query getRangeQuerySingle(String field, String part1, String part2, boolean startInclusive, boolean endInclusive) {
-        currentFieldType = context.fieldMapper(field);
+        currentFieldType = parseContext.fieldMapper(field);
         if (currentFieldType != null) {
             if (lowercaseExpandedTerms && !currentFieldType.isNumeric()) {
                 part1 = part1 == null ? null : part1.toLowerCase(locale);
@@ -422,7 +422,7 @@ public class MapperQueryParser extends QueryParser {
     }
 
     private Query getFuzzyQuerySingle(String field, String termStr, String minSimilarity) throws ParseException {
-        currentFieldType = context.fieldMapper(field);
+        currentFieldType = parseContext.fieldMapper(field);
         if (currentFieldType != null) {
             try {
                 return currentFieldType.fuzzyQuery(termStr, Fuzziness.build(minSimilarity), fuzzyPrefixLength, settings.fuzzyMaxExpansions(), FuzzyQuery.defaultTranspositions);
@@ -492,14 +492,14 @@ public class MapperQueryParser extends QueryParser {
         currentFieldType = null;
         Analyzer oldAnalyzer = getAnalyzer();
         try {
-            currentFieldType = context.fieldMapper(field);
+            currentFieldType = parseContext.fieldMapper(field);
             if (currentFieldType != null) {
                 if (!forcedAnalyzer) {
-                    setAnalyzer(context.getSearchAnalyzer(currentFieldType));
+                    setAnalyzer(parseContext.getSearchAnalyzer(currentFieldType));
                 }
                 Query query = null;
                 if (currentFieldType.useTermQueryWithQueryString()) {
-                    query = currentFieldType.prefixQuery(termStr, multiTermRewriteMethod, context);
+                    query = currentFieldType.prefixQuery(termStr, multiTermRewriteMethod, parseContext);
                 }
                 if (query == null) {
                     query = getPossiblyAnalyzedPrefixQuery(currentFieldType.names().indexName(), termStr);
@@ -584,7 +584,7 @@ public class MapperQueryParser extends QueryParser {
                     return newMatchAllDocsQuery();
                 }
                 // effectively, we check if a field exists or not
-                return fieldQueryExtensions.get(ExistsFieldQueryExtension.NAME).query(context, actualField);
+                return fieldQueryExtensions.get(ExistsFieldQueryExtension.NAME).query(parseContext, actualField);
             }
         }
         if (lowercaseExpandedTerms) {
@@ -633,10 +633,10 @@ public class MapperQueryParser extends QueryParser {
         currentFieldType = null;
         Analyzer oldAnalyzer = getAnalyzer();
         try {
-            currentFieldType = context.fieldMapper(field);
+            currentFieldType = parseContext.fieldMapper(field);
             if (currentFieldType != null) {
                 if (!forcedAnalyzer) {
-                    setAnalyzer(context.getSearchAnalyzer(currentFieldType));
+                    setAnalyzer(parseContext.getSearchAnalyzer(currentFieldType));
                 }
                 indexedNameField = currentFieldType.names().indexName();
                 return getPossiblyAnalyzedWildcardQuery(indexedNameField, termStr);
@@ -765,14 +765,14 @@ public class MapperQueryParser extends QueryParser {
         currentFieldType = null;
         Analyzer oldAnalyzer = getAnalyzer();
         try {
-            currentFieldType = context.fieldMapper(field);
+            currentFieldType = parseContext.fieldMapper(field);
             if (currentFieldType != null) {
                 if (!forcedAnalyzer) {
-                    setAnalyzer(context.getSearchAnalyzer(currentFieldType));
+                    setAnalyzer(parseContext.getSearchAnalyzer(currentFieldType));
                 }
                 Query query = null;
                 if (currentFieldType.useTermQueryWithQueryString()) {
-                    query = currentFieldType.regexpQuery(termStr, RegExp.ALL, maxDeterminizedStates, multiTermRewriteMethod, context);
+                    query = currentFieldType.regexpQuery(termStr, RegExp.ALL, maxDeterminizedStates, multiTermRewriteMethod, parseContext);
                 }
                 if (query == null) {
                     query = super.getRegexpQuery(field, termStr);
@@ -830,7 +830,7 @@ public class MapperQueryParser extends QueryParser {
     private Collection<String> extractMultiFields(String field) {
         Collection<String> fields = null;
         if (field != null) {
-            fields = context.simpleMatchToIndexNames(field);
+            fields = parseContext.simpleMatchToIndexNames(field);
         } else {
             fields = settings.fields();
         }
diff --git a/core/src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java b/core/src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java
index f9fc8c9..ed1b704 100644
--- a/core/src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java
+++ b/core/src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java
@@ -21,8 +21,8 @@ package org.apache.lucene.queryparser.classic;
 
 import org.apache.lucene.search.ConstantScoreQuery;
 import org.apache.lucene.search.Query;
-import org.elasticsearch.index.query.MissingQueryBuilder;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.MissingQueryParser;
+import org.elasticsearch.index.query.QueryParseContext;
 
 /**
  *
@@ -32,11 +32,8 @@ public class MissingFieldQueryExtension implements FieldQueryExtension {
     public static final String NAME = "_missing_";
 
     @Override
-    public Query query(QueryShardContext context, String queryText) {
-        Query query = MissingQueryBuilder.newFilter(context, queryText, MissingQueryBuilder.DEFAULT_EXISTENCE_VALUE, MissingQueryBuilder.DEFAULT_NULL_VALUE);
-        if (query != null) {
-            return new ConstantScoreQuery(query);
-        }
-        return null;
+    public Query query(QueryParseContext parseContext, String queryText) {
+        return new ConstantScoreQuery(MissingQueryParser.newFilter(parseContext, queryText,
+                MissingQueryParser.DEFAULT_EXISTENCE_VALUE, MissingQueryParser.DEFAULT_NULL_VALUE, null));
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/ElasticsearchException.java b/core/src/main/java/org/elasticsearch/ElasticsearchException.java
index d1693e5..4f47805 100644
--- a/core/src/main/java/org/elasticsearch/ElasticsearchException.java
+++ b/core/src/main/java/org/elasticsearch/ElasticsearchException.java
@@ -580,7 +580,6 @@ public class ElasticsearchException extends RuntimeException implements ToXConte
                 org.elasticsearch.index.engine.RecoveryEngineException.class,
                 org.elasticsearch.common.blobstore.BlobStoreException.class,
                 org.elasticsearch.index.snapshots.IndexShardRestoreException.class,
-                org.elasticsearch.index.query.QueryShardException.class,
                 org.elasticsearch.index.query.QueryParsingException.class,
                 org.elasticsearch.action.support.replication.TransportReplicationAction.RetryOnPrimaryException.class,
                 org.elasticsearch.index.engine.DeleteByQueryFailedEngineException.class,
diff --git a/core/src/main/java/org/elasticsearch/Version.java b/core/src/main/java/org/elasticsearch/Version.java
index 624aa02..494ae24 100644
--- a/core/src/main/java/org/elasticsearch/Version.java
+++ b/core/src/main/java/org/elasticsearch/Version.java
@@ -259,8 +259,9 @@ public class Version {
     public static final Version V_2_0_0 = new Version(V_2_0_0_ID, true, org.apache.lucene.util.Version.LUCENE_5_2_1);
     public static final int V_2_1_0_ID = 2010099;
     public static final Version V_2_1_0 = new Version(V_2_1_0_ID, true, org.apache.lucene.util.Version.LUCENE_5_3_0);
-
-    public static final Version CURRENT = V_2_1_0;
+    public static final int V_3_0_0_ID = 3000099;
+    public static final Version V_3_0_0 = new Version(V_3_0_0_ID, true, org.apache.lucene.util.Version.LUCENE_5_3_0);
+    public static final Version CURRENT = V_3_0_0;
 
     static {
         assert CURRENT.luceneVersion.equals(Lucene.VERSION) : "Version must be upgraded to [" + Lucene.VERSION + "] is still set to [" + CURRENT.luceneVersion + "]";
@@ -272,6 +273,8 @@ public class Version {
 
     public static Version fromId(int id) {
         switch (id) {
+            case V_3_0_0_ID:
+                return V_3_0_0;
             case V_2_1_0_ID:
                 return V_2_1_0;
             case V_2_0_0_ID:
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
index d73e0d2..6fefa0d 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
@@ -42,7 +42,6 @@ import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.engine.Engine;
 import org.elasticsearch.index.query.IndexQueryParserService;
-import org.elasticsearch.index.query.QueryShardException;
 import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.indices.IndicesService;
@@ -189,8 +188,8 @@ public class TransportValidateQueryAction extends TransportBroadcastAction<Valid
             }
             if (request.rewrite()) {
                 explanation = getRewrittenQuery(searcher.searcher(), searchContext.query());
-            }
-        } catch (QueryShardException|QueryParsingException e) {
+            }   
+        } catch (QueryParsingException e) {
             valid = false;
             error = e.getDetailedMessage();
         } catch (AssertionError|IOException e) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequestBuilder.java
index 515ecd1..4acdfdc 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequestBuilder.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.admin.indices.validate.query;
 
+import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.action.support.broadcast.BroadcastOperationRequestBuilder;
 import org.elasticsearch.client.ElasticsearchClient;
diff --git a/core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java b/core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java
index 6995859..d9c89e7 100644
--- a/core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java
+++ b/core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java
@@ -41,7 +41,7 @@ import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.index.IndexService;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.script.ScriptService;
@@ -166,10 +166,10 @@ public class TransportExistsAction extends TransportBroadcastAction<ExistsReques
             BytesReference source = request.querySource();
             if (source != null && source.length() > 0) {
                 try {
-                    QueryShardContext.setTypes(request.types());
+                    QueryParseContext.setTypes(request.types());
                     context.parsedQuery(indexService.queryParserService().parseQuery(source));
                 } finally {
-                    QueryShardContext.removeTypes();
+                    QueryParseContext.removeTypes();
                 }
             }
             context.preProcess();
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/AliasAction.java b/core/src/main/java/org/elasticsearch/cluster/metadata/AliasAction.java
index b69849b..a039ce8 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/AliasAction.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/AliasAction.java
@@ -154,14 +154,14 @@ public class AliasAction implements Streamable {
         }
     }
 
-    public AliasAction filter(QueryBuilder filterBuilder) {
-        if (filterBuilder == null) {
+    public AliasAction filter(QueryBuilder queryBuilder) {
+        if (queryBuilder == null) {
             this.filter = null;
             return this;
         }
         try {
             XContentBuilder builder = XContentFactory.jsonBuilder();
-            filterBuilder.toXContent(builder, ToXContent.EMPTY_PARAMS);
+            queryBuilder.toXContent(builder, ToXContent.EMPTY_PARAMS);
             builder.close();
             this.filter = builder.string();
             return this;
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/AliasValidator.java b/core/src/main/java/org/elasticsearch/cluster/metadata/AliasValidator.java
index d5b398c..f12824d 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/AliasValidator.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/AliasValidator.java
@@ -28,7 +28,7 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.Index;
 import org.elasticsearch.index.query.IndexQueryParserService;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.indices.InvalidAliasNameException;
 
 import java.io.IOException;
@@ -142,10 +142,10 @@ public class AliasValidator extends AbstractComponent {
     }
 
     private void validateAliasFilter(XContentParser parser, IndexQueryParserService indexQueryParserService) throws IOException {
-        QueryShardContext context = indexQueryParserService.getShardContext();
+        QueryParseContext context = indexQueryParserService.getParseContext();
         try {
             context.reset(parser);
-            context.parseContext().parseInnerFilter();
+            context.parseInnerFilter();
         } finally {
             context.reset(null);
             parser.close();
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/FilterStreamInput.java b/core/src/main/java/org/elasticsearch/common/io/stream/FilterStreamInput.java
index 5f3bd01..0dac786 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/FilterStreamInput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/FilterStreamInput.java
@@ -68,4 +68,4 @@ public abstract class FilterStreamInput extends StreamInput {
     public void setVersion(Version version) {
         delegate.setVersion(version);
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java b/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
index c2bbaa3..1b22a69 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
@@ -33,7 +33,6 @@ import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.text.StringAndBytesText;
 import org.elasticsearch.common.text.Text;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
 
@@ -46,18 +45,8 @@ import static org.elasticsearch.ElasticsearchException.readStackTrace;
 
 public abstract class StreamInput extends InputStream {
 
-    private final NamedWriteableRegistry namedWriteableRegistry;
-
     private Version version = Version.CURRENT;
 
-    protected StreamInput() {
-        this.namedWriteableRegistry = new NamedWriteableRegistry();
-    }
-
-    protected StreamInput(NamedWriteableRegistry namedWriteableRegistry) {
-        this.namedWriteableRegistry = namedWriteableRegistry;
-    }
-
     public Version getVersion() {
         return this.version;
     }
@@ -572,13 +561,6 @@ public abstract class StreamInput extends InputStream {
         throw new UnsupportedOperationException();
     }
 
-    /**
-     * Reads a {@link QueryBuilder} from the current stream
-     */
-    public QueryBuilder readQuery() throws IOException {
-        return readNamedWriteable(QueryBuilder.class);
-    }
-
     public static StreamInput wrap(BytesReference reference) {
         if (reference.hasArray() == false) {
             reference = reference.toBytesArray();
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java b/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
index a808919..536af8b 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
@@ -31,7 +31,6 @@ import org.elasticsearch.Version;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.text.Text;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.joda.time.ReadableInstant;
 
 import java.io.EOFException;
@@ -570,11 +569,4 @@ public abstract class StreamOutput extends OutputStream {
         writeString(namedWriteable.getWriteableName());
         namedWriteable.writeTo(this);
     }
-
-    /**
-     * Writes a {@link QueryBuilder} to the current stream
-     */
-    public void writeQuery(QueryBuilder queryBuilder) throws IOException {
-        writeNamedWriteable(queryBuilder);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/unit/Fuzziness.java b/core/src/main/java/org/elasticsearch/common/unit/Fuzziness.java
index bc48042..cfcd209 100644
--- a/core/src/main/java/org/elasticsearch/common/unit/Fuzziness.java
+++ b/core/src/main/java/org/elasticsearch/common/unit/Fuzziness.java
@@ -19,26 +19,22 @@
 package org.elasticsearch.common.unit;
 
 import com.google.common.base.Preconditions;
-
+import org.apache.lucene.search.FuzzyQuery;
+import org.apache.lucene.util.automaton.LevenshteinAutomata;
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
-import java.util.Locale;
-import java.util.Objects;
 
 /**
  * A unit class that encapsulates all in-exact search
  * parsing and conversion from similarities to edit distances
  * etc.
  */
-public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
+public final class Fuzziness implements ToXContent {
 
     public static final XContentBuilderString X_FIELD_NAME = new XContentBuilderString("fuzziness");
     public static final Fuzziness ZERO = new Fuzziness(0);
@@ -49,20 +45,13 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
 
     private final String fuzziness;
 
-    /** the prototype constant is intended for deserialization when used with
-     * {@link org.elasticsearch.common.io.stream.StreamableReader#readFrom(StreamInput)} */
-    static final Fuzziness PROTOTYPE = AUTO;
-
     private Fuzziness(int fuzziness) {
         Preconditions.checkArgument(fuzziness >= 0 && fuzziness <= 2, "Valid edit distances are [0, 1, 2] but was [" + fuzziness + "]");
         this.fuzziness = Integer.toString(fuzziness);
     }
 
     private Fuzziness(String fuzziness) {
-        if (fuzziness == null) {
-            throw new IllegalArgumentException("fuzziness can't be null!");
-        }
-        this.fuzziness = fuzziness.toUpperCase(Locale.ROOT);
+        this.fuzziness = fuzziness;
     }
 
     /**
@@ -132,7 +121,7 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
     }
 
     public int asDistance(String text) {
-        if (this.equals(AUTO)) { //AUTO
+        if (this == AUTO) { //AUTO
             final int len = termLen(text);
             if (len <= 2) {
                 return 0;
@@ -146,7 +135,7 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
     }
 
     public TimeValue asTimeValue() {
-        if (this.equals(AUTO)) {
+        if (this == AUTO) {
             return TimeValue.timeValueMillis(1);
         } else {
             return TimeValue.parseTimeValue(fuzziness.toString(), null, "fuzziness");
@@ -154,7 +143,7 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
     }
 
     public long asLong() {
-        if (this.equals(AUTO)) {
+        if (this == AUTO) {
             return 1;
         }
         try {
@@ -165,7 +154,7 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
     }
 
     public int asInt() {
-        if (this.equals(AUTO)) {
+        if (this == AUTO) {
             return 1;
         }
         try {
@@ -176,7 +165,7 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
     }
 
     public short asShort() {
-        if (this.equals(AUTO)) {
+        if (this == AUTO) {
             return 1;
         }
         try {
@@ -187,7 +176,7 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
     }
 
     public byte asByte() {
-        if (this.equals(AUTO)) {
+        if (this == AUTO) {
             return 1;
         }
         try {
@@ -198,14 +187,14 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
     }
 
     public double asDouble() {
-        if (this.equals(AUTO)) {
+        if (this == AUTO) {
             return 1d;
         }
         return Double.parseDouble(fuzziness.toString());
     }
 
     public float asFloat() {
-        if (this.equals(AUTO)) {
+        if (this == AUTO) {
             return 1f;
         }
         return Float.parseFloat(fuzziness.toString());
@@ -218,35 +207,4 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
     public String asString() {
         return fuzziness.toString();
     }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (this == obj) {
-            return true;
-        }
-        if (obj == null || getClass() != obj.getClass()) {
-            return false;
-        }
-        Fuzziness other = (Fuzziness) obj;
-        return Objects.equals(fuzziness, other.fuzziness);
-    }
-
-    @Override
-    public int hashCode() {
-        return fuzziness.hashCode();
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeString(fuzziness);
-    }
-
-    @Override
-    public Fuzziness readFrom(StreamInput in) throws IOException {
-        return new Fuzziness(in.readString());
-    }
-
-    public static Fuzziness readFuzzinessFrom(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/util/concurrent/BaseFuture.java b/core/src/main/java/org/elasticsearch/common/util/concurrent/BaseFuture.java
index 2ef8e19..a48eb60 100644
--- a/core/src/main/java/org/elasticsearch/common/util/concurrent/BaseFuture.java
+++ b/core/src/main/java/org/elasticsearch/common/util/concurrent/BaseFuture.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.common.util.concurrent;
 
-import com.google.common.annotations.Beta;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.transport.Transports;
 
@@ -195,7 +194,6 @@ public abstract class BaseFuture<V> implements Future<V> {
         return result;
     }
 
-    @Beta
     protected void done() {
     }
 
diff --git a/core/src/main/java/org/elasticsearch/index/indexing/IndexingOperationListener.java b/core/src/main/java/org/elasticsearch/index/indexing/IndexingOperationListener.java
index 8b95e0f..bb4c109 100644
--- a/core/src/main/java/org/elasticsearch/index/indexing/IndexingOperationListener.java
+++ b/core/src/main/java/org/elasticsearch/index/indexing/IndexingOperationListener.java
@@ -43,13 +43,20 @@ public abstract class IndexingOperationListener {
     }
 
     /**
-     * Called after the indexing operation occurred.
+     * Called after create index operation occurred.
      */
     public void postCreate(Engine.Create create) {
 
     }
 
     /**
+     * Called after create index operation occurred with exception.
+     */
+    public void postCreate(Engine.Create create, Throwable ex) {
+
+    }
+
+    /**
      * Called before the indexing occurs.
      */
     public Engine.Index preIndex(Engine.Index index) {
@@ -74,6 +81,13 @@ public abstract class IndexingOperationListener {
     }
 
     /**
+     * Called after the indexing operation occurred with exception.
+     */
+    public void postIndex(Engine.Index index, Throwable ex) {
+
+    }
+
+    /**
      * Called before the delete occurs.
      */
     public Engine.Delete preDelete(Engine.Delete delete) {
@@ -96,4 +110,11 @@ public abstract class IndexingOperationListener {
     public void postDelete(Engine.Delete delete) {
 
     }
+
+    /**
+     * Called after the delete operation occurred with exception.
+     */
+    public void postDelete(Engine.Delete delete, Throwable ex) {
+
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java b/core/src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java
index 2109eaf..7766a31 100644
--- a/core/src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java
+++ b/core/src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java
@@ -99,7 +99,7 @@ public class ShardIndexingService extends AbstractIndexShardComponent {
             try {
                 listener.postCreateUnderLock(create);
             } catch (Exception e) {
-                logger.warn("post listener [{}] failed", e, listener);
+                logger.warn("postCreateUnderLock listener [{}] failed", e, listener);
             }
         }
     }
@@ -124,12 +124,19 @@ public class ShardIndexingService extends AbstractIndexShardComponent {
             try {
                 listener.postCreate(create);
             } catch (Exception e) {
-                logger.warn("post listener [{}] failed", e, listener);
+                logger.warn("postCreate listener [{}] failed", e, listener);
             }
         }
     }
 
     public void postCreate(Engine.Create create, Throwable ex) {
+        for (IndexingOperationListener listener : listeners) {
+            try {
+                listener.postCreate(create, ex);
+            } catch (Throwable t) {
+                logger.warn("postCreate listener [{}] failed", t, listener);
+            }
+        }
     }
 
     public Engine.Index preIndex(Engine.Index index) {
@@ -146,7 +153,7 @@ public class ShardIndexingService extends AbstractIndexShardComponent {
             try {
                 listener.postIndexUnderLock(index);
             } catch (Exception e) {
-                logger.warn("post listener [{}] failed", e, listener);
+                logger.warn("postIndexUnderLock listener [{}] failed", e, listener);
             }
         }
     }
@@ -163,7 +170,7 @@ public class ShardIndexingService extends AbstractIndexShardComponent {
             try {
                 listener.postIndex(index);
             } catch (Exception e) {
-                logger.warn("post listener [{}] failed", e, listener);
+                logger.warn("postIndex listener [{}] failed", e, listener);
             }
         }
     }
@@ -171,6 +178,13 @@ public class ShardIndexingService extends AbstractIndexShardComponent {
     public void postIndex(Engine.Index index, Throwable ex) {
         totalStats.indexCurrent.dec();
         typeStats(index.type()).indexCurrent.dec();
+        for (IndexingOperationListener listener : listeners) {
+            try {
+                listener.postIndex(index, ex);
+            } catch (Throwable t) {
+                logger.warn("postIndex listener [{}] failed", t, listener);
+            }
+        }
     }
 
     public Engine.Delete preDelete(Engine.Delete delete) {
@@ -187,7 +201,7 @@ public class ShardIndexingService extends AbstractIndexShardComponent {
             try {
                 listener.postDeleteUnderLock(delete);
             } catch (Exception e) {
-                logger.warn("post listener [{}] failed", e, listener);
+                logger.warn("postDeleteUnderLock listener [{}] failed", e, listener);
             }
         }
     }
@@ -203,7 +217,7 @@ public class ShardIndexingService extends AbstractIndexShardComponent {
             try {
                 listener.postDelete(delete);
             } catch (Exception e) {
-                logger.warn("post listener [{}] failed", e, listener);
+                logger.warn("postDelete listener [{}] failed", e, listener);
             }
         }
     }
@@ -211,6 +225,13 @@ public class ShardIndexingService extends AbstractIndexShardComponent {
     public void postDelete(Engine.Delete delete, Throwable ex) {
         totalStats.deleteCurrent.dec();
         typeStats(delete.type()).deleteCurrent.dec();
+        for (IndexingOperationListener listener : listeners) {
+            try {
+                listener. postDelete(delete, ex);
+            } catch (Throwable t) {
+                logger.warn("postDelete listener [{}] failed", t, listener);
+            }
+        }
     }
 
     public void noopUpdate(String type) {
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java b/core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java
index 9b8d4e8..112a8f4 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java
@@ -33,7 +33,7 @@ import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.unit.Fuzziness;
 import org.elasticsearch.index.analysis.NamedAnalyzer;
 import org.elasticsearch.index.fielddata.FieldDataType;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.similarity.SimilarityProvider;
 
 import java.io.IOException;
@@ -437,7 +437,7 @@ public abstract class MappedFieldType extends FieldType {
     }
 
     /**
-     * Should the field query {@link #termQuery(Object, org.elasticsearch.index.query.QueryShardContext)}  be used when detecting this
+     * Should the field query {@link #termQuery(Object, org.elasticsearch.index.query.QueryParseContext)}  be used when detecting this
      * field in query string.
      */
     public boolean useTermQueryWithQueryString() {
@@ -449,11 +449,11 @@ public abstract class MappedFieldType extends FieldType {
         return new Term(names().indexName(), indexedValueForSearch(value));
     }
 
-    public Query termQuery(Object value, @Nullable QueryShardContext context) {
+    public Query termQuery(Object value, @Nullable QueryParseContext context) {
         return new TermQuery(createTerm(value));
     }
 
-    public Query termsQuery(List values, @Nullable QueryShardContext context) {
+    public Query termsQuery(List values, @Nullable QueryParseContext context) {
         BytesRef[] bytesRefs = new BytesRef[values.size()];
         for (int i = 0; i < bytesRefs.length; i++) {
             bytesRefs[i] = indexedValueForSearch(values.get(i));
@@ -472,7 +472,7 @@ public abstract class MappedFieldType extends FieldType {
         return new FuzzyQuery(createTerm(value), fuzziness.asDistance(BytesRefs.toString(value)), prefixLength, maxExpansions, transpositions);
     }
 
-    public Query prefixQuery(String value, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryShardContext context) {
+    public Query prefixQuery(String value, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryParseContext context) {
         PrefixQuery query = new PrefixQuery(createTerm(value));
         if (method != null) {
             query.setRewriteMethod(method);
@@ -480,7 +480,7 @@ public abstract class MappedFieldType extends FieldType {
         return query;
     }
 
-    public Query regexpQuery(String value, int flags, int maxDeterminizedStates, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryShardContext context) {
+    public Query regexpQuery(String value, int flags, int maxDeterminizedStates, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryParseContext context) {
         RegexpQuery query = new RegexpQuery(createTerm(value), flags, maxDeterminizedStates);
         if (method != null) {
             query.setRewriteMethod(method);
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java
index e538a00..f872207 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java
@@ -40,7 +40,7 @@ import org.elasticsearch.index.mapper.MergeMappingException;
 import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.MetadataFieldMapper;
 import org.elasticsearch.index.mapper.ParseContext;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.similarity.SimilarityLookupService;
 
 import java.io.IOException;
@@ -186,7 +186,7 @@ public class AllFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query termQuery(Object value, QueryShardContext context) {
+        public Query termQuery(Object value, QueryParseContext context) {
             return queryStringTermQuery(createTerm(value));
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java
index f6e09b2..63fa41f 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java
@@ -49,7 +49,7 @@ import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.MetadataFieldMapper;
 import org.elasticsearch.index.mapper.ParseContext;
 import org.elasticsearch.index.mapper.Uid;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 
 import java.io.IOException;
 import java.util.Collection;
@@ -167,7 +167,7 @@ public class IdFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query termQuery(Object value, @Nullable QueryShardContext context) {
+        public Query termQuery(Object value, @Nullable QueryParseContext context) {
             if (indexOptions() != IndexOptions.NONE || context == null) {
                 return super.termQuery(value, context);
             }
@@ -176,7 +176,7 @@ public class IdFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query termsQuery(List values, @Nullable QueryShardContext context) {
+        public Query termsQuery(List values, @Nullable QueryParseContext context) {
             if (indexOptions() != IndexOptions.NONE || context == null) {
                 return super.termsQuery(values, context);
             }
@@ -184,7 +184,7 @@ public class IdFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query prefixQuery(String value, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryShardContext context) {
+        public Query prefixQuery(String value, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryParseContext context) {
             if (indexOptions() != IndexOptions.NONE || context == null) {
                 return super.prefixQuery(value, method, context);
             }
@@ -201,7 +201,7 @@ public class IdFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query regexpQuery(String value, int flags, int maxDeterminizedStates, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryShardContext context) {
+        public Query regexpQuery(String value, int flags, int maxDeterminizedStates, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryParseContext context) {
             if (indexOptions() != IndexOptions.NONE || context == null) {
                 return super.regexpQuery(value, flags, maxDeterminizedStates, method, context);
             }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java
index 1b7168a..3f395a8 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java
@@ -38,7 +38,7 @@ import org.elasticsearch.index.mapper.MergeMappingException;
 import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.MetadataFieldMapper;
 import org.elasticsearch.index.mapper.ParseContext;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 
 import java.io.IOException;
 import java.util.Iterator;
@@ -157,7 +157,7 @@ public class IndexFieldMapper extends MetadataFieldMapper {
          * indices
          */
         @Override
-        public Query termQuery(Object value, @Nullable QueryShardContext context) {
+        public Query termQuery(Object value, @Nullable QueryParseContext context) {
             if (context == null) {
                 return super.termQuery(value, context);
             }
@@ -171,7 +171,7 @@ public class IndexFieldMapper extends MetadataFieldMapper {
         
 
         @Override
-        public Query termsQuery(List values, QueryShardContext context) {
+        public Query termsQuery(List values, QueryParseContext context) {
             if (context == null) {
                 return super.termsQuery(values, context);
             }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java
index 7cd4ac0..5fcd10c 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java
@@ -43,7 +43,7 @@ import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.MetadataFieldMapper;
 import org.elasticsearch.index.mapper.ParseContext;
 import org.elasticsearch.index.mapper.Uid;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -189,12 +189,12 @@ public class ParentFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query termQuery(Object value, @Nullable QueryShardContext context) {
+        public Query termQuery(Object value, @Nullable QueryParseContext context) {
             return termsQuery(Collections.singletonList(value), context);
         }
 
         @Override
-        public Query termsQuery(List values, @Nullable QueryShardContext context) {
+        public Query termsQuery(List values, @Nullable QueryParseContext context) {
             if (context == null) {
                 return super.termsQuery(values, context);
             }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java
index 12e40de..480d2a4 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java
@@ -43,7 +43,7 @@ import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.MetadataFieldMapper;
 import org.elasticsearch.index.mapper.ParseContext;
 import org.elasticsearch.index.mapper.Uid;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 
 import java.io.IOException;
 import java.util.List;
@@ -137,7 +137,7 @@ public class TypeFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query termQuery(Object value, @Nullable QueryShardContext context) {
+        public Query termQuery(Object value, @Nullable QueryParseContext context) {
             if (indexOptions() == IndexOptions.NONE) {
                 return new ConstantScoreQuery(new PrefixQuery(new Term(UidFieldMapper.NAME, Uid.typePrefixAsBytes(BytesRefs.toBytesRef(value)))));
             }
diff --git a/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java b/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java
index 52930e8..91ff1de 100644
--- a/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java
+++ b/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java
@@ -42,7 +42,7 @@ import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
 import org.elasticsearch.index.percolator.stats.ShardPercolateService;
 import org.elasticsearch.index.query.IndexQueryParserService;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.settings.IndexSettings;
 import org.elasticsearch.index.shard.AbstractIndexShardComponent;
@@ -185,13 +185,12 @@ public class PercolatorQueriesRegistry extends AbstractIndexShardComponent imple
         }
     }
 
-    //norelease this method parses from xcontent to lucene query, need to re-investigate how to split context here
     private Query parseQuery(String type, XContentParser parser) {
         String[] previousTypes = null;
         if (type != null) {
-            QueryShardContext.setTypesWithPrevious(new String[]{type});
+            QueryParseContext.setTypesWithPrevious(new String[]{type});
         }
-        QueryShardContext context = queryParserService.getShardContext();
+        QueryParseContext context = queryParserService.getParseContext();
         try {
             context.reset(parser);
             // This means that fields in the query need to exist in the mapping prior to registering this query
@@ -210,10 +209,10 @@ public class PercolatorQueriesRegistry extends AbstractIndexShardComponent imple
             context.setMapUnmappedFieldAsString(mapUnmappedFieldsAsString ? true : false);
             return queryParserService.parseInnerQuery(context);
         } catch (IOException e) {
-            throw new QueryParsingException(context.parseContext(), "Failed to parse", e);
+            throw new QueryParsingException(context, "Failed to parse", e);
         } finally {
             if (type != null) {
-                QueryShardContext.setTypes(previousTypes);
+                QueryParseContext.setTypes(previousTypes);
             }
             context.reset(null);
         }
diff --git a/core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java
deleted file mode 100644
index b248dc1..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java
+++ /dev/null
@@ -1,311 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.action.support.ToXContentToBytes;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.BytesRefs;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentType;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.List;
-import java.util.Objects;
-
-/**
- * Base class for all classes producing lucene queries.
- * Supports conversion to BytesReference and creation of lucene Query objects.
- */
-public abstract class AbstractQueryBuilder<QB extends AbstractQueryBuilder> extends ToXContentToBytes implements QueryBuilder<QB> {
-
-    /** Default for boost to apply to resulting Lucene query. Defaults to 1.0*/
-    public static final float DEFAULT_BOOST = 1.0f;
-
-    protected String queryName;
-    protected float boost = DEFAULT_BOOST;
-
-    protected AbstractQueryBuilder() {
-        super(XContentType.JSON);
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        doXContent(builder, params);
-        builder.endObject();
-        return builder;
-    }
-
-    protected abstract void doXContent(XContentBuilder builder, Params params) throws IOException;
-
-    protected void printBoostAndQueryName(XContentBuilder builder) throws IOException {
-        builder.field("boost", boost);
-        if (queryName != null) {
-            builder.field("_name", queryName);
-        }
-    }
-
-    @Override
-    public final Query toQuery(QueryShardContext context) throws IOException {
-        Query query = doToQuery(context);
-        if (query != null) {
-            query.setBoost(boost);
-            if (queryName != null) {
-                context.addNamedQuery(queryName, query);
-            }
-        }
-        return query;
-    }
-
-    @Override
-    public final Query toFilter(QueryShardContext context) throws IOException {
-        Query result = null;
-            final boolean originalIsFilter = context.isFilter;
-            try {
-                context.isFilter = true;
-                result = toQuery(context);
-            } finally {
-                context.isFilter = originalIsFilter;
-            }
-        return result;
-    }
-
-    //norelease to be made abstract once all query builders override doToQuery providing their own specific implementation.
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return context.indexQueryParserService().indicesQueriesRegistry().queryParsers().get(getName()).parse(context);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        // default impl does not validate, subclasses should override.
-        //norelease to be possibly made abstract once all queries support validation
-        return null;
-    }
-
-    /**
-     * Returns the query name for the query.
-     */
-    @SuppressWarnings("unchecked")
-    @Override
-    public final QB queryName(String queryName) {
-        this.queryName = queryName;
-        return (QB) this;
-    }
-
-    /**
-     * Sets the query name for the query.
-     */
-    @Override
-    public final String queryName() {
-        return queryName;
-    }
-
-    /**
-     * Returns the boost for this query.
-     */
-    @Override
-    public final float boost() {
-        return this.boost;
-    }
-
-    /**
-     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
-     * weightings) have their score multiplied by the boost provided.
-     */
-    @SuppressWarnings("unchecked")
-    @Override
-    public final QB boost(float boost) {
-        this.boost = boost;
-        return (QB) this;
-    }
-
-    @Override
-    public final QB readFrom(StreamInput in) throws IOException {
-        QB queryBuilder = doReadFrom(in);
-        queryBuilder.boost = in.readFloat();
-        queryBuilder.queryName = in.readOptionalString();
-        return queryBuilder;
-    }
-
-    //norelease make this abstract once all builders implement doReadFrom themselves
-    protected QB doReadFrom(StreamInput in) throws IOException {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public final void writeTo(StreamOutput out) throws IOException {
-        doWriteTo(out);
-        out.writeFloat(boost);
-        out.writeOptionalString(queryName);
-    }
-
-    //norelease make this abstract once all builders implement doWriteTo themselves
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        throw new UnsupportedOperationException();
-    }
-
-    protected final QueryValidationException addValidationError(String validationError, QueryValidationException validationException) {
-        return QueryValidationException.addValidationError(getName(), validationError, validationException);
-    }
-
-    @Override
-    public final boolean equals(Object obj) {
-        if (this == obj) {
-            return true;
-        }
-        if (obj == null || getClass() != obj.getClass()) {
-            return false;
-        }
-        @SuppressWarnings("unchecked")
-        QB other = (QB) obj;
-        return Objects.equals(queryName, other.queryName) &&
-                Objects.equals(boost, other.boost) &&
-                doEquals(other);
-    }
-
-    /**
-     * Indicates whether some other {@link QueryBuilder} object of the same type is "equal to" this one.
-     */
-    //norelease to be made abstract once all queries are refactored
-    protected boolean doEquals(QB other) {
-        return super.equals(other);
-    }
-
-    @Override
-    public final int hashCode() {
-        return Objects.hash(getClass(), queryName, boost, doHashCode());
-    }
-
-    //norelease to be made abstract once all queries are refactored
-    protected int doHashCode() {
-        return super.hashCode();
-    }
-
-    /**
-     * This helper method checks if the object passed in is a string, if so it
-     * converts it to a {@link BytesRef}.
-     * @param obj the input object
-     * @return the same input object or a {@link BytesRef} representation if input was of type string
-     */
-    protected static Object convertToBytesRefIfString(Object obj) {
-        if (obj instanceof String) {
-            return BytesRefs.toBytesRef(obj);
-        }
-        return obj;
-    }
-
-    /**
-     * This helper method checks if the object passed in is a {@link BytesRef}, if so it
-     * converts it to a utf8 string.
-     * @param obj the input object
-     * @return the same input object or a utf8 string if input was of type {@link BytesRef}
-     */
-    protected static Object convertToStringIfBytesRef(Object obj) {
-        if (obj instanceof BytesRef) {
-            return ((BytesRef) obj).utf8ToString();
-        }
-        return obj;
-    }
-
-    /**
-     * Helper method to convert collection of {@link QueryBuilder} instances to lucene
-     * {@link Query} instances. {@link QueryBuilder} that return <tt>null</tt> calling
-     * their {@link QueryBuilder#toQuery(QueryShardContext)} method are not added to the
-     * resulting collection.
-     *
-     * @throws IOException
-     * @throws QueryShardException
-     */
-    protected static Collection<Query> toQueries(Collection<QueryBuilder> queryBuilders, QueryShardContext context) throws QueryShardException,
-            IOException {
-        List<Query> queries = new ArrayList<>(queryBuilders.size());
-        for (QueryBuilder queryBuilder : queryBuilders) {
-            Query query = queryBuilder.toQuery(context);
-            if (query != null) {
-                queries.add(query);
-            }
-        }
-        return queries;
-    }
-
-    protected QueryValidationException validateInnerQueries(List<QueryBuilder> queryBuilders, QueryValidationException initialValidationException) {
-        QueryValidationException validationException = initialValidationException;
-        for (QueryBuilder queryBuilder : queryBuilders) {
-            validationException = validateInnerQuery(queryBuilder, validationException);
-        }
-        return validationException;
-    }
-
-    protected QueryValidationException validateInnerQuery(QueryBuilder queryBuilder, QueryValidationException initialValidationException) {
-        QueryValidationException validationException = initialValidationException;
-        if (queryBuilder != null) {
-            QueryValidationException queryValidationException = queryBuilder.validate();
-            if (queryValidationException != null) {
-                validationException = QueryValidationException.addValidationErrors(queryValidationException.validationErrors(), validationException);
-            }
-        } else {
-            validationException = addValidationError("inner query cannot be null", validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    public String getName() {
-        //default impl returns the same as writeable name, but we keep the distinction between the two just to make sure
-        return getWriteableName();
-    }
-
-    protected final void writeQueries(StreamOutput out, List<? extends QueryBuilder> queries) throws IOException {
-        out.writeVInt(queries.size());
-        for (QueryBuilder query : queries) {
-            out.writeQuery(query);
-        }
-    }
-
-    protected final List<QueryBuilder> readQueries(StreamInput in) throws IOException {
-        List<QueryBuilder> queries = new ArrayList<>();
-        int size = in.readVInt();
-        for (int i = 0; i < size; i++) {
-            queries.add(in.readQuery());
-        }
-        return queries;
-    }
-
-    protected final void writeOptionalQuery(StreamOutput out, QueryBuilder query) throws IOException {
-        if (query == null) {
-            out.writeBoolean(false);
-        } else {
-            out.writeBoolean(true);
-            out.writeQuery(query);
-        }
-    }
-
-    protected final QueryBuilder readOptionalQuery(StreamInput in) throws IOException {
-        if (in.readBoolean()) {
-            return in.readQuery();
-        }
-        return null;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java
index 994649a..1d55663 100644
--- a/core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java
@@ -19,44 +19,30 @@
 
 package org.elasticsearch.index.query;
 
-import com.google.common.collect.Lists;
-
-import org.apache.lucene.search.BooleanClause.Occur;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-import java.util.Objects;
 
 /**
  * A filter that matches documents matching boolean combinations of other filters.
  * @deprecated Use {@link BoolQueryBuilder} instead
  */
 @Deprecated
-public class AndQueryBuilder extends AbstractQueryBuilder<AndQueryBuilder> {
-
-    public static final String NAME = "and";
+public class AndQueryBuilder extends QueryBuilder {
 
-    private final ArrayList<QueryBuilder> filters = new ArrayList<>();
+    private ArrayList<QueryBuilder> filters = new ArrayList<>();
 
-    static final AndQueryBuilder PROTOTYPE = new AndQueryBuilder();
+    private String queryName;
 
-    /**
-     * @param filters nested filters, no <tt>null</tt> values are allowed
-     */
     public AndQueryBuilder(QueryBuilder... filters) {
-        Collections.addAll(this.filters, filters);
+        for (QueryBuilder filter : filters) {
+            this.filters.add(filter);
+        }
     }
 
     /**
      * Adds a filter to the list of filters to "and".
-     * @param filterBuilder nested filter, no <tt>null</tt> value allowed
      */
     public AndQueryBuilder add(QueryBuilder filterBuilder) {
         filters.add(filterBuilder);
@@ -64,79 +50,24 @@ public class AndQueryBuilder extends AbstractQueryBuilder<AndQueryBuilder> {
     }
 
     /**
-     * @return the list of queries added to "and".
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
      */
-    public List<QueryBuilder> innerQueries() {
-        return this.filters;
+    public AndQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(AndQueryParser.NAME);
         builder.startArray("filters");
         for (QueryBuilder filter : filters) {
             filter.toXContent(builder, params);
         }
         builder.endArray();
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        if (filters.isEmpty()) {
-            // no filters provided, this should be ignored upstream
-            return null;
-        }
-
-        BooleanQuery query = new BooleanQuery();
-        for (QueryBuilder f : filters) {
-            Query innerQuery = f.toFilter(context);
-            // ignore queries that are null
-            if (innerQuery != null) {
-                query.add(innerQuery, Occur.MUST);
-            }
-        }
-        if (query.clauses().isEmpty()) {
-            // no inner lucene query exists, ignore upstream
-            return null;
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return query;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        return validateInnerQueries(filters, null);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(filters);
-    }
-
-    @Override
-    protected boolean doEquals(AndQueryBuilder other) {
-        return Objects.equals(filters, other.filters);
-    }
-
-    @Override
-    protected AndQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        AndQueryBuilder andQueryBuilder = new AndQueryBuilder();
-        List<QueryBuilder> queryBuilders = readQueries(in);
-        for (QueryBuilder queryBuilder : queryBuilders) {
-            andQueryBuilder.add(queryBuilder);
-        }
-        return andQueryBuilder;
-
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        writeQueries(out, filters);
+        builder.endObject();
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/AndQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/AndQueryParser.java
index a233ead..bb0e1cb 100644
--- a/core/src/main/java/org/elasticsearch/index/query/AndQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/AndQueryParser.java
@@ -19,6 +19,9 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
 
@@ -26,11 +29,12 @@ import java.io.IOException;
 import java.util.ArrayList;
 
 /**
- * Parser for and query
- * @deprecated use bool query instead
+ *
  */
 @Deprecated
-public class AndQueryParser extends BaseQueryParser<AndQueryBuilder> {
+public class AndQueryParser implements QueryParser {
+
+    public static final String NAME = "and";
 
     @Inject
     public AndQueryParser() {
@@ -38,25 +42,26 @@ public class AndQueryParser extends BaseQueryParser<AndQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{AndQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public AndQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        final ArrayList<QueryBuilder> queries = new ArrayList<>();
+        ArrayList<Query> queries = new ArrayList<>();
         boolean queriesFound = false;
 
         String queryName = null;
         String currentFieldName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         XContentParser.Token token = parser.currentToken();
         if (token == XContentParser.Token.START_ARRAY) {
             while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                 queriesFound = true;
-                QueryBuilder filter = parseContext.parseInnerFilterToQueryBuilder();
-                queries.add(filter);
+                Query filter = parseContext.parseInnerFilter();
+                if (filter != null) {
+                    queries.add(filter);
+                }
             }
         } else {
             while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -68,15 +73,23 @@ public class AndQueryParser extends BaseQueryParser<AndQueryBuilder> {
                     if ("filters".equals(currentFieldName)) {
                         queriesFound = true;
                         while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                            QueryBuilder filter = parseContext.parseInnerFilterToQueryBuilder();
-                            queries.add(filter);
+                            Query filter = parseContext.parseInnerFilter();
+                            if (filter != null) {
+                                queries.add(filter);
+                            }
+                        }
+                    } else {
+                        queriesFound = true;
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                            Query filter = parseContext.parseInnerFilter();
+                            if (filter != null) {
+                                queries.add(filter);
+                            }
                         }
                     }
                 } else if (token.isValue()) {
                     if ("_name".equals(currentFieldName)) {
                         queryName = parser.text();
-                    } else if ("boost".equals(currentFieldName)) {
-                        boost = parser.floatValue();
                     } else {
                         throw new QueryParsingException(parseContext, "[and] query does not support [" + currentFieldName + "]");
                     }
@@ -88,17 +101,18 @@ public class AndQueryParser extends BaseQueryParser<AndQueryBuilder> {
             throw new QueryParsingException(parseContext, "[and] query requires 'filters' to be set on it'");
         }
 
-        AndQueryBuilder andQuery = new AndQueryBuilder();
-        for (QueryBuilder query : queries) {
-            andQuery.add(query);
+        if (queries.isEmpty()) {
+            // no filters provided, this should be ignored upstream
+            return null;
         }
-        andQuery.queryName(queryName);
-        andQuery.boost(boost);
-        return andQuery;
-    }
 
-    @Override
-    public AndQueryBuilder getBuilderPrototype() {
-        return AndQueryBuilder.PROTOTYPE;
+        BooleanQuery query = new BooleanQuery();
+        for (Query f : queries) {
+            query.add(f, Occur.MUST);
+        }
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/BaseQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/BaseQueryParser.java
deleted file mode 100644
index 4ff02df..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/BaseQueryParser.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-
-import java.io.IOException;
-
-/**
- * Class used during the query parsers refactoring. Will be removed once we can parse search requests on the coordinating node.
- * All query parsers that have a refactored "fromXContent" method can be changed to extend this instead of {@link BaseQueryParserTemp}.
- * Keeps old {@link QueryParser#parse(QueryShardContext)} method as a stub delegating to
- * {@link QueryParser#fromXContent(QueryParseContext)} and {@link QueryBuilder#toQuery(QueryShardContext)}}
- */
-//norelease needs to be removed once we parse search requests on the coordinating node, as the parse method is not needed anymore at that point.
-public abstract class BaseQueryParser<QB extends QueryBuilder<QB>> implements QueryParser<QB> {
-
-    @Override
-    public final Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        return fromXContent(context.parseContext()).toQuery(context);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/BaseQueryParserTemp.java b/core/src/main/java/org/elasticsearch/index/query/BaseQueryParserTemp.java
deleted file mode 100644
index 4dc3eae..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/BaseQueryParserTemp.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-
-import java.io.IOException;
-
-/**
- * This class with method impl is an intermediate step in the query parsers refactoring.
- * Provides a fromXContent default implementation for query parsers that don't have yet a
- * specific fromXContent implementation that returns a QueryBuilder.
- */
-//norelease to be removed once all queries are moved over to extend BaseQueryParser
-public abstract class BaseQueryParserTemp implements QueryParser {
-
-    @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
-        Query query = parse(parseContext.shardContext());
-        return new QueryWrappingQueryBuilder(query);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java
deleted file mode 100644
index 6444184..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java
+++ /dev/null
@@ -1,171 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-
-import java.io.IOException;
-import java.util.Objects;
-
-public abstract class BaseTermQueryBuilder<QB extends BaseTermQueryBuilder<QB>> extends AbstractQueryBuilder<QB> {
-
-    /** Name of field to match against. */
-    protected final String fieldName;
-
-    /** Value to find matches for. */
-    protected final Object value;
-
-    /**
-     * Constructs a new base term query.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, String value) {
-        this(fieldName, (Object) value);
-    }
-
-    /**
-     * Constructs a new base term query.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, int value) {
-        this(fieldName, (Object) value);
-    }
-
-    /**
-     * Constructs a new base term query.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, long value) {
-        this(fieldName, (Object) value);
-    }
-
-    /**
-     * Constructs a new base term query.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, float value) {
-        this(fieldName, (Object) value);
-    }
-
-    /**
-     * Constructs a new base term query.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, double value) {
-        this(fieldName, (Object) value);
-    }
-
-    /**
-     * Constructs a new base term query.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, boolean value) {
-        this(fieldName, (Object) value);
-    }
-
-    /**
-     * Constructs a new base term query.
-     * In case value is assigned to a string, we internally convert it to a {@link BytesRef}
-     * because in {@link TermQueryParser} and {@link SpanTermQueryParser} string values are parsed to {@link BytesRef}
-     * and we want internal representation of query to be equal regardless of whether it was created from XContent or via Java API.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, Object value) {
-        this.fieldName = fieldName;
-        this.value = convertToBytesRefIfString(value);
-    }
-
-    /** Returns the field name used in this query. */
-    public String fieldName() {
-        return this.fieldName;
-    }
-
-    /**
-     *  Returns the value used in this query.
-     *  If necessary, converts internal {@link BytesRef} representation back to string.
-     */
-    public Object value() {
-        return convertToStringIfBytesRef(this.value);
-    }
-
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(getName());
-        builder.startObject(fieldName);
-        builder.field("value", convertToStringIfBytesRef(this.value));
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
-    }
-
-    /** Returns a {@link QueryValidationException} if fieldName is null or empty, or if value is null. */
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (fieldName == null || fieldName.isEmpty()) {
-            validationException = addValidationError("field name cannot be null or empty.", validationException);
-        }
-        if (value == null) {
-            validationException = addValidationError("value cannot be null.", validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    protected final int doHashCode() {
-        return Objects.hash(fieldName, value);
-    }
-
-    @Override
-    protected final boolean doEquals(BaseTermQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-               Objects.equals(value, other.value);
-    }
-
-    @Override
-    protected final QB doReadFrom(StreamInput in) throws IOException {
-        return createBuilder(in.readString(), in.readGenericValue());
-    }
-
-    protected abstract QB createBuilder(String fieldName, Object value);
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-        out.writeGenericValue(value);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java
index 9a15f79..c377667 100644
--- a/core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java
@@ -19,35 +19,17 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanClause.Occur;
-import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
-import java.util.Objects;
-
-import static org.elasticsearch.common.lucene.search.Queries.fixNegativeQueryIfNeeded;
 
 /**
  * A Query that matches documents matching boolean combinations of other queries.
  */
-public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
-
-    public static final String NAME = "bool";
-
-    public static final boolean ADJUST_PURE_NEGATIVE_DEFAULT = true;
-
-    public static final boolean DISABLE_COORD_DEFAULT = false;
-
-    static final BoolQueryBuilder PROTOTYPE = new BoolQueryBuilder();
+public class BoolQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<BoolQueryBuilder> {
 
     private final List<QueryBuilder> mustClauses = new ArrayList<>();
 
@@ -57,15 +39,19 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
 
     private final List<QueryBuilder> shouldClauses = new ArrayList<>();
 
-    private boolean disableCoord = DISABLE_COORD_DEFAULT;
+    private float boost = -1;
 
-    private boolean adjustPureNegative = ADJUST_PURE_NEGATIVE_DEFAULT;
+    private Boolean disableCoord;
 
     private String minimumShouldMatch;
+    
+    private Boolean adjustPureNegative;
+
+    private String queryName;
 
     /**
      * Adds a query that <b>must</b> appear in the matching documents and will
-     * contribute to scoring. No <tt>null</tt> value allowed.
+     * contribute to scoring.
      */
     public BoolQueryBuilder must(QueryBuilder queryBuilder) {
         mustClauses.add(queryBuilder);
@@ -73,15 +59,8 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
     }
 
     /**
-     * Gets the queries that <b>must</b> appear in the matching documents.
-     */
-    public List<QueryBuilder> must() {
-        return this.mustClauses;
-    }
-
-    /**
      * Adds a query that <b>must</b> appear in the matching documents but will
-     * not contribute to scoring. No <tt>null</tt> value allowed.
+     * not contribute to scoring.
      */
     public BoolQueryBuilder filter(QueryBuilder queryBuilder) {
         filterClauses.add(queryBuilder);
@@ -89,15 +68,8 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
     }
 
     /**
-     * Gets the queries that <b>must</b> appear in the matching documents but don't conntribute to scoring
-     */
-    public List<QueryBuilder> filter() {
-        return this.filterClauses;
-    }
-
-    /**
-     * Adds a query that <b>must not</b> appear in the matching documents.
-     * No <tt>null</tt> value allowed.
+     * Adds a query that <b>must not</b> appear in the matching documents and
+     * will not contribute to scoring.
      */
     public BoolQueryBuilder mustNot(QueryBuilder queryBuilder) {
         mustNotClauses.add(queryBuilder);
@@ -105,16 +77,9 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
     }
 
     /**
-     * Gets the queries that <b>must not</b> appear in the matching documents.
-     */
-    public List<QueryBuilder> mustNot() {
-        return this.mustNotClauses;
-    }
-
-    /**
-     * Adds a clause that <i>should</i> be matched by the returned documents. For a boolean query with no
+     * Adds a query that <i>should</i> appear in the matching documents. For a boolean query with no
      * <tt>MUST</tt> clauses one or more <code>SHOULD</code> clauses must match a document
-     * for the BooleanQuery to match. No <tt>null</tt> value allowed.
+     * for the BooleanQuery to match.
      *
      * @see #minimumNumberShouldMatch(int)
      */
@@ -124,13 +89,13 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
     }
 
     /**
-     * Gets the list of clauses that <b>should</b> be matched by the returned documents.
-     *
-     * @see #should(QueryBuilder)
-     *  @see #minimumNumberShouldMatch(int)
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
      */
-    public List<QueryBuilder> should() {
-        return this.shouldClauses;
+    @Override
+    public BoolQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
@@ -142,13 +107,6 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
     }
 
     /**
-     * @return whether the <tt>Similarity#coord(int,int)</tt> in scoring are disabled. Defaults to <tt>false</tt>.
-     */
-    public boolean disableCoord() {
-        return this.disableCoord;
-    }
-
-    /**
      * Specifies a minimum number of the optional (should) boolean clauses which must be satisfied.
      * <p/>
      * <p>By default no optional clauses are necessary for a match
@@ -166,23 +124,6 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
         return this;
     }
 
-
-    /**
-     * Specifies a minimum number of the optional (should) boolean clauses which must be satisfied.
-     * @see BoolQueryBuilder#minimumNumberShouldMatch(int)
-     */
-    public BoolQueryBuilder minimumNumberShouldMatch(String minimumNumberShouldMatch) {
-        this.minimumShouldMatch = minimumNumberShouldMatch;
-        return this;
-    }
-
-    /**
-     * @return the string representation of the minimumShouldMatch settings for this query
-     */
-    public String minimumNumberShouldMatch() {
-        return this.minimumShouldMatch;
-    }
-
     /**
      * Sets the minimum should match using the special syntax (for example, supporting percentage).
      */
@@ -198,7 +139,7 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
     public boolean hasClauses() {
         return !(mustClauses.isEmpty() && shouldClauses.isEmpty() && mustNotClauses.isEmpty() && filterClauses.isEmpty());
     }
-
+    
     /**
      * If a boolean query contains only negative ("must not") clauses should the
      * BooleanQuery be enhanced with a {@link MatchAllDocsQuery} in order to act
@@ -210,136 +151,52 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
     }
 
     /**
-     * @return the setting for the adjust_pure_negative setting in this query
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public boolean adjustPureNegative() {
-        return this.adjustPureNegative;
+    public BoolQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject("bool");
         doXArrayContent("must", mustClauses, builder, params);
         doXArrayContent("filter", filterClauses, builder, params);
         doXArrayContent("must_not", mustNotClauses, builder, params);
         doXArrayContent("should", shouldClauses, builder, params);
-        builder.field("disable_coord", disableCoord);
-        builder.field("adjust_pure_negative", adjustPureNegative);
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
+        if (disableCoord != null) {
+            builder.field("disable_coord", disableCoord);
+        }
         if (minimumShouldMatch != null) {
             builder.field("minimum_should_match", minimumShouldMatch);
         }
-        printBoostAndQueryName(builder);
+        if (adjustPureNegative != null) {
+            builder.field("adjust_pure_negative", adjustPureNegative);
+        }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         builder.endObject();
     }
 
-    private static void doXArrayContent(String field, List<QueryBuilder> clauses, XContentBuilder builder, Params params) throws IOException {
+    private void doXArrayContent(String field, List<QueryBuilder> clauses, XContentBuilder builder, Params params) throws IOException {
         if (clauses.isEmpty()) {
             return;
         }
-        builder.startArray(field);
-        for (QueryBuilder clause : clauses) {
-            clause.toXContent(builder, params);
-        }
-        builder.endArray();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        BooleanQuery booleanQuery = new BooleanQuery(disableCoord);
-        addBooleanClauses(context, booleanQuery, mustClauses, BooleanClause.Occur.MUST);
-        addBooleanClauses(context, booleanQuery, mustNotClauses, BooleanClause.Occur.MUST_NOT);
-        addBooleanClauses(context, booleanQuery, shouldClauses, BooleanClause.Occur.SHOULD);
-        addBooleanClauses(context, booleanQuery, filterClauses, BooleanClause.Occur.FILTER);
-
-        if (booleanQuery.clauses().isEmpty()) {
-            return new MatchAllDocsQuery();
-        }
-
-        booleanQuery = Queries.applyMinimumShouldMatch(booleanQuery, minimumShouldMatch);
-        return adjustPureNegative ? fixNegativeQueryIfNeeded(booleanQuery) : booleanQuery;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        validationException = validateInnerQueries(mustClauses, validationException);
-        validationException = validateInnerQueries(shouldClauses, validationException);
-        validationException = validateInnerQueries(mustNotClauses, validationException);
-        validationException = validateInnerQueries(filterClauses, validationException);
-        return validationException;
-    }
-
-    private void addBooleanClauses(QueryShardContext context, BooleanQuery booleanQuery, List<QueryBuilder> clauses, Occur occurs) throws IOException {
-        for (QueryBuilder query : clauses) {
-            Query luceneQuery = null;
-            switch (occurs) {
-            case SHOULD:
-                if (context.isFilter() && minimumShouldMatch == null) {
-                    minimumShouldMatch = "1";
-                }
-                luceneQuery = query.toQuery(context);
-                break;
-            case FILTER:
-            case MUST_NOT:
-                luceneQuery = query.toFilter(context);
-                break;
-            case MUST:
-                luceneQuery = query.toQuery(context);
-            }
-            if (luceneQuery != null) {
-                booleanQuery.add(new BooleanClause(luceneQuery, occurs));
+        if (clauses.size() == 1) {
+            builder.field(field);
+            clauses.get(0).toXContent(builder, params);
+        } else {
+            builder.startArray(field);
+            for (QueryBuilder clause : clauses) {
+                clause.toXContent(builder, params);
             }
+            builder.endArray();
         }
     }
 
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(adjustPureNegative, disableCoord,
-                minimumShouldMatch, mustClauses, shouldClauses, mustNotClauses, filterClauses);
-    }
-
-    @Override
-    protected boolean doEquals(BoolQueryBuilder other) {
-        return Objects.equals(adjustPureNegative, other.adjustPureNegative) &&
-                Objects.equals(disableCoord, other.disableCoord) &&
-                Objects.equals(minimumShouldMatch, other.minimumShouldMatch) &&
-                Objects.equals(mustClauses, other.mustClauses) &&
-                Objects.equals(shouldClauses, other.shouldClauses) &&
-                Objects.equals(mustNotClauses, other.mustNotClauses) &&
-                Objects.equals(filterClauses, other.filterClauses);
-    }
-
-    @Override
-    protected BoolQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        BoolQueryBuilder boolQueryBuilder = new BoolQueryBuilder();
-        List<QueryBuilder> queryBuilders = readQueries(in);
-        boolQueryBuilder.mustClauses.addAll(queryBuilders);
-        queryBuilders = readQueries(in);
-        boolQueryBuilder.mustNotClauses.addAll(queryBuilders);
-        queryBuilders = readQueries(in);
-        boolQueryBuilder.shouldClauses.addAll(queryBuilders);
-        queryBuilders = readQueries(in);
-        boolQueryBuilder.filterClauses.addAll(queryBuilders);
-        boolQueryBuilder.adjustPureNegative = in.readBoolean();
-        boolQueryBuilder.disableCoord = in.readBoolean();
-        boolQueryBuilder.minimumShouldMatch = in.readOptionalString();
-        return boolQueryBuilder;
-
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        writeQueries(out, mustClauses);
-        writeQueries(out, mustNotClauses);
-        writeQueries(out, shouldClauses);
-        writeQueries(out, filterClauses);
-        out.writeBoolean(adjustPureNegative);
-        out.writeBoolean(disableCoord);
-        out.writeOptionalString(minimumShouldMatch);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java
index a1ff2fa..6476ea8 100644
--- a/core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java
@@ -19,7 +19,10 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.settings.Settings;
@@ -32,9 +35,11 @@ import java.util.List;
 import static org.elasticsearch.common.lucene.search.Queries.fixNegativeQueryIfNeeded;
 
 /**
- * Parser for bool query
+ *
  */
-public class BoolQueryParser extends BaseQueryParser<BoolQueryBuilder> {
+public class BoolQueryParser implements QueryParser {
+
+    public static final String NAME = "bool";
 
     @Inject
     public BoolQueryParser(Settings settings) {
@@ -43,27 +48,23 @@ public class BoolQueryParser extends BaseQueryParser<BoolQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{BoolQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public BoolQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        boolean disableCoord = BoolQueryBuilder.DISABLE_COORD_DEFAULT;
-        boolean adjustPureNegative = BoolQueryBuilder.ADJUST_PURE_NEGATIVE_DEFAULT;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        boolean disableCoord = false;
+        float boost = 1.0f;
         String minimumShouldMatch = null;
 
-        final List<QueryBuilder> mustClauses = new ArrayList<>();
-        final List<QueryBuilder> mustNotClauses = new ArrayList<>();
-        final List<QueryBuilder> shouldClauses = new ArrayList<>();
-        final List<QueryBuilder> filterClauses = new ArrayList<>();
+        List<BooleanClause> clauses = new ArrayList<>();
+        boolean adjustPureNegative = true;
         String queryName = null;
-
+        
         String currentFieldName = null;
         XContentParser.Token token;
-        QueryBuilder query;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
@@ -72,21 +73,32 @@ public class BoolQueryParser extends BaseQueryParser<BoolQueryBuilder> {
             } else if (token == XContentParser.Token.START_OBJECT) {
                 switch (currentFieldName) {
                 case "must":
-                    query = parseContext.parseInnerQueryBuilder();
-                    mustClauses.add(query);
+                    Query query = parseContext.parseInnerQuery();
+                    if (query != null) {
+                        clauses.add(new BooleanClause(query, BooleanClause.Occur.MUST));
+                    }
                     break;
                 case "should":
-                    query = parseContext.parseInnerQueryBuilder();
-                    shouldClauses.add(query);
+                    query = parseContext.parseInnerQuery();
+                    if (query != null) {
+                        clauses.add(new BooleanClause(query, BooleanClause.Occur.SHOULD));
+                        if (parseContext.isFilter() && minimumShouldMatch == null) {
+                            minimumShouldMatch = "1";
+                        }
+                    }
                     break;
                 case "filter":
-                    query = parseContext.parseInnerFilterToQueryBuilder();
-                    filterClauses.add(query);
+                    query = parseContext.parseInnerFilter();
+                    if (query != null) {
+                        clauses.add(new BooleanClause(query, BooleanClause.Occur.FILTER));
+                    }
                     break;
                 case "must_not":
                 case "mustNot":
-                    query = parseContext.parseInnerFilterToQueryBuilder();
-                    mustNotClauses.add(query);
+                    query = parseContext.parseInnerFilter();
+                    if (query != null) {
+                        clauses.add(new BooleanClause(query, BooleanClause.Occur.MUST_NOT));
+                    }
                     break;
                 default:
                     throw new QueryParsingException(parseContext, "[bool] query does not support [" + currentFieldName + "]");
@@ -95,21 +107,32 @@ public class BoolQueryParser extends BaseQueryParser<BoolQueryBuilder> {
                 while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                     switch (currentFieldName) {
                     case "must":
-                        query = parseContext.parseInnerQueryBuilder();
-                        mustClauses.add(query);
+                        Query query = parseContext.parseInnerQuery();
+                        if (query != null) {
+                            clauses.add(new BooleanClause(query, BooleanClause.Occur.MUST));
+                        }
                         break;
                     case "should":
-                        query = parseContext.parseInnerQueryBuilder();
-                        shouldClauses.add(query);
+                        query = parseContext.parseInnerQuery();
+                        if (query != null) {
+                            clauses.add(new BooleanClause(query, BooleanClause.Occur.SHOULD));
+                            if (parseContext.isFilter() && minimumShouldMatch == null) {
+                                minimumShouldMatch = "1";
+                            }
+                        }
                         break;
                     case "filter":
-                        query = parseContext.parseInnerFilterToQueryBuilder();
-                        filterClauses.add(query);
+                        query = parseContext.parseInnerFilter();
+                        if (query != null) {
+                            clauses.add(new BooleanClause(query, BooleanClause.Occur.FILTER));
+                        }
                         break;
                     case "must_not":
                     case "mustNot":
-                        query = parseContext.parseInnerFilterToQueryBuilder();
-                        mustNotClauses.add(query);
+                        query = parseContext.parseInnerFilter();
+                        if (query != null) {
+                            clauses.add(new BooleanClause(query, BooleanClause.Occur.MUST_NOT));
+                        }
                         break;
                     default:
                         throw new QueryParsingException(parseContext, "bool query does not support [" + currentFieldName + "]");
@@ -133,29 +156,21 @@ public class BoolQueryParser extends BaseQueryParser<BoolQueryBuilder> {
                 }
             }
         }
-        BoolQueryBuilder boolQuery = new BoolQueryBuilder();
-        for (QueryBuilder queryBuilder : mustClauses) {
-            boolQuery.must(queryBuilder);
-        }
-        for (QueryBuilder queryBuilder : mustNotClauses) {
-            boolQuery.mustNot(queryBuilder);
+
+        if (clauses.isEmpty()) {
+            return new MatchAllDocsQuery();
         }
-        for (QueryBuilder queryBuilder : shouldClauses) {
-            boolQuery.should(queryBuilder);
+
+        BooleanQuery booleanQuery = new BooleanQuery(disableCoord);
+        for (BooleanClause clause : clauses) {
+            booleanQuery.add(clause);
         }
-        for (QueryBuilder queryBuilder : filterClauses) {
-            boolQuery.filter(queryBuilder);
+        booleanQuery.setBoost(boost);
+        booleanQuery = Queries.applyMinimumShouldMatch(booleanQuery, minimumShouldMatch);
+        Query query = adjustPureNegative ? fixNegativeQueryIfNeeded(booleanQuery) : booleanQuery;
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
         }
-        boolQuery.boost(boost);
-        boolQuery.disableCoord(disableCoord);
-        boolQuery.adjustPureNegative(adjustPureNegative);
-        boolQuery.minimumNumberShouldMatch(minimumShouldMatch);
-        boolQuery.queryName(queryName);
-        return boolQuery;
-    }
-
-    @Override
-    public BoolQueryBuilder getBuilderPrototype() {
-        return BoolQueryBuilder.PROTOTYPE;
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/BoostableQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/BoostableQueryBuilder.java
new file mode 100644
index 0000000..31572ce
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/index/query/BoostableQueryBuilder.java
@@ -0,0 +1,33 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.query;
+
+/**
+ * Query builder which allow setting some boost
+ */
+public interface BoostableQueryBuilder<B extends BoostableQueryBuilder<B>> {
+
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
+    B boost(float boost);
+
+}
diff --git a/core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java
index 69ab70a..9d67469 100644
--- a/core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java
@@ -19,14 +19,9 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.queries.BoostingQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * The BoostingQuery class can be used to effectively demote results that match a given query.
@@ -40,132 +35,63 @@ import java.util.Objects;
  * multiplied by the supplied "boost" parameter, so this should be less than 1 to achieve a
  * demoting effect
  */
-public class BoostingQueryBuilder extends AbstractQueryBuilder<BoostingQueryBuilder> {
+public class BoostingQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<BoostingQueryBuilder> {
 
-    public static final String NAME = "boosting";
+    private QueryBuilder positiveQuery;
 
-    private final QueryBuilder positiveQuery;
-
-    private final QueryBuilder negativeQuery;
+    private QueryBuilder negativeQuery;
 
     private float negativeBoost = -1;
 
-    static final BoostingQueryBuilder PROTOTYPE = new BoostingQueryBuilder(null, null);
+    private float boost = -1;
+
+    public BoostingQueryBuilder() {
 
-    /**
-     * Create a new {@link BoostingQueryBuilder}
-     *
-     * @param positiveQuery the positive query for this boosting query.
-     * @param negativeQuery the negative query for this boosting query.
-     */
-    public BoostingQueryBuilder(QueryBuilder positiveQuery, QueryBuilder negativeQuery) {
-        this.positiveQuery = positiveQuery;
-        this.negativeQuery = negativeQuery;
     }
 
-    /**
-     * Get the positive query for this boosting query.
-     */
-    public QueryBuilder positiveQuery() {
-        return this.positiveQuery;
+    public BoostingQueryBuilder positive(QueryBuilder positiveQuery) {
+        this.positiveQuery = positiveQuery;
+        return this;
     }
 
-    /**
-     * Get the negative query for this boosting query.
-     */
-    public QueryBuilder negativeQuery() {
-        return this.negativeQuery;
+    public BoostingQueryBuilder negative(QueryBuilder negativeQuery) {
+        this.negativeQuery = negativeQuery;
+        return this;
     }
 
-    /**
-     * Set the negative boost factor.
-     */
     public BoostingQueryBuilder negativeBoost(float negativeBoost) {
         this.negativeBoost = negativeBoost;
         return this;
     }
 
-    /**
-     * Get the negative boost factor.
-     */
-    public float negativeBoost() {
-        return this.negativeBoost;
-    }
-
     @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("positive");
-        positiveQuery.toXContent(builder, params);
-        builder.field("negative");
-        negativeQuery.toXContent(builder, params);
-        builder.field("negative_boost", negativeBoost);
-        printBoostAndQueryName(builder);
-        builder.endObject();
+    public BoostingQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (negativeBoost < 0) {
-            validationException = addValidationError("query requires negativeBoost to be set to positive value", validationException);
+    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
+        if (positiveQuery == null) {
+            throw new IllegalArgumentException("boosting query requires positive query to be set");
         }
         if (negativeQuery == null) {
-            validationException = addValidationError("inner clause [negative] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(negativeQuery, validationException);
+            throw new IllegalArgumentException("boosting query requires negative query to be set");
         }
-        if (positiveQuery == null) {
-            validationException = addValidationError("inner clause [positive] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(positiveQuery, validationException);
+        if (negativeBoost == -1) {
+            throw new IllegalArgumentException("boosting query requires negativeBoost to be set");
         }
-        return validationException;
-    }
+        builder.startObject(BoostingQueryParser.NAME);
+        builder.field("positive");
+        positiveQuery.toXContent(builder, params);
+        builder.field("negative");
+        negativeQuery.toXContent(builder, params);
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
+        builder.field("negative_boost", negativeBoost);
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query positive = positiveQuery.toQuery(context);
-        Query negative = negativeQuery.toQuery(context);
-        // make upstream queries ignore this query by returning `null`
-        // if either inner query builder returns null
-        if (positive == null || negative == null) {
-            return null;
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-
-        return new BoostingQuery(positive, negative, negativeBoost);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(negativeBoost, positiveQuery, negativeQuery);
-    }
-
-    @Override
-    protected boolean doEquals(BoostingQueryBuilder other) {
-        return Objects.equals(negativeBoost, other.negativeBoost) &&
-                Objects.equals(positiveQuery, other.positiveQuery) &&
-                Objects.equals(negativeQuery, other.negativeQuery);
-    }
-
-    @Override
-    protected BoostingQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryBuilder positiveQuery = in.readQuery();
-        QueryBuilder negativeQuery = in.readQuery();
-        BoostingQueryBuilder boostingQuery = new BoostingQueryBuilder(positiveQuery, negativeQuery);
-        boostingQuery.negativeBoost = in.readFloat();
-        return boostingQuery;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(positiveQuery);
-        out.writeQuery(negativeQuery);
-        out.writeFloat(negativeBoost);
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java
index 699d23d..c160b2f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java
@@ -19,15 +19,19 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.queries.BoostingQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
 
 /**
- * Parser for boosting query
+ *
  */
-public class BoostingQueryParser extends BaseQueryParser<BoostingQueryBuilder> {
+public class BoostingQueryParser implements QueryParser {
+
+    public static final String NAME = "boosting";
 
     @Inject
     public BoostingQueryParser() {
@@ -35,20 +39,19 @@ public class BoostingQueryParser extends BaseQueryParser<BoostingQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{BoostingQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public BoostingQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        QueryBuilder positiveQuery = null;
+        Query positiveQuery = null;
         boolean positiveQueryFound = false;
-        QueryBuilder negativeQuery = null;
+        Query negativeQuery = null;
         boolean negativeQueryFound = false;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = -1;
         float negativeBoost = -1;
-        String queryName = null;
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -57,10 +60,10 @@ public class BoostingQueryParser extends BaseQueryParser<BoostingQueryBuilder> {
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("positive".equals(currentFieldName)) {
-                    positiveQuery = parseContext.parseInnerQueryBuilder();
+                    positiveQuery = parseContext.parseInnerQuery();
                     positiveQueryFound = true;
                 } else if ("negative".equals(currentFieldName)) {
-                    negativeQuery = parseContext.parseInnerQueryBuilder();
+                    negativeQuery = parseContext.parseInnerQuery();
                     negativeQueryFound = true;
                 } else {
                     throw new QueryParsingException(parseContext, "[boosting] query does not support [" + currentFieldName + "]");
@@ -68,8 +71,6 @@ public class BoostingQueryParser extends BaseQueryParser<BoostingQueryBuilder> {
             } else if (token.isValue()) {
                 if ("negative_boost".equals(currentFieldName) || "negativeBoost".equals(currentFieldName)) {
                     negativeBoost = parser.floatValue();
-                } else if ("_name".equals(currentFieldName)) {
-                    queryName = parser.text();
                 } else if ("boost".equals(currentFieldName)) {
                     boost = parser.floatValue();
                 } else {
@@ -78,25 +79,25 @@ public class BoostingQueryParser extends BaseQueryParser<BoostingQueryBuilder> {
             }
         }
 
-        if (!positiveQueryFound) {
+        if (positiveQuery == null && !positiveQueryFound) {
             throw new QueryParsingException(parseContext, "[boosting] query requires 'positive' query to be set'");
         }
-        if (!negativeQueryFound) {
+        if (negativeQuery == null && !negativeQueryFound) {
             throw new QueryParsingException(parseContext, "[boosting] query requires 'negative' query to be set'");
         }
-        if (negativeBoost < 0) {
-            throw new QueryParsingException(parseContext, "[boosting] query requires 'negative_boost' to be set to be a positive value'");
+        if (negativeBoost == -1) {
+            throw new QueryParsingException(parseContext, "[boosting] query requires 'negative_boost' to be set'");
         }
 
-        BoostingQueryBuilder boostingQuery = new BoostingQueryBuilder(positiveQuery, negativeQuery);
-        boostingQuery.negativeBoost(negativeBoost);
-        boostingQuery.boost(boost);
-        boostingQuery.queryName(queryName);
-        return boostingQuery;
-    }
+        // parsers returned null
+        if (positiveQuery == null || negativeQuery == null) {
+            return null;
+        }
 
-    @Override
-    public BoostingQueryBuilder getBuilderPrototype() {
-        return BoostingQueryBuilder.PROTOTYPE;
+        BoostingQuery boostingQuery = new BoostingQuery(positiveQuery, negativeQuery, negativeBoost);
+        if (boost != -1) {
+            boostingQuery.setBoost(boost);
+        }
+        return boostingQuery;
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java
index 853c583..ae9c10d 100644
--- a/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java
@@ -19,31 +19,18 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.queries.ExtendedCommonTermsQuery;
-import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
 import org.apache.lucene.search.similarities.Similarity;
-import org.apache.lucene.util.BytesRefBuilder;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * CommonTermsQuery query is a query that executes high-frequency terms in a
  * optional sub-query to prevent slow queries due to "common" terms like
- * stopwords. This query basically builds 2 queries off the
- * {@link org.apache.lucene.queries.CommonTermsQuery#add(Term) added} terms
- * where low-frequency terms are added to a required boolean clause
+ * stopwords. This query basically builds 2 queries off the {@link #add(Term)
+ * added} terms where low-frequency terms are added to a required boolean clause
  * and high-frequency terms are added to an optional boolean clause. The
  * optional clause is only executed if the required "low-frequency' clause
  * matches. Scores produced by this query will be slightly different to plain
@@ -55,52 +42,46 @@ import java.util.Objects;
  * execution times significantly if applicable.
  * <p>
  */
-public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQueryBuilder> {
+public class CommonTermsQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<CommonTermsQueryBuilder> {
 
-    public static final String NAME = "common";
-
-    public static final float DEFAULT_CUTOFF_FREQ = 0.01f;
-
-    public static final Operator DEFAULT_HIGH_FREQ_OCCUR = Operator.OR;
-
-    public static final Operator DEFAULT_LOW_FREQ_OCCUR = Operator.OR;
-
-    public static final boolean DEFAULT_DISABLE_COORD = true;
+    public static enum Operator {
+        OR, AND
+    }
 
-    private final String fieldName;
+    private final String name;
 
     private final Object text;
 
-    private Operator highFreqOperator = DEFAULT_HIGH_FREQ_OCCUR;
+    private Operator highFreqOperator = null;
 
-    private Operator lowFreqOperator = DEFAULT_LOW_FREQ_OCCUR;
+    private Operator lowFreqOperator = null;
 
     private String analyzer = null;
 
+    private Float boost = null;
+
     private String lowFreqMinimumShouldMatch = null;
 
     private String highFreqMinimumShouldMatch = null;
 
-    private boolean disableCoord = DEFAULT_DISABLE_COORD;
+    private Boolean disableCoord = null;
 
-    private float cutoffFrequency = DEFAULT_CUTOFF_FREQ;
+    private Float cutoffFrequency = null;
 
-    static final CommonTermsQueryBuilder PROTOTYPE = new CommonTermsQueryBuilder(null, null);
+    private String queryName;
 
     /**
      * Constructs a new common terms query.
      */
-    public CommonTermsQueryBuilder(String fieldName, Object text) {
-        this.fieldName = fieldName;
+    public CommonTermsQueryBuilder(String name, Object text) {
+        if (name == null) {
+            throw new IllegalArgumentException("Field name must not be null");
+        }
+        if (text == null) {
+            throw new IllegalArgumentException("Query must not be null");
+        }
         this.text = text;
-    }
-
-    public String fieldName() {
-        return this.fieldName;
-    }
-
-    public Object value() {
-        return this.text;
+        this.name = name;
     }
 
     /**
@@ -109,27 +90,19 @@ public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQue
      * <tt>AND</tt>.
      */
     public CommonTermsQueryBuilder highFreqOperator(Operator operator) {
-        this.highFreqOperator = (operator == null) ? DEFAULT_HIGH_FREQ_OCCUR : operator;
+        this.highFreqOperator = operator;
         return this;
     }
 
-    public Operator highFreqOperator() {
-        return highFreqOperator;
-    }
-
     /**
      * Sets the operator to use for terms with a low document frequency (less
      * than {@link #cutoffFrequency(float)}. Defaults to <tt>AND</tt>.
      */
     public CommonTermsQueryBuilder lowFreqOperator(Operator operator) {
-        this.lowFreqOperator = (operator == null) ? DEFAULT_LOW_FREQ_OCCUR : operator;
+        this.lowFreqOperator = operator;
         return this;
     }
 
-    public Operator lowFreqOperator() {
-        return lowFreqOperator;
-    }
-
     /**
      * Explicitly set the analyzer to use. Defaults to use explicit mapping
      * config for the field, or, if not set, the default search analyzer.
@@ -139,8 +112,13 @@ public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQue
         return this;
     }
 
-    public String analyzer() {
-        return this.analyzer;
+    /**
+     * Set the boost to apply to the query.
+     */
+    @Override
+    public CommonTermsQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
@@ -148,17 +126,13 @@ public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQue
      * in [0..1] (or absolute number >=1) representing the maximum threshold of
      * a terms document frequency to be considered a low frequency term.
      * Defaults to
-     * <tt>{@value #DEFAULT_CUTOFF_FREQ}</tt>
+     * <tt>{@value CommonTermsQueryParser#DEFAULT_MAX_TERM_DOC_FREQ}</tt>
      */
     public CommonTermsQueryBuilder cutoffFrequency(float cutoffFrequency) {
         this.cutoffFrequency = cutoffFrequency;
         return this;
     }
 
-    public float cutoffFrequency() {
-        return this.cutoffFrequency;
-    }
-
     /**
      * Sets the minimum number of high frequent query terms that need to match in order to
      * produce a hit when there are no low frequen terms.
@@ -168,10 +142,6 @@ public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQue
         return this;
     }
 
-    public String highFreqMinimumShouldMatch() {
-        return this.highFreqMinimumShouldMatch;
-    }
-
     /**
      * Sets the minimum number of low frequent query terms that need to match in order to
      * produce a hit.
@@ -180,32 +150,44 @@ public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQue
         this.lowFreqMinimumShouldMatch = lowFreqMinimumShouldMatch;
         return this;
     }
-
-    public String lowFreqMinimumShouldMatch() {
-        return this.lowFreqMinimumShouldMatch;
-    }
-
+    
     public CommonTermsQueryBuilder disableCoord(boolean disableCoord) {
         this.disableCoord = disableCoord;
         return this;
     }
 
-    public boolean disableCoord() {
-        return this.disableCoord;
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public CommonTermsQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.startObject(fieldName);
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(CommonTermsQueryParser.NAME);
+        builder.startObject(name);
+
         builder.field("query", text);
-        builder.field("disable_coord", disableCoord);
-        builder.field("high_freq_operator", highFreqOperator.toString());
-        builder.field("low_freq_operator", lowFreqOperator.toString());
+        if (disableCoord != null) {
+            builder.field("disable_coord", disableCoord);
+        }
+        if (highFreqOperator != null) {
+            builder.field("high_freq_operator", highFreqOperator.toString());
+        }
+        if (lowFreqOperator != null) {
+            builder.field("low_freq_operator", lowFreqOperator.toString());
+        }
         if (analyzer != null) {
             builder.field("analyzer", analyzer);
         }
-        builder.field("cutoff_frequency", cutoffFrequency);
+        if (boost != null) {
+            builder.field("boost", boost);
+        }
+        if (cutoffFrequency != null) {
+            builder.field("cutoff_frequency", cutoffFrequency);
+        }
         if (lowFreqMinimumShouldMatch != null || highFreqMinimumShouldMatch != null) {
             builder.startObject("minimum_should_match");
             if (lowFreqMinimumShouldMatch != null) {
@@ -216,125 +198,11 @@ public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQue
             }
             builder.endObject();
         }
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        String field;
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType != null) {
-            field = fieldType.names().indexName();
-        } else {
-            field = fieldName;
-        }
-
-        Analyzer analyzerObj;
-        if (analyzer == null) {
-            if (fieldType != null) {
-                analyzerObj = context.getSearchAnalyzer(fieldType);
-            } else {
-                analyzerObj = context.mapperService().searchAnalyzer();
-            }
-        } else {
-            analyzerObj = context.mapperService().analysisService().analyzer(analyzer);
-            if (analyzerObj == null) {
-                throw new QueryShardException(context, "[common] analyzer [" + analyzer + "] not found");
-            }
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
 
-        Occur highFreqOccur = highFreqOperator.toBooleanClauseOccur();
-        Occur lowFreqOccur = lowFreqOperator.toBooleanClauseOccur();
-
-        ExtendedCommonTermsQuery commonsQuery = new ExtendedCommonTermsQuery(highFreqOccur, lowFreqOccur, cutoffFrequency, disableCoord, fieldType);
-        return parseQueryString(commonsQuery, text, field, analyzerObj, lowFreqMinimumShouldMatch, highFreqMinimumShouldMatch);
-    }
-
-    static Query parseQueryString(ExtendedCommonTermsQuery query, Object queryString, String field, Analyzer analyzer,
-                                         String lowFreqMinimumShouldMatch, String highFreqMinimumShouldMatch) throws IOException {
-        // Logic similar to QueryParser#getFieldQuery
-        int count = 0;
-        try (TokenStream source = analyzer.tokenStream(field, queryString.toString())) {
-            source.reset();
-            CharTermAttribute termAtt = source.addAttribute(CharTermAttribute.class);
-            BytesRefBuilder builder = new BytesRefBuilder();
-            while (source.incrementToken()) {
-                // UTF-8
-                builder.copyChars(termAtt);
-                query.add(new Term(field, builder.toBytesRef()));
-                count++;
-            }
-        }
-
-        if (count == 0) {
-            return null;
-        }
-        query.setLowFreqMinimumNumberShouldMatch(lowFreqMinimumShouldMatch);
-        query.setHighFreqMinimumNumberShouldMatch(highFreqMinimumShouldMatch);
-        return query;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (Strings.isEmpty(this.fieldName)) {
-            validationException = addValidationError("field name cannot be null or empty.", validationException);
-        }
-        if (this.text == null) {
-            validationException = addValidationError("query text cannot be null", validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    protected CommonTermsQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        CommonTermsQueryBuilder commonTermsQueryBuilder = new CommonTermsQueryBuilder(in.readString(), in.readGenericValue());
-        commonTermsQueryBuilder.highFreqOperator = Operator.readOperatorFrom(in);
-        commonTermsQueryBuilder.lowFreqOperator = Operator.readOperatorFrom(in);
-        commonTermsQueryBuilder.analyzer = in.readOptionalString();
-        commonTermsQueryBuilder.lowFreqMinimumShouldMatch = in.readOptionalString();
-        commonTermsQueryBuilder.highFreqMinimumShouldMatch = in.readOptionalString();
-        commonTermsQueryBuilder.disableCoord = in.readBoolean();
-        commonTermsQueryBuilder.cutoffFrequency = in.readFloat();
-        return commonTermsQueryBuilder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(this.fieldName);
-        out.writeGenericValue(this.text);
-        highFreqOperator.writeTo(out);
-        lowFreqOperator.writeTo(out);
-        out.writeOptionalString(analyzer);
-        out.writeOptionalString(lowFreqMinimumShouldMatch);
-        out.writeOptionalString(highFreqMinimumShouldMatch);
-        out.writeBoolean(disableCoord);
-        out.writeFloat(cutoffFrequency);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldName, text, highFreqOperator, lowFreqOperator, analyzer,
-                lowFreqMinimumShouldMatch, highFreqMinimumShouldMatch, disableCoord, cutoffFrequency);
-    }
-
-    @Override
-    protected boolean doEquals(CommonTermsQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-                Objects.equals(text, other.text) &&
-                Objects.equals(highFreqOperator, other.highFreqOperator) &&
-                Objects.equals(lowFreqOperator, other.lowFreqOperator) &&
-                Objects.equals(analyzer, other.analyzer) &&
-                Objects.equals(lowFreqMinimumShouldMatch, other.lowFreqMinimumShouldMatch) &&
-                Objects.equals(highFreqMinimumShouldMatch, other.highFreqMinimumShouldMatch) &&
-                Objects.equals(disableCoord, other.disableCoord) &&
-                Objects.equals(cutoffFrequency, other.cutoffFrequency);
+        builder.endObject();
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java
index 65f4fa3..c18229e 100644
--- a/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java
@@ -19,15 +19,36 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.queries.ExtendedCommonTermsQuery;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.util.BytesRefBuilder;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
 
 import java.io.IOException;
 
 /**
- * Parser for common terms query
+ *
  */
-public class CommonTermsQueryParser extends BaseQueryParser<CommonTermsQueryBuilder> {
+public class CommonTermsQueryParser implements QueryParser {
+
+    public static final String NAME = "common";
+
+    static final float DEFAULT_MAX_TERM_DOC_FREQ = 0.01f;
+
+    static final Occur DEFAULT_HIGH_FREQ_OCCUR = Occur.SHOULD;
+
+    static final Occur DEFAULT_LOW_FREQ_OCCUR = Occur.SHOULD;
+
+    static final boolean DEFAULT_DISABLE_COORD = true;
+
 
     @Inject
     public CommonTermsQueryParser() {
@@ -35,26 +56,26 @@ public class CommonTermsQueryParser extends BaseQueryParser<CommonTermsQueryBuil
 
     @Override
     public String[] names() {
-        return new String[] { CommonTermsQueryBuilder.NAME };
+        return new String[] { NAME };
     }
 
     @Override
-    public CommonTermsQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
         XContentParser.Token token = parser.nextToken();
         if (token != XContentParser.Token.FIELD_NAME) {
             throw new QueryParsingException(parseContext, "[common] query malformed, no field");
         }
         String fieldName = parser.currentName();
-        Object text = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        String analyzer = null;
+        Object value = null;
+        float boost = 1.0f;
+        String queryAnalyzer = null;
         String lowFreqMinimumShouldMatch = null;
         String highFreqMinimumShouldMatch = null;
-        boolean disableCoord = CommonTermsQueryBuilder.DEFAULT_DISABLE_COORD;
-        Operator highFreqOperator = CommonTermsQueryBuilder.DEFAULT_HIGH_FREQ_OCCUR;
-        Operator lowFreqOperator = CommonTermsQueryBuilder.DEFAULT_LOW_FREQ_OCCUR;
-        float cutoffFrequency = CommonTermsQueryBuilder.DEFAULT_CUTOFF_FREQ;
+        boolean disableCoord = DEFAULT_DISABLE_COORD;
+        Occur highFreqOccur = DEFAULT_HIGH_FREQ_OCCUR;
+        Occur lowFreqOccur = DEFAULT_LOW_FREQ_OCCUR;
+        float maxTermFrequency = DEFAULT_MAX_TERM_DOC_FREQ;
         String queryName = null;
         token = parser.nextToken();
         if (token == XContentParser.Token.START_OBJECT) {
@@ -84,21 +105,41 @@ public class CommonTermsQueryParser extends BaseQueryParser<CommonTermsQueryBuil
                     }
                 } else if (token.isValue()) {
                     if ("query".equals(currentFieldName)) {
-                        text = parser.objectText();
+                        value = parser.objectText();
                     } else if ("analyzer".equals(currentFieldName)) {
-                        analyzer = parser.text();
+                        String analyzer = parser.text();
+                        if (parseContext.analysisService().analyzer(analyzer) == null) {
+                            throw new QueryParsingException(parseContext, "[common] analyzer [" + parser.text() + "] not found");
+                        }
+                        queryAnalyzer = analyzer;
                     } else if ("disable_coord".equals(currentFieldName) || "disableCoord".equals(currentFieldName)) {
                         disableCoord = parser.booleanValue();
                     } else if ("boost".equals(currentFieldName)) {
                         boost = parser.floatValue();
                     } else if ("high_freq_operator".equals(currentFieldName) || "highFreqOperator".equals(currentFieldName)) {
-                        highFreqOperator = Operator.fromString(parser.text());
+                        String op = parser.text();
+                        if ("or".equalsIgnoreCase(op)) {
+                            highFreqOccur = BooleanClause.Occur.SHOULD;
+                        } else if ("and".equalsIgnoreCase(op)) {
+                            highFreqOccur = BooleanClause.Occur.MUST;
+                        } else {
+                            throw new QueryParsingException(parseContext,
+                                    "[common] query requires operator to be either 'and' or 'or', not [" + op + "]");
+                        }
                     } else if ("low_freq_operator".equals(currentFieldName) || "lowFreqOperator".equals(currentFieldName)) {
-                        lowFreqOperator = Operator.fromString(parser.text());
+                        String op = parser.text();
+                        if ("or".equalsIgnoreCase(op)) {
+                            lowFreqOccur = BooleanClause.Occur.SHOULD;
+                        } else if ("and".equalsIgnoreCase(op)) {
+                            lowFreqOccur = BooleanClause.Occur.MUST;
+                        } else {
+                            throw new QueryParsingException(parseContext,
+                                    "[common] query requires operator to be either 'and' or 'or', not [" + op + "]");
+                        }
                     } else if ("minimum_should_match".equals(currentFieldName) || "minimumShouldMatch".equals(currentFieldName)) {
                         lowFreqMinimumShouldMatch = parser.text();
                     } else if ("cutoff_frequency".equals(currentFieldName)) {
-                        cutoffFrequency = parser.floatValue();
+                        maxTermFrequency = parser.floatValue();
                     } else if ("_name".equals(currentFieldName)) {
                         queryName = parser.text();
                     } else {
@@ -108,7 +149,7 @@ public class CommonTermsQueryParser extends BaseQueryParser<CommonTermsQueryBuil
             }
             parser.nextToken();
         } else {
-            text = parser.objectText();
+            value = parser.objectText();
             // move to the next token
             token = parser.nextToken();
             if (token != XContentParser.Token.END_OBJECT) {
@@ -118,23 +159,66 @@ public class CommonTermsQueryParser extends BaseQueryParser<CommonTermsQueryBuil
             }
         }
 
-        if (text == null) {
+        if (value == null) {
             throw new QueryParsingException(parseContext, "No text specified for text query");
         }
-        return new CommonTermsQueryBuilder(fieldName, text)
-                .lowFreqMinimumShouldMatch(lowFreqMinimumShouldMatch)
-                .highFreqMinimumShouldMatch(highFreqMinimumShouldMatch)
-                .analyzer(analyzer)
-                .highFreqOperator(highFreqOperator)
-                .lowFreqOperator(lowFreqOperator)
-                .disableCoord(disableCoord)
-                .cutoffFrequency(cutoffFrequency)
-                .boost(boost)
-                .queryName(queryName);
+        String field;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            field = fieldType.names().indexName();
+        } else {
+            field = fieldName;
+        }
+
+        Analyzer analyzer = null;
+        if (queryAnalyzer == null) {
+            if (fieldType != null) {
+                analyzer = fieldType.searchAnalyzer();
+            }
+            if (analyzer == null && fieldType != null) {
+                analyzer = parseContext.getSearchAnalyzer(fieldType);
+            }
+            if (analyzer == null) {
+                analyzer = parseContext.mapperService().searchAnalyzer();
+            }
+        } else {
+            analyzer = parseContext.mapperService().analysisService().analyzer(queryAnalyzer);
+            if (analyzer == null) {
+                throw new IllegalArgumentException("No analyzer found for [" + queryAnalyzer + "]");
+            }
+        }
+
+        ExtendedCommonTermsQuery commonsQuery = new ExtendedCommonTermsQuery(highFreqOccur, lowFreqOccur, maxTermFrequency, disableCoord, fieldType);
+        commonsQuery.setBoost(boost);
+        Query query = parseQueryString(commonsQuery, value.toString(), field, parseContext, analyzer, lowFreqMinimumShouldMatch, highFreqMinimumShouldMatch);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 
-    @Override
-    public CommonTermsQueryBuilder getBuilderPrototype() {
-        return CommonTermsQueryBuilder.PROTOTYPE;
+
+    private final Query parseQueryString(ExtendedCommonTermsQuery query, String queryString, String field, QueryParseContext parseContext,
+            Analyzer analyzer, String lowFreqMinimumShouldMatch, String highFreqMinimumShouldMatch) throws IOException {
+        // Logic similar to QueryParser#getFieldQuery
+        int count = 0;
+        try (TokenStream source = analyzer.tokenStream(field, queryString.toString())) {
+            source.reset();
+            CharTermAttribute termAtt = source.addAttribute(CharTermAttribute.class);
+            BytesRefBuilder builder = new BytesRefBuilder();
+            while (source.incrementToken()) {
+                // UTF-8
+                builder.copyChars(termAtt);
+                query.add(new Term(field, builder.toBytesRef()));
+                count++;
+            }
+        }
+
+        if (count == 0) {
+            return null;
+        }
+        query.setLowFreqMinimumNumberShouldMatch(lowFreqMinimumShouldMatch);
+        query.setHighFreqMinimumNumberShouldMatch(highFreqMinimumShouldMatch);
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java
index 10b14e0..bdcbe9c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java
@@ -19,10 +19,6 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
@@ -32,84 +28,41 @@ import java.util.Objects;
  * A query that wraps a filter and simply returns a constant score equal to the
  * query boost for every document in the filter.
  */
-public class ConstantScoreQueryBuilder extends AbstractQueryBuilder<ConstantScoreQueryBuilder> {
-
-    public static final String NAME = "constant_score";
+public class ConstantScoreQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<ConstantScoreQueryBuilder> {
 
     private final QueryBuilder filterBuilder;
 
-    static final ConstantScoreQueryBuilder PROTOTYPE = new ConstantScoreQueryBuilder(null);
+    private float boost = -1;
 
     /**
-     * A query that wraps another query and simply returns a constant score equal to the
+     * A query that wraps a query and simply returns a constant score equal to the
      * query boost for every document in the query.
      *
      * @param filterBuilder The query to wrap in a constant score query
      */
     public ConstantScoreQueryBuilder(QueryBuilder filterBuilder) {
-        this.filterBuilder = filterBuilder;
+        this.filterBuilder = Objects.requireNonNull(filterBuilder);
     }
 
     /**
-     * @return the query that was wrapped in this constant score query
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
      */
-    public QueryBuilder innerQuery() {
-        return this.filterBuilder;
+    @Override
+    public ConstantScoreQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(ConstantScoreQueryParser.NAME);
         builder.field("filter");
         filterBuilder.toXContent(builder, params);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query innerFilter = filterBuilder.toFilter(context);
-        if (innerFilter == null ) {
-            // return null so that parent queries (e.g. bool) also ignore this
-            return null;
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        return new ConstantScoreQuery(innerFilter);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (filterBuilder == null) {
-            validationException = addValidationError("inner clause [filter] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(filterBuilder, validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(filterBuilder);
-    }
-
-    @Override
-    protected boolean doEquals(ConstantScoreQueryBuilder other) {
-        return Objects.equals(filterBuilder, other.filterBuilder);
-    }
-
-    @Override
-    protected ConstantScoreQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryBuilder innerFilterBuilder = in.readQuery();
-        return new ConstantScoreQueryBuilder(innerFilterBuilder);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(filterBuilder);
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java
index ba261e8..d8a34b9 100644
--- a/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
@@ -27,10 +29,11 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import java.io.IOException;
 
 /**
- * Parser for constant_score query
+ *
  */
-public class ConstantScoreQueryParser extends BaseQueryParser<ConstantScoreQueryBuilder> {
+public class ConstantScoreQueryParser implements QueryParser {
 
+    public static final String NAME = "constant_score";
     private static final ParseField INNER_QUERY_FIELD = new ParseField("filter", "query");
 
     @Inject
@@ -39,17 +42,16 @@ public class ConstantScoreQueryParser extends BaseQueryParser<ConstantScoreQuery
 
     @Override
     public String[] names() {
-        return new String[]{ConstantScoreQueryBuilder.NAME, Strings.toCamelCase(ConstantScoreQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public ConstantScoreQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        QueryBuilder query = null;
+        Query filter = null;
         boolean queryFound = false;
-        String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -60,15 +62,13 @@ public class ConstantScoreQueryParser extends BaseQueryParser<ConstantScoreQuery
                 // skip
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if (parseContext.parseFieldMatcher().match(currentFieldName, INNER_QUERY_FIELD)) {
-                    query = parseContext.parseInnerFilterToQueryBuilder();
+                    filter = parseContext.parseInnerFilter();
                     queryFound = true;
                 } else {
                     throw new QueryParsingException(parseContext, "[constant_score] query does not support [" + currentFieldName + "]");
                 }
             } else if (token.isValue()) {
-                if ("_name".equals(currentFieldName)) {
-                    queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
+                if ("boost".equals(currentFieldName)) {
                     boost = parser.floatValue();
                 } else {
                     throw new QueryParsingException(parseContext, "[constant_score] query does not support [" + currentFieldName + "]");
@@ -79,14 +79,12 @@ public class ConstantScoreQueryParser extends BaseQueryParser<ConstantScoreQuery
             throw new QueryParsingException(parseContext, "[constant_score] requires a 'filter' element");
         }
 
-        ConstantScoreQueryBuilder constantScoreBuilder = new ConstantScoreQueryBuilder(query);
-        constantScoreBuilder.boost(boost);
-        constantScoreBuilder.queryName(queryName);
-        return constantScoreBuilder;
-    }
+        if (filter == null) {
+            return null;
+        }
 
-    @Override
-    public ConstantScoreQueryBuilder getBuilderPrototype() {
-        return ConstantScoreQueryBuilder.PROTOTYPE;
+        filter = new ConstantScoreQuery(filter);
+        filter.setBoost(boost);
+        return filter;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java
index 9b43de6..3724a05 100644
--- a/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java
@@ -19,34 +19,25 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.DisjunctionMaxQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collection;
-import java.util.List;
-import java.util.Objects;
 
 /**
  * A query that generates the union of documents produced by its sub-queries, and that scores each document
  * with the maximum score for that document as produced by any sub-query, plus a tie breaking increment for any
  * additional matching sub-queries.
  */
-public class DisMaxQueryBuilder extends AbstractQueryBuilder<DisMaxQueryBuilder> {
+public class DisMaxQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<DisMaxQueryBuilder> {
 
-    public static final String NAME = "dis_max";
+    private ArrayList<QueryBuilder> queries = new ArrayList<>();
 
-    private final ArrayList<QueryBuilder> queries = new ArrayList<>();
+    private float boost = -1;
 
-    /** Default multiplication factor for breaking ties in document scores.*/
-    public static float DEFAULT_TIE_BREAKER = 0.0f;
-    private float tieBreaker = DEFAULT_TIE_BREAKER;
+    private float tieBreaker = -1;
 
-    static final DisMaxQueryBuilder PROTOTYPE = new DisMaxQueryBuilder();
+    private String queryName;
 
     /**
      * Add a sub-query to this disjunction.
@@ -57,10 +48,13 @@ public class DisMaxQueryBuilder extends AbstractQueryBuilder<DisMaxQueryBuilder>
     }
 
     /**
-     * @return an immutable list copy of the current sub-queries of this disjunction
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
      */
-    public List<QueryBuilder> innerQueries() {
-        return this.queries;
+    @Override
+    public DisMaxQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
@@ -75,70 +69,30 @@ public class DisMaxQueryBuilder extends AbstractQueryBuilder<DisMaxQueryBuilder>
     }
 
     /**
-     * @return the tie breaker score
-     * @see DisMaxQueryBuilder#tieBreaker(float)
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public float tieBreaker() {
-        return this.tieBreaker;
+    public DisMaxQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("tie_breaker", tieBreaker);
+        builder.startObject(DisMaxQueryParser.NAME);
+        if (tieBreaker != -1) {
+            builder.field("tie_breaker", tieBreaker);
+        }
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         builder.startArray("queries");
         for (QueryBuilder queryBuilder : queries) {
             queryBuilder.toXContent(builder, params);
         }
         builder.endArray();
-        printBoostAndQueryName(builder);
         builder.endObject();
     }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        // return null if there are no queries at all
-        Collection<Query> luceneQueries = toQueries(queries, context);
-        if (luceneQueries.isEmpty()) {
-            return null;
-        }
-
-        return new DisjunctionMaxQuery(luceneQueries, tieBreaker);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        return validateInnerQueries(queries, null);
-    }
-
-    @Override
-    protected DisMaxQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        DisMaxQueryBuilder disMax = new DisMaxQueryBuilder();
-        List<QueryBuilder> queryBuilders = readQueries(in);
-        disMax.queries.addAll(queryBuilders);
-        disMax.tieBreaker = in.readFloat();
-        return disMax;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        writeQueries(out, queries);
-        out.writeFloat(tieBreaker);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(queries, tieBreaker);
-    }
-
-    @Override
-    protected boolean doEquals(DisMaxQueryBuilder other) {
-        return Objects.equals(queries, other.queries) &&
-               Objects.equals(tieBreaker, other.tieBreaker);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java
index 39bad2d..dc901d6 100644
--- a/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.DisjunctionMaxQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
@@ -28,9 +30,11 @@ import java.util.ArrayList;
 import java.util.List;
 
 /**
- * Parser for dis_max query
+ *
  */
-public class DisMaxQueryParser extends BaseQueryParser<DisMaxQueryBuilder> {
+public class DisMaxQueryParser implements QueryParser {
+
+    public static final String NAME = "dis_max";
 
     @Inject
     public DisMaxQueryParser() {
@@ -38,17 +42,17 @@ public class DisMaxQueryParser extends BaseQueryParser<DisMaxQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{DisMaxQueryBuilder.NAME, Strings.toCamelCase(DisMaxQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public DisMaxQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        float tieBreaker = DisMaxQueryBuilder.DEFAULT_TIE_BREAKER;
+        float boost = 1.0f;
+        float tieBreaker = 0.0f;
 
-        final List<QueryBuilder> queries = new ArrayList<>();
+        List<Query> queries = new ArrayList<>();
         boolean queriesFound = false;
         String queryName = null;
 
@@ -60,8 +64,10 @@ public class DisMaxQueryParser extends BaseQueryParser<DisMaxQueryBuilder> {
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("queries".equals(currentFieldName)) {
                     queriesFound = true;
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    queries.add(query);
+                    Query query = parseContext.parseInnerQuery();
+                    if (query != null) {
+                        queries.add(query);
+                    }
                 } else {
                     throw new QueryParsingException(parseContext, "[dis_max] query does not support [" + currentFieldName + "]");
                 }
@@ -69,8 +75,10 @@ public class DisMaxQueryParser extends BaseQueryParser<DisMaxQueryBuilder> {
                 if ("queries".equals(currentFieldName)) {
                     queriesFound = true;
                     while (token != XContentParser.Token.END_ARRAY) {
-                        QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                        queries.add(query);
+                        Query query = parseContext.parseInnerQuery();
+                        if (query != null) {
+                            queries.add(query);
+                        }
                         token = parser.nextToken();
                     }
                 } else {
@@ -93,18 +101,15 @@ public class DisMaxQueryParser extends BaseQueryParser<DisMaxQueryBuilder> {
             throw new QueryParsingException(parseContext, "[dis_max] requires 'queries' field");
         }
 
-        DisMaxQueryBuilder disMaxQuery = new DisMaxQueryBuilder();
-        disMaxQuery.tieBreaker(tieBreaker);
-        disMaxQuery.queryName(queryName);
-        disMaxQuery.boost(boost);
-        for (QueryBuilder query : queries) {
-            disMaxQuery.add(query);
+        if (queries.isEmpty()) {
+            return null;
         }
-        return disMaxQuery;
-    }
 
-    @Override
-    public DisMaxQueryBuilder getBuilderPrototype() {
-        return DisMaxQueryBuilder.PROTOTYPE;
+        DisjunctionMaxQuery query = new DisjunctionMaxQuery(queries, tieBreaker);
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/EmptyQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/EmptyQueryBuilder.java
deleted file mode 100644
index c59d8d3..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/EmptyQueryBuilder.java
+++ /dev/null
@@ -1,118 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.elasticsearch.action.support.ToXContentToBytes;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentType;
-
-import java.io.IOException;
-
-/**
- * A {@link QueryBuilder} that is a stand in replacement for an empty query clause in the DSL.
- * The current DSL allows parsing inner queries / filters like "{ }", in order to have a
- * valid non-null representation of these clauses that actually do nothing we can use this class.
- *
- * This builder has no corresponding parser and it is not registered under the query name. It is
- * intended to be used internally as a stand-in for nested queries that are left empty and should
- * be ignored upstream.
- */
-public class EmptyQueryBuilder extends ToXContentToBytes implements QueryBuilder<EmptyQueryBuilder> {
-
-    public static final String NAME = "empty_query";
-
-    /** the one and only empty query builder */
-    public static final EmptyQueryBuilder PROTOTYPE = new EmptyQueryBuilder();
-
-    // prevent instances other than prototype
-    private EmptyQueryBuilder() {
-        super(XContentType.JSON);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    public String getName() {
-        return getWriteableName();
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public Query toQuery(QueryShardContext context) throws IOException {
-        // empty
-        return null;
-    }
-
-    @Override
-    public Query toFilter(QueryShardContext context) throws IOException {
-        // empty
-        return null;
-    }
-
-
-    @Override
-    public QueryValidationException validate() {
-        // nothing to validate
-        return null;
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-    }
-
-    @Override
-    public EmptyQueryBuilder readFrom(StreamInput in) throws IOException {
-        return EmptyQueryBuilder.PROTOTYPE;
-    }
-
-    @Override
-    public EmptyQueryBuilder queryName(String queryName) {
-        //no-op
-        return this;
-    }
-
-    @Override
-    public String queryName() {
-        return null;
-    }
-
-    @Override
-    public float boost() {
-        return -1;
-    }
-
-    @Override
-    public EmptyQueryBuilder boost(float boost) {
-        //no-op
-        return this;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java
index 6808793..9980d81 100644
--- a/core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java
@@ -19,126 +19,38 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.*;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.internal.FieldNamesFieldMapper;
-import org.elasticsearch.index.mapper.object.ObjectMapper;
 
 import java.io.IOException;
-import java.util.Collection;
-import java.util.Objects;
 
 /**
  * Constructs a query that only match on documents that the field has a value in them.
  */
-public class ExistsQueryBuilder extends AbstractQueryBuilder<ExistsQueryBuilder> {
+public class ExistsQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "exists";
+    private String name;
 
-    private final String fieldName;
+    private String queryName;
 
-    static final ExistsQueryBuilder PROTOTYPE = new ExistsQueryBuilder(null);
-
-    public ExistsQueryBuilder(String fieldName) {
-        this.fieldName = fieldName;
+    public ExistsQueryBuilder(String name) {
+        this.name = name;
     }
 
     /**
-     * @return the field name that has to exist for this query to match
+     * Sets the query name for the query that can be used when searching for matched_queries per hit.
      */
-    public String fieldName() {
-        return this.fieldName;
+    public ExistsQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("field", fieldName);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return newFilter(context, fieldName);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        // nothing to validate
-        return null;
-    }
-
-    public static Query newFilter(QueryShardContext context, String fieldPattern) {
-        final FieldNamesFieldMapper.FieldNamesFieldType fieldNamesFieldType = (FieldNamesFieldMapper.FieldNamesFieldType)context.mapperService().fullName(FieldNamesFieldMapper.NAME);
-        if (fieldNamesFieldType == null) {
-            // can only happen when no types exist, so no docs exist either
-            return Queries.newMatchNoDocsQuery();
-        }
-
-        ObjectMapper objectMapper = context.getObjectMapper(fieldPattern);
-        if (objectMapper != null) {
-            // automatic make the object mapper pattern
-            fieldPattern = fieldPattern + ".*";
-        }
-
-        Collection<String> fields = context.simpleMatchToIndexNames(fieldPattern);
-        if (fields.isEmpty()) {
-            // no fields exists, so we should not match anything
-            return Queries.newMatchNoDocsQuery();
-        }
-
-        BooleanQuery boolFilter = new BooleanQuery();
-        for (String field : fields) {
-            MappedFieldType fieldType = context.fieldMapper(field);
-            Query filter = null;
-            if (fieldNamesFieldType.isEnabled()) {
-                final String f;
-                if (fieldType != null) {
-                    f = fieldType.names().indexName();
-                } else {
-                    f = field;
-                }
-                filter = fieldNamesFieldType.termQuery(f, context);
-            }
-            // if _field_names are not indexed, we need to go the slow way
-            if (filter == null && fieldType != null) {
-                filter = fieldType.rangeQuery(null, null, true, true);
-            }
-            if (filter == null) {
-                filter = new TermRangeQuery(field, null, null, true, true);
-            }
-            boolFilter.add(filter, BooleanClause.Occur.SHOULD);
+        builder.startObject(ExistsQueryParser.NAME);
+        builder.field("field", name);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return new ConstantScoreQuery(boolFilter);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldName);
-    }
-
-    @Override
-    protected boolean doEquals(ExistsQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName);
-    }
-
-    @Override
-    protected ExistsQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new ExistsQueryBuilder(in.readString());
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java
index bd584bc..0ce578c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java
@@ -19,15 +19,23 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.*;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.internal.FieldNamesFieldMapper;
+import org.elasticsearch.index.mapper.object.ObjectMapper;
 
 import java.io.IOException;
+import java.util.Collection;
 
 /**
- * Parser for exists query
+ *
  */
-public class ExistsQueryParser extends BaseQueryParser<ExistsQueryBuilder> {
+public class ExistsQueryParser implements QueryParser {
+
+    public static final String NAME = "exists";
 
     @Inject
     public ExistsQueryParser() {
@@ -35,16 +43,15 @@ public class ExistsQueryParser extends BaseQueryParser<ExistsQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{ExistsQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public ExistsQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldPattern = null;
         String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
 
         XContentParser.Token token;
         String currentFieldName = null;
@@ -56,8 +63,6 @@ public class ExistsQueryParser extends BaseQueryParser<ExistsQueryBuilder> {
                     fieldPattern = parser.text();
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else {
                     throw new QueryParsingException(parseContext, "[exists] query does not support [" + currentFieldName + "]");
                 }
@@ -68,14 +73,55 @@ public class ExistsQueryParser extends BaseQueryParser<ExistsQueryBuilder> {
             throw new QueryParsingException(parseContext, "exists must be provided with a [field]");
         }
 
-        ExistsQueryBuilder builder = new ExistsQueryBuilder(fieldPattern);
-        builder.queryName(queryName);
-        builder.boost(boost);
-        return builder;
+        return newFilter(parseContext, fieldPattern, queryName);
     }
 
-    @Override
-    public ExistsQueryBuilder getBuilderPrototype() {
-        return ExistsQueryBuilder.PROTOTYPE;
+    public static Query newFilter(QueryParseContext parseContext, String fieldPattern, String queryName) {
+        final FieldNamesFieldMapper.FieldNamesFieldType fieldNamesFieldType = (FieldNamesFieldMapper.FieldNamesFieldType)parseContext.mapperService().fullName(FieldNamesFieldMapper.NAME);
+        if (fieldNamesFieldType == null) {
+            // can only happen when no types exist, so no docs exist either
+            return Queries.newMatchNoDocsQuery();
+        }
+
+        ObjectMapper objectMapper = parseContext.getObjectMapper(fieldPattern);
+        if (objectMapper != null) {
+            // automatic make the object mapper pattern
+            fieldPattern = fieldPattern + ".*";
+        }
+
+        Collection<String> fields = parseContext.simpleMatchToIndexNames(fieldPattern);
+        if (fields.isEmpty()) {
+            // no fields exists, so we should not match anything
+            return Queries.newMatchNoDocsQuery();
+        }
+
+        BooleanQuery boolFilter = new BooleanQuery();
+        for (String field : fields) {
+            MappedFieldType fieldType = parseContext.fieldMapper(field);
+            Query filter = null;
+            if (fieldNamesFieldType.isEnabled()) {
+                final String f;
+                if (fieldType != null) {
+                    f = fieldType.names().indexName();
+                } else {
+                    f = field;
+                }
+                filter = fieldNamesFieldType.termQuery(f, parseContext);
+            }
+            // if _field_names are not indexed, we need to go the slow way
+            if (filter == null && fieldType != null) {
+                filter = fieldType.rangeQuery(null, null, true, true);
+            }
+            if (filter == null) {
+                filter = new TermRangeQuery(field, null, null, true, true);
+            }
+            boolFilter.add(filter, BooleanClause.Occur.SHOULD);
+        }
+
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, boolFilter);
+        }
+        return new ConstantScoreQuery(boolFilter);
     }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/FQueryFilterBuilder.java b/core/src/main/java/org/elasticsearch/index/query/FQueryFilterBuilder.java
deleted file mode 100644
index 85fdad1..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/FQueryFilterBuilder.java
+++ /dev/null
@@ -1,111 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-
-import java.io.IOException;
-import java.util.Objects;
-
-/**
- * A filter that simply wraps a query. Same as the {@link QueryFilterBuilder} except that it allows also to
- * associate a name with the query filter.
- * @deprecated Useless now that queries and filters are merged: pass the
- *             query as a filter directly.
- */
-@Deprecated
-public class FQueryFilterBuilder extends AbstractQueryBuilder<FQueryFilterBuilder> {
-
-    public static final String NAME = "fquery";
-
-    static final FQueryFilterBuilder PROTOTYPE = new FQueryFilterBuilder(null);
-
-    private final QueryBuilder queryBuilder;
-
-    /**
-     * A filter that simply wraps a query.
-     *
-     * @param queryBuilder The query to wrap as a filter
-     */
-    public FQueryFilterBuilder(QueryBuilder queryBuilder) {
-        this.queryBuilder = queryBuilder;
-    }
-
-    /**
-     * @return the query builder that is wrapped by this {@link FQueryFilterBuilder}
-     */
-    public QueryBuilder innerQuery() {
-        return this.queryBuilder;
-    }
-
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(FQueryFilterBuilder.NAME);
-        builder.field("query");
-        queryBuilder.toXContent(builder, params);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query innerQuery = this.queryBuilder.toQuery(context);
-        if (innerQuery == null) {
-            return null;
-        }
-        return new ConstantScoreQuery(innerQuery);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        return validateInnerQuery(queryBuilder, null);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(queryBuilder);
-    }
-
-    @Override
-    protected boolean doEquals(FQueryFilterBuilder other) {
-        return Objects.equals(queryBuilder, other.queryBuilder);
-    }
-
-    @Override
-    protected FQueryFilterBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryBuilder innerQueryBuilder = in.readQuery();
-        FQueryFilterBuilder fquery = new FQueryFilterBuilder(innerQueryBuilder);
-        return fquery;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(queryBuilder);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java b/core/src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java
index 46ab50f..4c0f782 100644
--- a/core/src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
 
@@ -27,11 +29,11 @@ import java.io.IOException;
 /**
  * The "fquery" filter is the same as the {@link QueryFilterParser} except that it allows also to
  * associate a name with the query filter.
- * @deprecated Useless now that queries and filters are merged: pass the
- *             query as a filter directly.
  */
 @Deprecated
-public class FQueryFilterParser extends BaseQueryParser<FQueryFilterBuilder> {
+public class FQueryFilterParser implements QueryParser {
+
+    public static final String NAME = "fquery";
 
     @Inject
     public FQueryFilterParser() {
@@ -39,17 +41,16 @@ public class FQueryFilterParser extends BaseQueryParser<FQueryFilterBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{FQueryFilterBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public FQueryFilterBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        QueryBuilder wrappedQuery = null;
+        Query query = null;
         boolean queryFound = false;
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         String queryName = null;
         String currentFieldName = null;
         XContentParser.Token token;
@@ -61,15 +62,13 @@ public class FQueryFilterParser extends BaseQueryParser<FQueryFilterBuilder> {
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("query".equals(currentFieldName)) {
                     queryFound = true;
-                    wrappedQuery = parseContext.parseInnerQueryBuilder();
+                    query = parseContext.parseInnerQuery();
                 } else {
                     throw new QueryParsingException(parseContext, "[fquery] query does not support [" + currentFieldName + "]");
                 }
             } else if (token.isValue()) {
                 if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else {
                     throw new QueryParsingException(parseContext, "[fquery] query does not support [" + currentFieldName + "]");
                 }
@@ -78,14 +77,13 @@ public class FQueryFilterParser extends BaseQueryParser<FQueryFilterBuilder> {
         if (!queryFound) {
             throw new QueryParsingException(parseContext, "[fquery] requires 'query' element");
         }
-        FQueryFilterBuilder queryBuilder = new FQueryFilterBuilder(wrappedQuery);
-        queryBuilder.queryName(queryName);
-        queryBuilder.boost(boost);
-        return queryBuilder;
-    }
-
-    @Override
-    public FQueryFilterBuilder getBuilderPrototype() {
-        return FQueryFilterBuilder.PROTOTYPE;
+        if (query == null) {
+            return null;
+        }
+        query = new ConstantScoreQuery(query);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java
index a577225..c118416 100644
--- a/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java
@@ -19,113 +19,52 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.FieldMaskingSpanQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
 
 import java.io.IOException;
-import java.util.Objects;
 
-public class FieldMaskingSpanQueryBuilder extends AbstractQueryBuilder<FieldMaskingSpanQueryBuilder> implements SpanQueryBuilder<FieldMaskingSpanQueryBuilder>{
-
-    public static final String NAME = "field_masking_span";
+public class FieldMaskingSpanQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<FieldMaskingSpanQueryBuilder> {
 
     private final SpanQueryBuilder queryBuilder;
 
-    private final String fieldName;
+    private final String field;
 
-    static final FieldMaskingSpanQueryBuilder PROTOTYPE = new FieldMaskingSpanQueryBuilder(null, null);
+    private float boost = -1;
 
-    /**
-     * Constructs a new {@link FieldMaskingSpanQueryBuilder} given an inner {@link SpanQueryBuilder} for
-     * a given field
-     * @param queryBuilder inner {@link SpanQueryBuilder}
-     * @param fieldName the field name
-     */
-    public FieldMaskingSpanQueryBuilder(SpanQueryBuilder queryBuilder, String fieldName) {
+    private String queryName;
+
+
+    public FieldMaskingSpanQueryBuilder(SpanQueryBuilder queryBuilder, String field) {
         this.queryBuilder = queryBuilder;
-        this.fieldName = fieldName;
+        this.field = field;
     }
 
-    /**
-     * @return the field name for this query
-     */
-    public String fieldName() {
-        return this.fieldName;
+    @Override
+    public FieldMaskingSpanQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
-     * @return the inner {@link QueryBuilder}
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public SpanQueryBuilder innerQuery() {
-        return this.queryBuilder;
+    public FieldMaskingSpanQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(FieldMaskingSpanQueryParser.NAME);
         builder.field("query");
         queryBuilder.toXContent(builder, params);
-        builder.field("field", fieldName);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected SpanQuery doToQuery(QueryShardContext context) throws IOException {
-        String fieldInQuery = fieldName;
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType != null) {
-            fieldInQuery = fieldType.names().indexName();
-        }
-        Query innerQuery = queryBuilder.toQuery(context);
-        assert innerQuery instanceof SpanQuery;
-        return new FieldMaskingSpanQuery((SpanQuery)innerQuery, fieldInQuery);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (queryBuilder == null) {
-            validationException = addValidationError("inner clause [query] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(queryBuilder, validationException);
+        builder.field("field", field);
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        if (fieldName == null || fieldName.isEmpty()) {
-            validationException = addValidationError("field name is null or empty", validationException);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return validationException;
-    }
-
-    @Override
-    protected FieldMaskingSpanQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryBuilder innerQueryBuilder = in.readQuery();
-        return new FieldMaskingSpanQueryBuilder((SpanQueryBuilder) innerQueryBuilder, in.readString());
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(queryBuilder);
-        out.writeString(fieldName);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(queryBuilder, fieldName);
-    }
-
-    @Override
-    protected boolean doEquals(FieldMaskingSpanQueryBuilder other) {
-        return Objects.equals(queryBuilder, other.queryBuilder) &&
-               Objects.equals(fieldName, other.fieldName);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java
index ad77039..2980be1 100644
--- a/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java
@@ -19,15 +19,23 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.FieldMaskingSpanQuery;
+import org.apache.lucene.search.spans.SpanQuery;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.FieldMapper;
+import org.elasticsearch.index.mapper.MappedFieldType;
+
 import java.io.IOException;
 
 /**
- * Parser for field_masking_span query
+ *
  */
-public class FieldMaskingSpanQueryParser extends BaseQueryParser<FieldMaskingSpanQueryBuilder> {
+public class FieldMaskingSpanQueryParser implements QueryParser {
+
+    public static final String NAME = "field_masking_span";
 
     @Inject
     public FieldMaskingSpanQueryParser() {
@@ -35,16 +43,16 @@ public class FieldMaskingSpanQueryParser extends BaseQueryParser<FieldMaskingSpa
 
     @Override
     public String[] names() {
-        return new String[]{FieldMaskingSpanQueryBuilder.NAME, Strings.toCamelCase(FieldMaskingSpanQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public FieldMaskingSpanQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
 
-        SpanQueryBuilder inner = null;
+        SpanQuery inner = null;
         String field = null;
         String queryName = null;
 
@@ -55,11 +63,11 @@ public class FieldMaskingSpanQueryParser extends BaseQueryParser<FieldMaskingSpa
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("query".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (!(query instanceof SpanQueryBuilder)) {
-                        throw new QueryParsingException(parseContext, "[field_masking_span] query must be of type span query");
+                    Query query = parseContext.parseInnerQuery();
+                    if (!(query instanceof SpanQuery)) {
+                        throw new QueryParsingException(parseContext, "[field_masking_span] query] must be of type span query");
                     }
-                    inner = (SpanQueryBuilder) query;
+                    inner = (SpanQuery) query;
                 } else {
                     throw new QueryParsingException(parseContext, "[field_masking_span] query does not support ["
                             + currentFieldName + "]");
@@ -83,14 +91,16 @@ public class FieldMaskingSpanQueryParser extends BaseQueryParser<FieldMaskingSpa
             throw new QueryParsingException(parseContext, "field_masking_span must have [field] set for it");
         }
 
-        FieldMaskingSpanQueryBuilder queryBuilder = new FieldMaskingSpanQueryBuilder(inner, field);
-        queryBuilder.boost(boost);
-        queryBuilder.queryName(queryName);
-        return queryBuilder;
-    }
+        MappedFieldType fieldType = parseContext.fieldMapper(field);
+        if (fieldType != null) {
+            field = fieldType.names().indexName();
+        }
 
-    @Override
-    public FieldMaskingSpanQueryBuilder getBuilderPrototype() {
-        return FieldMaskingSpanQueryBuilder.PROTOTYPE;
+        FieldMaskingSpanQuery query = new FieldMaskingSpanQuery(inner, field);
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java
index 471f138..93507cf 100644
--- a/core/src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java
@@ -19,140 +19,72 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
+import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * A query that applies a filter to the results of another query.
  * @deprecated Use {@link BoolQueryBuilder} instead.
  */
 @Deprecated
-public class FilteredQueryBuilder extends AbstractQueryBuilder<FilteredQueryBuilder> {
+public class FilteredQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<FilteredQueryBuilder> {
 
-    /** Name of the query in the REST API. */
-    public static final String NAME = "filtered";
-    /** The query to filter. */
     private final QueryBuilder queryBuilder;
-    /** The filter to apply to the query. */
+
     private final QueryBuilder filterBuilder;
 
-    static final FilteredQueryBuilder PROTOTYPE = new FilteredQueryBuilder(null, null);
+    private float boost = -1;
 
-    /**
-     * Returns a {@link MatchAllQueryBuilder} instance that will be used as
-     * default queryBuilder if none is supplied by the user. Feel free to
-     * set queryName and boost on that instance - it's always a new one.
-     * */
-    private static QueryBuilder generateDefaultQuery() {
-        return new MatchAllQueryBuilder();
-    }
-
-    /**
-     * A query that applies a filter to the results of a match_all query.
-     * @param filterBuilder The filter to apply on the query (Can be null)
-     * */
-    public FilteredQueryBuilder(QueryBuilder filterBuilder) {
-        this(generateDefaultQuery(), filterBuilder);
-    }
+    private String queryName;
 
     /**
      * A query that applies a filter to the results of another query.
      *
-     * @param queryBuilder  The query to apply the filter to
+     * @param queryBuilder  The query to apply the filter to (Can be null)
      * @param filterBuilder The filter to apply on the query (Can be null)
      */
-    public FilteredQueryBuilder(QueryBuilder queryBuilder, QueryBuilder filterBuilder) {
-        this.queryBuilder = (queryBuilder != null) ? queryBuilder : generateDefaultQuery();
-        this.filterBuilder = (filterBuilder != null) ? filterBuilder : EmptyQueryBuilder.PROTOTYPE;
-    }
-
-    /** Returns the query to apply the filter to. */
-    public QueryBuilder innerQuery() {
-        return queryBuilder;
-    }
-
-    /** Returns the filter to apply to the query results. */
-    public QueryBuilder innerFilter() {
-        return filterBuilder;
+    public FilteredQueryBuilder(@Nullable QueryBuilder queryBuilder, @Nullable QueryBuilder filterBuilder) {
+        this.queryBuilder = queryBuilder;
+        this.filterBuilder = filterBuilder;
     }
 
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
     @Override
-    protected boolean doEquals(FilteredQueryBuilder other) {
-        return Objects.equals(queryBuilder, other.queryBuilder) &&
-                Objects.equals(filterBuilder, other.filterBuilder);
+    public FilteredQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
-    @Override
-    public int doHashCode() {
-        return Objects.hash(queryBuilder, filterBuilder);
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public FilteredQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
-    public Query doToQuery(QueryShardContext context) throws QueryShardException, IOException {
-        Query query = queryBuilder.toQuery(context);
-        Query filter = filterBuilder.toFilter(context);
-
-        if (query == null) {
-            // Most likely this query was generated from the JSON query DSL - it parsed to an EmptyQueryBuilder so we ignore
-            // the whole filtered query as there is nothing to filter on. See FilteredQueryParser for an example.
-            return null;
+    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(FilteredQueryParser.NAME);
+        if (queryBuilder != null) {
+            builder.field("query");
+            queryBuilder.toXContent(builder, params);
         }
-
-        if (filter == null || Queries.isConstantMatchAllQuery(filter)) {
-            // no filter, or match all filter
-            return query;
-        } else if (Queries.isConstantMatchAllQuery(query)) {
-            // if its a match_all query, use constant_score
-            return new ConstantScoreQuery(filter);
+        if (filterBuilder != null) {
+            builder.field("filter");
+            filterBuilder.toXContent(builder, params);
+        }
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-
-        // use a BooleanQuery
-        return Queries.filtered(query, filter);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        validationException = validateInnerQuery(queryBuilder, validationException);
-        validationException = validateInnerQuery(filterBuilder, validationException);
-        return validationException;
-
-    }
-
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("query");
-        queryBuilder.toXContent(builder, params);
-        builder.field("filter");
-        filterBuilder.toXContent(builder, params);
-        printBoostAndQueryName(builder);
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    public FilteredQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryBuilder query = in.readQuery();
-        QueryBuilder filter = in.readQuery();
-        FilteredQueryBuilder qb = new FilteredQueryBuilder(query, filter);
-        return qb;
-    }
-
-    @Override
-    public void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(queryBuilder);
-        out.writeQuery(filterBuilder);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java
index 6b94d6d..774ff74 100644
--- a/core/src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java
@@ -19,17 +19,23 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
 
 /**
- * Parser for filtered query.
- * @deprecated Use {@link BoolQueryParser} instead.
+ *
  */
 @Deprecated
-public class FilteredQueryParser extends BaseQueryParser<FilteredQueryBuilder> {
+public class FilteredQueryParser implements QueryParser {
+
+    public static final String NAME = "filtered";
 
     @Inject
     public FilteredQueryParser() {
@@ -37,16 +43,17 @@ public class FilteredQueryParser extends BaseQueryParser<FilteredQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{FilteredQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public FilteredQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        QueryBuilder query = null;
-        QueryBuilder filter = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        Query query = Queries.newMatchAllQuery();
+        Query filter = null;
+        boolean filterFound = false;
+        float boost = 1.0f;
         String queryName = null;
 
         String currentFieldName = null;
@@ -59,9 +66,10 @@ public class FilteredQueryParser extends BaseQueryParser<FilteredQueryBuilder> {
                 // skip
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("query".equals(currentFieldName)) {
-                    query = parseContext.parseInnerQueryBuilder();
+                    query = parseContext.parseInnerQuery();
                 } else if ("filter".equals(currentFieldName)) {
-                    filter = parseContext.parseInnerFilterToQueryBuilder();
+                    filterFound = true;
+                    filter = parseContext.parseInnerFilter();
                 } else {
                     throw new QueryParsingException(parseContext, "[filtered] query does not support [" + currentFieldName + "]");
                 }
@@ -78,15 +86,39 @@ public class FilteredQueryParser extends BaseQueryParser<FilteredQueryBuilder> {
             }
         }
 
-        FilteredQueryBuilder qb = new FilteredQueryBuilder(query, filter);
-        qb.boost(boost);
-        qb.queryName(queryName);
-        return qb;
-    }
+        // parsed internally, but returned null during parsing...
+        if (query == null) {
+            return null;
+        }
 
-    @Override
-    public FilteredQueryBuilder getBuilderPrototype() {
-        return FilteredQueryBuilder.PROTOTYPE;
-    }
+        if (filter == null) {
+            if (!filterFound) {
+                // we allow for null filter, so it makes compositions on the client side to be simpler
+                return query;
+            } else {
+                // even if the filter is not found, and its null, we should simply ignore it, and go
+                // by the query
+                return query;
+            }
+        }
+        if (Queries.isConstantMatchAllQuery(filter)) {
+            // this is an instance of match all filter, just execute the query
+            return query;
+        }
+
+        // if its a match_all query, use constant_score
+        if (Queries.isConstantMatchAllQuery(query)) {
+            Query q = new ConstantScoreQuery(filter);
+            q.setBoost(boost);
+            return q;
+        }
 
+        BooleanQuery filteredQuery = Queries.filtered(query, filter);
+
+        filteredQuery.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, filteredQuery);
+        }
+        return filteredQuery;
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java
index 237b415..23557b1 100644
--- a/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java
@@ -19,273 +19,177 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.FuzzyQuery;
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.unit.Fuzziness;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * A Query that does fuzzy matching for a specific value.
  */
-public class FuzzyQueryBuilder extends AbstractQueryBuilder<FuzzyQueryBuilder> implements MultiTermQueryBuilder<FuzzyQueryBuilder> {
+public class FuzzyQueryBuilder extends MultiTermQueryBuilder implements BoostableQueryBuilder<FuzzyQueryBuilder> {
 
-    public static final String NAME = "fuzzy";
-
-    /** Default maximum edit distance. Defaults to AUTO. */
-    public static final Fuzziness DEFAULT_FUZZINESS = Fuzziness.AUTO;
-
-    /** Default number of initial characters which will not be fuzzified. Defaults to 0. */
-    public static final int DEFAULT_PREFIX_LENGTH = FuzzyQuery.defaultPrefixLength;
-
-    /** Default maximum number of terms that the fuzzy query will expand to. Defaults to 50. */
-    public static final int DEFAULT_MAX_EXPANSIONS = FuzzyQuery.defaultMaxExpansions;
-
-    /** Default as to whether transpositions should be treated as a primitive edit operation, 
-     * instead of classic Levenshtein algorithm. Defaults to false. */
-    public static final boolean DEFAULT_TRANSPOSITIONS = false;
-
-    private final String fieldName;
+    private final String name;
 
     private final Object value;
 
-    private Fuzziness fuzziness = DEFAULT_FUZZINESS;
+    private float boost = -1;
 
-    private int prefixLength = DEFAULT_PREFIX_LENGTH;
+    private Fuzziness fuzziness;
 
-    private int maxExpansions = DEFAULT_MAX_EXPANSIONS;
+    private Integer prefixLength;
 
+    private Integer maxExpansions;
+    
     //LUCENE 4 UPGRADE  we need a testcase for this + documentation
-    private boolean transpositions = DEFAULT_TRANSPOSITIONS;
+    private Boolean transpositions;
 
     private String rewrite;
 
-    static final FuzzyQueryBuilder PROTOTYPE = new FuzzyQueryBuilder(null, null);
+    private String queryName;
 
     /**
      * Constructs a new fuzzy query.
      *
-     * @param fieldName  The name of the field
+     * @param name  The name of the field
      * @param value The value of the text
      */
-    public FuzzyQueryBuilder(String fieldName, String value) {
-        this(fieldName, (Object) value);
+    public FuzzyQueryBuilder(String name, Object value) {
+        this.name = name;
+        this.value = value;
     }
 
     /**
      * Constructs a new fuzzy query.
      *
-     * @param fieldName  The name of the field
+     * @param name  The name of the field
      * @param value The value of the text
      */
-    public FuzzyQueryBuilder(String fieldName, int value) {
-        this(fieldName, (Object) value);
+    public FuzzyQueryBuilder(String name, String value) {
+        this(name, (Object) value);
     }
 
     /**
      * Constructs a new fuzzy query.
      *
-     * @param fieldName  The name of the field
+     * @param name  The name of the field
      * @param value The value of the text
      */
-    public FuzzyQueryBuilder(String fieldName, long value) {
-        this(fieldName, (Object) value);
+    public FuzzyQueryBuilder(String name, int value) {
+        this(name, (Object) value);
     }
 
     /**
      * Constructs a new fuzzy query.
      *
-     * @param fieldName  The name of the field
+     * @param name  The name of the field
      * @param value The value of the text
      */
-    public FuzzyQueryBuilder(String fieldName, float value) {
-        this(fieldName, (Object) value);
+    public FuzzyQueryBuilder(String name, long value) {
+        this(name, (Object) value);
     }
 
     /**
      * Constructs a new fuzzy query.
      *
-     * @param fieldName  The name of the field
+     * @param name  The name of the field
      * @param value The value of the text
      */
-    public FuzzyQueryBuilder(String fieldName, double value) {
-        this(fieldName, (Object) value);
+    public FuzzyQueryBuilder(String name, float value) {
+        this(name, (Object) value);
     }
 
     /**
      * Constructs a new fuzzy query.
      *
-     * @param fieldName  The name of the field
+     * @param name  The name of the field
      * @param value The value of the text
      */
-    public FuzzyQueryBuilder(String fieldName, boolean value) {
-        this(fieldName, (Object) value);
+    public FuzzyQueryBuilder(String name, double value) {
+        this(name, (Object) value);
     }
 
+    // NO COMMIT: not sure we should also allow boolean?
     /**
      * Constructs a new fuzzy query.
      *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
+     * @param name  The name of the field
+     * @param value The value of the text
      */
-    public FuzzyQueryBuilder(String fieldName, Object value) {
-        this.fieldName = fieldName;
-        this.value = convertToBytesRefIfString(value);
+    public FuzzyQueryBuilder(String name, boolean value) {
+        this(name, (Object) value);
     }
 
-    public String fieldName() {
-        return this.fieldName;
-    }
-
-    public Object value() {
-        return convertToStringIfBytesRef(this.value);
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
+    @Override
+    public FuzzyQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     public FuzzyQueryBuilder fuzziness(Fuzziness fuzziness) {
-        this.fuzziness = (fuzziness == null) ? DEFAULT_FUZZINESS : fuzziness;
+        this.fuzziness = fuzziness;
         return this;
     }
-    
-    public Fuzziness fuzziness() {
-        return this.fuzziness;
-    }
 
     public FuzzyQueryBuilder prefixLength(int prefixLength) {
         this.prefixLength = prefixLength;
         return this;
     }
-    
-    public int prefixLength() {
-        return this.prefixLength;
-    }
 
     public FuzzyQueryBuilder maxExpansions(int maxExpansions) {
         this.maxExpansions = maxExpansions;
         return this;
     }
-
-    public int maxExpansions() {
-        return this.maxExpansions;
-    }
-
+    
     public FuzzyQueryBuilder transpositions(boolean transpositions) {
       this.transpositions = transpositions;
       return this;
     }
 
-    public boolean transpositions() {
-        return this.transpositions;
-    }
-
     public FuzzyQueryBuilder rewrite(String rewrite) {
         this.rewrite = rewrite;
         return this;
     }
 
-    public String rewrite() {
-        return this.rewrite;
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public FuzzyQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.startObject(fieldName);
-        builder.field("value", convertToStringIfBytesRef(this.value));
-        fuzziness.toXContent(builder, params);
-        builder.field("prefix_length", prefixLength);
-        builder.field("max_expansions", maxExpansions);
-        builder.field("transpositions", transpositions);
-        if (rewrite != null) {
-            builder.field("rewrite", rewrite);
+        builder.startObject(FuzzyQueryParser.NAME);
+        builder.startObject(name);
+        builder.field("value", value);
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    public Query doToQuery(QueryShardContext context) throws QueryParsingException, IOException {
-        Query query = null;
-        if (rewrite == null && context.isFilter()) {
-            rewrite = QueryParsers.CONSTANT_SCORE.getPreferredName();
+        if (transpositions != null) {
+            builder.field("transpositions", transpositions);
         }
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType != null) {
-            query = fieldType.fuzzyQuery(value, fuzziness, prefixLength, maxExpansions, transpositions);
+        if (fuzziness != null) {
+            fuzziness.toXContent(builder, params);
         }
-        if (query == null) {
-            int maxEdits = fuzziness.asDistance(BytesRefs.toString(value));
-            query = new FuzzyQuery(new Term(fieldName, BytesRefs.toBytesRef(value)), maxEdits, prefixLength, maxExpansions, transpositions);
+        if (prefixLength != null) {
+            builder.field("prefix_length", prefixLength);
         }
-        if (query instanceof MultiTermQuery) {
-            MultiTermQuery.RewriteMethod rewriteMethod = QueryParsers.parseRewriteMethod(context.parseFieldMatcher(), rewrite, null);
-            QueryParsers.setRewriteMethod((MultiTermQuery) query, rewriteMethod);
+        if (maxExpansions != null) {
+            builder.field("max_expansions", maxExpansions);
         }
-        return query;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (Strings.isEmpty(this.fieldName)) {
-            validationException = addValidationError("field name cannot be null or empty.", validationException);
+        if (rewrite != null) {
+            builder.field("rewrite", rewrite);
         }
-        if (this.value == null) {
-            validationException = addValidationError("query text cannot be null", validationException);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return validationException;
-    }
-
-    @Override
-    public FuzzyQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        FuzzyQueryBuilder fuzzyQueryBuilder = new FuzzyQueryBuilder(in.readString(), in.readGenericValue());
-        fuzzyQueryBuilder.fuzziness = Fuzziness.readFuzzinessFrom(in);
-        fuzzyQueryBuilder.prefixLength = in.readVInt();
-        fuzzyQueryBuilder.maxExpansions = in.readVInt();
-        fuzzyQueryBuilder.transpositions = in.readBoolean();
-        fuzzyQueryBuilder.rewrite = in.readOptionalString();
-        return fuzzyQueryBuilder;
-    }
-
-    @Override
-    public void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(this.fieldName);
-        out.writeGenericValue(this.value);
-        this.fuzziness.writeTo(out);
-        out.writeVInt(this.prefixLength);
-        out.writeVInt(this.maxExpansions);
-        out.writeBoolean(this.transpositions);
-        out.writeOptionalString(this.rewrite);
-    }
-
-    @Override
-    public int doHashCode() {
-        return Objects.hash(fieldName, value, fuzziness, prefixLength, maxExpansions, transpositions, rewrite);
-    }
-
-    @Override
-    public boolean doEquals(FuzzyQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-                Objects.equals(value, other.value) &&
-                Objects.equals(fuzziness, other.fuzziness) &&
-                Objects.equals(prefixLength, other.prefixLength) &&
-                Objects.equals(maxExpansions, other.maxExpansions) &&
-                Objects.equals(transpositions, other.transpositions) &&
-                Objects.equals(rewrite, other.rewrite);
+        builder.endObject();
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java
index 694a303..aefdb4c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java
@@ -19,48 +19,60 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.FuzzyQuery;
+import org.apache.lucene.search.MultiTermQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.unit.Fuzziness;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
 
-public class FuzzyQueryParser extends BaseQueryParser {
+/**
+ *
+ */
+public class FuzzyQueryParser implements QueryParser {
 
+    public static final String NAME = "fuzzy";
+    private static final Fuzziness DEFAULT_FUZZINESS = Fuzziness.AUTO;
     private static final ParseField FUZZINESS = Fuzziness.FIELD.withDeprecation("min_similarity");
 
+
     @Inject
     public FuzzyQueryParser() {
     }
 
     @Override
     public String[] names() {
-        return new String[]{ FuzzyQueryBuilder.NAME };
+        return new String[]{NAME};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         XContentParser.Token token = parser.nextToken();
         if (token != XContentParser.Token.FIELD_NAME) {
             throw new QueryParsingException(parseContext, "[fuzzy] query malformed, no field");
         }
-        
         String fieldName = parser.currentName();
-        Object value = null;
-
-        Fuzziness fuzziness = FuzzyQueryBuilder.DEFAULT_FUZZINESS;
-        int prefixLength = FuzzyQueryBuilder.DEFAULT_PREFIX_LENGTH;
-        int maxExpansions = FuzzyQueryBuilder.DEFAULT_MAX_EXPANSIONS;
-        boolean transpositions = FuzzyQueryBuilder.DEFAULT_TRANSPOSITIONS;
-        String rewrite = null;
 
+        Object value = null;
+        float boost = 1.0f;
+        Fuzziness fuzziness = DEFAULT_FUZZINESS;
+        int prefixLength = FuzzyQuery.defaultPrefixLength;
+        int maxExpansions = FuzzyQuery.defaultMaxExpansions;
+        boolean transpositions = FuzzyQuery.defaultTranspositions;
         String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-
+        MultiTermQuery.RewriteMethod rewriteMethod = null;
+        if (parseContext.isFilter()) {
+            rewriteMethod = MultiTermQuery.CONSTANT_SCORE_REWRITE;
+        }
         token = parser.nextToken();
         if (token == XContentParser.Token.START_OBJECT) {
             String currentFieldName = null;
@@ -81,9 +93,9 @@ public class FuzzyQueryParser extends BaseQueryParser {
                     } else if ("max_expansions".equals(currentFieldName) || "maxExpansions".equals(currentFieldName)) {
                         maxExpansions = parser.intValue();
                     } else if ("transpositions".equals(currentFieldName)) {
-                        transpositions = parser.booleanValue();
+                      transpositions = parser.booleanValue();
                     } else if ("rewrite".equals(currentFieldName)) {
-                        rewrite = parser.textOrNull();
+                        rewriteMethod = QueryParsers.parseRewriteMethod(parseContext.parseFieldMatcher(), parser.textOrNull(), null);
                     } else if ("_name".equals(currentFieldName)) {
                         queryName = parser.text();
                     } else {
@@ -99,20 +111,26 @@ public class FuzzyQueryParser extends BaseQueryParser {
         }
 
         if (value == null) {
-            throw new QueryParsingException(parseContext, "no value specified for fuzzy query");
+            throw new QueryParsingException(parseContext, "No value specified for fuzzy query");
         }
-        return new FuzzyQueryBuilder(fieldName, value)
-                .fuzziness(fuzziness)
-                .prefixLength(prefixLength)
-                .maxExpansions(maxExpansions)
-                .transpositions(transpositions)
-                .rewrite(rewrite)
-                .boost(boost)
-                .queryName(queryName);
-    }
+        
+        Query query = null;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            query = fieldType.fuzzyQuery(value, fuzziness, prefixLength, maxExpansions, transpositions);
+        }
+        if (query == null) {
+            int maxEdits = fuzziness.asDistance(BytesRefs.toString(value));
+            query = new FuzzyQuery(new Term(fieldName, BytesRefs.toBytesRef(value)), maxEdits, prefixLength, maxExpansions, transpositions);
+        }
+        if (query instanceof MultiTermQuery) {
+            QueryParsers.setRewriteMethod((MultiTermQuery) query, rewriteMethod);
+        }
+        query.setBoost(boost);
 
-    @Override
-    public FuzzyQueryBuilder getBuilderPrototype() {
-        return FuzzyQueryBuilder.PROTOTYPE;
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java
index 594cc6e..99b348e 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java
@@ -25,9 +25,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 
-public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBoundingBoxQueryBuilder> {
-
-    public static final String NAME = "geo_bbox";
+public class GeoBoundingBoxQueryBuilder extends QueryBuilder {
 
     public static final String TOP_LEFT = GeoBoundingBoxQueryParser.TOP_LEFT;
     public static final String BOTTOM_RIGHT = GeoBoundingBoxQueryParser.BOTTOM_RIGHT;
@@ -36,17 +34,16 @@ public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBounding
     private static final int LEFT = 1;
     private static final int BOTTOM = 2;
     private static final int RIGHT = 3;
-
+    
     private final String name;
 
     private double[] box = {Double.NaN, Double.NaN, Double.NaN, Double.NaN};
 
+    private String queryName;
     private String type;
     private Boolean coerce;
     private Boolean ignoreMalformed;
 
-    static final GeoBoundingBoxQueryBuilder PROTOTYPE = new GeoBoundingBoxQueryBuilder(null);
-
     public GeoBoundingBoxQueryBuilder(String name) {
         this.name = name;
     }
@@ -110,7 +107,7 @@ public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBounding
     public GeoBoundingBoxQueryBuilder bottomLeft(String geohash) {
         return bottomLeft(GeoHashUtils.decode(geohash));
     }
-
+    
     /**
      * Adds top right point.
      *
@@ -131,6 +128,14 @@ public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBounding
         return topRight(GeoHashUtils.decode(geohash));
     }
 
+    /**
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public GeoBoundingBoxQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
     public GeoBoundingBoxQueryBuilder coerce(boolean coerce) {
         this.coerce = coerce;
         return this;
@@ -162,14 +167,17 @@ public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBounding
         } else if(Double.isNaN(box[LEFT])) {
             throw new IllegalArgumentException("geo_bounding_box requires left longitude to be set");
         }
-
-        builder.startObject(NAME);
+                
+        builder.startObject(GeoBoundingBoxQueryParser.NAME);
 
         builder.startObject(name);
         builder.array(TOP_LEFT, box[LEFT], box[TOP]);
         builder.array(BOTTOM_RIGHT, box[RIGHT], box[BOTTOM]);
         builder.endObject();
 
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         if (type != null) {
             builder.field("type", type);
         }
@@ -180,13 +188,6 @@ public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBounding
             builder.field("ignore_malformed", ignoreMalformed);
         }
 
-        printBoostAndQueryName(builder);
-
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java
index 1476c2e..6dead6e 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java
@@ -37,7 +37,7 @@ import java.io.IOException;
 /**
  *
  */
-public class GeoBoundingBoxQueryParser extends BaseQueryParserTemp {
+public class GeoBoundingBoxQueryParser implements QueryParser {
 
     public static final String NAME = "geo_bbox";
 
@@ -64,12 +64,11 @@ public class GeoBoundingBoxQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{GeoBoundingBoxQueryBuilder.NAME, "geoBbox", "geo_bounding_box", "geoBoundingBox"};
+        return new String[]{NAME, "geoBbox", "geo_bounding_box", "geoBoundingBox"};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldName = null;
@@ -78,17 +77,16 @@ public class GeoBoundingBoxQueryParser extends BaseQueryParserTemp {
         double bottom = Double.NaN;
         double left = Double.NaN;
         double right = Double.NaN;
-
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        
         String queryName = null;
         String currentFieldName = null;
         XContentParser.Token token;
-        final boolean indexCreatedBeforeV2_0 = parseContext.shardContext().indexVersionCreated().before(Version.V_2_0_0);
+        final boolean indexCreatedBeforeV2_0 = parseContext.indexVersionCreated().before(Version.V_2_0_0);
         boolean coerce = false;
         boolean ignoreMalformed = false;
 
         GeoPoint sparse = new GeoPoint();
-
+        
         String type = "memory";
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -141,11 +139,9 @@ public class GeoBoundingBoxQueryParser extends BaseQueryParserTemp {
             } else if (token.isValue()) {
                 if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else if ("coerce".equals(currentFieldName) || (indexCreatedBeforeV2_0 && "normalize".equals(currentFieldName))) {
                     coerce = parser.booleanValue();
-                    if (coerce) {
+                    if (coerce == true) {
                         ignoreMalformed = true;
                     }
                 } else if ("type".equals(currentFieldName)) {
@@ -189,7 +185,7 @@ public class GeoBoundingBoxQueryParser extends BaseQueryParserTemp {
             }
         }
 
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
         if (fieldType == null) {
             throw new QueryParsingException(parseContext, "failed to parse [{}] query. could not find [{}] field [{}]", NAME, GeoPointFieldMapper.CONTENT_TYPE, fieldName);
         }
@@ -202,22 +198,15 @@ public class GeoBoundingBoxQueryParser extends BaseQueryParserTemp {
         if ("indexed".equals(type)) {
             filter = IndexedGeoBoundingBoxQuery.create(topLeft, bottomRight, geoFieldType);
         } else if ("memory".equals(type)) {
-            IndexGeoPointFieldData indexFieldData = context.getForField(fieldType);
+            IndexGeoPointFieldData indexFieldData = parseContext.getForField(fieldType);
             filter = new InMemoryGeoBoundingBoxQuery(topLeft, bottomRight, indexFieldData);
         } else {
             throw new QueryParsingException(parseContext, "failed to parse [{}] query. geo bounding box type [{}] is not supported. either [indexed] or [memory] are allowed", NAME, type);
         }
-        if (filter != null) {
-            filter.setBoost(boost);
-        }
+
         if (queryName != null) {
-            context.addNamedQuery(queryName, filter);
+            parseContext.addNamedQuery(queryName, filter);
         }
         return filter;
-    }
-
-    @Override
-    public GeoBoundingBoxQueryBuilder getBuilderPrototype() {
-        return GeoBoundingBoxQueryBuilder.PROTOTYPE;
-    }
+    }    
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java
index 6f883bd..77c8f94 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java
@@ -26,9 +26,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import java.io.IOException;
 import java.util.Locale;
 
-public class GeoDistanceQueryBuilder extends AbstractQueryBuilder<GeoDistanceQueryBuilder> {
-
-    public static final String NAME = "geo_distance";
+public class GeoDistanceQueryBuilder extends QueryBuilder {
 
     private final String name;
 
@@ -44,7 +42,7 @@ public class GeoDistanceQueryBuilder extends AbstractQueryBuilder<GeoDistanceQue
 
     private String optimizeBbox;
 
-    static final GeoDistanceQueryBuilder PROTOTYPE = new GeoDistanceQueryBuilder(null);
+    private String queryName;
 
     private Boolean coerce;
 
@@ -95,6 +93,14 @@ public class GeoDistanceQueryBuilder extends AbstractQueryBuilder<GeoDistanceQue
         return this;
     }
 
+    /**
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public GeoDistanceQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
     public GeoDistanceQueryBuilder coerce(boolean coerce) {
         this.coerce = coerce;
         return this;
@@ -107,7 +113,7 @@ public class GeoDistanceQueryBuilder extends AbstractQueryBuilder<GeoDistanceQue
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(GeoDistanceQueryParser.NAME);
         if (geohash != null) {
             builder.field(name, geohash);
         } else {
@@ -120,18 +126,15 @@ public class GeoDistanceQueryBuilder extends AbstractQueryBuilder<GeoDistanceQue
         if (optimizeBbox != null) {
             builder.field("optimize_bbox", optimizeBbox);
         }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         if (coerce != null) {
             builder.field("coerce", coerce);
         }
         if (ignoreMalformed != null) {
             builder.field("ignore_malformed", ignoreMalformed);
         }
-        printBoostAndQueryName(builder);
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java
index 647e1d0..8201381 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java
@@ -43,7 +43,9 @@ import java.io.IOException;
  * }
  * </pre>
  */
-public class GeoDistanceQueryParser extends BaseQueryParserTemp {
+public class GeoDistanceQueryParser implements QueryParser {
+
+    public static final String NAME = "geo_distance";
 
     @Inject
     public GeoDistanceQueryParser() {
@@ -51,17 +53,15 @@ public class GeoDistanceQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{GeoDistanceQueryBuilder.NAME, "geoDistance"};
+        return new String[]{NAME, "geoDistance"};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         XContentParser.Token token;
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         String queryName = null;
         String currentFieldName = null;
         GeoPoint point = new GeoPoint();
@@ -71,7 +71,7 @@ public class GeoDistanceQueryParser extends BaseQueryParserTemp {
         DistanceUnit unit = DistanceUnit.DEFAULT;
         GeoDistance geoDistance = GeoDistance.DEFAULT;
         String optimizeBbox = "memory";
-        final boolean indexCreatedBeforeV2_0 = parseContext.shardContext().indexVersionCreated().before(Version.V_2_0_0);
+        final boolean indexCreatedBeforeV2_0 = parseContext.indexVersionCreated().before(Version.V_2_0_0);
         boolean coerce = false;
         boolean ignoreMalformed = false;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -124,8 +124,6 @@ public class GeoDistanceQueryParser extends BaseQueryParserTemp {
                     fieldName = currentFieldName.substring(0, currentFieldName.length() - GeoPointFieldMapper.Names.GEOHASH_SUFFIX.length());
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else if ("optimize_bbox".equals(currentFieldName) || "optimizeBbox".equals(currentFieldName)) {
                     optimizeBbox = parser.textOrNull();
                 } else if ("coerce".equals(currentFieldName) || (indexCreatedBeforeV2_0 && "normalize".equals(currentFieldName))) {
@@ -145,10 +143,10 @@ public class GeoDistanceQueryParser extends BaseQueryParserTemp {
         // validation was not available prior to 2.x, so to support bwc percolation queries we only ignore_malformed on 2.x created indexes
         if (!indexCreatedBeforeV2_0 && !ignoreMalformed) {
             if (point.lat() > 90.0 || point.lat() < -90.0) {
-                throw new QueryParsingException(parseContext, "illegal latitude value [{}] for [{}]", point.lat(), GeoDistanceQueryBuilder.NAME);
+                throw new QueryParsingException(parseContext, "illegal latitude value [{}] for [{}]", point.lat(), NAME);
             }
             if (point.lon() > 180.0 || point.lon() < -180) {
-                throw new QueryParsingException(parseContext, "illegal longitude value [{}] for [{}]", point.lon(), GeoDistanceQueryBuilder.NAME);
+                throw new QueryParsingException(parseContext, "illegal longitude value [{}] for [{}]", point.lon(), NAME);
             }
         }
 
@@ -165,7 +163,7 @@ public class GeoDistanceQueryParser extends BaseQueryParserTemp {
         }
         distance = geoDistance.normalize(distance, DistanceUnit.DEFAULT);
 
-        MappedFieldType fieldType = parseContext.shardContext().fieldMapper(fieldName);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
         if (fieldType == null) {
             throw new QueryParsingException(parseContext, "failed to find geo_point field [" + fieldName + "]");
         }
@@ -175,17 +173,11 @@ public class GeoDistanceQueryParser extends BaseQueryParserTemp {
         GeoPointFieldMapper.GeoPointFieldType geoFieldType = ((GeoPointFieldMapper.GeoPointFieldType) fieldType);
 
 
-        IndexGeoPointFieldData indexFieldData = context.getForField(fieldType);
+        IndexGeoPointFieldData indexFieldData = parseContext.getForField(fieldType);
         Query query = new GeoDistanceRangeQuery(point, null, distance, true, false, geoDistance, geoFieldType, indexFieldData, optimizeBbox);
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
-        query.setBoost(boost);
         return query;
     }
-
-    @Override
-    public GeoDistanceQueryBuilder getBuilderPrototype() {
-        return GeoDistanceQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java
index 4f426e8..6aa6f0f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java
@@ -25,9 +25,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import java.io.IOException;
 import java.util.Locale;
 
-public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistanceRangeQueryBuilder> {
-
-    public static final String NAME = "geo_distance_range";
+public class GeoDistanceRangeQueryBuilder extends QueryBuilder {
 
     private final String name;
 
@@ -44,14 +42,14 @@ public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistan
 
     private GeoDistance geoDistance;
 
+    private String queryName;
+
     private String optimizeBbox;
 
     private Boolean coerce;
 
     private Boolean ignoreMalformed;
 
-    static final GeoDistanceRangeQueryBuilder PROTOTYPE = new GeoDistanceRangeQueryBuilder(null);
-
     public GeoDistanceRangeQueryBuilder(String name) {
         this.name = name;
     }
@@ -141,9 +139,17 @@ public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistan
         return this;
     }
 
+    /**
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public GeoDistanceRangeQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(GeoDistanceRangeQueryParser.NAME);
         if (geohash != null) {
             builder.field(name, geohash);
         } else {
@@ -159,18 +165,15 @@ public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistan
         if (optimizeBbox != null) {
             builder.field("optimize_bbox", optimizeBbox);
         }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         if (coerce != null) {
             builder.field("coerce", coerce);
         }
         if (ignoreMalformed != null) {
             builder.field("ignore_malformed", ignoreMalformed);
         }
-        printBoostAndQueryName(builder);
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java
index dd1879c..f60d944 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java
@@ -43,7 +43,9 @@ import java.io.IOException;
  * }
  * </pre>
  */
-public class GeoDistanceRangeQueryParser extends BaseQueryParserTemp {
+public class GeoDistanceRangeQueryParser implements QueryParser {
+
+    public static final String NAME = "geo_distance_range";
 
     @Inject
     public GeoDistanceRangeQueryParser() {
@@ -51,17 +53,15 @@ public class GeoDistanceRangeQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{GeoDistanceRangeQueryBuilder.NAME, "geoDistanceRange"};
+        return new String[]{NAME, "geoDistanceRange"};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         XContentParser.Token token;
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         String queryName = null;
         String currentFieldName = null;
         GeoPoint point = new GeoPoint();
@@ -73,7 +73,7 @@ public class GeoDistanceRangeQueryParser extends BaseQueryParserTemp {
         DistanceUnit unit = DistanceUnit.DEFAULT;
         GeoDistance geoDistance = GeoDistance.DEFAULT;
         String optimizeBbox = "memory";
-        final boolean indexCreatedBeforeV2_0 = parseContext.shardContext().indexVersionCreated().before(Version.V_2_0_0);
+        final boolean indexCreatedBeforeV2_0 = parseContext.indexVersionCreated().before(Version.V_2_0_0);
         boolean coerce = false;
         boolean ignoreMalformed = false;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -154,8 +154,6 @@ public class GeoDistanceRangeQueryParser extends BaseQueryParserTemp {
                     fieldName = currentFieldName.substring(0, currentFieldName.length() - GeoPointFieldMapper.Names.GEOHASH_SUFFIX.length());
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else if ("optimize_bbox".equals(currentFieldName) || "optimizeBbox".equals(currentFieldName)) {
                     optimizeBbox = parser.textOrNull();
                 } else if ("coerce".equals(currentFieldName) || (indexCreatedBeforeV2_0 && "normalize".equals(currentFieldName))) {
@@ -175,10 +173,10 @@ public class GeoDistanceRangeQueryParser extends BaseQueryParserTemp {
         // validation was not available prior to 2.x, so to support bwc percolation queries we only ignore_malformed on 2.x created indexes
         if (!indexCreatedBeforeV2_0 && !ignoreMalformed) {
             if (point.lat() > 90.0 || point.lat() < -90.0) {
-                throw new QueryParsingException(parseContext, "illegal latitude value [{}] for [{}]", point.lat(), GeoDistanceQueryBuilder.NAME);
+                throw new QueryParsingException(parseContext, "illegal latitude value [{}] for [{}]", point.lat(), NAME);
             }
             if (point.lon() > 180.0 || point.lon() < -180) {
-                throw new QueryParsingException(parseContext, "illegal longitude value [{}] for [{}]", point.lon(), GeoDistanceQueryBuilder.NAME);
+                throw new QueryParsingException(parseContext, "illegal longitude value [{}] for [{}]", point.lon(), NAME);
             }
         }
 
@@ -205,7 +203,7 @@ public class GeoDistanceRangeQueryParser extends BaseQueryParserTemp {
             to = geoDistance.normalize(to, DistanceUnit.DEFAULT);
         }
 
-        MappedFieldType fieldType = parseContext.shardContext().fieldMapper(fieldName);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
         if (fieldType == null) {
             throw new QueryParsingException(parseContext, "failed to find geo_point field [" + fieldName + "]");
         }
@@ -214,17 +212,11 @@ public class GeoDistanceRangeQueryParser extends BaseQueryParserTemp {
         }
         GeoPointFieldMapper.GeoPointFieldType geoFieldType = ((GeoPointFieldMapper.GeoPointFieldType) fieldType);
 
-        IndexGeoPointFieldData indexFieldData = context.getForField(fieldType);
+        IndexGeoPointFieldData indexFieldData = parseContext.getForField(fieldType);
         Query query = new GeoDistanceRangeQuery(point, from, to, includeLower, includeUpper, geoDistance, geoFieldType, indexFieldData, optimizeBbox);
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
-        query.setBoost(boost);
         return query;
     }
-
-    @Override
-    public GeoDistanceRangeQueryBuilder getBuilderPrototype() {
-        return GeoDistanceRangeQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java
index 4f4ce47..400384b 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java
@@ -27,17 +27,15 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 
-public class GeoPolygonQueryBuilder extends AbstractQueryBuilder<GeoPolygonQueryBuilder> {
-
-    public static final String NAME = "geo_polygon";
+public class GeoPolygonQueryBuilder extends QueryBuilder {
 
     public static final String POINTS = GeoPolygonQueryParser.POINTS;
-
+    
     private final String name;
 
     private final List<GeoPoint> shell = new ArrayList<>();
 
-    static final GeoPolygonQueryBuilder PROTOTYPE = new GeoPolygonQueryBuilder(null);
+    private String queryName;
 
     private Boolean coerce;
 
@@ -52,7 +50,7 @@ public class GeoPolygonQueryBuilder extends AbstractQueryBuilder<GeoPolygonQuery
      *
      * @param lat The latitude
      * @param lon The longitude
-     * @return the current builder
+     * @return
      */
     public GeoPolygonQueryBuilder addPoint(double lat, double lon) {
         return addPoint(new GeoPoint(lat, lon));
@@ -66,6 +64,14 @@ public class GeoPolygonQueryBuilder extends AbstractQueryBuilder<GeoPolygonQuery
         shell.add(point);
         return this;
     }
+    
+    /**
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public GeoPolygonQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
 
     public GeoPolygonQueryBuilder coerce(boolean coerce) {
         this.coerce = coerce;
@@ -79,7 +85,7 @@ public class GeoPolygonQueryBuilder extends AbstractQueryBuilder<GeoPolygonQuery
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(GeoPolygonQueryParser.NAME);
 
         builder.startObject(name);
         builder.startArray(POINTS);
@@ -89,18 +95,16 @@ public class GeoPolygonQueryBuilder extends AbstractQueryBuilder<GeoPolygonQuery
         builder.endArray();
         builder.endObject();
 
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         if (coerce != null) {
             builder.field("coerce", coerce);
         }
         if (ignoreMalformed != null) {
             builder.field("ignore_malformed", ignoreMalformed);
         }
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java
index 2dae22b..e4cf677 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java
@@ -47,8 +47,9 @@ import java.util.List;
  * }
  * </pre>
  */
-public class GeoPolygonQueryParser extends BaseQueryParserTemp {
+public class GeoPolygonQueryParser implements QueryParser {
 
+    public static final String NAME = "geo_polygon";
     public static final String POINTS = "points";
 
     @Inject
@@ -57,20 +58,18 @@ public class GeoPolygonQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{GeoPolygonQueryBuilder.NAME, "geoPolygon"};
+        return new String[]{NAME, "geoPolygon"};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldName = null;
 
         List<GeoPoint> shell = new ArrayList<>();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        final boolean indexCreatedBeforeV2_0 = parseContext.shardContext().indexVersionCreated().before(Version.V_2_0_0);
+        final boolean indexCreatedBeforeV2_0 = parseContext.indexVersionCreated().before(Version.V_2_0_0);
         boolean coerce = false;
         boolean ignoreMalformed = false;
         String queryName = null;
@@ -108,8 +107,6 @@ public class GeoPolygonQueryParser extends BaseQueryParserTemp {
             } else if (token.isValue()) {
                 if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else if ("coerce".equals(currentFieldName) || (indexCreatedBeforeV2_0 && "normalize".equals(currentFieldName))) {
                     coerce = parser.booleanValue();
                     if (coerce == true) {
@@ -144,10 +141,10 @@ public class GeoPolygonQueryParser extends BaseQueryParserTemp {
         if (!indexCreatedBeforeV2_0 && !ignoreMalformed) {
             for (GeoPoint point : shell) {
                 if (point.lat() > 90.0 || point.lat() < -90.0) {
-                    throw new QueryParsingException(parseContext, "illegal latitude value [{}] for [{}]", point.lat(), GeoPolygonQueryBuilder.NAME);
+                    throw new QueryParsingException(parseContext, "illegal latitude value [{}] for [{}]", point.lat(), NAME);
                 }
                 if (point.lon() > 180.0 || point.lon() < -180) {
-                    throw new QueryParsingException(parseContext, "illegal longitude value [{}] for [{}]", point.lon(), GeoPolygonQueryBuilder.NAME);
+                    throw new QueryParsingException(parseContext, "illegal longitude value [{}] for [{}]", point.lon(), NAME);
                 }
             }
         }
@@ -158,7 +155,7 @@ public class GeoPolygonQueryParser extends BaseQueryParserTemp {
             }
         }
 
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
         if (fieldType == null) {
             throw new QueryParsingException(parseContext, "failed to find geo_point field [" + fieldName + "]");
         }
@@ -166,17 +163,11 @@ public class GeoPolygonQueryParser extends BaseQueryParserTemp {
             throw new QueryParsingException(parseContext, "field [" + fieldName + "] is not a geo_point field");
         }
 
-        IndexGeoPointFieldData indexFieldData = context.getForField(fieldType);
+        IndexGeoPointFieldData indexFieldData = parseContext.getForField(fieldType);
         Query query = new GeoPolygonQuery(indexFieldData, shell.toArray(new GeoPoint[shell.size()]));
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
-        query.setBoost(boost);
         return query;
     }
-
-    @Override
-    public GeoPolygonQueryBuilder getBuilderPrototype() {
-        return GeoPolygonQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java
index 9180d0e..3887874 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java
@@ -29,11 +29,7 @@ import java.io.IOException;
 /**
  * {@link QueryBuilder} that builds a GeoShape Filter
  */
-public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuilder> {
-
-    public static final String NAME = "geo_shape";
-
-    static final GeoShapeQueryBuilder PROTOTYPE = new GeoShapeQueryBuilder(null, null);
+public class GeoShapeQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<GeoShapeQueryBuilder> {
 
     private final String name;
 
@@ -41,6 +37,8 @@ public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuil
 
     private SpatialStrategy strategy = null;
 
+    private String queryName;
+
     private final String indexedShapeId;
     private final String indexedShapeType;
 
@@ -49,6 +47,8 @@ public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuil
 
     private ShapeRelation relation = null;
 
+    private float boost = -1;
+    
     /**
      * Creates a new GeoShapeQueryBuilder whose Filter will be against the
      * given field name using the given Shape
@@ -93,6 +93,17 @@ public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuil
     }
 
     /**
+     * Sets the name of the filter
+     *
+     * @param queryName Name of the filter
+     * @return this
+     */
+    public GeoShapeQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
+    /**
      * Defines which spatial strategy will be used for building the geo shape filter. When not set, the strategy that
      * will be used will be the one that is associated with the geo shape field in the mappings.
      *
@@ -138,8 +149,14 @@ public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuil
     }
 
     @Override
+    public GeoShapeQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
+    @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(GeoShapeQueryParser.NAME);
 
         builder.startObject(name);
 
@@ -168,13 +185,14 @@ public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuil
 
         builder.endObject();
 
-        printBoostAndQueryName(builder);
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
+
+        if (name != null) {
+            builder.field("_name", queryName);
+        }
 
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java
index 693db14..286fa1c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java
@@ -31,6 +31,7 @@ import org.elasticsearch.common.geo.ShapeRelation;
 import org.elasticsearch.common.geo.builders.ShapeBuilder;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.geo.GeoShapeFieldMapper;
 import org.elasticsearch.index.search.shape.ShapeFetchService;
@@ -38,7 +39,9 @@ import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
-public class GeoShapeQueryParser extends BaseQueryParserTemp {
+public class GeoShapeQueryParser implements QueryParser {
+
+    public static final String NAME = "geo_shape";
 
     private ShapeFetchService fetchService;
 
@@ -49,12 +52,11 @@ public class GeoShapeQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{GeoShapeQueryBuilder.NAME, Strings.toCamelCase(GeoShapeQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldName = null;
@@ -137,7 +139,7 @@ public class GeoShapeQueryParser extends BaseQueryParserTemp {
             throw new QueryParsingException(parseContext, "No Shape Relation defined");
         }
 
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
         if (fieldType == null) {
             throw new QueryParsingException(parseContext, "Failed to find geo_shape field [" + fieldName + "]");
         }
@@ -158,7 +160,7 @@ public class GeoShapeQueryParser extends BaseQueryParserTemp {
             // this strategy doesn't support disjoint anymore: but it did before, including creating lucene fieldcache (!)
             // in this case, execute disjoint as exists && !intersects
             BooleanQuery bool = new BooleanQuery();
-            Query exists = ExistsQueryBuilder.newFilter(context, fieldName);
+            Query exists = ExistsQueryParser.newFilter(parseContext, fieldName, null);
             Filter intersects = strategy.makeFilter(getArgs(shape, ShapeRelation.INTERSECTS));
             bool.add(exists, BooleanClause.Occur.MUST);
             bool.add(intersects, BooleanClause.Occur.MUST_NOT);
@@ -168,7 +170,7 @@ public class GeoShapeQueryParser extends BaseQueryParserTemp {
         }
         query.setBoost(boost);
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
         return query;
     }
@@ -188,11 +190,7 @@ public class GeoShapeQueryParser extends BaseQueryParserTemp {
             return new SpatialArgs(SpatialOperation.IsWithin, shape.build());
         default:
             throw new IllegalArgumentException("");
-        }
-    }
 
-    @Override
-    public GeoShapeQueryBuilder getBuilderPrototype() {
-        return GeoShapeQueryBuilder.PROTOTYPE;
+        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java b/core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java
index 5ea66b9..814aca4 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java
@@ -31,7 +31,9 @@ import org.elasticsearch.common.unit.DistanceUnit;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.XContentParser.Token;
+import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.core.StringFieldMapper;
 import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
 
 import java.io.IOException;
@@ -69,7 +71,7 @@ public class GeohashCellQuery {
      * @param geohashes   optional array of additional geohashes
      * @return a new GeoBoundinboxfilter
      */
-    public static Query create(QueryShardContext context, GeoPointFieldMapper.GeoPointFieldType fieldType, String geohash, @Nullable List<CharSequence> geohashes) {
+    public static Query create(QueryParseContext context, GeoPointFieldMapper.GeoPointFieldType fieldType, String geohash, @Nullable List<CharSequence> geohashes) {
         MappedFieldType geoHashMapper = fieldType.geohashFieldType();
         if (geoHashMapper == null) {
             throw new IllegalArgumentException("geohash filter needs geohash_prefix to be enabled");
@@ -88,7 +90,7 @@ public class GeohashCellQuery {
      * <code>geohash</code> to be set. the default for a neighbor filteing is
      * <code>false</code>.
      */
-    public static class Builder extends AbstractQueryBuilder<Builder> {
+    public static class Builder extends QueryBuilder {
         // we need to store the geohash rather than the corresponding point,
         // because a transformation from a geohash to a point an back to the
         // geohash will extend the accuracy of the hash to max precision
@@ -97,7 +99,6 @@ public class GeohashCellQuery {
         private String geohash;
         private int levels = -1;
         private boolean neighbors;
-        private static final Builder PROTOTYPE = new Builder(null);
 
 
         public Builder(String field) {
@@ -164,17 +165,12 @@ public class GeohashCellQuery {
                 builder.field(PRECISION, levels);
             }
             builder.field(field, geohash);
-            printBoostAndQueryName(builder);
-            builder.endObject();
-        }
 
-        @Override
-        public String getWriteableName() {
-            return NAME;
+            builder.endObject();
         }
     }
 
-    public static class Parser extends BaseQueryParserTemp {
+    public static class Parser implements QueryParser {
 
         @Inject
         public Parser() {
@@ -186,16 +182,14 @@ public class GeohashCellQuery {
         }
 
         @Override
-        public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-            QueryParseContext parseContext = context.parseContext();
+        public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
             XContentParser parser = parseContext.parser();
 
             String fieldName = null;
             String geohash = null;
             int levels = -1;
             boolean neighbors = false;
-            String queryName = null;
-            float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+
 
             XContentParser.Token token;
             if ((token = parser.currentToken()) != Token.START_OBJECT) {
@@ -219,17 +213,11 @@ public class GeohashCellQuery {
                     } else if (NEIGHBORS.equals(field)) {
                         parser.nextToken();
                         neighbors = parser.booleanValue();
-                    } else if ("_name".equals(field)) {
-                        parser.nextToken();
-                        queryName = parser.text();
-                    } else if ("boost".equals(field)) {
-                        parser.nextToken();
-                        boost = parser.floatValue();
                     } else {
                         fieldName = field;
                         token = parser.nextToken();
                         if(token == Token.VALUE_STRING) {
-                            // A string indicates either a geohash or a lat/lon string
+                            // A string indicates either a gehash or a lat/lon string
                             String location = parser.text();
                             if(location.indexOf(",")>0) {
                                 geohash = GeoUtils.parseGeoPoint(parser).geohash();
@@ -249,7 +237,7 @@ public class GeohashCellQuery {
                 throw new QueryParsingException(parseContext, "failed to parse [{}] query. missing geohash value", NAME);
             }
 
-            MappedFieldType fieldType = context.fieldMapper(fieldName);
+            MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
             if (fieldType == null) {
                 throw new QueryParsingException(parseContext, "failed to parse [{}] query. missing [{}] field [{}]", NAME, GeoPointFieldMapper.CONTENT_TYPE, fieldName);
             }
@@ -270,22 +258,12 @@ public class GeohashCellQuery {
 
             Query filter;
             if (neighbors) {
-                filter = create(context, geoFieldType, geohash, GeoHashUtils.addNeighbors(geohash, new ArrayList<CharSequence>(8)));
+                filter = create(parseContext, geoFieldType, geohash, GeoHashUtils.addNeighbors(geohash, new ArrayList<CharSequence>(8)));
             } else {
-                filter = create(context, geoFieldType, geohash, null);
+                filter = create(parseContext, geoFieldType, geohash, null);
             }
-            if (queryName != null) {
-                context.addNamedQuery(queryName, filter);
-            }
-            if (filter != null) {
-                filter.setBoost(boost);
-            }
-            return filter;
-        }
 
-        @Override
-        public GeohashCellQuery.Builder getBuilderPrototype() {
-            return Builder.PROTOTYPE;
+            return filter;
         }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java
index 64b852d..74a6a5c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java
@@ -23,14 +23,14 @@ import org.elasticsearch.index.query.support.QueryInnerHitBuilder;
 
 import java.io.IOException;
 
-public class HasChildQueryBuilder extends AbstractQueryBuilder<HasChildQueryBuilder> {
-
-    public static final String NAME = "has_child";
+public class HasChildQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<HasChildQueryBuilder> {
 
     private final QueryBuilder queryBuilder;
 
     private String childType;
 
+    private float boost = 1.0f;
+
     private String scoreType;
 
     private Integer minChildren;
@@ -39,9 +39,9 @@ public class HasChildQueryBuilder extends AbstractQueryBuilder<HasChildQueryBuil
 
     private Integer shortCircuitCutoff;
 
-    private QueryInnerHitBuilder innerHit = null;
+    private String queryName;
 
-    static final HasChildQueryBuilder PROTOTYPE = new HasChildQueryBuilder(null, null);
+    private QueryInnerHitBuilder innerHit = null;
 
     public HasChildQueryBuilder(String type, QueryBuilder queryBuilder) {
         this.childType = type;
@@ -49,6 +49,16 @@ public class HasChildQueryBuilder extends AbstractQueryBuilder<HasChildQueryBuil
     }
 
     /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
+    @Override
+    public HasChildQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
+    /**
      * Defines how the scores from the matching child documents are mapped into the parent document.
      */
     public HasChildQueryBuilder scoreType(String scoreType) {
@@ -82,6 +92,14 @@ public class HasChildQueryBuilder extends AbstractQueryBuilder<HasChildQueryBuil
     }
 
     /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public HasChildQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
+    /**
      * Sets inner hit definition in the scope of this query and reusing the defined type and query.
      */
     public HasChildQueryBuilder innerHit(QueryInnerHitBuilder innerHit) {
@@ -91,10 +109,13 @@ public class HasChildQueryBuilder extends AbstractQueryBuilder<HasChildQueryBuil
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(HasChildQueryParser.NAME);
         builder.field("query");
         queryBuilder.toXContent(builder, params);
         builder.field("child_type", childType);
+        if (boost != 1.0f) {
+            builder.field("boost", boost);
+        }
         if (scoreType != null) {
             builder.field("score_type", scoreType);
         }
@@ -107,7 +128,9 @@ public class HasChildQueryBuilder extends AbstractQueryBuilder<HasChildQueryBuil
         if (shortCircuitCutoff != null) {
             builder.field("short_circuit_cutoff", shortCircuitCutoff);
         }
-        printBoostAndQueryName(builder);
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         if (innerHit != null) {
             builder.startObject("inner_hits");
             builder.value(innerHit);
@@ -115,9 +138,4 @@ public class HasChildQueryBuilder extends AbstractQueryBuilder<HasChildQueryBuil
         }
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java
index cc63933..87a7668 100644
--- a/core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java
@@ -52,8 +52,9 @@ import java.io.IOException;
 /**
  *
  */
-public class HasChildQueryParser extends BaseQueryParserTemp {
+public class HasChildQueryParser implements QueryParser {
 
+    public static final String NAME = "has_child";
     private static final ParseField QUERY_FIELD = new ParseField("query", "filter");
 
     private final InnerHitsQueryParserHelper innerHitsQueryParserHelper;
@@ -65,16 +66,15 @@ public class HasChildQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[] { HasChildQueryBuilder.NAME, Strings.toCamelCase(HasChildQueryBuilder.NAME) };
+        return new String[] { NAME, Strings.toCamelCase(NAME) };
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         boolean queryFound = false;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         String childType = null;
         ScoreType scoreType = ScoreType.NONE;
         int minChildren = 0;
@@ -140,7 +140,7 @@ public class HasChildQueryParser extends BaseQueryParserTemp {
         }
         innerQuery.setBoost(boost);
 
-        DocumentMapper childDocMapper = context.mapperService().documentMapper(childType);
+        DocumentMapper childDocMapper = parseContext.mapperService().documentMapper(childType);
         if (childDocMapper == null) {
             throw new QueryParsingException(parseContext, "[has_child] No mapping for for type [" + childType + "]");
         }
@@ -150,14 +150,14 @@ public class HasChildQueryParser extends BaseQueryParserTemp {
         }
 
         if (innerHits != null) {
-            ParsedQuery parsedQuery = new ParsedQuery(innerQuery, context.copyNamedQueries());
-            InnerHitsContext.ParentChildInnerHits parentChildInnerHits = new InnerHitsContext.ParentChildInnerHits(innerHits.getSubSearchContext(), parsedQuery, null, context.mapperService(), childDocMapper);
+            ParsedQuery parsedQuery = new ParsedQuery(innerQuery, parseContext.copyNamedQueries());
+            InnerHitsContext.ParentChildInnerHits parentChildInnerHits = new InnerHitsContext.ParentChildInnerHits(innerHits.getSubSearchContext(), parsedQuery, null, parseContext.mapperService(), childDocMapper);
             String name = innerHits.getName() != null ? innerHits.getName() : childType;
-            context.addInnerHits(name, parentChildInnerHits);
+            parseContext.addInnerHits(name, parentChildInnerHits);
         }
 
         String parentType = parentFieldMapper.type();
-        DocumentMapper parentDocMapper = context.mapperService().documentMapper(parentType);
+        DocumentMapper parentDocMapper = parseContext.mapperService().documentMapper(parentType);
         if (parentDocMapper == null) {
             throw new QueryParsingException(parseContext, "[has_child]  Type [" + childType + "] points to a non existent parent type ["
                     + parentType + "]");
@@ -169,15 +169,15 @@ public class HasChildQueryParser extends BaseQueryParserTemp {
 
         BitDocIdSetFilter nonNestedDocsFilter = null;
         if (parentDocMapper.hasNestedObjects()) {
-            nonNestedDocsFilter = context.bitsetFilter(Queries.newNonNestedFilter());
+            nonNestedDocsFilter = parseContext.bitsetFilter(Queries.newNonNestedFilter());
         }
 
         // wrap the query with type query
         innerQuery = Queries.filtered(innerQuery, childDocMapper.typeFilter());
 
         final Query query;
-        final ParentChildIndexFieldData parentChildIndexFieldData = context.getForField(parentFieldMapper.fieldType());
-        if (context.indexVersionCreated().onOrAfter(Version.V_2_0_0_beta1)) {
+        final ParentChildIndexFieldData parentChildIndexFieldData = parseContext.getForField(parentFieldMapper.fieldType());
+        if (parseContext.indexVersionCreated().onOrAfter(Version.V_2_0_0_beta1)) {
             query = joinUtilHelper(parentType, parentChildIndexFieldData, parentDocMapper.typeFilter(), scoreType, innerQuery, minChildren, maxChildren);
         } else {
             // TODO: use the query API
@@ -191,7 +191,7 @@ public class HasChildQueryParser extends BaseQueryParserTemp {
             }
         }
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
         query.setBoost(boost);
         return query;
@@ -287,9 +287,4 @@ public class HasChildQueryParser extends BaseQueryParserTemp {
             return "LateParsingQuery {parentType=" + parentType + "}";
         }
     }
-
-    @Override
-    public HasChildQueryBuilder getBuilderPrototype() {
-        return HasChildQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java
index 67bfe07..743ad76 100644
--- a/core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java
@@ -26,14 +26,14 @@ import java.io.IOException;
 /**
  * Builder for the 'has_parent' query.
  */
-public class HasParentQueryBuilder extends AbstractQueryBuilder<HasParentQueryBuilder> {
+public class HasParentQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<HasParentQueryBuilder> {
 
-    public static final String NAME = "has_parent";
     private final QueryBuilder queryBuilder;
     private final String parentType;
     private String scoreType;
+    private float boost = 1.0f;
+    private String queryName;
     private QueryInnerHitBuilder innerHit = null;
-    static final HasParentQueryBuilder PROTOTYPE = new HasParentQueryBuilder(null, null);
 
     /**
      * @param parentType  The parent type
@@ -44,6 +44,12 @@ public class HasParentQueryBuilder extends AbstractQueryBuilder<HasParentQueryBu
         this.queryBuilder = parentQuery;
     }
 
+    @Override
+    public HasParentQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
     /**
      * Defines how the parent score is mapped into the child documents.
      */
@@ -53,6 +59,14 @@ public class HasParentQueryBuilder extends AbstractQueryBuilder<HasParentQueryBu
     }
 
     /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public HasParentQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
+    /**
      * Sets inner hit definition in the scope of this query and reusing the defined type and query.
      */
     public HasParentQueryBuilder innerHit(QueryInnerHitBuilder innerHit) {
@@ -62,14 +76,19 @@ public class HasParentQueryBuilder extends AbstractQueryBuilder<HasParentQueryBu
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(HasParentQueryParser.NAME);
         builder.field("query");
         queryBuilder.toXContent(builder, params);
         builder.field("parent_type", parentType);
         if (scoreType != null) {
             builder.field("score_type", scoreType);
         }
-        printBoostAndQueryName(builder);
+        if (boost != 1.0f) {
+            builder.field("boost", boost);
+        }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         if (innerHit != null) {
             builder.startObject("inner_hits");
             builder.value(innerHit);
@@ -77,10 +96,5 @@ public class HasParentQueryBuilder extends AbstractQueryBuilder<HasParentQueryBu
         }
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
 
diff --git a/core/src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java
index c652afc..67422d0 100644
--- a/core/src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java
@@ -42,8 +42,9 @@ import java.util.Set;
 
 import static org.elasticsearch.index.query.HasChildQueryParser.joinUtilHelper;
 
-public class HasParentQueryParser extends BaseQueryParserTemp {
+public class HasParentQueryParser implements QueryParser {
 
+    public static final String NAME = "has_parent";
     private static final ParseField QUERY_FIELD = new ParseField("query", "filter");
 
     private final InnerHitsQueryParserHelper innerHitsQueryParserHelper;
@@ -55,16 +56,15 @@ public class HasParentQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{HasParentQueryBuilder.NAME, Strings.toCamelCase(HasParentQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         boolean queryFound = false;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         String parentType = null;
         boolean score = false;
         String queryName = null;
@@ -129,40 +129,40 @@ public class HasParentQueryParser extends BaseQueryParserTemp {
         }
 
         innerQuery.setBoost(boost);
-        Query query = createParentQuery(innerQuery, parentType, score, context, innerHits);
+        Query query = createParentQuery(innerQuery, parentType, score, parseContext, innerHits);
         if (query == null) {
             return null;
         }
 
         query.setBoost(boost);
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
         return query;
     }
 
-    static Query createParentQuery(Query innerQuery, String parentType, boolean score, QueryShardContext context, InnerHitsSubSearchContext innerHits) throws IOException {
-        DocumentMapper parentDocMapper = context.mapperService().documentMapper(parentType);
+    static Query createParentQuery(Query innerQuery, String parentType, boolean score, QueryParseContext parseContext, InnerHitsSubSearchContext innerHits) throws IOException {
+        DocumentMapper parentDocMapper = parseContext.mapperService().documentMapper(parentType);
         if (parentDocMapper == null) {
-            throw new QueryParsingException(context.parseContext(), "[has_parent] query configured 'parent_type' [" + parentType
+            throw new QueryParsingException(parseContext, "[has_parent] query configured 'parent_type' [" + parentType
                     + "] is not a valid type");
         }
 
         if (innerHits != null) {
-            ParsedQuery parsedQuery = new ParsedQuery(innerQuery, context.copyNamedQueries());
-            InnerHitsContext.ParentChildInnerHits parentChildInnerHits = new InnerHitsContext.ParentChildInnerHits(innerHits.getSubSearchContext(), parsedQuery, null, context.mapperService(), parentDocMapper);
+            ParsedQuery parsedQuery = new ParsedQuery(innerQuery, parseContext.copyNamedQueries());
+            InnerHitsContext.ParentChildInnerHits parentChildInnerHits = new InnerHitsContext.ParentChildInnerHits(innerHits.getSubSearchContext(), parsedQuery, null, parseContext.mapperService(), parentDocMapper);
             String name = innerHits.getName() != null ? innerHits.getName() : parentType;
-            context.addInnerHits(name, parentChildInnerHits);
+            parseContext.addInnerHits(name, parentChildInnerHits);
         }
 
         Set<String> parentTypes = new HashSet<>(5);
         parentTypes.add(parentDocMapper.type());
         ParentChildIndexFieldData parentChildIndexFieldData = null;
-        for (DocumentMapper documentMapper : context.mapperService().docMappers(false)) {
+        for (DocumentMapper documentMapper : parseContext.mapperService().docMappers(false)) {
             ParentFieldMapper parentFieldMapper = documentMapper.parentFieldMapper();
             if (parentFieldMapper.active()) {
-                DocumentMapper parentTypeDocumentMapper = context.mapperService().documentMapper(parentFieldMapper.type());
-                parentChildIndexFieldData = context.getForField(parentFieldMapper.fieldType());
+                DocumentMapper parentTypeDocumentMapper = parseContext.mapperService().documentMapper(parentFieldMapper.type());
+                parentChildIndexFieldData = parseContext.getForField(parentFieldMapper.fieldType());
                 if (parentTypeDocumentMapper == null) {
                     // Only add this, if this parentFieldMapper (also a parent)  isn't a child of another parent.
                     parentTypes.add(parentFieldMapper.type());
@@ -170,19 +170,19 @@ public class HasParentQueryParser extends BaseQueryParserTemp {
             }
         }
         if (parentChildIndexFieldData == null) {
-            throw new QueryParsingException(context.parseContext(), "[has_parent] no _parent field configured");
+            throw new QueryParsingException(parseContext, "[has_parent] no _parent field configured");
         }
 
         Query parentFilter = null;
         if (parentTypes.size() == 1) {
-            DocumentMapper documentMapper = context.mapperService().documentMapper(parentTypes.iterator().next());
+            DocumentMapper documentMapper = parseContext.mapperService().documentMapper(parentTypes.iterator().next());
             if (documentMapper != null) {
                 parentFilter = documentMapper.typeFilter();
             }
         } else {
             BooleanQuery parentsFilter = new BooleanQuery();
             for (String parentTypeStr : parentTypes) {
-                DocumentMapper documentMapper = context.mapperService().documentMapper(parentTypeStr);
+                DocumentMapper documentMapper = parseContext.mapperService().documentMapper(parentTypeStr);
                 if (documentMapper != null) {
                     parentsFilter.add(documentMapper.typeFilter(), BooleanClause.Occur.SHOULD);
                 }
@@ -197,7 +197,7 @@ public class HasParentQueryParser extends BaseQueryParserTemp {
         // wrap the query with type query
         innerQuery = Queries.filtered(innerQuery, parentDocMapper.typeFilter());
         Filter childrenFilter = new QueryWrapperFilter(Queries.not(parentFilter));
-        if (context.indexVersionCreated().onOrAfter(Version.V_2_0_0_beta1)) {
+        if (parseContext.indexVersionCreated().onOrAfter(Version.V_2_0_0_beta1)) {
             ScoreType scoreMode = score ? ScoreType.MAX : ScoreType.NONE;
             return joinUtilHelper(parentType, parentChildIndexFieldData, childrenFilter, scoreMode, innerQuery, 0, Integer.MAX_VALUE);
         } else {
@@ -209,9 +209,4 @@ public class HasParentQueryParser extends BaseQueryParserTemp {
         }
     }
 
-    @Override
-    public HasParentQueryBuilder getBuilderPrototype() {
-        return HasParentQueryBuilder.PROTOTYPE;
-    }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java
index 461a800..02c2a17 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java
@@ -19,62 +19,44 @@
 
 package org.elasticsearch.index.query;
 
-import com.google.common.collect.Sets;
-
-import org.apache.lucene.queries.TermsQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.Uid;
-import org.elasticsearch.index.mapper.internal.UidFieldMapper;
 
 import java.io.IOException;
-import java.util.*;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.List;
 
 /**
  * A query that will return only documents matching specific ids (and a type).
  */
-public class IdsQueryBuilder extends AbstractQueryBuilder<IdsQueryBuilder> {
+public class IdsQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<IdsQueryBuilder> {
 
-    public static final String NAME = "ids";
+    private final List<String> types;
 
-    private final Set<String> ids = Sets.newHashSet();
+    private List<String> values = new ArrayList<>();
 
-    private final String[] types;
+    private float boost = -1;
 
-    static final IdsQueryBuilder PROTOTYPE = new IdsQueryBuilder();
+    private String queryName;
 
-    /**
-     * Creates a new IdsQueryBuilder by optionally providing the types of the documents to look for
-     */
-    public IdsQueryBuilder(@Nullable String... types) {
-        this.types = types;
-    }
-
-    /**
-     * Returns the types used in this query
-     */
-    public String[] types() {
-        return this.types;
+    public IdsQueryBuilder(String... types) {
+        this.types = types == null ? null : Arrays.asList(types);
     }
 
     /**
-     * Adds ids to the query.
+     * Adds ids to the filter.
      */
     public IdsQueryBuilder addIds(String... ids) {
-        Collections.addAll(this.ids, ids);
+        values.addAll(Arrays.asList(ids));
         return this;
     }
 
     /**
-     * Adds ids to the query.
+     * Adds ids to the filter.
      */
     public IdsQueryBuilder addIds(Collection<String> ids) {
-        this.ids.addAll(ids);
+        values.addAll(ids);
         return this;
     }
 
@@ -93,83 +75,48 @@ public class IdsQueryBuilder extends AbstractQueryBuilder<IdsQueryBuilder> {
     }
 
     /**
-     * Returns the ids for the query.
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
      */
-    public Set<String> ids() {
-        return this.ids;
+    @Override
+    public IdsQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public IdsQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(IdsQueryParser.NAME);
         if (types != null) {
-            if (types.length == 1) {
-                builder.field("type", types[0]);
+            if (types.size() == 1) {
+                builder.field("type", types.get(0));
             } else {
-                builder.array("types", types);
+                builder.startArray("types");
+                for (Object type : types) {
+                    builder.value(type);
+                }
+                builder.endArray();
             }
         }
         builder.startArray("values");
-        for (String value : ids) {
+        for (Object value : values) {
             builder.value(value);
         }
         builder.endArray();
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query query;
-        if (this.ids.isEmpty()) {
-             query = Queries.newMatchNoDocsQuery();
-        } else {
-            Collection<String> typesForQuery;
-            if (types == null || types.length == 0) {
-                typesForQuery = context.queryTypes();
-            } else if (types.length == 1 && MetaData.ALL.equals(types[0])) {
-                typesForQuery = context.mapperService().types();
-            } else {
-                typesForQuery = Sets.newHashSet(types);
-            }
-
-            query = new TermsQuery(UidFieldMapper.NAME, Uid.createUidsForTypesAndIds(typesForQuery, ids));
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        return query;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        // all fields can be empty or null
-        return null;
-    }
-
-    @Override
-    protected IdsQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        IdsQueryBuilder idsQueryBuilder = new IdsQueryBuilder(in.readStringArray());
-        idsQueryBuilder.addIds(in.readStringArray());
-        return idsQueryBuilder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeStringArray(types);
-        out.writeStringArray(ids.toArray(new String[ids.size()]));
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(ids, Arrays.hashCode(types));
-    }
-
-    @Override
-    protected boolean doEquals(IdsQueryBuilder other) {
-        return Objects.equals(ids, other.ids) &&
-               Arrays.equals(types, other.types);
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java
index 6612140..dcbb19f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java
@@ -19,18 +19,29 @@
 
 package org.elasticsearch.index.query;
 
+import com.google.common.collect.Iterables;
+
+import org.apache.lucene.queries.TermsQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.Uid;
+import org.elasticsearch.index.mapper.internal.UidFieldMapper;
 
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.Collection;
 import java.util.Collections;
 import java.util.List;
 
 /**
- * Parser for ids query
+ *
  */
-public class IdsQueryParser extends BaseQueryParser<IdsQueryBuilder> {
+public class IdsQueryParser implements QueryParser {
+
+    public static final String NAME = "ids";
 
     @Inject
     public IdsQueryParser() {
@@ -38,21 +49,18 @@ public class IdsQueryParser extends BaseQueryParser<IdsQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{IdsQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
-    /**
-     * @return a QueryBuilder representation of the query passed in as XContent in the parse context
-     */
     @Override
-    public IdsQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
-        List<String> ids = new ArrayList<>();
-        List<String> types = new ArrayList<>();
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        String queryName = null;
 
+        List<BytesRef> ids = new ArrayList<>();
+        Collection<String> types = null;
         String currentFieldName = null;
+        float boost = 1.0f;
+        String queryName = null;
         XContentParser.Token token;
         boolean idsProvided = false;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -64,17 +72,18 @@ public class IdsQueryParser extends BaseQueryParser<IdsQueryBuilder> {
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                         if ((token == XContentParser.Token.VALUE_STRING) ||
                                 (token == XContentParser.Token.VALUE_NUMBER)) {
-                            String id = parser.textOrNull();
-                            if (id == null) {
+                            BytesRef value = parser.utf8BytesOrNull();
+                            if (value == null) {
                                 throw new QueryParsingException(parseContext, "No value specified for term filter");
                             }
-                            ids.add(id);
+                            ids.add(value);
                         } else {
                             throw new QueryParsingException(parseContext, "Illegal value for id, expecting a string or number, got: "
                                     + token);
                         }
                     }
                 } else if ("types".equals(currentFieldName) || "type".equals(currentFieldName)) {
+                    types = new ArrayList<>();
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                         String value = parser.textOrNull();
                         if (value == null) {
@@ -97,18 +106,26 @@ public class IdsQueryParser extends BaseQueryParser<IdsQueryBuilder> {
                 }
             }
         }
+
         if (!idsProvided) {
             throw new QueryParsingException(parseContext, "[ids] query, no ids values provided");
         }
 
-        IdsQueryBuilder query = new IdsQueryBuilder(types.toArray(new String[types.size()]));
-        query.addIds(ids.toArray(new String[ids.size()]));
-        query.boost(boost).queryName(queryName);
-        return query;
-    }
+        if (ids.isEmpty()) {
+            return Queries.newMatchNoDocsQuery();
+        }
 
-    @Override
-    public IdsQueryBuilder getBuilderPrototype() {
-        return IdsQueryBuilder.PROTOTYPE;
+        if (types == null || types.isEmpty()) {
+            types = parseContext.queryTypes();
+        } else if (types.size() == 1 && Iterables.getFirst(types, null).equals("_all")) {
+            types = parseContext.mapperService().types();
+        }
+
+        TermsQuery query = new TermsQuery(UidFieldMapper.NAME, Uid.createUidsForTypesAndIds(types, ids));
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java b/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java
index c2ee7d8..810504a 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java
@@ -22,15 +22,11 @@ package org.elasticsearch.index.query;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.util.CloseableThreadLocal;
 import org.elasticsearch.Version;
-import org.elasticsearch.action.support.IndicesOptions;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.lucene.search.Queries;
-import org.elasticsearch.common.regex.Regex;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentHelper;
@@ -43,15 +39,12 @@ import org.elasticsearch.index.cache.bitset.BitsetFilterCache;
 import org.elasticsearch.index.fielddata.IndexFieldDataService;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.internal.AllFieldMapper;
-import org.elasticsearch.index.search.termslookup.TermsLookupFetchService;
 import org.elasticsearch.index.settings.IndexSettings;
 import org.elasticsearch.index.similarity.SimilarityService;
-import org.elasticsearch.indices.cache.query.terms.TermsLookup;
 import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.script.ScriptService;
 
 import java.io.IOException;
-import java.util.List;
 
 public class IndexQueryParserService extends AbstractIndexComponent {
 
@@ -60,10 +53,10 @@ public class IndexQueryParserService extends AbstractIndexComponent {
     public static final String PARSE_STRICT = "index.query.parse.strict";
     public static final String ALLOW_UNMAPPED = "index.query.parse.allow_unmapped_fields";
 
-    private CloseableThreadLocal<QueryShardContext> cache = new CloseableThreadLocal<QueryShardContext>() {
+    private CloseableThreadLocal<QueryParseContext> cache = new CloseableThreadLocal<QueryParseContext>() {
         @Override
-        protected QueryShardContext initialValue() {
-            return new QueryShardContext(index, IndexQueryParserService.this);
+        protected QueryParseContext initialValue() {
+            return new QueryParseContext(index, IndexQueryParserService.this);
         }
     };
 
@@ -79,10 +72,6 @@ public class IndexQueryParserService extends AbstractIndexComponent {
 
     final IndexFieldDataService fieldDataService;
 
-    final ClusterService clusterService;
-
-    final IndexNameExpressionResolver indexNameExpressionResolver;
-
     final BitsetFilterCache bitsetFilterCache;
 
     private final IndicesQueriesRegistry indicesQueriesRegistry;
@@ -92,16 +81,13 @@ public class IndexQueryParserService extends AbstractIndexComponent {
     private final ParseFieldMatcher parseFieldMatcher;
     private final boolean defaultAllowUnmappedFields;
 
-    private TermsLookupFetchService termsLookupFetchService;
-
     @Inject
     public IndexQueryParserService(Index index, @IndexSettings Settings indexSettings,
                                    IndicesQueriesRegistry indicesQueriesRegistry,
                                    ScriptService scriptService, AnalysisService analysisService,
                                    MapperService mapperService, IndexCache indexCache, IndexFieldDataService fieldDataService,
                                    BitsetFilterCache bitsetFilterCache,
-                                   @Nullable SimilarityService similarityService, ClusterService clusterService,
-                                   IndexNameExpressionResolver indexNameExpressionResolver) {
+                                   @Nullable SimilarityService similarityService) {
         super(index, indexSettings);
         this.scriptService = scriptService;
         this.analysisService = analysisService;
@@ -110,8 +96,6 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         this.indexCache = indexCache;
         this.fieldDataService = fieldDataService;
         this.bitsetFilterCache = bitsetFilterCache;
-        this.clusterService = clusterService;
-        this.indexNameExpressionResolver = indexNameExpressionResolver;
 
         this.defaultField = indexSettings.get(DEFAULT_FIELD, AllFieldMapper.NAME);
         this.queryStringLenient = indexSettings.getAsBoolean(QUERY_STRING_LENIENT, false);
@@ -120,11 +104,6 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         this.indicesQueriesRegistry = indicesQueriesRegistry;
     }
 
-    @Inject(optional=true)
-    public void setTermsLookupFetchService(@Nullable  TermsLookupFetchService termsLookupFetchService) {
-        this.termsLookupFetchService = termsLookupFetchService;
-    }
-
     public void close() {
         cache.close();
     }
@@ -137,8 +116,8 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         return this.queryStringLenient;
     }
 
-    IndicesQueriesRegistry indicesQueriesRegistry() {
-        return indicesQueriesRegistry;
+    public QueryParser queryParser(String name) {
+        return indicesQueriesRegistry.queryParsers().get(name);
     }
 
     public ParsedQuery parse(QueryBuilder queryBuilder) {
@@ -147,10 +126,10 @@ public class IndexQueryParserService extends AbstractIndexComponent {
             BytesReference bytes = queryBuilder.buildAsBytes();
             parser = XContentFactory.xContent(bytes).createParser(bytes);
             return parse(cache.get(), parser);
-        } catch (QueryShardException e) {
+        } catch (QueryParsingException e) {
             throw e;
         } catch (Exception e) {
-            throw new QueryParsingException(getShardContext().parseContext(), "Failed to parse", e);
+            throw new QueryParsingException(getParseContext(), "Failed to parse", e);
         } finally {
             if (parser != null) {
                 parser.close();
@@ -167,10 +146,10 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         try {
             parser = XContentFactory.xContent(source, offset, length).createParser(source, offset, length);
             return parse(cache.get(), parser);
-        } catch (QueryShardException e) {
+        } catch (QueryParsingException e) {
             throw e;
         } catch (Exception e) {
-            throw new QueryParsingException(getShardContext().parseContext(), "Failed to parse", e);
+            throw new QueryParsingException(getParseContext(), "Failed to parse", e);
         } finally {
             if (parser != null) {
                 parser.close();
@@ -182,8 +161,7 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         return parse(cache.get(), source);
     }
 
-    //norelease
-    public ParsedQuery parse(QueryShardContext context, BytesReference source) {
+    public ParsedQuery parse(QueryParseContext context, BytesReference source) {
         XContentParser parser = null;
         try {
             parser = XContentFactory.xContent(source).createParser(source);
@@ -191,7 +169,7 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         } catch (QueryParsingException e) {
             throw e;
         } catch (Exception e) {
-            throw new QueryParsingException(context.parseContext(), "Failed to parse", e);
+            throw new QueryParsingException(context, "Failed to parse", e);
         } finally {
             if (parser != null) {
                 parser.close();
@@ -199,15 +177,15 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         }
     }
 
-    public ParsedQuery parse(String source) throws QueryParsingException, QueryShardException {
+    public ParsedQuery parse(String source) throws QueryParsingException {
         XContentParser parser = null;
         try {
             parser = XContentFactory.xContent(source).createParser(source);
             return innerParse(cache.get(), parser);
-        } catch (QueryShardException|QueryParsingException e) {
+        } catch (QueryParsingException e) {
             throw e;
         } catch (Exception e) {
-            throw new QueryParsingException(getShardContext().parseContext(), "Failed to parse [" + source + "]", e);
+            throw new QueryParsingException(getParseContext(), "Failed to parse [" + source + "]", e);
         } finally {
             if (parser != null) {
                 parser.close();
@@ -219,12 +197,11 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         return parse(cache.get(), parser);
     }
 
-    //norelease
-    public ParsedQuery parse(QueryShardContext context, XContentParser parser) {
+    public ParsedQuery parse(QueryParseContext context, XContentParser parser) {
         try {
             return innerParse(context, parser);
         } catch (IOException e) {
-            throw new QueryParsingException(context.parseContext(), "Failed to parse", e);
+            throw new QueryParsingException(context, "Failed to parse", e);
         }
     }
 
@@ -232,12 +209,11 @@ public class IndexQueryParserService extends AbstractIndexComponent {
      * Parses an inner filter, returning null if the filter should be ignored.
      */
     @Nullable
-    //norelease
     public ParsedQuery parseInnerFilter(XContentParser parser) throws IOException {
-        QueryShardContext context = cache.get();
+        QueryParseContext context = cache.get();
         context.reset(parser);
         try {
-            Query filter = context.parseContext().parseInnerFilter();
+            Query filter = context.parseInnerFilter();
             if (filter == null) {
                 return null;
             }
@@ -248,22 +224,27 @@ public class IndexQueryParserService extends AbstractIndexComponent {
     }
 
     @Nullable
-    public QueryBuilder parseInnerQueryBuilder(QueryParseContext parseContext) throws IOException {
-        parseContext.parseFieldMatcher(parseFieldMatcher);
-        return parseContext.parseInnerQueryBuilder();
+    public Query parseInnerQuery(XContentParser parser) throws IOException {
+        QueryParseContext context = cache.get();
+        context.reset(parser);
+        try {
+            return context.parseInnerQuery();
+        } finally {
+            context.reset(null);
+        }
     }
 
     @Nullable
-    //norelease
-    public Query parseInnerQuery(QueryShardContext context) throws IOException {
-        Query query = context.parseContext().parseInnerQueryBuilder().toQuery(context);
+    public Query parseInnerQuery(QueryParseContext parseContext) throws IOException {
+        parseContext.parseFieldMatcher(parseFieldMatcher);
+        Query query = parseContext.parseInnerQuery();
         if (query == null) {
             query = Queries.newMatchNoDocsQuery();
         }
         return query;
     }
 
-    public QueryShardContext getShardContext() {
+    public QueryParseContext getParseContext() {
         return cache.get();
     }
 
@@ -295,56 +276,37 @@ public class IndexQueryParserService extends AbstractIndexComponent {
                         XContentParser qSourceParser = XContentFactory.xContent(querySource).createParser(querySource);
                         parsedQuery = parse(qSourceParser);
                     } else {
-                        throw new QueryParsingException(getShardContext().parseContext(), "request does not support [" + fieldName + "]");
+                        throw new QueryParsingException(getParseContext(), "request does not support [" + fieldName + "]");
                     }
                 }
             }
             if (parsedQuery != null) {
                 return parsedQuery;
             }
-        } catch (QueryShardException e) {
+        } catch (QueryParsingException e) {
             throw e;
         } catch (Throwable e) {
-            throw new QueryParsingException(getShardContext().parseContext(), "Failed to parse", e);
+            throw new QueryParsingException(getParseContext(), "Failed to parse", e);
         }
 
-        throw new QueryParsingException(getShardContext().parseContext(), "Required query is missing");
+        throw new QueryParsingException(getParseContext(), "Required query is missing");
     }
 
-    //norelease
-    private ParsedQuery innerParse(QueryShardContext context, XContentParser parser) throws IOException, QueryShardException {
-        context.reset(parser);
+    private ParsedQuery innerParse(QueryParseContext parseContext, XContentParser parser) throws IOException, QueryParsingException {
+        parseContext.reset(parser);
         try {
-            context.parseFieldMatcher(parseFieldMatcher);
-            return innerParse(context, context.parseContext().parseInnerQueryBuilder());
+            parseContext.parseFieldMatcher(parseFieldMatcher);
+            Query query = parseContext.parseInnerQuery();
+            if (query == null) {
+                query = Queries.newMatchNoDocsQuery();
+            }
+            return new ParsedQuery(query, parseContext.copyNamedQueries());
         } finally {
-            context.reset(null);
+            parseContext.reset(null);
         }
     }
 
-    private static ParsedQuery innerParse(QueryShardContext context, QueryBuilder queryBuilder) throws IOException, QueryShardException {
-        Query query = queryBuilder.toQuery(context);
-        if (query == null) {
-            query = Queries.newMatchNoDocsQuery();
-        }
-        return new ParsedQuery(query, context.copyNamedQueries());
-    }
-
     public ParseFieldMatcher parseFieldMatcher() {
         return parseFieldMatcher;
     }
-
-    public boolean matchesIndices(String... indices) {
-        final String[] concreteIndices = indexNameExpressionResolver.concreteIndices(clusterService.state(), IndicesOptions.lenientExpandOpen(), indices);
-        for (String index : concreteIndices) {
-            if (Regex.simpleMatch(index, this.index.name())) {
-                return true;
-            }
-        }
-        return false;
-    }
-
-    public List<Object> handleTermsLookup(TermsLookup termsLookup) {
-        return this.termsLookupFetchService.fetch(termsLookup);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java
index e481ed7..7c2af81 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java
@@ -19,135 +19,69 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Arrays;
-import java.util.Objects;
 
 /**
  * A query that will execute the wrapped query only for the specified indices, and "match_all" when
  * it does not match those indices (by default).
  */
-public class IndicesQueryBuilder extends AbstractQueryBuilder<IndicesQueryBuilder> {
+public class IndicesQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "indices";
-
-    private final QueryBuilder innerQuery;
+    private final QueryBuilder queryBuilder;
 
     private final String[] indices;
 
-    private QueryBuilder noMatchQuery = defaultNoMatchQuery();
-
-    static final IndicesQueryBuilder PROTOTYPE = new IndicesQueryBuilder();
+    private String sNoMatchQuery;
+    private QueryBuilder noMatchQuery;
 
-    private IndicesQueryBuilder() {
-        this.innerQuery = null;
-        this.indices = null;
-    }
+    private String queryName;
 
-    public IndicesQueryBuilder(QueryBuilder innerQuery, String... indices) {
-        this.innerQuery = Objects.requireNonNull(innerQuery);
+    public IndicesQueryBuilder(QueryBuilder queryBuilder, String... indices) {
+        this.queryBuilder = queryBuilder;
         this.indices = indices;
     }
 
-    public QueryBuilder innerQuery() {
-        return this.innerQuery;
-    }
-
-    public String[] indices() {
-        return this.indices;
+    /**
+     * Sets the no match query, can either be <tt>all</tt> or <tt>none</tt>.
+     */
+    public IndicesQueryBuilder noMatchQuery(String type) {
+        this.sNoMatchQuery = type;
+        return this;
     }
 
     /**
      * Sets the query to use when it executes on an index that does not match the indices provided.
      */
     public IndicesQueryBuilder noMatchQuery(QueryBuilder noMatchQuery) {
-        this.noMatchQuery = (noMatchQuery != null) ? noMatchQuery : defaultNoMatchQuery();
+        this.noMatchQuery = noMatchQuery;
         return this;
     }
 
     /**
-     * Sets the no match query, can either be <tt>all</tt> or <tt>none</tt>.
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public IndicesQueryBuilder noMatchQuery(String type) {
-        this.noMatchQuery = IndicesQueryParser.parseNoMatchQuery(type);
+    public IndicesQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
         return this;
     }
 
-    public QueryBuilder noMatchQuery() {
-        return this.noMatchQuery;
-    }
-
-    static QueryBuilder defaultNoMatchQuery() {
-        return QueryBuilders.matchAllQuery();
-    }
-
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(IndicesQueryParser.NAME);
         builder.field("indices", indices);
         builder.field("query");
-        innerQuery.toXContent(builder, params);
-        builder.field("no_match_query");
-        noMatchQuery.toXContent(builder, params);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        if (context.matchesIndices(indices)) {
-            return innerQuery.toQuery(context);
+        queryBuilder.toXContent(builder, params);
+        if (noMatchQuery != null) {
+            builder.field("no_match_query");
+            noMatchQuery.toXContent(builder, params);
+        } else if (sNoMatchQuery != null) {
+            builder.field("no_match_query", sNoMatchQuery);
         }
-        return noMatchQuery.toQuery(context);
-    }
-    
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (this.innerQuery == null) {
-            validationException = addValidationError("inner query cannot be null", validationException);
-        }
-        if (this.indices == null || this.indices.length == 0) {
-            validationException = addValidationError("list of indices cannot be null or empty", validationException);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        validationException = validateInnerQuery(innerQuery, validationException);
-        validationException = validateInnerQuery(noMatchQuery, validationException);
-        return validationException;
-    }
-
-    @Override
-    protected IndicesQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        IndicesQueryBuilder indicesQueryBuilder = new IndicesQueryBuilder(in.readQuery(), in.readStringArray());
-        indicesQueryBuilder.noMatchQuery = in.readQuery();
-        return indicesQueryBuilder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(innerQuery);
-        out.writeStringArray(indices);
-        out.writeQuery(noMatchQuery);
-    }
-    
-    @Override
-    public int doHashCode() {
-        return Objects.hash(innerQuery, noMatchQuery, Arrays.hashCode(indices));
-    }
-    
-    @Override
-    protected boolean doEquals(IndicesQueryBuilder other) {
-        return Objects.equals(innerQuery, other.innerQuery) &&
-                Arrays.equals(indices, other.indices) &&  // otherwise we are comparing pointers
-                Objects.equals(noMatchQuery, other.noMatchQuery);
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java
index b7a93ac..a18c865 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java
@@ -19,60 +19,78 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
+import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
+import org.elasticsearch.common.regex.Regex;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.query.support.XContentStructure;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collection;
 
 /**
- * Parser for {@link IndicesQueryBuilder}.
  */
-public class IndicesQueryParser extends BaseQueryParser {
+public class IndicesQueryParser implements QueryParser {
 
+    public static final String NAME = "indices";
     private static final ParseField QUERY_FIELD = new ParseField("query", "filter");
     private static final ParseField NO_MATCH_QUERY = new ParseField("no_match_query", "no_match_filter");
 
+    @Nullable
+    private final ClusterService clusterService;
+    private final IndexNameExpressionResolver indexNameExpressionResolver;
+
     @Inject
-    public IndicesQueryParser() {
+    public IndicesQueryParser(@Nullable ClusterService clusterService, IndexNameExpressionResolver indexNameExpressionResolver) {
+        this.clusterService = clusterService;
+        this.indexNameExpressionResolver = indexNameExpressionResolver;
     }
 
     @Override
     public String[] names() {
-        return new String[]{IndicesQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        QueryBuilder innerQuery = null;
-        Collection<String> indices = new ArrayList<>();
-        QueryBuilder noMatchQuery = IndicesQueryBuilder.defaultNoMatchQuery();
-
+        Query noMatchQuery = null;
+        boolean queryFound = false;
+        boolean indicesFound = false;
+        boolean currentIndexMatchesIndices = false;
         String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
 
         String currentFieldName = null;
         XContentParser.Token token;
+        XContentStructure.InnerQuery innerQuery = null;
+        XContentStructure.InnerQuery innerNoMatchQuery = null;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if (parseContext.parseFieldMatcher().match(currentFieldName, QUERY_FIELD)) {
-                    innerQuery = parseContext.parseInnerQueryBuilder();
+                    innerQuery = new XContentStructure.InnerQuery(parseContext, null);
+                    queryFound = true;
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, NO_MATCH_QUERY)) {
-                    noMatchQuery = parseContext.parseInnerQueryBuilder();
+                    innerNoMatchQuery = new XContentStructure.InnerQuery(parseContext, null);
                 } else {
                     throw new QueryParsingException(parseContext, "[indices] query does not support [" + currentFieldName + "]");
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if ("indices".equals(currentFieldName)) {
-                    if (indices.isEmpty() == false) {
+                    if (indicesFound) {
                         throw new QueryParsingException(parseContext, "[indices] indices or index already specified");
                     }
+                    indicesFound = true;
+                    Collection<String> indices = new ArrayList<>();
                     while (parser.nextToken() != XContentParser.Token.END_ARRAY) {
                         String value = parser.textOrNull();
                         if (value == null) {
@@ -80,50 +98,67 @@ public class IndicesQueryParser extends BaseQueryParser {
                         }
                         indices.add(value);
                     }
+                    currentIndexMatchesIndices = matchesIndices(parseContext.index().name(), indices.toArray(new String[indices.size()]));
                 } else {
                     throw new QueryParsingException(parseContext, "[indices] query does not support [" + currentFieldName + "]");
                 }
             } else if (token.isValue()) {
                 if ("index".equals(currentFieldName)) {
-                    if (indices.isEmpty() == false) {
+                    if (indicesFound) {
                         throw new QueryParsingException(parseContext, "[indices] indices or index already specified");
                     }
-                    indices.add(parser.text());
+                    indicesFound = true;
+                    currentIndexMatchesIndices = matchesIndices(parseContext.index().name(), parser.text());
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, NO_MATCH_QUERY)) {
-                    noMatchQuery = parseNoMatchQuery(parser.text());
+                    String type = parser.text();
+                    if ("all".equals(type)) {
+                        noMatchQuery = Queries.newMatchAllQuery();
+                    } else if ("none".equals(type)) {
+                        noMatchQuery = Queries.newMatchNoDocsQuery();
+                    }
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else {
                     throw new QueryParsingException(parseContext, "[indices] query does not support [" + currentFieldName + "]");
                 }
             }
         }
-        
-        if (innerQuery == null) {
+        if (!queryFound) {
             throw new QueryParsingException(parseContext, "[indices] requires 'query' element");
         }
-        if (indices.isEmpty()) {
+        if (!indicesFound) {
             throw new QueryParsingException(parseContext, "[indices] requires 'indices' or 'index' element");
         }
-        return new IndicesQueryBuilder(innerQuery, indices.toArray(new String[indices.size()]))
-                .noMatchQuery(noMatchQuery)
-                .boost(boost)
-                .queryName(queryName);
-    }
 
-    static QueryBuilder parseNoMatchQuery(String type) {
-        if ("all".equals(type)) {
-            return QueryBuilders.matchAllQuery();
-        } else if ("none".equals(type)) {
-            return new MatchNoneQueryBuilder();
+        Query chosenQuery;
+        if (currentIndexMatchesIndices) {
+            chosenQuery = innerQuery.asQuery();
+        } else {
+            // If noMatchQuery is set, it means "no_match_query" was "all" or "none"
+            if (noMatchQuery != null) {
+                chosenQuery = noMatchQuery;
+            } else {
+                // There might be no "no_match_query" set, so default to the match_all if not set
+                if (innerNoMatchQuery == null) {
+                    chosenQuery = Queries.newMatchAllQuery();
+                } else {
+                    chosenQuery = innerNoMatchQuery.asQuery();
+                }
+            }
+        }
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, chosenQuery);
         }
-        throw new IllegalArgumentException("query type can only be [all] or [none] but not " + "[" + type + "]");
+        return chosenQuery;
     }
 
-    @Override
-    public IndicesQueryBuilder getBuilderPrototype() {
-        return IndicesQueryBuilder.PROTOTYPE;
+    protected boolean matchesIndices(String currentIndex, String... indices) {
+        final String[] concreteIndices = indexNameExpressionResolver.concreteIndices(clusterService.state(), IndicesOptions.lenientExpandOpen(), indices);
+        for (String index : concreteIndices) {
+            if (Regex.simpleMatch(index, currentIndex)) {
+                return true;
+            }
+        }
+        return false;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java
index b217a5e..9d44f39 100644
--- a/core/src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java
@@ -19,11 +19,7 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
 import org.elasticsearch.action.search.SearchRequestBuilder;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
@@ -32,62 +28,18 @@ import java.io.IOException;
  * @deprecated Use {@link SearchRequestBuilder#setTerminateAfter(int)} instead.
  */
 @Deprecated
-public class LimitQueryBuilder extends AbstractQueryBuilder<LimitQueryBuilder> {
+public class LimitQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "limit";
     private final int limit;
-    static final LimitQueryBuilder PROTOTYPE = new LimitQueryBuilder(-1);
 
     public LimitQueryBuilder(int limit) {
         this.limit = limit;
     }
 
-    public int limit() {
-        return limit;
-    }
-
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(LimitQueryParser.NAME);
         builder.field("value", limit);
-        printBoostAndQueryName(builder);
         builder.endObject();
     }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        // this filter is deprecated and parses to a filter that matches everything
-        return Queries.newMatchAllQuery();
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        // nothing to validate
-        return null;
-    }
-
-    @Override
-    protected boolean doEquals(LimitQueryBuilder other) {
-        return Integer.compare(other.limit, limit) == 0;
-    }
-
-    @Override
-    protected int doHashCode() {
-        return this.limit;
-    }
-
-    @Override
-    protected LimitQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new LimitQueryBuilder(in.readInt());
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeInt(limit);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/LimitQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/LimitQueryParser.java
index ed47198..3419f61 100644
--- a/core/src/main/java/org/elasticsearch/index/query/LimitQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/LimitQueryParser.java
@@ -19,17 +19,17 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
 
-/**
- * Parser for limit query
- * @deprecated use terminate_after feature instead
- */
 @Deprecated
-public class LimitQueryParser extends BaseQueryParser<LimitQueryBuilder> {
+public class LimitQueryParser implements QueryParser {
+
+    public static final String NAME = "limit";
 
     @Inject
     public LimitQueryParser() {
@@ -37,16 +37,14 @@ public class LimitQueryParser extends BaseQueryParser<LimitQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{LimitQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public LimitQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         int limit = -1;
-        String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         String currentFieldName = null;
         XContentParser.Token token;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -55,10 +53,6 @@ public class LimitQueryParser extends BaseQueryParser<LimitQueryBuilder> {
             } else if (token.isValue()) {
                 if ("value".equals(currentFieldName)) {
                     limit = parser.intValue();
-                } else if ("_name".equals(currentFieldName)) {
-                    queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else {
                     throw new QueryParsingException(parseContext, "[limit] query does not support [" + currentFieldName + "]");
                 }
@@ -69,11 +63,7 @@ public class LimitQueryParser extends BaseQueryParser<LimitQueryBuilder> {
             throw new QueryParsingException(parseContext, "No value specified for limit query");
         }
 
-        return new LimitQueryBuilder(limit).boost(boost).queryName(queryName);
-    }
-
-    @Override
-    public LimitQueryBuilder getBuilderPrototype() {
-        return LimitQueryBuilder.PROTOTYPE;
+        // this filter is deprecated and parses to a filter that matches everything
+        return Queries.newMatchAllQuery();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java
index 00c5019..b09bc9f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java
@@ -19,10 +19,6 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
@@ -30,52 +26,26 @@ import java.io.IOException;
 /**
  * A query that matches on all documents.
  */
-public class MatchAllQueryBuilder extends AbstractQueryBuilder<MatchAllQueryBuilder> {
+public class MatchAllQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<MatchAllQueryBuilder> {
 
-    public static final String NAME = "match_all";
-
-    static final MatchAllQueryBuilder PROTOTYPE = new MatchAllQueryBuilder();
-
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return Queries.newMatchAllQuery();
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        // nothing to validate
-        return null;
-    }
+    private float boost = -1;
 
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
     @Override
-    protected boolean doEquals(MatchAllQueryBuilder other) {
-        return true;
+    public MatchAllQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     @Override
-    protected int doHashCode() {
-        return 0;
-    }
-
-    @Override
-    protected MatchAllQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new MatchAllQueryBuilder();
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        //nothing to write really
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(MatchAllQueryParser.NAME);
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java
index 4066c75..933d3d3 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java
@@ -19,16 +19,21 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
 
 /**
- * Parser for match_all query
+ *
  */
-public class MatchAllQueryParser extends BaseQueryParser<MatchAllQueryBuilder> {
+public class MatchAllQueryParser implements QueryParser {
+
+    public static final String NAME = "match_all";
 
     @Inject
     public MatchAllQueryParser() {
@@ -36,38 +41,35 @@ public class MatchAllQueryParser extends BaseQueryParser<MatchAllQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{MatchAllQueryBuilder.NAME, Strings.toCamelCase(MatchAllQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public MatchAllQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
+        float boost = 1.0f;
         String currentFieldName = null;
+
         XContentParser.Token token;
-        String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         while (((token = parser.nextToken()) != XContentParser.Token.END_OBJECT && token != XContentParser.Token.END_ARRAY)) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
             } else if (token.isValue()) {
-                if ("_name".equals(currentFieldName)) {
-                    queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
+                if ("boost".equals(currentFieldName)) {
                     boost = parser.floatValue();
                 } else {
                     throw new QueryParsingException(parseContext, "[match_all] query does not support [" + currentFieldName + "]");
                 }
             }
         }
-        MatchAllQueryBuilder queryBuilder = new MatchAllQueryBuilder();
-        queryBuilder.boost(boost);
-        queryBuilder.queryName(queryName);
-        return queryBuilder;
-    }
 
-    @Override
-    public MatchAllQueryBuilder getBuilderPrototype() {
-        return MatchAllQueryBuilder.PROTOTYPE;
+        if (boost == 1.0f) {
+            return Queries.newMatchAllQuery();
+        }
+
+        MatchAllDocsQuery query = new MatchAllDocsQuery();
+        query.setBoost(boost);
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/MatchNoneQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MatchNoneQueryBuilder.java
deleted file mode 100644
index 247f514..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/MatchNoneQueryBuilder.java
+++ /dev/null
@@ -1,80 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-
-import java.io.IOException;
-
-/**
- * A query that matches no document.
- */
-public class MatchNoneQueryBuilder extends AbstractQueryBuilder<MatchNoneQueryBuilder> {
-
-    public static final String NAME = "match_none";
-
-    public static final MatchNoneQueryBuilder PROTOTYPE = new MatchNoneQueryBuilder();
-
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return Queries.newMatchNoDocsQuery();
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        // nothing to validate
-        return null;
-    }
-
-    @Override
-    protected boolean doEquals(MatchNoneQueryBuilder other) {
-        return true;
-    }
-
-    @Override
-    protected int doHashCode() {
-        return 0;
-    }
-
-    @Override
-    protected MatchNoneQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new MatchNoneQueryBuilder();
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        //nothing to write really
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/MatchNoneQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/MatchNoneQueryParser.java
deleted file mode 100644
index 3536a5d..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/MatchNoneQueryParser.java
+++ /dev/null
@@ -1,55 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.xcontent.XContentParser;
-
-import java.io.IOException;
-
-public class MatchNoneQueryParser extends BaseQueryParser {
-
-    @Inject
-    public MatchNoneQueryParser() {
-    }
-
-    @Override
-    public String[] names() {
-        return new String[]{MatchNoneQueryBuilder.NAME, Strings.toCamelCase(MatchNoneQueryBuilder.NAME)};
-    }
-
-    @Override
-    public MatchNoneQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
-        XContentParser parser = parseContext.parser();
-
-        XContentParser.Token token = parser.nextToken();
-        if (token != XContentParser.Token.END_OBJECT) {
-            throw new QueryParsingException(parseContext, "[match_none] query malformed");
-        }
-
-        return new MatchNoneQueryBuilder();
-    }
-
-    @Override
-    public MatchNoneQueryBuilder getBuilderPrototype() {
-        return MatchNoneQueryBuilder.PROTOTYPE;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java
index 5fbfff7..6f73f08 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java
@@ -29,9 +29,12 @@ import java.util.Locale;
  * Match query is a query that analyzes the text and constructs a query as the result of the analysis. It
  * can construct different queries based on the type provided.
  */
-public class MatchQueryBuilder extends AbstractQueryBuilder<MatchQueryBuilder> {
+public class MatchQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<MatchQueryBuilder> {
 
-    public static final String NAME = "match";
+    public enum Operator {
+        OR,
+        AND
+    }
 
     public enum Type {
         /**
@@ -63,6 +66,8 @@ public class MatchQueryBuilder extends AbstractQueryBuilder<MatchQueryBuilder> {
 
     private String analyzer;
 
+    private Float boost;
+
     private Integer slop;
 
     private Fuzziness fuzziness;
@@ -83,7 +88,7 @@ public class MatchQueryBuilder extends AbstractQueryBuilder<MatchQueryBuilder> {
 
     private Float cutoff_Frequency = null;
 
-    static final MatchQueryBuilder PROTOTYPE = new MatchQueryBuilder(null, null);
+    private String queryName;
 
     /**
      * Constructs a new text query.
@@ -119,6 +124,15 @@ public class MatchQueryBuilder extends AbstractQueryBuilder<MatchQueryBuilder> {
     }
 
     /**
+     * Set the boost to apply to the query.
+     */
+    @Override
+    public MatchQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
+    /**
      * Set the phrase slop if evaluated to a phrase query type.
      */
     public MatchQueryBuilder slop(int slop) {
@@ -187,9 +201,17 @@ public class MatchQueryBuilder extends AbstractQueryBuilder<MatchQueryBuilder> {
         return this;
     }
 
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public MatchQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
     @Override
     public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(MatchQueryParser.NAME);
         builder.startObject(name);
 
         builder.field("query", text);
@@ -202,6 +224,9 @@ public class MatchQueryBuilder extends AbstractQueryBuilder<MatchQueryBuilder> {
         if (analyzer != null) {
             builder.field("analyzer", analyzer);
         }
+        if (boost != null) {
+            builder.field("boost", boost);
+        }
         if (slop != null) {
             builder.field("slop", slop);
         }
@@ -233,13 +258,12 @@ public class MatchQueryBuilder extends AbstractQueryBuilder<MatchQueryBuilder> {
         if (cutoff_Frequency != null) {
             builder.field("cutoff_frequency", cutoff_Frequency);
         }
-        printBoostAndQueryName(builder);
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
+
+
         builder.endObject();
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java
index 7997af6..2bf0d7c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.index.query;
 
 import org.apache.lucene.queries.ExtendedCommonTermsQuery;
+import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
@@ -34,7 +35,9 @@ import java.io.IOException;
 /**
  *
  */
-public class MatchQueryParser extends BaseQueryParserTemp {
+public class MatchQueryParser implements QueryParser {
+
+    public static final String NAME = "match";
 
     @Inject
     public MatchQueryParser() {
@@ -43,13 +46,12 @@ public class MatchQueryParser extends BaseQueryParserTemp {
     @Override
     public String[] names() {
         return new String[]{
-                MatchQueryBuilder.NAME, "match_phrase", "matchPhrase", "match_phrase_prefix", "matchPhrasePrefix", "matchFuzzy", "match_fuzzy", "fuzzy_match"
+                NAME, "match_phrase", "matchPhrase", "match_phrase_prefix", "matchPhrasePrefix", "matchFuzzy", "match_fuzzy", "fuzzy_match"
         };
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         MatchQuery.Type type = MatchQuery.Type.BOOLEAN;
@@ -68,8 +70,8 @@ public class MatchQueryParser extends BaseQueryParserTemp {
         String fieldName = parser.currentName();
 
         Object value = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        MatchQuery matchQuery = new MatchQuery(context);
+        float boost = 1.0f;
+        MatchQuery matchQuery = new MatchQuery(parseContext);
         String minimumShouldMatch = null;
         String queryName = null;
 
@@ -95,7 +97,7 @@ public class MatchQueryParser extends BaseQueryParserTemp {
                         }
                     } else if ("analyzer".equals(currentFieldName)) {
                         String analyzer = parser.text();
-                        if (context.analysisService().analyzer(analyzer) == null) {
+                        if (parseContext.analysisService().analyzer(analyzer) == null) {
                             throw new QueryParsingException(parseContext, "[match] analyzer [" + parser.text() + "] not found");
                         }
                         matchQuery.setAnalyzer(analyzer);
@@ -110,7 +112,15 @@ public class MatchQueryParser extends BaseQueryParserTemp {
                     } else if ("max_expansions".equals(currentFieldName) || "maxExpansions".equals(currentFieldName)) {
                         matchQuery.setMaxExpansions(parser.intValue());
                     } else if ("operator".equals(currentFieldName)) {
-                        matchQuery.setOccur(Operator.fromString(parser.text()).toBooleanClauseOccur());
+                        String op = parser.text();
+                        if ("or".equalsIgnoreCase(op)) {
+                            matchQuery.setOccur(BooleanClause.Occur.SHOULD);
+                        } else if ("and".equalsIgnoreCase(op)) {
+                            matchQuery.setOccur(BooleanClause.Occur.MUST);
+                        } else {
+                            throw new QueryParsingException(parseContext, "text query requires operator to be either 'and' or 'or', not ["
+                                    + op + "]");
+                        }
                     } else if ("minimum_should_match".equals(currentFieldName) || "minimumShouldMatch".equals(currentFieldName)) {
                         minimumShouldMatch = parser.textOrNull();
                     } else if ("fuzzy_rewrite".equals(currentFieldName) || "fuzzyRewrite".equals(currentFieldName)) {
@@ -164,13 +174,8 @@ public class MatchQueryParser extends BaseQueryParserTemp {
         }
         query.setBoost(boost);
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
         return query;
     }
-
-    @Override
-    public MatchQueryBuilder getBuilderPrototype() {
-        return MatchQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java
index 253af16..ac3f279 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java
@@ -19,45 +19,25 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.*;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.internal.FieldNamesFieldMapper;
-import org.elasticsearch.index.mapper.object.ObjectMapper;
 
 import java.io.IOException;
-import java.util.Collection;
-import java.util.Objects;
 
 /**
- * Constructs a filter that have only null values or no value in the original field.
+ * Constructs a filter that only match on documents that the field has a value in them.
  */
-public class MissingQueryBuilder extends AbstractQueryBuilder<MissingQueryBuilder> {
+public class MissingQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "missing";
+    private String name;
 
-    public static final boolean DEFAULT_NULL_VALUE = false;
+    private String queryName;
 
-    public static final boolean DEFAULT_EXISTENCE_VALUE = true;
+    private Boolean nullValue;
 
-    private final String fieldPattern;
+    private Boolean existence;
 
-    private boolean nullValue = DEFAULT_NULL_VALUE;
-
-    private boolean existence = DEFAULT_EXISTENCE_VALUE;
-
-    static final MissingQueryBuilder PROTOTYPE = new MissingQueryBuilder(null);
-
-    public MissingQueryBuilder(String fieldPattern) {
-        this.fieldPattern = fieldPattern;
-    }
-
-    public String fieldPattern() {
-        return this.fieldPattern;
+    public MissingQueryBuilder(String name) {
+        this.name = name;
     }
 
     /**
@@ -70,15 +50,7 @@ public class MissingQueryBuilder extends AbstractQueryBuilder<MissingQueryBuilde
     }
 
     /**
-     * Returns true if the missing filter will include documents where the field contains a null value, otherwise
-     * these documents will not be included.
-     */
-    public boolean nullValue() {
-        return this.nullValue;
-    }
-
-    /**
-     * Should the missing filter include documents where the field doesn't exist in the docs.
+     * Should the missing filter include documents where the field doesn't exists in the docs.
      * Defaults to <tt>true</tt>.
      */
     public MissingQueryBuilder existence(boolean existence) {
@@ -87,157 +59,26 @@ public class MissingQueryBuilder extends AbstractQueryBuilder<MissingQueryBuilde
     }
 
     /**
-     * Returns true if the missing filter will include documents where the field has no values, otherwise
-     * these documents will not be included.
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
      */
-    public boolean existence() {
-        return this.existence;
+    public MissingQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("field", fieldPattern);
-        builder.field("null_value", nullValue);
-        builder.field("existence", existence);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return newFilter(context, fieldPattern, existence, nullValue);
-    }
-
-    public static Query newFilter(QueryShardContext context, String fieldPattern, boolean existence, boolean nullValue) {
-        if (!existence && !nullValue) {
-            throw new QueryShardException(context, "missing must have either existence, or null_value, or both set to true");
-        }
-
-        final FieldNamesFieldMapper.FieldNamesFieldType fieldNamesFieldType = (FieldNamesFieldMapper.FieldNamesFieldType) context.mapperService().fullName(FieldNamesFieldMapper.NAME);
-        if (fieldNamesFieldType == null) {
-            // can only happen when no types exist, so no docs exist either
-            return Queries.newMatchNoDocsQuery();
-        }
-
-        ObjectMapper objectMapper = context.getObjectMapper(fieldPattern);
-        if (objectMapper != null) {
-            // automatic make the object mapper pattern
-            fieldPattern = fieldPattern + ".*";
-        }
-
-        Collection<String> fields = context.simpleMatchToIndexNames(fieldPattern);
-        if (fields.isEmpty()) {
-            if (existence) {
-                // if we ask for existence of fields, and we found none, then we should match on all
-                return Queries.newMatchAllQuery();
-            }
-            return null;
-        }
-
-        Query existenceFilter = null;
-        Query nullFilter = null;
-
-        if (existence) {
-            BooleanQuery boolFilter = new BooleanQuery();
-            for (String field : fields) {
-                MappedFieldType fieldType = context.fieldMapper(field);
-                Query filter = null;
-                if (fieldNamesFieldType.isEnabled()) {
-                    final String f;
-                    if (fieldType != null) {
-                        f = fieldType.names().indexName();
-                    } else {
-                        f = field;
-                    }
-                    filter = fieldNamesFieldType.termQuery(f, context);
-                }
-                // if _field_names are not indexed, we need to go the slow way
-                if (filter == null && fieldType != null) {
-                    filter = fieldType.rangeQuery(null, null, true, true);
-                }
-                if (filter == null) {
-                    filter = new TermRangeQuery(field, null, null, true, true);
-                }
-                boolFilter.add(filter, BooleanClause.Occur.SHOULD);
-            }
-
-            existenceFilter = boolFilter;
-            existenceFilter = Queries.not(existenceFilter);;
-        }
-
-        if (nullValue) {
-            for (String field : fields) {
-                MappedFieldType fieldType = context.fieldMapper(field);
-                if (fieldType != null) {
-                    nullFilter = fieldType.nullValueQuery();
-                }
-            }
-        }
-
-        Query filter;
-        if (nullFilter != null) {
-            if (existenceFilter != null) {
-                BooleanQuery combined = new BooleanQuery();
-                combined.add(existenceFilter, BooleanClause.Occur.SHOULD);
-                combined.add(nullFilter, BooleanClause.Occur.SHOULD);
-                // cache the not filter as well, so it will be faster
-                filter = combined;
-            } else {
-                filter = nullFilter;
-            }
-        } else {
-            filter = existenceFilter;
+        builder.startObject(MissingQueryParser.NAME);
+        builder.field("field", name);
+        if (nullValue != null) {
+            builder.field("null_value", nullValue);
         }
-
-        if (filter == null) {
-            return null;
-        }
-
-        return new ConstantScoreQuery(filter);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (Strings.isEmpty(this.fieldPattern)) {
-            validationException = addValidationError("missing must be provided with a [field]", validationException);
+        if (existence != null) {
+            builder.field("existence", existence);
         }
-        if (!existence && !nullValue) {
-            validationException = addValidationError("missing must have either existence, or null_value, or both set to true", validationException);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return validationException;
-    }
-
-    @Override
-    protected MissingQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        MissingQueryBuilder missingQueryBuilder = new MissingQueryBuilder(in.readString());
-        missingQueryBuilder.nullValue = in.readBoolean();
-        missingQueryBuilder.existence = in.readBoolean();
-        return missingQueryBuilder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldPattern);
-        out.writeBoolean(nullValue);
-        out.writeBoolean(existence);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldPattern, nullValue, existence);
-    }
-
-    @Override
-    protected boolean doEquals(MissingQueryBuilder other) {
-        return Objects.equals(fieldPattern, other.fieldPattern) &&
-                Objects.equals(nullValue, other.nullValue) &&
-                Objects.equals(existence, other.existence);
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java
index 1dd6bd1..6ef19d7 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java
@@ -19,15 +19,29 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermRangeQuery;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.internal.FieldNamesFieldMapper;
+import org.elasticsearch.index.mapper.object.ObjectMapper;
 
 import java.io.IOException;
+import java.util.Collection;
 
 /**
- * Parser for missing query
+ *
  */
-public class MissingQueryParser extends BaseQueryParser<MissingQueryBuilder> {
+public class MissingQueryParser implements QueryParser {
+
+    public static final String NAME = "missing";
+    public static final boolean DEFAULT_NULL_VALUE = false;
+    public static final boolean DEFAULT_EXISTENCE_VALUE = true;
 
     @Inject
     public MissingQueryParser() {
@@ -35,18 +49,17 @@ public class MissingQueryParser extends BaseQueryParser<MissingQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{MissingQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public MissingQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldPattern = null;
         String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        boolean nullValue = MissingQueryBuilder.DEFAULT_NULL_VALUE;
-        boolean existence = MissingQueryBuilder.DEFAULT_EXISTENCE_VALUE;
+        boolean nullValue = DEFAULT_NULL_VALUE;
+        boolean existence = DEFAULT_EXISTENCE_VALUE;
 
         XContentParser.Token token;
         String currentFieldName = null;
@@ -62,8 +75,6 @@ public class MissingQueryParser extends BaseQueryParser<MissingQueryBuilder> {
                     existence = parser.booleanValue();
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else {
                     throw new QueryParsingException(parseContext, "[missing] query does not support [" + currentFieldName + "]");
                 }
@@ -73,15 +84,98 @@ public class MissingQueryParser extends BaseQueryParser<MissingQueryBuilder> {
         if (fieldPattern == null) {
             throw new QueryParsingException(parseContext, "missing must be provided with a [field]");
         }
-        return new MissingQueryBuilder(fieldPattern)
-                .nullValue(nullValue)
-                .existence(existence)
-                .boost(boost)
-                .queryName(queryName);
+
+        return newFilter(parseContext, fieldPattern, existence, nullValue, queryName);
     }
 
-    @Override
-    public MissingQueryBuilder getBuilderPrototype() {
-        return MissingQueryBuilder.PROTOTYPE;
+    public static Query newFilter(QueryParseContext parseContext, String fieldPattern, boolean existence, boolean nullValue, String queryName) {
+        if (!existence && !nullValue) {
+            throw new QueryParsingException(parseContext, "missing must have either existence, or null_value, or both set to true");
+        }
+
+        final FieldNamesFieldMapper.FieldNamesFieldType fieldNamesFieldType = (FieldNamesFieldMapper.FieldNamesFieldType)parseContext.mapperService().fullName(FieldNamesFieldMapper.NAME);
+        if (fieldNamesFieldType == null) {
+            // can only happen when no types exist, so no docs exist either
+            return Queries.newMatchNoDocsQuery();
+        }
+
+        ObjectMapper objectMapper = parseContext.getObjectMapper(fieldPattern);
+        if (objectMapper != null) {
+            // automatic make the object mapper pattern
+            fieldPattern = fieldPattern + ".*";
+        }
+
+        Collection<String> fields = parseContext.simpleMatchToIndexNames(fieldPattern);
+        if (fields.isEmpty()) {
+            if (existence) {
+                // if we ask for existence of fields, and we found none, then we should match on all
+                return Queries.newMatchAllQuery();
+            }
+            return null;
+        }
+
+        Query existenceFilter = null;
+        Query nullFilter = null;
+
+        if (existence) {
+            BooleanQuery boolFilter = new BooleanQuery();
+            for (String field : fields) {
+                MappedFieldType fieldType = parseContext.fieldMapper(field);
+                Query filter = null;
+                if (fieldNamesFieldType.isEnabled()) {
+                    final String f;
+                    if (fieldType != null) {
+                        f = fieldType.names().indexName();
+                    } else {
+                        f = field;
+                    }
+                    filter = fieldNamesFieldType.termQuery(f, parseContext);
+                }
+                // if _field_names are not indexed, we need to go the slow way
+                if (filter == null && fieldType != null) {
+                    filter = fieldType.rangeQuery(null, null, true, true);
+                }
+                if (filter == null) {
+                    filter = new TermRangeQuery(field, null, null, true, true);
+                }
+                boolFilter.add(filter, BooleanClause.Occur.SHOULD);
+            }
+
+            existenceFilter = boolFilter;
+            existenceFilter = Queries.not(existenceFilter);;
+        }
+
+        if (nullValue) {
+            for (String field : fields) {
+                MappedFieldType fieldType = parseContext.fieldMapper(field);
+                if (fieldType != null) {
+                    nullFilter = fieldType.nullValueQuery();
+                }
+            }
+        }
+
+        Query filter;
+        if (nullFilter != null) {
+            if (existenceFilter != null) {
+                BooleanQuery combined = new BooleanQuery();
+                combined.add(existenceFilter, BooleanClause.Occur.SHOULD);
+                combined.add(nullFilter, BooleanClause.Occur.SHOULD);
+                // cache the not filter as well, so it will be faster
+                filter = combined;
+            } else {
+                filter = nullFilter;
+            }
+        } else {
+            filter = existenceFilter;
+        }
+
+        if (filter == null) {
+            return null;
+        }
+
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, existenceFilter);
+        }
+        return new ConstantScoreQuery(filter);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java
index fbd13ea..19d65d9 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java
@@ -23,11 +23,7 @@ import org.elasticsearch.action.get.MultiGetRequest;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.lucene.uid.Versions;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentType;
+import org.elasticsearch.common.xcontent.*;
 import org.elasticsearch.index.VersionType;
 import org.elasticsearch.search.fetch.source.FetchSourceContext;
 
@@ -41,7 +37,7 @@ import java.util.Locale;
  * A more like this query that finds documents that are "like" the provided {@link #likeText(String)}
  * which is checked against the fields the query is constructed with.
  */
-public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQueryBuilder> {
+public class MoreLikeThisQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<MoreLikeThisQueryBuilder> {
 
     /**
      * A single get item. Pure delegate to multi get.
@@ -132,8 +128,6 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
         }
     }
 
-    public static final String NAME = "mlt";
-
     private final String[] fields;
     private List<Item> docs = new ArrayList<>();
     private List<Item> unlikeDocs = new ArrayList<>();
@@ -147,10 +141,10 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
     private int minWordLength = -1;
     private int maxWordLength = -1;
     private float boostTerms = -1;
+    private float boost = -1;
     private String analyzer;
     private Boolean failOnUnsupportedField;
-
-    static final MoreLikeThisQueryBuilder PROTOTYPE = new MoreLikeThisQueryBuilder();
+    private String queryName;
 
     /**
      * Constructs a new more like this query which uses the "_all" field.
@@ -346,6 +340,12 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
         return this;
     }
 
+    @Override
+    public MoreLikeThisQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
     /**
      * Whether to fail or return no result when this query is run against a field which is not supported such as binary/numeric fields.
      */
@@ -354,10 +354,18 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
         return this;
     }
 
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public MoreLikeThisQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
         String likeFieldName = MoreLikeThisQueryParser.Fields.LIKE.getPreferredName();
-        builder.startObject(NAME);
+        builder.startObject(MoreLikeThisQueryParser.NAME);
         if (fields != null) {
             builder.startArray("fields");
             for (String field : fields) {
@@ -404,21 +412,21 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
         if (boostTerms != -1) {
             builder.field(MoreLikeThisQueryParser.Fields.BOOST_TERMS.getPreferredName(), boostTerms);
         }
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
         if (analyzer != null) {
             builder.field("analyzer", analyzer);
         }
         if (failOnUnsupportedField != null) {
             builder.field(MoreLikeThisQueryParser.Fields.FAIL_ON_UNSUPPORTED_FIELD.getPreferredName(), failOnUnsupportedField);
         }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         if (include != null) {
             builder.field("include", include);
         }
-        printBoostAndQueryName(builder);
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java
index 8822e8f..98c3e2b 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.index.query;
 
 import com.google.common.collect.Sets;
-
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.queries.TermsQuery;
 import org.apache.lucene.search.BooleanClause;
@@ -55,8 +54,9 @@ import static org.elasticsearch.index.mapper.Uid.createUidAsBytes;
 /**
  *
  */
-public class MoreLikeThisQueryParser extends BaseQueryParserTemp {
+public class MoreLikeThisQueryParser implements QueryParser {
 
+    public static final String NAME = "mlt";
     private MoreLikeThisFetchService fetchService = null;
 
     public static class Fields {
@@ -89,16 +89,15 @@ public class MoreLikeThisQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{MoreLikeThisQueryBuilder.NAME, "more_like_this", "moreLikeThis"};
+        return new String[]{NAME, "more_like_this", "moreLikeThis"};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         MoreLikeThisQuery mltQuery = new MoreLikeThisQuery();
-        mltQuery.setSimilarity(context.searchSimilarity());
+        mltQuery.setSimilarity(parseContext.searchSimilarity());
         Analyzer analyzer = null;
         List<String> moreLikeFields = null;
         boolean failOnUnsupportedField = true;
@@ -145,7 +144,7 @@ public class MoreLikeThisQueryParser extends BaseQueryParserTemp {
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, Fields.MINIMUM_SHOULD_MATCH)) {
                     mltQuery.setMinimumShouldMatch(parser.text());
                 } else if ("analyzer".equals(currentFieldName)) {
-                    analyzer = context.analysisService().analyzer(parser.text());
+                    analyzer = parseContext.analysisService().analyzer(parser.text());
                 } else if ("boost".equals(currentFieldName)) {
                     mltQuery.setBoost(parser.floatValue());
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, Fields.FAIL_ON_UNSUPPORTED_FIELD)) {
@@ -168,7 +167,7 @@ public class MoreLikeThisQueryParser extends BaseQueryParserTemp {
                     moreLikeFields = new LinkedList<>();
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                         String field = parser.text();
-                        MappedFieldType fieldType = context.fieldMapper(field);
+                        MappedFieldType fieldType = parseContext.fieldMapper(field);
                         moreLikeFields.add(fieldType == null ? field : fieldType.names().indexName());
                     }
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, Fields.DOCUMENT_IDS)) {
@@ -217,14 +216,14 @@ public class MoreLikeThisQueryParser extends BaseQueryParserTemp {
 
         // set analyzer
         if (analyzer == null) {
-            analyzer = context.mapperService().searchAnalyzer();
+            analyzer = parseContext.mapperService().searchAnalyzer();
         }
         mltQuery.setAnalyzer(analyzer);
 
         // set like text fields
         boolean useDefaultField = (moreLikeFields == null);
         if (useDefaultField) {
-            moreLikeFields = Collections.singletonList(context.defaultField());
+            moreLikeFields = Collections.singletonList(parseContext.defaultField());
         }
         // possibly remove unsupported fields
         removeUnsupportedFields(moreLikeFields, analyzer, failOnUnsupportedField);
@@ -235,7 +234,7 @@ public class MoreLikeThisQueryParser extends BaseQueryParserTemp {
 
         // support for named query
         if (queryName != null) {
-            context.addNamedQuery(queryName, mltQuery);
+            parseContext.addNamedQuery(queryName, mltQuery);
         }
 
         // handle like texts
@@ -256,15 +255,15 @@ public class MoreLikeThisQueryParser extends BaseQueryParserTemp {
 
             for (TermVectorsRequest item : items) {
                 if (item.index() == null) {
-                    item.index(context.index().name());
+                    item.index(parseContext.index().name());
                 }
                 if (item.type() == null) {
-                    if (context.queryTypes().size() > 1) {
+                    if (parseContext.queryTypes().size() > 1) {
                         throw new QueryParsingException(parseContext,
                                     "ambiguous type for item with id: " + item.id()
                                 + " and index: " + item.index());
                     } else {
-                        item.type(context.queryTypes().iterator().next());
+                        item.type(parseContext.queryTypes().iterator().next());
                     }
                 }
                 // default fields if not present but don't override for artificial docs
@@ -357,9 +356,4 @@ public class MoreLikeThisQueryParser extends BaseQueryParserTemp {
             boolQuery.add(query, BooleanClause.Occur.MUST_NOT);
         }
     }
-
-    @Override
-    public MoreLikeThisQueryBuilder getBuilderPrototype() {
-        return MoreLikeThisQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java
index e46d2fd..9059865 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.index.query;
 
 import com.carrotsearch.hppc.ObjectFloatHashMap;
-
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
@@ -37,9 +36,7 @@ import java.util.Locale;
 /**
  * Same as {@link MatchQueryBuilder} but supports multiple fields.
  */
-public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQueryBuilder> {
-
-    public static final String NAME = "multi_match";
+public class MultiMatchQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<MultiMatchQueryBuilder> {
 
     private final Object text;
 
@@ -48,10 +45,12 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
 
     private MultiMatchQueryBuilder.Type type;
 
-    private Operator operator;
+    private MatchQueryBuilder.Operator operator;
 
     private String analyzer;
 
+    private Float boost;
+
     private Integer slop;
 
     private Fuzziness fuzziness;
@@ -74,7 +73,8 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
 
     private MatchQueryBuilder.ZeroTermsQuery zeroTermsQuery = null;
 
-    static final MultiMatchQueryBuilder PROTOTYPE = new MultiMatchQueryBuilder(null);
+    private String queryName;
+
 
     public enum Type {
 
@@ -141,7 +141,7 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
                 }
             }
             if (type == null) {
-                throw new ElasticsearchParseException("failed to parse [{}] query type [{}]. unknown type.", NAME, value);
+                throw new ElasticsearchParseException("failed to parse [{}] query type [{}]. unknown type.", MultiMatchQueryParser.NAME, value);
             }
             return type;
         }
@@ -195,7 +195,7 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
     /**
      * Sets the operator to use when using a boolean query. Defaults to <tt>OR</tt>.
      */
-    public MultiMatchQueryBuilder operator(Operator operator) {
+    public MultiMatchQueryBuilder operator(MatchQueryBuilder.Operator operator) {
         this.operator = operator;
         return this;
     }
@@ -210,6 +210,15 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
     }
 
     /**
+     * Set the boost to apply to the query.
+     */
+    @Override
+    public MultiMatchQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
+    /**
      * Set the phrase slop if evaluated to a phrase query type.
      */
     public MultiMatchQueryBuilder slop(int slop) {
@@ -301,9 +310,17 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
         return this;
     }
 
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public MultiMatchQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
     @Override
     public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(MultiMatchQueryParser.NAME);
 
         builder.field("query", text);
         builder.startArray("fields");
@@ -325,6 +342,9 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
         if (analyzer != null) {
             builder.field("analyzer", analyzer);
         }
+        if (boost != null) {
+            builder.field("boost", boost);
+        }
         if (slop != null) {
             builder.field("slop", slop);
         }
@@ -364,13 +384,11 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
             builder.field("zero_terms_query", zeroTermsQuery.toString());
         }
 
-        printBoostAndQueryName(builder);
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
 
         builder.endObject();
     }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java
index fcd79d8..5922f52 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java
@@ -21,6 +21,7 @@ package org.elasticsearch.index.query;
 
 import com.google.common.collect.Maps;
 
+import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.regex.Regex;
@@ -36,7 +37,9 @@ import java.util.Map;
 /**
  * Same as {@link MatchQueryParser} but has support for multiple fields.
  */
-public class MultiMatchQueryParser extends BaseQueryParserTemp {
+public class MultiMatchQueryParser implements QueryParser {
+
+    public static final String NAME = "multi_match";
 
     @Inject
     public MultiMatchQueryParser() {
@@ -45,20 +48,19 @@ public class MultiMatchQueryParser extends BaseQueryParserTemp {
     @Override
     public String[] names() {
         return new String[]{
-                MultiMatchQueryBuilder.NAME, "multiMatch"
+                NAME, "multiMatch"
         };
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         Object value = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         Float tieBreaker = null;
         MultiMatchQueryBuilder.Type type = null;
-        MultiMatchQuery multiMatchQuery = new MultiMatchQuery(context);
+        MultiMatchQuery multiMatchQuery = new MultiMatchQuery(parseContext);
         String minimumShouldMatch = null;
         Map<String, Float> fieldNameWithBoosts = Maps.newHashMap();
         String queryName = null;
@@ -71,12 +73,12 @@ public class MultiMatchQueryParser extends BaseQueryParserTemp {
             } else if ("fields".equals(currentFieldName)) {
                 if (token == XContentParser.Token.START_ARRAY) {
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        extractFieldAndBoost(context, parser, fieldNameWithBoosts);
+                        extractFieldAndBoost(parseContext, parser, fieldNameWithBoosts);
                     }
                 } else if (token.isValue()) {
-                    extractFieldAndBoost(context, parser, fieldNameWithBoosts);
+                    extractFieldAndBoost(parseContext, parser, fieldNameWithBoosts);
                 } else {
-                    throw new QueryParsingException(parseContext, "[" + MultiMatchQueryBuilder.NAME + "] query does not support [" + currentFieldName + "]");
+                    throw new QueryParsingException(parseContext, "[" + NAME + "] query does not support [" + currentFieldName + "]");
                 }
             } else if (token.isValue()) {
                 if ("query".equals(currentFieldName)) {
@@ -85,8 +87,8 @@ public class MultiMatchQueryParser extends BaseQueryParserTemp {
                     type = MultiMatchQueryBuilder.Type.parse(parser.text(), parseContext.parseFieldMatcher());
                 } else if ("analyzer".equals(currentFieldName)) {
                     String analyzer = parser.text();
-                    if (context.analysisService().analyzer(analyzer) == null) {
-                        throw new QueryParsingException(parseContext, "[" + MultiMatchQueryBuilder.NAME + "] analyzer [" + parser.text() + "] not found");
+                    if (parseContext.analysisService().analyzer(analyzer) == null) {
+                        throw new QueryParsingException(parseContext, "[" + NAME + "] analyzer [" + parser.text() + "] not found");
                     }
                     multiMatchQuery.setAnalyzer(analyzer);
                 } else if ("boost".equals(currentFieldName)) {
@@ -100,7 +102,15 @@ public class MultiMatchQueryParser extends BaseQueryParserTemp {
                 } else if ("max_expansions".equals(currentFieldName) || "maxExpansions".equals(currentFieldName)) {
                     multiMatchQuery.setMaxExpansions(parser.intValue());
                 } else if ("operator".equals(currentFieldName)) {
-                    multiMatchQuery.setOccur(Operator.fromString(parser.text()).toBooleanClauseOccur());
+                    String op = parser.text();
+                    if ("or".equalsIgnoreCase(op)) {
+                        multiMatchQuery.setOccur(BooleanClause.Occur.SHOULD);
+                    } else if ("and".equalsIgnoreCase(op)) {
+                        multiMatchQuery.setOccur(BooleanClause.Occur.MUST);
+                    } else {
+                        throw new QueryParsingException(parseContext, "text query requires operator to be either 'and' or 'or', not [" + op
+                                + "]");
+                    }
                 } else if ("minimum_should_match".equals(currentFieldName) || "minimumShouldMatch".equals(currentFieldName)) {
                     minimumShouldMatch = parser.textOrNull();
                 } else if ("fuzzy_rewrite".equals(currentFieldName) || "fuzzyRewrite".equals(currentFieldName)) {
@@ -157,12 +167,12 @@ public class MultiMatchQueryParser extends BaseQueryParserTemp {
 
         query.setBoost(boost);
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
         return query;
     }
 
-    private void extractFieldAndBoost(QueryShardContext context, XContentParser parser, Map<String, Float> fieldNameWithBoosts) throws IOException {
+    private void extractFieldAndBoost(QueryParseContext parseContext, XContentParser parser, Map<String, Float> fieldNameWithBoosts) throws IOException {
         String fField = null;
         Float fBoost = null;
         char[] fieldText = parser.textCharacters();
@@ -180,16 +190,11 @@ public class MultiMatchQueryParser extends BaseQueryParserTemp {
         }
 
         if (Regex.isSimpleMatchPattern(fField)) {
-            for (String field : context.mapperService().simpleMatchToIndexNames(fField)) {
+            for (String field : parseContext.mapperService().simpleMatchToIndexNames(fField)) {
                 fieldNameWithBoosts.put(field, fBoost);
             }
         } else {
             fieldNameWithBoosts.put(fField, fBoost);
         }
     }
-
-    @Override
-    public MultiMatchQueryBuilder getBuilderPrototype() {
-        return MultiMatchQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MultiTermQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MultiTermQueryBuilder.java
index 0e946d6..9c7383d 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MultiTermQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MultiTermQueryBuilder.java
@@ -18,6 +18,6 @@
  */
 package org.elasticsearch.index.query;
 
-public interface MultiTermQueryBuilder<QB extends MultiTermQueryBuilder<QB>> extends QueryBuilder<QB> {
+public abstract class MultiTermQueryBuilder extends QueryBuilder {
 
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java
index bb2e4b4..63b40dc 100644
--- a/core/src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java
@@ -25,9 +25,7 @@ import org.elasticsearch.index.query.support.QueryInnerHitBuilder;
 import java.io.IOException;
 import java.util.Objects;
 
-public class NestedQueryBuilder extends AbstractQueryBuilder<NestedQueryBuilder> {
-
-    public static final String NAME = "nested";
+public class NestedQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<NestedQueryBuilder> {
 
     private final QueryBuilder queryBuilder;
 
@@ -35,28 +33,39 @@ public class NestedQueryBuilder extends AbstractQueryBuilder<NestedQueryBuilder>
 
     private String scoreMode;
 
-    private QueryInnerHitBuilder innerHit;
+    private float boost = 1.0f;
 
-    static final NestedQueryBuilder PROTOTYPE = new NestedQueryBuilder();
+    private String queryName;
+
+    private QueryInnerHitBuilder innerHit;
 
     public NestedQueryBuilder(String path, QueryBuilder queryBuilder) {
         this.path = path;
         this.queryBuilder = Objects.requireNonNull(queryBuilder);
     }
+    /**
+     * The score mode.
+     */
+    public NestedQueryBuilder scoreMode(String scoreMode) {
+        this.scoreMode = scoreMode;
+        return this;
+    }
 
     /**
-     * private constructor only used internally
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
      */
-    private NestedQueryBuilder() {
-        this.path = null;
-        this.queryBuilder = null;
+    @Override
+    public NestedQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
-     * The score mode.
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public NestedQueryBuilder scoreMode(String scoreMode) {
-        this.scoreMode = scoreMode;
+    public NestedQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
         return this;
     }
 
@@ -70,14 +79,19 @@ public class NestedQueryBuilder extends AbstractQueryBuilder<NestedQueryBuilder>
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(NestedQueryParser.NAME);
         builder.field("query");
         queryBuilder.toXContent(builder, params);
         builder.field("path", path);
         if (scoreMode != null) {
             builder.field("score_mode", scoreMode);
         }
-        printBoostAndQueryName(builder);
+        if (boost != 1.0f) {
+            builder.field("boost", boost);
+        }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         if (innerHit != null) {
             builder.startObject("inner_hits");
             builder.value(innerHit);
@@ -86,8 +100,4 @@ public class NestedQueryBuilder extends AbstractQueryBuilder<NestedQueryBuilder>
         builder.endObject();
     }
 
-    @Override
-    public final String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java
index 01bfaa6..e14720b 100644
--- a/core/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java
@@ -36,8 +36,9 @@ import org.elasticsearch.search.fetch.innerhits.InnerHitsSubSearchContext;
 
 import java.io.IOException;
 
-public class NestedQueryParser extends BaseQueryParserTemp {
+public class NestedQueryParser implements QueryParser {
 
+    public static final String NAME = "nested";
     private static final ParseField FILTER_FIELD = new ParseField("filter").withAllDeprecated("query");
 
     private final InnerHitsQueryParserHelper innerHitsQueryParserHelper;
@@ -49,16 +50,15 @@ public class NestedQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{NestedQueryBuilder.NAME, Strings.toCamelCase(NestedQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
-        final ToBlockJoinQueryBuilder builder = new ToBlockJoinQueryBuilder(context);
+        final ToBlockJoinQueryBuilder builder = new ToBlockJoinQueryBuilder(parseContext);
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         ScoreMode scoreMode = ScoreMode.Avg;
         String queryName = null;
 
@@ -110,7 +110,7 @@ public class NestedQueryParser extends BaseQueryParserTemp {
         if (joinQuery != null) {
             joinQuery.setBoost(boost);
             if (queryName != null) {
-                context.addNamedQuery(queryName, joinQuery);
+                parseContext.addNamedQuery(queryName, joinQuery);
             }
         }
         return joinQuery;
@@ -121,8 +121,8 @@ public class NestedQueryParser extends BaseQueryParserTemp {
         private ScoreMode scoreMode;
         private InnerHitsSubSearchContext innerHits;
 
-        public ToBlockJoinQueryBuilder(QueryShardContext context) throws IOException {
-            super(context);
+        public ToBlockJoinQueryBuilder(QueryParseContext parseContext) throws IOException {
+            super(parseContext);
         }
 
         public void setScoreMode(ScoreMode scoreMode) {
@@ -146,14 +146,14 @@ public class NestedQueryParser extends BaseQueryParserTemp {
                     innerQuery = null;
                 }
             } else {
-                throw new QueryShardException(shardContext, "[nested] requires either 'query' or 'filter' field");
+                throw new QueryParsingException(parseContext, "[nested] requires either 'query' or 'filter' field");
             }
 
             if (innerHits != null) {
-                ParsedQuery parsedQuery = new ParsedQuery(innerQuery, shardContext.copyNamedQueries());
+                ParsedQuery parsedQuery = new ParsedQuery(innerQuery, parseContext.copyNamedQueries());
                 InnerHitsContext.NestedInnerHits nestedInnerHits = new InnerHitsContext.NestedInnerHits(innerHits.getSubSearchContext(), parsedQuery, null, getParentObjectMapper(), nestedObjectMapper);
                 String name = innerHits.getName() != null ? innerHits.getName() : path;
-                shardContext.addInnerHits(name, nestedInnerHits);
+                parseContext.addInnerHits(name, nestedInnerHits);
             }
 
             if (innerQuery != null) {
@@ -164,9 +164,4 @@ public class NestedQueryParser extends BaseQueryParserTemp {
         }
 
     }
-
-    @Override
-    public NestedQueryBuilder getBuilderPrototype() {
-        return NestedQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java
index a26ebb7..c16cf64 100644
--- a/core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java
@@ -19,10 +19,6 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
@@ -31,71 +27,29 @@ import java.util.Objects;
 /**
  * A filter that matches documents matching boolean combinations of other filters.
  */
-public class NotQueryBuilder extends AbstractQueryBuilder<NotQueryBuilder> {
-
-    public static final String NAME = "not";
+public class NotQueryBuilder extends QueryBuilder {
 
     private final QueryBuilder filter;
 
-    static final NotQueryBuilder PROTOTYPE = new NotQueryBuilder(null);
+    private String queryName;
 
     public NotQueryBuilder(QueryBuilder filter) {
-        this.filter = filter;
+        this.filter = Objects.requireNonNull(filter);
     }
 
-    /**
-     * @return the query added to "not".
-     */
-    public QueryBuilder innerQuery() {
-        return this.filter;
+    public NotQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(NotQueryParser.NAME);
         builder.field("query");
         filter.toXContent(builder, params);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query luceneQuery = filter.toFilter(context);
-        if (luceneQuery == null) {
-            return null;
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return Queries.not(luceneQuery);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        return validateInnerQuery(filter, null);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(filter);
-    }
-
-    @Override
-    protected boolean doEquals(NotQueryBuilder other) {
-        return Objects.equals(filter, other.filter);
-    }
-
-    @Override
-    protected NotQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryBuilder queryBuilder = in.readQuery();
-        return new NotQueryBuilder(queryBuilder);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(filter);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/NotQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/NotQueryParser.java
index 2388eb1..6bfe4c7 100644
--- a/core/src/main/java/org/elasticsearch/index/query/NotQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/NotQueryParser.java
@@ -19,17 +19,20 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
 
 /**
- * Parser for not query
+ *
  */
-public class NotQueryParser extends BaseQueryParser<NotQueryBuilder> {
+public class NotQueryParser implements QueryParser {
 
+    public static final String NAME = "not";
     private static final ParseField QUERY_FIELD = new ParseField("filter", "query");
 
     @Inject
@@ -38,19 +41,18 @@ public class NotQueryParser extends BaseQueryParser<NotQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{NotQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public NotQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        QueryBuilder query = null;
+        Query query = null;
         boolean queryFound = false;
 
         String queryName = null;
         String currentFieldName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         XContentParser.Token token;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
@@ -59,18 +61,16 @@ public class NotQueryParser extends BaseQueryParser<NotQueryBuilder> {
                 // skip
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if (parseContext.parseFieldMatcher().match(currentFieldName, QUERY_FIELD)) {
-                    query = parseContext.parseInnerFilterToQueryBuilder();
+                    query = parseContext.parseInnerFilter();
                     queryFound = true;
                 } else {
                     queryFound = true;
                     // its the filter, and the name is the field
-                    query = parseContext.parseInnerFilterToQueryBuilder(currentFieldName);
+                    query = parseContext.parseInnerFilter(currentFieldName);
                 }
             } else if (token.isValue()) {
                 if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else {
                     throw new QueryParsingException(parseContext, "[not] query does not support [" + currentFieldName + "]");
                 }
@@ -81,14 +81,14 @@ public class NotQueryParser extends BaseQueryParser<NotQueryBuilder> {
             throw new QueryParsingException(parseContext, "filter is required when using `not` query");
         }
 
-        NotQueryBuilder notQueryBuilder = new NotQueryBuilder(query);
-        notQueryBuilder.queryName(queryName);
-        notQueryBuilder.boost(boost);
-        return notQueryBuilder;
-    }
+        if (query == null) {
+            return null;
+        }
 
-    @Override
-    public NotQueryBuilder getBuilderPrototype() {
-        return NotQueryBuilder.PROTOTYPE;
+        Query notQuery = Queries.not(query);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, notQuery);
+        }
+        return notQuery;
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/Operator.java b/core/src/main/java/org/elasticsearch/index/query/Operator.java
deleted file mode 100644
index 1470737..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/Operator.java
+++ /dev/null
@@ -1,84 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.BooleanClause;
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.util.CollectionUtils;
-
-import java.io.IOException;
-
-public enum Operator implements Writeable<Operator> {
-    OR(0), AND(1);
-
-    private final int ordinal;
-
-    private static final Operator PROTOTYPE = OR;
-
-    private Operator(int ordinal) {
-        this.ordinal = ordinal;
-    }
-
-    public BooleanClause.Occur toBooleanClauseOccur() {
-        switch (this) {
-            case OR:
-                return BooleanClause.Occur.SHOULD;
-            case AND:
-                return BooleanClause.Occur.MUST;
-            default:
-                throw Operator.newOperatorException(this.toString());
-        }
-    }
-
-    @Override
-    public Operator readFrom(StreamInput in) throws IOException {
-        int ord = in.readVInt();
-        for (Operator operator : Operator.values()) {
-            if (operator.ordinal == ord) {
-                return operator;
-            }
-        }
-        throw new ElasticsearchException("unknown serialized operator [" + ord + "]");
-    }
-
-    public static Operator readOperatorFrom(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeVInt(this.ordinal);
-    }
-
-    public static Operator fromString(String op) {
-        for (Operator operator : Operator.values()) {
-            if (operator.name().equalsIgnoreCase(op)) {
-                return operator;
-            }
-        }
-        throw Operator.newOperatorException(op);
-    }
-
-    private static IllegalArgumentException newOperatorException(String op) {
-        return new IllegalArgumentException("operator needs to be either " + CollectionUtils.arrayAsArrayList(Operator.values()) + ", but not [" + op + "]");
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java
index 0dbbbe9..e8ad48b 100644
--- a/core/src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java
@@ -19,31 +19,22 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.BooleanClause.Occur;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collections;
-import java.util.List;
-import java.util.Objects;
 
 /**
  * A filter that matches documents matching boolean combinations of other filters.
  * @deprecated Use {@link BoolQueryBuilder} instead
  */
 @Deprecated
-public class OrQueryBuilder extends AbstractQueryBuilder<OrQueryBuilder> {
+public class OrQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "or";
+    private ArrayList<QueryBuilder> filters = new ArrayList<>();
 
-    private final ArrayList<QueryBuilder> filters = new ArrayList<>();
-
-    static final OrQueryBuilder PROTOTYPE = new OrQueryBuilder();
+    private String queryName;
 
     public OrQueryBuilder(QueryBuilder... filters) {
         Collections.addAll(this.filters, filters);
@@ -51,87 +42,28 @@ public class OrQueryBuilder extends AbstractQueryBuilder<OrQueryBuilder> {
 
     /**
      * Adds a filter to the list of filters to "or".
-     * No <tt>null</tt> value allowed.
      */
     public OrQueryBuilder add(QueryBuilder filterBuilder) {
         filters.add(filterBuilder);
         return this;
     }
 
-    /**
-     * @return the list of queries added to "or".
-     */
-    public List<QueryBuilder> innerQueries() {
-        return this.filters;
+    public OrQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(OrQueryParser.NAME);
         builder.startArray("filters");
         for (QueryBuilder filter : filters) {
             filter.toXContent(builder, params);
         }
         builder.endArray();
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        if (filters.isEmpty()) {
-            // no filters provided, this should be ignored upstream
-            return null;
-        }
-
-        BooleanQuery query = new BooleanQuery();
-        for (QueryBuilder f : filters) {
-            Query innerQuery = f.toFilter(context);
-            // ignore queries that are null
-            if (innerQuery != null) {
-                query.add(innerQuery, Occur.SHOULD);
-            }
-        }
-        if (query.clauses().isEmpty()) {
-            // no inner lucene query exists, ignore upstream
-            return null;
-        }
-        return query;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        return validateInnerQueries(filters, null);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(filters);
-    }
-
-    @Override
-    protected boolean doEquals(OrQueryBuilder other) {
-        return Objects.equals(filters, other.filters);
-    }
-
-    @Override
-    protected OrQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        OrQueryBuilder orQueryBuilder = new OrQueryBuilder();
-        List<QueryBuilder> queryBuilders = readQueries(in);
-        for (QueryBuilder queryBuilder : queryBuilders) {
-            orQueryBuilder.add(queryBuilder);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return orQueryBuilder;
-
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        writeQueries(out, filters);
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/OrQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/OrQueryParser.java
index f1b38e4..ff2c0b2 100644
--- a/core/src/main/java/org/elasticsearch/index/query/OrQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/OrQueryParser.java
@@ -19,6 +19,9 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
 
@@ -26,11 +29,12 @@ import java.io.IOException;
 import java.util.ArrayList;
 
 /**
- * Parser for or query
- * @deprecated use bool query instead
+ *
  */
 @Deprecated
-public class OrQueryParser extends BaseQueryParser<OrQueryBuilder> {
+public class OrQueryParser implements QueryParser {
+
+    public static final String NAME = "or";
 
     @Inject
     public OrQueryParser() {
@@ -38,24 +42,23 @@ public class OrQueryParser extends BaseQueryParser<OrQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{OrQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public OrQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        final ArrayList<QueryBuilder> queries = new ArrayList<>();
+        ArrayList<Query> queries = new ArrayList<>();
         boolean queriesFound = false;
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         String queryName = null;
         String currentFieldName = null;
         XContentParser.Token token = parser.currentToken();
         if (token == XContentParser.Token.START_ARRAY) {
             while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                 queriesFound = true;
-                QueryBuilder filter = parseContext.parseInnerFilterToQueryBuilder();
+                Query filter = parseContext.parseInnerFilter();
                 if (filter != null) {
                     queries.add(filter);
                 }
@@ -68,7 +71,15 @@ public class OrQueryParser extends BaseQueryParser<OrQueryBuilder> {
                     if ("filters".equals(currentFieldName)) {
                         queriesFound = true;
                         while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                            QueryBuilder filter = parseContext.parseInnerFilterToQueryBuilder();
+                            Query filter = parseContext.parseInnerFilter();
+                            if (filter != null) {
+                                queries.add(filter);
+                            }
+                        }
+                    } else {
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                            queriesFound = true;
+                            Query filter = parseContext.parseInnerFilter();
                             if (filter != null) {
                                 queries.add(filter);
                             }
@@ -77,8 +88,6 @@ public class OrQueryParser extends BaseQueryParser<OrQueryBuilder> {
                 } else if (token.isValue()) {
                     if ("_name".equals(currentFieldName)) {
                         queryName = parser.text();
-                    } else if ("boost".equals(currentFieldName)) {
-                        boost = parser.floatValue();
                     } else {
                         throw new QueryParsingException(parseContext, "[or] query does not support [" + currentFieldName + "]");
                     }
@@ -90,17 +99,17 @@ public class OrQueryParser extends BaseQueryParser<OrQueryBuilder> {
             throw new QueryParsingException(parseContext, "[or] query requires 'filters' to be set on it'");
         }
 
-        OrQueryBuilder orQuery = new OrQueryBuilder();
-        for (QueryBuilder query : queries) {
-            orQuery.add(query);
+        if (queries.isEmpty()) {
+            return null;
         }
-        orQuery.queryName(queryName);
-        orQuery.boost(boost);
-        return orQuery;
-    }
 
-    @Override
-    public OrQueryBuilder getBuilderPrototype() {
-        return OrQueryBuilder.PROTOTYPE;
+        BooleanQuery query = new BooleanQuery();
+        for (Query f : queries) {
+            query.add(f, Occur.SHOULD);
+        }
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java
index a49580c..e0e5b2f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java
@@ -19,53 +19,44 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.PrefixQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * A Query that matches documents containing terms with a specified prefix.
  */
-public class PrefixQueryBuilder extends AbstractQueryBuilder<PrefixQueryBuilder> implements MultiTermQueryBuilder<PrefixQueryBuilder> {
+public class PrefixQueryBuilder extends MultiTermQueryBuilder implements BoostableQueryBuilder<PrefixQueryBuilder> {
 
-    public static final String NAME = "prefix";
+    private final String name;
 
-    private final String fieldName;
+    private final String prefix;
 
-    private final String value;
+    private float boost = -1;
 
     private String rewrite;
 
-    static final PrefixQueryBuilder PROTOTYPE = new PrefixQueryBuilder(null, null);
+    private String queryName;
 
     /**
      * A Query that matches documents containing terms with a specified prefix.
      *
-     * @param fieldName The name of the field
-     * @param value The prefix query
+     * @param name   The name of the field
+     * @param prefix The prefix query
      */
-    public PrefixQueryBuilder(String fieldName, String value) {
-        this.fieldName = fieldName;
-        this.value = value;
+    public PrefixQueryBuilder(String name, String prefix) {
+        this.name = name;
+        this.prefix = prefix;
     }
 
-    public String fieldName() {
-        return this.fieldName;
-    }
-
-    public String value() {
-        return this.value;
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
+    @Override
+    public PrefixQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     public PrefixQueryBuilder rewrite(String rewrite) {
@@ -73,83 +64,33 @@ public class PrefixQueryBuilder extends AbstractQueryBuilder<PrefixQueryBuilder>
         return this;
     }
 
-    public String rewrite() {
-        return this.rewrite;
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public PrefixQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.startObject(fieldName);
-        builder.field("prefix", this.value);
-        if (rewrite != null) {
-            builder.field("rewrite", rewrite);
-        }
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        MultiTermQuery.RewriteMethod method = QueryParsers.parseRewriteMethod(context.parseFieldMatcher(), rewrite, null);
-
-        Query query = null;
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType != null) {
-            query = fieldType.prefixQuery(value, method, context);
-        }
-        if (query == null) {
-            PrefixQuery prefixQuery = new PrefixQuery(new Term(fieldName, BytesRefs.toBytesRef(value)));
-            if (method != null) {
-                prefixQuery.setRewriteMethod(method);
+        builder.startObject(PrefixQueryParser.NAME);
+        if (boost == -1 && rewrite == null && queryName == null) {
+            builder.field(name, prefix);
+        } else {
+            builder.startObject(name);
+            builder.field("prefix", prefix);
+            if (boost != -1) {
+                builder.field("boost", boost);
             }
-            query = prefixQuery;
-        }
-
-        return query;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (Strings.isEmpty(this.fieldName)) {
-            validationException = addValidationError("field name cannot be null or empty.", validationException);
-        }
-        if (this.value == null) {
-            validationException = addValidationError("query text cannot be null", validationException);
+            if (rewrite != null) {
+                builder.field("rewrite", rewrite);
+            }
+            if (queryName != null) {
+                builder.field("_name", queryName);
+            }
+            builder.endObject();
         }
-        return validationException;
-    }
-
-    @Override
-    protected PrefixQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        PrefixQueryBuilder prefixQueryBuilder = new PrefixQueryBuilder(in.readString(), in.readString());
-        prefixQueryBuilder.rewrite = in.readOptionalString();
-        return prefixQueryBuilder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-        out.writeString(value);
-        out.writeOptionalString(rewrite);
-    }
-
-    @Override
-    protected final int doHashCode() {
-        return Objects.hash(fieldName, value, rewrite);
-    }
-
-    @Override
-    protected boolean doEquals(PrefixQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-                Objects.equals(value, other.value) &&
-                Objects.equals(rewrite, other.rewrite);
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java
index eac29fa..d61fec7 100644
--- a/core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java
@@ -19,16 +19,25 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.MultiTermQuery;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
 
 /**
- * Parser for prefix query
+ *
  */
-public class PrefixQueryParser extends BaseQueryParser<PrefixQueryBuilder> {
+public class PrefixQueryParser implements QueryParser {
+
+    public static final String NAME = "prefix";
 
     private static final ParseField NAME_FIELD = new ParseField("_name").withAllDeprecated("query name is not supported in short version of prefix query");
 
@@ -38,19 +47,19 @@ public class PrefixQueryParser extends BaseQueryParser<PrefixQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{PrefixQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public PrefixQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldName = parser.currentName();
-        String value = null;
-        String rewrite = null;
-
+        String rewriteMethod = null;
         String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+
+        String value = null;
+        float boost = 1.0f;
         String currentFieldName = null;
         XContentParser.Token token;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -71,7 +80,7 @@ public class PrefixQueryParser extends BaseQueryParser<PrefixQueryBuilder> {
                         } else if ("boost".equals(currentFieldName)) {
                             boost = parser.floatValue();
                         } else if ("rewrite".equals(currentFieldName)) {
-                            rewrite = parser.textOrNull();
+                            rewriteMethod = parser.textOrNull();
                         } else {
                             throw new QueryParsingException(parseContext, "[regexp] query does not support [" + currentFieldName + "]");
                         }
@@ -90,14 +99,25 @@ public class PrefixQueryParser extends BaseQueryParser<PrefixQueryBuilder> {
         if (value == null) {
             throw new QueryParsingException(parseContext, "No value specified for prefix query");
         }
-        return new PrefixQueryBuilder(fieldName, value)
-                .rewrite(rewrite)
-                .boost(boost)
-                .queryName(queryName);
-    }
 
-    @Override
-    public PrefixQueryBuilder getBuilderPrototype() {
-        return PrefixQueryBuilder.PROTOTYPE;
+        MultiTermQuery.RewriteMethod method = QueryParsers.parseRewriteMethod(parseContext.parseFieldMatcher(), rewriteMethod, null);
+
+        Query query = null;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            query = fieldType.prefixQuery(value, method, parseContext);
+        }
+        if (query == null) {
+            PrefixQuery prefixQuery = new PrefixQuery(new Term(fieldName, BytesRefs.toBytesRef(value)));
+            if (method != null) {
+                prefixQuery.setRewriteMethod(method);
+            }
+            query = prefixQuery;
+        }
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return  query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java
index 3f69375..fa11d32 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java
@@ -19,79 +19,25 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.NamedWriteable;
-import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.action.support.ToXContentToBytes;
+import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentType;
 
 import java.io.IOException;
 
-public interface QueryBuilder<QB extends QueryBuilder> extends NamedWriteable<QB>, ToXContent {
+public abstract class QueryBuilder extends ToXContentToBytes {
 
-    /**
-     * Validate the query.
-     * @return a {@link QueryValidationException} containing error messages, {@code null} if query is valid.
-     * e.g. if fields that are needed to create the lucene query are missing.
-     */
-    QueryValidationException validate();
+    protected QueryBuilder() {
+        super(XContentType.JSON);
+    }
 
-    /**
-     * Converts this QueryBuilder to a lucene {@link Query}.
-     * Returns <tt>null</tt> if this query should be ignored in the context of
-     * parent queries.
-     *
-     * @param context additional information needed to construct the queries
-     * @return the {@link Query} or <tt>null</tt> if this query should be ignored upstream
-     * @throws QueryShardException
-     * @throws IOException
-     */
-    Query toQuery(QueryShardContext context) throws IOException;
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        doXContent(builder, params);
+        builder.endObject();
+        return builder;
+    }
 
-    /**
-     * Converts this QueryBuilder to an unscored lucene {@link Query} that acts as a filter.
-     * Returns <tt>null</tt> if this query should be ignored in the context of
-     * parent queries.
-     *
-     * @param context additional information needed to construct the queries
-     * @return the {@link Query} or <tt>null</tt> if this query should be ignored upstream
-     * @throws QueryShardException
-     * @throws IOException
-     */
-    Query toFilter(QueryShardContext context) throws IOException;
-
-    /**
-     * Returns a {@link org.elasticsearch.common.bytes.BytesReference}
-     * containing the {@link ToXContent} output in binary format.
-     * Builds the request based on the default {@link XContentType}, either {@link Requests#CONTENT_TYPE} or provided as a constructor argument
-     */
-    //norelease once we move to serializing queries over the wire in Streamable format, this method shouldn't be needed anymore
-    BytesReference buildAsBytes();
-
-    /**
-     * Sets the arbitrary name to be assigned to the query (see named queries).
-     */
-    QB queryName(String queryName);
-
-    /**
-     * Returns the arbitrary name assigned to the query (see named queries).
-     */
-    String queryName();
-
-    /**
-     * Returns the boost for this query.
-     */
-    float boost();
-
-    /**
-     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
-     * weightings) have their score multiplied by the boost provided.
-     */
-    QB boost(float boost);
-
-    /**
-     * Returns the name that identifies uniquely the query
-     */
-    String getName();
+    protected abstract void doXContent(XContentBuilder builder, Params params) throws IOException;
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java b/core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java
index 9b6ac01..fe2852d 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java
@@ -40,7 +40,7 @@ import java.util.Map;
 public abstract class QueryBuilders {
 
     /**
-     * A query that matches on all documents.
+     * A query that match on all documents.
      */
     public static MatchAllQueryBuilder matchAllQuery() {
         return new MatchAllQueryBuilder();
@@ -59,11 +59,11 @@ public abstract class QueryBuilders {
     /**
      * Creates a common query for the provided field name and text.
      *
-     * @param fieldName The field name.
+     * @param name The field name.
      * @param text The query text (to be analyzed).
      */
-    public static CommonTermsQueryBuilder commonTermsQuery(String fieldName, Object text) {
-        return new CommonTermsQueryBuilder(fieldName, text);
+    public static CommonTermsQueryBuilder commonTermsQuery(String name, Object text) {
+        return new CommonTermsQueryBuilder(name, text);
     }
 
     /**
@@ -277,8 +277,8 @@ public abstract class QueryBuilders {
      * Unlike the "NOT" clause, this still selects documents that contain undesirable terms,
      * but reduces their overall score:
      */
-    public static BoostingQueryBuilder boostingQuery(QueryBuilder positiveQuery, QueryBuilder negativeQuery) {
-        return new BoostingQueryBuilder(positiveQuery, negativeQuery);
+    public static BoostingQueryBuilder boostingQuery() {
+        return new BoostingQueryBuilder();
     }
 
     /**
@@ -312,33 +312,26 @@ public abstract class QueryBuilders {
         return new SpanFirstQueryBuilder(match, end);
     }
 
-    public static SpanNearQueryBuilder spanNearQuery(int slop) {
-        return new SpanNearQueryBuilder(slop);
+    public static SpanNearQueryBuilder spanNearQuery() {
+        return new SpanNearQueryBuilder();
     }
 
-    public static SpanNotQueryBuilder spanNotQuery(SpanQueryBuilder include, SpanQueryBuilder exclude) {
-        return new SpanNotQueryBuilder(include, exclude);
+    public static SpanNotQueryBuilder spanNotQuery() {
+        return new SpanNotQueryBuilder();
     }
 
     public static SpanOrQueryBuilder spanOrQuery() {
         return new SpanOrQueryBuilder();
     }
 
-    /** Creates a new {@code span_within} builder.
-    * @param big the big clause, it must enclose {@code little} for a match.
-    * @param little the little clause, it must be contained within {@code big} for a match.
-    */
-    public static SpanWithinQueryBuilder spanWithinQuery(SpanQueryBuilder big, SpanQueryBuilder little) {
-        return new SpanWithinQueryBuilder(big, little);
+    /** Creates a new {@code span_within} builder. */
+    public static SpanWithinQueryBuilder spanWithinQuery() {
+        return new SpanWithinQueryBuilder();
     }
 
-    /**
-     * Creates a new {@code span_containing} builder.
-     * @param big the big clause, it must enclose {@code little} for a match.
-     * @param little the little clause, it must be contained within {@code big} for a match.
-     */
-    public static SpanContainingQueryBuilder spanContainingQuery(SpanQueryBuilder big, SpanQueryBuilder little) {
-        return new SpanContainingQueryBuilder(big, little);
+    /** Creates a new {@code span_containing} builder. */
+    public static SpanContainingQueryBuilder spanContainingQuery() {
+        return new SpanContainingQueryBuilder();
     }
 
     /**
@@ -556,8 +549,8 @@ public abstract class QueryBuilders {
     /**
      * A Query builder which allows building a query thanks to a JSON string or binary data.
      */
-    public static WrapperQueryBuilder wrapperQuery(byte[] source) {
-        return new WrapperQueryBuilder(source);
+    public static WrapperQueryBuilder wrapperQuery(byte[] source, int offset, int length) {
+        return new WrapperQueryBuilder(source, offset, length);
     }
 
     /**
@@ -600,10 +593,11 @@ public abstract class QueryBuilders {
     }
 
     /**
-     * A terms query that can extract the terms from another doc in an index.
+     * A terms lookup filter for the provided field name. A lookup terms filter can
+     * extract the terms to filter by from another doc in an index.
      */
-    public static TermsQueryBuilder termsLookupQuery(String name) {
-        return new TermsQueryBuilder(name);
+    public static TermsLookupQueryBuilder termsLookupQuery(String name) {
+        return new TermsLookupQueryBuilder(name);
     }
 
     /**
@@ -690,7 +684,7 @@ public abstract class QueryBuilders {
     public static GeohashCellQuery.Builder geoHashCellQuery(String name, String geohash, boolean neighbors) {
         return new GeohashCellQuery.Builder(name, geohash, neighbors);
     }
-
+    
     /**
      * A filter to filter based on a polygon defined by a set of locations  / points.
      *
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java b/core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java
index 6dc2d39..936e466 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java
@@ -19,14 +19,9 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * A filter that simply wraps a query.
@@ -34,13 +29,11 @@ import java.util.Objects;
  *             query as a filter directly.
  */
 @Deprecated
-public class QueryFilterBuilder extends AbstractQueryBuilder<QueryFilterBuilder> {
-
-    public static final String NAME = "query";
+public class QueryFilterBuilder extends QueryBuilder {
 
     private final QueryBuilder queryBuilder;
 
-    static final QueryFilterBuilder PROTOTYPE = new QueryFilterBuilder(null);
+    private String queryName;
 
     /**
      * A filter that simply wraps a query.
@@ -52,56 +45,26 @@ public class QueryFilterBuilder extends AbstractQueryBuilder<QueryFilterBuilder>
     }
 
     /**
-     * @return the query builder that is wrapped by this {@link QueryFilterBuilder}
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public QueryBuilder innerQuery() {
-        return this.queryBuilder;
+    public QueryFilterBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.field(NAME);
-        queryBuilder.toXContent(builder, params);
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        // inner query builder can potentially be `null`, in that case we ignore it
-        Query innerQuery = this.queryBuilder.toQuery(context);
-        if (innerQuery == null) {
-            return null;
+        if (queryName == null) {
+            builder.field(QueryFilterParser.NAME);
+            queryBuilder.toXContent(builder, params);
+        } else {
+            builder.startObject(FQueryFilterParser.NAME);
+            builder.field("query");
+            queryBuilder.toXContent(builder, params);
+            if (queryName != null) {
+                builder.field("_name", queryName);
+            }
+            builder.endObject();
         }
-        return new ConstantScoreQuery(innerQuery);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        return validateInnerQuery(queryBuilder, null);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(queryBuilder);
-    }
-
-    @Override
-    protected boolean doEquals(QueryFilterBuilder other) {
-        return Objects.equals(queryBuilder, other.queryBuilder);
-    }
-
-    @Override
-    protected QueryFilterBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryBuilder innerQueryBuilder = in.readQuery();
-        return new QueryFilterBuilder(innerQueryBuilder);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(queryBuilder);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryFilterParser.java b/core/src/main/java/org/elasticsearch/index/query/QueryFilterParser.java
index 03513ab..fdb9cb3 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryFilterParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryFilterParser.java
@@ -19,16 +19,16 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
 
 import java.io.IOException;
 
-/**
- * Parser for query filter
- * @deprecated use any query instead directly, possible since queries and filters are merged.
- */
 @Deprecated
-public class QueryFilterParser extends BaseQueryParser<QueryFilterBuilder> {
+public class QueryFilterParser implements QueryParser {
+
+    public static final String NAME = "query";
 
     @Inject
     public QueryFilterParser() {
@@ -36,16 +36,11 @@ public class QueryFilterParser extends BaseQueryParser<QueryFilterBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{QueryFilterBuilder.NAME};
-    }
-
-    @Override
-    public QueryFilterBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
-        return new QueryFilterBuilder(parseContext.parseInnerQueryBuilder());
+        return new String[]{NAME};
     }
 
     @Override
-    public QueryFilterBuilder getBuilderPrototype() {
-        return QueryFilterBuilder.PROTOTYPE;
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
+        return new ConstantScoreQuery(parseContext.parseInnerQuery());
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java b/core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java
index a8e055f..4b12200 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java
@@ -19,105 +19,207 @@
 
 package org.elasticsearch.index.query;
 
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.Maps;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.queryparser.classic.MapperQueryParser;
+import org.apache.lucene.queryparser.classic.QueryParserSettings;
+import org.apache.lucene.search.Filter;
 import org.apache.lucene.search.Query;
+import org.apache.lucene.search.join.BitDocIdSetFilter;
+import org.apache.lucene.search.similarities.Similarity;
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
+import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.Index;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
+import org.elasticsearch.index.analysis.AnalysisService;
+import org.elasticsearch.index.fielddata.IndexFieldData;
+import org.elasticsearch.index.mapper.*;
+import org.elasticsearch.index.mapper.core.StringFieldMapper;
+import org.elasticsearch.index.mapper.object.ObjectMapper;
+import org.elasticsearch.index.query.support.NestedScope;
+import org.elasticsearch.index.similarity.SimilarityService;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.search.fetch.innerhits.InnerHitsContext;
+import org.elasticsearch.search.internal.SearchContext;
+import org.elasticsearch.search.lookup.SearchLookup;
 
 import java.io.IOException;
+import java.util.*;
 
 public class QueryParseContext {
 
     private static final ParseField CACHE = new ParseField("_cache").withAllDeprecated("Elasticsearch makes its own caching decisions");
     private static final ParseField CACHE_KEY = new ParseField("_cache_key").withAllDeprecated("Filters are always used as cache keys");
 
-    private XContentParser parser;
+    private static ThreadLocal<String[]> typesContext = new ThreadLocal<>();
+
+    public static void setTypes(String[] types) {
+        typesContext.set(types);
+    }
+
+    public static String[] getTypes() {
+        return typesContext.get();
+    }
+
+    public static String[] setTypesWithPrevious(String[] types) {
+        String[] old = typesContext.get();
+        setTypes(types);
+        return old;
+    }
+
+    public static void removeTypes() {
+        typesContext.remove();
+    }
+
     private final Index index;
-    //norelease this flag is also used in the QueryShardContext, we need to make sure we set it there correctly in doToQuery()
+
+    private final Version indexVersionCreated;
+
+    private final IndexQueryParserService indexQueryParser;
+
+    private final Map<String, Query> namedQueries = Maps.newHashMap();
+
+    private final MapperQueryParser queryParser = new MapperQueryParser(this);
+
+    private XContentParser parser;
+
     private ParseFieldMatcher parseFieldMatcher;
 
-    //norelease this can eventually be deleted when context() method goes away
-    private final QueryShardContext shardContext;
-    private IndicesQueriesRegistry indicesQueriesRegistry;
+    private boolean allowUnmappedFields;
+
+    private boolean mapUnmappedFieldAsString;
 
-    public QueryParseContext(Index index, IndicesQueriesRegistry registry) {
+    private NestedScope nestedScope;
+
+    private boolean isFilter;
+
+    public QueryParseContext(Index index, IndexQueryParserService indexQueryParser) {
         this.index = index;
-        this.indicesQueriesRegistry = registry;
-        this.shardContext = null;
+        this.indexVersionCreated = Version.indexCreated(indexQueryParser.indexSettings());
+        this.indexQueryParser = indexQueryParser;
+    }
+
+    public void parseFieldMatcher(ParseFieldMatcher parseFieldMatcher) {
+        this.parseFieldMatcher = parseFieldMatcher;
     }
 
-    QueryParseContext(QueryShardContext context) {
-        this.shardContext = context;
-        this.index = context.index();
-        this.indicesQueriesRegistry = context.indexQueryParserService().indicesQueriesRegistry();
+    public ParseFieldMatcher parseFieldMatcher() {
+        return parseFieldMatcher;
     }
 
     public void reset(XContentParser jp) {
+        allowUnmappedFields = indexQueryParser.defaultAllowUnmappedFields();
         this.parseFieldMatcher = ParseFieldMatcher.EMPTY;
+        this.lookup = null;
         this.parser = jp;
+        this.namedQueries.clear();
+        this.nestedScope = new NestedScope();
+        this.isFilter = false;
     }
 
-    //norelease this is still used in BaseQueryParserTemp and FunctionScoreQueryParser, remove if not needed there anymore
-    @Deprecated
-    public QueryShardContext shardContext() {
-        return this.shardContext;
+    public Index index() {
+        return this.index;
+    }
+
+    public void parser(XContentParser parser) {
+        this.parser = parser;
     }
 
     public XContentParser parser() {
-        return this.parser;
+        return parser;
+    }
+    
+    public IndexQueryParserService indexQueryParserService() {
+        return indexQueryParser;
     }
 
-    public void parseFieldMatcher(ParseFieldMatcher parseFieldMatcher) {
-        this.parseFieldMatcher = parseFieldMatcher;
+    public AnalysisService analysisService() {
+        return indexQueryParser.analysisService;
     }
 
-    public boolean isDeprecatedSetting(String setting) {
-        return parseFieldMatcher.match(setting, CACHE) || parseFieldMatcher.match(setting, CACHE_KEY);
+    public ScriptService scriptService() {
+        return indexQueryParser.scriptService;
     }
 
-    public Index index() {
-        return this.index;
+    public MapperService mapperService() {
+        return indexQueryParser.mapperService;
     }
 
-    /**
-     * @deprecated replaced by calls to parseInnerFilterToQueryBuilder() for the resulting queries
-     */
     @Nullable
-    @Deprecated
-    //norelease should be possible to remove after refactoring all queries
-    public Query parseInnerFilter() throws QueryShardException, IOException {
-        assert this.shardContext != null;
-        QueryBuilder builder = parseInnerFilterToQueryBuilder();
-        Query result = null;
-        if (builder != null) {
-            result = builder.toQuery(this.shardContext);
+    public SimilarityService similarityService() {
+        return indexQueryParser.similarityService;
+    }
+
+    public Similarity searchSimilarity() {
+        return indexQueryParser.similarityService != null ? indexQueryParser.similarityService.similarity() : null;
+    }
+
+    public String defaultField() {
+        return indexQueryParser.defaultField();
+    }
+
+    public boolean queryStringLenient() {
+        return indexQueryParser.queryStringLenient();
+    }
+
+    public MapperQueryParser queryParser(QueryParserSettings settings) {
+        queryParser.reset(settings);
+        return queryParser;
+    }
+
+    public BitDocIdSetFilter bitsetFilter(Filter filter) {
+        return indexQueryParser.bitsetFilterCache.getBitDocIdSetFilter(filter);
+    }
+
+    public <IFD extends IndexFieldData<?>> IFD getForField(MappedFieldType mapper) {
+        return indexQueryParser.fieldDataService.getForField(mapper);
+    }
+
+    public void addNamedQuery(String name, Query query) {
+        if (query != null) {
+            namedQueries.put(name, query);
         }
-        return result;
+    }
+
+    public ImmutableMap<String, Query> copyNamedQueries() {
+        return ImmutableMap.copyOf(namedQueries);
+    }
+
+    public void combineNamedQueries(QueryParseContext context) {
+        namedQueries.putAll(context.namedQueries);
     }
 
     /**
-     * @deprecated replaced by calls to parseInnerQueryBuilder() for the resulting queries
+     * Return whether we are currently parsing a filter or a query.
      */
-    @Nullable
-    @Deprecated
-    //norelease this method will be removed once all queries are refactored
-    public Query parseInnerQuery() throws IOException, QueryShardException {
-        QueryBuilder builder = parseInnerQueryBuilder();
-        Query result = null;
-        if (builder != null) {
-            result = builder.toQuery(this.shardContext);
+    public boolean isFilter() {
+        return isFilter;
+    }
+
+    public void addInnerHits(String name, InnerHitsContext.BaseInnerHits context) {
+        SearchContext sc = SearchContext.current();
+        if (sc == null) {
+            throw new QueryParsingException(this, "inner_hits unsupported");
         }
-        return result;
+
+        InnerHitsContext innerHitsContext;
+        if (sc.innerHits() == null) {
+            innerHitsContext = new InnerHitsContext(new HashMap<String, InnerHitsContext.BaseInnerHits>());
+            sc.innerHits(innerHitsContext);
+        } else {
+            innerHitsContext = sc.innerHits();
+        }
+        innerHitsContext.addInnerHitDefinition(name, context);
     }
 
-    /**
-     * @return a new QueryBuilder based on the current state of the parser
-     * @throws IOException
-     */
-    public QueryBuilder parseInnerQueryBuilder() throws IOException {
+    @Nullable
+    public Query parseInnerQuery() throws QueryParsingException, IOException {
         // move to START object
         XContentParser.Token token;
         if (parser.currentToken() != XContentParser.Token.START_OBJECT) {
@@ -129,7 +231,7 @@ public class QueryParseContext {
         token = parser.nextToken();
         if (token == XContentParser.Token.END_OBJECT) {
             // empty query
-            return EmptyQueryBuilder.PROTOTYPE;
+            return null;
         }
         if (token != XContentParser.Token.FIELD_NAME) {
             throw new QueryParsingException(this, "[_na] query malformed, no field after start_object");
@@ -141,11 +243,11 @@ public class QueryParseContext {
             throw new QueryParsingException(this, "[_na] query malformed, no field after start_object");
         }
 
-        QueryParser queryParser = queryParser(queryName);
+        QueryParser queryParser = indexQueryParser.queryParser(queryName);
         if (queryParser == null) {
             throw new QueryParsingException(this, "No query registered for [" + queryName + "]");
         }
-        QueryBuilder result = queryParser.fromXContent(this);
+        Query result = queryParser.parse(this);
         if (parser.currentToken() == XContentParser.Token.END_OBJECT || parser.currentToken() == XContentParser.Token.END_ARRAY) {
             // if we are at END_OBJECT, move to the next one...
             parser.nextToken();
@@ -153,46 +255,138 @@ public class QueryParseContext {
         return result;
     }
 
-    /**
-     * @return a new QueryBuilder based on the current state of the parser, but does so that the inner query
-     * is parsed to a filter
-     * @throws IOException
-     */
-    //norelease setting and checking the isFilter Flag should completely be moved to toQuery/toFilter after query refactoring
-    public QueryBuilder parseInnerFilterToQueryBuilder() throws IOException {
-        final boolean originalIsFilter = this.shardContext.isFilter;
+    @Nullable
+    public Query parseInnerFilter() throws QueryParsingException, IOException {
+        final boolean originalIsFilter = isFilter;
         try {
-            this.shardContext.isFilter = true;
-            return parseInnerQueryBuilder();
+            isFilter = true;
+            return parseInnerQuery();
         } finally {
-            this.shardContext.isFilter = originalIsFilter;
+            isFilter = originalIsFilter;
         }
     }
 
-    //norelease setting and checking the isFilter Flag should completely be moved to toQuery/toFilter after query refactoring
-    public QueryBuilder parseInnerFilterToQueryBuilder(String queryName) throws IOException, QueryParsingException {
-        final boolean originalIsFilter = this.shardContext.isFilter;
+    public Query parseInnerFilter(String queryName) throws IOException, QueryParsingException {
+        final boolean originalIsFilter = isFilter;
         try {
-            this.shardContext.isFilter = true;
-            QueryParser queryParser = queryParser(queryName);
+            isFilter = true;
+            QueryParser queryParser = indexQueryParser.queryParser(queryName);
             if (queryParser == null) {
                 throw new QueryParsingException(this, "No query registered for [" + queryName + "]");
             }
-            return queryParser.fromXContent(this);
+            return queryParser.parse(this);
         } finally {
-            this.shardContext.isFilter = originalIsFilter;
+            isFilter = originalIsFilter;
         }
     }
 
-    public ParseFieldMatcher parseFieldMatcher() {
-        return parseFieldMatcher;
+    public Collection<String> simpleMatchToIndexNames(String pattern) {
+        return indexQueryParser.mapperService.simpleMatchToIndexNames(pattern, getTypes());
+    }
+
+    public MappedFieldType fieldMapper(String name) {
+        return failIfFieldMappingNotFound(name, indexQueryParser.mapperService.smartNameFieldType(name, getTypes()));
+    }
+
+    public ObjectMapper getObjectMapper(String name) {
+        return indexQueryParser.mapperService.getObjectMapper(name, getTypes());
+    }
+
+    /** Gets the search analyzer for the given field, or the default if there is none present for the field
+     * TODO: remove this by moving defaults into mappers themselves
+     */
+    public Analyzer getSearchAnalyzer(MappedFieldType fieldType) {
+        if (fieldType.searchAnalyzer() != null) {
+            return fieldType.searchAnalyzer();
+        }
+        return mapperService().searchAnalyzer();
     }
 
-    public void parser(XContentParser innerParser) {
-        this.parser = innerParser;
+    /** Gets the search quote nalyzer for the given field, or the default if there is none present for the field
+     * TODO: remove this by moving defaults into mappers themselves
+     */
+    public Analyzer getSearchQuoteAnalyzer(MappedFieldType fieldType) {
+        if (fieldType.searchQuoteAnalyzer() != null) {
+            return fieldType.searchQuoteAnalyzer();
+        }
+        return mapperService().searchQuoteAnalyzer();
     }
 
-    QueryParser queryParser(String name) {
-        return indicesQueriesRegistry.queryParsers().get(name);
+    public void setAllowUnmappedFields(boolean allowUnmappedFields) {
+        this.allowUnmappedFields = allowUnmappedFields;
     }
+
+    public void setMapUnmappedFieldAsString(boolean mapUnmappedFieldAsString) {
+        this.mapUnmappedFieldAsString = mapUnmappedFieldAsString;
+    }
+
+    private MappedFieldType failIfFieldMappingNotFound(String name, MappedFieldType fieldMapping) {
+        if (allowUnmappedFields) {
+            return fieldMapping;
+        } else if (mapUnmappedFieldAsString){
+            StringFieldMapper.Builder builder = MapperBuilders.stringField(name);
+            // it would be better to pass the real index settings, but they are not easily accessible from here...
+            Settings settings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, indexQueryParser.getIndexCreatedVersion()).build();
+            return builder.build(new Mapper.BuilderContext(settings, new ContentPath(1))).fieldType();
+        } else {
+            Version indexCreatedVersion = indexQueryParser.getIndexCreatedVersion();
+            if (fieldMapping == null && indexCreatedVersion.onOrAfter(Version.V_1_4_0_Beta1)) {
+                throw new QueryParsingException(this, "Strict field resolution and no field mapping can be found for the field with name ["
+                        + name + "]");
+            } else {
+                return fieldMapping;
+            }
+        }
+    }
+
+    /**
+     * Returns the narrowed down explicit types, or, if not set, all types.
+     */
+    public Collection<String> queryTypes() {
+        String[] types = getTypes();
+        if (types == null || types.length == 0) {
+            return mapperService().types();
+        }
+        if (types.length == 1 && types[0].equals("_all")) {
+            return mapperService().types();
+        }
+        return Arrays.asList(types);
+    }
+
+    private SearchLookup lookup = null;
+
+    public SearchLookup lookup() {
+        SearchContext current = SearchContext.current();
+        if (current != null) {
+            return current.lookup();
+        }
+        if (lookup == null) {
+            lookup = new SearchLookup(mapperService(), indexQueryParser.fieldDataService, null);
+        }
+        return lookup;
+    }
+
+    public long nowInMillis() {
+        SearchContext current = SearchContext.current();
+        if (current != null) {
+            return current.nowInMillis();
+        }
+        return System.currentTimeMillis();
+    }
+
+    public NestedScope nestedScope() {
+        return nestedScope;
+    }
+
+    /**
+     * Return whether the setting is deprecated.
+     */
+    public boolean isDeprecatedSetting(String setting) {
+        return parseFieldMatcher.match(setting, CACHE) || parseFieldMatcher.match(setting, CACHE_KEY);
+    }
+
+    public Version indexVersionCreated() {
+        return indexVersionCreated;
+    }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryParser.java b/core/src/main/java/org/elasticsearch/index/query/QueryParser.java
index d54971b..eff585a 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryParser.java
@@ -25,10 +25,9 @@ import org.elasticsearch.common.Nullable;
 import java.io.IOException;
 
 /**
- * Defines a query parser that is able to read and parse a query object in {@link org.elasticsearch.common.xcontent.XContent}
- * format and create an internal object representing the query, implementing {@link QueryBuilder}, which can be streamed to other nodes.
+ *
  */
-public interface QueryParser<QB extends QueryBuilder<QB>> {
+public interface QueryParser {
 
     /**
      * The names this query parser is registered under.
@@ -36,33 +35,11 @@ public interface QueryParser<QB extends QueryBuilder<QB>> {
     String[] names();
 
     /**
-     * Parses the into a query from the current parser location. Will be at
-     * "START_OBJECT" location, and should end when the token is at the matching
-     * "END_OBJECT".
+     * Parses the into a query from the current parser location. Will be at "START_OBJECT" location,
+     * and should end when the token is at the matching "END_OBJECT".
      * <p/>
-     * Returns <tt>null</tt> if this query should be ignored in the context of
-     * the DSL.
+     * Returns <tt>null</tt> if this query should be ignored in the context of the DSL.
      */
-    //norelease can be removed in favour of fromXContent once search requests can be parsed on the coordinating node
     @Nullable
-    Query parse(QueryShardContext context) throws IOException, QueryParsingException;
-
-    /**
-     * Creates a new {@link QueryBuilder} from the query held by the {@link QueryShardContext}
-     * in {@link org.elasticsearch.common.xcontent.XContent} format
-     *
-     * @param parseContext
-     *            the input parse context. The state on the parser contained in
-     *            this context will be changed as a side effect of this method
-     *            call
-     * @return the new QueryBuilder
-     * @throws IOException
-     * @throws QueryParsingException
-     */
-    QB fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException;
-
-    /**
-     * @return an empty {@link QueryBuilder} instance for this parser that can be used for deserialization
-     */
-    QB getBuilderPrototype();
+    Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException;
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryParsingException.java b/core/src/main/java/org/elasticsearch/index/query/QueryParsingException.java
index 80acae7..c606953 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryParsingException.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryParsingException.java
@@ -31,8 +31,7 @@ import org.elasticsearch.rest.RestStatus;
 import java.io.IOException;
 
 /**
- * Exception that can be used when parsing queries with a given {@link QueryParseContext}.
- * Can contain information about location of the error.
+ *
  */
 public class QueryParsingException extends ElasticsearchException {
 
@@ -72,15 +71,9 @@ public class QueryParsingException extends ElasticsearchException {
         this.columnNumber = col;
     }
 
-    public QueryParsingException(StreamInput in) throws IOException{
-        super(in);
-        lineNumber = in.readInt();
-        columnNumber = in.readInt();
-    }
-
     /**
      * Line number of the location of the error
-     *
+     * 
      * @return the line number or -1 if unknown
      */
     public int getLineNumber() {
@@ -89,7 +82,7 @@ public class QueryParsingException extends ElasticsearchException {
 
     /**
      * Column number of the location of the error
-     *
+     * 
      * @return the column number or -1 if unknown
      */
     public int getColumnNumber() {
@@ -116,4 +109,11 @@ public class QueryParsingException extends ElasticsearchException {
         out.writeInt(lineNumber);
         out.writeInt(columnNumber);
     }
+
+    public QueryParsingException(StreamInput in) throws IOException{
+        super(in);
+        lineNumber = in.readInt();
+        columnNumber = in.readInt();
+    }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java b/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java
deleted file mode 100644
index 5723ec6..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java
+++ /dev/null
@@ -1,326 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import com.google.common.collect.ImmutableMap;
-import com.google.common.collect.Maps;
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.queryparser.classic.MapperQueryParser;
-import org.apache.lucene.queryparser.classic.QueryParserSettings;
-import org.apache.lucene.search.Filter;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.join.BitDocIdSetFilter;
-import org.apache.lucene.search.similarities.Similarity;
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.analysis.AnalysisService;
-import org.elasticsearch.index.fielddata.IndexFieldData;
-import org.elasticsearch.index.mapper.*;
-import org.elasticsearch.index.mapper.core.StringFieldMapper;
-import org.elasticsearch.index.mapper.object.ObjectMapper;
-import org.elasticsearch.index.query.support.NestedScope;
-import org.elasticsearch.indices.cache.query.terms.TermsLookup;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsContext;
-import org.elasticsearch.search.internal.SearchContext;
-import org.elasticsearch.search.lookup.SearchLookup;
-
-import java.util.*;
-
-/**
- * Context object used to create lucene queries on the shard level.
- */
-public class QueryShardContext {
-
-    private static ThreadLocal<String[]> typesContext = new ThreadLocal<>();
-
-    public static void setTypes(String[] types) {
-        typesContext.set(types);
-    }
-
-    public static String[] getTypes() {
-        return typesContext.get();
-    }
-
-    public static String[] setTypesWithPrevious(String[] types) {
-        String[] old = typesContext.get();
-        setTypes(types);
-        return old;
-    }
-
-    public static void removeTypes() {
-        typesContext.remove();
-    }
-
-    private final Index index;
-
-    private final Version indexVersionCreated;
-
-    private final IndexQueryParserService indexQueryParser;
-
-    private final Map<String, Query> namedQueries = Maps.newHashMap();
-
-    private final MapperQueryParser queryParser = new MapperQueryParser(this);
-
-    private ParseFieldMatcher parseFieldMatcher;
-
-    private boolean allowUnmappedFields;
-
-    private boolean mapUnmappedFieldAsString;
-
-    private NestedScope nestedScope;
-
-    //norelease this should be possible to remove once query context are completely separated
-    private QueryParseContext parseContext;
-
-    boolean isFilter;
-
-    public QueryShardContext(Index index, IndexQueryParserService indexQueryParser) {
-        this.index = index;
-        this.indexVersionCreated = Version.indexCreated(indexQueryParser.indexSettings());
-        this.indexQueryParser = indexQueryParser;
-        this.parseContext = new QueryParseContext(this);
-    }
-
-    public void parseFieldMatcher(ParseFieldMatcher parseFieldMatcher) {
-        this.parseFieldMatcher = parseFieldMatcher;
-    }
-
-    public ParseFieldMatcher parseFieldMatcher() {
-        return parseFieldMatcher;
-    }
-
-    private void reset() {
-        allowUnmappedFields = indexQueryParser.defaultAllowUnmappedFields();
-        this.parseFieldMatcher = ParseFieldMatcher.EMPTY;
-        this.lookup = null;
-        this.namedQueries.clear();
-        this.nestedScope = new NestedScope();
-    }
-
-    //norelease remove parser argument once query contexts are separated
-    public void reset(XContentParser jp) {
-        this.reset();
-        this.parseContext.reset(jp);
-    }
-
-    public Index index() {
-        return this.index;
-    }
-
-    //norelease we might be able to avoid exposing the service to the outside world once all queries are refactored
-    public IndexQueryParserService indexQueryParserService() {
-        return indexQueryParser;
-    }
-
-    public AnalysisService analysisService() {
-        return indexQueryParser.analysisService;
-    }
-
-    public ScriptService scriptService() {
-        return indexQueryParser.scriptService;
-    }
-
-    public MapperService mapperService() {
-        return indexQueryParser.mapperService;
-    }
-
-    public Similarity searchSimilarity() {
-        return indexQueryParser.similarityService != null ? indexQueryParser.similarityService.similarity() : null;
-    }
-
-    public String defaultField() {
-        return indexQueryParser.defaultField();
-    }
-
-    public boolean queryStringLenient() {
-        return indexQueryParser.queryStringLenient();
-    }
-
-    public MapperQueryParser queryParser(QueryParserSettings settings) {
-        queryParser.reset(settings);
-        return queryParser;
-    }
-
-    public BitDocIdSetFilter bitsetFilter(Filter filter) {
-        return indexQueryParser.bitsetFilterCache.getBitDocIdSetFilter(filter);
-    }
-
-    public <IFD extends IndexFieldData<?>> IFD getForField(MappedFieldType mapper) {
-        return indexQueryParser.fieldDataService.getForField(mapper);
-    }
-
-    public void addNamedQuery(String name, Query query) {
-        if (query != null) {
-            namedQueries.put(name, query);
-        }
-    }
-
-    public ImmutableMap<String, Query> copyNamedQueries() {
-        return ImmutableMap.copyOf(namedQueries);
-    }
-
-    public void combineNamedQueries(QueryShardContext context) {
-        namedQueries.putAll(context.namedQueries);
-    }
-
-    /**
-     * Return whether we are currently parsing a filter or a query.
-     */
-    public boolean isFilter() {
-        return isFilter;
-    }
-
-    public void addInnerHits(String name, InnerHitsContext.BaseInnerHits context) {
-        SearchContext sc = SearchContext.current();
-        if (sc == null) {
-            throw new QueryShardException(this, "inner_hits unsupported");
-        }
-
-        InnerHitsContext innerHitsContext;
-        if (sc.innerHits() == null) {
-            innerHitsContext = new InnerHitsContext(new HashMap<String, InnerHitsContext.BaseInnerHits>());
-            sc.innerHits(innerHitsContext);
-        } else {
-            innerHitsContext = sc.innerHits();
-        }
-        innerHitsContext.addInnerHitDefinition(name, context);
-    }
-
-    public Collection<String> simpleMatchToIndexNames(String pattern) {
-        return indexQueryParser.mapperService.simpleMatchToIndexNames(pattern);
-    }
-
-    public MappedFieldType fieldMapper(String name) {
-        return failIfFieldMappingNotFound(name, indexQueryParser.mapperService.smartNameFieldType(name, getTypes()));
-    }
-
-    public ObjectMapper getObjectMapper(String name) {
-        return indexQueryParser.mapperService.getObjectMapper(name, getTypes());
-    }
-
-    /**
-     * Gets the search analyzer for the given field, or the default if there is none present for the field
-     * TODO: remove this by moving defaults into mappers themselves
-     */
-    public Analyzer getSearchAnalyzer(MappedFieldType fieldType) {
-        if (fieldType.searchAnalyzer() != null) {
-            return fieldType.searchAnalyzer();
-        }
-        return mapperService().searchAnalyzer();
-    }
-
-    /**
-     * Gets the search quote analyzer for the given field, or the default if there is none present for the field
-     * TODO: remove this by moving defaults into mappers themselves
-     */
-    public Analyzer getSearchQuoteAnalyzer(MappedFieldType fieldType) {
-        if (fieldType.searchQuoteAnalyzer() != null) {
-            return fieldType.searchQuoteAnalyzer();
-        }
-        return mapperService().searchQuoteAnalyzer();
-    }
-
-    public void setAllowUnmappedFields(boolean allowUnmappedFields) {
-        this.allowUnmappedFields = allowUnmappedFields;
-    }
-
-    public void setMapUnmappedFieldAsString(boolean mapUnmappedFieldAsString) {
-        this.mapUnmappedFieldAsString = mapUnmappedFieldAsString;
-    }
-
-    private MappedFieldType failIfFieldMappingNotFound(String name, MappedFieldType fieldMapping) {
-        if (allowUnmappedFields) {
-            return fieldMapping;
-        } else if (mapUnmappedFieldAsString) {
-            StringFieldMapper.Builder builder = MapperBuilders.stringField(name);
-            // it would be better to pass the real index settings, but they are not easily accessible from here...
-            Settings settings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, indexQueryParser.getIndexCreatedVersion()).build();
-            return builder.build(new Mapper.BuilderContext(settings, new ContentPath(1))).fieldType();
-        } else {
-            Version indexCreatedVersion = indexQueryParser.getIndexCreatedVersion();
-            if (fieldMapping == null && indexCreatedVersion.onOrAfter(Version.V_1_4_0_Beta1)) {
-                throw new QueryShardException(this, "Strict field resolution and no field mapping can be found for the field with name ["
-                        + name + "]");
-            } else {
-                return fieldMapping;
-            }
-        }
-    }
-
-    /**
-     * Returns the narrowed down explicit types, or, if not set, all types.
-     */
-    public Collection<String> queryTypes() {
-        String[] types = getTypes();
-        if (types == null || types.length == 0) {
-            return mapperService().types();
-        }
-        if (types.length == 1 && types[0].equals("_all")) {
-            return mapperService().types();
-        }
-        return Arrays.asList(types);
-    }
-
-    private SearchLookup lookup = null;
-
-    public SearchLookup lookup() {
-        SearchContext current = SearchContext.current();
-        if (current != null) {
-            return current.lookup();
-        }
-        if (lookup == null) {
-            lookup = new SearchLookup(mapperService(), indexQueryParser.fieldDataService, null);
-        }
-        return lookup;
-    }
-
-    public long nowInMillis() {
-        SearchContext current = SearchContext.current();
-        if (current != null) {
-            return current.nowInMillis();
-        }
-        return System.currentTimeMillis();
-    }
-
-    public NestedScope nestedScope() {
-        return nestedScope;
-    }
-
-    public Version indexVersionCreated() {
-        return indexVersionCreated;
-    }
-
-    public QueryParseContext parseContext() {
-        return this.parseContext;
-    }
-
-    public boolean matchesIndices(String... indices) {
-        return this.indexQueryParser.matchesIndices(indices);
-    }
-
-    public List<Object> handleTermsLookup(TermsLookup termsLookup) {
-        return this.indexQueryParser.handleTermsLookup(termsLookup);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryShardException.java b/core/src/main/java/org/elasticsearch/index/query/QueryShardException.java
deleted file mode 100644
index 1e31c7c..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/QueryShardException.java
+++ /dev/null
@@ -1,72 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.rest.RestStatus;
-
-import java.io.IOException;
-
-/**
- * Exception that is thrown when creating lucene queries on the shard
- */
-public class QueryShardException extends ElasticsearchException {
-
-    public QueryShardException(QueryShardContext context, String msg, Object... args) {
-        this(context, msg, null, args);
-    }
-
-    public QueryShardException(QueryShardContext context, String msg, Throwable cause, Object... args) {
-        super(msg, cause, args);
-        setIndex(context.index());
-    }
-
-    /**
-     * This constructor is provided for use in unit tests where a
-     * {@link QueryShardContext} may not be available
-     */
-    public QueryShardException(Index index, String msg, Throwable cause) {
-        super(msg, cause);
-        setIndex(index);
-    }
-
-    public QueryShardException(StreamInput in) throws IOException{
-        super(in);
-    }
-
-    @Override
-    public RestStatus status() {
-        return RestStatus.BAD_REQUEST;
-    }
-
-    @Override
-    protected void innerToXContent(XContentBuilder builder, Params params) throws IOException {
-        super.innerToXContent(builder, params);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java
index a6efa2f..c7a297e 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java
@@ -36,9 +36,12 @@ import java.util.Locale;
  * them either using DisMax or a plain boolean query (see {@link #useDisMax(boolean)}).
  * <p/>
  */
-public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQueryBuilder> {
+public class QueryStringQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<QueryStringQueryBuilder> {
 
-    public static final String NAME = "query_string";
+    public enum Operator {
+        OR,
+        AND
+    }
 
     private final String queryString;
 
@@ -63,6 +66,8 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
 
     private Locale locale;
 
+    private float boost = -1;
+
     private Fuzziness fuzziness;
     private int fuzzyPrefixLength = -1;
     private int fuzzyMaxExpansions = -1;
@@ -84,14 +89,14 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
 
     private Boolean lenient;
 
-    private String timeZone;
+    private String queryName;
 
-    private Boolean escape;
+    private String timeZone;
 
     /** To limit effort spent determinizing regexp queries. */
     private Integer maxDeterminizedStates;
 
-    static final QueryStringQueryBuilder PROTOTYPE = new QueryStringQueryBuilder(null);
+    private Boolean escape;
 
     public QueryStringQueryBuilder(String queryString) {
         this.queryString = queryString;
@@ -289,6 +294,16 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
     }
 
     /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
+    @Override
+    public QueryStringQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
+    /**
      * An optional field name suffix to automatically try and add to the field searched when using quoted text.
      */
     public QueryStringQueryBuilder quoteFieldSuffix(String quoteFieldSuffix) {
@@ -305,6 +320,14 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
         return this;
     }
 
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public QueryStringQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
     public QueryStringQueryBuilder locale(Locale locale) {
         this.locale = locale;
         return this;
@@ -328,7 +351,7 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(QueryStringQueryParser.NAME);
         builder.field("query", queryString);
         if (defaultField != null) {
             builder.field("default_field", defaultField);
@@ -376,6 +399,9 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
         if (fuzziness != null) {
             fuzziness.toXContent(builder, params);
         }
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
         if (fuzzyPrefixLength != -1) {
             builder.field("fuzzy_prefix_length", fuzzyPrefixLength);
         }
@@ -403,6 +429,9 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
         if (lenient != null) {
             builder.field("lenient", lenient);
         }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         if (locale != null) {
             builder.field("locale", locale.toString());
         }
@@ -412,12 +441,6 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
         if (escape != null) {
             builder.field("escape", escape);
         }
-        printBoostAndQueryName(builder);
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java
index dcca133..64afdd2 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java
@@ -46,8 +46,9 @@ import static org.elasticsearch.common.lucene.search.Queries.fixNegativeQueryIfN
 /**
  *
  */
-public class QueryStringQueryParser extends BaseQueryParserTemp {
+public class QueryStringQueryParser implements QueryParser {
 
+    public static final String NAME = "query_string";
     private static final ParseField FUZZINESS = Fuzziness.FIELD.withDeprecation("fuzzy_min_sim");
 
     private final boolean defaultAnalyzeWildcard;
@@ -61,18 +62,17 @@ public class QueryStringQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{QueryStringQueryBuilder.NAME, Strings.toCamelCase(QueryStringQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String queryName = null;
         QueryParserSettings qpSettings = new QueryParserSettings();
-        qpSettings.defaultField(context.defaultField());
-        qpSettings.lenient(context.queryStringLenient());
+        qpSettings.defaultField(parseContext.defaultField());
+        qpSettings.lenient(parseContext.queryStringLenient());
         qpSettings.analyzeWildcard(defaultAnalyzeWildcard);
         qpSettings.allowLeadingWildcard(defaultAllowLeadingWildcard);
         qpSettings.locale(Locale.ROOT);
@@ -105,7 +105,7 @@ public class QueryStringQueryParser extends BaseQueryParserTemp {
                         }
 
                         if (Regex.isSimpleMatchPattern(fField)) {
-                            for (String field : context.mapperService().simpleMatchToIndexNames(fField)) {
+                            for (String field : parseContext.mapperService().simpleMatchToIndexNames(fField)) {
                                 qpSettings.fields().add(field);
                                 if (fBoost != -1) {
                                     if (qpSettings.boosts() == null) {
@@ -143,13 +143,13 @@ public class QueryStringQueryParser extends BaseQueryParserTemp {
                         throw new QueryParsingException(parseContext, "Query default operator [" + op + "] is not allowed");
                     }
                 } else if ("analyzer".equals(currentFieldName)) {
-                    NamedAnalyzer analyzer = context.analysisService().analyzer(parser.text());
+                    NamedAnalyzer analyzer = parseContext.analysisService().analyzer(parser.text());
                     if (analyzer == null) {
                         throw new QueryParsingException(parseContext, "[query_string] analyzer [" + parser.text() + "] not found");
                     }
                     qpSettings.forcedAnalyzer(analyzer);
                 } else if ("quote_analyzer".equals(currentFieldName) || "quoteAnalyzer".equals(currentFieldName)) {
-                    NamedAnalyzer analyzer = context.analysisService().analyzer(parser.text());
+                    NamedAnalyzer analyzer = parseContext.analysisService().analyzer(parser.text());
                     if (analyzer == null) {
                         throw new QueryParsingException(parseContext, "[query_string] quote_analyzer [" + parser.text()
                                 + "] not found");
@@ -214,14 +214,14 @@ public class QueryStringQueryParser extends BaseQueryParserTemp {
         if (qpSettings.queryString() == null) {
             throw new QueryParsingException(parseContext, "query_string must be provided with a [query]");
         }
-        qpSettings.defaultAnalyzer(context.mapperService().searchAnalyzer());
-        qpSettings.defaultQuoteAnalyzer(context.mapperService().searchQuoteAnalyzer());
+        qpSettings.defaultAnalyzer(parseContext.mapperService().searchAnalyzer());
+        qpSettings.defaultQuoteAnalyzer(parseContext.mapperService().searchQuoteAnalyzer());
 
         if (qpSettings.escape()) {
             qpSettings.queryString(org.apache.lucene.queryparser.classic.QueryParser.escape(qpSettings.queryString()));
         }
 
-        MapperQueryParser queryParser = context.queryParser(qpSettings);
+        MapperQueryParser queryParser = parseContext.queryParser(qpSettings);
 
         try {
             Query query = queryParser.parse(qpSettings.queryString());
@@ -236,16 +236,11 @@ public class QueryStringQueryParser extends BaseQueryParserTemp {
                 query = Queries.applyMinimumShouldMatch((BooleanQuery) query, qpSettings.minimumShouldMatch());
             }
             if (queryName != null) {
-                context.addNamedQuery(queryName, query);
+                parseContext.addNamedQuery(queryName, query);
             }
             return query;
         } catch (org.apache.lucene.queryparser.classic.ParseException e) {
             throw new QueryParsingException(parseContext, "Failed to parse query [" + qpSettings.queryString() + "]", e);
         }
     }
-
-    @Override
-    public QueryStringQueryBuilder getBuilderPrototype() {
-        return QueryStringQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryValidationException.java b/core/src/main/java/org/elasticsearch/index/query/QueryValidationException.java
deleted file mode 100644
index 9e0ee2a..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/QueryValidationException.java
+++ /dev/null
@@ -1,62 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.elasticsearch.common.ValidationException;
-
-import java.util.List;
-
-/**
- * This exception can be used to indicate various reasons why validation of a query has failed.
- */
-public class QueryValidationException extends ValidationException {
-
-    /**
-     * Helper method than can be used to add error messages to an existing {@link QueryValidationException}.
-     * When passing {@code null} as the initial exception, a new exception is created.
-     *
-     * @param queryId the query that caused the error
-     * @param validationError the error message to add to an initial exception
-     * @param validationException an initial exception. Can be {@code null}, in which case a new exception is created.
-     * @return a {@link QueryValidationException} with added validation error message
-     */
-    public static QueryValidationException addValidationError(String queryId, String validationError, QueryValidationException validationException) {
-        if (validationException == null) {
-            validationException = new QueryValidationException();
-        }
-        validationException.addValidationError("[" + queryId + "] " + validationError);
-        return validationException;
-    }
-
-    /**
-     * Helper method than can be used to add error messages to an existing {@link QueryValidationException}.
-     * When passing {@code null} as the initial exception, a new exception is created.
-     * @param validationErrors the error messages to add to an initial exception
-     * @param validationException an initial exception. Can be {@code null}, in which case a new exception is created.
-     * @return a {@link QueryValidationException} with added validation error message
-     */
-    public static QueryValidationException addValidationErrors(List<String> validationErrors, QueryValidationException validationException) {
-        if (validationException == null) {
-            validationException = new QueryValidationException();
-        }
-        validationException.addValidationErrors(validationErrors);
-        return validationException;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryWrappingQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/QueryWrappingQueryBuilder.java
deleted file mode 100644
index d3be9da..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/QueryWrappingQueryBuilder.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-
-import java.io.IOException;
-
-/**
- * QueryBuilder implementation that  holds a lucene query, which can be returned by {@link QueryBuilder#toQuery(QueryShardContext)}.
- * Doesn't support conversion to {@link org.elasticsearch.common.xcontent.XContent} via {@link #doXContent(XContentBuilder, Params)}.
- */
-//norelease to be removed once all queries support separate fromXContent and toQuery methods. Make AbstractQueryBuilder#toQuery final as well then.
-public class QueryWrappingQueryBuilder extends AbstractQueryBuilder<QueryWrappingQueryBuilder> implements SpanQueryBuilder<QueryWrappingQueryBuilder>, MultiTermQueryBuilder<QueryWrappingQueryBuilder>{
-
-    private Query query;
-
-    public QueryWrappingQueryBuilder(Query query) {
-        this.query = query;
-        //hack to make sure that the boost from the wrapped query is used, otherwise it gets overwritten.
-        if (query != null) {
-            this.boost = query.getBoost();
-        }
-    }
-
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return query;
-    }
-
-    @Override
-    public String getWriteableName() {
-        // this should not be called since we overwrite BaseQueryBuilder#toQuery() in this class
-        throw new UnsupportedOperationException();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java
index 0db4152..da23698 100644
--- a/core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java
@@ -19,111 +19,187 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermRangeQuery;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.joda.DateMathParser;
-import org.elasticsearch.common.joda.Joda;
-import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.core.DateFieldMapper;
-import org.joda.time.DateTimeZone;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * A Query that matches documents within an range of terms.
  */
-public class RangeQueryBuilder extends AbstractQueryBuilder<RangeQueryBuilder> implements MultiTermQueryBuilder<RangeQueryBuilder> {
+public class RangeQueryBuilder extends MultiTermQueryBuilder implements BoostableQueryBuilder<RangeQueryBuilder> {
 
-    public static final boolean DEFAULT_INCLUDE_UPPER = true;
+    private final String name;
+    private Object from;
+    private Object to;
+    private String timeZone;
+    private boolean includeLower = true;
+    private boolean includeUpper = true;
+    private float boost = -1;
+    private String queryName;
+    private String format;
 
-    public static final boolean DEFAULT_INCLUDE_LOWER = true;
+    /**
+     * A Query that matches documents within an range of terms.
+     *
+     * @param name The field name
+     */
+    public RangeQueryBuilder(String name) {
+        this.name = name;
+    }
 
-    public static final String NAME = "range";
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder from(Object from) {
+        this.from = from;
+        return this;
+    }
 
-    private final String fieldName;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder from(String from) {
+        this.from = from;
+        return this;
+    }
 
-    private Object from;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder from(int from) {
+        this.from = from;
+        return this;
+    }
 
-    private Object to;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder from(long from) {
+        this.from = from;
+        return this;
+    }
 
-    private String timeZone;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder from(float from) {
+        this.from = from;
+        return this;
+    }
 
-    private boolean includeLower = DEFAULT_INCLUDE_LOWER;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder from(double from) {
+        this.from = from;
+        return this;
+    }
 
-    private boolean includeUpper = DEFAULT_INCLUDE_UPPER;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder gt(String from) {
+        this.from = from;
+        this.includeLower = false;
+        return this;
+    }
 
-    private String format;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder gt(Object from) {
+        this.from = from;
+        this.includeLower = false;
+        return this;
+    }
 
-    static final RangeQueryBuilder PROTOTYPE = new RangeQueryBuilder(null);
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder gt(int from) {
+        this.from = from;
+        this.includeLower = false;
+        return this;
+    }
 
     /**
-     * A Query that matches documents within an range of terms.
-     *
-     * @param fieldName The field name
+     * The from part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder(String fieldName) {
-        this.fieldName = fieldName;
+    public RangeQueryBuilder gt(long from) {
+        this.from = from;
+        this.includeLower = false;
+        return this;
     }
 
     /**
-     * Get the field name for this query.
+     * The from part of the range query. Null indicates unbounded.
      */
-    public String fieldName() {
-        return this.fieldName;
+    public RangeQueryBuilder gt(float from) {
+        this.from = from;
+        this.includeLower = false;
+        return this;
     }
 
     /**
      * The from part of the range query. Null indicates unbounded.
-     * In case lower bound is assigned to a string, we internally convert it to a {@link BytesRef} because
-     * in {@link RangeQueryParser} field are later parsed as {@link BytesRef} and we need internal representation
-     * of query to be equal regardless of whether it was created from XContent or via Java API.
      */
-    public RangeQueryBuilder from(Object from, boolean includeLower) {
-        this.from = convertToBytesRefIfString(from);
-        this.includeLower = includeLower;
+    public RangeQueryBuilder gt(double from) {
+        this.from = from;
+        this.includeLower = false;
         return this;
     }
 
     /**
      * The from part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder from(Object from) {
-        return from(from, this.includeLower);
+    public RangeQueryBuilder gte(String from) {
+        this.from = from;
+        this.includeLower = true;
+        return this;
     }
 
     /**
-     * Gets the lower range value for this query.
+     * The from part of the range query. Null indicates unbounded.
      */
-    public Object from() {
-        return convertToStringIfBytesRef(this.from);
+    public RangeQueryBuilder gte(Object from) {
+        this.from = from;
+        this.includeLower = true;
+        return this;
     }
 
     /**
      * The from part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder gt(Object from) {
-        return from(from, false);
+    public RangeQueryBuilder gte(int from) {
+        this.from = from;
+        this.includeLower = true;
+        return this;
     }
 
     /**
      * The from part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder gte(Object from) {
-        return from(from, true);
+    public RangeQueryBuilder gte(long from) {
+        this.from = from;
+        this.includeLower = true;
+        return this;
     }
 
     /**
-     * The to part of the range query. Null indicates unbounded.
+     * The from part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder to(Object to, boolean includeUpper) {
-        this.to = convertToBytesRefIfString(to);
-        this.includeUpper = includeUpper;
+    public RangeQueryBuilder gte(float from) {
+        this.from = from;
+        this.includeLower = true;
+        return this;
+    }
+
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder gte(double from) {
+        this.from = from;
+        this.includeLower = true;
         return this;
     }
 
@@ -131,214 +207,229 @@ public class RangeQueryBuilder extends AbstractQueryBuilder<RangeQueryBuilder> i
      * The to part of the range query. Null indicates unbounded.
      */
     public RangeQueryBuilder to(Object to) {
-        return to(to, this.includeUpper);
+        this.to = to;
+        return this;
     }
 
     /**
-     * Gets the upper range value for this query.
-     * In case upper bound is assigned to a string, we internally convert it to a {@link BytesRef} because
-     * in {@link RangeQueryParser} field are later parsed as {@link BytesRef} and we need internal representation
-     * of query to be equal regardless of whether it was created from XContent or via Java API.
+     * The to part of the range query. Null indicates unbounded.
      */
-    public Object to() {
-        return convertToStringIfBytesRef(this.to);
+    public RangeQueryBuilder to(String to) {
+        this.to = to;
+        return this;
     }
 
     /**
      * The to part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder lt(Object to) {
-        return to(to, false);
+    public RangeQueryBuilder to(int to) {
+        this.to = to;
+        return this;
     }
 
     /**
      * The to part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder lte(Object to) {
-        return to(to, true);
+    public RangeQueryBuilder to(long to) {
+        this.to = to;
+        return this;
     }
 
     /**
-     * Should the lower bound be included or not. Defaults to <tt>true</tt>.
+     * The to part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder includeLower(boolean includeLower) {
-        this.includeLower = includeLower;
+    public RangeQueryBuilder to(float to) {
+        this.to = to;
         return this;
     }
 
     /**
-     * Gets the includeLower flag for this query.
+     * The to part of the range query. Null indicates unbounded.
      */
-    public boolean includeLower() {
-        return this.includeLower;
+    public RangeQueryBuilder to(double to) {
+        this.to = to;
+        return this;
     }
 
     /**
-     * Should the upper bound be included or not. Defaults to <tt>true</tt>.
+     * The to part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder includeUpper(boolean includeUpper) {
-        this.includeUpper = includeUpper;
+    public RangeQueryBuilder lt(String to) {
+        this.to = to;
+        this.includeUpper = false;
         return this;
     }
 
     /**
-     * Gets the includeUpper flag for this query.
+     * The to part of the range query. Null indicates unbounded.
      */
-    public boolean includeUpper() {
-        return this.includeUpper;
+    public RangeQueryBuilder lt(Object to) {
+        this.to = to;
+        this.includeUpper = false;
+        return this;
     }
 
     /**
-     * In case of date field, we can adjust the from/to fields using a timezone
+     * The to part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder timeZone(String timezone) {
-        this.timeZone = timezone;
+    public RangeQueryBuilder lt(int to) {
+        this.to = to;
+        this.includeUpper = false;
         return this;
     }
 
     /**
-     * In case of date field, gets the from/to fields timezone adjustment
+     * The to part of the range query. Null indicates unbounded.
      */
-    public String timeZone() {
-        return this.timeZone;
+    public RangeQueryBuilder lt(long to) {
+        this.to = to;
+        this.includeUpper = false;
+        return this;
     }
 
     /**
-     * In case of format field, we can parse the from/to fields using this time format
+     * The to part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder format(String format) {
-        this.format = format;
+    public RangeQueryBuilder lt(float to) {
+        this.to = to;
+        this.includeUpper = false;
         return this;
     }
 
     /**
-     * Gets the format field to parse the from/to fields
+     * The to part of the range query. Null indicates unbounded.
      */
-    public String format() {
-        return this.format;
+    public RangeQueryBuilder lt(double to) {
+        this.to = to;
+        this.includeUpper = false;
+        return this;
     }
 
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.startObject(fieldName);
-        builder.field("from", convertToStringIfBytesRef(this.from));
-        builder.field("to", convertToStringIfBytesRef(this.to));
-        builder.field("include_lower", includeLower);
-        builder.field("include_upper", includeUpper);
-        if (timeZone != null) {
-            builder.field("time_zone", timeZone);
-        }
-        if (format != null) {
-            builder.field("format", format);
-        }
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
+    /**
+     * The to part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder lte(String to) {
+        this.to = to;
+        this.includeUpper = true;
+        return this;
     }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
+    /**
+     * The to part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder lte(Object to) {
+        this.to = to;
+        this.includeUpper = true;
+        return this;
     }
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query query = null;
-        MappedFieldType mapper = context.fieldMapper(this.fieldName);
-        if (mapper != null) {
-            if (mapper instanceof DateFieldMapper.DateFieldType) {
-                DateMathParser forcedDateParser = null;
-                if (this.format  != null) {
-                    forcedDateParser = new DateMathParser(Joda.forPattern(this.format));
-                }
-                DateTimeZone dateTimeZone = null;
-                if (this.timeZone != null) {
-                    dateTimeZone = DateTimeZone.forID(this.timeZone);
-                }
-                query = ((DateFieldMapper.DateFieldType) mapper).rangeQuery(from, to, includeLower, includeUpper, dateTimeZone, forcedDateParser);
-            } else  {
-                if (timeZone != null) {
-                    throw new QueryShardException(context, "[range] time_zone can not be applied to non date field ["
-                            + fieldName + "]");
-                }
-                //LUCENE 4 UPGRADE Mapper#rangeQuery should use bytesref as well?
-                query = mapper.rangeQuery(from, to, includeLower, includeUpper);
-            }
-        } else {
-            if (timeZone != null) {
-                throw new QueryShardException(context, "[range] time_zone can not be applied to non unmapped field ["
-                        + fieldName + "]");
-            }
-        }
+    /**
+     * The to part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder lte(int to) {
+        this.to = to;
+        this.includeUpper = true;
+        return this;
+    }
 
-        if (query == null) {
-            query = new TermRangeQuery(this.fieldName, BytesRefs.toBytesRef(from), BytesRefs.toBytesRef(to), includeLower, includeUpper);
-        }
-        return query;
+    /**
+     * The to part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder lte(long to) {
+        this.to = to;
+        this.includeUpper = true;
+        return this;
     }
 
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (this.fieldName == null || this.fieldName.isEmpty()) {
-            validationException = addValidationError("field name cannot be null or empty.", validationException);
-        }
-        if (this.timeZone != null) {
-            try {
-                DateTimeZone.forID(this.timeZone);
-            } catch (Exception e) {
-                validationException = addValidationError("error parsing timezone." + e.getMessage(),
-                        validationException);
-            }
-        }
-        if (this.format != null) {
-            try {
-                Joda.forPattern(this.format);
-            } catch (Exception e) {
-                validationException = addValidationError("error parsing format." + e.getMessage(),
-                        validationException);
-            }
-        }
-        return validationException;
+    /**
+     * The to part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder lte(float to) {
+        this.to = to;
+        this.includeUpper = true;
+        return this;
     }
 
-    @Override
-    protected RangeQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        RangeQueryBuilder rangeQueryBuilder = new RangeQueryBuilder(in.readString());
-        rangeQueryBuilder.from = in.readGenericValue();
-        rangeQueryBuilder.to = in.readGenericValue();
-        rangeQueryBuilder.includeLower = in.readBoolean();
-        rangeQueryBuilder.includeUpper = in.readBoolean();
-        rangeQueryBuilder.timeZone = in.readOptionalString();
-        rangeQueryBuilder.format = in.readOptionalString();
-        return rangeQueryBuilder;
+    /**
+     * The to part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder lte(double to) {
+        this.to = to;
+        this.includeUpper = true;
+        return this;
     }
 
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(this.fieldName);
-        out.writeGenericValue(this.from);
-        out.writeGenericValue(this.to);
-        out.writeBoolean(this.includeLower);
-        out.writeBoolean(this.includeUpper);
-        out.writeOptionalString(this.timeZone);
-        out.writeOptionalString(this.format);
+    /**
+     * Should the lower bound be included or not. Defaults to <tt>true</tt>.
+     */
+    public RangeQueryBuilder includeLower(boolean includeLower) {
+        this.includeLower = includeLower;
+        return this;
     }
 
+    /**
+     * Should the upper bound be included or not. Defaults to <tt>true</tt>.
+     */
+    public RangeQueryBuilder includeUpper(boolean includeUpper) {
+        this.includeUpper = includeUpper;
+        return this;
+    }
+
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
     @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldName, from, to, timeZone, includeLower, includeUpper, format);
+    public RangeQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public RangeQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
+    /**
+     * In case of date field, we can adjust the from/to fields using a timezone
+     */
+    public RangeQueryBuilder timeZone(String timezone) {
+        this.timeZone = timezone;
+        return this;
+    }
+
+    /**
+     * In case of date field, we can set the format to be used instead of the mapper format
+     */
+    public RangeQueryBuilder format(String format) {
+        this.format = format;
+        return this;
     }
 
     @Override
-    protected boolean doEquals(RangeQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-               Objects.equals(from, other.from) &&
-               Objects.equals(to, other.to) &&
-               Objects.equals(timeZone, other.timeZone) &&
-               Objects.equals(includeLower, other.includeLower) &&
-               Objects.equals(includeUpper, other.includeUpper) &&
-               Objects.equals(format, other.format);
+    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(RangeQueryParser.NAME);
+        builder.startObject(name);
+        builder.field("from", from);
+        builder.field("to", to);
+        if (timeZone != null) {
+            builder.field("time_zone", timeZone);
+        }
+        if (format != null) {
+            builder.field("format", format);
+        }
+        builder.field("include_lower", includeLower);
+        builder.field("include_upper", includeUpper);
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
+        builder.endObject();
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java
index d10f6b0..355f9f2 100644
--- a/core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java
@@ -19,17 +19,26 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermRangeQuery;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.joda.DateMathParser;
+import org.elasticsearch.common.joda.Joda;
+import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.core.DateFieldMapper;
+import org.joda.time.DateTimeZone;
 
 import java.io.IOException;
 
 /**
- * Parser for range query
+ *
  */
-public class RangeQueryParser extends BaseQueryParser<RangeQueryBuilder> {
+public class RangeQueryParser implements QueryParser {
 
+    public static final String NAME = "range";
     private static final ParseField FIELDDATA_FIELD = new ParseField("fielddata").withAllDeprecated("[no replacement]");
     private static final ParseField NAME_FIELD = new ParseField("_name").withAllDeprecated("query name is not supported in short version of range query");
 
@@ -39,22 +48,22 @@ public class RangeQueryParser extends BaseQueryParser<RangeQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{RangeQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public RangeQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldName = null;
         Object from = null;
         Object to = null;
-        boolean includeLower = RangeQueryBuilder.DEFAULT_INCLUDE_LOWER;
-        boolean includeUpper = RangeQueryBuilder.DEFAULT_INCLUDE_UPPER;
-        String timeZone = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        boolean includeLower = true;
+        boolean includeUpper = true;
+        DateTimeZone timeZone = null;
+        DateMathParser forcedDateParser = null;
+        float boost = 1.0f;
         String queryName = null;
-        String format = null;
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -92,11 +101,9 @@ public class RangeQueryParser extends BaseQueryParser<RangeQueryBuilder> {
                             to = parser.objectBytes();
                             includeUpper = true;
                         } else if ("time_zone".equals(currentFieldName) || "timeZone".equals(currentFieldName)) {
-                            timeZone = parser.text();
+                            timeZone = DateTimeZone.forID(parser.text());
                         } else if ("format".equals(currentFieldName)) {
-                            format = parser.text();
-                        } else if ("_name".equals(currentFieldName)) {
-                            queryName = parser.text();
+                            forcedDateParser = new DateMathParser(Joda.forPattern(parser.text()));
                         } else {
                             throw new QueryParsingException(parseContext, "[range] query does not support [" + currentFieldName + "]");
                         }
@@ -113,20 +120,27 @@ public class RangeQueryParser extends BaseQueryParser<RangeQueryBuilder> {
             }
         }
 
-        RangeQueryBuilder rangeQuery = new RangeQueryBuilder(fieldName);
-        rangeQuery.from(from);
-        rangeQuery.to(to);
-        rangeQuery.includeLower(includeLower);
-        rangeQuery.includeUpper(includeUpper);
-        rangeQuery.timeZone(timeZone);
-        rangeQuery.boost(boost);
-        rangeQuery.queryName(queryName);
-        rangeQuery.format(format);
-        return rangeQuery;
-    }
-
-    @Override
-    public RangeQueryBuilder getBuilderPrototype() {
-        return RangeQueryBuilder.PROTOTYPE;
+        Query query = null;
+        MappedFieldType mapper = parseContext.fieldMapper(fieldName);
+        if (mapper != null) {
+            if (mapper instanceof DateFieldMapper.DateFieldType) {
+                query = ((DateFieldMapper.DateFieldType) mapper).rangeQuery(from, to, includeLower, includeUpper, timeZone, forcedDateParser);
+            } else  {
+                if (timeZone != null) {
+                    throw new QueryParsingException(parseContext, "[range] time_zone can not be applied to non date field ["
+                            + fieldName + "]");
+                }
+                //LUCENE 4 UPGRADE Mapper#rangeQuery should use bytesref as well?
+                query = mapper.rangeQuery(from, to, includeLower, includeUpper);
+            }
+        }
+        if (query == null) {
+            query = new TermRangeQuery(fieldName, BytesRefs.toBytesRef(from), BytesRefs.toBytesRef(to), includeLower, includeUpper);
+        }
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java
index 6399089..ee143eb 100644
--- a/core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java
@@ -19,73 +19,48 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.RegexpQuery;
 import org.apache.lucene.util.automaton.Operations;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * A Query that does fuzzy matching for a specific value.
  */
-public class RegexpQueryBuilder extends AbstractQueryBuilder<RegexpQueryBuilder> implements MultiTermQueryBuilder<RegexpQueryBuilder> {
+public class RegexpQueryBuilder extends MultiTermQueryBuilder implements BoostableQueryBuilder<RegexpQueryBuilder> {
 
-    public static final String NAME = "regexp";
+    private final String name;
+    private final String regexp;
 
-    public static final int DEFAULT_FLAGS_VALUE = RegexpFlag.ALL.value();
-
-    public static final int DEFAULT_MAX_DETERMINIZED_STATES = Operations.DEFAULT_MAX_DETERMINIZED_STATES;
-
-    private final String fieldName;
-    
-    private final String value;
-    
-    private int flagsValue = DEFAULT_FLAGS_VALUE;
-    
-    private int maxDeterminizedStates = DEFAULT_MAX_DETERMINIZED_STATES;
-    
+    private int flags = RegexpQueryParser.DEFAULT_FLAGS_VALUE;
+    private float boost = -1;
     private String rewrite;
-    
-    static final RegexpQueryBuilder PROTOTYPE = new RegexpQueryBuilder(null, null);
+    private String queryName;
+    private int maxDeterminizedStates = Operations.DEFAULT_MAX_DETERMINIZED_STATES;
+    private boolean maxDetermizedStatesSet;
 
     /**
-     * Constructs a new regex query.
-     * 
-     * @param fieldName  The name of the field
-     * @param value The regular expression
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param regexp The regular expression
      */
-    public RegexpQueryBuilder(String fieldName, String value) {
-        this.fieldName = fieldName;
-        this.value = value;
-    }
-
-    /** Returns the field name used in this query. */
-    public String fieldName() {
-        return this.fieldName;
+    public RegexpQueryBuilder(String name, String regexp) {
+        this.name = name;
+        this.regexp = regexp;
     }
 
     /**
-     *  Returns the value used in this query.
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
      */
-    public String value() {
-        return this.value;
+    @Override
+    public RegexpQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     public RegexpQueryBuilder flags(RegexpFlag... flags) {
-        if (flags == null) {
-            this.flagsValue = DEFAULT_FLAGS_VALUE;
-            return this;
-        }
         int value = 0;
         if (flags.length == 0) {
             value = RegexpFlag.ALL.value;
@@ -94,120 +69,53 @@ public class RegexpQueryBuilder extends AbstractQueryBuilder<RegexpQueryBuilder>
                 value |= flag.value;
             }
         }
-        this.flagsValue = value;
-        return this;
-    }
-
-    public RegexpQueryBuilder flags(int flags) {
-        this.flagsValue = flags;
+        this.flags = value;
         return this;
     }
 
-    public int flags() {
-        return this.flagsValue;
-    }
-
     /**
      * Sets the regexp maxDeterminizedStates.
      */
     public RegexpQueryBuilder maxDeterminizedStates(int value) {
         this.maxDeterminizedStates = value;
+        this.maxDetermizedStatesSet = true;
         return this;
     }
-    
-    public int maxDeterminizedStates() {
-        return this.maxDeterminizedStates;
-    }
 
     public RegexpQueryBuilder rewrite(String rewrite) {
         this.rewrite = rewrite;
         return this;
     }
-    
-    public String rewrite() {
-        return this.rewrite;
+
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public RegexpQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.startObject(fieldName);
-        builder.field("value", this.value);
-        builder.field("flags_value", flagsValue);
-        builder.field("max_determinized_states", maxDeterminizedStates);
-        if (rewrite != null) {
-            builder.field("rewrite", rewrite);
+        builder.startObject(RegexpQueryParser.NAME);
+        builder.startObject(name);
+        builder.field("value", regexp);
+        if (flags != -1) {
+            builder.field("flags_value", flags);
         }
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    public Query doToQuery(QueryShardContext context) throws QueryShardException, IOException {
-        MultiTermQuery.RewriteMethod method = QueryParsers.parseRewriteMethod(context.parseFieldMatcher(), rewrite, null);
-
-        Query query = null;
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType != null) {
-            query = fieldType.regexpQuery(value, flagsValue, maxDeterminizedStates, method, context);
+        if (maxDetermizedStatesSet) {
+            builder.field("max_determinized_states", maxDeterminizedStates);
         }
-        if (query == null) {
-            RegexpQuery regexpQuery = new RegexpQuery(new Term(fieldName, BytesRefs.toBytesRef(value)), flagsValue, maxDeterminizedStates);
-            if (method != null) {
-                regexpQuery.setRewriteMethod(method);
-            }
-            query = regexpQuery;
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        return query;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (Strings.isEmpty(this.fieldName)) {
-            validationException = addValidationError("field name cannot be null or empty.", validationException);
+        if (rewrite != null) {
+            builder.field("rewrite", rewrite);
         }
-        if (this.value == null) {
-            validationException = addValidationError("query text cannot be null", validationException);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return validationException;
-    }
-
-    @Override
-    public RegexpQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        RegexpQueryBuilder regexpQueryBuilder = new RegexpQueryBuilder(in.readString(), in.readString());
-        regexpQueryBuilder.flagsValue = in.readVInt();
-        regexpQueryBuilder.maxDeterminizedStates = in.readVInt();
-        regexpQueryBuilder.rewrite = in.readOptionalString();
-        return regexpQueryBuilder;
-    }
-
-    @Override
-    public void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-        out.writeString(value);
-        out.writeVInt(flagsValue);
-        out.writeVInt(maxDeterminizedStates);
-        out.writeOptionalString(rewrite);
-    }
-
-    @Override
-    public int doHashCode() {
-        return Objects.hash(fieldName, value, flagsValue, maxDeterminizedStates, rewrite);
-    }
-
-    @Override
-    public boolean doEquals(RegexpQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-                Objects.equals(value, other.value) &&
-                Objects.equals(flagsValue, other.flagsValue) &&
-                Objects.equals(maxDeterminizedStates, other.maxDeterminizedStates) &&
-                Objects.equals(rewrite, other.rewrite);
+        builder.endObject();
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java
index 66fd44a..5844c17 100644
--- a/core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java
@@ -19,16 +19,28 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.MultiTermQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.RegexpQuery;
+import org.apache.lucene.util.automaton.Operations;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
 
 /**
- * Parser for regexp query
+ *
  */
-public class RegexpQueryParser extends BaseQueryParser<RegexpQueryBuilder> {
+public class RegexpQueryParser implements QueryParser {
+
+    public static final String NAME = "regexp";
+
+    public static final int DEFAULT_FLAGS_VALUE = RegexpFlag.ALL.value();
 
     private static final ParseField NAME_FIELD = new ParseField("_name").withAllDeprecated("query name is not supported in short version of regexp query");
 
@@ -38,20 +50,20 @@ public class RegexpQueryParser extends BaseQueryParser<RegexpQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{RegexpQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public RegexpQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldName = parser.currentName();
-        String rewrite = null;
+        String rewriteMethod = null;
 
         String value = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        int flagsValue = RegexpQueryBuilder.DEFAULT_FLAGS_VALUE;
-        int maxDeterminizedStates = RegexpQueryBuilder.DEFAULT_MAX_DETERMINIZED_STATES;
+        float boost = 1.0f;
+        int flagsValue = DEFAULT_FLAGS_VALUE;
+        int maxDeterminizedStates = Operations.DEFAULT_MAX_DETERMINIZED_STATES;
         String queryName = null;
         String currentFieldName = null;
         XContentParser.Token token;
@@ -71,7 +83,7 @@ public class RegexpQueryParser extends BaseQueryParser<RegexpQueryBuilder> {
                         } else if ("boost".equals(currentFieldName)) {
                             boost = parser.floatValue();
                         } else if ("rewrite".equals(currentFieldName)) {
-                            rewrite = parser.textOrNull();
+                            rewriteMethod = parser.textOrNull();
                         } else if ("flags".equals(currentFieldName)) {
                             String flags = parser.textOrNull();
                             flagsValue = RegexpFlag.resolveValue(flags);
@@ -99,16 +111,27 @@ public class RegexpQueryParser extends BaseQueryParser<RegexpQueryBuilder> {
         if (value == null) {
             throw new QueryParsingException(parseContext, "No value specified for regexp query");
         }
-        return new RegexpQueryBuilder(fieldName, value)
-                .flags(flagsValue)
-                .maxDeterminizedStates(maxDeterminizedStates)
-                .rewrite(rewrite)
-                .boost(boost)
-                .queryName(queryName);
-    }
 
-    @Override
-    public RegexpQueryBuilder getBuilderPrototype() {
-        return RegexpQueryBuilder.PROTOTYPE;
+        MultiTermQuery.RewriteMethod method = QueryParsers.parseRewriteMethod(parseContext.parseFieldMatcher(), rewriteMethod, null);
+
+        Query query = null;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            query = fieldType.regexpQuery(value, flagsValue, maxDeterminizedStates, method, parseContext);
+        }
+        if (query == null) {
+            RegexpQuery regexpQuery = new RegexpQuery(new Term(fieldName, BytesRefs.toBytesRef(value)), flagsValue, maxDeterminizedStates);
+            if (method != null) {
+                regexpQuery.setRewriteMethod(method);
+            }
+            query = regexpQuery;
+        }
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
+
+
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java
index 519f065..a9a35ac 100644
--- a/core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java
@@ -19,155 +19,40 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.RandomAccessWeight;
-import org.apache.lucene.search.Weight;
-import org.apache.lucene.util.Bits;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.script.*;
+import org.elasticsearch.script.Script;
 import org.elasticsearch.script.Script.ScriptField;
-import org.elasticsearch.search.lookup.SearchLookup;
 
 import java.io.IOException;
-import java.util.Objects;
+import java.util.HashMap;
+import java.util.Map;
 
-public class ScriptQueryBuilder extends AbstractQueryBuilder<ScriptQueryBuilder> {
+public class ScriptQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "script";
+    private Script script;
 
-    static final ScriptQueryBuilder PROTOTYPE = new ScriptQueryBuilder(null);
-
-    private final Script script;
+    private String queryName;
 
     public ScriptQueryBuilder(Script script) {
         this.script = script;
     }
 
-    public Script script() {
-        return this.script;
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+    /**
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public ScriptQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params builderParams) throws IOException {
-        builder.startObject(NAME);
-        builder.field(ScriptField.SCRIPT.getPreferredName(), script);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return new ScriptQuery(script, context.scriptService(), context.lookup());
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (this.script == null) {
-            validationException = addValidationError("script cannot be null", validationException);
-        }
-        return validationException;
-    }
-
-    static class ScriptQuery extends Query {
-
-        private final Script script;
-
-        private final SearchScript searchScript;
-
-        public ScriptQuery(Script script, ScriptService scriptService, SearchLookup searchLookup) {
-            this.script = script;
-            this.searchScript = scriptService.search(searchLookup, script, ScriptContext.Standard.SEARCH);
-        }
-
-        @Override
-        public String toString(String field) {
-            StringBuilder buffer = new StringBuilder();
-            buffer.append("ScriptFilter(");
-            buffer.append(script);
-            buffer.append(")");
-            return buffer.toString();
-        }
 
-        @Override
-        public boolean equals(Object obj) {
-            if (this == obj)
-                return true;
-            if (!super.equals(obj))
-                return false;
-            ScriptQuery other = (ScriptQuery) obj;
-            return Objects.equals(script, other.script);
-        }
-
-        @Override
-        public int hashCode() {
-            final int prime = 31;
-            int result = super.hashCode();
-            result = prime * result + Objects.hashCode(script);
-            return result;
-        }
-
-        @Override
-        public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {
-            return new RandomAccessWeight(this) {
-                @Override
-                protected Bits getMatchingDocs(final LeafReaderContext context) throws IOException {
-                    final LeafSearchScript leafScript = searchScript.getLeafSearchScript(context);
-                    return new Bits() {
-
-                        @Override
-                        public boolean get(int doc) {
-                            leafScript.setDocument(doc);
-                            Object val = leafScript.run();
-                            if (val == null) {
-                                return false;
-                            }
-                            if (val instanceof Boolean) {
-                                return (Boolean) val;
-                            }
-                            if (val instanceof Number) {
-                                return ((Number) val).longValue() != 0;
-                            }
-                            throw new IllegalArgumentException("Can't handle type [" + val + "] in script filter");
-                        }
-
-                        @Override
-                        public int length() {
-                            return context.reader().maxDoc();
-                        }
-
-                    };
-                }
-            };
+        builder.startObject(ScriptQueryParser.NAME);
+        builder.field(ScriptField.SCRIPT.getPreferredName(), script);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-    }
-
-    @Override
-    protected ScriptQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new ScriptQueryBuilder(Script.readScript(in));
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        script.writeTo(out);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(script);
-    }
-
-    @Override
-    protected boolean doEquals(ScriptQueryBuilder other) {
-        return Objects.equals(script, other.script);
+        builder.endObject();
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java
index ccbfd66..62561f3 100644
--- a/core/src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java
@@ -19,12 +19,20 @@
 
 package org.elasticsearch.index.query;
 
+import com.google.common.base.Objects;
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.RandomAccessWeight;
+import org.apache.lucene.search.Weight;
+import org.apache.lucene.util.Bits;
+import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.script.Script;
+import org.elasticsearch.script.*;
 import org.elasticsearch.script.Script.ScriptField;
-import org.elasticsearch.script.ScriptParameterParser;
 import org.elasticsearch.script.ScriptParameterParser.ScriptParameterValue;
+import org.elasticsearch.search.lookup.SearchLookup;
 
 import java.io.IOException;
 import java.util.Map;
@@ -32,9 +40,11 @@ import java.util.Map;
 import static com.google.common.collect.Maps.newHashMap;
 
 /**
- * Parser for script query
+ *
  */
-public class ScriptQueryParser extends BaseQueryParser<ScriptQueryBuilder> {
+public class ScriptQueryParser implements QueryParser {
+
+    public static final String NAME = "script";
 
     @Inject
     public ScriptQueryParser() {
@@ -42,23 +52,23 @@ public class ScriptQueryParser extends BaseQueryParser<ScriptQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{ScriptQueryBuilder.NAME};
+        return new String[] { NAME };
     }
 
     @Override
-    public ScriptQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
         ScriptParameterParser scriptParameterParser = new ScriptParameterParser();
-        
+
+        XContentParser.Token token;
+
         // also, when caching, since its isCacheable is false, will result in loading all bit set...
         Script script = null;
         Map<String, Object> params = null;
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         String queryName = null;
-
-        XContentParser.Token token;
         String currentFieldName = null;
+
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
@@ -75,8 +85,6 @@ public class ScriptQueryParser extends BaseQueryParser<ScriptQueryBuilder> {
             } else if (token.isValue()) {
                 if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else if (!scriptParameterParser.token(currentFieldName, token, parser, parseContext.parseFieldMatcher())) {
                     throw new QueryParsingException(parseContext, "[script] query does not support [" + currentFieldName + "]");
                 }
@@ -99,13 +107,83 @@ public class ScriptQueryParser extends BaseQueryParser<ScriptQueryBuilder> {
             throw new QueryParsingException(parseContext, "script must be provided with a [script] filter");
         }
 
-        return new ScriptQueryBuilder(script)
-                .boost(boost)
-                .queryName(queryName);
+        Query query = new ScriptQuery(script, parseContext.scriptService(), parseContext.lookup());
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 
-    @Override
-    public ScriptQueryBuilder getBuilderPrototype() {
-        return ScriptQueryBuilder.PROTOTYPE;
+    static class ScriptQuery extends Query {
+
+        private final Script script;
+
+        private final SearchScript searchScript;
+
+        public ScriptQuery(Script script, ScriptService scriptService, SearchLookup searchLookup) {
+            this.script = script;
+            this.searchScript = scriptService.search(searchLookup, script, ScriptContext.Standard.SEARCH);
+        }
+
+        @Override
+        public String toString(String field) {
+            StringBuilder buffer = new StringBuilder();
+            buffer.append("ScriptFilter(");
+            buffer.append(script);
+            buffer.append(")");
+            return buffer.toString();
+        }
+
+        @Override
+        public boolean equals(Object obj) {
+            if (this == obj)
+                return true;
+            if (!super.equals(obj))
+                return false;
+            ScriptQuery other = (ScriptQuery) obj;
+            return Objects.equal(script, other.script);
+        }
+
+        @Override
+        public int hashCode() {
+            final int prime = 31;
+            int result = super.hashCode();
+            result = prime * result + Objects.hashCode(script);
+            return result;
+        }
+
+        @Override
+        public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {
+            return new RandomAccessWeight(this) {
+                @Override
+                protected Bits getMatchingDocs(final LeafReaderContext context) throws IOException {
+                    final LeafSearchScript leafScript = searchScript.getLeafSearchScript(context);
+                    return new Bits() {
+
+                        @Override
+                        public boolean get(int doc) {
+                            leafScript.setDocument(doc);
+                            Object val = leafScript.run();
+                            if (val == null) {
+                                return false;
+                            }
+                            if (val instanceof Boolean) {
+                                return (Boolean) val;
+                            }
+                            if (val instanceof Number) {
+                                return ((Number) val).longValue() != 0;
+                            }
+                            throw new IllegalArgumentException("Can't handle type [" + val + "] in script filter");
+                        }
+
+                        @Override
+                        public int length() {
+                            return context.reader().maxDoc();
+                        }
+
+                    };
+                }
+            };
+        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryParser.java
index 027f350..48f3ce6 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryParser.java
@@ -29,7 +29,6 @@ import org.apache.lucene.util.BytesRef;
 import java.io.IOException;
 import java.util.Locale;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Wrapper class for Lucene's SimpleQueryParser that allows us to redefine
@@ -200,102 +199,51 @@ public class SimpleQueryParser extends org.apache.lucene.queryparser.simple.Simp
             return new PrefixQuery(new Term(field, termStr));
         }
     }
+
     /**
      * Class encapsulating the settings for the SimpleQueryString query, with
      * their default values
      */
-    static class Settings {
-        /** Locale to use for parsing. */
-        private Locale locale = SimpleQueryStringBuilder.DEFAULT_LOCALE;
-        /** Specifies whether parsed terms should be lowercased. */
-        private boolean lowercaseExpandedTerms = SimpleQueryStringBuilder.DEFAULT_LOWERCASE_EXPANDED_TERMS;
-        /** Specifies whether lenient query parsing should be used. */
-        private boolean lenient = SimpleQueryStringBuilder.DEFAULT_LENIENT;
-        /** Specifies whether wildcards should be analyzed. */
-        private boolean analyzeWildcard = SimpleQueryStringBuilder.DEFAULT_ANALYZE_WILDCARD;
+    public static class Settings {
+        private Locale locale = Locale.ROOT;
+        private boolean lowercaseExpandedTerms = true;
+        private boolean lenient = false;
+        private boolean analyzeWildcard = false;
 
-        /**
-         * Generates default {@link Settings} object (uses ROOT locale, does
-         * lowercase terms, no lenient parsing, no wildcard analysis).
-         * */
         public Settings() {
-        }
 
-        public Settings(Locale locale, Boolean lowercaseExpandedTerms, Boolean lenient, Boolean analyzeWildcard) {
-            this.locale = locale;
-            this.lowercaseExpandedTerms = lowercaseExpandedTerms;
-            this.lenient = lenient;
-            this.analyzeWildcard = analyzeWildcard;
         }
 
-        /** Specifies the locale to use for parsing, Locale.ROOT by default. */
         public void locale(Locale locale) {
-            this.locale = (locale != null) ? locale : SimpleQueryStringBuilder.DEFAULT_LOCALE;
+            this.locale = locale;
         }
 
-        /** Returns the locale to use for parsing. */
         public Locale locale() {
             return this.locale;
         }
 
-        /**
-         * Specifies whether to lowercase parse terms, defaults to true if
-         * unset.
-         */
         public void lowercaseExpandedTerms(boolean lowercaseExpandedTerms) {
             this.lowercaseExpandedTerms = lowercaseExpandedTerms;
         }
 
-        /** Returns whether to lowercase parse terms. */
         public boolean lowercaseExpandedTerms() {
             return this.lowercaseExpandedTerms;
         }
 
-        /** Specifies whether to use lenient parsing, defaults to false. */
         public void lenient(boolean lenient) {
             this.lenient = lenient;
         }
 
-        /** Returns whether to use lenient parsing. */
         public boolean lenient() {
             return this.lenient;
         }
 
-        /** Specifies whether to analyze wildcards. Defaults to false if unset. */
         public void analyzeWildcard(boolean analyzeWildcard) {
             this.analyzeWildcard = analyzeWildcard;
         }
 
-        /** Returns whether to analyze wildcards. */
         public boolean analyzeWildcard() {
             return analyzeWildcard;
         }
-
-        @Override
-        public int hashCode() {
-            // checking the return value of toLanguageTag() for locales only.
-            // For further reasoning see
-            // https://issues.apache.org/jira/browse/LUCENE-4021
-            return Objects.hash(locale.toLanguageTag(), lowercaseExpandedTerms, lenient, analyzeWildcard);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (this == obj) {
-                return true;
-            }
-            if (obj == null || getClass() != obj.getClass()) {
-                return false;
-            }
-            Settings other = (Settings) obj;
-
-            // checking the return value of toLanguageTag() for locales only.
-            // For further reasoning see
-            // https://issues.apache.org/jira/browse/LUCENE-4021
-            return (Objects.equals(locale.toLanguageTag(), other.locale.toLanguageTag())
-                    && Objects.equals(lowercaseExpandedTerms, other.lowercaseExpandedTerms) 
-                    && Objects.equals(lenient, other.lenient)
-                    && Objects.equals(analyzeWildcard, other.analyzeWildcard));
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java
index 9cab899..700ad41 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java
@@ -19,388 +19,202 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
-import org.elasticsearch.common.regex.Regex;
+import org.elasticsearch.common.xcontent.ToXContent.Params;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.SimpleQueryParser.Settings;
 
 import java.io.IOException;
 import java.util.HashMap;
 import java.util.Locale;
 import java.util.Map;
-import java.util.Objects;
-import java.util.TreeMap;
 
 /**
- * SimpleQuery is a query parser that acts similar to a query_string query, but
- * won't throw exceptions for any weird string syntax.
- *
- * For more detailed explanation of the query string syntax see also the <a
- * href=
- * "https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-simple-query-string-query.html"
- * > online documentation</a>.
+ * SimpleQuery is a query parser that acts similar to a query_string
+ * query, but won't throw exceptions for any weird string syntax.
  */
-public class SimpleQueryStringBuilder extends AbstractQueryBuilder<SimpleQueryStringBuilder> {
-    /** Default locale used for parsing.*/
-    public static final Locale DEFAULT_LOCALE = Locale.ROOT;
-    /** Default for lowercasing parsed terms.*/
-    public static final boolean DEFAULT_LOWERCASE_EXPANDED_TERMS = true;
-    /** Default for using lenient query parsing.*/
-    public static final boolean DEFAULT_LENIENT = false;
-    /** Default for wildcard analysis.*/
-    public static final boolean DEFAULT_ANALYZE_WILDCARD = false;
-    /** Default for default operator to use for linking boolean clauses.*/
-    public static final Operator DEFAULT_OPERATOR = Operator.OR;
-    /** Default for search flags to use. */
-    public static final int DEFAULT_FLAGS = SimpleQueryStringFlag.ALL.value;
-    /** Name for (de-)serialization. */
-    public static final String NAME = "simple_query_string";
-
-    static final SimpleQueryStringBuilder PROTOTYPE = new SimpleQueryStringBuilder(null);
-
-    /** Query text to parse. */
-    private final String queryText;
-    /**
-     * Fields to query against. If left empty will query default field,
-     * currently _ALL. Uses a TreeMap to hold the fields so boolean clauses are
-     * always sorted in same order for generated Lucene query for easier
-     * testing.
-     *
-     * Can be changed back to HashMap once https://issues.apache.org/jira/browse/LUCENE-6305 is fixed.
-     */
-    private final Map<String, Float> fieldsAndWeights = new TreeMap<>();
-    /** If specified, analyzer to use to parse the query text, defaults to registered default in toQuery. */
+public class SimpleQueryStringBuilder extends QueryBuilder implements BoostableQueryBuilder<SimpleQueryStringBuilder> {
+    private Map<String, Float> fields = new HashMap<>();
     private String analyzer;
-    /** Default operator to use for linking boolean clauses. Defaults to OR according to docs. */
-    private Operator defaultOperator = DEFAULT_OPERATOR;
-    /** If result is a boolean query, minimumShouldMatch parameter to apply. Ignored otherwise. */
+    private Operator operator;
+    private final String queryText;
+    private String queryName;
     private String minimumShouldMatch;
-    /** Any search flags to be used, ALL by default. */
-    private int flags = DEFAULT_FLAGS;
+    private int flags = -1;
+    private float boost = -1.0f;
+    private Boolean lowercaseExpandedTerms;
+    private Boolean lenient;
+    private Boolean analyzeWildcard;
+    private Locale locale;
 
-    /** Further search settings needed by the ES specific query string parser only. */
-    private Settings settings = new Settings();
+    /**
+     * Operators for the default_operator
+     */
+    public static enum Operator {
+        AND,
+        OR
+    }
 
-    /** Construct a new simple query with this query string. */
-    public SimpleQueryStringBuilder(String queryText) {
-        this.queryText = queryText;
+    /**
+     * Construct a new simple query with the given text
+     */
+    public SimpleQueryStringBuilder(String text) {
+        this.queryText = text;
     }
 
-    /** Returns the text to parse the query from. */
-    public String value() {
-        return this.queryText;
+    /** Set the boost of this query. */
+    @Override
+    public SimpleQueryStringBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+    
+    /** Returns the boost of this query. */
+    public float boost() {
+        return this.boost;
     }
 
-    /** Add a field to run the query against. */
+    /**
+     * Add a field to run the query against
+     */
     public SimpleQueryStringBuilder field(String field) {
-        if (Strings.isEmpty(field)) {
-            throw new IllegalArgumentException("supplied field is null or empty.");
-        }
-        this.fieldsAndWeights.put(field, AbstractQueryBuilder.DEFAULT_BOOST);
+        this.fields.put(field, null);
         return this;
     }
 
-    /** Add a field to run the query against with a specific boost. */
+    /**
+     * Add a field to run the query against with a specific boost
+     */
     public SimpleQueryStringBuilder field(String field, float boost) {
-        if (Strings.isEmpty(field)) {
-            throw new IllegalArgumentException("supplied field is null or empty.");
-        }
-        this.fieldsAndWeights.put(field, boost);
+        this.fields.put(field, boost);
         return this;
     }
 
-    /** Add several fields to run the query against with a specific boost. */
-    public SimpleQueryStringBuilder fields(Map<String, Float> fields) {
-        this.fieldsAndWeights.putAll(fields);
+    /**
+     * Specify a name for the query
+     */
+    public SimpleQueryStringBuilder queryName(String name) {
+        this.queryName = name;
         return this;
     }
 
-    /** Returns the fields including their respective boosts to run the query against. */
-    public Map<String, Float> fields() {
-        return this.fieldsAndWeights;
-    }
-
-    /** Specify an analyzer to use for the query. */
+    /**
+     * Specify an analyzer to use for the query
+     */
     public SimpleQueryStringBuilder analyzer(String analyzer) {
         this.analyzer = analyzer;
         return this;
     }
 
-    /** Returns the analyzer to use for the query. */
-    public String analyzer() {
-        return this.analyzer;
-    }
-
     /**
      * Specify the default operator for the query. Defaults to "OR" if no
-     * operator is specified.
+     * operator is specified
      */
     public SimpleQueryStringBuilder defaultOperator(Operator defaultOperator) {
-        this.defaultOperator = (defaultOperator != null) ? defaultOperator : DEFAULT_OPERATOR;
+        this.operator = defaultOperator;
         return this;
     }
 
-    /** Returns the default operator for the query. */
-    public Operator defaultOperator() {
-        return this.defaultOperator;
-    }
-
     /**
-     * Specify the enabled features of the SimpleQueryString. Defaults to ALL if
-     * none are specified.
+     * Specify the enabled features of the SimpleQueryString.
      */
     public SimpleQueryStringBuilder flags(SimpleQueryStringFlag... flags) {
-        if (flags != null && flags.length > 0) {
-            int value = 0;
+        int value = 0;
+        if (flags.length == 0) {
+            value = SimpleQueryStringFlag.ALL.value;
+        } else {
             for (SimpleQueryStringFlag flag : flags) {
                 value |= flag.value;
             }
-            this.flags = value;
-        } else {
-            this.flags = DEFAULT_FLAGS;
         }
-
-        return this;
-    }
-
-    /** For testing and serialisation only. */
-    SimpleQueryStringBuilder flags(int flags) {
-        this.flags = flags;
+        this.flags = value;
         return this;
     }
 
-    /** For testing only: Return the flags set for this query. */
-    int flags() {
-        return this.flags;
-    }
-
-    /**
-     * Specifies whether parsed terms for this query should be lower-cased.
-     * Defaults to true if not set.
-     */
     public SimpleQueryStringBuilder lowercaseExpandedTerms(boolean lowercaseExpandedTerms) {
-        this.settings.lowercaseExpandedTerms(lowercaseExpandedTerms);
+        this.lowercaseExpandedTerms = lowercaseExpandedTerms;
         return this;
     }
 
-    /** Returns whether parsed terms should be lower cased for this query. */
-    public boolean lowercaseExpandedTerms() {
-        return this.settings.lowercaseExpandedTerms();
-    }
-
-    /** Specifies the locale for parsing terms. Defaults to ROOT if none is set. */
     public SimpleQueryStringBuilder locale(Locale locale) {
-        this.settings.locale(locale);
+        this.locale = locale;
         return this;
     }
 
-    /** Returns the locale for parsing terms for this query. */
-    public Locale locale() {
-        return this.settings.locale();
-    }
-
-    /** Specifies whether query parsing should be lenient. Defaults to false. */
     public SimpleQueryStringBuilder lenient(boolean lenient) {
-        this.settings.lenient(lenient);
+        this.lenient = lenient;
         return this;
     }
 
-    /** Returns whether query parsing should be lenient. */
-    public boolean lenient() {
-        return this.settings.lenient();
-    }
-
-    /** Specifies whether wildcards should be analyzed. Defaults to false. */
     public SimpleQueryStringBuilder analyzeWildcard(boolean analyzeWildcard) {
-        this.settings.analyzeWildcard(analyzeWildcard);
+        this.analyzeWildcard = analyzeWildcard;
         return this;
     }
 
-    /** Returns whether wildcards should by analyzed. */
-    public boolean analyzeWildcard() {
-        return this.settings.analyzeWildcard();
-    }
-
-    /**
-     * Specifies the minimumShouldMatch to apply to the resulting query should
-     * that be a Boolean query.
-     */
     public SimpleQueryStringBuilder minimumShouldMatch(String minimumShouldMatch) {
         this.minimumShouldMatch = minimumShouldMatch;
         return this;
     }
 
-    /**
-     * Returns the minimumShouldMatch to apply to the resulting query should
-     * that be a Boolean query.
-     */
-    public String minimumShouldMatch() {
-        return minimumShouldMatch;
-    }
-
-    /**
-     * {@inheritDoc}
-     *
-     * Checks that mandatory queryText is neither null nor empty.
-     * */
     @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        // Query text is required
-        if (queryText == null) {
-            validationException = addValidationError("query text missing", validationException);
-        }
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(SimpleQueryStringParser.NAME);
 
-        return validationException;
-    }
+        builder.field("query", queryText);
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        // field names in builder can have wildcards etc, need to resolve them here
-        Map<String, Float> resolvedFieldsAndWeights = new TreeMap<>();
-        // Use the default field if no fields specified
-        if (fieldsAndWeights.isEmpty()) {
-            resolvedFieldsAndWeights.put(resolveIndexName(context.defaultField(), context), AbstractQueryBuilder.DEFAULT_BOOST);
-        } else {
-            for (Map.Entry<String, Float> fieldEntry : fieldsAndWeights.entrySet()) {
-                if (Regex.isSimpleMatchPattern(fieldEntry.getKey())) {
-                    for (String fieldName : context.mapperService().simpleMatchToIndexNames(fieldEntry.getKey())) {
-                        resolvedFieldsAndWeights.put(fieldName, fieldEntry.getValue());
-                    }
+        if (fields.size() > 0) {
+            builder.startArray("fields");
+            for (Map.Entry<String, Float> entry : fields.entrySet()) {
+                String field = entry.getKey();
+                Float boost = entry.getValue();
+                if (boost != null) {
+                    builder.value(field + "^" + boost);
                 } else {
-                    resolvedFieldsAndWeights.put(resolveIndexName(fieldEntry.getKey(), context), fieldEntry.getValue());
+                    builder.value(field);
                 }
             }
+            builder.endArray();
         }
 
-        // Use standard analyzer by default if none specified
-        Analyzer luceneAnalyzer;
-        if (analyzer == null) {
-            luceneAnalyzer = context.mapperService().searchAnalyzer();
-        } else {
-            luceneAnalyzer = context.analysisService().analyzer(analyzer);
-            if (luceneAnalyzer == null) {
-                throw new QueryShardException(context, "[" + SimpleQueryStringBuilder.NAME + "] analyzer [" + analyzer
-                        + "] not found");
-            }
-
+        if (flags != -1) {
+            builder.field("flags", flags);
         }
 
-        SimpleQueryParser sqp = new SimpleQueryParser(luceneAnalyzer, resolvedFieldsAndWeights, flags, settings);
-        sqp.setDefaultOperator(defaultOperator.toBooleanClauseOccur());
-
-        Query query = sqp.parse(queryText);
-        if (minimumShouldMatch != null && query instanceof BooleanQuery) {
-            query = Queries.applyMinimumShouldMatch((BooleanQuery) query, minimumShouldMatch);
+        if (analyzer != null) {
+            builder.field("analyzer", analyzer);
         }
-        return query;
-    }
 
-    private static String resolveIndexName(String fieldName, QueryShardContext context) {
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType != null) {
-            return fieldType.names().indexName();
+        if (operator != null) {
+            builder.field("default_operator", operator.name().toLowerCase(Locale.ROOT));
         }
-        return fieldName;
-    }
 
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        if (lowercaseExpandedTerms != null) {
+            builder.field("lowercase_expanded_terms", lowercaseExpandedTerms);
+        }
 
-        builder.field("query", queryText);
+        if (lenient != null) {
+            builder.field("lenient", lenient);
+        }
 
-        if (fieldsAndWeights.size() > 0) {
-            builder.startArray("fields");
-            for (Map.Entry<String, Float> entry : fieldsAndWeights.entrySet()) {
-                builder.value(entry.getKey() + "^" + entry.getValue());
-            }
-            builder.endArray();
+        if (analyzeWildcard != null) {
+            builder.field("analyze_wildcard", analyzeWildcard);
         }
 
-        if (analyzer != null) {
-            builder.field("analyzer", analyzer);
+        if (locale != null) {
+            builder.field("locale", locale.toString());
         }
 
-        builder.field("flags", flags);
-        builder.field("default_operator", defaultOperator.name().toLowerCase(Locale.ROOT));
-        builder.field("lowercase_expanded_terms", settings.lowercaseExpandedTerms());
-        builder.field("lenient", settings.lenient());
-        builder.field("analyze_wildcard", settings.analyzeWildcard());
-        builder.field("locale", (settings.locale().toLanguageTag()));
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
 
         if (minimumShouldMatch != null) {
             builder.field("minimum_should_match", minimumShouldMatch);
         }
-
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected SimpleQueryStringBuilder doReadFrom(StreamInput in) throws IOException {
-        SimpleQueryStringBuilder result = new SimpleQueryStringBuilder(in.readString());
-        int size = in.readInt();
-        Map<String, Float> fields = new HashMap<>();
-        for (int i = 0; i < size; i++) {
-            String field = in.readString();
-            Float weight = in.readFloat();
-            fields.put(field, weight);
-        }
-        result.fieldsAndWeights.putAll(fields);
-        result.flags = in.readInt();
-        result.analyzer = in.readOptionalString();
-        result.defaultOperator = Operator.readOperatorFrom(in);
-        result.settings.lowercaseExpandedTerms(in.readBoolean());
-        result.settings.lenient(in.readBoolean());
-        result.settings.analyzeWildcard(in.readBoolean());
-        String localeStr = in.readString();
-        result.settings.locale(Locale.forLanguageTag(localeStr));
-        result.minimumShouldMatch = in.readOptionalString();
-        return result;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(queryText);
-        out.writeInt(fieldsAndWeights.size());
-        for (Map.Entry<String, Float> entry : fieldsAndWeights.entrySet()) {
-            out.writeString(entry.getKey());
-            out.writeFloat(entry.getValue());
+        
+        if (boost != -1.0f) {
+            builder.field("boost", boost);
         }
-        out.writeInt(flags);
-        out.writeOptionalString(analyzer);
-        defaultOperator.writeTo(out);
-        out.writeBoolean(settings.lowercaseExpandedTerms());
-        out.writeBoolean(settings.lenient());
-        out.writeBoolean(settings.analyzeWildcard());
-        out.writeString(settings.locale().toLanguageTag());
-        out.writeOptionalString(minimumShouldMatch);
-    }
 
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldsAndWeights, analyzer, defaultOperator, queryText, minimumShouldMatch, settings, flags);
+        builder.endObject();
     }
 
-    @Override
-    protected boolean doEquals(SimpleQueryStringBuilder other) {
-        return Objects.equals(fieldsAndWeights, other.fieldsAndWeights) && Objects.equals(analyzer, other.analyzer)
-                && Objects.equals(defaultOperator, other.defaultOperator) && Objects.equals(queryText, other.queryText)
-                && Objects.equals(minimumShouldMatch, other.minimumShouldMatch)
-                && Objects.equals(settings, other.settings) && (flags == other.flags);
-    }
 }
-
diff --git a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringFlag.java b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringFlag.java
index 68d19db..ce0ce88 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringFlag.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringFlag.java
@@ -71,7 +71,7 @@ public enum SimpleQueryStringFlag {
                         magic |= flag.value();
                 }
             } catch (IllegalArgumentException iae) {
-                throw new IllegalArgumentException("Unknown " + SimpleQueryStringBuilder.NAME + " flag [" + s + "]");
+                throw new IllegalArgumentException("Unknown " + SimpleQueryStringParser.NAME + " flag [" + s + "]");
             }
         }
         return magic;
diff --git a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java
index e45659e..a3614be 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java
@@ -19,11 +19,20 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
+import org.elasticsearch.common.regex.Regex;
+import org.elasticsearch.common.util.LocaleUtils;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
 
 import java.io.IOException;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.Locale;
 import java.util.Map;
@@ -59,7 +68,9 @@ import java.util.Map;
  * {@code fields} - fields to search, defaults to _all if not set, allows
  * boosting a field with ^n
  */
-public class SimpleQueryStringParser extends BaseQueryParser<SimpleQueryStringBuilder> {
+public class SimpleQueryStringParser implements QueryParser {
+
+    public static final String NAME = "simple_query_string";
 
     @Inject
     public SimpleQueryStringParser() {
@@ -68,26 +79,23 @@ public class SimpleQueryStringParser extends BaseQueryParser<SimpleQueryStringBu
 
     @Override
     public String[] names() {
-        return new String[]{SimpleQueryStringBuilder.NAME, Strings.toCamelCase(SimpleQueryStringBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SimpleQueryStringBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String currentFieldName = null;
         String queryBody = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f; 
         String queryName = null;
         String minimumShouldMatch = null;
-        Map<String, Float> fieldsAndWeights = new HashMap<>();
-        Operator defaultOperator = null;
-        String analyzerName = null;
-        int flags = SimpleQueryStringFlag.ALL.value();
-        boolean lenient = SimpleQueryStringBuilder.DEFAULT_LENIENT;
-        boolean lowercaseExpandedTerms = SimpleQueryStringBuilder.DEFAULT_LOWERCASE_EXPANDED_TERMS;
-        boolean analyzeWildcard = SimpleQueryStringBuilder.DEFAULT_ANALYZE_WILDCARD;
-        Locale locale = null;
+        Map<String, Float> fieldsAndWeights = null;
+        BooleanClause.Occur defaultOperator = null;
+        Analyzer analyzer = null;
+        int flags = -1;
+        SimpleQueryParser.Settings sqsSettings = new SimpleQueryParser.Settings();
 
         XContentParser.Token token;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -111,10 +119,26 @@ public class SimpleQueryStringParser extends BaseQueryParser<SimpleQueryStringBu
                         if (fField == null) {
                             fField = parser.text();
                         }
-                        fieldsAndWeights.put(fField, fBoost);
+
+                        if (fieldsAndWeights == null) {
+                            fieldsAndWeights = new HashMap<>();
+                        }
+
+                        if (Regex.isSimpleMatchPattern(fField)) {
+                            for (String fieldName : parseContext.mapperService().simpleMatchToIndexNames(fField)) {
+                                fieldsAndWeights.put(fieldName, fBoost);
+                            }
+                        } else {
+                            MappedFieldType fieldType = parseContext.fieldMapper(fField);
+                            if (fieldType != null) {
+                                fieldsAndWeights.put(fieldType.names().indexName(), fBoost);
+                            } else {
+                                fieldsAndWeights.put(fField, fBoost);
+                            }
+                        }
                     }
                 } else {
-                    throw new QueryParsingException(parseContext, "[" + SimpleQueryStringBuilder.NAME + "] query does not support [" + currentFieldName + "]");
+                    throw new QueryParsingException(parseContext, "[" + NAME + "] query does not support [" + currentFieldName + "]");
                 }
             } else if (token.isValue()) {
                 if ("query".equals(currentFieldName)) {
@@ -122,9 +146,19 @@ public class SimpleQueryStringParser extends BaseQueryParser<SimpleQueryStringBu
                 } else if ("boost".equals(currentFieldName)) {
                     boost = parser.floatValue();
                 } else if ("analyzer".equals(currentFieldName)) {
-                    analyzerName = parser.text();
+                    analyzer = parseContext.analysisService().analyzer(parser.text());
+                    if (analyzer == null) {
+                        throw new QueryParsingException(parseContext, "[" + NAME + "] analyzer [" + parser.text() + "] not found");
+                    }
                 } else if ("default_operator".equals(currentFieldName) || "defaultOperator".equals(currentFieldName)) {
-                    defaultOperator = Operator.fromString(parser.text());
+                    String op = parser.text();
+                    if ("or".equalsIgnoreCase(op)) {
+                        defaultOperator = BooleanClause.Occur.SHOULD;
+                    } else if ("and".equalsIgnoreCase(op)) {
+                        defaultOperator = BooleanClause.Occur.MUST;
+                    } else {
+                        throw new QueryParsingException(parseContext, "[" + NAME + "] default operator [" + op + "] is not allowed");
+                    }
                 } else if ("flags".equals(currentFieldName)) {
                     if (parser.currentToken() != XContentParser.Token.VALUE_NUMBER) {
                         // Possible options are:
@@ -138,38 +172,56 @@ public class SimpleQueryStringParser extends BaseQueryParser<SimpleQueryStringBu
                     }
                 } else if ("locale".equals(currentFieldName)) {
                     String localeStr = parser.text();
-                    locale = Locale.forLanguageTag(localeStr);
+                    Locale locale = LocaleUtils.parse(localeStr);
+                    sqsSettings.locale(locale);
                 } else if ("lowercase_expanded_terms".equals(currentFieldName)) {
-                    lowercaseExpandedTerms = parser.booleanValue();
+                    sqsSettings.lowercaseExpandedTerms(parser.booleanValue());
                 } else if ("lenient".equals(currentFieldName)) {
-                    lenient = parser.booleanValue();
+                    sqsSettings.lenient(parser.booleanValue());
                 } else if ("analyze_wildcard".equals(currentFieldName)) {
-                    analyzeWildcard = parser.booleanValue();
+                    sqsSettings.analyzeWildcard(parser.booleanValue());
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
                 } else if ("minimum_should_match".equals(currentFieldName)) {
                     minimumShouldMatch = parser.textOrNull();
                 } else {
-                    throw new QueryParsingException(parseContext, "[" + SimpleQueryStringBuilder.NAME + "] unsupported field [" + parser.currentName() + "]");
+                    throw new QueryParsingException(parseContext, "[" + NAME + "] unsupported field [" + parser.currentName() + "]");
                 }
             }
         }
 
         // Query text is required
         if (queryBody == null) {
-            throw new QueryParsingException(parseContext, "[" + SimpleQueryStringBuilder.NAME + "] query text missing");
+            throw new QueryParsingException(parseContext, "[" + NAME + "] query text missing");
+        }
+
+        // Use standard analyzer by default
+        if (analyzer == null) {
+            analyzer = parseContext.mapperService().searchAnalyzer();
         }
 
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder(queryBody);
-        qb.boost(boost).fields(fieldsAndWeights).analyzer(analyzerName).queryName(queryName).minimumShouldMatch(minimumShouldMatch);
-        qb.flags(flags).defaultOperator(defaultOperator).locale(locale).lowercaseExpandedTerms(lowercaseExpandedTerms);
-        qb.lenient(lenient).analyzeWildcard(analyzeWildcard).boost(boost);
+        if (fieldsAndWeights == null) {
+            fieldsAndWeights = Collections.singletonMap(parseContext.defaultField(), 1.0F);
+        }
+        SimpleQueryParser sqp = new SimpleQueryParser(analyzer, fieldsAndWeights, flags, sqsSettings);
 
-        return qb;
-    }
+        if (defaultOperator != null) {
+            sqp.setDefaultOperator(defaultOperator);
+        }
 
-    @Override
-    public SimpleQueryStringBuilder getBuilderPrototype() {
-        return SimpleQueryStringBuilder.PROTOTYPE;
+        Query query = sqp.parse(queryBody);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+
+        if (minimumShouldMatch != null && query instanceof BooleanQuery) {
+            query = Queries.applyMinimumShouldMatch((BooleanQuery) query, minimumShouldMatch);
+        }
+
+        if (query != null) {
+            query.setBoost(boost);
+        }
+
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java
index 81aa01c..0b7a3cd 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java
@@ -19,111 +19,74 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanContainingQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * Builder for {@link org.apache.lucene.search.spans.SpanContainingQuery}.
  */
-public class SpanContainingQueryBuilder extends AbstractQueryBuilder<SpanContainingQueryBuilder> implements SpanQueryBuilder<SpanContainingQueryBuilder> {
+public class SpanContainingQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanContainingQueryBuilder> {
 
-    public static final String NAME = "span_containing";
-    private final SpanQueryBuilder big;
-    private final SpanQueryBuilder little;
-    static final SpanContainingQueryBuilder PROTOTYPE = new SpanContainingQueryBuilder(null, null);
+    private SpanQueryBuilder big;
+    private SpanQueryBuilder little;
+    private float boost = -1;
+    private String queryName;
 
-    /**
-     * @param big the big clause, it must enclose {@code little} for a match.
-     * @param little the little clause, it must be contained within {@code big} for a match.
+    /** 
+     * Sets the little clause, it must be contained within {@code big} for a match.
      */
-    public SpanContainingQueryBuilder(SpanQueryBuilder big, SpanQueryBuilder little) {
-        this.little = little;
-        this.big = big;
+    public SpanContainingQueryBuilder little(SpanQueryBuilder clause) {
+        this.little = clause;
+        return this;
     }
 
-    /**
-     * @return the big clause, it must enclose {@code little} for a match.
+    /** 
+     * Sets the big clause, it must enclose {@code little} for a match.
      */
-    public SpanQueryBuilder bigQuery() {
-        return this.big;
-    }
-
-    /**
-     * @return the little clause, it must be contained within {@code big} for a match.
-     */
-    public SpanQueryBuilder littleQuery() {
-        return this.little;
+    public SpanContainingQueryBuilder big(SpanQueryBuilder clause) {
+        this.big = clause;
+        return this;
     }
 
     @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("big");
-        big.toXContent(builder, params);
-        builder.field("little");
-        little.toXContent(builder, params);
-        printBoostAndQueryName(builder);
-        builder.endObject();
+    public SpanContainingQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query innerBig = big.toQuery(context);
-        assert innerBig instanceof SpanQuery;
-        Query innerLittle = little.toQuery(context);
-        assert innerLittle instanceof SpanQuery;
-        return new SpanContainingQuery((SpanQuery) innerBig, (SpanQuery) innerLittle);
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public SpanContainingQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
+    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
         if (big == null) {
-            validationException = addValidationError("inner clause [big] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(big, validationException);
+            throw new IllegalArgumentException("Must specify big clause when building a span_containing query");
         }
         if (little == null) {
-            validationException = addValidationError("inner clause [little] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(little, validationException);
+            throw new IllegalArgumentException("Must specify little clause when building a span_containing query");
         }
-        return validationException;
-    }
+        builder.startObject(SpanContainingQueryParser.NAME);
 
-    @Override
-    protected SpanContainingQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        SpanQueryBuilder big = (SpanQueryBuilder)in.readQuery();
-        SpanQueryBuilder little = (SpanQueryBuilder)in.readQuery();
-        return new SpanContainingQueryBuilder(big, little);
-    }
+        builder.field("big");
+        big.toXContent(builder, params);
 
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(big);
-        out.writeQuery(little);
-    }
+        builder.field("little");
+        little.toXContent(builder, params);
 
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(big, little);
-    }
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
 
-    @Override
-    protected boolean doEquals(SpanContainingQueryBuilder other) {
-        return Objects.equals(big, other.big) &&
-               Objects.equals(little, other.little);
-    }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java
index affc853..63e312b 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java
@@ -19,6 +19,9 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanContainingQuery;
+import org.apache.lucene.search.spans.SpanQuery;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
@@ -26,9 +29,11 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import java.io.IOException;
 
 /**
- * Parser for span_containing query
+ * Parser for {@link SpanContainingQuery}
  */
-public class SpanContainingQueryParser extends BaseQueryParser<SpanContainingQueryBuilder> {
+public class SpanContainingQueryParser implements QueryParser {
+
+    public static final String NAME = "span_containing";
 
     @Inject
     public SpanContainingQueryParser() {
@@ -36,16 +41,17 @@ public class SpanContainingQueryParser extends BaseQueryParser<SpanContainingQue
 
     @Override
     public String[] names() {
-        return new String[]{SpanContainingQueryBuilder.NAME, Strings.toCamelCase(SpanContainingQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SpanContainingQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+
+        float boost = 1.0f;
         String queryName = null;
-        SpanQueryBuilder<?> big = null;
-        SpanQueryBuilder<?> little = null;
+        SpanQuery big = null;
+        SpanQuery little = null;
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -54,17 +60,17 @@ public class SpanContainingQueryParser extends BaseQueryParser<SpanContainingQue
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("big".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (!(query instanceof SpanQueryBuilder<?>)) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (!(query instanceof SpanQuery)) {
                         throw new QueryParsingException(parseContext, "span_containing [big] must be of type span query");
                     }
-                    big = (SpanQueryBuilder<?>) query;
+                    big = (SpanQuery) query;
                 } else if ("little".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (!(query instanceof SpanQueryBuilder<?>)) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (!(query instanceof SpanQuery)) {
                         throw new QueryParsingException(parseContext, "span_containing [little] must be of type span query");
                     }
-                    little = (SpanQueryBuilder<?>) query;
+                    little = (SpanQuery) query;
                 } else {
                     throw new QueryParsingException(parseContext, "[span_containing] query does not support [" + currentFieldName + "]");
                 }
@@ -75,15 +81,20 @@ public class SpanContainingQueryParser extends BaseQueryParser<SpanContainingQue
             } else {
                 throw new QueryParsingException(parseContext, "[span_containing] query does not support [" + currentFieldName + "]");
             }
+        }        
+        
+        if (big == null) {
+            throw new QueryParsingException(parseContext, "span_containing must include [big]");
+        }
+        if (little == null) {
+            throw new QueryParsingException(parseContext, "span_containing must include [little]");
         }
 
-        SpanContainingQueryBuilder query = new SpanContainingQueryBuilder(big, little);
-        query.boost(boost).queryName(queryName);
+        Query query = new SpanContainingQuery(big, little);
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
         return query;
     }
-
-    @Override
-    public SpanContainingQueryBuilder getBuilderPrototype() {
-        return SpanContainingQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java
index a7c4572..f967a1c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java
@@ -19,109 +19,51 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanFirstQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
-public class SpanFirstQueryBuilder extends AbstractQueryBuilder<SpanFirstQueryBuilder> implements SpanQueryBuilder<SpanFirstQueryBuilder>{
-
-    public static final String NAME = "span_first";
+public class SpanFirstQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanFirstQueryBuilder> {
 
     private final SpanQueryBuilder matchBuilder;
 
     private final int end;
 
-    static final SpanFirstQueryBuilder PROTOTYPE = new SpanFirstQueryBuilder(null, -1);
+    private float boost = -1;
+
+    private String queryName;
 
-    /**
-     * Query that matches spans queries defined in <code>matchBuilder</code>
-     * whose end position is less than or equal to <code>end</code>.
-     * @param matchBuilder inner {@link SpanQueryBuilder}
-     * @param end maximum end position of the match, needs to be positive
-     * @throws IllegalArgumentException for negative <code>end</code> positions
-     */
     public SpanFirstQueryBuilder(SpanQueryBuilder matchBuilder, int end) {
         this.matchBuilder = matchBuilder;
         this.end = end;
     }
 
-    /**
-     * @return the inner {@link SpanQueryBuilder} defined in this query
-     */
-    public SpanQueryBuilder innerQuery() {
-        return this.matchBuilder;
+    @Override
+    public SpanFirstQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
-     * @return maximum end position of the matching inner span query
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public int end() {
-        return this.end;
+    public SpanFirstQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(SpanFirstQueryParser.NAME);
         builder.field("match");
         matchBuilder.toXContent(builder, params);
         builder.field("end", end);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query innerSpanQuery = matchBuilder.toQuery(context);
-        assert innerSpanQuery instanceof SpanQuery;
-        return new SpanFirstQuery((SpanQuery) innerSpanQuery, end);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (matchBuilder == null) {
-            validationException = addValidationError("inner clause [match] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(matchBuilder, validationException);
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        if (end < 0) {
-            validationException = addValidationError("parameter [end] needs to be positive.", validationException);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return validationException;
-    }
-
-    @Override
-    protected SpanFirstQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        SpanQueryBuilder matchBuilder = (SpanQueryBuilder)in.readQuery();
-        int end = in.readInt();
-        return new SpanFirstQueryBuilder(matchBuilder, end);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(matchBuilder);
-        out.writeInt(end);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(matchBuilder, end);
-    }
-
-    @Override
-    protected boolean doEquals(SpanFirstQueryBuilder other) {
-        return Objects.equals(matchBuilder, other.matchBuilder) &&
-               Objects.equals(end, other.end);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java
index 995bb4d..5a302eb 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java
@@ -19,6 +19,9 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanFirstQuery;
+import org.apache.lucene.search.spans.SpanQuery;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
@@ -26,9 +29,11 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import java.io.IOException;
 
 /**
- * Parser for span_first query
+ *
  */
-public class SpanFirstQueryParser extends BaseQueryParser<SpanFirstQueryBuilder> {
+public class SpanFirstQueryParser implements QueryParser {
+
+    public static final String NAME = "span_first";
 
     @Inject
     public SpanFirstQueryParser() {
@@ -36,17 +41,17 @@ public class SpanFirstQueryParser extends BaseQueryParser<SpanFirstQueryBuilder>
 
     @Override
     public String[] names() {
-        return new String[]{SpanFirstQueryBuilder.NAME, Strings.toCamelCase(SpanFirstQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SpanFirstQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
 
-        SpanQueryBuilder match = null;
-        Integer end = null;
+        SpanQuery match = null;
+        int end = -1;
         String queryName = null;
 
         String currentFieldName = null;
@@ -56,11 +61,11 @@ public class SpanFirstQueryParser extends BaseQueryParser<SpanFirstQueryBuilder>
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("match".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (!(query instanceof SpanQueryBuilder)) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (!(query instanceof SpanQuery)) {
                         throw new QueryParsingException(parseContext, "spanFirst [match] must be of type span query");
                     }
-                    match = (SpanQueryBuilder) query;
+                    match = (SpanQuery) query;
                 } else {
                     throw new QueryParsingException(parseContext, "[span_first] query does not support [" + currentFieldName + "]");
                 }
@@ -79,16 +84,15 @@ public class SpanFirstQueryParser extends BaseQueryParser<SpanFirstQueryBuilder>
         if (match == null) {
             throw new QueryParsingException(parseContext, "spanFirst must have [match] span query clause");
         }
-        if (end == null) {
+        if (end == -1) {
             throw new QueryParsingException(parseContext, "spanFirst must have [end] set for it");
         }
-        SpanFirstQueryBuilder queryBuilder = new SpanFirstQueryBuilder(match, end);
-        queryBuilder.boost(boost).queryName(queryName);
-        return queryBuilder;
-    }
 
-    @Override
-    public SpanFirstQueryBuilder getBuilderPrototype() {
-        return SpanFirstQueryBuilder.PROTOTYPE;
+        SpanFirstQuery query = new SpanFirstQuery(match, end);
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java
index a31b17e..11b9897 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java
@@ -18,88 +18,25 @@
  */
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanMultiTermQueryWrapper;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
-/**
- * Query that allows wraping a {@link MultiTermQueryBuilder} (one of wildcard, fuzzy, prefix, term, range or regexp query)
- * as a {@link SpanQueryBuilder} so it can be nested.
- */
-public class SpanMultiTermQueryBuilder extends AbstractQueryBuilder<SpanMultiTermQueryBuilder> implements SpanQueryBuilder<SpanMultiTermQueryBuilder> {
+public class SpanMultiTermQueryBuilder extends SpanQueryBuilder {
 
-    public static final String NAME = "span_multi";
-    private final MultiTermQueryBuilder multiTermQueryBuilder;
-    static final SpanMultiTermQueryBuilder PROTOTYPE = new SpanMultiTermQueryBuilder(null);
+    private MultiTermQueryBuilder multiTermQueryBuilder;
 
     public SpanMultiTermQueryBuilder(MultiTermQueryBuilder multiTermQueryBuilder) {
         this.multiTermQueryBuilder = multiTermQueryBuilder;
     }
 
-    public MultiTermQueryBuilder innerQuery() {
-        return this.multiTermQueryBuilder;
-    }
-
     @Override
     protected void doXContent(XContentBuilder builder, Params params)
             throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(SpanMultiTermQueryParser.NAME);
         builder.field(SpanMultiTermQueryParser.MATCH_NAME);
         multiTermQueryBuilder.toXContent(builder, params);
-        printBoostAndQueryName(builder);
         builder.endObject();
     }
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query subQuery = multiTermQueryBuilder.toQuery(context);
-        if (subQuery instanceof MultiTermQuery == false) {
-            throw new UnsupportedOperationException("unsupported inner query, should be " + MultiTermQuery.class.getName() +" but was "
-                    + subQuery.getClass().getName());
-        }
-        return new SpanMultiTermQueryWrapper<>((MultiTermQuery) subQuery);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (multiTermQueryBuilder == null) {
-            validationException = addValidationError("inner clause ["+ SpanMultiTermQueryParser.MATCH_NAME +"] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(multiTermQueryBuilder, validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    protected SpanMultiTermQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        MultiTermQueryBuilder multiTermBuilder = (MultiTermQueryBuilder)in.readQuery();
-        return new SpanMultiTermQueryBuilder(multiTermBuilder);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(multiTermQueryBuilder);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(multiTermQueryBuilder);
-    }
-
-    @Override
-    protected boolean doEquals(SpanMultiTermQueryBuilder other) {
-        return Objects.equals(multiTermQueryBuilder, other.multiTermQueryBuilder);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryParser.java
index 77e9def..a44580a 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryParser.java
@@ -18,17 +18,22 @@
  */
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.MultiTermQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanMultiTermQueryWrapper;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.common.xcontent.XContentParser.Token;
 
 import java.io.IOException;
 
 /**
- * Parser for span_multi query
+ *
  */
-public class SpanMultiTermQueryParser extends BaseQueryParser<SpanMultiTermQueryBuilder> {
+public class SpanMultiTermQueryParser implements QueryParser {
 
+    public static final String NAME = "span_multi";
     public static final String MATCH_NAME = "match";
 
     @Inject
@@ -37,50 +42,29 @@ public class SpanMultiTermQueryParser extends BaseQueryParser<SpanMultiTermQuery
 
     @Override
     public String[] names() {
-        return new String[]{SpanMultiTermQueryBuilder.NAME, Strings.toCamelCase(SpanMultiTermQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SpanMultiTermQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
-        String currentFieldName = null;
-        MultiTermQueryBuilder subQuery = null;
-        String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        XContentParser.Token token;
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                currentFieldName = parser.currentName();
-            } else if (token == XContentParser.Token.START_OBJECT) {
-                if (MATCH_NAME.equals(currentFieldName)) {
-                    QueryBuilder innerQuery = parseContext.parseInnerQueryBuilder();
-                    if (innerQuery instanceof MultiTermQueryBuilder == false) {
-                        throw new QueryParsingException(parseContext, "[span_multi] [" + MATCH_NAME + "] must be of type multi term query");
-                    }
-                    subQuery = (MultiTermQueryBuilder) innerQuery;
-                } else {
-                    throw new QueryParsingException(parseContext, "[span_multi] query does not support [" + currentFieldName + "]");
-                }
-            } else if (token.isValue()) {
-                if ("_name".equals(currentFieldName)) {
-                    queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
-                } else {
-                    throw new QueryParsingException(parseContext, "[span_multi] query does not support [" + currentFieldName + "]");
-                }
-            }
+
+        Token token = parser.nextToken();
+        if (!MATCH_NAME.equals(parser.currentName()) || token != XContentParser.Token.FIELD_NAME) {
+            throw new QueryParsingException(parseContext, "spanMultiTerm must have [" + MATCH_NAME + "] multi term query clause");
         }
 
-        if (subQuery == null) {
-            throw new QueryParsingException(parseContext, "[span_multi] must have [" + MATCH_NAME + "] multi term query clause");
+        token = parser.nextToken();
+        if (token != XContentParser.Token.START_OBJECT) {
+            throw new QueryParsingException(parseContext, "spanMultiTerm must have [" + MATCH_NAME + "] multi term query clause");
         }
 
-        return new SpanMultiTermQueryBuilder(subQuery).queryName(queryName).boost(boost);
-    }
+        Query subQuery = parseContext.parseInnerQuery();
+        if (!(subQuery instanceof MultiTermQuery)) {
+            throw new QueryParsingException(parseContext, "spanMultiTerm [" + MATCH_NAME + "] must be of type multi term query");
+        }
 
-    @Override
-    public SpanMultiTermQueryBuilder getBuilderPrototype() {
-        return SpanMultiTermQueryBuilder.PROTOTYPE;
+        parser.nextToken();
+        return new SpanMultiTermQueryWrapper<>((MultiTermQuery) subQuery);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java
index e00cc32..cb05e08 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java
@@ -19,179 +19,86 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanNearQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.List;
-import java.util.Objects;
 
-/**
- * Matches spans which are near one another. One can specify slop, the maximum number
- * of intervening unmatched positions, as well as whether matches are required to be in-order.
- * The span near query maps to Lucene {@link SpanNearQuery}.
- */
-public class SpanNearQueryBuilder extends AbstractQueryBuilder<SpanNearQueryBuilder> implements SpanQueryBuilder<SpanNearQueryBuilder> {
-
-    public static final String NAME = "span_near";
-
-    /** Default for flag controlling whether matches are required to be in-order */
-    public static boolean DEFAULT_IN_ORDER = true;
-
-    /** Default for flag controlling whether payloads are collected */
-    public static boolean DEFAULT_COLLECT_PAYLOADS = true;
+public class SpanNearQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanNearQueryBuilder> {
 
-    private final List<SpanQueryBuilder> clauses = new ArrayList<>();
+    private ArrayList<SpanQueryBuilder> clauses = new ArrayList<>();
 
-    private final int slop;
+    private Integer slop = null;
 
-    private boolean inOrder = DEFAULT_IN_ORDER;
+    private Boolean inOrder;
 
-    private boolean collectPayloads = DEFAULT_COLLECT_PAYLOADS;
+    private Boolean collectPayloads;
 
-    static final SpanNearQueryBuilder PROTOTYPE = new SpanNearQueryBuilder(0);
+    private float boost = -1;
 
-    /**
-     * @param slop controls the maximum number of intervening unmatched positions permitted
-     */
-    public SpanNearQueryBuilder(int slop) {
-        this.slop = slop;
-    }
-
-    /**
-     * @return the maximum number of intervening unmatched positions permitted
-     */
-    public int slop() {
-        return this.slop;
-    }
+    private String queryName;
 
     public SpanNearQueryBuilder clause(SpanQueryBuilder clause) {
         clauses.add(clause);
         return this;
     }
 
-    /**
-     * @return the {@link SpanQueryBuilder} clauses that were set for this query
-     */
-    public List<SpanQueryBuilder> clauses() {
-        return this.clauses;
+    public SpanNearQueryBuilder slop(int slop) {
+        this.slop = slop;
+        return this;
     }
 
-    /**
-     * When <code>inOrder</code> is true, the spans from each clause
-     * must be in the same order as in <code>clauses</code> and must be non-overlapping.
-     * Defaults to <code>true</code>
-     */
     public SpanNearQueryBuilder inOrder(boolean inOrder) {
         this.inOrder = inOrder;
         return this;
     }
 
-    /**
-     * @see SpanNearQueryBuilder#inOrder(boolean))
-     */
-    public boolean inOrder() {
-        return this.inOrder;
-    }
-
-    /**
-     * @param collectPayloads flag controlling whether payloads are collected
-     */
     public SpanNearQueryBuilder collectPayloads(boolean collectPayloads) {
         this.collectPayloads = collectPayloads;
         return this;
     }
 
+    @Override
+    public SpanNearQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
     /**
-     * @see SpanNearQueryBuilder#collectPayloads(boolean))
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public boolean collectPayloads() {
-        return this.collectPayloads;
+    public SpanNearQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        if (clauses.isEmpty()) {
+            throw new IllegalArgumentException("Must have at least one clause when building a spanNear query");
+        }
+        if (slop == null) {
+            throw new IllegalArgumentException("Must set the slop when building a spanNear query");
+        }
+        builder.startObject(SpanNearQueryParser.NAME);
         builder.startArray("clauses");
         for (SpanQueryBuilder clause : clauses) {
             clause.toXContent(builder, params);
         }
         builder.endArray();
-        builder.field("slop", slop);
-        builder.field("in_order", inOrder);
-        builder.field("collect_payloads", collectPayloads);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        SpanQuery[] spanQueries = new SpanQuery[clauses.size()];
-        for (int i = 0; i < clauses.size(); i++) {
-            Query query = clauses.get(i).toQuery(context);
-            assert query instanceof SpanQuery;
-            spanQueries[i] = (SpanQuery) query;
+        builder.field("slop", slop.intValue());
+        if (inOrder != null) {
+            builder.field("in_order", inOrder);
         }
-        return new SpanNearQuery(spanQueries, slop, inOrder, collectPayloads);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (clauses.isEmpty()) {
-            validationException =  addValidationError("query must include [clauses]", validationException);
+        if (collectPayloads != null) {
+            builder.field("collect_payloads", collectPayloads);
         }
-        for (SpanQueryBuilder innerClause : clauses) {
-            if (innerClause == null) {
-                validationException =  addValidationError("[clauses] contains null element", validationException);
-            } else {
-                validationException = validateInnerQuery(innerClause, validationException);
-            }
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        return validationException;
-    }
-
-    @Override
-    protected SpanNearQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        SpanNearQueryBuilder queryBuilder = new SpanNearQueryBuilder(in.readVInt());
-        List<QueryBuilder> clauses = readQueries(in);
-        for (QueryBuilder subClause : clauses) {
-            queryBuilder.clauses.add((SpanQueryBuilder)subClause);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        queryBuilder.collectPayloads = in.readBoolean();
-        queryBuilder.inOrder = in.readBoolean();
-        return queryBuilder;
-
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeVInt(slop);
-        writeQueries(out, clauses);
-        out.writeBoolean(collectPayloads);
-        out.writeBoolean(inOrder);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(clauses, slop, collectPayloads, inOrder);
-    }
-
-    @Override
-    protected boolean doEquals(SpanNearQueryBuilder other) {
-        return Objects.equals(clauses, other.clauses) &&
-               Objects.equals(slop, other.slop) &&
-               Objects.equals(collectPayloads, other.collectPayloads) &&
-               Objects.equals(inOrder, other.inOrder);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java
index cc6d8bc..506bce2 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java
@@ -19,6 +19,9 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanNearQuery;
+import org.apache.lucene.search.spans.SpanQuery;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
@@ -28,9 +31,11 @@ import java.util.ArrayList;
 import java.util.List;
 
 /**
- * Parser for span_near query
+ *
  */
-public class SpanNearQueryParser extends BaseQueryParser<SpanNearQueryBuilder> {
+public class SpanNearQueryParser implements QueryParser {
+
+    public static final String NAME = "span_near";
 
     @Inject
     public SpanNearQueryParser() {
@@ -38,20 +43,20 @@ public class SpanNearQueryParser extends BaseQueryParser<SpanNearQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{SpanNearQueryBuilder.NAME, Strings.toCamelCase(SpanNearQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SpanNearQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         Integer slop = null;
-        boolean inOrder = SpanNearQueryBuilder.DEFAULT_IN_ORDER;
-        boolean collectPayloads = SpanNearQueryBuilder.DEFAULT_COLLECT_PAYLOADS;
+        boolean inOrder = true;
+        boolean collectPayloads = true;
         String queryName = null;
 
-        List<SpanQueryBuilder> clauses = new ArrayList<>();
+        List<SpanQuery> clauses = new ArrayList<>();
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -61,11 +66,11 @@ public class SpanNearQueryParser extends BaseQueryParser<SpanNearQueryBuilder> {
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if ("clauses".equals(currentFieldName)) {
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                        if (!(query instanceof SpanQueryBuilder)) {
+                        Query query = parseContext.parseInnerQuery();
+                        if (!(query instanceof SpanQuery)) {
                             throw new QueryParsingException(parseContext, "spanNear [clauses] must be of type span query");
                         }
-                        clauses.add((SpanQueryBuilder) query);
+                        clauses.add((SpanQuery) query);
                     }
                 } else {
                     throw new QueryParsingException(parseContext, "[span_near] query does not support [" + currentFieldName + "]");
@@ -76,7 +81,7 @@ public class SpanNearQueryParser extends BaseQueryParser<SpanNearQueryBuilder> {
                 } else if ("collect_payloads".equals(currentFieldName) || "collectPayloads".equals(currentFieldName)) {
                     collectPayloads = parser.booleanValue();
                 } else if ("slop".equals(currentFieldName)) {
-                    slop = parser.intValue();
+                    slop = Integer.valueOf(parser.intValue());
                 } else if ("boost".equals(currentFieldName)) {
                     boost = parser.floatValue();
                 } else if ("_name".equals(currentFieldName)) {
@@ -88,24 +93,18 @@ public class SpanNearQueryParser extends BaseQueryParser<SpanNearQueryBuilder> {
                 throw new QueryParsingException(parseContext, "[span_near] query does not support [" + currentFieldName + "]");
             }
         }
-
+        if (clauses.isEmpty()) {
+            throw new QueryParsingException(parseContext, "span_near must include [clauses]");
+        }
         if (slop == null) {
             throw new QueryParsingException(parseContext, "span_near must include [slop]");
         }
 
-        SpanNearQueryBuilder queryBuilder = new SpanNearQueryBuilder(slop);
-        for (SpanQueryBuilder subQuery : clauses) {
-            queryBuilder.clause(subQuery);
+        SpanNearQuery query = new SpanNearQuery(clauses.toArray(new SpanQuery[clauses.size()]), slop.intValue(), inOrder, collectPayloads);
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
         }
-        queryBuilder.inOrder(inOrder);
-        queryBuilder.collectPayloads(collectPayloads);
-        queryBuilder.boost(boost);
-        queryBuilder.queryName(queryName);
-        return queryBuilder;
-    }
-
-    @Override
-    public SpanNearQueryBuilder getBuilderPrototype() {
-        return SpanNearQueryBuilder.PROTOTYPE;
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java
index 3af88e3..e37cd80 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java
@@ -19,176 +19,100 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanNotQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
-public class SpanNotQueryBuilder extends AbstractQueryBuilder<SpanNotQueryBuilder> implements SpanQueryBuilder<SpanNotQueryBuilder> {
+public class SpanNotQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanNotQueryBuilder> {
 
-    public static final String NAME = "span_not";
+    private SpanQueryBuilder include;
 
-    /** the default pre parameter size */
-    public static final int DEFAULT_PRE = 0;
-    /** the default post parameter size */
-    public static final int DEFAULT_POST = 0;
+    private SpanQueryBuilder exclude;
 
-    private final SpanQueryBuilder include;
+    private Integer dist;
 
-    private final SpanQueryBuilder exclude;
+    private Integer pre;
 
-    private int pre = DEFAULT_PRE;
+    private Integer post;
 
-    private int post = DEFAULT_POST;
+    private Float boost;
 
-    static final SpanNotQueryBuilder PROTOTYPE = new SpanNotQueryBuilder(null, null);
+    private String queryName;
 
-    /**
-     * Construct a span query matching spans from <code>include</code> which
-     * have no overlap with spans from <code>exclude</code>.
-     * @param include the span query whose matches are filtered
-     * @param exclude the span query whose matches must not overlap
-     */
-    public SpanNotQueryBuilder(SpanQueryBuilder include, SpanQueryBuilder exclude) {
+    public SpanNotQueryBuilder include(SpanQueryBuilder include) {
         this.include = include;
-        this.exclude = exclude;
-    }
-
-    /**
-     * @return the span query whose matches are filtered
-     */
-    public SpanQueryBuilder includeQuery() {
-        return this.include;
+        return this;
     }
 
-    /**
-     * @return the span query whose matches must not overlap
-     */
-    public SpanQueryBuilder excludeQuery() {
-        return this.exclude;
+    public SpanNotQueryBuilder exclude(SpanQueryBuilder exclude) {
+        this.exclude = exclude;
+        return this;
     }
 
-    /**
-     * @param dist the amount of tokens from within the include span cant have overlap with the exclude span.
-     * Equivalent to setting both pre and post parameter.
-     */
     public SpanNotQueryBuilder dist(int dist) {
-        pre(dist);
-        post(dist);
+        this.dist = dist;
         return this;
     }
 
-    /**
-     * @param pre the amount of tokens before the include span that cant have overlap with the exclude span. Values
-     * smaller than 0 will be ignored and 0 used instead.
-     */
     public SpanNotQueryBuilder pre(int pre) {
-        this.pre = (pre >= 0) ? pre : 0;
+        this.pre = (pre >=0) ? pre : 0;
         return this;
     }
 
-    /**
-     * @return the amount of tokens before the include span that cant have overlap with the exclude span.
-     * @see SpanNotQueryBuilder#pre(int)
-     */
-    public Integer pre() {
-        return this.pre;
-    }
-
-    /**
-     * @param post the amount of tokens after the include span that cant have overlap with the exclude span.
-     */
     public SpanNotQueryBuilder post(int post) {
         this.post = (post >= 0) ? post : 0;
         return this;
     }
 
-    /**
-     * @return the amount of tokens after the include span that cant have overlap with the exclude span.
-     * @see SpanNotQueryBuilder#post(int)
-     */
-    public Integer post() {
-        return this.post;
-    }
-
     @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("include");
-        include.toXContent(builder, params);
-        builder.field("exclude");
-        exclude.toXContent(builder, params);
-        builder.field("pre", pre);
-        builder.field("post", post);
-        printBoostAndQueryName(builder);
-        builder.endObject();
+    public SpanNotQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-
-        Query includeQuery = this.include.toQuery(context);
-        assert includeQuery instanceof SpanQuery;
-        Query excludeQuery = this.exclude.toQuery(context);
-        assert excludeQuery instanceof SpanQuery;
-
-        return new SpanNotQuery((SpanQuery) includeQuery, (SpanQuery) excludeQuery, pre, post);
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     * @param queryName The query name
+     * @return this
+     */
+    public SpanNotQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
+    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
         if (include == null) {
-            validationException = addValidationError("inner clause [include] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(include, validationException);
+            throw new IllegalArgumentException("Must specify include when using spanNot query");
         }
         if (exclude == null) {
-            validationException = addValidationError("inner clause [exclude] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(exclude, validationException);
+            throw new IllegalArgumentException("Must specify exclude when using spanNot query");
         }
-        return validationException;
-    }
-
-    @Override
-    protected SpanNotQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        SpanQueryBuilder include = (SpanQueryBuilder)in.readQuery();
-        SpanQueryBuilder exclude = (SpanQueryBuilder)in.readQuery();
-        SpanNotQueryBuilder queryBuilder = new SpanNotQueryBuilder(include, exclude);
-        queryBuilder.pre(in.readVInt());
-        queryBuilder.post(in.readVInt());
-        return queryBuilder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(include);
-        out.writeQuery(exclude);
-        out.writeVInt(pre);
-        out.writeVInt(post);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(include, exclude, pre, post);
-    }
 
-    @Override
-    protected boolean doEquals(SpanNotQueryBuilder other) {
-        return Objects.equals(include, other.include) &&
-               Objects.equals(exclude, other.exclude) &&
-               (pre == other.pre) &&
-               (post == other.post);
-    }
+        if (dist != null && (pre != null || post != null)) {
+             throw new IllegalArgumentException("spanNot can either use [dist] or [pre] & [post] (or none)");
+        }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.startObject(SpanNotQueryParser.NAME);
+        builder.field("include");
+        include.toXContent(builder, params);
+        builder.field("exclude");
+        exclude.toXContent(builder, params);
+        if (dist != null) {
+            builder.field("dist", dist);
+        }
+        if (pre != null) {
+            builder.field("pre", pre);
+        }
+        if (post != null) {
+            builder.field("post", post);
+        }
+        if (boost != null) {
+            builder.field("boost", boost);
+        }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java
index bc9ee51..bcb62e7 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java
@@ -19,6 +19,9 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanNotQuery;
+import org.apache.lucene.search.spans.SpanQuery;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
@@ -26,9 +29,11 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import java.io.IOException;
 
 /**
- * Parser for span_not query
+ *
  */
-public class SpanNotQueryParser extends BaseQueryParser<SpanNotQueryBuilder> {
+public class SpanNotQueryParser implements QueryParser {
+
+    public static final String NAME = "span_not";
 
     @Inject
     public SpanNotQueryParser() {
@@ -36,17 +41,17 @@ public class SpanNotQueryParser extends BaseQueryParser<SpanNotQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{SpanNotQueryBuilder.NAME, Strings.toCamelCase(SpanNotQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SpanNotQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
 
-        SpanQueryBuilder include = null;
-        SpanQueryBuilder exclude = null;
+        SpanQuery include = null;
+        SpanQuery exclude = null;
 
         Integer dist = null;
         Integer pre  = null;
@@ -61,17 +66,17 @@ public class SpanNotQueryParser extends BaseQueryParser<SpanNotQueryBuilder> {
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("include".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (!(query instanceof SpanQueryBuilder)) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (!(query instanceof SpanQuery)) {
                         throw new QueryParsingException(parseContext, "spanNot [include] must be of type span query");
                     }
-                    include = (SpanQueryBuilder) query;
+                    include = (SpanQuery) query;
                 } else if ("exclude".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (!(query instanceof SpanQueryBuilder)) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (!(query instanceof SpanQuery)) {
                         throw new QueryParsingException(parseContext, "spanNot [exclude] must be of type span query");
                     }
-                    exclude = (SpanQueryBuilder) query;
+                    exclude = (SpanQuery) query;
                 } else {
                     throw new QueryParsingException(parseContext, "[span_not] query does not support [" + currentFieldName + "]");
                 }
@@ -101,23 +106,26 @@ public class SpanNotQueryParser extends BaseQueryParser<SpanNotQueryBuilder> {
             throw new QueryParsingException(parseContext, "spanNot can either use [dist] or [pre] & [post] (or none)");
         }
 
-        SpanNotQueryBuilder spanNotQuery = new SpanNotQueryBuilder(include, exclude);
-        if (dist != null) {
-            spanNotQuery.dist(dist);
+        // set appropriate defaults
+        if (pre != null && post == null) {
+            post = 0;
+        } else if (pre == null && post != null){
+            pre = 0;
         }
-        if (pre != null) {
-            spanNotQuery.pre(pre);
-        }
-        if (post != null) {
-            spanNotQuery.post(post);
+
+        SpanNotQuery query;
+        if (pre != null && post != null) {
+            query = new SpanNotQuery(include, exclude, pre, post);
+        } else if (dist != null) {
+            query = new SpanNotQuery(include, exclude, dist);
+        } else {
+            query = new SpanNotQuery(include, exclude);
         }
-        spanNotQuery.boost(boost);
-        spanNotQuery.queryName(queryName);
-        return spanNotQuery;
-    }
 
-    @Override
-    public SpanNotQueryBuilder getBuilderPrototype() {
-        return SpanNotQueryBuilder.PROTOTYPE;
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java
index 8e9b7ae..0042aa7 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java
@@ -19,108 +19,55 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanOrQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.List;
-import java.util.Objects;
 
-/**
- * Span query that matches the union of its clauses. Maps to {@link SpanOrQuery}.
- */
-public class SpanOrQueryBuilder extends AbstractQueryBuilder<SpanOrQueryBuilder> implements SpanQueryBuilder<SpanOrQueryBuilder> {
+public class SpanOrQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanOrQueryBuilder> {
 
-    public static final String NAME = "span_or";
+    private ArrayList<SpanQueryBuilder> clauses = new ArrayList<>();
 
-    private final List<SpanQueryBuilder> clauses = new ArrayList<>();
+    private float boost = -1;
 
-    static final SpanOrQueryBuilder PROTOTYPE = new SpanOrQueryBuilder();
+    private String queryName;
 
     public SpanOrQueryBuilder clause(SpanQueryBuilder clause) {
         clauses.add(clause);
         return this;
     }
 
+    @Override
+    public SpanOrQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
     /**
-     * @return the {@link SpanQueryBuilder} clauses that were set for this query
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public List<SpanQueryBuilder> clauses() {
-        return this.clauses;
+    public SpanOrQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        if (clauses.isEmpty()) {
+            throw new IllegalArgumentException("Must have at least one clause when building a spanOr query");
+        }
+        builder.startObject(SpanOrQueryParser.NAME);
         builder.startArray("clauses");
         for (SpanQueryBuilder clause : clauses) {
             clause.toXContent(builder, params);
         }
         builder.endArray();
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        SpanQuery[] spanQueries = new SpanQuery[clauses.size()];
-        for (int i = 0; i < clauses.size(); i++) {
-            Query query = clauses.get(i).toQuery(context);
-            assert query instanceof SpanQuery;
-            spanQueries[i] = (SpanQuery) query;
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        return new SpanOrQuery(spanQueries);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (clauses.isEmpty()) {
-            validationException =  addValidationError("query must include [clauses]", validationException);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        for (SpanQueryBuilder innerClause : clauses) {
-            if (innerClause == null) {
-                validationException =  addValidationError("[clauses] contains null element", validationException);
-            } else {
-                validationException = validateInnerQuery(innerClause, validationException);
-            }
-        }
-        return validationException;
-    }
-
-    @Override
-    protected SpanOrQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        SpanOrQueryBuilder queryBuilder = new SpanOrQueryBuilder();
-        List<QueryBuilder> clauses = readQueries(in);
-        for (QueryBuilder subClause : clauses) {
-            queryBuilder.clauses.add((SpanQueryBuilder)subClause);
-        }
-        return queryBuilder;
-
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        writeQueries(out, clauses);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(clauses);
-    }
-
-    @Override
-    protected boolean doEquals(SpanOrQueryBuilder other) {
-        return Objects.equals(clauses, other.clauses);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java
index c424f66..e28a9cc 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java
@@ -19,7 +19,11 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanOrQuery;
+import org.apache.lucene.search.spans.SpanQuery;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
@@ -27,23 +31,29 @@ import java.util.ArrayList;
 import java.util.List;
 
 /**
- * Parser for span_or query
+ *
  */
-public class SpanOrQueryParser extends BaseQueryParser<SpanOrQueryBuilder> {
+public class SpanOrQueryParser implements QueryParser {
+
+    public static final String NAME = "span_or";
+
+    @Inject
+    public SpanOrQueryParser() {
+    }
 
     @Override
     public String[] names() {
-        return new String[]{SpanOrQueryBuilder.NAME, Strings.toCamelCase(SpanOrQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SpanOrQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         String queryName = null;
 
-        List<SpanQueryBuilder> clauses = new ArrayList<>();
+        List<SpanQuery> clauses = new ArrayList<>();
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -53,11 +63,11 @@ public class SpanOrQueryParser extends BaseQueryParser<SpanOrQueryBuilder> {
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if ("clauses".equals(currentFieldName)) {
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                        if (!(query instanceof SpanQueryBuilder)) {
+                        Query query = parseContext.parseInnerQuery();
+                        if (!(query instanceof SpanQuery)) {
                             throw new QueryParsingException(parseContext, "spanOr [clauses] must be of type span query");
                         }
-                        clauses.add((SpanQueryBuilder) query);
+                        clauses.add((SpanQuery) query);
                     }
                 } else {
                     throw new QueryParsingException(parseContext, "[span_or] query does not support [" + currentFieldName + "]");
@@ -76,17 +86,11 @@ public class SpanOrQueryParser extends BaseQueryParser<SpanOrQueryBuilder> {
             throw new QueryParsingException(parseContext, "spanOr must include [clauses]");
         }
 
-        SpanOrQueryBuilder queryBuilder = new SpanOrQueryBuilder();
-        for (SpanQueryBuilder clause : clauses) {
-            queryBuilder.clause(clause);
+        SpanOrQuery query = new SpanOrQuery(clauses.toArray(new SpanQuery[clauses.size()]));
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
         }
-        queryBuilder.boost(boost);
-        queryBuilder.queryName(queryName);
-        return queryBuilder;
-    }
-
-    @Override
-    public SpanOrQueryBuilder getBuilderPrototype() {
-        return SpanOrQueryBuilder.PROTOTYPE;
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanQueryBuilder.java
index d35dcbc..4216f22 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanQueryBuilder.java
@@ -19,9 +19,6 @@
 
 package org.elasticsearch.index.query;
 
-/**
- * Marker interface for a specific type of {@link QueryBuilder} that allows to build span queries
- */
-public interface SpanQueryBuilder<QB extends SpanQueryBuilder> extends QueryBuilder<QB> {
+public abstract class SpanQueryBuilder extends QueryBuilder {
 
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java
index 24cd816..9d0176e 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java
@@ -19,76 +19,75 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.apache.lucene.search.spans.SpanTermQuery;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.lucene.BytesRefs;
-import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 
-/**
- * A Span Query that matches documents containing a term.
- * @see SpanTermQuery
- */
-public class SpanTermQueryBuilder extends BaseTermQueryBuilder<SpanTermQueryBuilder> implements SpanQueryBuilder<SpanTermQueryBuilder> {
+public class SpanTermQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanTermQueryBuilder> {
+
+    private final String name;
+
+    private final Object value;
+
+    private float boost = -1;
 
-    public static final String NAME = "span_term";
-    static final SpanTermQueryBuilder PROTOTYPE = new SpanTermQueryBuilder(null, null);
+    private String queryName;
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, String) */
     public SpanTermQueryBuilder(String name, String value) {
-        super(name, (Object) value);
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, int) */
     public SpanTermQueryBuilder(String name, int value) {
-        super(name, (Object) value);
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, long) */
     public SpanTermQueryBuilder(String name, long value) {
-        super(name, (Object) value);
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, float) */
     public SpanTermQueryBuilder(String name, float value) {
-        super(name, (Object) value);
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, double) */
     public SpanTermQueryBuilder(String name, double value) {
-        super(name, (Object) value);
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, Object) */
-    public SpanTermQueryBuilder(String name, Object value) {
-        super(name, value);
+    private SpanTermQueryBuilder(String name, Object value) {
+        this.name = name;
+        this.value = value;
     }
 
     @Override
-    public SpanQuery doToQuery(QueryShardContext context) throws IOException {
-        BytesRef valueBytes = null;
-        String fieldName = this.fieldName;
-        MappedFieldType mapper = context.fieldMapper(fieldName);
-        if (mapper != null) {
-            fieldName = mapper.names().indexName();
-            valueBytes = mapper.indexedValueForSearch(value);
-        }
-        if (valueBytes == null) {
-            valueBytes = BytesRefs.toBytesRef(this.value);
-        }
-        return new SpanTermQuery(new Term(fieldName, valueBytes));
+    public SpanTermQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
-    @Override
-    protected SpanTermQueryBuilder createBuilder(String fieldName, Object value) {
-        return new SpanTermQueryBuilder(fieldName, value);
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public SpanTermQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
-    public String getWriteableName() {
-        return NAME;
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(SpanTermQueryParser.NAME);
+        if (boost == -1 && queryName != null) {
+            builder.field(name, value);
+        } else {
+            builder.startObject(name);
+            builder.field("value", value);
+            if (boost != -1) {
+                builder.field("boost", boost);
+            }
+            if (queryName != null) {
+                builder.field("_name", queryName);
+            }
+            builder.endObject();
+        }
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java
index 824b474..c4ff2ee 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java
@@ -19,16 +19,23 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanTermQuery;
+import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
 
 import java.io.IOException;
 
 /**
- * Parser for span_term query
+ *
  */
-public class SpanTermQueryParser extends BaseQueryParser<SpanTermQueryBuilder> {
+public class SpanTermQueryParser implements QueryParser {
+
+    public static final String NAME = "span_term";
 
     @Inject
     public SpanTermQueryParser() {
@@ -36,24 +43,23 @@ public class SpanTermQueryParser extends BaseQueryParser<SpanTermQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{SpanTermQueryBuilder.NAME, Strings.toCamelCase(SpanTermQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SpanTermQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         XContentParser.Token token = parser.currentToken();
         if (token == XContentParser.Token.START_OBJECT) {
             token = parser.nextToken();
         }
-
         assert token == XContentParser.Token.FIELD_NAME;
         String fieldName = parser.currentName();
 
 
-        Object value = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        String value = null;
+        float boost = 1.0f;
         String queryName = null;
         token = parser.nextToken();
         if (token == XContentParser.Token.START_OBJECT) {
@@ -63,9 +69,9 @@ public class SpanTermQueryParser extends BaseQueryParser<SpanTermQueryBuilder> {
                     currentFieldName = parser.currentName();
                 } else {
                     if ("term".equals(currentFieldName)) {
-                        value = parser.objectBytes();
+                        value = parser.text();
                     } else if ("value".equals(currentFieldName)) {
-                        value = parser.objectBytes();
+                        value = parser.text();
                     } else if ("boost".equals(currentFieldName)) {
                         boost = parser.floatValue();
                     } else if ("_name".equals(currentFieldName)) {
@@ -77,7 +83,7 @@ public class SpanTermQueryParser extends BaseQueryParser<SpanTermQueryBuilder> {
             }
             parser.nextToken();
         } else {
-            value = parser.objectBytes();
+            value = parser.text();
             // move to the next token
             parser.nextToken();
         }
@@ -86,13 +92,21 @@ public class SpanTermQueryParser extends BaseQueryParser<SpanTermQueryBuilder> {
             throw new QueryParsingException(parseContext, "No value specified for term query");
         }
 
-        SpanTermQueryBuilder result = new SpanTermQueryBuilder(fieldName, value);
-        result.boost(boost).queryName(queryName);
-        return result;
-    }
+        BytesRef valueBytes = null;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            fieldName = fieldType.names().indexName();
+            valueBytes = fieldType.indexedValueForSearch(value);
+        }
+        if (valueBytes == null) {
+            valueBytes = new BytesRef(value);
+        }
 
-    @Override
-    public SpanTermQueryBuilder getBuilderPrototype() {
-        return SpanTermQueryBuilder.PROTOTYPE;
+        SpanTermQuery query = new SpanTermQuery(new Term(fieldName, valueBytes));
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java
index 83c7716..d2b2fdc 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java
@@ -19,53 +19,59 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.apache.lucene.search.spans.SpanWithinQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * Builder for {@link org.apache.lucene.search.spans.SpanWithinQuery}.
  */
-public class SpanWithinQueryBuilder extends AbstractQueryBuilder<SpanWithinQueryBuilder> implements SpanQueryBuilder<SpanWithinQueryBuilder> {
+public class SpanWithinQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanWithinQueryBuilder> {
 
-    public static final String NAME = "span_within";
-    private final SpanQueryBuilder big;
-    private final SpanQueryBuilder little;
-    static final SpanWithinQueryBuilder PROTOTYPE = new SpanWithinQueryBuilder(null, null);
+    private SpanQueryBuilder big;
+    private SpanQueryBuilder little;
+    private float boost = -1;
+    private String queryName;
 
-    /**
-     * Query that returns spans from <code>little</code> that are contained in a spans from <code>big</code>.
-     * @param big clause that must enclose {@code little} for a match.
-     * @param little the little clause, it must be contained within {@code big} for a match.
+    /** 
+     * Sets the little clause, it must be contained within {@code big} for a match.
      */
-    public SpanWithinQueryBuilder(SpanQueryBuilder big, SpanQueryBuilder little) {
-        this.little = little;
-        this.big = big;
+    public SpanWithinQueryBuilder little(SpanQueryBuilder clause) {
+        this.little = clause;
+        return this;
     }
 
-    /**
-     * @return the little clause, contained within {@code big} for a match.
+    /** 
+     * Sets the big clause, it must enclose {@code little} for a match.
      */
-    public SpanQueryBuilder littleQuery() {
-        return this.little;
+    public SpanWithinQueryBuilder big(SpanQueryBuilder clause) {
+        this.big = clause;
+        return this;
+    }
+
+    @Override
+    public SpanWithinQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
-     * @return the big clause that must enclose {@code little} for a match.
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public SpanQueryBuilder bigQuery() {
-        return this.big;
+    public SpanWithinQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        if (big == null) {
+            throw new IllegalArgumentException("Must specify big clause when building a span_within query");
+        }
+        if (little == null) {
+            throw new IllegalArgumentException("Must specify little clause when building a span_within query");
+        }
+        builder.startObject(SpanWithinQueryParser.NAME);
 
         builder.field("big");
         big.toXContent(builder, params);
@@ -73,62 +79,14 @@ public class SpanWithinQueryBuilder extends AbstractQueryBuilder<SpanWithinQuery
         builder.field("little");
         little.toXContent(builder, params);
 
-        printBoostAndQueryName(builder);
-
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query innerBig = big.toQuery(context);
-        assert innerBig instanceof SpanQuery;
-        Query innerLittle = little.toQuery(context);
-        assert innerLittle instanceof SpanQuery;
-        return new SpanWithinQuery((SpanQuery) innerBig, (SpanQuery) innerLittle);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (big == null) {
-            validationException = addValidationError("inner clause [big] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(big, validationException);
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        if (little == null) {
-            validationException = addValidationError("inner clause [little] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(little, validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    protected SpanWithinQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        SpanQueryBuilder big = (SpanQueryBuilder)in.readQuery();
-        SpanQueryBuilder little = (SpanQueryBuilder)in.readQuery();
-        return new SpanWithinQueryBuilder(big, little);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(big);
-        out.writeQuery(little);
-    }
 
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(big, little);
-    }
-
-    @Override
-    protected boolean doEquals(SpanWithinQueryBuilder other) {
-        return Objects.equals(big, other.big) &&
-               Objects.equals(little, other.little);
-    }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java
index 00ddb0e..9194cbd 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java
@@ -19,6 +19,9 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanQuery;
+import org.apache.lucene.search.spans.SpanWithinQuery;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
@@ -26,9 +29,11 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import java.io.IOException;
 
 /**
- * Parser for span_within query
+ * Parser for {@link SpanWithinQuery}
  */
-public class SpanWithinQueryParser extends BaseQueryParser<SpanWithinQueryBuilder> {
+public class SpanWithinQueryParser implements QueryParser {
+
+    public static final String NAME = "span_within";
 
     @Inject
     public SpanWithinQueryParser() {
@@ -36,17 +41,17 @@ public class SpanWithinQueryParser extends BaseQueryParser<SpanWithinQueryBuilde
 
     @Override
     public String[] names() {
-        return new String[]{SpanWithinQueryBuilder.NAME, Strings.toCamelCase(SpanWithinQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SpanWithinQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         String queryName = null;
-        SpanQueryBuilder big = null;
-        SpanQueryBuilder little = null;
+        SpanQuery big = null;
+        SpanQuery little = null;
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -55,17 +60,17 @@ public class SpanWithinQueryParser extends BaseQueryParser<SpanWithinQueryBuilde
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("big".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (query instanceof SpanQueryBuilder == false) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (query instanceof SpanQuery == false) {
                         throw new QueryParsingException(parseContext, "span_within [big] must be of type span query");
                     }
-                    big = (SpanQueryBuilder) query;
+                    big = (SpanQuery) query;
                 } else if ("little".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (query instanceof SpanQueryBuilder == false) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (query instanceof SpanQuery == false) {
                         throw new QueryParsingException(parseContext, "span_within [little] must be of type span query");
                     }
-                    little = (SpanQueryBuilder) query;
+                    little = (SpanQuery) query;
                 } else {
                     throw new QueryParsingException(parseContext, "[span_within] query does not support [" + currentFieldName + "]");
                 }
@@ -76,8 +81,8 @@ public class SpanWithinQueryParser extends BaseQueryParser<SpanWithinQueryBuilde
             } else {
                 throw new QueryParsingException(parseContext, "[span_within] query does not support [" + currentFieldName + "]");
             }
-        }
-
+        }        
+        
         if (big == null) {
             throw new QueryParsingException(parseContext, "span_within must include [big]");
         }
@@ -85,13 +90,11 @@ public class SpanWithinQueryParser extends BaseQueryParser<SpanWithinQueryBuilde
             throw new QueryParsingException(parseContext, "span_within must include [little]");
         }
 
-        SpanWithinQueryBuilder query = new SpanWithinQueryBuilder(big, little);
-        query.boost(boost).queryName(queryName);
+        Query query = new SpanWithinQuery(big, little);
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
         return query;
     }
-
-    @Override
-    public SpanWithinQueryBuilder getBuilderPrototype() {
-        return SpanWithinQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java
index 63721c0..852977f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java
@@ -28,10 +28,7 @@ import java.util.Map;
 /**
  * Facilitates creating template query requests.
  * */
-public class TemplateQueryBuilder extends AbstractQueryBuilder<TemplateQueryBuilder> {
-
-    /** Name to reference this type of query. */
-    public static final String NAME = "template";
+public class TemplateQueryBuilder extends QueryBuilder {
 
     /** Template to fill. */
     private Template template;
@@ -42,8 +39,6 @@ public class TemplateQueryBuilder extends AbstractQueryBuilder<TemplateQueryBuil
 
     private ScriptService.ScriptType templateType;
 
-    static final TemplateQueryBuilder PROTOTYPE = new TemplateQueryBuilder(null, null);
-
     /**
      * @param template
      *            the template to use for that query.
@@ -82,16 +77,11 @@ public class TemplateQueryBuilder extends AbstractQueryBuilder<TemplateQueryBuil
 
     @Override
     protected void doXContent(XContentBuilder builder, Params builderParams) throws IOException {
-        builder.field(TemplateQueryBuilder.NAME);
+        builder.field(TemplateQueryParser.NAME);
         if (template == null) {
             new Template(templateString, templateType, null, null, this.vars).toXContent(builder, builderParams);
         } else {
             template.toXContent(builder, builderParams);
         }
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
index a4c3ef9..1b5210d 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
@@ -39,8 +39,10 @@ import java.util.Map;
  * In the simplest case, parse template string and variables from the request,
  * compile the template and execute the template against the given variables.
  * */
-public class TemplateQueryParser extends BaseQueryParserTemp {
+public class TemplateQueryParser implements QueryParser {
 
+    /** Name to reference this type of query. */
+    public static final String NAME = "template";
     /** Name of query parameter containing the template string. */
     public static final String QUERY = "query";
 
@@ -60,7 +62,7 @@ public class TemplateQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[] {TemplateQueryBuilder.NAME};
+        return new String[] { NAME };
     }
 
     /**
@@ -68,13 +70,12 @@ public class TemplateQueryParser extends BaseQueryParserTemp {
      * values. Handles both submitting the template as part of the request as
      * well as referencing only the template name.
      *
-     * @param context
+     * @param parseContext
      *            parse context containing the templated query.
      */
     @Override
     @Nullable
-    public Query parse(QueryShardContext context) throws IOException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException {
         XContentParser parser = parseContext.parser();
         Template template = parse(parser, parseContext.parseFieldMatcher());
         ExecutableScript executable = this.scriptService.executable(template, ScriptContext.Standard.SEARCH, SearchContext.current());
@@ -82,9 +83,9 @@ public class TemplateQueryParser extends BaseQueryParserTemp {
         BytesReference querySource = (BytesReference) executable.run();
 
         try (XContentParser qSourceParser = XContentFactory.xContent(querySource).createParser(querySource)) {
-            final QueryShardContext contextCopy = new QueryShardContext(context.index(), context.indexQueryParserService());
-            contextCopy.reset(qSourceParser);
-            return contextCopy.parseContext().parseInnerQuery();
+            final QueryParseContext context = new QueryParseContext(parseContext.index(), parseContext.indexQueryParserService());
+            context.reset(qSourceParser);
+            return context.parseInnerQuery();
         }
     }
 
@@ -113,9 +114,4 @@ public class TemplateQueryParser extends BaseQueryParserTemp {
     public static Template parse(XContentParser parser, Map<String, ScriptService.ScriptType> parameterMap, ParseFieldMatcher parseFieldMatcher) throws IOException {
         return Template.parse(parser, parameterMap, parseFieldMatcher);
     }
-
-    @Override
-    public TemplateQueryBuilder getBuilderPrototype() {
-        return TemplateQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java
index 5c8bf3f..5bd911a 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java
@@ -19,77 +19,128 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.elasticsearch.common.lucene.BytesRefs;
-import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 
 /**
  * A Query that matches documents containing a term.
  */
-public class TermQueryBuilder extends BaseTermQueryBuilder<TermQueryBuilder> {
+public class TermQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<TermQueryBuilder> {
 
-    public static final String NAME = "term";
-    static final TermQueryBuilder PROTOTYPE = new TermQueryBuilder(null, null);
+    private final String name;
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, String) */
-    public TermQueryBuilder(String fieldName, String value) {
-        super(fieldName, (Object) value);
+    private final Object value;
+
+    private float boost = -1;
+
+    private String queryName;
+
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, String value) {
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, int) */
-    public TermQueryBuilder(String fieldName, int value) {
-        super(fieldName, (Object) value);
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, int value) {
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, long) */
-    public TermQueryBuilder(String fieldName, long value) {
-        super(fieldName, (Object) value);
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, long value) {
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, float) */
-    public TermQueryBuilder(String fieldName, float value) {
-        super(fieldName, (Object) value);
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, float value) {
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, double) */
-    public TermQueryBuilder(String fieldName, double value) {
-        super(fieldName, (Object) value);
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, double value) {
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, boolean) */
-    public TermQueryBuilder(String fieldName, boolean value) {
-        super(fieldName, (Object) value);
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, boolean value) {
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, Object) */
-    public TermQueryBuilder(String fieldName, Object value) {
-        super(fieldName, value);
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, Object value) {
+        this.name = name;
+        this.value = value;
     }
 
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
     @Override
-    public Query doToQuery(QueryShardContext context) throws IOException {
-        Query query = null;
-        MappedFieldType mapper = context.fieldMapper(this.fieldName);
-        if (mapper != null) {
-            query = mapper.termQuery(this.value, context);
-        }
-        if (query == null) {
-            query = new TermQuery(new Term(this.fieldName, BytesRefs.toBytesRef(this.value)));
-        }
-        return query;
+    public TermQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
-    @Override
-    protected TermQueryBuilder createBuilder(String fieldName, Object value) {
-        return new TermQueryBuilder(fieldName, value);
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public TermQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
-    public String getWriteableName() {
-        return NAME;
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(TermQueryParser.NAME);
+        if (boost == -1 && queryName == null) {
+            builder.field(name, value);
+        } else {
+            builder.startObject(name);
+            builder.field("value", value);
+            if (boost != -1) {
+                builder.field("boost", boost);
+            }
+            if (queryName != null) {
+                builder.field("_name", queryName);
+            }
+            builder.endObject();
+        }
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/TermQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/TermQueryParser.java
index 43d4d95..1c3876f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TermQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TermQueryParser.java
@@ -19,16 +19,23 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
 
 import java.io.IOException;
 
 /**
- * Parser for the term query
+ *
  */
-public class TermQueryParser extends BaseQueryParser<TermQueryBuilder> {
+public class TermQueryParser implements QueryParser {
+
+    public static final String NAME = "term";
 
     private static final ParseField NAME_FIELD = new ParseField("_name").withAllDeprecated("query name is not supported in short version of term query");
     private static final ParseField BOOST_FIELD = new ParseField("boost").withAllDeprecated("boost is not supported in short version of term query");
@@ -39,17 +46,17 @@ public class TermQueryParser extends BaseQueryParser<TermQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{TermQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public TermQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String queryName = null;
         String fieldName = null;
         Object value = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         String currentFieldName = null;
         XContentParser.Token token;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -97,16 +104,22 @@ public class TermQueryParser extends BaseQueryParser<TermQueryBuilder> {
             }
         }
 
-        TermQueryBuilder termQuery = new TermQueryBuilder(fieldName, value);
-        termQuery.boost(boost);
-        if (queryName != null) {
-            termQuery.queryName(queryName);
+        if (value == null) {
+            throw new QueryParsingException(parseContext, "No value specified for term query");
         }
-        return termQuery;
-    }
 
-    @Override
-    public TermQueryBuilder getBuilderPrototype() {
-        return TermQueryBuilder.PROTOTYPE;
+        Query query = null;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            query = fieldType.termQuery(value, parseContext);
+        }
+        if (query == null) {
+            query = new TermQuery(new Term(fieldName, BytesRefs.toBytesRef(value)));
+        }
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/TermsLookupQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/TermsLookupQueryBuilder.java
index a074e2a..4bdd0da 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TermsLookupQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TermsLookupQueryBuilder.java
@@ -19,20 +19,93 @@
 
 package org.elasticsearch.index.query;
 
+import org.elasticsearch.common.xcontent.XContentBuilder;
+
+import java.io.IOException;
 
 /**
- * A filter for a field based on several terms matching on any of them.
- * @deprecated use {@link TermsQueryBuilder} instead.
+ * A filer for a field based on several terms matching on any of them.
  */
-@Deprecated
-public class TermsLookupQueryBuilder extends TermsQueryBuilder {
+public class TermsLookupQueryBuilder extends QueryBuilder {
+
+    private final String name;
+    private String lookupIndex;
+    private String lookupType;
+    private String lookupId;
+    private String lookupRouting;
+    private String lookupPath;
+
+    private String queryName;
 
     public TermsLookupQueryBuilder(String name) {
-        super(name, (Object[]) null);
+        this.name = name;
+    }
+
+    /**
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public TermsLookupQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
+    /**
+     * Sets the index name to lookup the terms from.
+     */
+    public TermsLookupQueryBuilder lookupIndex(String lookupIndex) {
+        this.lookupIndex = lookupIndex;
+        return this;
+    }
+
+    /**
+     * Sets the index type to lookup the terms from.
+     */
+    public TermsLookupQueryBuilder lookupType(String lookupType) {
+        this.lookupType = lookupType;
+        return this;
+    }
+
+    /**
+     * Sets the doc id to lookup the terms from.
+     */
+    public TermsLookupQueryBuilder lookupId(String lookupId) {
+        this.lookupId = lookupId;
+        return this;
+    }
+
+    /**
+     * Sets the path within the document to lookup the terms from.
+     */
+    public TermsLookupQueryBuilder lookupPath(String lookupPath) {
+        this.lookupPath = lookupPath;
+        return this;
+    }
+
+    public TermsLookupQueryBuilder lookupRouting(String lookupRouting) {
+        this.lookupRouting = lookupRouting;
+        return this;
     }
 
     @Override
-    public String getWriteableName() {
-        return TermsQueryBuilder.NAME;
-   }
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(TermsQueryParser.NAME);
+
+        builder.startObject(name);
+        if (lookupIndex != null) {
+            builder.field("index", lookupIndex);
+        }
+        builder.field("type", lookupType);
+        builder.field("id", lookupId);
+        if (lookupRouting != null) {
+            builder.field("routing", lookupRouting);
+        }
+        builder.field("path", lookupPath);
+        builder.endObject();
+
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
+
+        builder.endObject();
+    }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java
index e72fe93..ca54eb3 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java
@@ -19,137 +19,101 @@
 
 package org.elasticsearch.index.query;
 
-import com.google.common.primitives.Doubles;
-import com.google.common.primitives.Floats;
-import com.google.common.primitives.Ints;
-import com.google.common.primitives.Longs;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.queries.TermsQuery;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.BytesRefs;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.indices.cache.query.terms.TermsLookup;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-import java.util.Objects;
 
 /**
- * A filter for a field based on several terms matching on any of them.
+ * A filer for a field based on several terms matching on any of them.
  */
-public class TermsQueryBuilder extends AbstractQueryBuilder<TermsQueryBuilder> {
+public class TermsQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<TermsQueryBuilder> {
 
-    public static final String NAME = "terms";
+    private final String name;
 
-    static final TermsQueryBuilder PROTOTYPE = new TermsQueryBuilder(null);
+    private final Object values;
 
-    public static final boolean DEFAULT_DISABLE_COORD = false;
-
-    private final String fieldName;
-    private List<Object> values;
     private String minimumShouldMatch;
-    private boolean disableCoord = DEFAULT_DISABLE_COORD;
-    private TermsLookup termsLookup;
+
+    private Boolean disableCoord;
+
+    private String queryName;
+
+    private float boost = -1;
 
     /**
-     * A filter for a field based on several terms matching on any of them.
-     *
-     * @param fieldName The field name
-     * @param values The terms
-     */
-    public TermsQueryBuilder(String fieldName, String... values) {
-        this(fieldName, values != null ? Arrays.asList(values) : (Iterable<?>) null);
-    }
-    
-    /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
-     * @param fieldName The field name
+     * @param name   The field name
      * @param values The terms
      */
-    public TermsQueryBuilder(String fieldName, int... values) {
-        this(fieldName, values != null ? Ints.asList(values) : (Iterable<?>) null);
+    public TermsQueryBuilder(String name, String... values) {
+        this(name, (Object[]) values);
     }
 
     /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
-     * @param fieldName The field name
+     * @param name   The field name
      * @param values The terms
      */
-    public TermsQueryBuilder(String fieldName, long... values) {
-        this(fieldName, values != null ? Longs.asList(values) : (Iterable<?>) null);
+    public TermsQueryBuilder(String name, int... values) {
+        this.name = name;
+        this.values = values;
     }
 
     /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
-     * @param fieldName The field name
+     * @param name   The field name
      * @param values The terms
      */
-    public TermsQueryBuilder(String fieldName, float... values) {
-        this(fieldName, values != null ? Floats.asList(values) : (Iterable<?>) null);
+    public TermsQueryBuilder(String name, long... values) {
+        this.name = name;
+        this.values = values;
     }
 
     /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
-     * @param fieldName The field name
+     * @param name   The field name
      * @param values The terms
      */
-    public TermsQueryBuilder(String fieldName, double... values) {
-        this(fieldName, values != null ? Doubles.asList(values) : (Iterable<?>) null);
+    public TermsQueryBuilder(String name, float... values) {
+        this.name = name;
+        this.values = values;
     }
 
     /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
-     * @param fieldName The field name
+     * @param name   The field name
      * @param values The terms
      */
-    public TermsQueryBuilder(String fieldName, Object... values) {
-        this(fieldName, values != null ? Arrays.asList(values) : (Iterable<?>) null);
+    public TermsQueryBuilder(String name, double... values) {
+        this.name = name;
+        this.values = values;
     }
 
     /**
-     * Constructor used for terms query lookup.
+     * A filer for a field based on several terms matching on any of them.
      *
-     * @param fieldName The field name
+     * @param name   The field name
+     * @param values The terms
      */
-    public TermsQueryBuilder(String fieldName) {
-        this.fieldName = fieldName;
+    public TermsQueryBuilder(String name, Object... values) {
+        this.name = name;
+        this.values = values;
     }
 
     /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
-     * @param fieldName The field name
+     * @param name   The field name
      * @param values The terms
      */
-    public TermsQueryBuilder(String fieldName, Iterable<?> values) {
-        if (values == null) {
-            throw new IllegalArgumentException("No value specified for terms query");
-        }
-        this.fieldName = fieldName;
-        this.values = convertToBytesRefListIfStringList(values);
-    }
-
-    public String fieldName() {
-        return this.fieldName;
-    }
-
-    public List<Object> values() {
-        return convertToStringListIfBytesRefList(this.values);
+    public TermsQueryBuilder(String name, Iterable values) {
+        this.name = name;
+        this.values = values;
     }
 
     /**
@@ -162,10 +126,6 @@ public class TermsQueryBuilder extends AbstractQueryBuilder<TermsQueryBuilder> {
         return this;
     }
 
-    public String minimumShouldMatch() {
-        return this.minimumShouldMatch;
-    }
-
     /**
      * Disables <tt>Similarity#coord(int,int)</tt> in scoring. Defaults to <tt>false</tt>.
      * @deprecated use [bool] query instead
@@ -176,254 +136,41 @@ public class TermsQueryBuilder extends AbstractQueryBuilder<TermsQueryBuilder> {
         return this;
     }
 
-    public boolean disableCoord() {
-        return this.disableCoord;
-    }
-
-    private boolean isTermsLookupQuery() {
-        return this.termsLookup != null;
-    }
-
-    public TermsQueryBuilder termsLookup(TermsLookup termsLookup) {
-        this.termsLookup = termsLookup;
-        return this;
-    }
-
-    public TermsLookup termsLookup() {
-        return this.termsLookup;
-    }
-
-    /**
-     * Sets the index name to lookup the terms from.
-     */
-    public TermsQueryBuilder lookupIndex(String lookupIndex) {
-        if (lookupIndex == null) {
-            throw new IllegalArgumentException("Lookup index cannot be set to null");
-        }
-        if (this.termsLookup == null) {
-            this.termsLookup = new TermsLookup();
-        }
-        this.termsLookup.index(lookupIndex);
-        return this;
-    }
-
-    /**
-     * Sets the type name to lookup the terms from.
-     */
-    public TermsQueryBuilder lookupType(String lookupType) {
-        if (lookupType == null) {
-            throw new IllegalArgumentException("Lookup type cannot be set to null");
-        }
-        if (this.termsLookup == null) {
-            this.termsLookup = new TermsLookup();
-        }
-        this.termsLookup.type(lookupType);
-        return this;
-    }
-
-    /**
-     * Sets the document id to lookup the terms from.
-     */
-    public TermsQueryBuilder lookupId(String lookupId) {
-        if (lookupId == null) {
-            throw new IllegalArgumentException("Lookup id cannot be set to null");
-        }
-        if (this.termsLookup == null) {
-            this.termsLookup = new TermsLookup();
-        }
-        this.termsLookup.id(lookupId);
-        return this;
-    }
-
     /**
-     * Sets the path name to lookup the terms from.
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
      */
-    public TermsQueryBuilder lookupPath(String lookupPath) {
-        if (lookupPath == null) {
-            throw new IllegalArgumentException("Lookup path cannot be set to null");
-        }
-        if (this.termsLookup == null) {
-            this.termsLookup = new TermsLookup();
-        }
-        this.termsLookup.path(lookupPath);
+    public TermsQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
         return this;
     }
 
-    /**
-     * Sets the routing to lookup the terms from.
-     */
-    public TermsQueryBuilder lookupRouting(String lookupRouting) {
-        if (lookupRouting == null) {
-            throw new IllegalArgumentException("Lookup routing cannot be set to null");
-        }
-        if (this.termsLookup == null) {
-            this.termsLookup = new TermsLookup();
-        }
-        this.termsLookup.routing(lookupRouting);
+    @Override
+    public TermsQueryBuilder boost(float boost) {
+        this.boost = boost;
         return this;
     }
 
-    /**
-     * Same as {@link #convertToBytesRefIfString} but on Iterable.
-     * @param objs the Iterable of input object
-     * @return the same input or a list of {@link BytesRef} representation if input was a list of type string
-     */
-    private static List<Object> convertToBytesRefListIfStringList(Iterable<?> objs) {
-        if (objs == null) {
-            return null;
-        }
-        List<Object> newObjs = new ArrayList<>();
-        for (Object obj : objs) {
-            newObjs.add(convertToBytesRefIfString(obj));
-        }
-        return newObjs;
-    }
-
-    /**
-     * Same as {@link #convertToStringIfBytesRef} but on Iterable.
-     * @param objs the Iterable of input object
-     * @return the same input or a list of utf8 string if input was a list of type {@link BytesRef}
-     */
-    private static List<Object> convertToStringListIfBytesRefList(Iterable<?> objs) {
-        if (objs == null) {
-            return null;
-        }
-        List<Object> newObjs = new ArrayList<>();
-        for (Object obj : objs) {
-            newObjs.add(convertToStringIfBytesRef(obj));
-        }
-        return newObjs;
-    }
-
     @Override
     public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        if (isTermsLookupQuery()) {
-            builder.startObject(fieldName);
-            termsLookup.toXContent(builder, params);
-            builder.endObject();
-        } else {
-            builder.field(fieldName, convertToStringListIfBytesRefList(values));
-        }
+        builder.startObject(TermsQueryParser.NAME);
+        builder.field(name, values);
+
         if (minimumShouldMatch != null) {
             builder.field("minimum_should_match", minimumShouldMatch);
         }
-        builder.field("disable_coord", disableCoord);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        List<Object> terms;
-        if (isTermsLookupQuery()) {
-            if (termsLookup.index() == null) {
-                termsLookup.index(context.index().name());
-            }
-            terms = context.handleTermsLookup(termsLookup);
-        } else {
-            terms = values;
-        }
-        if (terms == null || terms.isEmpty()) {
-            return Queries.newMatchNoDocsQuery();
-        }
-        return handleTermsQuery(terms, fieldName, context, minimumShouldMatch, disableCoord);
-    }
-
-    private static Query handleTermsQuery(List<Object> terms, String fieldName, QueryShardContext context, String minimumShouldMatch, boolean disableCoord) {
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        String indexFieldName;
-        if (fieldType != null) {
-            indexFieldName = fieldType.names().indexName();
-        } else {
-            indexFieldName = fieldName;
-        }
-
-        Query query;
-        if (context.isFilter()) {
-            if (fieldType != null) {
-                query = fieldType.termsQuery(terms, context);
-            } else {
-                BytesRef[] filterValues = new BytesRef[terms.size()];
-                for (int i = 0; i < filterValues.length; i++) {
-                    filterValues[i] = BytesRefs.toBytesRef(terms.get(i));
-                }
-                query = new TermsQuery(indexFieldName, filterValues);
-            }
-        } else {
-            BooleanQuery bq = new BooleanQuery(disableCoord);
-            for (Object term : terms) {
-                if (fieldType != null) {
-                    bq.add(fieldType.termQuery(term, context), BooleanClause.Occur.SHOULD);
-                } else {
-                    bq.add(new TermQuery(new Term(indexFieldName, BytesRefs.toBytesRef(term))), BooleanClause.Occur.SHOULD);
-                }
-            }
-            bq = Queries.applyMinimumShouldMatch(bq, minimumShouldMatch);
-            query = bq;
-        }
-        return query;
-    }
 
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (this.fieldName == null) {
-            validationException = addValidationError("field name cannot be null.", validationException);
+        if (disableCoord != null) {
+            builder.field("disable_coord", disableCoord);
         }
-        if (isTermsLookupQuery() && this.values != null) {
-            validationException = addValidationError("can't have both a terms query and a lookup query.", validationException);
-        }
-        if (isTermsLookupQuery()) {
-            QueryValidationException exception = termsLookup.validate();
-            if (exception != null) {
-                validationException = QueryValidationException.addValidationErrors(exception.validationErrors(), validationException);
-            }
-        }
-        return validationException;
-    }
 
-    @SuppressWarnings("unchecked")
-    @Override
-    protected TermsQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        TermsQueryBuilder termsQueryBuilder = new TermsQueryBuilder(in.readString());
-        if (in.readBoolean()) {
-            termsQueryBuilder.termsLookup = TermsLookup.readTermsLookupFrom(in);
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        termsQueryBuilder.values = ((List<Object>) in.readGenericValue());
-        termsQueryBuilder.minimumShouldMatch = in.readOptionalString();
-        termsQueryBuilder.disableCoord = in.readBoolean();
-        return termsQueryBuilder;
-    }
 
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-        out.writeBoolean(isTermsLookupQuery());
-        if (isTermsLookupQuery()) {
-            termsLookup.writeTo(out);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        out.writeGenericValue(values);
-        out.writeOptionalString(minimumShouldMatch);
-        out.writeBoolean(disableCoord);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldName, values, minimumShouldMatch, disableCoord, termsLookup);
-    }
 
-    @Override
-    protected boolean doEquals(TermsQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-                Objects.equals(values, other.values) &&
-                Objects.equals(minimumShouldMatch, other.minimumShouldMatch) &&
-                Objects.equals(disableCoord, other.disableCoord) &&
-                Objects.equals(termsLookup, other.termsLookup);
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/TermsQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/TermsQueryParser.java
index 350672a..c18ef81 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TermsQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TermsQueryParser.java
@@ -19,29 +19,40 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.queries.TermsQuery;
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.util.BytesRef;
+import org.elasticsearch.action.get.GetRequest;
+import org.elasticsearch.action.get.GetResponse;
+import org.elasticsearch.client.Client;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.BytesRefs;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.common.xcontent.support.XContentMapValues;
+import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.indices.cache.query.terms.TermsLookup;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 
 /**
- * Parser for terms query and terms lookup.
  *
- * Filters documents that have fields that match any of the provided terms (not analyzed)
- *
- * It also supports a terms lookup mechanism which can be used to fetch the term values from
- * a document in an index.
  */
-public class TermsQueryParser extends BaseQueryParser {
+public class TermsQueryParser implements QueryParser {
 
-    private static final ParseField MIN_SHOULD_MATCH_FIELD = new ParseField("min_match", "min_should_match", "minimum_should_match")
-            .withAllDeprecated("Use [bool] query instead");
+    public static final String NAME = "terms";
+    private static final ParseField MIN_SHOULD_MATCH_FIELD = new ParseField("min_match", "min_should_match").withAllDeprecated("Use [bool] query instead");
     private static final ParseField DISABLE_COORD_FIELD = new ParseField("disable_coord").withAllDeprecated("Use [bool] query instead");
     private static final ParseField EXECUTION_FIELD = new ParseField("execution").withAllDeprecated("execution is deprecated and has no effect");
+    private Client client;
 
     @Inject
     public TermsQueryParser() {
@@ -49,24 +60,34 @@ public class TermsQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{TermsQueryBuilder.NAME, "in"};
+        return new String[]{NAME, "in"};
+    }
+
+    @Inject(optional = true)
+    public void setClient(Client client) {
+        this.client = client;
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        String fieldName = null;
-        List<Object> values = null;
+        String queryName = null;
+        String currentFieldName = null;
+
+        String lookupIndex = parseContext.index().name();
+        String lookupType = null;
+        String lookupId = null;
+        String lookupPath = null;
+        String lookupRouting = null;
         String minShouldMatch = null;
-        boolean disableCoord = TermsQueryBuilder.DEFAULT_DISABLE_COORD;
-        TermsLookup termsLookup = null;
 
-        String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        boolean disableCoord = false;
 
         XContentParser.Token token;
-        String currentFieldName = null;
+        List<Object> terms = new ArrayList<>();
+        String fieldName = null;
+        float boost = 1f;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
@@ -77,16 +98,51 @@ public class TermsQueryParser extends BaseQueryParser {
                     throw new QueryParsingException(parseContext, "[terms] query does not support multiple fields");
                 }
                 fieldName = currentFieldName;
-                values = parseValues(parseContext, parser);
+
+                while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                    Object value = parser.objectBytes();
+                    if (value == null) {
+                        throw new QueryParsingException(parseContext, "No value specified for terms query");
+                    }
+                    terms.add(value);
+                }
             } else if (token == XContentParser.Token.START_OBJECT) {
                 fieldName = currentFieldName;
-                termsLookup = parseTermsLookup(parseContext, parser);
+                while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                    if (token == XContentParser.Token.FIELD_NAME) {
+                        currentFieldName = parser.currentName();
+                    } else if (token.isValue()) {
+                        if ("index".equals(currentFieldName)) {
+                            lookupIndex = parser.text();
+                        } else if ("type".equals(currentFieldName)) {
+                            lookupType = parser.text();
+                        } else if ("id".equals(currentFieldName)) {
+                            lookupId = parser.text();
+                        } else if ("path".equals(currentFieldName)) {
+                            lookupPath = parser.text();
+                        } else if ("routing".equals(currentFieldName)) {
+                            lookupRouting = parser.textOrNull();
+                        } else {
+                            throw new QueryParsingException(parseContext, "[terms] query does not support [" + currentFieldName
+                                    + "] within lookup element");
+                        }
+                    }
+                }
+                if (lookupType == null) {
+                    throw new QueryParsingException(parseContext, "[terms] query lookup element requires specifying the type");
+                }
+                if (lookupId == null) {
+                    throw new QueryParsingException(parseContext, "[terms] query lookup element requires specifying the id");
+                }
+                if (lookupPath == null) {
+                    throw new QueryParsingException(parseContext, "[terms] query lookup element requires specifying the path");
+                }
             } else if (token.isValue()) {
                 if (parseContext.parseFieldMatcher().match(currentFieldName, EXECUTION_FIELD)) {
                     // ignore
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, MIN_SHOULD_MATCH_FIELD)) {
                     if (minShouldMatch != null) {
-                        throw new IllegalArgumentException("[" + currentFieldName + "] is not allowed in a filter context for the [" + TermsQueryBuilder.NAME + "] query");
+                        throw new IllegalArgumentException("[" + currentFieldName + "] is not allowed in a filter context for the [" + NAME + "] query");
                     }
                     minShouldMatch = parser.textOrNull();
                 } else if ("boost".equals(currentFieldName)) {
@@ -102,73 +158,56 @@ public class TermsQueryParser extends BaseQueryParser {
         }
 
         if (fieldName == null) {
-            throw new QueryParsingException(parseContext, "terms query requires a field name, followed by array of terms or a document lookup specification");
+            throw new QueryParsingException(parseContext, "terms query requires a field name, followed by array of terms");
         }
-        TermsQueryBuilder termsQueryBuilder;
-        if (values == null) {
-            termsQueryBuilder = new TermsQueryBuilder(fieldName);
-        } else {
-            termsQueryBuilder = new TermsQueryBuilder(fieldName, values);
+
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            fieldName = fieldType.names().indexName();
         }
-        return termsQueryBuilder
-                .disableCoord(disableCoord)
-                .minimumShouldMatch(minShouldMatch)
-                .termsLookup(termsLookup)
-                .boost(boost)
-                .queryName(queryName);
-    }
 
-    private static List<Object> parseValues(QueryParseContext parseContext, XContentParser parser) throws IOException {
-        List<Object> values = new ArrayList<>();
-        XContentParser.Token token;
-        while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-            Object value = parser.objectBytes();
-            if (value == null) {
-                throw new QueryParsingException(parseContext, "No value specified for terms query");
+        if (lookupId != null) {
+            final TermsLookup lookup = new TermsLookup(lookupIndex, lookupType, lookupId, lookupRouting, lookupPath, parseContext);
+            GetRequest getRequest = new GetRequest(lookup.getIndex(), lookup.getType(), lookup.getId()).preference("_local").routing(lookup.getRouting());
+            getRequest.copyContextAndHeadersFrom(SearchContext.current());
+            final GetResponse getResponse = client.get(getRequest).actionGet();
+            if (getResponse.isExists()) {
+                List<Object> values = XContentMapValues.extractRawValues(lookup.getPath(), getResponse.getSourceAsMap());
+                terms.addAll(values);
             }
-            values.add(value);
         }
-        return values;
-    }
 
-    private static TermsLookup parseTermsLookup(QueryParseContext parseContext, XContentParser parser) throws IOException {
-        TermsLookup termsLookup = new TermsLookup();
-        XContentParser.Token token;
-        String currentFieldName = null;
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                currentFieldName = parser.currentName();
-            } else if (token.isValue()) {
-                if ("index".equals(currentFieldName)) {
-                    termsLookup.index(parser.textOrNull());
-                } else if ("type".equals(currentFieldName)) {
-                    termsLookup.type(parser.text());
-                } else if ("id".equals(currentFieldName)) {
-                    termsLookup.id(parser.text());
-                } else if ("routing".equals(currentFieldName)) {
-                    termsLookup.routing(parser.textOrNull());
-                } else if ("path".equals(currentFieldName)) {
-                    termsLookup.path(parser.text());
+        if (terms.isEmpty()) {
+            return Queries.newMatchNoDocsQuery();
+        }
+
+        Query query;
+        if (parseContext.isFilter()) {
+            if (fieldType != null) {
+                query = fieldType.termsQuery(terms, parseContext);
+            } else {
+                BytesRef[] filterValues = new BytesRef[terms.size()];
+                for (int i = 0; i < filterValues.length; i++) {
+                    filterValues[i] = BytesRefs.toBytesRef(terms.get(i));
+                }
+                query = new TermsQuery(fieldName, filterValues);
+            }
+        } else {
+            BooleanQuery bq = new BooleanQuery(disableCoord);
+            for (Object term : terms) {
+                if (fieldType != null) {
+                    bq.add(fieldType.termQuery(term, parseContext), Occur.SHOULD);
                 } else {
-                    throw new QueryParsingException(parseContext, "[terms] query does not support [" + currentFieldName
-                            + "] within lookup element");
+                    bq.add(new TermQuery(new Term(fieldName, BytesRefs.toBytesRef(term))), Occur.SHOULD);
                 }
             }
+            query = Queries.applyMinimumShouldMatch(bq, minShouldMatch);
         }
-        if (termsLookup.type() == null) {
-            throw new QueryParsingException(parseContext, "[terms] query lookup element requires specifying the type");
-        }
-        if (termsLookup.id() == null) {
-            throw new QueryParsingException(parseContext, "[terms] query lookup element requires specifying the id");
-        }
-        if (termsLookup.path() == null) {
-            throw new QueryParsingException(parseContext, "[terms] query lookup element requires specifying the path");
-        }
-        return termsLookup;
-    }
+        query.setBoost(boost);
 
-    @Override
-    public TermsQueryBuilder getBuilderPrototype() {
-        return TermsQueryBuilder.PROTOTYPE;
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java
index 9f89a94..2a9a6c5 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java
@@ -19,92 +19,22 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.DocumentMapper;
-import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
 
 import java.io.IOException;
-import java.util.Objects;
 
-public class TypeQueryBuilder extends AbstractQueryBuilder<TypeQueryBuilder> {
+public class TypeQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "type";
-
-    private final BytesRef type;
-
-    static final TypeQueryBuilder PROTOTYPE = new TypeQueryBuilder((BytesRef) null);
+    private final String type;
 
     public TypeQueryBuilder(String type) {
-        this.type = BytesRefs.toBytesRef(type);
-    }
-
-    TypeQueryBuilder(BytesRef type) {
         this.type = type;
     }
 
-    public String type() {
-        return BytesRefs.toString(this.type);
-    }
-
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("value", type.utf8ToString());
-        printBoostAndQueryName(builder);
+        builder.startObject(TypeQueryParser.NAME);
+        builder.field("value", type);
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query filter;
-        //LUCENE 4 UPGRADE document mapper should use bytesref as well?
-        DocumentMapper documentMapper = context.mapperService().documentMapper(type.utf8ToString());
-        if (documentMapper == null) {
-            filter = new TermQuery(new Term(TypeFieldMapper.NAME, type));
-        } else {
-            filter = documentMapper.typeFilter();
-        }
-        return filter;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (type == null) {
-            validationException = addValidationError("[type] cannot be null", validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    protected TypeQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new TypeQueryBuilder(in.readBytesRef());
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeBytesRef(type);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(type);
-    }
-
-    @Override
-    protected boolean doEquals(TypeQueryBuilder other) {
-        return Objects.equals(type, other.type);
-    }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/TypeQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/TypeQueryParser.java
index ee5e772..e4b7889 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TypeQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TypeQueryParser.java
@@ -19,16 +19,20 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.DocumentMapper;
+import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
 
 import java.io.IOException;
 
-/**
- * Parser for type query
- */
-public class TypeQueryParser extends BaseQueryParser<TypeQueryBuilder> {
+public class TypeQueryParser implements QueryParser {
+
+    public static final String NAME = "type";
 
     @Inject
     public TypeQueryParser() {
@@ -36,45 +40,37 @@ public class TypeQueryParser extends BaseQueryParser<TypeQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{TypeQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public TypeQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
-        BytesRef type = null;
 
-        String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-
-        String currentFieldName = null;
-        XContentParser.Token token;
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                currentFieldName = parser.currentName();
-            } else if (token.isValue()) {
-                if ("_name".equals(currentFieldName)) {
-                    queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
-                } else if ("value".equals(currentFieldName)) {
-                    type = parser.utf8Bytes();
-                }
-            } else {
-                throw new QueryParsingException(parseContext, "[type] filter doesn't support [" + currentFieldName + "]");
-            }
+        XContentParser.Token token = parser.nextToken();
+        if (token != XContentParser.Token.FIELD_NAME) {
+            throw new QueryParsingException(parseContext, "[type] filter should have a value field, and the type name");
         }
-
-        if (type == null) {
-            throw new QueryParsingException(parseContext, "[type] filter needs to be provided with a value for the type");
+        String fieldName = parser.currentName();
+        if (!fieldName.equals("value")) {
+            throw new QueryParsingException(parseContext, "[type] filter should have a value field, and the type name");
         }
-        return new TypeQueryBuilder(type)
-                .boost(boost)
-                .queryName(queryName);
-    }
+        token = parser.nextToken();
+        if (token != XContentParser.Token.VALUE_STRING) {
+            throw new QueryParsingException(parseContext, "[type] filter should have a value field, and the type name");
+        }
+        BytesRef type = parser.utf8Bytes();
+        // move to the next token
+        parser.nextToken();
 
-    @Override
-    public TypeQueryBuilder getBuilderPrototype() {
-        return TypeQueryBuilder.PROTOTYPE;
+        Query filter;
+        //LUCENE 4 UPGRADE document mapper should use bytesref as well? 
+        DocumentMapper documentMapper = parseContext.mapperService().documentMapper(type.utf8ToString());
+        if (documentMapper == null) {
+            filter = new TermQuery(new Term(TypeFieldMapper.NAME, type));
+        } else {
+            filter = documentMapper.typeFilter();
+        }
+        return filter;
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java
index 89b753e..654f14e 100644
--- a/core/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java
@@ -19,20 +19,9 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.WildcardQuery;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * Implements the wildcard search query. Supported wildcards are <tt>*</tt>, which
@@ -42,17 +31,17 @@ import java.util.Objects;
  * a Wildcard term should not start with one of the wildcards <tt>*</tt> or
  * <tt>?</tt>.
  */
-public class WildcardQueryBuilder extends AbstractQueryBuilder<WildcardQueryBuilder> implements MultiTermQueryBuilder<WildcardQueryBuilder> {
+public class WildcardQueryBuilder extends MultiTermQueryBuilder implements BoostableQueryBuilder<WildcardQueryBuilder> {
 
-    public static final String NAME = "wildcard";
+    private final String name;
 
-    private final String fieldName;
+    private final String wildcard;
 
-    private final String value;
+    private float boost = -1;
 
     private String rewrite;
 
-    static final WildcardQueryBuilder PROTOTYPE = new WildcardQueryBuilder(null, null);
+    private String queryName;
 
     /**
      * Implements the wildcard search query. Supported wildcards are <tt>*</tt>, which
@@ -62,20 +51,12 @@ public class WildcardQueryBuilder extends AbstractQueryBuilder<WildcardQueryBuil
      * a Wildcard term should not start with one of the wildcards <tt>*</tt> or
      * <tt>?</tt>.
      *
-     * @param fieldName The field name
-     * @param value The wildcard query string
+     * @param name     The field name
+     * @param wildcard The wildcard query string
      */
-    public WildcardQueryBuilder(String fieldName, String value) {
-        this.fieldName = fieldName;
-        this.value = value;
-    }
-
-    public String fieldName() {
-        return fieldName;
-    }
-
-    public String value() {
-        return value;
+    public WildcardQueryBuilder(String name, String wildcard) {
+        this.name = name;
+        this.wildcard = wildcard;
     }
 
     public WildcardQueryBuilder rewrite(String rewrite) {
@@ -83,83 +64,43 @@ public class WildcardQueryBuilder extends AbstractQueryBuilder<WildcardQueryBuil
         return this;
     }
 
-    public String rewrite() {
-        return this.rewrite;
-    }
-
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
     @Override
-    public String getWriteableName() {
-        return NAME;
+    public WildcardQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
-    @Override
-    public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.startObject(fieldName);
-        builder.field("wildcard", value);
-        if (rewrite != null) {
-            builder.field("rewrite", rewrite);
-        }
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public WildcardQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        String indexFieldName;
-        BytesRef valueBytes;
-
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType != null) {
-            indexFieldName = fieldType.names().indexName();
-            valueBytes = fieldType.indexedValueForSearch(value);
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(WildcardQueryParser.NAME);
+        if (boost == -1 && rewrite == null && queryName == null) {
+            builder.field(name, wildcard);
         } else {
-            indexFieldName = fieldName;
-            valueBytes = new BytesRef(value);
+            builder.startObject(name);
+            builder.field("wildcard", wildcard);
+            if (boost != -1) {
+                builder.field("boost", boost);
+            }
+            if (rewrite != null) {
+                builder.field("rewrite", rewrite);
+            }
+            if (queryName != null) {
+                builder.field("_name", queryName);
+            }
+            builder.endObject();
         }
-
-        WildcardQuery query = new WildcardQuery(new Term(indexFieldName, valueBytes));
-        MultiTermQuery.RewriteMethod rewriteMethod = QueryParsers.parseRewriteMethod(context.parseFieldMatcher(), rewrite, null);
-        QueryParsers.setRewriteMethod(query, rewriteMethod);
-        return query;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (Strings.isEmpty(this.fieldName)) {
-            validationException = addValidationError("field name cannot be null or empty.", validationException);
-        }
-        if (this.value == null) {
-            validationException = addValidationError("wildcard cannot be null", validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    protected WildcardQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        WildcardQueryBuilder wildcardQueryBuilder = new WildcardQueryBuilder(in.readString(), in.readString());
-        wildcardQueryBuilder.rewrite = in.readOptionalString();
-        return wildcardQueryBuilder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-        out.writeString(value);
-        out.writeOptionalString(rewrite);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldName, value, rewrite);
-    }
-
-    @Override
-    protected boolean doEquals(WildcardQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-                Objects.equals(value, other.value) &&
-                Objects.equals(rewrite, other.rewrite);
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java
index d3b3e26..da92db4 100644
--- a/core/src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java
@@ -19,15 +19,23 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.WildcardQuery;
+import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
 
 /**
- * Parser for wildcard query
+ *
  */
-public class WildcardQueryParser extends BaseQueryParser<WildcardQueryBuilder> {
+public class WildcardQueryParser implements QueryParser {
+
+    public static final String NAME = "wildcard";
 
     @Inject
     public WildcardQueryParser() {
@@ -35,11 +43,11 @@ public class WildcardQueryParser extends BaseQueryParser<WildcardQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{WildcardQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public WildcardQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         XContentParser.Token token = parser.nextToken();
@@ -47,10 +55,10 @@ public class WildcardQueryParser extends BaseQueryParser<WildcardQueryBuilder> {
             throw new QueryParsingException(parseContext, "[wildcard] query malformed, no field");
         }
         String fieldName = parser.currentName();
-        String rewrite = null;
+        String rewriteMethod = null;
 
         String value = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         String queryName = null;
         token = parser.nextToken();
         if (token == XContentParser.Token.START_OBJECT) {
@@ -66,7 +74,7 @@ public class WildcardQueryParser extends BaseQueryParser<WildcardQueryBuilder> {
                     } else if ("boost".equals(currentFieldName)) {
                         boost = parser.floatValue();
                     } else if ("rewrite".equals(currentFieldName)) {
-                        rewrite = parser.textOrNull();
+                        rewriteMethod = parser.textOrNull();
                     } else if ("_name".equals(currentFieldName)) {
                         queryName = parser.text();
                     } else {
@@ -83,14 +91,22 @@ public class WildcardQueryParser extends BaseQueryParser<WildcardQueryBuilder> {
         if (value == null) {
             throw new QueryParsingException(parseContext, "No value specified for prefix query");
         }
-        return new WildcardQueryBuilder(fieldName, value)
-                .rewrite(rewrite)
-                .boost(boost)
-                .queryName(queryName);
-    }
 
-    @Override
-    public WildcardQueryBuilder getBuilderPrototype() {
-        return WildcardQueryBuilder.PROTOTYPE;
+        BytesRef valueBytes;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            fieldName = fieldType.names().indexName();
+            valueBytes = fieldType.indexedValueForSearch(value);
+        } else {
+            valueBytes = new BytesRef(value);
+        }
+
+        WildcardQuery wildcardQuery = new WildcardQuery(new Term(fieldName, valueBytes));
+        QueryParsers.setRewriteMethod(wildcardQuery, parseContext.parseFieldMatcher(), rewriteMethod);
+        wildcardQuery.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, wildcardQuery);
+        }
+        return wildcardQuery;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java
index 7be9293..6fde3c7 100644
--- a/core/src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java
@@ -20,16 +20,10 @@
 package org.elasticsearch.index.query;
 
 import com.google.common.base.Charsets;
-import org.apache.lucene.search.Query;
 import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
-import java.util.Arrays;
 
 /**
  * A Query builder which allows building a query given JSON string or binary data provided as input. This is useful when you want
@@ -45,24 +39,28 @@ import java.util.Arrays;
  * }
  * </pre>
  */
-public class WrapperQueryBuilder extends AbstractQueryBuilder<WrapperQueryBuilder> {
+public class WrapperQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "wrapper";
     private final byte[] source;
-    static final WrapperQueryBuilder PROTOTYPE = new WrapperQueryBuilder((byte[]) null);
+    private final int offset;
+    private final int length;
 
     /**
      * Creates a query builder given a query provided as a string
      */
     public WrapperQueryBuilder(String source) {
         this.source = source.getBytes(Charsets.UTF_8);
+        this.offset = 0;
+        this.length = this.source.length;
     }
 
     /**
      * Creates a query builder given a query provided as a bytes array
      */
-    public WrapperQueryBuilder(byte[] source) {
+    public WrapperQueryBuilder(byte[] source, int offset, int length) {
         this.source = source;
+        this.offset = offset;
+        this.length = length;
     }
 
     /**
@@ -70,66 +68,14 @@ public class WrapperQueryBuilder extends AbstractQueryBuilder<WrapperQueryBuilde
      */
     public WrapperQueryBuilder(BytesReference source) {
         this.source = source.array();
-    }
-
-    public byte[] source() {
-        return this.source;
-    }
-
-    @Override
-    public String getName() {
-        return NAME;
+        this.offset = source.arrayOffset();
+        this.length = source.length();
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("query", source);
+        builder.startObject(WrapperQueryParser.NAME);
+        builder.field("query", source, offset, length);
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        try (XContentParser qSourceParser = XContentFactory.xContent(source).createParser(source)) {
-            final QueryShardContext contextCopy = new QueryShardContext(context.index(), context.indexQueryParserService());
-            contextCopy.reset(qSourceParser);
-            QueryBuilder result = contextCopy.parseContext().parseInnerQueryBuilder();
-            context.combineNamedQueries(contextCopy);
-            return result.toQuery(context);
-        }
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (this.source == null || this.source.length == 0) {
-            validationException = addValidationError("query source text cannot be null or empty", validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    protected WrapperQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new WrapperQueryBuilder(in.readByteArray());
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeByteArray(this.source);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Arrays.hashCode(source);
-    }
-
-    @Override
-    protected boolean doEquals(WrapperQueryBuilder other) {
-        return Arrays.equals(source, other.source);   // otherwise we compare pointers
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java
index cd9eb83..331ba78 100644
--- a/core/src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java
@@ -19,7 +19,9 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
@@ -27,7 +29,9 @@ import java.io.IOException;
 /**
  * Query parser for JSON Queries.
  */
-public class WrapperQueryParser extends BaseQueryParser {
+public class WrapperQueryParser implements QueryParser {
+
+    public static final String NAME = "wrapper";
 
     @Inject
     public WrapperQueryParser() {
@@ -35,11 +39,11 @@ public class WrapperQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{WrapperQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         XContentParser.Token token = parser.nextToken();
@@ -52,18 +56,14 @@ public class WrapperQueryParser extends BaseQueryParser {
         }
         parser.nextToken();
 
-        byte[] source = parser.binaryValue();
-
-        parser.nextToken();
-
-        if (source == null) {
-            throw new QueryParsingException(parseContext, "wrapper query has no [query] specified");
+        byte[] querySource = parser.binaryValue();
+        try (XContentParser qSourceParser = XContentFactory.xContent(querySource).createParser(querySource)) {
+            final QueryParseContext context = new QueryParseContext(parseContext.index(), parseContext.indexQueryParserService());
+            context.reset(qSourceParser);
+            Query result = context.parseInnerQuery();
+            parser.nextToken();
+            parseContext.combineNamedQueries(context);
+            return result;
         }
-        return new WrapperQueryBuilder(source);
-    }
-
-    @Override
-    public WrapperQueryBuilder getBuilderPrototype() {
-        return WrapperQueryBuilder.PROTOTYPE;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java
index 7580c84..3dc2427 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java
@@ -43,7 +43,7 @@ import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.core.DateFieldMapper;
 import org.elasticsearch.index.mapper.core.NumberFieldMapper;
 import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.query.functionscore.gauss.GaussDecayFunctionBuilder;
 import org.elasticsearch.index.query.functionscore.gauss.GaussDecayFunctionParser;
@@ -119,7 +119,7 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
      *
      * */
     @Override
-    public ScoreFunction parse(QueryShardContext context, XContentParser parser) throws IOException, QueryParsingException {
+    public ScoreFunction parse(QueryParseContext parseContext, XContentParser parser) throws IOException, QueryParsingException {
         String currentFieldName;
         XContentParser.Token token;
         AbstractDistanceScoreFunction scoreFunction;
@@ -132,7 +132,7 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
             if (token == XContentParser.Token.START_OBJECT) {
                 variableContent.copyCurrentStructure(parser);
                 fieldName = currentFieldName;
-            } else if (context.parseFieldMatcher().match(currentFieldName, MULTI_VALUE_MODE)) {
+            } else if (parseContext.parseFieldMatcher().match(currentFieldName, MULTI_VALUE_MODE)) {
                 multiValueMode = parser.text();
             } else {
                 throw new ElasticsearchParseException("malformed score function score parameters.");
@@ -142,34 +142,34 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
             throw new ElasticsearchParseException("malformed score function score parameters.");
         }
         XContentParser variableParser = XContentFactory.xContent(variableContent.string()).createParser(variableContent.string());
-        scoreFunction = parseVariable(fieldName, variableParser, context, MultiValueMode.fromString(multiValueMode.toUpperCase(Locale.ROOT)));
+        scoreFunction = parseVariable(fieldName, variableParser, parseContext, MultiValueMode.fromString(multiValueMode.toUpperCase(Locale.ROOT)));
         return scoreFunction;
     }
 
     // parses origin and scale parameter for field "fieldName"
-    private AbstractDistanceScoreFunction parseVariable(String fieldName, XContentParser parser, QueryShardContext context, MultiValueMode mode) throws IOException {
+    private AbstractDistanceScoreFunction parseVariable(String fieldName, XContentParser parser, QueryParseContext parseContext, MultiValueMode mode) throws IOException {
 
         // now, the field must exist, else we cannot read the value for
         // the doc later
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
         if (fieldType == null) {
-            throw new QueryParsingException(context.parseContext(), "unknown field [{}]", fieldName);
+            throw new QueryParsingException(parseContext, "unknown field [{}]", fieldName);
         }
 
         // dates and time need special handling
         parser.nextToken();
         if (fieldType instanceof DateFieldMapper.DateFieldType) {
-            return parseDateVariable(fieldName, parser, context, (DateFieldMapper.DateFieldType) fieldType, mode);
+            return parseDateVariable(fieldName, parser, parseContext, (DateFieldMapper.DateFieldType) fieldType, mode);
         } else if (fieldType instanceof GeoPointFieldMapper.GeoPointFieldType) {
-            return parseGeoVariable(fieldName, parser, context, (GeoPointFieldMapper.GeoPointFieldType) fieldType, mode);
+            return parseGeoVariable(fieldName, parser, parseContext, (GeoPointFieldMapper.GeoPointFieldType) fieldType, mode);
         } else if (fieldType instanceof NumberFieldMapper.NumberFieldType) {
-            return parseNumberVariable(fieldName, parser, context, (NumberFieldMapper.NumberFieldType) fieldType, mode);
+            return parseNumberVariable(fieldName, parser, parseContext, (NumberFieldMapper.NumberFieldType) fieldType, mode);
         } else {
-            throw new QueryParsingException(context.parseContext(), "field [{}] is of type [{}], but only numeric types are supported.", fieldName, fieldType);
+            throw new QueryParsingException(parseContext, "field [{}] is of type [{}], but only numeric types are supported.", fieldName, fieldType);
         }
     }
 
-    private AbstractDistanceScoreFunction parseNumberVariable(String fieldName, XContentParser parser, QueryShardContext context,
+    private AbstractDistanceScoreFunction parseNumberVariable(String fieldName, XContentParser parser, QueryParseContext parseContext,
             NumberFieldMapper.NumberFieldType fieldType, MultiValueMode mode) throws IOException {
         XContentParser.Token token;
         String parameterName = null;
@@ -199,11 +199,11 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
         if (!scaleFound || !refFound) {
             throw new ElasticsearchParseException("both [{}] and [{}] must be set for numeric fields.", DecayFunctionBuilder.SCALE, DecayFunctionBuilder.ORIGIN);
         }
-        IndexNumericFieldData numericFieldData = context.getForField(fieldType);
+        IndexNumericFieldData numericFieldData = parseContext.getForField(fieldType);
         return new NumericFieldDataScoreFunction(origin, scale, decay, offset, getDecayFunction(), numericFieldData, mode);
     }
 
-    private AbstractDistanceScoreFunction parseGeoVariable(String fieldName, XContentParser parser, QueryShardContext context,
+    private AbstractDistanceScoreFunction parseGeoVariable(String fieldName, XContentParser parser, QueryParseContext parseContext,
             GeoPointFieldMapper.GeoPointFieldType fieldType, MultiValueMode mode) throws IOException {
         XContentParser.Token token;
         String parameterName = null;
@@ -231,12 +231,12 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
         }
         double scale = DistanceUnit.DEFAULT.parse(scaleString, DistanceUnit.DEFAULT);
         double offset = DistanceUnit.DEFAULT.parse(offsetString, DistanceUnit.DEFAULT);
-        IndexGeoPointFieldData indexFieldData = context.getForField(fieldType);
+        IndexGeoPointFieldData indexFieldData = parseContext.getForField(fieldType);
         return new GeoFieldDataScoreFunction(origin, scale, decay, offset, getDecayFunction(), indexFieldData, mode);
 
     }
 
-    private AbstractDistanceScoreFunction parseDateVariable(String fieldName, XContentParser parser, QueryShardContext context,
+    private AbstractDistanceScoreFunction parseDateVariable(String fieldName, XContentParser parser, QueryParseContext parseContext,
             DateFieldMapper.DateFieldType dateFieldType, MultiValueMode mode) throws IOException {
         XContentParser.Token token;
         String parameterName = null;
@@ -271,7 +271,7 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
         double scale = val.getMillis();
         val = TimeValue.parseTimeValue(offsetString, TimeValue.timeValueHours(24), getClass().getSimpleName() + ".offset");
         double offset = val.getMillis();
-        IndexNumericFieldData numericFieldData = context.getForField(dateFieldType);
+        IndexNumericFieldData numericFieldData = parseContext.getForField(dateFieldType);
         return new NumericFieldDataScoreFunction(origin, scale, decay, offset, getDecayFunction(), numericFieldData, mode);
     }
 
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java
index 3880592..dc7571a 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java
@@ -21,7 +21,7 @@ package org.elasticsearch.index.query.functionscore;
 
 import org.elasticsearch.common.lucene.search.function.CombineFunction;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.AbstractQueryBuilder;
+import org.elasticsearch.index.query.BoostableQueryBuilder;
 import org.elasticsearch.index.query.QueryBuilder;
 
 import java.io.IOException;
@@ -31,12 +31,14 @@ import java.util.ArrayList;
  * A query that uses a filters with a script associated with them to compute the
  * score.
  */
-public class FunctionScoreQueryBuilder extends AbstractQueryBuilder<FunctionScoreQueryBuilder> {
+public class FunctionScoreQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<FunctionScoreQueryBuilder> {
 
     private final QueryBuilder queryBuilder;
 
     private final QueryBuilder filterBuilder;
 
+    private Float boost;
+
     private Float maxBoost;
 
     private String scoreMode;
@@ -47,8 +49,6 @@ public class FunctionScoreQueryBuilder extends AbstractQueryBuilder<FunctionScor
     private ArrayList<ScoreFunctionBuilder> scoreFunctions = new ArrayList<>();
     private Float minScore = null;
 
-    static final FunctionScoreQueryBuilder PROTOTYPE = new FunctionScoreQueryBuilder();
-
     /**
      * Creates a function_score query that executes on documents that match query a query.
      * Query and filter will be wrapped into a filtered_query.
@@ -143,6 +143,17 @@ public class FunctionScoreQueryBuilder extends AbstractQueryBuilder<FunctionScor
         return this;
     }
 
+    /**
+     * Sets the boost for this query. Documents matching this query will (in
+     * addition to the normal weightings) have their score multiplied by the
+     * boost provided.
+     */
+    @Override
+    public FunctionScoreQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
         builder.startObject(FunctionScoreQueryParser.NAME);
@@ -175,10 +186,13 @@ public class FunctionScoreQueryBuilder extends AbstractQueryBuilder<FunctionScor
         if (maxBoost != null) {
             builder.field("max_boost", maxBoost);
         }
+        if (boost != null) {
+            builder.field("boost", boost);
+        }
         if (minScore != null) {
             builder.field("min_score", minScore);
         }
-        printBoostAndQueryName(builder);
+
         builder.endObject();
     }
 
@@ -186,9 +200,4 @@ public class FunctionScoreQueryBuilder extends AbstractQueryBuilder<FunctionScor
         this.minScore = minScore;
         return this;
     }
-
-    @Override
-    public String getWriteableName() {
-        return FunctionScoreQueryParser.NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java
index 3df6b78..02fc425 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java
@@ -37,7 +37,9 @@ import org.elasticsearch.common.lucene.search.function.FunctionScoreQuery;
 import org.elasticsearch.common.lucene.search.function.ScoreFunction;
 import org.elasticsearch.common.lucene.search.function.WeightFactorFunction;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.*;
+import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.query.QueryParser;
+import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.query.functionscore.factor.FactorParser;
 
 import java.io.IOException;
@@ -45,7 +47,7 @@ import java.util.ArrayList;
 import java.util.Arrays;
 
 /**
- * Parser for function_score query
+ *
  */
 public class FunctionScoreQueryParser implements QueryParser {
 
@@ -82,14 +84,12 @@ public class FunctionScoreQueryParser implements QueryParser {
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         Query query = null;
         Query filter = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        String queryName = null;
+        float boost = 1.0f;
 
         FiltersFunctionScoreQuery.ScoreMode scoreMode = FiltersFunctionScoreQuery.ScoreMode.Multiply;
         ArrayList<FiltersFunctionScoreQuery.FilterFunction> filterFunctions = new ArrayList<>();
@@ -119,8 +119,6 @@ public class FunctionScoreQueryParser implements QueryParser {
                 maxBoost = parser.floatValue();
             } else if ("boost".equals(currentFieldName)) {
                 boost = parser.floatValue();
-            } else if ("_name".equals(currentFieldName)) {
-                queryName = parser.text();
             } else if ("min_score".equals(currentFieldName) || "minScore".equals(currentFieldName)) {
                 minScore = parser.floatValue();
             } else if ("functions".equals(currentFieldName)) {
@@ -128,7 +126,7 @@ public class FunctionScoreQueryParser implements QueryParser {
                     String errorString = "already found [" + singleFunctionName + "], now encountering [functions].";
                     handleMisplacedFunctionsDeclaration(errorString, singleFunctionName);
                 }
-                currentFieldName = parseFiltersAndFunctions(context, parser, filterFunctions, currentFieldName);
+                currentFieldName = parseFiltersAndFunctions(parseContext, parser, filterFunctions, currentFieldName);
                 functionArrayFound = true;
             } else {
                 ScoreFunction scoreFunction;
@@ -139,7 +137,7 @@ public class FunctionScoreQueryParser implements QueryParser {
                     // we try to parse a score function. If there is no score
                     // function for the current field name,
                     // functionParserMapper.get() will throw an Exception.
-                    scoreFunction = functionParserMapper.get(parseContext, currentFieldName).parse(context, parser);
+                    scoreFunction = functionParserMapper.get(parseContext, currentFieldName).parse(parseContext, parser);
                 }
                 if (functionArrayFound) {
                     String errorString = "already found [functions] array, now encountering [" + currentFieldName + "].";
@@ -170,7 +168,6 @@ public class FunctionScoreQueryParser implements QueryParser {
         if (maxBoost == null) {
             maxBoost = Float.MAX_VALUE;
         }
-        Query result;
         // handle cases where only one score function and no filter was
         // provided. In this case we create a FunctionScoreQuery.
         if (filterFunctions.size() == 0 || filterFunctions.size() == 1 && (filterFunctions.get(0).filter == null || Queries.isConstantMatchAllQuery(filterFunctions.get(0).filter))) {
@@ -179,8 +176,9 @@ public class FunctionScoreQueryParser implements QueryParser {
             if (combineFunction != null) {
                 theQuery.setCombineFunction(combineFunction);
             }
+            theQuery.setBoost(boost);
             theQuery.setMaxBoost(maxBoost);
-            result = theQuery;
+            return theQuery;
             // in all other cases we create a FiltersFunctionScoreQuery.
         } else {
             FiltersFunctionScoreQuery functionScoreQuery = new FiltersFunctionScoreQuery(query, scoreMode,
@@ -188,13 +186,9 @@ public class FunctionScoreQueryParser implements QueryParser {
             if (combineFunction != null) {
                 functionScoreQuery.setCombineFunction(combineFunction);
             }
-            result = functionScoreQuery;
-        }
-        result.setBoost(boost);
-        if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            functionScoreQuery.setBoost(boost);
+            return functionScoreQuery;
         }
-        return result;
     }
 
     private void handleMisplacedFunctionsDeclaration(String errorString, String functionName) {
@@ -205,9 +199,8 @@ public class FunctionScoreQueryParser implements QueryParser {
         throw new ElasticsearchParseException("failed to parse [{}] query. [{}]", NAME, errorString);
     }
 
-    private String parseFiltersAndFunctions(QueryShardContext context, XContentParser parser,
+    private String parseFiltersAndFunctions(QueryParseContext parseContext, XContentParser parser,
                                             ArrayList<FiltersFunctionScoreQuery.FilterFunction> filterFunctions, String currentFieldName) throws IOException {
-        QueryParseContext parseContext = context.parseContext();
         XContentParser.Token token;
         while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
             Query filter = null;
@@ -229,7 +222,7 @@ public class FunctionScoreQueryParser implements QueryParser {
                             // functionParserMapper throws exception if parser
                             // non-existent
                             ScoreFunctionParser functionParser = functionParserMapper.get(parseContext, currentFieldName);
-                            scoreFunction = functionParser.parse(context, parser);
+                            scoreFunction = functionParser.parse(parseContext, parser);
                         }
                     }
                 }
@@ -276,16 +269,4 @@ public class FunctionScoreQueryParser implements QueryParser {
         }
         return cf;
     }
-
-    //norelease to be removed once all queries are moved over to extend BaseQueryParser
-    @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
-        Query query = parse(parseContext.shardContext());
-        return new QueryWrappingQueryBuilder(query);
-    }
-
-    @Override
-    public FunctionScoreQueryBuilder getBuilderPrototype() {
-        return FunctionScoreQueryBuilder.PROTOTYPE;
-    }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionParser.java
index 4065f08..74c3d08 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionParser.java
@@ -21,14 +21,14 @@ package org.elasticsearch.index.query.functionscore;
 
 import org.elasticsearch.common.lucene.search.function.ScoreFunction;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParsingException;
 
 import java.io.IOException;
 
 public interface ScoreFunctionParser {
 
-    ScoreFunction parse(QueryShardContext context, XContentParser parser) throws IOException, QueryParsingException;
+    ScoreFunction parse(QueryParseContext parseContext, XContentParser parser) throws IOException, QueryParsingException;
 
     /**
      * Returns the name of the function, for example "linear", "gauss" etc. This
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/factor/FactorParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/factor/FactorParser.java
index 2635c2b..a1c8d20 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/factor/FactorParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/factor/FactorParser.java
@@ -19,13 +19,14 @@
 
 package org.elasticsearch.index.query.functionscore.factor;
 
+import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
+
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.lucene.search.function.BoostScoreFunction;
 import org.elasticsearch.common.lucene.search.function.ScoreFunction;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParsingException;
-import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
 
 import java.io.IOException;
 
@@ -42,7 +43,7 @@ public class FactorParser implements ScoreFunctionParser {
     }
 
     @Override
-    public ScoreFunction parse(QueryShardContext context, XContentParser parser) throws IOException, QueryParsingException {
+    public ScoreFunction parse(QueryParseContext parseContext, XContentParser parser) throws IOException, QueryParsingException {
         float boostFactor = parser.floatValue();
         return new BoostScoreFunction(boostFactor);
     }
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/fieldvaluefactor/FieldValueFactorFunctionParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/fieldvaluefactor/FieldValueFactorFunctionParser.java
index 140f541..6f68db5 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/fieldvaluefactor/FieldValueFactorFunctionParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/fieldvaluefactor/FieldValueFactorFunctionParser.java
@@ -19,13 +19,15 @@
 
 package org.elasticsearch.index.query.functionscore.fieldvaluefactor;
 
+import org.apache.lucene.document.FieldType;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.common.lucene.search.function.FieldValueFactorFunction;
 import org.elasticsearch.common.lucene.search.function.ScoreFunction;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.fielddata.IndexNumericFieldData;
+import org.elasticsearch.index.fielddata.plain.DoubleArrayIndexFieldData;
+import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.QueryShardContext;
 import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
@@ -52,8 +54,7 @@ public class FieldValueFactorFunctionParser implements ScoreFunctionParser {
     public static String[] NAMES = { "field_value_factor", "fieldValueFactor" };
 
     @Override
-    public ScoreFunction parse(QueryShardContext context, XContentParser parser) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public ScoreFunction parse(QueryParseContext parseContext, XContentParser parser) throws IOException, QueryParsingException {
 
         String currentFieldName = null;
         String field = null;
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionParser.java
index 20c2f55..124336c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionParser.java
@@ -27,8 +27,8 @@ import org.elasticsearch.common.lucene.search.function.RandomScoreFunction;
 import org.elasticsearch.common.lucene.search.function.ScoreFunction;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.fielddata.IndexFieldData;
+import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.QueryShardContext;
 import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
@@ -51,8 +51,8 @@ public class RandomScoreFunctionParser implements ScoreFunctionParser {
     }
 
     @Override
-    public ScoreFunction parse(QueryShardContext context, XContentParser parser) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public ScoreFunction parse(QueryParseContext parseContext, XContentParser parser) throws IOException, QueryParsingException {
+
         int seed = -1;
 
         String currentFieldName = null;
@@ -90,7 +90,7 @@ public class RandomScoreFunctionParser implements ScoreFunctionParser {
         }
 
         if (seed == -1) {
-            seed = Longs.hashCode(context.nowInMillis());
+            seed = Longs.hashCode(parseContext.nowInMillis());
         }
         final ShardId shardId = SearchContext.current().indexShard().shardId();
         final int salt = (shardId.index().name().hashCode() << 10) | shardId.id();
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/script/ScriptScoreFunctionParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/script/ScriptScoreFunctionParser.java
index 38a29f3..2cf066f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/script/ScriptScoreFunctionParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/script/ScriptScoreFunctionParser.java
@@ -21,11 +21,11 @@
 
 package org.elasticsearch.index.query.functionscore.script;
 
+import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.lucene.search.function.ScoreFunction;
 import org.elasticsearch.common.lucene.search.function.ScriptScoreFunction;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardContext;
 import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
@@ -58,8 +58,7 @@ public class ScriptScoreFunctionParser implements ScoreFunctionParser {
     }
 
     @Override
-    public ScoreFunction parse(QueryShardContext context, XContentParser parser) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public ScoreFunction parse(QueryParseContext parseContext, XContentParser parser) throws IOException, QueryParsingException {
         ScriptParameterParser scriptParameterParser = new ScriptParameterParser();
         Script script = null;
         Map<String, Object> vars = null;
@@ -101,7 +100,7 @@ public class ScriptScoreFunctionParser implements ScoreFunctionParser {
 
         SearchScript searchScript;
         try {
-            searchScript = context.scriptService().search(context.lookup(), script, ScriptContext.Standard.SEARCH);
+            searchScript = parseContext.scriptService().search(parseContext.lookup(), script, ScriptContext.Standard.SEARCH);
             return new ScriptScoreFunction(script, searchScript);
         } catch (Exception e) {
             throw new QueryParsingException(parseContext, NAMES[0] + " the script could not be loaded", e);
diff --git a/core/src/main/java/org/elasticsearch/index/query/support/InnerHitsQueryParserHelper.java b/core/src/main/java/org/elasticsearch/index/query/support/InnerHitsQueryParserHelper.java
index d164896..b4d3e63 100644
--- a/core/src/main/java/org/elasticsearch/index/query/support/InnerHitsQueryParserHelper.java
+++ b/core/src/main/java/org/elasticsearch/index/query/support/InnerHitsQueryParserHelper.java
@@ -21,7 +21,6 @@ package org.elasticsearch.index.query.support;
 
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardException;
 import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsParseElement;
diff --git a/core/src/main/java/org/elasticsearch/index/query/support/NestedInnerQueryParseSupport.java b/core/src/main/java/org/elasticsearch/index/query/support/NestedInnerQueryParseSupport.java
index 8c90f28..49e1a21 100644
--- a/core/src/main/java/org/elasticsearch/index/query/support/NestedInnerQueryParseSupport.java
+++ b/core/src/main/java/org/elasticsearch/index/query/support/NestedInnerQueryParseSupport.java
@@ -28,9 +28,8 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
-import org.elasticsearch.index.query.QueryShardContext;
-import org.elasticsearch.index.query.QueryShardException;
 import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
@@ -42,7 +41,6 @@ import java.io.IOException;
  */
 public class NestedInnerQueryParseSupport {
 
-    protected final QueryShardContext shardContext;
     protected final QueryParseContext parseContext;
 
     private BytesReference source;
@@ -62,15 +60,12 @@ public class NestedInnerQueryParseSupport {
     private ObjectMapper parentObjectMapper;
 
     public NestedInnerQueryParseSupport(XContentParser parser, SearchContext searchContext) {
-        parseContext = searchContext.queryParserService().getShardContext().parseContext();
-        shardContext = searchContext.queryParserService().getShardContext();
-        shardContext.reset(parser);
-
+        parseContext = searchContext.queryParserService().getParseContext();
+        parseContext.reset(parser);
     }
 
-    public NestedInnerQueryParseSupport(QueryShardContext context) {
-        this.parseContext = context.parseContext();
-        this.shardContext = context;
+    public NestedInnerQueryParseSupport(QueryParseContext parseContext) {
+        this.parseContext = parseContext;
     }
 
     public void query() throws IOException {
@@ -108,10 +103,10 @@ public class NestedInnerQueryParseSupport {
             return innerQuery;
         } else {
             if (path == null) {
-                throw new QueryShardException(shardContext, "[nested] requires 'path' field");
+                throw new QueryParsingException(parseContext, "[nested] requires 'path' field");
             }
             if (!queryFound) {
-                throw new QueryShardException(shardContext, "[nested] requires either 'query' or 'filter' field");
+                throw new QueryParsingException(parseContext, "[nested] requires either 'query' or 'filter' field");
             }
 
             XContentParser old = parseContext.parser();
@@ -137,10 +132,10 @@ public class NestedInnerQueryParseSupport {
             return innerFilter;
         } else {
             if (path == null) {
-                throw new QueryShardException(shardContext, "[nested] requires 'path' field");
+                throw new QueryParsingException(parseContext, "[nested] requires 'path' field");
             }
             if (!filterFound) {
-                throw new QueryShardException(shardContext, "[nested] requires either 'query' or 'filter' field");
+                throw new QueryParsingException(parseContext, "[nested] requires either 'query' or 'filter' field");
             }
 
             setPathLevel();
@@ -160,12 +155,12 @@ public class NestedInnerQueryParseSupport {
 
     public void setPath(String path) {
         this.path = path;
-        nestedObjectMapper = shardContext.getObjectMapper(path);
+        nestedObjectMapper = parseContext.getObjectMapper(path);
         if (nestedObjectMapper == null) {
-            throw new QueryShardException(shardContext, "[nested] failed to find nested object under path [" + path + "]");
+            throw new QueryParsingException(parseContext, "[nested] failed to find nested object under path [" + path + "]");
         }
         if (!nestedObjectMapper.nested().isNested()) {
-            throw new QueryShardException(shardContext, "[nested] nested object under path [" + path + "] is not of nested type");
+            throw new QueryParsingException(parseContext, "[nested] nested object under path [" + path + "] is not of nested type");
         }
     }
 
@@ -190,18 +185,18 @@ public class NestedInnerQueryParseSupport {
     }
 
     private void setPathLevel() {
-        ObjectMapper objectMapper = shardContext.nestedScope().getObjectMapper();
+        ObjectMapper objectMapper = parseContext.nestedScope().getObjectMapper();
         if (objectMapper == null) {
-            parentFilter = shardContext.bitsetFilter(Queries.newNonNestedFilter());
+            parentFilter = parseContext.bitsetFilter(Queries.newNonNestedFilter());
         } else {
-            parentFilter = shardContext.bitsetFilter(objectMapper.nestedTypeFilter());
+            parentFilter = parseContext.bitsetFilter(objectMapper.nestedTypeFilter());
         }
         childFilter = nestedObjectMapper.nestedTypeFilter();
-        parentObjectMapper = shardContext.nestedScope().nextLevel(nestedObjectMapper);
+        parentObjectMapper = parseContext.nestedScope().nextLevel(nestedObjectMapper);
     }
 
     private void resetPathLevel() {
-        shardContext.nestedScope().previousLevel();
+        parseContext.nestedScope().previousLevel();
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/support/QueryParsers.java b/core/src/main/java/org/elasticsearch/index/query/support/QueryParsers.java
index a500393..1a12c74 100644
--- a/core/src/main/java/org/elasticsearch/index/query/support/QueryParsers.java
+++ b/core/src/main/java/org/elasticsearch/index/query/support/QueryParsers.java
@@ -29,12 +29,12 @@ import org.elasticsearch.common.ParseFieldMatcher;
  */
 public final class QueryParsers {
 
-    public static final ParseField CONSTANT_SCORE = new ParseField("constant_score", "constant_score_auto", "constant_score_filter");
-    public static final ParseField SCORING_BOOLEAN = new ParseField("scoring_boolean");
-    public static final ParseField CONSTANT_SCORE_BOOLEAN = new ParseField("constant_score_boolean");
-    public static final ParseField TOP_TERMS = new ParseField("top_terms_");
-    public static final ParseField TOP_TERMS_BOOST = new ParseField("top_terms_boost_");
-    public static final ParseField TOP_TERMS_BLENDED_FREQS = new ParseField("top_terms_blended_freqs_");
+    private static final ParseField CONSTANT_SCORE = new ParseField("constant_score", "constant_score_auto", "constant_score_filter");
+    private static final ParseField SCORING_BOOLEAN = new ParseField("scoring_boolean");
+    private static final ParseField CONSTANT_SCORE_BOOLEAN = new ParseField("constant_score_boolean");
+    private static final ParseField TOP_TERMS = new ParseField("top_terms_");
+    private static final ParseField TOP_TERMS_BOOST = new ParseField("top_terms_boost_");
+    private static final ParseField TOP_TERMS_BLENDED_FREQS = new ParseField("top_terms_blended_freqs_");
 
     private QueryParsers() {
 
diff --git a/core/src/main/java/org/elasticsearch/index/query/support/XContentStructure.java b/core/src/main/java/org/elasticsearch/index/query/support/XContentStructure.java
index 0d4a4d9..37716d1 100644
--- a/core/src/main/java/org/elasticsearch/index/query/support/XContentStructure.java
+++ b/core/src/main/java/org/elasticsearch/index/query/support/XContentStructure.java
@@ -25,7 +25,6 @@ import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardContext;
 import org.elasticsearch.index.query.QueryParseContext;
 
 import java.io.IOException;
@@ -38,7 +37,6 @@ import java.io.IOException;
  * immediately, however, the extra overhead means that the type not be
  * extracted prior to query parsing (in the case of unordered JSON).
  */
-//norelease we should be able to delete this class once all queries are refactored
 public abstract class XContentStructure {
 
     private final QueryParseContext parseContext;
@@ -86,14 +84,14 @@ public abstract class XContentStructure {
         BytesReference br = this.bytes();
         assert br != null : "innerBytes must be set with .bytes(bytes) or .freeze() before parsing";
         XContentParser innerParser = XContentHelper.createParser(br);
-        String[] origTypes = QueryShardContext.setTypesWithPrevious(types);
+        String[] origTypes = QueryParseContext.setTypesWithPrevious(types);
         XContentParser old = parseContext.parser();
         parseContext.parser(innerParser);
         try {
             return parseContext.parseInnerQuery();
         } finally {
             parseContext.parser(old);
-            QueryShardContext.setTypes(origTypes);
+            QueryParseContext.setTypes(origTypes);
         }
     }
 
@@ -102,20 +100,18 @@ public abstract class XContentStructure {
      * parses the query in a streaming manner if the types are available at
      * construction time.
      */
-    //norelease we should be able to delete this class once all queries are refactored
-    @Deprecated
     public static class InnerQuery extends XContentStructure {
         private Query query = null;
         private boolean queryParsed = false;
         public InnerQuery(QueryParseContext parseContext1, @Nullable String... types) throws IOException {
             super(parseContext1);
             if (types != null) {
-                String[] origTypes = QueryShardContext.setTypesWithPrevious(types);
+                String[] origTypes = QueryParseContext.setTypesWithPrevious(types);
                 try {
                     query = parseContext1.parseInnerQuery();
                     queryParsed = true;
                 } finally {
-                    QueryShardContext.setTypes(origTypes);
+                    QueryParseContext.setTypes(origTypes);
                 }
             } else {
                 BytesReference innerBytes = XContentFactory.smileBuilder().copyCurrentStructure(parseContext1.parser()).bytes();
diff --git a/core/src/main/java/org/elasticsearch/index/search/MatchQuery.java b/core/src/main/java/org/elasticsearch/index/search/MatchQuery.java
index 0b5dae6..fb5fff8 100644
--- a/core/src/main/java/org/elasticsearch/index/search/MatchQuery.java
+++ b/core/src/main/java/org/elasticsearch/index/search/MatchQuery.java
@@ -30,7 +30,7 @@ import org.elasticsearch.common.lucene.search.MultiPhrasePrefixQuery;
 import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.unit.Fuzziness;
 import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
@@ -49,7 +49,7 @@ public class MatchQuery {
         ALL
     }
 
-    protected final QueryShardContext context;
+    protected final QueryParseContext parseContext;
 
     protected String analyzer;
 
@@ -60,9 +60,9 @@ public class MatchQuery {
     protected int phraseSlop = 0;
 
     protected Fuzziness fuzziness = null;
-
+    
     protected int fuzzyPrefixLength = FuzzyQuery.defaultPrefixLength;
-
+    
     protected int maxExpansions = FuzzyQuery.defaultMaxExpansions;
 
     protected boolean transpositions = FuzzyQuery.defaultTranspositions;
@@ -72,11 +72,11 @@ public class MatchQuery {
     protected boolean lenient;
 
     protected ZeroTermsQuery zeroTermsQuery = ZeroTermsQuery.NONE;
-
+    
     protected Float commonTermsCutoff = null;
-
-    public MatchQuery(QueryShardContext context) {
-        this.context = context;
+    
+    public MatchQuery(QueryParseContext parseContext) {
+        this.parseContext = parseContext;
     }
 
     public void setAnalyzer(String analyzer) {
@@ -86,7 +86,7 @@ public class MatchQuery {
     public void setOccur(BooleanClause.Occur occur) {
         this.occur = occur;
     }
-
+    
     public void setCommonTermsCutoff(float cutoff) {
         this.commonTermsCutoff = Float.valueOf(cutoff);
     }
@@ -134,11 +134,11 @@ public class MatchQuery {
     protected Analyzer getAnalyzer(MappedFieldType fieldType) {
         if (this.analyzer == null) {
             if (fieldType != null) {
-                return context.getSearchAnalyzer(fieldType);
+                return parseContext.getSearchAnalyzer(fieldType);
             }
-            return context.mapperService().searchAnalyzer();
+            return parseContext.mapperService().searchAnalyzer();
         } else {
-            Analyzer analyzer = context.mapperService().analysisService().analyzer(this.analyzer);
+            Analyzer analyzer = parseContext.mapperService().analysisService().analyzer(this.analyzer);
             if (analyzer == null) {
                 throw new IllegalArgumentException("No analyzer found for [" + this.analyzer + "]");
             }
@@ -148,7 +148,7 @@ public class MatchQuery {
 
     public Query parse(Type type, String fieldName, Object value) throws IOException {
         final String field;
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
         if (fieldType != null) {
             field = fieldType.names().indexName();
         } else {
@@ -157,14 +157,14 @@ public class MatchQuery {
 
         if (fieldType != null && fieldType.useTermQueryWithQueryString() && !forceAnalyzeQueryString()) {
             try {
-                return fieldType.termQuery(value, context);
+                return fieldType.termQuery(value, parseContext);
             } catch (RuntimeException e) {
                 if (lenient) {
                     return null;
                 }
                 throw e;
             }
-
+            
         }
         Analyzer analyzer = getAnalyzer(fieldType);
         assert analyzer != null;
diff --git a/core/src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java b/core/src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java
index 363e427..34bf944 100644
--- a/core/src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java
+++ b/core/src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java
@@ -31,7 +31,7 @@ import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.query.MultiMatchQueryBuilder;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -47,10 +47,10 @@ public class MultiMatchQuery extends MatchQuery {
         this.groupTieBreaker = tieBreaker;
     }
 
-    public MultiMatchQuery(QueryShardContext context) {
-        super(context);
+    public MultiMatchQuery(QueryParseContext parseContext) {
+        super(parseContext);
     }
-
+    
     private Query parseAndApply(Type type, String fieldName, Object value, String minimumShouldMatch, Float boostValue) throws IOException {
         Query query = parse(type, fieldName, value);
         if (query instanceof BooleanQuery) {
@@ -162,7 +162,7 @@ public class MultiMatchQuery extends MatchQuery {
             List<Tuple<String, Float>> missing = new ArrayList<>();
             for (Map.Entry<String, Float> entry : fieldNames.entrySet()) {
                 String name = entry.getKey();
-                MappedFieldType fieldType = context.fieldMapper(name);
+                MappedFieldType fieldType = parseContext.fieldMapper(name);
                 if (fieldType != null) {
                     Analyzer actualAnalyzer = getAnalyzer(fieldType);
                     name = fieldType.names().indexName();
diff --git a/core/src/main/java/org/elasticsearch/index/search/termslookup/TermsLookupFetchService.java b/core/src/main/java/org/elasticsearch/index/search/termslookup/TermsLookupFetchService.java
deleted file mode 100644
index b787344..0000000
--- a/core/src/main/java/org/elasticsearch/index/search/termslookup/TermsLookupFetchService.java
+++ /dev/null
@@ -1,60 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.search.termslookup;
-
-import org.elasticsearch.action.get.GetRequest;
-import org.elasticsearch.action.get.GetResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.component.AbstractComponent;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.support.XContentMapValues;
-import org.elasticsearch.indices.cache.query.terms.TermsLookup;
-import org.elasticsearch.search.internal.SearchContext;
-
-import java.util.ArrayList;
-import java.util.List;
-
-/**
- * Service which retrieves terms from a {@link TermsLookup} specification
- */
-public class TermsLookupFetchService extends AbstractComponent {
-
-    private final Client client;
-
-    @Inject
-    public TermsLookupFetchService(Client client, Settings settings) {
-        super(settings);
-        this.client = client;
-    }
-
-    public List<Object> fetch(TermsLookup termsLookup) {
-        List<Object> terms = new ArrayList<>();
-        GetRequest getRequest = new GetRequest(termsLookup.index(), termsLookup.type(), termsLookup.id())
-                .preference("_local").routing(termsLookup.routing());
-        getRequest.copyContextAndHeadersFrom(SearchContext.current());
-        final GetResponse getResponse = client.get(getRequest).actionGet();
-        if (getResponse.isExists()) {
-            List<Object> extractedValues = XContentMapValues.extractRawValues(termsLookup.path(), getResponse.getSourceAsMap());
-            terms.addAll(extractedValues);
-        }
-        return terms;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/indices/IndicesModule.java b/core/src/main/java/org/elasticsearch/indices/IndicesModule.java
index e3be629..759c9e5 100644
--- a/core/src/main/java/org/elasticsearch/indices/IndicesModule.java
+++ b/core/src/main/java/org/elasticsearch/indices/IndicesModule.java
@@ -114,7 +114,6 @@ public class IndicesModule extends AbstractModule {
         registerQueryParser(NotQueryParser.class);
         registerQueryParser(ExistsQueryParser.class);
         registerQueryParser(MissingQueryParser.class);
-        registerQueryParser(MatchNoneQueryParser.class);
 
         if (ShapesAvailability.JTS_AVAILABLE) {
             registerQueryParser(GeoShapeQueryParser.class);
diff --git a/core/src/main/java/org/elasticsearch/indices/cache/query/terms/TermsLookup.java b/core/src/main/java/org/elasticsearch/indices/cache/query/terms/TermsLookup.java
index 8da06ea..28ab04b 100644
--- a/core/src/main/java/org/elasticsearch/indices/cache/query/terms/TermsLookup.java
+++ b/core/src/main/java/org/elasticsearch/indices/cache/query/terms/TermsLookup.java
@@ -19,162 +19,58 @@
 
 package org.elasticsearch.indices.cache.query.terms;
 
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.QueryValidationException;
-
-import java.io.IOException;
-import java.util.Objects;
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.index.query.QueryParseContext;
 
 /**
- * Encapsulates the parameters needed to fetch terms.
  */
-public class TermsLookup implements Writeable<TermsLookup>, ToXContent {
-    static final TermsLookup PROTOTYPE = new TermsLookup();
+public class TermsLookup {
 
-    private String index;
-    private String type;
-    private String id;
-    private String path;
-    private String routing;
+    private final String index;
+    private final String type;
+    private final String id;
+    private final String routing;
+    private final String path;
 
-    public TermsLookup() {
-    }
+    @Nullable
+    private final QueryParseContext queryParseContext;
 
-    public TermsLookup(String index, String type, String id, String path) {
+    public TermsLookup(String index, String type, String id, String routing, String path, @Nullable QueryParseContext queryParseContext) {
         this.index = index;
         this.type = type;
         this.id = id;
+        this.routing = routing;
         this.path = path;
+        this.queryParseContext = queryParseContext;
     }
 
-    public String index() {
+    public String getIndex() {
         return index;
     }
 
-    public TermsLookup index(String index) {
-        this.index = index;
-        return this;
-    }
-
-    public String type() {
+    public String getType() {
         return type;
     }
 
-    public TermsLookup type(String type) {
-        this.type = type;
-        return this;
-    }
-
-    public String id() {
+    public String getId() {
         return id;
     }
 
-    public TermsLookup id(String id) {
-        this.id = id;
-        return this;
+    public String getRouting() {
+        return this.routing;
     }
 
-    public String path() {
+    public String getPath() {
         return path;
     }
 
-    public TermsLookup path(String path) {
-        this.path = path;
-        return this;
-    }
-
-    public String routing() {
-        return routing;
-    }
-
-    public TermsLookup routing(String routing) {
-        this.routing = routing;
-        return this;
+    @Nullable
+    public QueryParseContext getQueryParseContext() {
+        return queryParseContext;
     }
 
     @Override
     public String toString() {
         return index + "/" + type + "/" + id + "/" + path;
     }
-
-    @Override
-    public TermsLookup readFrom(StreamInput in) throws IOException {
-        TermsLookup termsLookup = new TermsLookup();
-        termsLookup.index = in.readOptionalString();
-        termsLookup.type = in.readString();
-        termsLookup.id = in.readString();
-        termsLookup.path = in.readString();
-        termsLookup.routing = in.readOptionalString();
-        return termsLookup;
-    }
-
-    public static TermsLookup readTermsLookupFrom(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeOptionalString(index);
-        out.writeString(type);
-        out.writeString(id);
-        out.writeString(path);
-        out.writeOptionalString(routing);
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        if (index != null) {
-            builder.field("index", index);
-        }
-        builder.field("type", type);
-        builder.field("id", id);
-        builder.field("path", path);
-        if (routing != null) {
-            builder.field("routing", routing);
-        }
-        return builder;
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(index, type, id, path, routing);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (this == obj) {
-            return true;
-        }
-        if (obj == null || getClass() != obj.getClass()) {
-            return false;
-        }
-        TermsLookup other = (TermsLookup) obj;
-        return Objects.equals(index, other.index) &&
-                Objects.equals(type, other.type) &&
-                Objects.equals(id, other.id) &&
-                Objects.equals(path, other.path) &&
-                Objects.equals(routing, other.routing);
-    }
-
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (id == null) {
-            validationException = addValidationError("[terms] query lookup element requires specifying the id.", validationException);
-        }
-        if (type == null) {
-            validationException = addValidationError("[terms] query lookup element requires specifying the type.", validationException);
-        }
-        if (path == null) {
-            validationException = addValidationError("[terms] query lookup element requires specifying the path.", validationException);
-        }
-        return validationException;
-    }
-
-    private static QueryValidationException addValidationError(String validationError, QueryValidationException validationException) {
-        return QueryValidationException.addValidationError("terms_lookup", validationError, validationException);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java b/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java
index 2a5d815..7d13fe0 100644
--- a/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java
+++ b/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java
@@ -21,12 +21,10 @@ package org.elasticsearch.indices.query;
 
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Maps;
+
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.query.EmptyQueryBuilder;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.index.query.QueryParser;
 
 import java.util.Map;
@@ -34,28 +32,24 @@ import java.util.Set;
 
 public class IndicesQueriesRegistry extends AbstractComponent {
 
-    private ImmutableMap<String, QueryParser<?>> queryParsers;
+    private ImmutableMap<String, QueryParser> queryParsers;
 
     @Inject
-    public IndicesQueriesRegistry(Settings settings, Set<QueryParser> injectedQueryParsers, NamedWriteableRegistry namedWriteableRegistry) {
+    public IndicesQueriesRegistry(Settings settings, Set<QueryParser> injectedQueryParsers) {
         super(settings);
-        Map<String, QueryParser<?>> queryParsers = Maps.newHashMap();
-        for (QueryParser<?> queryParser : injectedQueryParsers) {
+        Map<String, QueryParser> queryParsers = Maps.newHashMap();
+        for (QueryParser queryParser : injectedQueryParsers) {
             for (String name : queryParser.names()) {
                 queryParsers.put(name, queryParser);
             }
-            namedWriteableRegistry.registerPrototype(QueryBuilder.class, queryParser.getBuilderPrototype());
         }
-        // EmptyQueryBuilder is not registered as query parser but used internally.
-        // We need to register it with the NamedWriteableRegistry in order to serialize it
-        namedWriteableRegistry.registerPrototype(QueryBuilder.class, EmptyQueryBuilder.PROTOTYPE);
         this.queryParsers = ImmutableMap.copyOf(queryParsers);
     }
 
     /**
      * Returns all the registered query parsers
      */
-    public ImmutableMap<String, QueryParser<?>> queryParsers() {
+    public ImmutableMap<String, QueryParser> queryParsers() {
         return queryParsers;
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginManager.java b/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
index 8025be2..a2152a8 100644
--- a/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
+++ b/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
@@ -80,15 +80,16 @@ public class PluginManager {
                     "analysis-phonetic",
                     "analysis-smartcn",
                     "analysis-stempel",
-                    "cloud-aws",
                     "cloud-azure",
                     "cloud-gce",
                     "delete-by-query",
+                    "discovery-ec2",
                     "discovery-multicast",
                     "lang-javascript",
                     "lang-python",
                     "mapper-murmur3",
-                    "mapper-size"
+                    "mapper-size",
+                    "repository-s3"
             ).build();
 
     private final Environment environment;
diff --git a/core/src/main/java/org/elasticsearch/rest/action/explain/RestExplainAction.java b/core/src/main/java/org/elasticsearch/rest/action/explain/RestExplainAction.java
index 7c01fdd..ce306c6 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/explain/RestExplainAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/explain/RestExplainAction.java
@@ -30,7 +30,6 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.index.get.GetResult;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.QueryStringQueryBuilder;
 import org.elasticsearch.rest.*;
@@ -75,7 +74,13 @@ public class RestExplainAction extends BaseRestHandler {
             queryStringBuilder.lenient(request.paramAsBoolean("lenient", null));
             String defaultOperator = request.param("default_operator");
             if (defaultOperator != null) {
-                queryStringBuilder.defaultOperator(Operator.fromString(defaultOperator));
+                if ("OR".equals(defaultOperator)) {
+                    queryStringBuilder.defaultOperator(QueryStringQueryBuilder.Operator.OR);
+                } else if ("AND".equals(defaultOperator)) {
+                    queryStringBuilder.defaultOperator(QueryStringQueryBuilder.Operator.AND);
+                } else {
+                    throw new IllegalArgumentException("Unsupported defaultOperator [" + defaultOperator + "], can either be [OR] or [AND]");
+                }
             }
 
             QuerySourceBuilder querySourceBuilder = new QuerySourceBuilder();
diff --git a/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java b/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java
index 674aa69..bd17c1d 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java
@@ -27,7 +27,6 @@ import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.lucene.uid.Versions;
 import org.elasticsearch.common.xcontent.*;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.QueryStringQueryBuilder;
 import org.elasticsearch.rest.RestRequest;
@@ -98,7 +97,13 @@ public class RestActions {
         queryBuilder.lenient(request.paramAsBoolean("lenient", null));
         String defaultOperator = request.param("default_operator");
         if (defaultOperator != null) {
-            queryBuilder.defaultOperator(Operator.fromString(defaultOperator));
+            if ("OR".equals(defaultOperator)) {
+                queryBuilder.defaultOperator(QueryStringQueryBuilder.Operator.OR);
+            } else if ("AND".equals(defaultOperator)) {
+                queryBuilder.defaultOperator(QueryStringQueryBuilder.Operator.AND);
+            } else {
+                throw new IllegalArgumentException("Unsupported defaultOperator [" + defaultOperator + "], can either be [OR] or [AND]");
+            }
         }
         return new QuerySourceBuilder().setQuery(queryBuilder);
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java
index 99ee7c7..00d12a8 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java
@@ -28,7 +28,7 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
@@ -117,7 +117,7 @@ public class GND extends NXYSignificanceHeuristic {
 
         @Override
         public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
-                throws IOException, QueryShardException {
+                throws IOException, QueryParsingException {
             String givenName = parser.currentName();
             boolean backgroundIsSuperset = true;
             XContentParser.Token token = parser.nextToken();
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java
index 97264e7..d5bfc5c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java
@@ -27,7 +27,7 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
@@ -110,7 +110,7 @@ public class JLHScore extends SignificanceHeuristic {
 
         @Override
         public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
-                throws IOException, QueryShardException {
+                throws IOException, QueryParsingException {
             // move to the closing bracket
             if (!parser.nextToken().equals(XContentParser.Token.END_OBJECT)) {
                 throw new ElasticsearchParseException("failed to parse [jhl] significance heuristic. expected an empty object, but found [{}] instead", parser.currentToken());
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java
index c6a6924..4d86661 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java
@@ -27,7 +27,7 @@ import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
@@ -140,7 +140,7 @@ public abstract class NXYSignificanceHeuristic extends SignificanceHeuristic {
 
         @Override
         public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
-                throws IOException, QueryShardException {
+                throws IOException, QueryParsingException {
             String givenName = parser.currentName();
             boolean includeNegatives = false;
             boolean backgroundIsSuperset = true;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java
index aceae8c..d613ef2 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java
@@ -27,7 +27,7 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
@@ -79,7 +79,7 @@ public class PercentageScore extends SignificanceHeuristic {
 
         @Override
         public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
-                throws IOException, QueryShardException {
+                throws IOException, QueryParsingException {
             // move to the closing bracket
             if (!parser.nextToken().equals(XContentParser.Token.END_OBJECT)) {
                 throw new ElasticsearchParseException("failed to parse [percentage] significance heuristic. expected an empty object, but got [{}] instead", parser.currentToken());
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java
index d117969..c20399e 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java
@@ -29,10 +29,14 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.logging.ESLoggerFactory;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardException;
-import org.elasticsearch.script.*;
+import org.elasticsearch.index.query.QueryParsingException;
+import org.elasticsearch.script.ExecutableScript;
+import org.elasticsearch.script.Script;
 import org.elasticsearch.script.Script.ScriptField;
+import org.elasticsearch.script.ScriptContext;
+import org.elasticsearch.script.ScriptParameterParser;
 import org.elasticsearch.script.ScriptParameterParser.ScriptParameterValue;
+import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.internal.SearchContext;
 
@@ -131,7 +135,7 @@ public class ScriptHeuristic extends SignificanceHeuristic {
 
         @Override
         public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
-                throws IOException, QueryShardException {
+                throws IOException, QueryParsingException {
             String heuristicName = parser.currentName();
             Script script = null;
             XContentParser.Token token;
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsParseElement.java b/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsParseElement.java
index ac6dc18..c02e2c6 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsParseElement.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsParseElement.java
@@ -24,7 +24,7 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.index.query.ParsedQuery;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.search.SearchParseElement;
 import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsParseElement;
 import org.elasticsearch.search.fetch.script.ScriptFieldsParseElement;
@@ -59,15 +59,15 @@ public class InnerHitsParseElement implements SearchParseElement {
 
     @Override
     public void parse(XContentParser parser, SearchContext searchContext) throws Exception {
-        QueryShardContext context = searchContext.queryParserService().getShardContext();
-        context.reset(parser);
-        Map<String, InnerHitsContext.BaseInnerHits> innerHitsMap = parseInnerHits(parser, context, searchContext);
+        QueryParseContext parseContext = searchContext.queryParserService().getParseContext();
+        parseContext.reset(parser);
+        Map<String, InnerHitsContext.BaseInnerHits> innerHitsMap = parseInnerHits(parser, parseContext, searchContext);
         if (innerHitsMap != null) {
             searchContext.innerHits(new InnerHitsContext(innerHitsMap));
         }
     }
 
-    private Map<String, InnerHitsContext.BaseInnerHits> parseInnerHits(XContentParser parser, QueryShardContext context, SearchContext searchContext) throws Exception {
+    private Map<String, InnerHitsContext.BaseInnerHits> parseInnerHits(XContentParser parser, QueryParseContext parseContext, SearchContext searchContext) throws Exception {
         XContentParser.Token token;
         Map<String, InnerHitsContext.BaseInnerHits> innerHitsMap = null;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -79,7 +79,7 @@ public class InnerHitsParseElement implements SearchParseElement {
             if (token != XContentParser.Token.START_OBJECT) {
                 throw new IllegalArgumentException("Inner hit definition for [" + innerHitName + " starts with a [" + token + "], expected a [" + XContentParser.Token.START_OBJECT + "].");
             }
-            InnerHitsContext.BaseInnerHits innerHits = parseInnerHit(parser, context, searchContext, innerHitName);
+            InnerHitsContext.BaseInnerHits innerHits = parseInnerHit(parser, parseContext, searchContext, innerHitName);
             if (innerHitsMap == null) {
                 innerHitsMap = new HashMap<>();
             }
@@ -88,7 +88,7 @@ public class InnerHitsParseElement implements SearchParseElement {
         return innerHitsMap;
     }
 
-    private InnerHitsContext.BaseInnerHits parseInnerHit(XContentParser parser, QueryShardContext context, SearchContext searchContext, String innerHitName) throws Exception {
+    private InnerHitsContext.BaseInnerHits parseInnerHit(XContentParser parser, QueryParseContext parseContext, SearchContext searchContext, String innerHitName) throws Exception {
         XContentParser.Token token = parser.nextToken();
         if (token != XContentParser.Token.FIELD_NAME) {
             throw new IllegalArgumentException("Unexpected token " + token + " inside inner hit definition. Either specify [path] or [type] object");
@@ -123,9 +123,9 @@ public class InnerHitsParseElement implements SearchParseElement {
 
         final InnerHitsContext.BaseInnerHits innerHits;
         if (nestedPath != null) {
-            innerHits = parseNested(parser, context, searchContext, fieldName);
+            innerHits = parseNested(parser, parseContext, searchContext, fieldName);
         } else if (type != null) {
-            innerHits = parseParentChild(parser, context, searchContext, fieldName);
+            innerHits = parseParentChild(parser, parseContext, searchContext, fieldName);
         } else {
             throw new IllegalArgumentException("Either [path] or [type] must be defined");
         }
@@ -143,16 +143,16 @@ public class InnerHitsParseElement implements SearchParseElement {
         return innerHits;
     }
 
-    private InnerHitsContext.ParentChildInnerHits parseParentChild(XContentParser parser, QueryShardContext context, SearchContext searchContext, String type) throws Exception {
-        ParseResult parseResult = parseSubSearchContext(searchContext, context, parser);
+    private InnerHitsContext.ParentChildInnerHits parseParentChild(XContentParser parser, QueryParseContext parseContext, SearchContext searchContext, String type) throws Exception {
+        ParseResult parseResult = parseSubSearchContext(searchContext, parseContext, parser);
         DocumentMapper documentMapper = searchContext.mapperService().documentMapper(type);
         if (documentMapper == null) {
             throw new IllegalArgumentException("type [" + type + "] doesn't exist");
         }
-        return new InnerHitsContext.ParentChildInnerHits(parseResult.context(), parseResult.query(), parseResult.childInnerHits(), context.mapperService(), documentMapper);
+        return new InnerHitsContext.ParentChildInnerHits(parseResult.context(), parseResult.query(), parseResult.childInnerHits(), parseContext.mapperService(), documentMapper);
     }
 
-    private InnerHitsContext.NestedInnerHits parseNested(XContentParser parser, QueryShardContext context, SearchContext searchContext, String nestedPath) throws Exception {
+    private InnerHitsContext.NestedInnerHits parseNested(XContentParser parser, QueryParseContext parseContext, SearchContext searchContext, String nestedPath) throws Exception {
         ObjectMapper objectMapper = searchContext.getObjectMapper(nestedPath);
         if (objectMapper == null) {
             throw new IllegalArgumentException("path [" + nestedPath +"] doesn't exist");
@@ -160,14 +160,14 @@ public class InnerHitsParseElement implements SearchParseElement {
         if (objectMapper.nested().isNested() == false) {
             throw new IllegalArgumentException("path [" + nestedPath +"] isn't nested");
         }
-        ObjectMapper parentObjectMapper = context.nestedScope().nextLevel(objectMapper);
-        ParseResult parseResult = parseSubSearchContext(searchContext, context, parser);
-        context.nestedScope().previousLevel();
+        ObjectMapper parentObjectMapper = parseContext.nestedScope().nextLevel(objectMapper);
+        ParseResult parseResult = parseSubSearchContext(searchContext, parseContext, parser);
+        parseContext.nestedScope().previousLevel();
 
         return new InnerHitsContext.NestedInnerHits(parseResult.context(), parseResult.query(), parseResult.childInnerHits(), parentObjectMapper, objectMapper);
     }
 
-    private ParseResult parseSubSearchContext(SearchContext searchContext, QueryShardContext context, XContentParser parser) throws Exception {
+    private ParseResult parseSubSearchContext(SearchContext searchContext, QueryParseContext parseContext, XContentParser parser) throws Exception {
         ParsedQuery query = null;
         Map<String, InnerHitsContext.BaseInnerHits> childInnerHits = null;
         SubSearchContext subSearchContext = new SubSearchContext(searchContext);
@@ -178,10 +178,10 @@ public class InnerHitsParseElement implements SearchParseElement {
                 fieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("query".equals(fieldName)) {
-                    Query q = searchContext.queryParserService().parseInnerQuery(context);
-                    query = new ParsedQuery(q, context.copyNamedQueries());
+                    Query q = searchContext.queryParserService().parseInnerQuery(parseContext);
+                    query = new ParsedQuery(q, parseContext.copyNamedQueries());
                 } else if ("inner_hits".equals(fieldName)) {
-                    childInnerHits = parseInnerHits(parser, context, searchContext);
+                    childInnerHits = parseInnerHits(parser, parseContext, searchContext);
                 } else {
                     parseCommonInnerHitOptions(parser, token, fieldName, subSearchContext, sortParseElement, sourceParseElement, highlighterParseElement, scriptFieldsParseElement, fieldDataFieldsParseElement);
                 }
diff --git a/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java b/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
index 19cbf93..6b6c422 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
@@ -43,7 +43,7 @@ import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.index.query.IndexQueryParserService;
 import org.elasticsearch.index.query.ParsedQuery;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.script.ScriptService;
@@ -76,12 +76,12 @@ public abstract class SearchContext extends DelegatingHasContextAndHeaders imple
 
     public static void setCurrent(SearchContext value) {
         current.set(value);
-        QueryShardContext.setTypes(value.types());
+        QueryParseContext.setTypes(value.types());
     }
 
     public static void removeCurrent() {
         current.remove();
-        QueryShardContext.removeTypes();
+        QueryParseContext.removeTypes();
     }
 
     public static SearchContext current() {
diff --git a/core/src/main/java/org/elasticsearch/search/query/QueryPhase.java b/core/src/main/java/org/elasticsearch/search/query/QueryPhase.java
index 06451af..a7c022a 100644
--- a/core/src/main/java/org/elasticsearch/search/query/QueryPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/query/QueryPhase.java
@@ -20,6 +20,9 @@
 package org.elasticsearch.search.query;
 
 import com.google.common.collect.ImmutableMap;
+
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.index.Term;
 import org.apache.lucene.queries.MinDocQuery;
 import org.apache.lucene.search.*;
 import org.elasticsearch.action.search.SearchType;
@@ -100,23 +103,39 @@ public class QueryPhase implements SearchPhase {
         // here to make sure it happens during the QUERY phase
         aggregationPhase.preProcess(searchContext);
 
-        searchContext.queryResult().searchTimedOut(false);
+        boolean rescore = execute(searchContext, searchContext.searcher());
+
+        if (rescore) { // only if we do a regular search
+            rescorePhase.execute(searchContext);
+        }
+        suggestPhase.execute(searchContext);
+        aggregationPhase.execute(searchContext);
+    }
+
+    /**
+     * In a package-private method so that it can be tested without having to
+     * wire everything (mapperService, etc.)
+     * @return whether the rescoring phase should be executed
+     */
+    static boolean execute(SearchContext searchContext, final IndexSearcher searcher) throws QueryPhaseExecutionException {
+        QuerySearchResult queryResult = searchContext.queryResult();
+        queryResult.searchTimedOut(false);
 
         final SearchType searchType = searchContext.searchType();
         boolean rescore = false;
         try {
-            searchContext.queryResult().from(searchContext.from());
-            searchContext.queryResult().size(searchContext.size());
+            queryResult.from(searchContext.from());
+            queryResult.size(searchContext.size());
 
-            final IndexSearcher searcher = searchContext.searcher();
             Query query = searchContext.query();
 
             final int totalNumDocs = searcher.getIndexReader().numDocs();
             int numDocs = Math.min(searchContext.from() + searchContext.size(), totalNumDocs);
 
             Collector collector;
-            final Callable<TopDocs> topDocsCallable;
+            Callable<TopDocs> topDocsCallable;
 
+            assert query == searcher.rewrite(query); // already rewritten
             if (searchContext.size() == 0) { // no matter what the value of from is
                 final TotalHitCountCollector totalHitCountCollector = new TotalHitCountCollector();
                 collector = totalHitCountCollector;
@@ -240,36 +259,75 @@ public class QueryPhase implements SearchPhase {
                 collector = new MinimumScoreCollector(collector, searchContext.minimumScore());
             }
 
+            if (collector.getClass() == TotalHitCountCollector.class) {
+                // Optimize counts in simple cases to return in constant time
+                // instead of using a collector
+                while (true) {
+                    // remove wrappers that don't matter for counts
+                    // this is necessary so that we don't only optimize match_all
+                    // queries but also match_all queries that are nested in
+                    // a constant_score query
+                    if (query instanceof ConstantScoreQuery) {
+                        query = ((ConstantScoreQuery) query).getQuery();
+                    } else {
+                        break;
+                    }
+                }
+
+                if (query.getClass() == MatchAllDocsQuery.class) {
+                    collector = null;
+                    topDocsCallable = new Callable<TopDocs>() {
+                        @Override
+                        public TopDocs call() throws Exception {
+                            int count = searcher.getIndexReader().numDocs();
+                            return new TopDocs(count, Lucene.EMPTY_SCORE_DOCS, 0);
+                        }
+                    };
+                } else if (query.getClass() == TermQuery.class && searcher.getIndexReader().hasDeletions() == false) {
+                    final Term term = ((TermQuery) query).getTerm();
+                    collector = null;
+                    topDocsCallable = new Callable<TopDocs>() {
+                        @Override
+                        public TopDocs call() throws Exception {
+                            int count = 0;
+                            for (LeafReaderContext context : searcher.getIndexReader().leaves()) {
+                                count += context.reader().docFreq(term);
+                            }
+                            return new TopDocs(count, Lucene.EMPTY_SCORE_DOCS, 0);
+                        }
+                    };
+                }
+            }
+
             final boolean timeoutSet = searchContext.timeoutInMillis() != SearchService.NO_TIMEOUT.millis();
-            if (timeoutSet) {
+            if (timeoutSet && collector != null) { // collector might be null if no collection is actually needed
                 // TODO: change to use our own counter that uses the scheduler in ThreadPool
                 // throws TimeLimitingCollector.TimeExceededException when timeout has reached
                 collector = Lucene.wrapTimeLimitingCollector(collector, searchContext.timeEstimateCounter(), searchContext.timeoutInMillis());
             }
 
             try {
-                searchContext.searcher().search(query, collector);
+                if (collector != null) {
+                    searcher.search(query, collector);
+                }
             } catch (TimeLimitingCollector.TimeExceededException e) {
                 assert timeoutSet : "TimeExceededException thrown even though timeout wasn't set";
-                searchContext.queryResult().searchTimedOut(true);
+                queryResult.searchTimedOut(true);
             } catch (Lucene.EarlyTerminationException e) {
                 assert terminateAfterSet : "EarlyTerminationException thrown even though terminateAfter wasn't set";
-                searchContext.queryResult().terminatedEarly(true);
+                queryResult.terminatedEarly(true);
             } finally {
                 searchContext.clearReleasables(SearchContext.Lifetime.COLLECTION);
             }
-            if (terminateAfterSet && searchContext.queryResult().terminatedEarly() == null) {
-                searchContext.queryResult().terminatedEarly(false);
+            if (terminateAfterSet && queryResult.terminatedEarly() == null) {
+                queryResult.terminatedEarly(false);
             }
 
-            searchContext.queryResult().topDocs(topDocsCallable.call());
+            queryResult.topDocs(topDocsCallable.call());
+
+            return rescore;
         } catch (Throwable e) {
             throw new QueryPhaseExecutionException(searchContext, "Failed to execute main query", e);
         }
-        if (rescore) { // only if we do a regular search
-            rescorePhase.execute(searchContext);
-        }
-        suggestPhase.execute(searchContext);
-        aggregationPhase.execute(searchContext);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java b/core/src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java
index f3ef9bd..6f4a0df 100644
--- a/core/src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java
+++ b/core/src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java
@@ -173,7 +173,7 @@ public class GeoDistanceSortParser implements SortParser {
             ObjectMapper objectMapper = context.mapperService().resolveClosestNestedObjectMapper(fieldName);
             if (objectMapper != null && objectMapper.nested().isNested()) {
                 if (nestedHelper == null) {
-                    nestedHelper = new NestedInnerQueryParseSupport(context.queryParserService().getShardContext());
+                    nestedHelper = new NestedInnerQueryParseSupport(context.queryParserService().getParseContext());
                 }
                 nestedHelper.setPath(objectMapper.fullPath());
             }
@@ -181,7 +181,7 @@ public class GeoDistanceSortParser implements SortParser {
 
         final Nested nested;
         if (nestedHelper != null && nestedHelper.getPath() != null) {
-
+            
             BitDocIdSetFilter rootDocumentsFilter = context.bitsetFilterCache().getBitDocIdSetFilter(Queries.newNonNestedFilter());
             Filter innerDocumentsFilter;
             if (nestedHelper.filterFound()) {
diff --git a/core/src/main/java/org/elasticsearch/search/sort/SortParseElement.java b/core/src/main/java/org/elasticsearch/search/sort/SortParseElement.java
index 2dc5f62..2f4dcb3 100644
--- a/core/src/main/java/org/elasticsearch/search/sort/SortParseElement.java
+++ b/core/src/main/java/org/elasticsearch/search/sort/SortParseElement.java
@@ -243,7 +243,7 @@ public class SortParseElement implements SearchParseElement {
                     ObjectMapper objectMapper = context.mapperService().resolveClosestNestedObjectMapper(fieldName);
                     if (objectMapper != null && objectMapper.nested().isNested()) {
                         if (nestedHelper == null) {
-                            nestedHelper = new NestedInnerQueryParseSupport(context.queryParserService().getShardContext());
+                            nestedHelper = new NestedInnerQueryParseSupport(context.queryParserService().getParseContext());
                         }
                         nestedHelper.setPath(objectMapper.fullPath());
                     }
diff --git a/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help b/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
index 6bce1dd..758d055 100644
--- a/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
+++ b/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
@@ -38,15 +38,16 @@ OFFICIAL PLUGINS
     - analysis-phonetic
     - analysis-smartcn
     - analysis-stempel
-    - cloud-aws
     - cloud-azure
     - cloud-gce
     - delete-by-query
+    - discovery-ec2
     - discovery-multicast
     - lang-javascript
     - lang-python
     - mapper-murmur3
     - mapper-size
+    - repository-s3
 
 
 OPTIONS
diff --git a/core/src/test/java/org/apache/lucene/queries/BlendedTermQueryTest.java b/core/src/test/java/org/apache/lucene/queries/BlendedTermQueryTest.java
deleted file mode 100644
index 74eb4ba..0000000
--- a/core/src/test/java/org/apache/lucene/queries/BlendedTermQueryTest.java
+++ /dev/null
@@ -1,240 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.lucene.queries;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexOptions;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.MultiReader;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.DisjunctionMaxQuery;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.QueryUtils;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.search.similarities.BM25Similarity;
-import org.apache.lucene.search.similarities.DefaultSimilarity;
-import org.apache.lucene.search.similarities.Similarity;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.TestUtil;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-
-import static org.hamcrest.Matchers.containsInAnyOrder;
-import static org.hamcrest.Matchers.equalTo;
-
-/**
- */
-public class BlendedTermQueryTest extends ESTestCase {
-
-    @Test
-    public void testBooleanQuery() throws IOException {
-        Directory dir = newDirectory();
-        IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));
-        String[] firstNames = new String[]{
-                "simon", "paul"
-        };
-        String[] surNames = new String[]{
-                "willnauer", "simon"
-        };
-        for (int i = 0; i < surNames.length; i++) {
-            Document d = new Document();
-            d.add(new TextField("id", Integer.toString(i), Field.Store.YES));
-            d.add(new TextField("firstname", firstNames[i], Field.Store.NO));
-            d.add(new TextField("surname", surNames[i], Field.Store.NO));
-            w.addDocument(d);
-        }
-        int iters = scaledRandomIntBetween(25, 100);
-        for (int j = 0; j < iters; j++) {
-            Document d = new Document();
-            d.add(new TextField("id", Integer.toString(firstNames.length + j), Field.Store.YES));
-            d.add(new TextField("firstname", rarely() ? "some_other_name" :
-                    "simon the sorcerer", Field.Store.NO)); // make sure length-norm is the tie-breaker
-            d.add(new TextField("surname", "bogus", Field.Store.NO));
-            w.addDocument(d);
-        }
-        w.commit();
-        DirectoryReader reader = DirectoryReader.open(w, true);
-        IndexSearcher searcher = setSimilarity(newSearcher(reader));
-
-        {
-            Term[] terms = new Term[]{new Term("firstname", "simon"), new Term("surname", "simon")};
-            BlendedTermQuery query = BlendedTermQuery.booleanBlendedQuery(terms, true);
-            TopDocs search = searcher.search(query, 3);
-            ScoreDoc[] scoreDocs = search.scoreDocs;
-            assertEquals(3, scoreDocs.length);
-            assertEquals(Integer.toString(0), reader.document(scoreDocs[0].doc).getField("id").stringValue());
-        }
-        {
-            BooleanQuery query = new BooleanQuery(false);
-            query.add(new TermQuery(new Term("firstname", "simon")), BooleanClause.Occur.SHOULD);
-            query.add(new TermQuery(new Term("surname", "simon")), BooleanClause.Occur.SHOULD);
-            TopDocs search = searcher.search(query, 1);
-            ScoreDoc[] scoreDocs = search.scoreDocs;
-            assertEquals(Integer.toString(1), reader.document(scoreDocs[0].doc).getField("id").stringValue());
-
-        }
-        reader.close();
-        w.close();
-        dir.close();
-
-    }
-
-    @Test
-    public void testDismaxQuery() throws IOException {
-        Directory dir = newDirectory();
-        IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));
-        String[] username = new String[]{
-                "foo fighters", "some cool fan", "cover band"};
-        String[] song = new String[]{
-                "generator", "foo fighers - generator", "foo fighters generator"
-        };
-        final boolean omitNorms = random().nextBoolean();
-        FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);
-        ft.setIndexOptions(random().nextBoolean() ? IndexOptions.DOCS : IndexOptions.DOCS_AND_FREQS);
-        ft.setOmitNorms(omitNorms);
-        ft.freeze();
-
-        FieldType ft1 = new FieldType(TextField.TYPE_NOT_STORED);
-        ft1.setIndexOptions(random().nextBoolean() ? IndexOptions.DOCS : IndexOptions.DOCS_AND_FREQS);
-        ft1.setOmitNorms(omitNorms);
-        ft1.freeze();
-        for (int i = 0; i < username.length; i++) {
-            Document d = new Document();
-            d.add(new TextField("id", Integer.toString(i), Field.Store.YES));
-            d.add(new Field("username", username[i], ft));
-            d.add(new Field("song", song[i], ft));
-            w.addDocument(d);
-        }
-        int iters = scaledRandomIntBetween(25, 100);
-        for (int j = 0; j < iters; j++) {
-            Document d = new Document();
-            d.add(new TextField("id", Integer.toString(username.length + j), Field.Store.YES));
-            d.add(new Field("username", "foo fighters", ft1));
-            d.add(new Field("song", "some bogus text to bump up IDF", ft1));
-            w.addDocument(d);
-        }
-        w.commit();
-        DirectoryReader reader = DirectoryReader.open(w, true);
-        IndexSearcher searcher = setSimilarity(newSearcher(reader));
-        {
-            String[] fields = new String[]{"username", "song"};
-            BooleanQuery query = new BooleanQuery(false);
-            query.add(BlendedTermQuery.dismaxBlendedQuery(toTerms(fields, "foo"), 0.1f), BooleanClause.Occur.SHOULD);
-            query.add(BlendedTermQuery.dismaxBlendedQuery(toTerms(fields, "fighters"), 0.1f), BooleanClause.Occur.SHOULD);
-            query.add(BlendedTermQuery.dismaxBlendedQuery(toTerms(fields, "generator"), 0.1f), BooleanClause.Occur.SHOULD);
-            TopDocs search = searcher.search(query, 10);
-            ScoreDoc[] scoreDocs = search.scoreDocs;
-            assertEquals(Integer.toString(0), reader.document(scoreDocs[0].doc).getField("id").stringValue());
-        }
-        {
-            BooleanQuery query = new BooleanQuery(false);
-            DisjunctionMaxQuery uname = new DisjunctionMaxQuery(0.0f);
-            uname.add(new TermQuery(new Term("username", "foo")));
-            uname.add(new TermQuery(new Term("song", "foo")));
-
-            DisjunctionMaxQuery s = new DisjunctionMaxQuery(0.0f);
-            s.add(new TermQuery(new Term("username", "fighers")));
-            s.add(new TermQuery(new Term("song", "fighers")));
-            DisjunctionMaxQuery gen = new DisjunctionMaxQuery(0f);
-            gen.add(new TermQuery(new Term("username", "generator")));
-            gen.add(new TermQuery(new Term("song", "generator")));
-            query.add(uname, BooleanClause.Occur.SHOULD);
-            query.add(s, BooleanClause.Occur.SHOULD);
-            query.add(gen, BooleanClause.Occur.SHOULD);
-            TopDocs search = searcher.search(query, 4);
-            ScoreDoc[] scoreDocs = search.scoreDocs;
-            assertEquals(Integer.toString(1), reader.document(scoreDocs[0].doc).getField("id").stringValue());
-
-        }
-        reader.close();
-        w.close();
-        dir.close();
-    }
-
-    @Test
-    public void testBasics() {
-        final int iters = scaledRandomIntBetween(5, 25);
-        for (int j = 0; j < iters; j++) {
-            String[] fields = new String[1 + random().nextInt(10)];
-            for (int i = 0; i < fields.length; i++) {
-                fields[i] = TestUtil.randomRealisticUnicodeString(random(), 1, 10);
-            }
-            String term = TestUtil.randomRealisticUnicodeString(random(), 1, 10);
-            Term[] terms = toTerms(fields, term);
-            boolean disableCoord = random().nextBoolean();
-            boolean useBoolean = random().nextBoolean();
-            float tieBreaker = random().nextFloat();
-            BlendedTermQuery query = useBoolean ? BlendedTermQuery.booleanBlendedQuery(terms, disableCoord) : BlendedTermQuery.dismaxBlendedQuery(terms, tieBreaker);
-            QueryUtils.check(query);
-            terms = toTerms(fields, term);
-            BlendedTermQuery query2 = useBoolean ? BlendedTermQuery.booleanBlendedQuery(terms, disableCoord) : BlendedTermQuery.dismaxBlendedQuery(terms, tieBreaker);
-            assertEquals(query, query2);
-        }
-    }
-
-    public Term[] toTerms(String[] fields, String term) {
-        Term[] terms = new Term[fields.length];
-        List<String> fieldsList = Arrays.asList(fields);
-        Collections.shuffle(fieldsList, random());
-        fields = fieldsList.toArray(new String[0]);
-        for (int i = 0; i < fields.length; i++) {
-            terms[i] = new Term(fields[i], term);
-        }
-        return terms;
-    }
-
-    public IndexSearcher setSimilarity(IndexSearcher searcher) {
-        Similarity similarity = random().nextBoolean() ? new BM25Similarity() : new DefaultSimilarity();
-        searcher.setSimilarity(similarity);
-        return searcher;
-    }
-
-    @Test
-    public void testExtractTerms() throws IOException {
-        Set<Term> terms = new HashSet<>();
-        int num = scaledRandomIntBetween(1, 10);
-        for (int i = 0; i < num; i++) {
-            terms.add(new Term(TestUtil.randomRealisticUnicodeString(random(), 1, 10), TestUtil.randomRealisticUnicodeString(random(), 1, 10)));
-        }
-
-        BlendedTermQuery blendedTermQuery = random().nextBoolean() ? BlendedTermQuery.dismaxBlendedQuery(terms.toArray(new Term[0]), random().nextFloat()) :
-                BlendedTermQuery.booleanBlendedQuery(terms.toArray(new Term[0]), random().nextBoolean());
-        Set<Term> extracted = new HashSet<>();
-        IndexSearcher searcher = new IndexSearcher(new MultiReader());
-        searcher.createNormalizedWeight(blendedTermQuery, false).extractTerms(extracted);
-        assertThat(extracted.size(), equalTo(terms.size()));
-        assertThat(extracted, containsInAnyOrder(terms.toArray(new Term[0])));
-    }
-}
diff --git a/core/src/test/java/org/apache/lucene/queries/BlendedTermQueryTests.java b/core/src/test/java/org/apache/lucene/queries/BlendedTermQueryTests.java
new file mode 100644
index 0000000..ec66a53
--- /dev/null
+++ b/core/src/test/java/org/apache/lucene/queries/BlendedTermQueryTests.java
@@ -0,0 +1,240 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.lucene.queries;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexOptions;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.MultiReader;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.DisjunctionMaxQuery;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.QueryUtils;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.search.similarities.BM25Similarity;
+import org.apache.lucene.search.similarities.DefaultSimilarity;
+import org.apache.lucene.search.similarities.Similarity;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.TestUtil;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+
+import static org.hamcrest.Matchers.containsInAnyOrder;
+import static org.hamcrest.Matchers.equalTo;
+
+/**
+ */
+public class BlendedTermQueryTests extends ESTestCase {
+
+    @Test
+    public void testBooleanQuery() throws IOException {
+        Directory dir = newDirectory();
+        IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));
+        String[] firstNames = new String[]{
+                "simon", "paul"
+        };
+        String[] surNames = new String[]{
+                "willnauer", "simon"
+        };
+        for (int i = 0; i < surNames.length; i++) {
+            Document d = new Document();
+            d.add(new TextField("id", Integer.toString(i), Field.Store.YES));
+            d.add(new TextField("firstname", firstNames[i], Field.Store.NO));
+            d.add(new TextField("surname", surNames[i], Field.Store.NO));
+            w.addDocument(d);
+        }
+        int iters = scaledRandomIntBetween(25, 100);
+        for (int j = 0; j < iters; j++) {
+            Document d = new Document();
+            d.add(new TextField("id", Integer.toString(firstNames.length + j), Field.Store.YES));
+            d.add(new TextField("firstname", rarely() ? "some_other_name" :
+                    "simon the sorcerer", Field.Store.NO)); // make sure length-norm is the tie-breaker
+            d.add(new TextField("surname", "bogus", Field.Store.NO));
+            w.addDocument(d);
+        }
+        w.commit();
+        DirectoryReader reader = DirectoryReader.open(w, true);
+        IndexSearcher searcher = setSimilarity(newSearcher(reader));
+
+        {
+            Term[] terms = new Term[]{new Term("firstname", "simon"), new Term("surname", "simon")};
+            BlendedTermQuery query = BlendedTermQuery.booleanBlendedQuery(terms, true);
+            TopDocs search = searcher.search(query, 3);
+            ScoreDoc[] scoreDocs = search.scoreDocs;
+            assertEquals(3, scoreDocs.length);
+            assertEquals(Integer.toString(0), reader.document(scoreDocs[0].doc).getField("id").stringValue());
+        }
+        {
+            BooleanQuery query = new BooleanQuery(false);
+            query.add(new TermQuery(new Term("firstname", "simon")), BooleanClause.Occur.SHOULD);
+            query.add(new TermQuery(new Term("surname", "simon")), BooleanClause.Occur.SHOULD);
+            TopDocs search = searcher.search(query, 1);
+            ScoreDoc[] scoreDocs = search.scoreDocs;
+            assertEquals(Integer.toString(1), reader.document(scoreDocs[0].doc).getField("id").stringValue());
+
+        }
+        reader.close();
+        w.close();
+        dir.close();
+
+    }
+
+    @Test
+    public void testDismaxQuery() throws IOException {
+        Directory dir = newDirectory();
+        IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));
+        String[] username = new String[]{
+                "foo fighters", "some cool fan", "cover band"};
+        String[] song = new String[]{
+                "generator", "foo fighers - generator", "foo fighters generator"
+        };
+        final boolean omitNorms = random().nextBoolean();
+        FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);
+        ft.setIndexOptions(random().nextBoolean() ? IndexOptions.DOCS : IndexOptions.DOCS_AND_FREQS);
+        ft.setOmitNorms(omitNorms);
+        ft.freeze();
+
+        FieldType ft1 = new FieldType(TextField.TYPE_NOT_STORED);
+        ft1.setIndexOptions(random().nextBoolean() ? IndexOptions.DOCS : IndexOptions.DOCS_AND_FREQS);
+        ft1.setOmitNorms(omitNorms);
+        ft1.freeze();
+        for (int i = 0; i < username.length; i++) {
+            Document d = new Document();
+            d.add(new TextField("id", Integer.toString(i), Field.Store.YES));
+            d.add(new Field("username", username[i], ft));
+            d.add(new Field("song", song[i], ft));
+            w.addDocument(d);
+        }
+        int iters = scaledRandomIntBetween(25, 100);
+        for (int j = 0; j < iters; j++) {
+            Document d = new Document();
+            d.add(new TextField("id", Integer.toString(username.length + j), Field.Store.YES));
+            d.add(new Field("username", "foo fighters", ft1));
+            d.add(new Field("song", "some bogus text to bump up IDF", ft1));
+            w.addDocument(d);
+        }
+        w.commit();
+        DirectoryReader reader = DirectoryReader.open(w, true);
+        IndexSearcher searcher = setSimilarity(newSearcher(reader));
+        {
+            String[] fields = new String[]{"username", "song"};
+            BooleanQuery query = new BooleanQuery(false);
+            query.add(BlendedTermQuery.dismaxBlendedQuery(toTerms(fields, "foo"), 0.1f), BooleanClause.Occur.SHOULD);
+            query.add(BlendedTermQuery.dismaxBlendedQuery(toTerms(fields, "fighters"), 0.1f), BooleanClause.Occur.SHOULD);
+            query.add(BlendedTermQuery.dismaxBlendedQuery(toTerms(fields, "generator"), 0.1f), BooleanClause.Occur.SHOULD);
+            TopDocs search = searcher.search(query, 10);
+            ScoreDoc[] scoreDocs = search.scoreDocs;
+            assertEquals(Integer.toString(0), reader.document(scoreDocs[0].doc).getField("id").stringValue());
+        }
+        {
+            BooleanQuery query = new BooleanQuery(false);
+            DisjunctionMaxQuery uname = new DisjunctionMaxQuery(0.0f);
+            uname.add(new TermQuery(new Term("username", "foo")));
+            uname.add(new TermQuery(new Term("song", "foo")));
+
+            DisjunctionMaxQuery s = new DisjunctionMaxQuery(0.0f);
+            s.add(new TermQuery(new Term("username", "fighers")));
+            s.add(new TermQuery(new Term("song", "fighers")));
+            DisjunctionMaxQuery gen = new DisjunctionMaxQuery(0f);
+            gen.add(new TermQuery(new Term("username", "generator")));
+            gen.add(new TermQuery(new Term("song", "generator")));
+            query.add(uname, BooleanClause.Occur.SHOULD);
+            query.add(s, BooleanClause.Occur.SHOULD);
+            query.add(gen, BooleanClause.Occur.SHOULD);
+            TopDocs search = searcher.search(query, 4);
+            ScoreDoc[] scoreDocs = search.scoreDocs;
+            assertEquals(Integer.toString(1), reader.document(scoreDocs[0].doc).getField("id").stringValue());
+
+        }
+        reader.close();
+        w.close();
+        dir.close();
+    }
+
+    @Test
+    public void testBasics() {
+        final int iters = scaledRandomIntBetween(5, 25);
+        for (int j = 0; j < iters; j++) {
+            String[] fields = new String[1 + random().nextInt(10)];
+            for (int i = 0; i < fields.length; i++) {
+                fields[i] = TestUtil.randomRealisticUnicodeString(random(), 1, 10);
+            }
+            String term = TestUtil.randomRealisticUnicodeString(random(), 1, 10);
+            Term[] terms = toTerms(fields, term);
+            boolean disableCoord = random().nextBoolean();
+            boolean useBoolean = random().nextBoolean();
+            float tieBreaker = random().nextFloat();
+            BlendedTermQuery query = useBoolean ? BlendedTermQuery.booleanBlendedQuery(terms, disableCoord) : BlendedTermQuery.dismaxBlendedQuery(terms, tieBreaker);
+            QueryUtils.check(query);
+            terms = toTerms(fields, term);
+            BlendedTermQuery query2 = useBoolean ? BlendedTermQuery.booleanBlendedQuery(terms, disableCoord) : BlendedTermQuery.dismaxBlendedQuery(terms, tieBreaker);
+            assertEquals(query, query2);
+        }
+    }
+
+    public Term[] toTerms(String[] fields, String term) {
+        Term[] terms = new Term[fields.length];
+        List<String> fieldsList = Arrays.asList(fields);
+        Collections.shuffle(fieldsList, random());
+        fields = fieldsList.toArray(new String[0]);
+        for (int i = 0; i < fields.length; i++) {
+            terms[i] = new Term(fields[i], term);
+        }
+        return terms;
+    }
+
+    public IndexSearcher setSimilarity(IndexSearcher searcher) {
+        Similarity similarity = random().nextBoolean() ? new BM25Similarity() : new DefaultSimilarity();
+        searcher.setSimilarity(similarity);
+        return searcher;
+    }
+
+    @Test
+    public void testExtractTerms() throws IOException {
+        Set<Term> terms = new HashSet<>();
+        int num = scaledRandomIntBetween(1, 10);
+        for (int i = 0; i < num; i++) {
+            terms.add(new Term(TestUtil.randomRealisticUnicodeString(random(), 1, 10), TestUtil.randomRealisticUnicodeString(random(), 1, 10)));
+        }
+
+        BlendedTermQuery blendedTermQuery = random().nextBoolean() ? BlendedTermQuery.dismaxBlendedQuery(terms.toArray(new Term[0]), random().nextFloat()) :
+                BlendedTermQuery.booleanBlendedQuery(terms.toArray(new Term[0]), random().nextBoolean());
+        Set<Term> extracted = new HashSet<>();
+        IndexSearcher searcher = new IndexSearcher(new MultiReader());
+        searcher.createNormalizedWeight(blendedTermQuery, false).extractTerms(extracted);
+        assertThat(extracted.size(), equalTo(terms.size()));
+        assertThat(extracted, containsInAnyOrder(terms.toArray(new Term[0])));
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ESExceptionTests.java b/core/src/test/java/org/elasticsearch/ESExceptionTests.java
index eb3d870..dea127a 100644
--- a/core/src/test/java/org/elasticsearch/ESExceptionTests.java
+++ b/core/src/test/java/org/elasticsearch/ESExceptionTests.java
@@ -35,7 +35,8 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentLocation;
 import org.elasticsearch.index.Index;
 import org.elasticsearch.index.IndexNotFoundException;
-import org.elasticsearch.index.query.*;
+import org.elasticsearch.index.query.QueryParsingException;
+import org.elasticsearch.index.query.TestQueryParsingException;
 import org.elasticsearch.rest.RestStatus;
 import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.SearchShardTarget;
@@ -51,6 +52,7 @@ import java.io.EOFException;
 import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.nio.file.NoSuchFileException;
+import java.util.Collections;
 
 import static org.hamcrest.Matchers.equalTo;
 
@@ -307,7 +309,7 @@ public class ESExceptionTests extends ESTestCase {
                 new OutOfMemoryError("no memory left"),
                 new AlreadyClosedException("closed!!", new NullPointerException()),
                 new LockObtainFailedException("can't lock directory", new NullPointerException()),
-                new Throwable("this exception is unknown", new QueryShardException(new Index("foo"), "foobar", null) ), // somethin unknown
+                new Throwable("this exception is unknown", new QueryParsingException(new Index("foo"), 1, 2, "foobar", null) ), // somethin unknown
         };
         for (Throwable t : causes) {
             BytesStreamOutput out = new BytesStreamOutput();
diff --git a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
index 8117a69..dc0dd76 100644
--- a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
+++ b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
@@ -28,22 +28,18 @@ import org.elasticsearch.action.RoutingMissingException;
 import org.elasticsearch.action.TimestampParsingException;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.ShardSearchFailure;
+import org.elasticsearch.client.AbstractClientHeadersTestCase;
 import org.elasticsearch.cluster.block.ClusterBlockException;
 import org.elasticsearch.cluster.metadata.SnapshotId;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.routing.*;
 import org.elasticsearch.common.breaker.CircuitBreakingException;
 import org.elasticsearch.common.io.PathUtils;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.NotSerializableExceptionWrapper;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.io.stream.*;
 import org.elasticsearch.common.transport.LocalTransportAddress;
 import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentLocation;
+import org.elasticsearch.common.util.CancellableThreadsTests;
+import org.elasticsearch.common.xcontent.*;
 import org.elasticsearch.discovery.DiscoverySettings;
 import org.elasticsearch.index.AlreadyExpiredException;
 import org.elasticsearch.index.Index;
@@ -52,11 +48,7 @@ import org.elasticsearch.index.engine.IndexFailedEngineException;
 import org.elasticsearch.index.engine.RecoveryEngineException;
 import org.elasticsearch.index.mapper.MergeMappingException;
 import org.elasticsearch.index.query.QueryParsingException;
-import org.elasticsearch.index.query.QueryShardException;
-import org.elasticsearch.index.shard.IllegalIndexShardStateException;
-import org.elasticsearch.index.shard.IndexShardState;
-import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.index.shard.TranslogRecoveryPerformer;
+import org.elasticsearch.index.shard.*;
 import org.elasticsearch.indices.IndexTemplateAlreadyExistsException;
 import org.elasticsearch.indices.IndexTemplateMissingException;
 import org.elasticsearch.indices.InvalidIndexTemplateException;
@@ -104,9 +96,9 @@ public class ExceptionSerializationTests extends ESTestCase {
                 org.elasticsearch.test.rest.parser.RestTestParseException.class,
                 org.elasticsearch.index.query.TestQueryParsingException.class,
                 org.elasticsearch.test.rest.client.RestException.class,
-                org.elasticsearch.common.util.CancellableThreadsTest.CustomException.class,
+                CancellableThreadsTests.CustomException.class,
                 org.elasticsearch.rest.BytesRestResponseTests.WithHeadersException.class,
-                org.elasticsearch.client.AbstractClientHeadersTests.InternalException.class);
+                AbstractClientHeadersTestCase.InternalException.class);
         FileVisitor<Path> visitor = new FileVisitor<Path>() {
             private Path pkgPrefix = PathUtils.get(path).getParent();
 
@@ -231,16 +223,6 @@ public class ExceptionSerializationTests extends ESTestCase {
         assertEquals(ex.getColumnNumber(), 2);
     }
 
-    public void testQueryShardException() throws IOException {
-        QueryShardException ex = serialize(new QueryShardException(new Index("foo"), "fobar", null));
-        assertEquals(ex.getIndex(), "foo");
-        assertEquals(ex.getMessage(), "fobar");
-
-        ex = serialize(new QueryShardException((Index)null, null, null));
-        assertNull(ex.getIndex());
-        assertNull(ex.getMessage());
-    }
-
     public void testSearchException() throws IOException {
         SearchShardTarget target = new SearchShardTarget("foo", "bar", 1);
         SearchException ex = serialize(new SearchException(target, "hello world"));
diff --git a/core/src/test/java/org/elasticsearch/NamingConventionTests.java b/core/src/test/java/org/elasticsearch/NamingConventionTests.java
index 071cd25..051edd5 100644
--- a/core/src/test/java/org/elasticsearch/NamingConventionTests.java
+++ b/core/src/test/java/org/elasticsearch/NamingConventionTests.java
@@ -18,25 +18,25 @@
  */
 package org.elasticsearch;
 
-import com.google.common.base.Joiner;
-import com.google.common.collect.Sets;
-
 import junit.framework.TestCase;
 
+import com.google.common.base.Joiner;
+
 import org.apache.lucene.util.LuceneTestCase;
-import org.elasticsearch.common.SuppressForbidden;
 import org.elasticsearch.common.io.PathUtils;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESTestCase;
 import org.elasticsearch.test.ESTokenStreamTestCase;
-import org.junit.Ignore;
 import org.junit.Test;
 
 import java.io.IOException;
 import java.lang.reflect.Method;
 import java.lang.reflect.Modifier;
 import java.net.URISyntaxException;
-import java.nio.file.*;
+import java.nio.file.FileVisitResult;
+import java.nio.file.FileVisitor;
+import java.nio.file.Files;
+import java.nio.file.Path;
 import java.nio.file.attribute.BasicFileAttributes;
 import java.util.HashSet;
 import java.util.Set;
@@ -53,20 +53,17 @@ public class NamingConventionTests extends ESTestCase {
         final Set<Class> pureUnitTest = new HashSet<>();
         final Set<Class> missingSuffix = new HashSet<>();
         final Set<Class> integTestsInDisguise = new HashSet<>();
+        final Set<Class> notRunnable = new HashSet<>();
+        final Set<Class> innerClasses = new HashSet<>();
         String[] packages = {"org.elasticsearch", "org.apache.lucene"};
         for (final String packageName : packages) {
             final String path = "/" + packageName.replace('.', '/');
             final Path startPath = getDataPath(path);
-            final Set<Path> ignore = Sets.newHashSet(PathUtils.get("/org/elasticsearch/stresstest"), PathUtils.get("/org/elasticsearch/benchmark/stress"));
             Files.walkFileTree(startPath, new FileVisitor<Path>() {
                 private Path pkgPrefix = PathUtils.get(path).getParent();
                 @Override
                 public FileVisitResult preVisitDirectory(Path dir, BasicFileAttributes attrs) throws IOException {
-                    Path next =  pkgPrefix.resolve(dir.getFileName());
-                    if (ignore.contains(next)) {
-                        return FileVisitResult.SKIP_SUBTREE;
-                    }
-                    pkgPrefix = next;
+                    pkgPrefix = pkgPrefix.resolve(dir.getFileName());
                     return FileVisitResult.CONTINUE;
                 }
 
@@ -76,28 +73,30 @@ public class NamingConventionTests extends ESTestCase {
                         String filename = file.getFileName().toString();
                         if (filename.endsWith(".class")) {
                             Class<?> clazz = loadClass(filename);
-                            if (Modifier.isAbstract(clazz.getModifiers()) == false && Modifier.isInterface(clazz.getModifiers()) == false) {
-                                if (clazz.getName().endsWith("Tests") || 
-                                    clazz.getName().endsWith("Test")) { // don't worry about the ones that match the pattern
-
-                                    if (ESIntegTestCase.class.isAssignableFrom(clazz)) {
-                                        integTestsInDisguise.add(clazz);
-                                    }
-                                    if (isTestCase(clazz) == false) {
-                                        notImplementing.add(clazz);
-                                    }
-                                } else if (clazz.getName().endsWith("IT")) {
-                                    if (isTestCase(clazz) == false) {
-                                        notImplementing.add(clazz);
-                                    }
-                                    // otherwise fine
-                                } else if (isTestCase(clazz)) {
+                            if (clazz.getName().endsWith("Tests")) { // don't worry about the ones that match the pattern
+
+                                if (ESIntegTestCase.class.isAssignableFrom(clazz)) {
+                                    integTestsInDisguise.add(clazz);
+                                }
+                                if (Modifier.isAbstract(clazz.getModifiers()) || Modifier.isInterface(clazz.getModifiers())) {
+                                    notRunnable.add(clazz);
+                                } else if (isTestCase(clazz) == false) {
+                                    notImplementing.add(clazz);
+                                } else if (Modifier.isStatic(clazz.getModifiers())) {
+                                    innerClasses.add(clazz);
+                                }
+                            } else if (clazz.getName().endsWith("IT")) {
+                                if (isTestCase(clazz) == false) {
+                                    notImplementing.add(clazz);
+                                }
+                                // otherwise fine
+                            } else if (Modifier.isAbstract(clazz.getModifiers()) == false && Modifier.isInterface(clazz.getModifiers()) == false) {
+                                if (isTestCase(clazz)) {
                                     missingSuffix.add(clazz);
                                 } else if (junit.framework.Test.class.isAssignableFrom(clazz) || hasTestAnnotation(clazz)) {
                                     pureUnitTest.add(clazz);
                                 }
                             }
-
                         }
                     } catch (ClassNotFoundException e) {
                         throw new RuntimeException(e);
@@ -143,39 +142,47 @@ public class NamingConventionTests extends ESTestCase {
         }
         assertTrue(missingSuffix.remove(WrongName.class));
         assertTrue(missingSuffix.remove(WrongNameTheSecond.class));
+        assertTrue(notRunnable.remove(DummyAbstractTests.class));
+        assertTrue(notRunnable.remove(DummyInterfaceTests.class));
+        assertTrue(innerClasses.remove(InnerTests.class));
         assertTrue(notImplementing.remove(NotImplementingTests.class));
-        assertTrue(notImplementing.remove(NotImplementingTest.class));
         assertTrue(pureUnitTest.remove(PlainUnit.class));
         assertTrue(pureUnitTest.remove(PlainUnitTheSecond.class));
 
         String classesToSubclass = Joiner.on(',').join(
-                ESTestCase.class.getSimpleName(),
-                ESTestCase.class.getSimpleName(),
-                ESTokenStreamTestCase.class.getSimpleName(),
-                LuceneTestCase.class.getSimpleName());
+            ESTestCase.class.getSimpleName(),
+            ESTestCase.class.getSimpleName(),
+            ESTokenStreamTestCase.class.getSimpleName(),
+            LuceneTestCase.class.getSimpleName());
         assertTrue("Not all subclasses of " + ESTestCase.class.getSimpleName() +
-                        " match the naming convention. Concrete classes must end with [Test|Tests]: " + missingSuffix.toString(),
-                missingSuffix.isEmpty());
-        assertTrue("Pure Unit-Test found must subclass one of [" + classesToSubclass +"] " + pureUnitTest.toString(),
-                pureUnitTest.isEmpty());
-        assertTrue("Classes ending with Test|Tests] must subclass [" + classesToSubclass +"] " + notImplementing.toString(),
-                notImplementing.isEmpty());
-        assertTrue("Subclasses of ESIntegTestCase should end with IT as they are integration tests: " + integTestsInDisguise, integTestsInDisguise.isEmpty());
+ " match the naming convention. Concrete classes must end with [Tests]:\n" + Joiner.on('\n').join(missingSuffix),
+            missingSuffix.isEmpty());
+        assertTrue("Classes ending with [Tests] are abstract or interfaces:\n" + Joiner.on('\n').join(notRunnable),
+            notRunnable.isEmpty());
+        assertTrue("Found inner classes that are tests, which are excluded from the test runner:\n" + Joiner.on('\n').join(innerClasses),
+            innerClasses.isEmpty());
+        assertTrue("Pure Unit-Test found must subclass one of [" + classesToSubclass +"]:\n" + Joiner.on('\n').join(pureUnitTest),
+            pureUnitTest.isEmpty());
+        assertTrue("Classes ending with [Tests] must subclass [" + classesToSubclass + "]:\n" + Joiner.on('\n').join(notImplementing),
+            notImplementing.isEmpty());
+        assertTrue("Subclasses of ESIntegTestCase should end with IT as they are integration tests:\n" + Joiner.on('\n').join(integTestsInDisguise),
+            integTestsInDisguise.isEmpty());
     }
 
     /*
      * Some test the test classes
      */
 
-    @SuppressForbidden(reason = "Ignoring test the tester")
-    @Ignore
     public static final class NotImplementingTests {}
-    @SuppressForbidden(reason = "Ignoring test the tester")
-    @Ignore
-    public static final class NotImplementingTest {}
 
     public static final class WrongName extends ESTestCase {}
 
+    public static abstract class DummyAbstractTests extends ESTestCase {}
+
+    public interface DummyInterfaceTests {}
+
+    public static final class InnerTests extends ESTestCase {}
+
     public static final class WrongNameTheSecond extends ESTestCase {}
 
     public static final class PlainUnit extends TestCase {}
diff --git a/core/src/test/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequestTest.java b/core/src/test/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequestTest.java
deleted file mode 100644
index 1b5a4ef..0000000
--- a/core/src/test/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequestTest.java
+++ /dev/null
@@ -1,77 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.admin.cluster.state;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.action.support.IndicesOptions;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.VersionUtils;
-import org.junit.Test;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-/**
- * Unit tests for the {@link ClusterStateRequest}.
- */
-public class ClusterStateRequestTest extends ESTestCase {
-
-    @Test
-    public void testSerialization() throws Exception {
-        int iterations = randomIntBetween(5, 20);
-        for (int i = 0; i < iterations; i++) {
-
-            IndicesOptions indicesOptions = IndicesOptions.fromOptions(randomBoolean(), randomBoolean(), randomBoolean(), randomBoolean());
-            ClusterStateRequest clusterStateRequest = new ClusterStateRequest().routingTable(randomBoolean()).metaData(randomBoolean())
-                    .nodes(randomBoolean()).blocks(randomBoolean()).indices("testindex", "testindex2").indicesOptions(indicesOptions);
-
-            Version testVersion = VersionUtils.randomVersionBetween(random(), Version.CURRENT.minimumCompatibilityVersion(), Version.CURRENT);
-            BytesStreamOutput output = new BytesStreamOutput();
-            output.setVersion(testVersion);
-            clusterStateRequest.writeTo(output);
-
-            StreamInput streamInput = StreamInput.wrap(output.bytes());
-            streamInput.setVersion(testVersion);
-            ClusterStateRequest deserializedCSRequest = new ClusterStateRequest();
-            deserializedCSRequest.readFrom(streamInput);
-
-            assertThat(deserializedCSRequest.routingTable(), equalTo(clusterStateRequest.routingTable()));
-            assertThat(deserializedCSRequest.metaData(), equalTo(clusterStateRequest.metaData()));
-            assertThat(deserializedCSRequest.nodes(), equalTo(clusterStateRequest.nodes()));
-            assertThat(deserializedCSRequest.blocks(), equalTo(clusterStateRequest.blocks()));
-            assertThat(deserializedCSRequest.indices(), equalTo(clusterStateRequest.indices()));
-
-            if (testVersion.onOrAfter(Version.V_1_5_0)) {
-                assertOptionsMatch(deserializedCSRequest.indicesOptions(), clusterStateRequest.indicesOptions());
-            } else {
-                // versions before V_1_5_0 use IndicesOptions.lenientExpandOpen()
-                assertOptionsMatch(deserializedCSRequest.indicesOptions(), IndicesOptions.lenientExpandOpen());
-            }
-        }
-    }
-
-    private static void assertOptionsMatch(IndicesOptions in, IndicesOptions out) {
-        assertThat(in.ignoreUnavailable(), equalTo(out.ignoreUnavailable()));
-        assertThat(in.expandWildcardsClosed(), equalTo(out.expandWildcardsClosed()));
-        assertThat(in.expandWildcardsOpen(), equalTo(out.expandWildcardsOpen()));
-        assertThat(in.allowNoIndices(), equalTo(out.allowNoIndices()));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequestTests.java b/core/src/test/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequestTests.java
new file mode 100644
index 0000000..5c06275
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequestTests.java
@@ -0,0 +1,77 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.admin.cluster.state;
+
+import org.elasticsearch.Version;
+import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.BytesStreamOutput;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.VersionUtils;
+import org.junit.Test;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+
+/**
+ * Unit tests for the {@link ClusterStateRequest}.
+ */
+public class ClusterStateRequestTests extends ESTestCase {
+
+    @Test
+    public void testSerialization() throws Exception {
+        int iterations = randomIntBetween(5, 20);
+        for (int i = 0; i < iterations; i++) {
+
+            IndicesOptions indicesOptions = IndicesOptions.fromOptions(randomBoolean(), randomBoolean(), randomBoolean(), randomBoolean());
+            ClusterStateRequest clusterStateRequest = new ClusterStateRequest().routingTable(randomBoolean()).metaData(randomBoolean())
+                    .nodes(randomBoolean()).blocks(randomBoolean()).indices("testindex", "testindex2").indicesOptions(indicesOptions);
+
+            Version testVersion = VersionUtils.randomVersionBetween(random(), Version.CURRENT.minimumCompatibilityVersion(), Version.CURRENT);
+            BytesStreamOutput output = new BytesStreamOutput();
+            output.setVersion(testVersion);
+            clusterStateRequest.writeTo(output);
+
+            StreamInput streamInput = StreamInput.wrap(output.bytes());
+            streamInput.setVersion(testVersion);
+            ClusterStateRequest deserializedCSRequest = new ClusterStateRequest();
+            deserializedCSRequest.readFrom(streamInput);
+
+            assertThat(deserializedCSRequest.routingTable(), equalTo(clusterStateRequest.routingTable()));
+            assertThat(deserializedCSRequest.metaData(), equalTo(clusterStateRequest.metaData()));
+            assertThat(deserializedCSRequest.nodes(), equalTo(clusterStateRequest.nodes()));
+            assertThat(deserializedCSRequest.blocks(), equalTo(clusterStateRequest.blocks()));
+            assertThat(deserializedCSRequest.indices(), equalTo(clusterStateRequest.indices()));
+
+            if (testVersion.onOrAfter(Version.V_1_5_0)) {
+                assertOptionsMatch(deserializedCSRequest.indicesOptions(), clusterStateRequest.indicesOptions());
+            } else {
+                // versions before V_1_5_0 use IndicesOptions.lenientExpandOpen()
+                assertOptionsMatch(deserializedCSRequest.indicesOptions(), IndicesOptions.lenientExpandOpen());
+            }
+        }
+    }
+
+    private static void assertOptionsMatch(IndicesOptions in, IndicesOptions out) {
+        assertThat(in.ignoreUnavailable(), equalTo(out.ignoreUnavailable()));
+        assertThat(in.expandWildcardsClosed(), equalTo(out.expandWildcardsClosed()));
+        assertThat(in.expandWildcardsOpen(), equalTo(out.expandWildcardsOpen()));
+        assertThat(in.allowNoIndices(), equalTo(out.allowNoIndices()));
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequestBuilderTest.java b/core/src/test/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequestBuilderTest.java
deleted file mode 100644
index 2fff835..0000000
--- a/core/src/test/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequestBuilderTest.java
+++ /dev/null
@@ -1,111 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.admin.indices.create;
-
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.rest.NoOpClient;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-public class CreateIndexRequestBuilderTest extends ESTestCase {
-
-    private static final String KEY = "my.settings.key";
-    private static final String VALUE = "my.settings.value";
-    private NoOpClient testClient;
-
-    @Override
-    @Before
-    public void setUp() throws Exception {
-        super.setUp();
-        this.testClient = new NoOpClient(getTestName());
-    }
-
-    @Override
-    @After
-    public void tearDown() throws Exception {
-        this.testClient.close();
-        super.tearDown();
-    }
-
-    /**
-     * test setting the source with available setters
-     */
-    @Test
-    public void testSetSource() throws IOException {
-        CreateIndexRequestBuilder builder = new CreateIndexRequestBuilder(this.testClient, CreateIndexAction.INSTANCE);
-        builder.setSource("{\""+KEY+"\" : \""+VALUE+"\"}");
-        assertEquals(VALUE, builder.request().settings().get(KEY));
-
-        XContentBuilder xContent = XContentFactory.jsonBuilder().startObject().field(KEY, VALUE).endObject();
-        xContent.close();
-        builder.setSource(xContent);
-        assertEquals(VALUE, builder.request().settings().get(KEY));
-
-        ByteArrayOutputStream docOut = new ByteArrayOutputStream();
-        XContentBuilder doc = XContentFactory.jsonBuilder(docOut).startObject().field(KEY, VALUE).endObject();
-        doc.close();
-        builder.setSource(docOut.toByteArray());
-        assertEquals(VALUE, builder.request().settings().get(KEY));
-
-        Map<String, String> settingsMap = new HashMap<>();
-        settingsMap.put(KEY, VALUE);
-        builder.setSettings(settingsMap);
-        assertEquals(VALUE, builder.request().settings().get(KEY));
-    }
-
-    /**
-     * test setting the settings with available setters
-     */
-    @Test
-    public void testSetSettings() throws IOException {
-        CreateIndexRequestBuilder builder = new CreateIndexRequestBuilder(this.testClient, CreateIndexAction.INSTANCE);
-        builder.setSettings(KEY, VALUE);
-        assertEquals(VALUE, builder.request().settings().get(KEY));
-
-        builder.setSettings("{\""+KEY+"\" : \""+VALUE+"\"}");
-        assertEquals(VALUE, builder.request().settings().get(KEY));
-
-        builder.setSettings(Settings.builder().put(KEY, VALUE));
-        assertEquals(VALUE, builder.request().settings().get(KEY));
-
-        builder.setSettings(Settings.builder().put(KEY, VALUE).build());
-        assertEquals(VALUE, builder.request().settings().get(KEY));
-
-        Map<String, String> settingsMap = new HashMap<>();
-        settingsMap.put(KEY, VALUE);
-        builder.setSettings(settingsMap);
-        assertEquals(VALUE, builder.request().settings().get(KEY));
-
-        XContentBuilder xContent = XContentFactory.jsonBuilder().startObject().field(KEY, VALUE).endObject();
-        xContent.close();
-        builder.setSettings(xContent);
-        assertEquals(VALUE, builder.request().settings().get(KEY));
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequestBuilderTests.java b/core/src/test/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequestBuilderTests.java
new file mode 100644
index 0000000..98569b7
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequestBuilderTests.java
@@ -0,0 +1,111 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.admin.indices.create;
+
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.rest.NoOpClient;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+public class CreateIndexRequestBuilderTests extends ESTestCase {
+
+    private static final String KEY = "my.settings.key";
+    private static final String VALUE = "my.settings.value";
+    private NoOpClient testClient;
+
+    @Override
+    @Before
+    public void setUp() throws Exception {
+        super.setUp();
+        this.testClient = new NoOpClient(getTestName());
+    }
+
+    @Override
+    @After
+    public void tearDown() throws Exception {
+        this.testClient.close();
+        super.tearDown();
+    }
+
+    /**
+     * test setting the source with available setters
+     */
+    @Test
+    public void testSetSource() throws IOException {
+        CreateIndexRequestBuilder builder = new CreateIndexRequestBuilder(this.testClient, CreateIndexAction.INSTANCE);
+        builder.setSource("{\""+KEY+"\" : \""+VALUE+"\"}");
+        assertEquals(VALUE, builder.request().settings().get(KEY));
+
+        XContentBuilder xContent = XContentFactory.jsonBuilder().startObject().field(KEY, VALUE).endObject();
+        xContent.close();
+        builder.setSource(xContent);
+        assertEquals(VALUE, builder.request().settings().get(KEY));
+
+        ByteArrayOutputStream docOut = new ByteArrayOutputStream();
+        XContentBuilder doc = XContentFactory.jsonBuilder(docOut).startObject().field(KEY, VALUE).endObject();
+        doc.close();
+        builder.setSource(docOut.toByteArray());
+        assertEquals(VALUE, builder.request().settings().get(KEY));
+
+        Map<String, String> settingsMap = new HashMap<>();
+        settingsMap.put(KEY, VALUE);
+        builder.setSettings(settingsMap);
+        assertEquals(VALUE, builder.request().settings().get(KEY));
+    }
+
+    /**
+     * test setting the settings with available setters
+     */
+    @Test
+    public void testSetSettings() throws IOException {
+        CreateIndexRequestBuilder builder = new CreateIndexRequestBuilder(this.testClient, CreateIndexAction.INSTANCE);
+        builder.setSettings(KEY, VALUE);
+        assertEquals(VALUE, builder.request().settings().get(KEY));
+
+        builder.setSettings("{\""+KEY+"\" : \""+VALUE+"\"}");
+        assertEquals(VALUE, builder.request().settings().get(KEY));
+
+        builder.setSettings(Settings.builder().put(KEY, VALUE));
+        assertEquals(VALUE, builder.request().settings().get(KEY));
+
+        builder.setSettings(Settings.builder().put(KEY, VALUE).build());
+        assertEquals(VALUE, builder.request().settings().get(KEY));
+
+        Map<String, String> settingsMap = new HashMap<>();
+        settingsMap.put(KEY, VALUE);
+        builder.setSettings(settingsMap);
+        assertEquals(VALUE, builder.request().settings().get(KEY));
+
+        XContentBuilder xContent = XContentFactory.jsonBuilder().startObject().field(KEY, VALUE).endObject();
+        xContent.close();
+        builder.setSettings(xContent);
+        assertEquals(VALUE, builder.request().settings().get(KEY));
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoreResponseTest.java b/core/src/test/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoreResponseTest.java
deleted file mode 100644
index 925e01e..0000000
--- a/core/src/test/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoreResponseTest.java
+++ /dev/null
@@ -1,122 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.admin.indices.shards;
-
-import org.apache.lucene.util.CollectionUtil;
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.collect.ImmutableOpenIntMap;
-import org.elasticsearch.common.collect.ImmutableOpenMap;
-import org.elasticsearch.common.transport.DummyTransportAddress;
-import org.elasticsearch.common.xcontent.*;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.transport.NodeDisconnectedException;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.*;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public class IndicesShardStoreResponseTest extends ESTestCase {
-
-    @Test
-    public void testBasicSerialization() throws Exception {
-        ImmutableOpenMap.Builder<String, ImmutableOpenIntMap<List<IndicesShardStoresResponse.StoreStatus>>> indexStoreStatuses = ImmutableOpenMap.builder();
-        List<IndicesShardStoresResponse.Failure> failures = new ArrayList<>();
-        ImmutableOpenIntMap.Builder<List<IndicesShardStoresResponse.StoreStatus>> storeStatuses = ImmutableOpenIntMap.builder();
-
-        DiscoveryNode node1 = new DiscoveryNode("node1", DummyTransportAddress.INSTANCE, Version.CURRENT);
-        DiscoveryNode node2 = new DiscoveryNode("node2", DummyTransportAddress.INSTANCE, Version.CURRENT);
-        List<IndicesShardStoresResponse.StoreStatus> storeStatusList = new ArrayList<>();
-        storeStatusList.add(new IndicesShardStoresResponse.StoreStatus(node1, 3, IndicesShardStoresResponse.StoreStatus.Allocation.PRIMARY, null));
-        storeStatusList.add(new IndicesShardStoresResponse.StoreStatus(node2, 2, IndicesShardStoresResponse.StoreStatus.Allocation.REPLICA, null));
-        storeStatusList.add(new IndicesShardStoresResponse.StoreStatus(node1, 1, IndicesShardStoresResponse.StoreStatus.Allocation.UNUSED, new IOException("corrupted")));
-        storeStatuses.put(0, storeStatusList);
-        storeStatuses.put(1, storeStatusList);
-        ImmutableOpenIntMap<List<IndicesShardStoresResponse.StoreStatus>> storesMap = storeStatuses.build();
-        indexStoreStatuses.put("test", storesMap);
-        indexStoreStatuses.put("test2", storesMap);
-
-        failures.add(new IndicesShardStoresResponse.Failure("node1", "test", 3, new NodeDisconnectedException(node1, "")));
-
-        IndicesShardStoresResponse storesResponse = new IndicesShardStoresResponse(indexStoreStatuses.build(), Collections.unmodifiableList(failures));
-        XContentBuilder contentBuilder = XContentFactory.jsonBuilder();
-        contentBuilder.startObject();
-        storesResponse.toXContent(contentBuilder, ToXContent.EMPTY_PARAMS);
-        contentBuilder.endObject();
-        BytesReference bytes = contentBuilder.bytes();
-
-        try (XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(bytes)) {
-            Map<String, Object> map = parser.map();
-            List failureList = (List) map.get("failures");
-            assertThat(failureList.size(), equalTo(1));
-            HashMap failureMap = (HashMap) failureList.get(0);
-            assertThat(failureMap.containsKey("index"), equalTo(true));
-            assertThat(((String) failureMap.get("index")), equalTo("test"));
-            assertThat(failureMap.containsKey("shard"), equalTo(true));
-            assertThat(((int) failureMap.get("shard")), equalTo(3));
-            assertThat(failureMap.containsKey("node"), equalTo(true));
-            assertThat(((String) failureMap.get("node")), equalTo("node1"));
-
-            Map<String, Object> indices = (Map<String, Object>) map.get("indices");
-            for (String index : new String[] {"test", "test2"}) {
-                assertThat(indices.containsKey(index), equalTo(true));
-                Map<String, Object> shards = ((Map<String, Object>) ((Map<String, Object>) indices.get(index)).get("shards"));
-                assertThat(shards.size(), equalTo(2));
-                for (String shardId : shards.keySet()) {
-                    HashMap shardStoresStatus = (HashMap) shards.get(shardId);
-                    assertThat(shardStoresStatus.containsKey("stores"), equalTo(true));
-                    List stores = (ArrayList) shardStoresStatus.get("stores");
-                    assertThat(stores.size(), equalTo(storeStatusList.size()));
-                    for (int i = 0; i < stores.size(); i++) {
-                        HashMap storeInfo = ((HashMap) stores.get(i));
-                        IndicesShardStoresResponse.StoreStatus storeStatus = storeStatusList.get(i);
-                        assertThat(storeInfo.containsKey("version"), equalTo(true));
-                        assertThat(((int) storeInfo.get("version")), equalTo(((int) storeStatus.getVersion())));
-                        assertThat(storeInfo.containsKey("allocation"), equalTo(true));
-                        assertThat(((String) storeInfo.get("allocation")), equalTo(storeStatus.getAllocation().value()));
-                        assertThat(storeInfo.containsKey(storeStatus.getNode().id()), equalTo(true));
-                        if (storeStatus.getStoreException() != null) {
-                            assertThat(storeInfo.containsKey("store_exception"), equalTo(true));
-                        }
-                    }
-                }
-            }
-        }
-    }
-
-    @Test
-    public void testStoreStatusOrdering() throws Exception {
-        DiscoveryNode node1 = new DiscoveryNode("node1", DummyTransportAddress.INSTANCE, Version.CURRENT);
-        List<IndicesShardStoresResponse.StoreStatus> orderedStoreStatuses = new ArrayList<>();
-        orderedStoreStatuses.add(new IndicesShardStoresResponse.StoreStatus(node1, 2, IndicesShardStoresResponse.StoreStatus.Allocation.PRIMARY, null));
-        orderedStoreStatuses.add(new IndicesShardStoresResponse.StoreStatus(node1, 1, IndicesShardStoresResponse.StoreStatus.Allocation.PRIMARY, null));
-        orderedStoreStatuses.add(new IndicesShardStoresResponse.StoreStatus(node1, 1, IndicesShardStoresResponse.StoreStatus.Allocation.REPLICA, null));
-        orderedStoreStatuses.add(new IndicesShardStoresResponse.StoreStatus(node1, 1, IndicesShardStoresResponse.StoreStatus.Allocation.UNUSED, null));
-        orderedStoreStatuses.add(new IndicesShardStoresResponse.StoreStatus(node1, 3, IndicesShardStoresResponse.StoreStatus.Allocation.REPLICA, new IOException("corrupted")));
-
-        List<IndicesShardStoresResponse.StoreStatus> storeStatuses = new ArrayList<>(orderedStoreStatuses);
-        Collections.shuffle(storeStatuses);
-        CollectionUtil.timSort(storeStatuses);
-        assertThat(storeStatuses, equalTo(orderedStoreStatuses));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoreResponseTests.java b/core/src/test/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoreResponseTests.java
new file mode 100644
index 0000000..777555f
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoreResponseTests.java
@@ -0,0 +1,122 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.admin.indices.shards;
+
+import org.apache.lucene.util.CollectionUtil;
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.collect.ImmutableOpenIntMap;
+import org.elasticsearch.common.collect.ImmutableOpenMap;
+import org.elasticsearch.common.transport.DummyTransportAddress;
+import org.elasticsearch.common.xcontent.*;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.transport.NodeDisconnectedException;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.*;
+
+import static org.hamcrest.Matchers.equalTo;
+
+public class IndicesShardStoreResponseTests extends ESTestCase {
+
+    @Test
+    public void testBasicSerialization() throws Exception {
+        ImmutableOpenMap.Builder<String, ImmutableOpenIntMap<List<IndicesShardStoresResponse.StoreStatus>>> indexStoreStatuses = ImmutableOpenMap.builder();
+        List<IndicesShardStoresResponse.Failure> failures = new ArrayList<>();
+        ImmutableOpenIntMap.Builder<List<IndicesShardStoresResponse.StoreStatus>> storeStatuses = ImmutableOpenIntMap.builder();
+
+        DiscoveryNode node1 = new DiscoveryNode("node1", DummyTransportAddress.INSTANCE, Version.CURRENT);
+        DiscoveryNode node2 = new DiscoveryNode("node2", DummyTransportAddress.INSTANCE, Version.CURRENT);
+        List<IndicesShardStoresResponse.StoreStatus> storeStatusList = new ArrayList<>();
+        storeStatusList.add(new IndicesShardStoresResponse.StoreStatus(node1, 3, IndicesShardStoresResponse.StoreStatus.Allocation.PRIMARY, null));
+        storeStatusList.add(new IndicesShardStoresResponse.StoreStatus(node2, 2, IndicesShardStoresResponse.StoreStatus.Allocation.REPLICA, null));
+        storeStatusList.add(new IndicesShardStoresResponse.StoreStatus(node1, 1, IndicesShardStoresResponse.StoreStatus.Allocation.UNUSED, new IOException("corrupted")));
+        storeStatuses.put(0, storeStatusList);
+        storeStatuses.put(1, storeStatusList);
+        ImmutableOpenIntMap<List<IndicesShardStoresResponse.StoreStatus>> storesMap = storeStatuses.build();
+        indexStoreStatuses.put("test", storesMap);
+        indexStoreStatuses.put("test2", storesMap);
+
+        failures.add(new IndicesShardStoresResponse.Failure("node1", "test", 3, new NodeDisconnectedException(node1, "")));
+
+        IndicesShardStoresResponse storesResponse = new IndicesShardStoresResponse(indexStoreStatuses.build(), Collections.unmodifiableList(failures));
+        XContentBuilder contentBuilder = XContentFactory.jsonBuilder();
+        contentBuilder.startObject();
+        storesResponse.toXContent(contentBuilder, ToXContent.EMPTY_PARAMS);
+        contentBuilder.endObject();
+        BytesReference bytes = contentBuilder.bytes();
+
+        try (XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(bytes)) {
+            Map<String, Object> map = parser.map();
+            List failureList = (List) map.get("failures");
+            assertThat(failureList.size(), equalTo(1));
+            HashMap failureMap = (HashMap) failureList.get(0);
+            assertThat(failureMap.containsKey("index"), equalTo(true));
+            assertThat(((String) failureMap.get("index")), equalTo("test"));
+            assertThat(failureMap.containsKey("shard"), equalTo(true));
+            assertThat(((int) failureMap.get("shard")), equalTo(3));
+            assertThat(failureMap.containsKey("node"), equalTo(true));
+            assertThat(((String) failureMap.get("node")), equalTo("node1"));
+
+            Map<String, Object> indices = (Map<String, Object>) map.get("indices");
+            for (String index : new String[] {"test", "test2"}) {
+                assertThat(indices.containsKey(index), equalTo(true));
+                Map<String, Object> shards = ((Map<String, Object>) ((Map<String, Object>) indices.get(index)).get("shards"));
+                assertThat(shards.size(), equalTo(2));
+                for (String shardId : shards.keySet()) {
+                    HashMap shardStoresStatus = (HashMap) shards.get(shardId);
+                    assertThat(shardStoresStatus.containsKey("stores"), equalTo(true));
+                    List stores = (ArrayList) shardStoresStatus.get("stores");
+                    assertThat(stores.size(), equalTo(storeStatusList.size()));
+                    for (int i = 0; i < stores.size(); i++) {
+                        HashMap storeInfo = ((HashMap) stores.get(i));
+                        IndicesShardStoresResponse.StoreStatus storeStatus = storeStatusList.get(i);
+                        assertThat(storeInfo.containsKey("version"), equalTo(true));
+                        assertThat(((int) storeInfo.get("version")), equalTo(((int) storeStatus.getVersion())));
+                        assertThat(storeInfo.containsKey("allocation"), equalTo(true));
+                        assertThat(((String) storeInfo.get("allocation")), equalTo(storeStatus.getAllocation().value()));
+                        assertThat(storeInfo.containsKey(storeStatus.getNode().id()), equalTo(true));
+                        if (storeStatus.getStoreException() != null) {
+                            assertThat(storeInfo.containsKey("store_exception"), equalTo(true));
+                        }
+                    }
+                }
+            }
+        }
+    }
+
+    @Test
+    public void testStoreStatusOrdering() throws Exception {
+        DiscoveryNode node1 = new DiscoveryNode("node1", DummyTransportAddress.INSTANCE, Version.CURRENT);
+        List<IndicesShardStoresResponse.StoreStatus> orderedStoreStatuses = new ArrayList<>();
+        orderedStoreStatuses.add(new IndicesShardStoresResponse.StoreStatus(node1, 2, IndicesShardStoresResponse.StoreStatus.Allocation.PRIMARY, null));
+        orderedStoreStatuses.add(new IndicesShardStoresResponse.StoreStatus(node1, 1, IndicesShardStoresResponse.StoreStatus.Allocation.PRIMARY, null));
+        orderedStoreStatuses.add(new IndicesShardStoresResponse.StoreStatus(node1, 1, IndicesShardStoresResponse.StoreStatus.Allocation.REPLICA, null));
+        orderedStoreStatuses.add(new IndicesShardStoresResponse.StoreStatus(node1, 1, IndicesShardStoresResponse.StoreStatus.Allocation.UNUSED, null));
+        orderedStoreStatuses.add(new IndicesShardStoresResponse.StoreStatus(node1, 3, IndicesShardStoresResponse.StoreStatus.Allocation.REPLICA, new IOException("corrupted")));
+
+        List<IndicesShardStoresResponse.StoreStatus> storeStatuses = new ArrayList<>(orderedStoreStatuses);
+        Collections.shuffle(storeStatuses);
+        CollectionUtil.timSort(storeStatuses);
+        assertThat(storeStatuses, equalTo(orderedStoreStatuses));
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/action/fieldstats/FieldStatsRequestTest.java b/core/src/test/java/org/elasticsearch/action/fieldstats/FieldStatsRequestTest.java
deleted file mode 100644
index dd562a9..0000000
--- a/core/src/test/java/org/elasticsearch/action/fieldstats/FieldStatsRequestTest.java
+++ /dev/null
@@ -1,72 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.fieldstats;
-
-import org.elasticsearch.common.bytes.BytesArray;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.StreamsUtils;
-
-import static org.elasticsearch.action.fieldstats.IndexConstraint.Comparison.*;
-import static org.elasticsearch.action.fieldstats.IndexConstraint.Property.MAX;
-import static org.elasticsearch.action.fieldstats.IndexConstraint.Property.MIN;
-import static org.hamcrest.Matchers.equalTo;
-
-public class FieldStatsRequestTest extends ESTestCase {
-
-    public void testFieldsParsing() throws Exception {
-        byte[] data = StreamsUtils.copyToBytesFromClasspath("/org/elasticsearch/action/fieldstats/fieldstats-index-constraints-request.json");
-        FieldStatsRequest request = new FieldStatsRequest();
-        request.source(new BytesArray(data));
-
-        assertThat(request.getFields().length, equalTo(5));
-        assertThat(request.getFields()[0], equalTo("field1"));
-        assertThat(request.getFields()[1], equalTo("field2"));
-        assertThat(request.getFields()[2], equalTo("field3"));
-        assertThat(request.getFields()[3], equalTo("field4"));
-        assertThat(request.getFields()[4], equalTo("field5"));
-
-        assertThat(request.getIndexConstraints().length, equalTo(6));
-        assertThat(request.getIndexConstraints()[0].getField(), equalTo("field2"));
-        assertThat(request.getIndexConstraints()[0].getValue(), equalTo("9"));
-        assertThat(request.getIndexConstraints()[0].getProperty(), equalTo(MAX));
-        assertThat(request.getIndexConstraints()[0].getComparison(), equalTo(GTE));
-        assertThat(request.getIndexConstraints()[1].getField(), equalTo("field3"));
-        assertThat(request.getIndexConstraints()[1].getValue(), equalTo("5"));
-        assertThat(request.getIndexConstraints()[1].getProperty(), equalTo(MIN));
-        assertThat(request.getIndexConstraints()[1].getComparison(), equalTo(GT));
-        assertThat(request.getIndexConstraints()[2].getField(), equalTo("field4"));
-        assertThat(request.getIndexConstraints()[2].getValue(), equalTo("a"));
-        assertThat(request.getIndexConstraints()[2].getProperty(), equalTo(MIN));
-        assertThat(request.getIndexConstraints()[2].getComparison(), equalTo(GTE));
-        assertThat(request.getIndexConstraints()[3].getField(), equalTo("field4"));
-        assertThat(request.getIndexConstraints()[3].getValue(), equalTo("g"));
-        assertThat(request.getIndexConstraints()[3].getProperty(), equalTo(MAX));
-        assertThat(request.getIndexConstraints()[3].getComparison(), equalTo(LTE));
-        assertThat(request.getIndexConstraints()[4].getField(), equalTo("field5"));
-        assertThat(request.getIndexConstraints()[4].getValue(), equalTo("2"));
-        assertThat(request.getIndexConstraints()[4].getProperty(), equalTo(MAX));
-        assertThat(request.getIndexConstraints()[4].getComparison(), equalTo(GT));
-        assertThat(request.getIndexConstraints()[5].getField(), equalTo("field5"));
-        assertThat(request.getIndexConstraints()[5].getValue(), equalTo("9"));
-        assertThat(request.getIndexConstraints()[5].getProperty(), equalTo(MAX));
-        assertThat(request.getIndexConstraints()[5].getComparison(), equalTo(LT));
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/action/fieldstats/FieldStatsRequestTests.java b/core/src/test/java/org/elasticsearch/action/fieldstats/FieldStatsRequestTests.java
new file mode 100644
index 0000000..e33fba6
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/action/fieldstats/FieldStatsRequestTests.java
@@ -0,0 +1,72 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.fieldstats;
+
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.StreamsUtils;
+
+import static org.elasticsearch.action.fieldstats.IndexConstraint.Comparison.*;
+import static org.elasticsearch.action.fieldstats.IndexConstraint.Property.MAX;
+import static org.elasticsearch.action.fieldstats.IndexConstraint.Property.MIN;
+import static org.hamcrest.Matchers.equalTo;
+
+public class FieldStatsRequestTests extends ESTestCase {
+
+    public void testFieldsParsing() throws Exception {
+        byte[] data = StreamsUtils.copyToBytesFromClasspath("/org/elasticsearch/action/fieldstats/fieldstats-index-constraints-request.json");
+        FieldStatsRequest request = new FieldStatsRequest();
+        request.source(new BytesArray(data));
+
+        assertThat(request.getFields().length, equalTo(5));
+        assertThat(request.getFields()[0], equalTo("field1"));
+        assertThat(request.getFields()[1], equalTo("field2"));
+        assertThat(request.getFields()[2], equalTo("field3"));
+        assertThat(request.getFields()[3], equalTo("field4"));
+        assertThat(request.getFields()[4], equalTo("field5"));
+
+        assertThat(request.getIndexConstraints().length, equalTo(6));
+        assertThat(request.getIndexConstraints()[0].getField(), equalTo("field2"));
+        assertThat(request.getIndexConstraints()[0].getValue(), equalTo("9"));
+        assertThat(request.getIndexConstraints()[0].getProperty(), equalTo(MAX));
+        assertThat(request.getIndexConstraints()[0].getComparison(), equalTo(GTE));
+        assertThat(request.getIndexConstraints()[1].getField(), equalTo("field3"));
+        assertThat(request.getIndexConstraints()[1].getValue(), equalTo("5"));
+        assertThat(request.getIndexConstraints()[1].getProperty(), equalTo(MIN));
+        assertThat(request.getIndexConstraints()[1].getComparison(), equalTo(GT));
+        assertThat(request.getIndexConstraints()[2].getField(), equalTo("field4"));
+        assertThat(request.getIndexConstraints()[2].getValue(), equalTo("a"));
+        assertThat(request.getIndexConstraints()[2].getProperty(), equalTo(MIN));
+        assertThat(request.getIndexConstraints()[2].getComparison(), equalTo(GTE));
+        assertThat(request.getIndexConstraints()[3].getField(), equalTo("field4"));
+        assertThat(request.getIndexConstraints()[3].getValue(), equalTo("g"));
+        assertThat(request.getIndexConstraints()[3].getProperty(), equalTo(MAX));
+        assertThat(request.getIndexConstraints()[3].getComparison(), equalTo(LTE));
+        assertThat(request.getIndexConstraints()[4].getField(), equalTo("field5"));
+        assertThat(request.getIndexConstraints()[4].getValue(), equalTo("2"));
+        assertThat(request.getIndexConstraints()[4].getProperty(), equalTo(MAX));
+        assertThat(request.getIndexConstraints()[4].getComparison(), equalTo(GT));
+        assertThat(request.getIndexConstraints()[5].getField(), equalTo("field5"));
+        assertThat(request.getIndexConstraints()[5].getValue(), equalTo("9"));
+        assertThat(request.getIndexConstraints()[5].getProperty(), equalTo(MAX));
+        assertThat(request.getIndexConstraints()[5].getComparison(), equalTo(LT));
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/action/index/IndexRequestBuilderTest.java b/core/src/test/java/org/elasticsearch/action/index/IndexRequestBuilderTest.java
deleted file mode 100644
index 8e3b8b2..0000000
--- a/core/src/test/java/org/elasticsearch/action/index/IndexRequestBuilderTest.java
+++ /dev/null
@@ -1,87 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.index;
-
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentHelper;
-import org.elasticsearch.common.xcontent.XContentType;
-import org.elasticsearch.rest.NoOpClient;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.io.ByteArrayOutputStream;
-import java.util.HashMap;
-import java.util.Map;
-
-public class IndexRequestBuilderTest extends ESTestCase {
-
-    private static final String EXPECTED_SOURCE = "{\"SomeKey\":\"SomeValue\"}";
-    private NoOpClient testClient;
-
-    @Override
-    @Before
-    public void setUp() throws Exception {
-        super.setUp();
-        this.testClient = new NoOpClient(getTestName());
-    }
-
-    @Override
-    @After
-    public void tearDown() throws Exception {
-        this.testClient.close();
-        super.tearDown();
-    }
-
-    /**
-     * test setting the source for the request with different available setters
-     */
-    @Test
-    public void testSetSource() throws Exception {
-        IndexRequestBuilder indexRequestBuilder = new IndexRequestBuilder(this.testClient, IndexAction.INSTANCE);
-        Map<String, String> source = new HashMap<>();
-        source.put("SomeKey", "SomeValue");
-        indexRequestBuilder.setSource(source);
-        assertEquals(EXPECTED_SOURCE, XContentHelper.convertToJson(indexRequestBuilder.request().source(), true));
-
-        indexRequestBuilder.setSource(source, XContentType.JSON);
-        assertEquals(EXPECTED_SOURCE, XContentHelper.convertToJson(indexRequestBuilder.request().source(), true));
-
-        indexRequestBuilder.setSource("SomeKey", "SomeValue");
-        assertEquals(EXPECTED_SOURCE, XContentHelper.convertToJson(indexRequestBuilder.request().source(), true));
-
-        // force the Object... setter
-        indexRequestBuilder.setSource((Object) "SomeKey", "SomeValue");
-        assertEquals(EXPECTED_SOURCE, XContentHelper.convertToJson(indexRequestBuilder.request().source(), true));
-
-        ByteArrayOutputStream docOut = new ByteArrayOutputStream();
-        XContentBuilder doc = XContentFactory.jsonBuilder(docOut).startObject().field("SomeKey", "SomeValue").endObject();
-        doc.close();
-        indexRequestBuilder.setSource(docOut.toByteArray());
-        assertEquals(EXPECTED_SOURCE, XContentHelper.convertToJson(indexRequestBuilder.request().source(), true));
-
-        doc = XContentFactory.jsonBuilder().startObject().field("SomeKey", "SomeValue").endObject();
-        doc.close();
-        indexRequestBuilder.setSource(doc);
-        assertEquals(EXPECTED_SOURCE, XContentHelper.convertToJson(indexRequestBuilder.request().source(), true));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/action/index/IndexRequestBuilderTests.java b/core/src/test/java/org/elasticsearch/action/index/IndexRequestBuilderTests.java
new file mode 100644
index 0000000..f9dc86b
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/action/index/IndexRequestBuilderTests.java
@@ -0,0 +1,87 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.index;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentHelper;
+import org.elasticsearch.common.xcontent.XContentType;
+import org.elasticsearch.rest.NoOpClient;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import java.io.ByteArrayOutputStream;
+import java.util.HashMap;
+import java.util.Map;
+
+public class IndexRequestBuilderTests extends ESTestCase {
+
+    private static final String EXPECTED_SOURCE = "{\"SomeKey\":\"SomeValue\"}";
+    private NoOpClient testClient;
+
+    @Override
+    @Before
+    public void setUp() throws Exception {
+        super.setUp();
+        this.testClient = new NoOpClient(getTestName());
+    }
+
+    @Override
+    @After
+    public void tearDown() throws Exception {
+        this.testClient.close();
+        super.tearDown();
+    }
+
+    /**
+     * test setting the source for the request with different available setters
+     */
+    @Test
+    public void testSetSource() throws Exception {
+        IndexRequestBuilder indexRequestBuilder = new IndexRequestBuilder(this.testClient, IndexAction.INSTANCE);
+        Map<String, String> source = new HashMap<>();
+        source.put("SomeKey", "SomeValue");
+        indexRequestBuilder.setSource(source);
+        assertEquals(EXPECTED_SOURCE, XContentHelper.convertToJson(indexRequestBuilder.request().source(), true));
+
+        indexRequestBuilder.setSource(source, XContentType.JSON);
+        assertEquals(EXPECTED_SOURCE, XContentHelper.convertToJson(indexRequestBuilder.request().source(), true));
+
+        indexRequestBuilder.setSource("SomeKey", "SomeValue");
+        assertEquals(EXPECTED_SOURCE, XContentHelper.convertToJson(indexRequestBuilder.request().source(), true));
+
+        // force the Object... setter
+        indexRequestBuilder.setSource((Object) "SomeKey", "SomeValue");
+        assertEquals(EXPECTED_SOURCE, XContentHelper.convertToJson(indexRequestBuilder.request().source(), true));
+
+        ByteArrayOutputStream docOut = new ByteArrayOutputStream();
+        XContentBuilder doc = XContentFactory.jsonBuilder(docOut).startObject().field("SomeKey", "SomeValue").endObject();
+        doc.close();
+        indexRequestBuilder.setSource(docOut.toByteArray());
+        assertEquals(EXPECTED_SOURCE, XContentHelper.convertToJson(indexRequestBuilder.request().source(), true));
+
+        doc = XContentFactory.jsonBuilder().startObject().field("SomeKey", "SomeValue").endObject();
+        doc.close();
+        indexRequestBuilder.setSource(doc);
+        assertEquals(EXPECTED_SOURCE, XContentHelper.convertToJson(indexRequestBuilder.request().source(), true));
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java b/core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java
index 80f4c45..8f8759e 100644
--- a/core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java
+++ b/core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java
@@ -151,7 +151,7 @@ public class IndexAliasesIT extends ESIntegTestCase {
         logger.info("--> making sure that filter was stored with alias [alias1] and filter [user:kimchy]");
         ClusterState clusterState = admin().cluster().prepareState().get().getState();
         IndexMetaData indexMd = clusterState.metaData().index("test");
-        assertThat(indexMd.aliases().get("alias1").filter().string(), equalTo("{\"term\":{\"user\":{\"value\":\"kimchy\",\"boost\":1.0}}}"));
+        assertThat(indexMd.aliases().get("alias1").filter().string(), equalTo("{\"term\":{\"user\":\"kimchy\"}}"));
 
     }
 
@@ -413,8 +413,8 @@ public class IndexAliasesIT extends ESIntegTestCase {
         assertThat(client().prepareCount("bars").setQuery(QueryBuilders.matchAllQuery()).get().getCount(), equalTo(1L));
     }
 
-
-
+    
+    
     @Test
     public void testDeleteAliases() throws Exception {
         logger.info("--> creating index [test1] and [test2]");
@@ -434,17 +434,17 @@ public class IndexAliasesIT extends ESIntegTestCase {
                 .addAlias("test2", "aliasToTests")
                 .addAlias("test2", "foos", termQuery("name", "foo"))
                 .addAlias("test2", "tests", termQuery("name", "test")));
-
-        String[] indices = {"test1", "test2"};
+        
+        String[] indices = {"test1", "test2"}; 
         String[] aliases = {"aliasToTest1", "foos", "bars", "tests", "aliasToTest2", "aliasToTests"};
-
+        
         admin().indices().prepareAliases().removeAlias(indices, aliases).get();
-
+        
         AliasesExistResponse response = admin().indices().prepareAliasesExist(aliases).get();
         assertThat(response.exists(), equalTo(false));
     }
 
-
+    
     @Test
     public void testWaitForAliasCreationMultipleShards() throws Exception {
         logger.info("--> creating index [test]");
@@ -532,16 +532,16 @@ public class IndexAliasesIT extends ESIntegTestCase {
 
         logger.info("--> verify that filter was updated");
         AliasMetaData aliasMetaData = ((AliasOrIndex.Alias) internalCluster().clusterService().state().metaData().getAliasAndIndexLookup().get("alias1")).getFirstAliasMetaData();
-        assertThat(aliasMetaData.getFilter().toString(), equalTo("{\"term\":{\"name\":{\"value\":\"bar\",\"boost\":1.0}}}"));
+        assertThat(aliasMetaData.getFilter().toString(), equalTo("{\"term\":{\"name\":\"bar\"}}"));
 
         logger.info("--> deleting alias1");
         stopWatch.start();
         assertAcked((admin().indices().prepareAliases().removeAlias("test", "alias1").setTimeout(timeout)));
         assertThat(stopWatch.stop().lastTaskTime().millis(), lessThan(timeout.millis()));
 
-
+        
     }
-
+    
     @Test(expected = AliasesNotFoundException.class)
     public void testIndicesRemoveNonExistingAliasResponds404() throws Exception {
         logger.info("--> creating index [test]");
diff --git a/core/src/test/java/org/elasticsearch/benchmark/stress/NodesStressTest.java b/core/src/test/java/org/elasticsearch/benchmark/stress/NodesStressTest.java
deleted file mode 100644
index 1f53234..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/stress/NodesStressTest.java
+++ /dev/null
@@ -1,282 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.stress;
-
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.node.Node;
-
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.CyclicBarrier;
-import java.util.concurrent.atomic.AtomicLong;
-
-import static org.elasticsearch.client.Requests.searchRequest;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.settings.Settings.Builder.EMPTY_SETTINGS;
-import static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-import static org.elasticsearch.search.builder.SearchSourceBuilder.searchSource;
-
-/**
- *
- */
-public class NodesStressTest {
-
-    private Node[] nodes;
-
-    private int numberOfNodes = 2;
-
-    private Client[] clients;
-
-    private AtomicLong idGenerator = new AtomicLong();
-
-    private int fieldNumLimit = 50;
-
-    private long searcherIterations = 10;
-    private Searcher[] searcherThreads = new Searcher[1];
-
-    private long indexIterations = 10;
-    private Indexer[] indexThreads = new Indexer[1];
-
-    private TimeValue sleepAfterDone = TimeValue.timeValueMillis(0);
-    private TimeValue sleepBeforeClose = TimeValue.timeValueMillis(0);
-
-    private CountDownLatch latch;
-    private CyclicBarrier barrier1;
-    private CyclicBarrier barrier2;
-
-    public NodesStressTest() {
-    }
-
-    public NodesStressTest numberOfNodes(int numberOfNodes) {
-        this.numberOfNodes = numberOfNodes;
-        return this;
-    }
-
-    public NodesStressTest fieldNumLimit(int fieldNumLimit) {
-        this.fieldNumLimit = fieldNumLimit;
-        return this;
-    }
-
-    public NodesStressTest searchIterations(int searchIterations) {
-        this.searcherIterations = searchIterations;
-        return this;
-    }
-
-    public NodesStressTest searcherThreads(int numberOfSearcherThreads) {
-        searcherThreads = new Searcher[numberOfSearcherThreads];
-        return this;
-    }
-
-    public NodesStressTest indexIterations(long indexIterations) {
-        this.indexIterations = indexIterations;
-        return this;
-    }
-
-    public NodesStressTest indexThreads(int numberOfWriterThreads) {
-        indexThreads = new Indexer[numberOfWriterThreads];
-        return this;
-    }
-
-    public NodesStressTest sleepAfterDone(TimeValue time) {
-        this.sleepAfterDone = time;
-        return this;
-    }
-
-    public NodesStressTest sleepBeforeClose(TimeValue time) {
-        this.sleepBeforeClose = time;
-        return this;
-    }
-
-    public NodesStressTest build(Settings settings) throws Exception {
-        settings = settingsBuilder()
-//                .put("index.refresh_interval", 1, TimeUnit.SECONDS)
-                .put(SETTING_NUMBER_OF_SHARDS, 5)
-                .put(SETTING_NUMBER_OF_REPLICAS, 1)
-                .put(settings)
-                .build();
-
-        nodes = new Node[numberOfNodes];
-        clients = new Client[numberOfNodes];
-        for (int i = 0; i < numberOfNodes; i++) {
-            nodes[i] = nodeBuilder().settings(settingsBuilder().put(settings).put("name", "node" + i)).node();
-            clients[i] = nodes[i].client();
-        }
-
-        for (int i = 0; i < searcherThreads.length; i++) {
-            searcherThreads[i] = new Searcher(i);
-        }
-        for (int i = 0; i < indexThreads.length; i++) {
-            indexThreads[i] = new Indexer(i);
-        }
-
-        latch = new CountDownLatch(1);
-        barrier1 = new CyclicBarrier(2);
-        barrier2 = new CyclicBarrier(2);
-        // warmup
-        StopWatch stopWatch = new StopWatch().start();
-        Indexer warmup = new Indexer(-1).max(10000);
-        warmup.start();
-        barrier1.await();
-        barrier2.await();
-        latch.await();
-        stopWatch.stop();
-        System.out.println("Done Warmup, took [" + stopWatch.totalTime() + "]");
-
-        latch = new CountDownLatch(searcherThreads.length + indexThreads.length);
-        barrier1 = new CyclicBarrier(searcherThreads.length + indexThreads.length + 1);
-        barrier2 = new CyclicBarrier(searcherThreads.length + indexThreads.length + 1);
-
-        return this;
-    }
-
-    public void start() throws Exception {
-        for (Thread t : searcherThreads) {
-            t.start();
-        }
-        for (Thread t : indexThreads) {
-            t.start();
-        }
-        barrier1.await();
-
-        StopWatch stopWatch = new StopWatch();
-        stopWatch.start();
-
-        barrier2.await();
-
-        latch.await();
-        stopWatch.stop();
-
-        System.out.println("Done, took [" + stopWatch.totalTime() + "]");
-        System.out.println("Sleeping before close: " + sleepBeforeClose);
-        Thread.sleep(sleepBeforeClose.millis());
-
-        for (Client client : clients) {
-            client.close();
-        }
-        for (Node node : nodes) {
-            node.close();
-        }
-
-        System.out.println("Sleeping before exit: " + sleepBeforeClose);
-        Thread.sleep(sleepAfterDone.millis());
-    }
-
-    class Searcher extends Thread {
-        final int id;
-        long counter = 0;
-        long max = searcherIterations;
-
-        Searcher(int id) {
-            super("Searcher" + id);
-            this.id = id;
-        }
-
-        @Override
-        public void run() {
-            try {
-                barrier1.await();
-                barrier2.await();
-                for (; counter < max; counter++) {
-                    Client client = client(counter);
-                    QueryBuilder query = termQuery("num", counter % fieldNumLimit);
-                    query = constantScoreQuery(query);
-
-                    SearchResponse search = client.search(searchRequest()
-                            .source(searchSource().query(query)))
-                            .actionGet();
-//                    System.out.println("Got search response, hits [" + search.hits().totalHits() + "]");
-                }
-            } catch (Exception e) {
-                System.err.println("Failed to search:");
-                e.printStackTrace();
-            } finally {
-                latch.countDown();
-            }
-        }
-    }
-
-    class Indexer extends Thread {
-
-        final int id;
-        long counter = 0;
-        long max = indexIterations;
-
-        Indexer(int id) {
-            super("Indexer" + id);
-            this.id = id;
-        }
-
-        Indexer max(int max) {
-            this.max = max;
-            return this;
-        }
-
-        @Override
-        public void run() {
-            try {
-                barrier1.await();
-                barrier2.await();
-                for (; counter < max; counter++) {
-                    Client client = client(counter);
-                    long id = idGenerator.incrementAndGet();
-                    client.index(Requests.indexRequest().index("test").type("type1").id(Long.toString(id))
-                            .source(XContentFactory.jsonBuilder().startObject()
-                                    .field("num", id % fieldNumLimit)
-                                    .endObject()))
-                            .actionGet();
-                }
-                System.out.println("Indexer [" + id + "]: Done");
-            } catch (Exception e) {
-                System.err.println("Failed to index:");
-                e.printStackTrace();
-            } finally {
-                latch.countDown();
-            }
-        }
-    }
-
-    private Client client(long i) {
-        return clients[((int) (i % clients.length))];
-    }
-
-    public static void main(String[] args) throws Exception {
-        NodesStressTest test = new NodesStressTest()
-                .numberOfNodes(2)
-                .indexThreads(5)
-                .indexIterations(10 * 1000)
-                .searcherThreads(5)
-                .searchIterations(10 * 1000)
-                .sleepBeforeClose(TimeValue.timeValueMinutes(10))
-                .sleepAfterDone(TimeValue.timeValueMinutes(10))
-                .build(EMPTY_SETTINGS);
-
-        test.start();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/stress/SingleThreadBulkStress.java b/core/src/test/java/org/elasticsearch/benchmark/stress/SingleThreadBulkStress.java
deleted file mode 100644
index 32c35cc..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/stress/SingleThreadBulkStress.java
+++ /dev/null
@@ -1,123 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.stress;
-
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.node.Node;
-
-import java.io.IOException;
-import java.util.Random;
-
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-public class SingleThreadBulkStress {
-
-    public static void main(String[] args) throws Exception {
-        Random random = new Random();
-
-        int shardsCount = Integer.parseInt(System.getProperty("es.shards", "1"));
-        int replicaCount = Integer.parseInt(System.getProperty("es.replica", "1"));
-        boolean autoGenerateId = true;
-
-        Settings settings = settingsBuilder()
-                .put("index.refresh_interval", "1s")
-                .put("index.merge.async", true)
-                .put("index.translog.flush_threshold_ops", 5000)
-                .put(SETTING_NUMBER_OF_SHARDS, shardsCount)
-                .put(SETTING_NUMBER_OF_REPLICAS, replicaCount)
-                .build();
-
-        Node[] nodes = new Node[1];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = nodeBuilder().settings(settingsBuilder().put(settings).put("name", "node" + i)).node();
-        }
-
-        //Node client = nodeBuilder().settings(settingsBuilder().put(settings).put("name", "client")).client(true).node();
-        Node client = nodes[0];
-
-        Client client1 = client.client();
-
-        Thread.sleep(1000);
-        client1.admin().indices().prepareCreate("test").setSettings(settings).addMapping("type1", XContentFactory.jsonBuilder().startObject().startObject("type1")
-                .startObject("_source").field("enabled", false).endObject()
-                .startObject("_all").field("enabled", false).endObject()
-                .startObject("_type").field("index", "no").endObject()
-                .startObject("_id").field("index", "no").endObject()
-                .startObject("properties")
-                .startObject("field").field("type", "string").field("index", "not_analyzed").field("omit_norms", true).endObject()
-//                .startObject("field").field("index", "analyzed").field("omit_norms", false).endObject()
-                .endObject()
-                .endObject().endObject()).execute().actionGet();
-        Thread.sleep(5000);
-
-        StopWatch stopWatch = new StopWatch().start();
-        long COUNT = SizeValue.parseSizeValue("2m").singles();
-        int BATCH = 500;
-        System.out.println("Indexing [" + COUNT + "] ...");
-        long ITERS = COUNT / BATCH;
-        long i = 1;
-        int counter = 0;
-        for (; i <= ITERS; i++) {
-            BulkRequestBuilder request = client1.prepareBulk();
-            for (int j = 0; j < BATCH; j++) {
-                counter++;
-                request.add(Requests.indexRequest("test").type("type1").id(autoGenerateId ? null : Integer.toString(counter)).source(source(Integer.toString(counter), "test" + counter)));
-            }
-            BulkResponse response = request.execute().actionGet();
-            if (response.hasFailures()) {
-                System.err.println("failures...");
-            }
-            if (((i * BATCH) % 10000) == 0) {
-                System.out.println("Indexed " + (i * BATCH) + " took " + stopWatch.stop().lastTaskTime());
-                stopWatch.start();
-            }
-        }
-        System.out.println("Indexing took " + stopWatch.totalTime() + ", TPS " + (((double) COUNT) / stopWatch.totalTime().secondsFrac()));
-
-        client.client().admin().indices().prepareRefresh().execute().actionGet();
-        System.out.println("Count: " + client.client().prepareCount().setQuery(matchAllQuery()).execute().actionGet().getCount());
-
-        client.close();
-
-        for (Node node : nodes) {
-            node.close();
-        }
-    }
-
-    private static XContentBuilder source(String id, String nameValue) throws IOException {
-        return jsonBuilder().startObject().field("field", nameValue).endObject();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/stress/SingleThreadIndexingStress.java b/core/src/test/java/org/elasticsearch/benchmark/stress/SingleThreadIndexingStress.java
deleted file mode 100644
index 610745c..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/stress/SingleThreadIndexingStress.java
+++ /dev/null
@@ -1,108 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.stress;
-
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.node.Node;
-
-import java.io.IOException;
-
-import static org.elasticsearch.client.Requests.createIndexRequest;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-public class SingleThreadIndexingStress {
-
-    public static void main(String[] args) throws Exception {
-        Settings settings = settingsBuilder()
-                .put("index.refresh_interval", "1s")
-                .put("index.merge.async", true)
-                .put("index.translog.flush_threshold_ops", 5000)
-                .put(SETTING_NUMBER_OF_SHARDS, 2)
-                .put(SETTING_NUMBER_OF_REPLICAS, 1)
-                .build();
-
-        Node[] nodes = new Node[1];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = nodeBuilder().settings(settingsBuilder().put(settings).put("name", "node" + i)).node();
-        }
-
-        Node client = nodeBuilder().settings(settingsBuilder().put(settings).put("name", "client")).client(true).node();
-
-        Client client1 = client.client();
-
-        Thread.sleep(1000);
-        client1.admin().indices().create(createIndexRequest("test")).actionGet();
-        Thread.sleep(5000);
-
-        StopWatch stopWatch = new StopWatch().start();
-        int COUNT = 200000;
-        int ID_RANGE = 100;
-        System.out.println("Indexing [" + COUNT + "] ...");
-        int i = 1;
-        for (; i <= COUNT; i++) {
-//            client1.admin().cluster().preparePingSingle("test", "type1", Integer.toString(i)).execute().actionGet();
-            client1.prepareIndex("test", "type1").setId(Integer.toString(i % ID_RANGE)).setSource(source(Integer.toString(i), "test" + i))
-                    .setCreate(false).execute().actionGet();
-            if ((i % 10000) == 0) {
-                System.out.println("Indexed " + i + " took " + stopWatch.stop().lastTaskTime());
-                stopWatch.start();
-            }
-        }
-        System.out.println("Indexing took " + stopWatch.totalTime() + ", TPS " + (((double) COUNT) / stopWatch.totalTime().secondsFrac()));
-
-        client.client().admin().indices().prepareRefresh().execute().actionGet();
-        System.out.println("Count: " + client.client().prepareCount().setQuery(matchAllQuery()).execute().actionGet().getCount());
-
-        client.close();
-
-        for (Node node : nodes) {
-            node.close();
-        }
-    }
-
-    private static XContentBuilder source(String id, String nameValue) throws IOException {
-        long time = System.currentTimeMillis();
-        return jsonBuilder().startObject()
-                .field("id", id)
-//                .field("numeric1", time)
-//                .field("numeric2", time)
-//                .field("numeric3", time)
-//                .field("numeric4", time)
-//                .field("numeric5", time)
-//                .field("numeric6", time)
-//                .field("numeric7", time)
-//                .field("numeric8", time)
-//                .field("numeric9", time)
-//                .field("numeric10", time)
-                .field("name", nameValue)
-                .endObject();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/client/AbstractClientHeadersTestCase.java b/core/src/test/java/org/elasticsearch/client/AbstractClientHeadersTestCase.java
new file mode 100644
index 0000000..964a47c
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/client/AbstractClientHeadersTestCase.java
@@ -0,0 +1,241 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.client;
+
+import com.google.common.base.Throwables;
+import com.google.common.collect.ImmutableMap;
+import org.elasticsearch.action.ActionListener;
+import org.elasticsearch.action.GenericAction;
+import org.elasticsearch.action.admin.cluster.reroute.ClusterRerouteAction;
+import org.elasticsearch.action.admin.cluster.reroute.ClusterRerouteResponse;
+import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotAction;
+import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;
+import org.elasticsearch.action.admin.cluster.stats.ClusterStatsAction;
+import org.elasticsearch.action.admin.cluster.stats.ClusterStatsResponse;
+import org.elasticsearch.action.admin.indices.cache.clear.ClearIndicesCacheAction;
+import org.elasticsearch.action.admin.indices.cache.clear.ClearIndicesCacheResponse;
+import org.elasticsearch.action.admin.indices.create.CreateIndexAction;
+import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;
+import org.elasticsearch.action.admin.indices.flush.FlushAction;
+import org.elasticsearch.action.admin.indices.flush.FlushResponse;
+import org.elasticsearch.action.admin.indices.stats.IndicesStatsAction;
+import org.elasticsearch.action.admin.indices.stats.IndicesStatsResponse;
+import org.elasticsearch.action.delete.DeleteAction;
+import org.elasticsearch.action.delete.DeleteResponse;
+import org.elasticsearch.action.get.GetAction;
+import org.elasticsearch.action.get.GetResponse;
+import org.elasticsearch.action.index.IndexAction;
+import org.elasticsearch.action.index.IndexResponse;
+import org.elasticsearch.action.indexedscripts.delete.DeleteIndexedScriptAction;
+import org.elasticsearch.action.indexedscripts.delete.DeleteIndexedScriptResponse;
+import org.elasticsearch.action.search.SearchAction;
+import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.client.support.Headers;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.elasticsearch.transport.TransportMessage;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.Matchers.*;
+
+/**
+ *
+ */
+public abstract class AbstractClientHeadersTestCase extends ESTestCase {
+
+    protected static final Settings HEADER_SETTINGS = Settings.builder()
+            .put(Headers.PREFIX + ".key1", "val1")
+            .put(Headers.PREFIX + ".key2", "val 2")
+            .build();
+
+    @SuppressWarnings("unchecked")
+    private static final GenericAction[] ACTIONS = new GenericAction[] {
+                // client actions
+                GetAction.INSTANCE, SearchAction.INSTANCE, DeleteAction.INSTANCE, DeleteIndexedScriptAction.INSTANCE,
+                IndexAction.INSTANCE,
+
+                // cluster admin actions
+                ClusterStatsAction.INSTANCE, CreateSnapshotAction.INSTANCE, ClusterRerouteAction.INSTANCE,
+
+                // indices admin actions
+                CreateIndexAction.INSTANCE, IndicesStatsAction.INSTANCE, ClearIndicesCacheAction.INSTANCE, FlushAction.INSTANCE
+    };
+
+    protected ThreadPool threadPool;
+    private Client client;
+
+    @Before
+    public void initClient() {
+        Settings settings = Settings.builder()
+                .put(HEADER_SETTINGS)
+                .put("path.home", createTempDir().toString())
+                .build();
+        threadPool = new ThreadPool("test-" + getTestName());
+        client = buildClient(settings, ACTIONS);
+    }
+
+    @After
+    public void cleanupClient() throws Exception {
+        client.close();
+        terminate(threadPool);
+    }
+
+    protected abstract Client buildClient(Settings headersSettings, GenericAction[] testedActions);
+
+
+    @Test
+    public void testActions() {
+
+        // TODO this is a really shitty way to test it, we need to figure out a way to test all the client methods
+        //      without specifying each one (reflection doesn't as each action needs its own special settings, without
+        //      them, request validation will fail before the test is executed. (one option is to enable disabling the
+        //      validation in the settings??? - ugly and conceptually wrong)
+
+        // choosing arbitrary top level actions to test
+        client.prepareGet("idx", "type", "id").execute().addListener(new AssertingActionListener<GetResponse>(GetAction.NAME));
+        client.prepareSearch().execute().addListener(new AssertingActionListener<SearchResponse>(SearchAction.NAME));
+        client.prepareDelete("idx", "type", "id").execute().addListener(new AssertingActionListener<DeleteResponse>(DeleteAction.NAME));
+        client.prepareDeleteIndexedScript("lang", "id").execute().addListener(new AssertingActionListener<DeleteIndexedScriptResponse>(DeleteIndexedScriptAction.NAME));
+        client.prepareIndex("idx", "type", "id").setSource("source").execute().addListener(new AssertingActionListener<IndexResponse>(IndexAction.NAME));
+
+        // choosing arbitrary cluster admin actions to test
+        client.admin().cluster().prepareClusterStats().execute().addListener(new AssertingActionListener<ClusterStatsResponse>(ClusterStatsAction.NAME));
+        client.admin().cluster().prepareCreateSnapshot("repo", "bck").execute().addListener(new AssertingActionListener<CreateSnapshotResponse>(CreateSnapshotAction.NAME));
+        client.admin().cluster().prepareReroute().execute().addListener(new AssertingActionListener<ClusterRerouteResponse>(ClusterRerouteAction.NAME));
+
+        // choosing arbitrary indices admin actions to test
+        client.admin().indices().prepareCreate("idx").execute().addListener(new AssertingActionListener<CreateIndexResponse>(CreateIndexAction.NAME));
+        client.admin().indices().prepareStats().execute().addListener(new AssertingActionListener<IndicesStatsResponse>(IndicesStatsAction.NAME));
+        client.admin().indices().prepareClearCache("idx1", "idx2").execute().addListener(new AssertingActionListener<ClearIndicesCacheResponse>(ClearIndicesCacheAction.NAME));
+        client.admin().indices().prepareFlush().execute().addListener(new AssertingActionListener<FlushResponse>(FlushAction.NAME));
+    }
+
+    @Test
+    public void testOverideHeader() throws Exception {
+        String key1Val = randomAsciiOfLength(5);
+        Map<String, Object> expected = ImmutableMap.<String, Object>builder()
+                .put("key1", key1Val)
+                .put("key2", "val 2")
+                .build();
+
+        client.prepareGet("idx", "type", "id")
+                .putHeader("key1", key1Val)
+                .execute().addListener(new AssertingActionListener<GetResponse>(GetAction.NAME, expected));
+
+        client.admin().cluster().prepareClusterStats()
+                .putHeader("key1", key1Val)
+                .execute().addListener(new AssertingActionListener<ClusterStatsResponse>(ClusterStatsAction.NAME, expected));
+
+        client.admin().indices().prepareCreate("idx")
+                .putHeader("key1", key1Val)
+                .execute().addListener(new AssertingActionListener<CreateIndexResponse>(CreateIndexAction.NAME, expected));
+    }
+
+    protected static void assertHeaders(Map<String, Object> headers, Map<String, Object> expected) {
+        assertThat(headers, notNullValue());
+        assertThat(headers.size(), is(expected.size()));
+        for (Map.Entry<String, Object> expectedEntry : expected.entrySet()) {
+            assertThat(headers.get(expectedEntry.getKey()), equalTo(expectedEntry.getValue()));
+        }
+    }
+
+    protected static void assertHeaders(TransportMessage<?> message) {
+        assertHeaders(message, HEADER_SETTINGS.getAsSettings(Headers.PREFIX).getAsStructuredMap());
+    }
+
+    protected static void assertHeaders(TransportMessage<?> message, Map<String, Object> expected) {
+        assertThat(message.getHeaders(), notNullValue());
+        assertThat(message.getHeaders().size(), is(expected.size()));
+        for (Map.Entry<String, Object> expectedEntry : expected.entrySet()) {
+            assertThat(message.getHeader(expectedEntry.getKey()), equalTo(expectedEntry.getValue()));
+        }
+    }
+
+    public static class InternalException extends Exception {
+
+        private final String action;
+        private final Map<String, Object> headers;
+
+        public InternalException(String action, TransportMessage<?> message) {
+            this.action = action;
+            this.headers = new HashMap<>();
+            for (String key : message.getHeaders()) {
+                headers.put(key, message.getHeader(key));
+            }
+        }
+    }
+
+    protected static class AssertingActionListener<T> implements ActionListener<T> {
+
+        private final String action;
+        private final Map<String, Object> expectedHeaders;
+
+        public AssertingActionListener(String action) {
+            this(action, HEADER_SETTINGS.getAsSettings(Headers.PREFIX).getAsStructuredMap());
+        }
+
+       public AssertingActionListener(String action, Map<String, Object> expectedHeaders) {
+            this.action = action;
+            this.expectedHeaders = expectedHeaders;
+        }
+
+        @Override
+        public void onResponse(T t) {
+            fail("an internal exception was expected for action [" + action + "]");
+        }
+
+        @Override
+        public void onFailure(Throwable t) {
+            Throwable e = unwrap(t, InternalException.class);
+            assertThat("expected action [" + action + "] to throw an internal exception", e, notNullValue());
+            assertThat(action, equalTo(((InternalException) e).action));
+            Map<String, Object> headers = ((InternalException) e).headers;
+            assertHeaders(headers, expectedHeaders);
+        }
+
+        public Throwable unwrap(Throwable t, Class<? extends Throwable> exceptionType) {
+            int counter = 0;
+            Throwable result = t;
+            while (!exceptionType.isInstance(result)) {
+                if (result.getCause() == null) {
+                    return null;
+                }
+                if (result.getCause() == result) {
+                    return null;
+                }
+                if (counter++ > 10) {
+                    // dear god, if we got more than 10 levels down, WTF? just bail
+                    fail("Exception cause unwrapping ran for 10 levels: " + Throwables.getStackTraceAsString(t));
+                    return null;
+                }
+                result = result.getCause();
+            }
+            return result;
+        }
+
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/client/AbstractClientHeadersTests.java b/core/src/test/java/org/elasticsearch/client/AbstractClientHeadersTests.java
deleted file mode 100644
index ab4c35f..0000000
--- a/core/src/test/java/org/elasticsearch/client/AbstractClientHeadersTests.java
+++ /dev/null
@@ -1,241 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.client;
-
-import com.google.common.base.Throwables;
-import com.google.common.collect.ImmutableMap;
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.GenericAction;
-import org.elasticsearch.action.admin.cluster.reroute.ClusterRerouteAction;
-import org.elasticsearch.action.admin.cluster.reroute.ClusterRerouteResponse;
-import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotAction;
-import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;
-import org.elasticsearch.action.admin.cluster.stats.ClusterStatsAction;
-import org.elasticsearch.action.admin.cluster.stats.ClusterStatsResponse;
-import org.elasticsearch.action.admin.indices.cache.clear.ClearIndicesCacheAction;
-import org.elasticsearch.action.admin.indices.cache.clear.ClearIndicesCacheResponse;
-import org.elasticsearch.action.admin.indices.create.CreateIndexAction;
-import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;
-import org.elasticsearch.action.admin.indices.flush.FlushAction;
-import org.elasticsearch.action.admin.indices.flush.FlushResponse;
-import org.elasticsearch.action.admin.indices.stats.IndicesStatsAction;
-import org.elasticsearch.action.admin.indices.stats.IndicesStatsResponse;
-import org.elasticsearch.action.delete.DeleteAction;
-import org.elasticsearch.action.delete.DeleteResponse;
-import org.elasticsearch.action.get.GetAction;
-import org.elasticsearch.action.get.GetResponse;
-import org.elasticsearch.action.index.IndexAction;
-import org.elasticsearch.action.index.IndexResponse;
-import org.elasticsearch.action.indexedscripts.delete.DeleteIndexedScriptAction;
-import org.elasticsearch.action.indexedscripts.delete.DeleteIndexedScriptResponse;
-import org.elasticsearch.action.search.SearchAction;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.support.Headers;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportMessage;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.*;
-
-/**
- *
- */
-public abstract class AbstractClientHeadersTests extends ESTestCase {
-
-    protected static final Settings HEADER_SETTINGS = Settings.builder()
-            .put(Headers.PREFIX + ".key1", "val1")
-            .put(Headers.PREFIX + ".key2", "val 2")
-            .build();
-
-    @SuppressWarnings("unchecked")
-    private static final GenericAction[] ACTIONS = new GenericAction[] {
-                // client actions
-                GetAction.INSTANCE, SearchAction.INSTANCE, DeleteAction.INSTANCE, DeleteIndexedScriptAction.INSTANCE,
-                IndexAction.INSTANCE,
-
-                // cluster admin actions
-                ClusterStatsAction.INSTANCE, CreateSnapshotAction.INSTANCE, ClusterRerouteAction.INSTANCE,
-
-                // indices admin actions
-                CreateIndexAction.INSTANCE, IndicesStatsAction.INSTANCE, ClearIndicesCacheAction.INSTANCE, FlushAction.INSTANCE
-    };
-
-    protected ThreadPool threadPool;
-    private Client client;
-
-    @Before
-    public void initClient() {
-        Settings settings = Settings.builder()
-                .put(HEADER_SETTINGS)
-                .put("path.home", createTempDir().toString())
-                .build();
-        threadPool = new ThreadPool("test-" + getTestName());
-        client = buildClient(settings, ACTIONS);
-    }
-
-    @After
-    public void cleanupClient() throws Exception {
-        client.close();
-        terminate(threadPool);
-    }
-
-    protected abstract Client buildClient(Settings headersSettings, GenericAction[] testedActions);
-
-
-    @Test
-    public void testActions() {
-
-        // TODO this is a really shitty way to test it, we need to figure out a way to test all the client methods
-        //      without specifying each one (reflection doesn't as each action needs its own special settings, without
-        //      them, request validation will fail before the test is executed. (one option is to enable disabling the
-        //      validation in the settings??? - ugly and conceptually wrong)
-
-        // choosing arbitrary top level actions to test
-        client.prepareGet("idx", "type", "id").execute().addListener(new AssertingActionListener<GetResponse>(GetAction.NAME));
-        client.prepareSearch().execute().addListener(new AssertingActionListener<SearchResponse>(SearchAction.NAME));
-        client.prepareDelete("idx", "type", "id").execute().addListener(new AssertingActionListener<DeleteResponse>(DeleteAction.NAME));
-        client.prepareDeleteIndexedScript("lang", "id").execute().addListener(new AssertingActionListener<DeleteIndexedScriptResponse>(DeleteIndexedScriptAction.NAME));
-        client.prepareIndex("idx", "type", "id").setSource("source").execute().addListener(new AssertingActionListener<IndexResponse>(IndexAction.NAME));
-
-        // choosing arbitrary cluster admin actions to test
-        client.admin().cluster().prepareClusterStats().execute().addListener(new AssertingActionListener<ClusterStatsResponse>(ClusterStatsAction.NAME));
-        client.admin().cluster().prepareCreateSnapshot("repo", "bck").execute().addListener(new AssertingActionListener<CreateSnapshotResponse>(CreateSnapshotAction.NAME));
-        client.admin().cluster().prepareReroute().execute().addListener(new AssertingActionListener<ClusterRerouteResponse>(ClusterRerouteAction.NAME));
-
-        // choosing arbitrary indices admin actions to test
-        client.admin().indices().prepareCreate("idx").execute().addListener(new AssertingActionListener<CreateIndexResponse>(CreateIndexAction.NAME));
-        client.admin().indices().prepareStats().execute().addListener(new AssertingActionListener<IndicesStatsResponse>(IndicesStatsAction.NAME));
-        client.admin().indices().prepareClearCache("idx1", "idx2").execute().addListener(new AssertingActionListener<ClearIndicesCacheResponse>(ClearIndicesCacheAction.NAME));
-        client.admin().indices().prepareFlush().execute().addListener(new AssertingActionListener<FlushResponse>(FlushAction.NAME));
-    }
-
-    @Test
-    public void testOverideHeader() throws Exception {
-        String key1Val = randomAsciiOfLength(5);
-        Map<String, Object> expected = ImmutableMap.<String, Object>builder()
-                .put("key1", key1Val)
-                .put("key2", "val 2")
-                .build();
-
-        client.prepareGet("idx", "type", "id")
-                .putHeader("key1", key1Val)
-                .execute().addListener(new AssertingActionListener<GetResponse>(GetAction.NAME, expected));
-
-        client.admin().cluster().prepareClusterStats()
-                .putHeader("key1", key1Val)
-                .execute().addListener(new AssertingActionListener<ClusterStatsResponse>(ClusterStatsAction.NAME, expected));
-
-        client.admin().indices().prepareCreate("idx")
-                .putHeader("key1", key1Val)
-                .execute().addListener(new AssertingActionListener<CreateIndexResponse>(CreateIndexAction.NAME, expected));
-    }
-
-    protected static void assertHeaders(Map<String, Object> headers, Map<String, Object> expected) {
-        assertThat(headers, notNullValue());
-        assertThat(headers.size(), is(expected.size()));
-        for (Map.Entry<String, Object> expectedEntry : expected.entrySet()) {
-            assertThat(headers.get(expectedEntry.getKey()), equalTo(expectedEntry.getValue()));
-        }
-    }
-
-    protected static void assertHeaders(TransportMessage<?> message) {
-        assertHeaders(message, HEADER_SETTINGS.getAsSettings(Headers.PREFIX).getAsStructuredMap());
-    }
-
-    protected static void assertHeaders(TransportMessage<?> message, Map<String, Object> expected) {
-        assertThat(message.getHeaders(), notNullValue());
-        assertThat(message.getHeaders().size(), is(expected.size()));
-        for (Map.Entry<String, Object> expectedEntry : expected.entrySet()) {
-            assertThat(message.getHeader(expectedEntry.getKey()), equalTo(expectedEntry.getValue()));
-        }
-    }
-
-    public static class InternalException extends Exception {
-
-        private final String action;
-        private final Map<String, Object> headers;
-
-        public InternalException(String action, TransportMessage<?> message) {
-            this.action = action;
-            this.headers = new HashMap<>();
-            for (String key : message.getHeaders()) {
-                headers.put(key, message.getHeader(key));
-            }
-        }
-    }
-
-    protected static class AssertingActionListener<T> implements ActionListener<T> {
-
-        private final String action;
-        private final Map<String, Object> expectedHeaders;
-
-        public AssertingActionListener(String action) {
-            this(action, HEADER_SETTINGS.getAsSettings(Headers.PREFIX).getAsStructuredMap());
-        }
-
-       public AssertingActionListener(String action, Map<String, Object> expectedHeaders) {
-            this.action = action;
-            this.expectedHeaders = expectedHeaders;
-        }
-
-        @Override
-        public void onResponse(T t) {
-            fail("an internal exception was expected for action [" + action + "]");
-        }
-
-        @Override
-        public void onFailure(Throwable t) {
-            Throwable e = unwrap(t, InternalException.class);
-            assertThat("expected action [" + action + "] to throw an internal exception", e, notNullValue());
-            assertThat(action, equalTo(((InternalException) e).action));
-            Map<String, Object> headers = ((InternalException) e).headers;
-            assertHeaders(headers, expectedHeaders);
-        }
-
-        public Throwable unwrap(Throwable t, Class<? extends Throwable> exceptionType) {
-            int counter = 0;
-            Throwable result = t;
-            while (!exceptionType.isInstance(result)) {
-                if (result.getCause() == null) {
-                    return null;
-                }
-                if (result.getCause() == result) {
-                    return null;
-                }
-                if (counter++ > 10) {
-                    // dear god, if we got more than 10 levels down, WTF? just bail
-                    fail("Exception cause unwrapping ran for 10 levels: " + Throwables.getStackTraceAsString(t));
-                    return null;
-                }
-                result = result.getCause();
-            }
-            return result;
-        }
-
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/client/node/NodeClientHeadersTests.java b/core/src/test/java/org/elasticsearch/client/node/NodeClientHeadersTests.java
index 8293fbb..e93fbc8 100644
--- a/core/src/test/java/org/elasticsearch/client/node/NodeClientHeadersTests.java
+++ b/core/src/test/java/org/elasticsearch/client/node/NodeClientHeadersTests.java
@@ -25,7 +25,7 @@ import org.elasticsearch.action.GenericAction;
 import org.elasticsearch.action.support.ActionFilter;
 import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.TransportAction;
-import org.elasticsearch.client.AbstractClientHeadersTests;
+import org.elasticsearch.client.AbstractClientHeadersTestCase;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.support.Headers;
 import org.elasticsearch.common.settings.Settings;
@@ -37,7 +37,7 @@ import java.util.HashMap;
 /**
  *
  */
-public class NodeClientHeadersTests extends AbstractClientHeadersTests {
+public class NodeClientHeadersTests extends AbstractClientHeadersTestCase {
 
     private static final ActionFilters EMPTY_FILTERS = new ActionFilters(Collections.<ActionFilter>emptySet());
 
diff --git a/core/src/test/java/org/elasticsearch/client/transport/TransportClientHeadersTests.java b/core/src/test/java/org/elasticsearch/client/transport/TransportClientHeadersTests.java
index 631b2d1..22d5ba2 100644
--- a/core/src/test/java/org/elasticsearch/client/transport/TransportClientHeadersTests.java
+++ b/core/src/test/java/org/elasticsearch/client/transport/TransportClientHeadersTests.java
@@ -25,7 +25,7 @@ import org.elasticsearch.action.admin.cluster.node.liveness.LivenessResponse;
 import org.elasticsearch.action.admin.cluster.node.liveness.TransportLivenessAction;
 import org.elasticsearch.action.admin.cluster.state.ClusterStateAction;
 import org.elasticsearch.action.admin.cluster.state.ClusterStateResponse;
-import org.elasticsearch.client.AbstractClientHeadersTests;
+import org.elasticsearch.client.AbstractClientHeadersTestCase;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.cluster.ClusterState;
@@ -48,7 +48,6 @@ import org.elasticsearch.transport.TransportResponseHandler;
 import org.elasticsearch.transport.TransportService;
 import org.junit.Test;
 
-import java.util.Collection;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.TimeUnit;
 
@@ -58,7 +57,7 @@ import static org.hamcrest.Matchers.is;
 /**
  *
  */
-public class TransportClientHeadersTests extends AbstractClientHeadersTests {
+public class TransportClientHeadersTests extends AbstractClientHeadersTestCase {
 
     private static final LocalTransportAddress address = new LocalTransportAddress("test");
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeServiceTest.java b/core/src/test/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeServiceTest.java
deleted file mode 100644
index e83e68b..0000000
--- a/core/src/test/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeServiceTest.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.cluster.metadata;
-
-import com.carrotsearch.randomizedtesting.generators.RandomPicks;
-import org.elasticsearch.Version;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.store.IndexStoreModule;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Arrays;
-import java.util.Locale;
-
-public class MetaDataIndexUpgradeServiceTest extends ESTestCase {
-
-    public void testUpgradeStoreSettings() {
-        final String type = RandomPicks.randomFrom(random(), Arrays.asList("nio_fs", "mmap_fs", "simple_fs", "default", "fs"));
-        MetaDataIndexUpgradeService metaDataIndexUpgradeService = new MetaDataIndexUpgradeService(Settings.EMPTY, null);
-        Settings indexSettings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
-                .put(IndexStoreModule.STORE_TYPE, randomBoolean() ? type : type.toUpperCase(Locale.ROOT))
-                .build();
-        IndexMetaData test = IndexMetaData.builder("test")
-                .settings(indexSettings)
-                .numberOfShards(1)
-                .numberOfReplicas(1)
-                .build();
-        IndexMetaData indexMetaData = metaDataIndexUpgradeService.upgradeSettings(test);
-        assertEquals(type.replace("_", ""), indexMetaData.getSettings().get(IndexStoreModule.STORE_TYPE));
-    }
-
-    public void testNoStoreSetting() {
-        Settings indexSettings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
-                .build();
-        IndexMetaData test = IndexMetaData.builder("test")
-                .settings(indexSettings)
-                .numberOfShards(1)
-                .numberOfReplicas(1)
-                .build();
-        MetaDataIndexUpgradeService metaDataIndexUpgradeService = new MetaDataIndexUpgradeService(Settings.EMPTY, null);
-        IndexMetaData indexMetaData = metaDataIndexUpgradeService.upgradeSettings(test);
-        assertSame(indexMetaData, test);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeServiceTests.java b/core/src/test/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeServiceTests.java
new file mode 100644
index 0000000..88f27bc
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeServiceTests.java
@@ -0,0 +1,59 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.cluster.metadata;
+
+import com.carrotsearch.randomizedtesting.generators.RandomPicks;
+import org.elasticsearch.Version;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.store.IndexStoreModule;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.Arrays;
+import java.util.Locale;
+
+public class MetaDataIndexUpgradeServiceTests extends ESTestCase {
+
+    public void testUpgradeStoreSettings() {
+        final String type = RandomPicks.randomFrom(random(), Arrays.asList("nio_fs", "mmap_fs", "simple_fs", "default", "fs"));
+        MetaDataIndexUpgradeService metaDataIndexUpgradeService = new MetaDataIndexUpgradeService(Settings.EMPTY, null);
+        Settings indexSettings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
+                .put(IndexStoreModule.STORE_TYPE, randomBoolean() ? type : type.toUpperCase(Locale.ROOT))
+                .build();
+        IndexMetaData test = IndexMetaData.builder("test")
+                .settings(indexSettings)
+                .numberOfShards(1)
+                .numberOfReplicas(1)
+                .build();
+        IndexMetaData indexMetaData = metaDataIndexUpgradeService.upgradeSettings(test);
+        assertEquals(type.replace("_", ""), indexMetaData.getSettings().get(IndexStoreModule.STORE_TYPE));
+    }
+
+    public void testNoStoreSetting() {
+        Settings indexSettings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
+                .build();
+        IndexMetaData test = IndexMetaData.builder("test")
+                .settings(indexSettings)
+                .numberOfShards(1)
+                .numberOfReplicas(1)
+                .build();
+        MetaDataIndexUpgradeService metaDataIndexUpgradeService = new MetaDataIndexUpgradeService(Settings.EMPTY, null);
+        IndexMetaData indexMetaData = metaDataIndexUpgradeService.upgradeSettings(test);
+        assertSame(indexMetaData, test);
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingTableTest.java b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingTableTest.java
deleted file mode 100644
index 9a0cbb2..0000000
--- a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingTableTest.java
+++ /dev/null
@@ -1,267 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cluster.routing;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.cluster.node.DiscoveryNodes.Builder;
-import org.elasticsearch.cluster.routing.allocation.AllocationService;
-import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.IndexNotFoundException;
-import org.elasticsearch.test.ESAllocationTestCase;
-import org.junit.Before;
-import org.junit.Test;
-
-import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.nullValue;
-
-public class RoutingTableTest extends ESAllocationTestCase {
-
-    private static final String TEST_INDEX_1 = "test1";
-    private static final String TEST_INDEX_2 = "test2";
-    private RoutingTable emptyRoutingTable;
-    private RoutingTable testRoutingTable;
-    private int numberOfShards;
-    private int numberOfReplicas;
-    private int shardsPerIndex;
-    private int totalNumberOfShards;
-    private final static Settings DEFAULT_SETTINGS = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).build();
-    private final AllocationService ALLOCATION_SERVICE = createAllocationService(settingsBuilder()
-            .put("cluster.routing.allocation.concurrent_recoveries", 10)
-            .put("cluster.routing.allocation.node_initial_primaries_recoveries", 10)
-            .build());
-    private ClusterState clusterState;
-
-    @Before
-    public void setUp() throws Exception {
-        super.setUp();
-        this.numberOfShards = randomIntBetween(1, 5);
-        this.numberOfReplicas = randomIntBetween(1, 5);
-        this.shardsPerIndex = this.numberOfShards * (this.numberOfReplicas + 1);
-        this.totalNumberOfShards = this.shardsPerIndex * 2;
-        logger.info("Setup test with " + this.numberOfShards + " shards and " + this.numberOfReplicas + " replicas.");
-        this.emptyRoutingTable = new RoutingTable.Builder().build();
-        MetaData metaData = MetaData.builder()
-                .put(createIndexMetaData(TEST_INDEX_1))
-                .put(createIndexMetaData(TEST_INDEX_2))
-                .build();
-
-        this.testRoutingTable = new RoutingTable.Builder()
-                .add(new IndexRoutingTable.Builder(TEST_INDEX_1).initializeAsNew(metaData.index(TEST_INDEX_1)).build())
-                .add(new IndexRoutingTable.Builder(TEST_INDEX_2).initializeAsNew(metaData.index(TEST_INDEX_2)).build())
-                .build();
-        this.clusterState = ClusterState.builder(org.elasticsearch.cluster.ClusterName.DEFAULT).metaData(metaData).routingTable(testRoutingTable).build();
-    }
-
-    /**
-     * puts primary shard routings into initializing state
-     */
-    private void initPrimaries() {
-        logger.info("adding " + (this.numberOfReplicas + 1) + " nodes and performing rerouting");
-        Builder discoBuilder = DiscoveryNodes.builder();
-        for (int i = 0; i < this.numberOfReplicas + 1; i++) {
-            discoBuilder = discoBuilder.put(newNode("node" + i));
-        }
-        this.clusterState = ClusterState.builder(clusterState).nodes(discoBuilder).build();
-        RoutingAllocation.Result rerouteResult = ALLOCATION_SERVICE.reroute(clusterState);
-        this.testRoutingTable = rerouteResult.routingTable();
-        assertThat(rerouteResult.changed(), is(true));
-        this.clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
-    }
-
-    private void startInitializingShards(String index) {
-        this.clusterState = ClusterState.builder(clusterState).routingTable(this.testRoutingTable).build();
-        logger.info("start primary shards for index " + index);
-        RoutingAllocation.Result rerouteResult = ALLOCATION_SERVICE.applyStartedShards(this.clusterState, this.clusterState.getRoutingNodes().shardsWithState(index, INITIALIZING));
-        this.clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
-        this.testRoutingTable = rerouteResult.routingTable();
-    }
-
-    private IndexMetaData.Builder createIndexMetaData(String indexName) {
-        return new IndexMetaData.Builder(indexName)
-                .settings(DEFAULT_SETTINGS)
-                .numberOfReplicas(this.numberOfReplicas)
-                .numberOfShards(this.numberOfShards);
-    }
-
-    @Test
-    public void testAllShards() {
-        assertThat(this.emptyRoutingTable.allShards().size(), is(0));
-        assertThat(this.testRoutingTable.allShards().size(), is(this.totalNumberOfShards));
-
-        assertThat(this.testRoutingTable.allShards(TEST_INDEX_1).size(), is(this.shardsPerIndex));
-        try {
-            assertThat(this.testRoutingTable.allShards("not_existing").size(), is(0));
-            fail("Exception expected when calling allShards() with non existing index name");
-        } catch (IndexNotFoundException e) {
-            // expected
-        }
-    }
-
-    @Test
-    public void testHasIndex() {
-        assertThat(this.testRoutingTable.hasIndex(TEST_INDEX_1), is(true));
-        assertThat(this.testRoutingTable.hasIndex("foobar"), is(false));
-    }
-
-    @Test
-    public void testIndex() {
-        assertThat(this.testRoutingTable.index(TEST_INDEX_1).getIndex(), is(TEST_INDEX_1));
-        assertThat(this.testRoutingTable.index("foobar"), is(nullValue()));
-    }
-
-    @Test
-    public void testIndicesRouting() {
-        assertThat(this.testRoutingTable.indicesRouting().size(), is(2));
-        assertThat(this.testRoutingTable.getIndicesRouting().size(), is(2));
-        assertSame(this.testRoutingTable.getIndicesRouting(), this.testRoutingTable.indicesRouting());
-    }
-
-    @Test
-    public void testShardsWithState() {
-        assertThat(this.testRoutingTable.shardsWithState(ShardRoutingState.UNASSIGNED).size(), is(this.totalNumberOfShards));
-
-        initPrimaries();
-        assertThat(this.testRoutingTable.shardsWithState(ShardRoutingState.UNASSIGNED).size(), is(this.totalNumberOfShards - 2 * this.numberOfShards));
-        assertThat(this.testRoutingTable.shardsWithState(ShardRoutingState.INITIALIZING).size(), is(2 * this.numberOfShards));
-
-        startInitializingShards(TEST_INDEX_1);
-        assertThat(this.testRoutingTable.shardsWithState(ShardRoutingState.STARTED).size(), is(this.numberOfShards));
-        int initializingExpected = this.numberOfShards + this.numberOfShards * this.numberOfReplicas;
-        assertThat(this.testRoutingTable.shardsWithState(ShardRoutingState.INITIALIZING).size(), is(initializingExpected));
-        assertThat(this.testRoutingTable.shardsWithState(ShardRoutingState.UNASSIGNED).size(), is(this.totalNumberOfShards - initializingExpected - this.numberOfShards));
-
-        startInitializingShards(TEST_INDEX_2);
-        assertThat(this.testRoutingTable.shardsWithState(ShardRoutingState.STARTED).size(), is(2 * this.numberOfShards));
-        initializingExpected = 2 * this.numberOfShards * this.numberOfReplicas;
-        assertThat(this.testRoutingTable.shardsWithState(ShardRoutingState.INITIALIZING).size(), is(initializingExpected));
-        assertThat(this.testRoutingTable.shardsWithState(ShardRoutingState.UNASSIGNED).size(), is(this.totalNumberOfShards - initializingExpected - 2 * this.numberOfShards));
-
-        // now start all replicas too
-        startInitializingShards(TEST_INDEX_1);
-        startInitializingShards(TEST_INDEX_2);
-        assertThat(this.testRoutingTable.shardsWithState(ShardRoutingState.STARTED).size(), is(this.totalNumberOfShards));
-    }
-
-    @Test
-    public void testActivePrimaryShardsGrouped() {
-        assertThat(this.emptyRoutingTable.activePrimaryShardsGrouped(new String[0], true).size(), is(0));
-        assertThat(this.emptyRoutingTable.activePrimaryShardsGrouped(new String[0], false).size(), is(0));
-
-        assertThat(this.testRoutingTable.activePrimaryShardsGrouped(new String[]{TEST_INDEX_1}, false).size(), is(0));
-        assertThat(this.testRoutingTable.activePrimaryShardsGrouped(new String[]{TEST_INDEX_1}, true).size(), is(this.numberOfShards));
-
-        initPrimaries();
-        assertThat(this.testRoutingTable.activePrimaryShardsGrouped(new String[]{TEST_INDEX_1}, false).size(), is(0));
-        assertThat(this.testRoutingTable.activePrimaryShardsGrouped(new String[]{TEST_INDEX_1}, true).size(), is(this.numberOfShards));
-
-        startInitializingShards(TEST_INDEX_1);
-        assertThat(this.testRoutingTable.activePrimaryShardsGrouped(new String[]{TEST_INDEX_1}, false).size(), is(this.numberOfShards));
-        assertThat(this.testRoutingTable.activePrimaryShardsGrouped(new String[]{TEST_INDEX_1, TEST_INDEX_2}, false).size(), is(this.numberOfShards));
-        assertThat(this.testRoutingTable.activePrimaryShardsGrouped(new String[]{TEST_INDEX_1}, true).size(), is(this.numberOfShards));
-
-        startInitializingShards(TEST_INDEX_2);
-        assertThat(this.testRoutingTable.activePrimaryShardsGrouped(new String[]{TEST_INDEX_2}, false).size(), is(this.numberOfShards));
-        assertThat(this.testRoutingTable.activePrimaryShardsGrouped(new String[]{TEST_INDEX_1, TEST_INDEX_2}, false).size(), is(2 * this.numberOfShards));
-        assertThat(this.testRoutingTable.activePrimaryShardsGrouped(new String[]{TEST_INDEX_1, TEST_INDEX_2}, true).size(), is(2 * this.numberOfShards));
-
-        try {
-            this.testRoutingTable.activePrimaryShardsGrouped(new String[]{TEST_INDEX_1, "not_exists"}, true);
-            fail("Calling with non-existing index name should raise IndexMissingException");
-        } catch (IndexNotFoundException e) {
-            // expected
-        }
-    }
-
-    @Test
-    public void testAllActiveShardsGrouped() {
-        assertThat(this.emptyRoutingTable.allActiveShardsGrouped(new String[0], true).size(), is(0));
-        assertThat(this.emptyRoutingTable.allActiveShardsGrouped(new String[0], false).size(), is(0));
-
-        assertThat(this.testRoutingTable.allActiveShardsGrouped(new String[]{TEST_INDEX_1}, false).size(), is(0));
-        assertThat(this.testRoutingTable.allActiveShardsGrouped(new String[]{TEST_INDEX_1}, true).size(), is(this.shardsPerIndex));
-
-        initPrimaries();
-        assertThat(this.testRoutingTable.allActiveShardsGrouped(new String[]{TEST_INDEX_1}, false).size(), is(0));
-        assertThat(this.testRoutingTable.allActiveShardsGrouped(new String[]{TEST_INDEX_1}, true).size(), is(this.shardsPerIndex));
-
-        startInitializingShards(TEST_INDEX_1);
-        assertThat(this.testRoutingTable.allActiveShardsGrouped(new String[]{TEST_INDEX_1}, false).size(), is(this.numberOfShards));
-        assertThat(this.testRoutingTable.allActiveShardsGrouped(new String[]{TEST_INDEX_1, TEST_INDEX_2}, false).size(), is(this.numberOfShards));
-        assertThat(this.testRoutingTable.allActiveShardsGrouped(new String[]{TEST_INDEX_1}, true).size(), is(this.shardsPerIndex));
-
-        startInitializingShards(TEST_INDEX_2);
-        assertThat(this.testRoutingTable.allActiveShardsGrouped(new String[]{TEST_INDEX_2}, false).size(), is(this.numberOfShards));
-        assertThat(this.testRoutingTable.allActiveShardsGrouped(new String[]{TEST_INDEX_1, TEST_INDEX_2}, false).size(), is(2 * this.numberOfShards));
-        assertThat(this.testRoutingTable.allActiveShardsGrouped(new String[]{TEST_INDEX_1, TEST_INDEX_2}, true).size(), is(this.totalNumberOfShards));
-
-        try {
-            this.testRoutingTable.allActiveShardsGrouped(new String[]{TEST_INDEX_1, "not_exists"}, true);
-        } catch (IndexNotFoundException e) {
-            fail("Calling with non-existing index should be ignored at the moment");
-        }
-    }
-
-    @Test
-    public void testAllAssignedShardsGrouped() {
-        assertThat(this.testRoutingTable.allAssignedShardsGrouped(new String[]{TEST_INDEX_1}, false).size(), is(0));
-        assertThat(this.testRoutingTable.allAssignedShardsGrouped(new String[]{TEST_INDEX_1}, true).size(), is(this.shardsPerIndex));
-
-        initPrimaries();
-        assertThat(this.testRoutingTable.allAssignedShardsGrouped(new String[]{TEST_INDEX_1}, false).size(), is(this.numberOfShards));
-        assertThat(this.testRoutingTable.allAssignedShardsGrouped(new String[]{TEST_INDEX_1}, true).size(), is(this.shardsPerIndex));
-
-        assertThat(this.testRoutingTable.allAssignedShardsGrouped(new String[]{TEST_INDEX_1, TEST_INDEX_2}, false).size(), is(2 * this.numberOfShards));
-        assertThat(this.testRoutingTable.allAssignedShardsGrouped(new String[]{TEST_INDEX_1, TEST_INDEX_2}, true).size(), is(this.totalNumberOfShards));
-
-        try {
-            this.testRoutingTable.allAssignedShardsGrouped(new String[]{TEST_INDEX_1, "not_exists"}, false);
-        } catch (IndexNotFoundException e) {
-            fail("Calling with non-existing index should be ignored at the moment");
-        }
-    }
-
-    public void testAllShardsForMultipleIndices() {
-        assertThat(this.emptyRoutingTable.allShards(new String[0]).size(), is(0));
-
-        assertThat(this.testRoutingTable.allShards(new String[]{TEST_INDEX_1}).size(), is(this.shardsPerIndex));
-
-        initPrimaries();
-        assertThat(this.testRoutingTable.allShards(new String[]{TEST_INDEX_1}).size(), is(this.shardsPerIndex));
-
-        startInitializingShards(TEST_INDEX_1);
-        assertThat(this.testRoutingTable.allShards(new String[]{TEST_INDEX_1}).size(), is(this.shardsPerIndex));
-
-        startInitializingShards(TEST_INDEX_2);
-        assertThat(this.testRoutingTable.allShards(new String[]{TEST_INDEX_1, TEST_INDEX_2}).size(), is(this.totalNumberOfShards));
-
-        try {
-            this.testRoutingTable.allShards(new String[]{TEST_INDEX_1, "not_exists"});
-        } catch (IndexNotFoundException e) {
-            fail("Calling with non-existing index should be ignored at the moment");
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingTableTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingTableTests.java
new file mode 100644
index 0000000..ea5dda0
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingTableTests.java
@@ -0,0 +1,267 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cluster.routing;
+
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.cluster.metadata.MetaData;
+import org.elasticsearch.cluster.node.DiscoveryNodes;
+import org.elasticsearch.cluster.node.DiscoveryNodes.Builder;
+import org.elasticsearch.cluster.routing.allocation.AllocationService;
+import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.IndexNotFoundException;
+import org.elasticsearch.test.ESAllocationTestCase;
+import org.junit.Before;
+import org.junit.Test;
+
+import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
+import static org.elasticsearch.common.settings.Settings.settingsBuilder;
+import static org.hamcrest.Matchers.is;
+import static org.hamcrest.Matchers.nullValue;
+
+public class RoutingTableTests extends ESAllocationTestCase {
+
+    private static final String TEST_INDEX_1 = "test1";
+    private static final String TEST_INDEX_2 = "test2";
+    private RoutingTable emptyRoutingTable;
+    private RoutingTable testRoutingTable;
+    private int numberOfShards;
+    private int numberOfReplicas;
+    private int shardsPerIndex;
+    private int totalNumberOfShards;
+    private final static Settings DEFAULT_SETTINGS = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).build();
+    private final AllocationService ALLOCATION_SERVICE = createAllocationService(settingsBuilder()
+            .put("cluster.routing.allocation.concurrent_recoveries", 10)
+            .put("cluster.routing.allocation.node_initial_primaries_recoveries", 10)
+            .build());
+    private ClusterState clusterState;
+
+    @Before
+    public void setUp() throws Exception {
+        super.setUp();
+        this.numberOfShards = randomIntBetween(1, 5);
+        this.numberOfReplicas = randomIntBetween(1, 5);
+        this.shardsPerIndex = this.numberOfShards * (this.numberOfReplicas + 1);
+        this.totalNumberOfShards = this.shardsPerIndex * 2;
+        logger.info("Setup test with " + this.numberOfShards + " shards and " + this.numberOfReplicas + " replicas.");
+        this.emptyRoutingTable = new RoutingTable.Builder().build();
+        MetaData metaData = MetaData.builder()
+                .put(createIndexMetaData(TEST_INDEX_1))
+                .put(createIndexMetaData(TEST_INDEX_2))
+                .build();
+
+        this.testRoutingTable = new RoutingTable.Builder()
+                .add(new IndexRoutingTable.Builder(TEST_INDEX_1).initializeAsNew(metaData.index(TEST_INDEX_1)).build())
+                .add(new IndexRoutingTable.Builder(TEST_INDEX_2).initializeAsNew(metaData.index(TEST_INDEX_2)).build())
+                .build();
+        this.clusterState = ClusterState.builder(org.elasticsearch.cluster.ClusterName.DEFAULT).metaData(metaData).routingTable(testRoutingTable).build();
+    }
+
+    /**
+     * puts primary shard routings into initializing state
+     */
+    private void initPrimaries() {
+        logger.info("adding " + (this.numberOfReplicas + 1) + " nodes and performing rerouting");
+        Builder discoBuilder = DiscoveryNodes.builder();
+        for (int i = 0; i < this.numberOfReplicas + 1; i++) {
+            discoBuilder = discoBuilder.put(newNode("node" + i));
+        }
+        this.clusterState = ClusterState.builder(clusterState).nodes(discoBuilder).build();
+        RoutingAllocation.Result rerouteResult = ALLOCATION_SERVICE.reroute(clusterState);
+        this.testRoutingTable = rerouteResult.routingTable();
+        assertThat(rerouteResult.changed(), is(true));
+        this.clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
+    }
+
+    private void startInitializingShards(String index) {
+        this.clusterState = ClusterState.builder(clusterState).routingTable(this.testRoutingTable).build();
+        logger.info("start primary shards for index " + index);
+        RoutingAllocation.Result rerouteResult = ALLOCATION_SERVICE.applyStartedShards(this.clusterState, this.clusterState.getRoutingNodes().shardsWithState(index, INITIALIZING));
+        this.clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
+        this.testRoutingTable = rerouteResult.routingTable();
+    }
+
+    private IndexMetaData.Builder createIndexMetaData(String indexName) {
+        return new IndexMetaData.Builder(indexName)
+                .settings(DEFAULT_SETTINGS)
+                .numberOfReplicas(this.numberOfReplicas)
+                .numberOfShards(this.numberOfShards);
+    }
+
+    @Test
+    public void testAllShards() {
+        assertThat(this.emptyRoutingTable.allShards().size(), is(0));
+        assertThat(this.testRoutingTable.allShards().size(), is(this.totalNumberOfShards));
+
+        assertThat(this.testRoutingTable.allShards(TEST_INDEX_1).size(), is(this.shardsPerIndex));
+        try {
+            assertThat(this.testRoutingTable.allShards("not_existing").size(), is(0));
+            fail("Exception expected when calling allShards() with non existing index name");
+        } catch (IndexNotFoundException e) {
+            // expected
+        }
+    }
+
+    @Test
+    public void testHasIndex() {
+        assertThat(this.testRoutingTable.hasIndex(TEST_INDEX_1), is(true));
+        assertThat(this.testRoutingTable.hasIndex("foobar"), is(false));
+    }
+
+    @Test
+    public void testIndex() {
+        assertThat(this.testRoutingTable.index(TEST_INDEX_1).getIndex(), is(TEST_INDEX_1));
+        assertThat(this.testRoutingTable.index("foobar"), is(nullValue()));
+    }
+
+    @Test
+    public void testIndicesRouting() {
+        assertThat(this.testRoutingTable.indicesRouting().size(), is(2));
+        assertThat(this.testRoutingTable.getIndicesRouting().size(), is(2));
+        assertSame(this.testRoutingTable.getIndicesRouting(), this.testRoutingTable.indicesRouting());
+    }
+
+    @Test
+    public void testShardsWithState() {
+        assertThat(this.testRoutingTable.shardsWithState(ShardRoutingState.UNASSIGNED).size(), is(this.totalNumberOfShards));
+
+        initPrimaries();
+        assertThat(this.testRoutingTable.shardsWithState(ShardRoutingState.UNASSIGNED).size(), is(this.totalNumberOfShards - 2 * this.numberOfShards));
+        assertThat(this.testRoutingTable.shardsWithState(ShardRoutingState.INITIALIZING).size(), is(2 * this.numberOfShards));
+
+        startInitializingShards(TEST_INDEX_1);
+        assertThat(this.testRoutingTable.shardsWithState(ShardRoutingState.STARTED).size(), is(this.numberOfShards));
+        int initializingExpected = this.numberOfShards + this.numberOfShards * this.numberOfReplicas;
+        assertThat(this.testRoutingTable.shardsWithState(ShardRoutingState.INITIALIZING).size(), is(initializingExpected));
+        assertThat(this.testRoutingTable.shardsWithState(ShardRoutingState.UNASSIGNED).size(), is(this.totalNumberOfShards - initializingExpected - this.numberOfShards));
+
+        startInitializingShards(TEST_INDEX_2);
+        assertThat(this.testRoutingTable.shardsWithState(ShardRoutingState.STARTED).size(), is(2 * this.numberOfShards));
+        initializingExpected = 2 * this.numberOfShards * this.numberOfReplicas;
+        assertThat(this.testRoutingTable.shardsWithState(ShardRoutingState.INITIALIZING).size(), is(initializingExpected));
+        assertThat(this.testRoutingTable.shardsWithState(ShardRoutingState.UNASSIGNED).size(), is(this.totalNumberOfShards - initializingExpected - 2 * this.numberOfShards));
+
+        // now start all replicas too
+        startInitializingShards(TEST_INDEX_1);
+        startInitializingShards(TEST_INDEX_2);
+        assertThat(this.testRoutingTable.shardsWithState(ShardRoutingState.STARTED).size(), is(this.totalNumberOfShards));
+    }
+
+    @Test
+    public void testActivePrimaryShardsGrouped() {
+        assertThat(this.emptyRoutingTable.activePrimaryShardsGrouped(new String[0], true).size(), is(0));
+        assertThat(this.emptyRoutingTable.activePrimaryShardsGrouped(new String[0], false).size(), is(0));
+
+        assertThat(this.testRoutingTable.activePrimaryShardsGrouped(new String[]{TEST_INDEX_1}, false).size(), is(0));
+        assertThat(this.testRoutingTable.activePrimaryShardsGrouped(new String[]{TEST_INDEX_1}, true).size(), is(this.numberOfShards));
+
+        initPrimaries();
+        assertThat(this.testRoutingTable.activePrimaryShardsGrouped(new String[]{TEST_INDEX_1}, false).size(), is(0));
+        assertThat(this.testRoutingTable.activePrimaryShardsGrouped(new String[]{TEST_INDEX_1}, true).size(), is(this.numberOfShards));
+
+        startInitializingShards(TEST_INDEX_1);
+        assertThat(this.testRoutingTable.activePrimaryShardsGrouped(new String[]{TEST_INDEX_1}, false).size(), is(this.numberOfShards));
+        assertThat(this.testRoutingTable.activePrimaryShardsGrouped(new String[]{TEST_INDEX_1, TEST_INDEX_2}, false).size(), is(this.numberOfShards));
+        assertThat(this.testRoutingTable.activePrimaryShardsGrouped(new String[]{TEST_INDEX_1}, true).size(), is(this.numberOfShards));
+
+        startInitializingShards(TEST_INDEX_2);
+        assertThat(this.testRoutingTable.activePrimaryShardsGrouped(new String[]{TEST_INDEX_2}, false).size(), is(this.numberOfShards));
+        assertThat(this.testRoutingTable.activePrimaryShardsGrouped(new String[]{TEST_INDEX_1, TEST_INDEX_2}, false).size(), is(2 * this.numberOfShards));
+        assertThat(this.testRoutingTable.activePrimaryShardsGrouped(new String[]{TEST_INDEX_1, TEST_INDEX_2}, true).size(), is(2 * this.numberOfShards));
+
+        try {
+            this.testRoutingTable.activePrimaryShardsGrouped(new String[]{TEST_INDEX_1, "not_exists"}, true);
+            fail("Calling with non-existing index name should raise IndexMissingException");
+        } catch (IndexNotFoundException e) {
+            // expected
+        }
+    }
+
+    @Test
+    public void testAllActiveShardsGrouped() {
+        assertThat(this.emptyRoutingTable.allActiveShardsGrouped(new String[0], true).size(), is(0));
+        assertThat(this.emptyRoutingTable.allActiveShardsGrouped(new String[0], false).size(), is(0));
+
+        assertThat(this.testRoutingTable.allActiveShardsGrouped(new String[]{TEST_INDEX_1}, false).size(), is(0));
+        assertThat(this.testRoutingTable.allActiveShardsGrouped(new String[]{TEST_INDEX_1}, true).size(), is(this.shardsPerIndex));
+
+        initPrimaries();
+        assertThat(this.testRoutingTable.allActiveShardsGrouped(new String[]{TEST_INDEX_1}, false).size(), is(0));
+        assertThat(this.testRoutingTable.allActiveShardsGrouped(new String[]{TEST_INDEX_1}, true).size(), is(this.shardsPerIndex));
+
+        startInitializingShards(TEST_INDEX_1);
+        assertThat(this.testRoutingTable.allActiveShardsGrouped(new String[]{TEST_INDEX_1}, false).size(), is(this.numberOfShards));
+        assertThat(this.testRoutingTable.allActiveShardsGrouped(new String[]{TEST_INDEX_1, TEST_INDEX_2}, false).size(), is(this.numberOfShards));
+        assertThat(this.testRoutingTable.allActiveShardsGrouped(new String[]{TEST_INDEX_1}, true).size(), is(this.shardsPerIndex));
+
+        startInitializingShards(TEST_INDEX_2);
+        assertThat(this.testRoutingTable.allActiveShardsGrouped(new String[]{TEST_INDEX_2}, false).size(), is(this.numberOfShards));
+        assertThat(this.testRoutingTable.allActiveShardsGrouped(new String[]{TEST_INDEX_1, TEST_INDEX_2}, false).size(), is(2 * this.numberOfShards));
+        assertThat(this.testRoutingTable.allActiveShardsGrouped(new String[]{TEST_INDEX_1, TEST_INDEX_2}, true).size(), is(this.totalNumberOfShards));
+
+        try {
+            this.testRoutingTable.allActiveShardsGrouped(new String[]{TEST_INDEX_1, "not_exists"}, true);
+        } catch (IndexNotFoundException e) {
+            fail("Calling with non-existing index should be ignored at the moment");
+        }
+    }
+
+    @Test
+    public void testAllAssignedShardsGrouped() {
+        assertThat(this.testRoutingTable.allAssignedShardsGrouped(new String[]{TEST_INDEX_1}, false).size(), is(0));
+        assertThat(this.testRoutingTable.allAssignedShardsGrouped(new String[]{TEST_INDEX_1}, true).size(), is(this.shardsPerIndex));
+
+        initPrimaries();
+        assertThat(this.testRoutingTable.allAssignedShardsGrouped(new String[]{TEST_INDEX_1}, false).size(), is(this.numberOfShards));
+        assertThat(this.testRoutingTable.allAssignedShardsGrouped(new String[]{TEST_INDEX_1}, true).size(), is(this.shardsPerIndex));
+
+        assertThat(this.testRoutingTable.allAssignedShardsGrouped(new String[]{TEST_INDEX_1, TEST_INDEX_2}, false).size(), is(2 * this.numberOfShards));
+        assertThat(this.testRoutingTable.allAssignedShardsGrouped(new String[]{TEST_INDEX_1, TEST_INDEX_2}, true).size(), is(this.totalNumberOfShards));
+
+        try {
+            this.testRoutingTable.allAssignedShardsGrouped(new String[]{TEST_INDEX_1, "not_exists"}, false);
+        } catch (IndexNotFoundException e) {
+            fail("Calling with non-existing index should be ignored at the moment");
+        }
+    }
+
+    public void testAllShardsForMultipleIndices() {
+        assertThat(this.emptyRoutingTable.allShards(new String[0]).size(), is(0));
+
+        assertThat(this.testRoutingTable.allShards(new String[]{TEST_INDEX_1}).size(), is(this.shardsPerIndex));
+
+        initPrimaries();
+        assertThat(this.testRoutingTable.allShards(new String[]{TEST_INDEX_1}).size(), is(this.shardsPerIndex));
+
+        startInitializingShards(TEST_INDEX_1);
+        assertThat(this.testRoutingTable.allShards(new String[]{TEST_INDEX_1}).size(), is(this.shardsPerIndex));
+
+        startInitializingShards(TEST_INDEX_2);
+        assertThat(this.testRoutingTable.allShards(new String[]{TEST_INDEX_1, TEST_INDEX_2}).size(), is(this.totalNumberOfShards));
+
+        try {
+            this.testRoutingTable.allShards(new String[]{TEST_INDEX_1, "not_exists"});
+        } catch (IndexNotFoundException e) {
+            fail("Calling with non-existing index should be ignored at the moment");
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceUnbalancedClusterTest.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceUnbalancedClusterTest.java
deleted file mode 100644
index 89e0cf8..0000000
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceUnbalancedClusterTest.java
+++ /dev/null
@@ -1,98 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.cluster.routing.allocation;
-
-import org.apache.lucene.util.TestUtil;
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
-import org.elasticsearch.cluster.routing.RoutingTable;
-import org.elasticsearch.cluster.routing.ShardRouting;
-
-import java.io.IOException;
-import java.io.InputStream;
-import java.nio.file.Files;
-import java.nio.file.Path;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-
-/**
- * see issue #9023
- */
-public class BalanceUnbalancedClusterTest extends CatAllocationTestCase {
-
-    @Override
-    protected Path getCatPath() throws IOException {
-        Path tmp = createTempDir();
-        try (InputStream stream = Files.newInputStream(getDataPath("/org/elasticsearch/cluster/routing/issue_9023.zip"))) {
-            TestUtil.unzip(stream, tmp);
-        }
-        return tmp.resolve("issue_9023");
-    }
-
-    @Override
-    protected ClusterState allocateNew(ClusterState state) {
-        String index = "tweets-2014-12-29:00";
-        AllocationService strategy = createAllocationService(settingsBuilder()
-                .build());
-        MetaData metaData = MetaData.builder(state.metaData())
-                .put(IndexMetaData.builder(index).settings(settings(Version.CURRENT)).numberOfShards(5).numberOfReplicas(1))
-                .build();
-
-        RoutingTable routingTable = RoutingTable.builder(state.routingTable())
-                .addAsNew(metaData.index(index))
-                .build();
-
-        ClusterState clusterState = ClusterState.builder(state).metaData(metaData).routingTable(routingTable).build();
-        routingTable = strategy.reroute(clusterState).routingTable();
-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        while (true) {
-            if (routingTable.shardsWithState(INITIALIZING).isEmpty()) {
-                break;
-            }
-            routingTable = strategy.applyStartedShards(clusterState, routingTable.shardsWithState(INITIALIZING)).routingTable();
-            clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        }
-        Map<String, Integer> counts = new HashMap<>();
-        for (IndexShardRoutingTable table : routingTable.index(index)) {
-            for (ShardRouting r : table) {
-                String s = r.currentNodeId();
-                Integer count = counts.get(s);
-                if (count == null) {
-                    count = 0;
-                }
-                count++;
-                counts.put(s, count);
-            }
-        }
-        for (Map.Entry<String, Integer> count : counts.entrySet()) {
-            // we have 10 shards and 4 nodes so 2 nodes have 3 shards and 2 nodes have 2 shards
-            assertTrue("Node: " + count.getKey() + " has shard mismatch: " + count.getValue(), count.getValue() >= 2);
-            assertTrue("Node: " + count.getKey() + " has shard mismatch: " + count.getValue(), count.getValue() <= 3);
-
-        }
-        return clusterState;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceUnbalancedClusterTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceUnbalancedClusterTests.java
new file mode 100644
index 0000000..d0e3d72
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceUnbalancedClusterTests.java
@@ -0,0 +1,98 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.cluster.routing.allocation;
+
+import org.apache.lucene.util.TestUtil;
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.cluster.metadata.MetaData;
+import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
+import org.elasticsearch.cluster.routing.RoutingTable;
+import org.elasticsearch.cluster.routing.ShardRouting;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
+import static org.elasticsearch.common.settings.Settings.settingsBuilder;
+
+/**
+ * see issue #9023
+ */
+public class BalanceUnbalancedClusterTests extends CatAllocationTestCase {
+
+    @Override
+    protected Path getCatPath() throws IOException {
+        Path tmp = createTempDir();
+        try (InputStream stream = Files.newInputStream(getDataPath("/org/elasticsearch/cluster/routing/issue_9023.zip"))) {
+            TestUtil.unzip(stream, tmp);
+        }
+        return tmp.resolve("issue_9023");
+    }
+
+    @Override
+    protected ClusterState allocateNew(ClusterState state) {
+        String index = "tweets-2014-12-29:00";
+        AllocationService strategy = createAllocationService(settingsBuilder()
+                .build());
+        MetaData metaData = MetaData.builder(state.metaData())
+                .put(IndexMetaData.builder(index).settings(settings(Version.CURRENT)).numberOfShards(5).numberOfReplicas(1))
+                .build();
+
+        RoutingTable routingTable = RoutingTable.builder(state.routingTable())
+                .addAsNew(metaData.index(index))
+                .build();
+
+        ClusterState clusterState = ClusterState.builder(state).metaData(metaData).routingTable(routingTable).build();
+        routingTable = strategy.reroute(clusterState).routingTable();
+        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
+        while (true) {
+            if (routingTable.shardsWithState(INITIALIZING).isEmpty()) {
+                break;
+            }
+            routingTable = strategy.applyStartedShards(clusterState, routingTable.shardsWithState(INITIALIZING)).routingTable();
+            clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
+        }
+        Map<String, Integer> counts = new HashMap<>();
+        for (IndexShardRoutingTable table : routingTable.index(index)) {
+            for (ShardRouting r : table) {
+                String s = r.currentNodeId();
+                Integer count = counts.get(s);
+                if (count == null) {
+                    count = 0;
+                }
+                count++;
+                counts.put(s, count);
+            }
+        }
+        for (Map.Entry<String, Integer> count : counts.entrySet()) {
+            // we have 10 shards and 4 nodes so 2 nodes have 3 shards and 2 nodes have 2 shards
+            assertTrue("Node: " + count.getKey() + " has shard mismatch: " + count.getValue(), count.getValue() >= 2);
+            assertTrue("Node: " + count.getKey() + " has shard mismatch: " + count.getValue(), count.getValue() <= 3);
+
+        }
+        return clusterState;
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/common/Base64Test.java b/core/src/test/java/org/elasticsearch/common/Base64Test.java
deleted file mode 100644
index 07a3689..0000000
--- a/core/src/test/java/org/elasticsearch/common/Base64Test.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.common;
-
-import com.google.common.base.Charsets;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.Locale;
-
-import static org.hamcrest.Matchers.is;
-
-/**
- *
- */
-public class Base64Test extends ESTestCase {
-
-    @Test // issue #6334
-    public void testBase64DecodeWithExtraCharactersAfterPadding() throws Exception {
-        String plain = randomAsciiOfLengthBetween(1, 20) + ":" + randomAsciiOfLengthBetween(1, 20);
-        String encoded = Base64.encodeBytes(plain.getBytes(Charsets.UTF_8));
-        assertValidBase64(encoded, plain);
-
-        // lets append some trash here, if the encoded string has been padded
-        char lastChar = encoded.charAt(encoded.length() - 1);
-        if (lastChar == '=') {
-            assertInvalidBase64(encoded + randomAsciiOfLength(3));
-        }
-    }
-
-    private void assertValidBase64(String base64, String expected) throws IOException {
-        String decoded = new String(Base64.decode(base64.getBytes(Charsets.UTF_8)), Charsets.UTF_8);
-        assertThat(decoded, is(expected));
-    }
-
-    private void assertInvalidBase64(String base64) {
-        try {
-            Base64.decode(base64.getBytes(Charsets.UTF_8));
-            fail(String.format(Locale.ROOT, "Expected IOException to be thrown for string %s (len %d)", base64, base64.length()));
-        } catch (IOException e) {}
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/common/Base64Tests.java b/core/src/test/java/org/elasticsearch/common/Base64Tests.java
new file mode 100644
index 0000000..6bceec9
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/Base64Tests.java
@@ -0,0 +1,59 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.common;
+
+import com.google.common.base.Charsets;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.Locale;
+
+import static org.hamcrest.Matchers.is;
+
+/**
+ *
+ */
+public class Base64Tests extends ESTestCase {
+
+    @Test // issue #6334
+    public void testBase64DecodeWithExtraCharactersAfterPadding() throws Exception {
+        String plain = randomAsciiOfLengthBetween(1, 20) + ":" + randomAsciiOfLengthBetween(1, 20);
+        String encoded = Base64.encodeBytes(plain.getBytes(Charsets.UTF_8));
+        assertValidBase64(encoded, plain);
+
+        // lets append some trash here, if the encoded string has been padded
+        char lastChar = encoded.charAt(encoded.length() - 1);
+        if (lastChar == '=') {
+            assertInvalidBase64(encoded + randomAsciiOfLength(3));
+        }
+    }
+
+    private void assertValidBase64(String base64, String expected) throws IOException {
+        String decoded = new String(Base64.decode(base64.getBytes(Charsets.UTF_8)), Charsets.UTF_8);
+        assertThat(decoded, is(expected));
+    }
+
+    private void assertInvalidBase64(String base64) {
+        try {
+            Base64.decode(base64.getBytes(Charsets.UTF_8));
+            fail(String.format(Locale.ROOT, "Expected IOException to be thrown for string %s (len %d)", base64, base64.length()));
+        } catch (IOException e) {}
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/common/blobstore/BlobStoreTest.java b/core/src/test/java/org/elasticsearch/common/blobstore/BlobStoreTest.java
deleted file mode 100644
index 62daeb7..0000000
--- a/core/src/test/java/org/elasticsearch/common/blobstore/BlobStoreTest.java
+++ /dev/null
@@ -1,147 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.common.blobstore;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.LuceneTestCase;
-import org.elasticsearch.common.blobstore.fs.FsBlobStore;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.ByteSizeUnit;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-import java.nio.file.Path;
-import java.util.Arrays;
-import java.util.Map;
-
-import static com.google.common.collect.Maps.newHashMap;
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.notNullValue;
-
-@LuceneTestCase.SuppressFileSystems("ExtrasFS")
-public class BlobStoreTest extends ESTestCase {
-
-    @Test
-    public void testWriteRead() throws IOException {
-        final BlobStore store = newBlobStore();
-        final BlobContainer container = store.blobContainer(new BlobPath());
-        byte[] data = randomBytes(randomIntBetween(10, scaledRandomIntBetween(1024, 1 << 16)));
-        try (OutputStream stream = container.createOutput("foobar")) {
-            stream.write(data);
-        }
-        try (InputStream stream = container.openInput("foobar")) {
-            BytesRefBuilder target = new BytesRefBuilder();
-            while (target.length() < data.length) {
-                byte[] buffer = new byte[scaledRandomIntBetween(1, data.length - target.length())];
-                int offset = scaledRandomIntBetween(0, buffer.length - 1);
-                int read = stream.read(buffer, offset, buffer.length - offset);
-                target.append(new BytesRef(buffer, offset, read));
-            }
-            assertEquals(data.length, target.length());
-            assertArrayEquals(data, Arrays.copyOfRange(target.bytes(), 0, target.length()));
-        }
-        store.close();
-    }
-
-    @Test
-    public void testMoveAndList() throws IOException {
-        final BlobStore store = newBlobStore();
-        final BlobContainer container = store.blobContainer(new BlobPath());
-        assertThat(container.listBlobs().size(), equalTo(0));
-        int numberOfFooBlobs = randomIntBetween(0, 10);
-        int numberOfBarBlobs = randomIntBetween(3, 20);
-        Map<String, Long> generatedBlobs = newHashMap();
-        for (int i = 0; i < numberOfFooBlobs; i++) {
-            int length = randomIntBetween(10, 100);
-            String name = "foo-" + i + "-";
-            generatedBlobs.put(name, (long) length);
-            createRandomBlob(container, name, length);
-        }
-        for (int i = 1; i < numberOfBarBlobs; i++) {
-            int length = randomIntBetween(10, 100);
-            String name = "bar-" + i + "-";
-            generatedBlobs.put(name, (long) length);
-            createRandomBlob(container, name, length);
-        }
-        int length = randomIntBetween(10, 100);
-        String name = "bar-0-";
-        generatedBlobs.put(name, (long) length);
-        byte[] data = createRandomBlob(container, name, length);
-
-        Map<String, BlobMetaData> blobs = container.listBlobs();
-        assertThat(blobs.size(), equalTo(numberOfFooBlobs + numberOfBarBlobs));
-        for (Map.Entry<String, Long> generated : generatedBlobs.entrySet()) {
-            BlobMetaData blobMetaData = blobs.get(generated.getKey());
-            assertThat(generated.getKey(), blobMetaData, notNullValue());
-            assertThat(blobMetaData.name(), equalTo(generated.getKey()));
-            assertThat(blobMetaData.length(), equalTo(generated.getValue()));
-        }
-
-        assertThat(container.listBlobsByPrefix("foo-").size(), equalTo(numberOfFooBlobs));
-        assertThat(container.listBlobsByPrefix("bar-").size(), equalTo(numberOfBarBlobs));
-        assertThat(container.listBlobsByPrefix("baz-").size(), equalTo(0));
-
-        String newName = "bar-new";
-        // Move to a new location
-        container.move(name, newName);
-        assertThat(container.listBlobsByPrefix(name).size(), equalTo(0));
-        blobs = container.listBlobsByPrefix(newName);
-        assertThat(blobs.size(), equalTo(1));
-        assertThat(blobs.get(newName).length(), equalTo(generatedBlobs.get(name)));
-        assertThat(data, equalTo(readBlobFully(container, newName, length)));
-        store.close();
-    }
-
-    protected byte[] createRandomBlob(BlobContainer container, String name, int length) throws IOException {
-        byte[] data = randomBytes(length);
-        try (OutputStream stream = container.createOutput(name)) {
-            stream.write(data);
-        }
-        return data;
-    }
-
-    protected byte[] readBlobFully(BlobContainer container, String name, int length) throws IOException {
-        byte[] data = new byte[length];
-        try (InputStream inputStream = container.openInput(name)) {
-            assertThat(inputStream.read(data), equalTo(length));
-            assertThat(inputStream.read(), equalTo(-1));
-        }
-        return data;
-    }
-
-    protected byte[] randomBytes(int length) {
-        byte[] data = new byte[length];
-        for (int i = 0; i < data.length; i++) {
-            data[i] = (byte) randomInt();
-        }
-        return data;
-    }
-
-    protected BlobStore newBlobStore() throws IOException {
-        Path tempDir = createTempDir();
-        Settings settings = randomBoolean() ? Settings.EMPTY : Settings.builder().put("buffer_size", new ByteSizeValue(randomIntBetween(1, 100), ByteSizeUnit.KB)).build();
-        FsBlobStore store = new FsBlobStore(settings, tempDir);
-        return store;
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/common/blobstore/BlobStoreTests.java b/core/src/test/java/org/elasticsearch/common/blobstore/BlobStoreTests.java
new file mode 100644
index 0000000..57322e2
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/blobstore/BlobStoreTests.java
@@ -0,0 +1,147 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.common.blobstore;
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefBuilder;
+import org.apache.lucene.util.LuceneTestCase;
+import org.elasticsearch.common.blobstore.fs.FsBlobStore;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.ByteSizeUnit;
+import org.elasticsearch.common.unit.ByteSizeValue;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.OutputStream;
+import java.nio.file.Path;
+import java.util.Arrays;
+import java.util.Map;
+
+import static com.google.common.collect.Maps.newHashMap;
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.notNullValue;
+
+@LuceneTestCase.SuppressFileSystems("ExtrasFS")
+public class BlobStoreTests extends ESTestCase {
+
+    @Test
+    public void testWriteRead() throws IOException {
+        final BlobStore store = newBlobStore();
+        final BlobContainer container = store.blobContainer(new BlobPath());
+        byte[] data = randomBytes(randomIntBetween(10, scaledRandomIntBetween(1024, 1 << 16)));
+        try (OutputStream stream = container.createOutput("foobar")) {
+            stream.write(data);
+        }
+        try (InputStream stream = container.openInput("foobar")) {
+            BytesRefBuilder target = new BytesRefBuilder();
+            while (target.length() < data.length) {
+                byte[] buffer = new byte[scaledRandomIntBetween(1, data.length - target.length())];
+                int offset = scaledRandomIntBetween(0, buffer.length - 1);
+                int read = stream.read(buffer, offset, buffer.length - offset);
+                target.append(new BytesRef(buffer, offset, read));
+            }
+            assertEquals(data.length, target.length());
+            assertArrayEquals(data, Arrays.copyOfRange(target.bytes(), 0, target.length()));
+        }
+        store.close();
+    }
+
+    @Test
+    public void testMoveAndList() throws IOException {
+        final BlobStore store = newBlobStore();
+        final BlobContainer container = store.blobContainer(new BlobPath());
+        assertThat(container.listBlobs().size(), equalTo(0));
+        int numberOfFooBlobs = randomIntBetween(0, 10);
+        int numberOfBarBlobs = randomIntBetween(3, 20);
+        Map<String, Long> generatedBlobs = newHashMap();
+        for (int i = 0; i < numberOfFooBlobs; i++) {
+            int length = randomIntBetween(10, 100);
+            String name = "foo-" + i + "-";
+            generatedBlobs.put(name, (long) length);
+            createRandomBlob(container, name, length);
+        }
+        for (int i = 1; i < numberOfBarBlobs; i++) {
+            int length = randomIntBetween(10, 100);
+            String name = "bar-" + i + "-";
+            generatedBlobs.put(name, (long) length);
+            createRandomBlob(container, name, length);
+        }
+        int length = randomIntBetween(10, 100);
+        String name = "bar-0-";
+        generatedBlobs.put(name, (long) length);
+        byte[] data = createRandomBlob(container, name, length);
+
+        Map<String, BlobMetaData> blobs = container.listBlobs();
+        assertThat(blobs.size(), equalTo(numberOfFooBlobs + numberOfBarBlobs));
+        for (Map.Entry<String, Long> generated : generatedBlobs.entrySet()) {
+            BlobMetaData blobMetaData = blobs.get(generated.getKey());
+            assertThat(generated.getKey(), blobMetaData, notNullValue());
+            assertThat(blobMetaData.name(), equalTo(generated.getKey()));
+            assertThat(blobMetaData.length(), equalTo(generated.getValue()));
+        }
+
+        assertThat(container.listBlobsByPrefix("foo-").size(), equalTo(numberOfFooBlobs));
+        assertThat(container.listBlobsByPrefix("bar-").size(), equalTo(numberOfBarBlobs));
+        assertThat(container.listBlobsByPrefix("baz-").size(), equalTo(0));
+
+        String newName = "bar-new";
+        // Move to a new location
+        container.move(name, newName);
+        assertThat(container.listBlobsByPrefix(name).size(), equalTo(0));
+        blobs = container.listBlobsByPrefix(newName);
+        assertThat(blobs.size(), equalTo(1));
+        assertThat(blobs.get(newName).length(), equalTo(generatedBlobs.get(name)));
+        assertThat(data, equalTo(readBlobFully(container, newName, length)));
+        store.close();
+    }
+
+    protected byte[] createRandomBlob(BlobContainer container, String name, int length) throws IOException {
+        byte[] data = randomBytes(length);
+        try (OutputStream stream = container.createOutput(name)) {
+            stream.write(data);
+        }
+        return data;
+    }
+
+    protected byte[] readBlobFully(BlobContainer container, String name, int length) throws IOException {
+        byte[] data = new byte[length];
+        try (InputStream inputStream = container.openInput(name)) {
+            assertThat(inputStream.read(data), equalTo(length));
+            assertThat(inputStream.read(), equalTo(-1));
+        }
+        return data;
+    }
+
+    protected byte[] randomBytes(int length) {
+        byte[] data = new byte[length];
+        for (int i = 0; i < data.length; i++) {
+            data[i] = (byte) randomInt();
+        }
+        return data;
+    }
+
+    protected BlobStore newBlobStore() throws IOException {
+        Path tempDir = createTempDir();
+        Settings settings = randomBoolean() ? Settings.EMPTY : Settings.builder().put("buffer_size", new ByteSizeValue(randomIntBetween(1, 100), ByteSizeUnit.KB)).build();
+        FsBlobStore store = new FsBlobStore(settings, tempDir);
+        return store;
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/common/bytes/PagedBytesReferenceTest.java b/core/src/test/java/org/elasticsearch/common/bytes/PagedBytesReferenceTest.java
deleted file mode 100644
index 59a3d01..0000000
--- a/core/src/test/java/org/elasticsearch/common/bytes/PagedBytesReferenceTest.java
+++ /dev/null
@@ -1,582 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common.bytes;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefBuilder;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.ReleasableBytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.util.BigArrays;
-import org.elasticsearch.common.util.ByteArray;
-import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
-import org.elasticsearch.test.ESTestCase;
-import org.hamcrest.Matchers;
-import org.jboss.netty.buffer.ChannelBuffer;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.io.EOFException;
-import java.io.IOException;
-import java.nio.channels.FileChannel;
-import java.nio.file.Files;
-import java.nio.file.Path;
-import java.nio.file.StandardOpenOption;
-import java.util.Arrays;
-
-public class PagedBytesReferenceTest extends ESTestCase {
-
-    private static final int PAGE_SIZE = BigArrays.BYTE_PAGE_SIZE;
-
-    private BigArrays bigarrays;
-
-    @Override
-    @Before
-    public void setUp() throws Exception {
-        super.setUp();
-        bigarrays = new BigArrays(null, new NoneCircuitBreakerService());
-    }
-
-    @Override
-    @After
-    public void tearDown() throws Exception {
-        super.tearDown();
-    }
-
-    @Test
-    public void testGet() {
-        int length = randomIntBetween(1, PAGE_SIZE * 3);
-        BytesReference pbr = getRandomizedPagedBytesReference(length);
-        int sliceOffset = randomIntBetween(0, length / 2);
-        int sliceLength = Math.max(1, length - sliceOffset - 1);
-        BytesReference slice = pbr.slice(sliceOffset, sliceLength);
-        assertEquals(pbr.get(sliceOffset), slice.get(0));
-        assertEquals(pbr.get(sliceOffset + sliceLength - 1), slice.get(sliceLength - 1));
-    }
-
-    public void testLength() {
-        int[] sizes = {0, randomInt(PAGE_SIZE), PAGE_SIZE, randomInt(PAGE_SIZE * 3)};
-
-        for (int i = 0; i < sizes.length; i++) {
-            BytesReference pbr = getRandomizedPagedBytesReference(sizes[i]);
-            assertEquals(sizes[i], pbr.length());
-        }
-    }
-
-    public void testSlice() {
-        int length = randomInt(PAGE_SIZE * 3);
-        BytesReference pbr = getRandomizedPagedBytesReference(length);
-        int sliceOffset = randomIntBetween(0, length / 2);
-        int sliceLength = Math.max(0, length - sliceOffset - 1);
-        BytesReference slice = pbr.slice(sliceOffset, sliceLength);
-        assertEquals(sliceLength, slice.length());
-
-        if (slice.hasArray()) {
-            assertEquals(sliceOffset, slice.arrayOffset());
-        } else {
-            try {
-                slice.arrayOffset();
-                fail("expected IllegalStateException");
-            } catch (IllegalStateException ise) {
-                // expected
-            }
-        }
-    }
-
-    public void testStreamInput() throws IOException {
-        int length = randomIntBetween(10, scaledRandomIntBetween(PAGE_SIZE * 2, PAGE_SIZE * 20));
-        BytesReference pbr = getRandomizedPagedBytesReference(length);
-        StreamInput si = pbr.streamInput();
-        assertNotNull(si);
-
-        // read single bytes one by one
-        assertEquals(pbr.get(0), si.readByte());
-        assertEquals(pbr.get(1), si.readByte());
-        assertEquals(pbr.get(2), si.readByte());
-
-        // reset the stream for bulk reading
-        si.reset();
-
-        // buffer for bulk reads
-        byte[] origBuf = new byte[length];
-        getRandom().nextBytes(origBuf);
-        byte[] targetBuf = Arrays.copyOf(origBuf, origBuf.length);
-
-        // bulk-read 0 bytes: must not modify buffer
-        si.readBytes(targetBuf, 0, 0);
-        assertEquals(origBuf[0], targetBuf[0]);
-        si.reset();
-
-        // read a few few bytes as ints
-        int bytesToRead = randomIntBetween(1, length / 2);
-        for (int i = 0; i < bytesToRead; i++) {
-            int b = si.read();
-            assertEquals(pbr.get(i), b);
-        }
-        si.reset();
-
-        // bulk-read all
-        si.readFully(targetBuf);
-        assertArrayEquals(pbr.toBytes(), targetBuf);
-
-        // continuing to read should now fail with EOFException
-        try {
-            si.readByte();
-            fail("expected EOF");
-        } catch (EOFException eof) {
-            // yay
-        }
-
-        // try to read more than the stream contains
-        si.reset();
-        try {
-            si.readBytes(targetBuf, 0, length * 2);
-            fail("expected IndexOutOfBoundsException: le > stream.length");
-        } catch (IndexOutOfBoundsException ioob) {
-            // expected
-        }
-    }
-
-    public void testStreamInputBulkReadWithOffset() throws IOException {
-        int length = randomIntBetween(10, scaledRandomIntBetween(PAGE_SIZE * 2, PAGE_SIZE * 20));
-        BytesReference pbr = getRandomizedPagedBytesReference(length);
-        StreamInput si = pbr.streamInput();
-        assertNotNull(si);
-
-        // read a bunch of single bytes one by one
-        int offset = randomIntBetween(1, length / 2);
-        for (int i = 0; i < offset; i++) {
-            assertEquals(pbr.get(i), si.readByte());
-        }
-
-        // now do NOT reset the stream - keep the stream's offset!
-
-        // buffer to compare remaining bytes against bulk read
-        byte[] pbrBytesWithOffset = Arrays.copyOfRange(pbr.toBytes(), offset, length);
-        // randomized target buffer to ensure no stale slots
-        byte[] targetBytes = new byte[pbrBytesWithOffset.length];
-        getRandom().nextBytes(targetBytes);
-
-        // bulk-read all
-        si.readFully(targetBytes);
-        assertArrayEquals(pbrBytesWithOffset, targetBytes);
-    }
-
-    public void testRandomReads() throws IOException {
-        int length = randomIntBetween(10, scaledRandomIntBetween(PAGE_SIZE * 2, PAGE_SIZE * 20));
-        BytesReference pbr = getRandomizedPagedBytesReference(length);
-        StreamInput streamInput = pbr.streamInput();
-        BytesRefBuilder target = new BytesRefBuilder();
-        while (target.length() < pbr.length()) {
-            switch (randomIntBetween(0, 10)) {
-                case 6:
-                case 5:
-                    target.append(new BytesRef(new byte[]{streamInput.readByte()}));
-                    break;
-                case 4:
-                case 3:
-                    BytesRef bytesRef = streamInput.readBytesRef(scaledRandomIntBetween(1, pbr.length() - target.length()));
-                    target.append(bytesRef);
-                    break;
-                default:
-                    byte[] buffer = new byte[scaledRandomIntBetween(1, pbr.length() - target.length())];
-                    int offset = scaledRandomIntBetween(0, buffer.length - 1);
-                    int read = streamInput.read(buffer, offset, buffer.length - offset);
-                    target.append(new BytesRef(buffer, offset, read));
-                    break;
-            }
-        }
-        assertEquals(pbr.length(), target.length());
-        BytesRef targetBytes = target.get();
-        assertArrayEquals(pbr.toBytes(), Arrays.copyOfRange(targetBytes.bytes, targetBytes.offset, targetBytes.length));
-    }
-
-    public void testSliceStreamInput() throws IOException {
-        int length = randomIntBetween(10, scaledRandomIntBetween(PAGE_SIZE * 2, PAGE_SIZE * 20));
-        BytesReference pbr = getRandomizedPagedBytesReference(length);
-
-        // test stream input over slice (upper half of original)
-        int sliceOffset = randomIntBetween(1, length / 2);
-        int sliceLength = length - sliceOffset;
-        BytesReference slice = pbr.slice(sliceOffset, sliceLength);
-        StreamInput sliceInput = slice.streamInput();
-
-        // single reads
-        assertEquals(slice.get(0), sliceInput.readByte());
-        assertEquals(slice.get(1), sliceInput.readByte());
-        assertEquals(slice.get(2), sliceInput.readByte());
-
-        // reset the slice stream for bulk reading
-        sliceInput.reset();
-
-        // bulk read
-        byte[] sliceBytes = new byte[sliceLength];
-        sliceInput.readFully(sliceBytes);
-
-        // compare slice content with upper half of original
-        byte[] pbrSliceBytes = Arrays.copyOfRange(pbr.toBytes(), sliceOffset, length);
-        assertArrayEquals(pbrSliceBytes, sliceBytes);
-
-        // compare slice bytes with bytes read from slice via streamInput :D
-        byte[] sliceToBytes = slice.toBytes();
-        assertEquals(sliceBytes.length, sliceToBytes.length);
-        assertArrayEquals(sliceBytes, sliceToBytes);
-
-        sliceInput.reset();
-        byte[] buffer = new byte[sliceLength + scaledRandomIntBetween(1, 100)];
-        int offset = scaledRandomIntBetween(0, Math.max(1, buffer.length - sliceLength - 1));
-        int read = sliceInput.read(buffer, offset, sliceLength / 2);
-        sliceInput.read(buffer, offset + read, sliceLength);
-        assertArrayEquals(sliceBytes, Arrays.copyOfRange(buffer, offset, offset + sliceLength));
-    }
-
-    public void testWriteToOutputStream() throws IOException {
-        int length = randomIntBetween(10, PAGE_SIZE * 4);
-        BytesReference pbr = getRandomizedPagedBytesReference(length);
-        BytesStreamOutput out = new BytesStreamOutput();
-        pbr.writeTo(out);
-        assertEquals(pbr.length(), out.size());
-        assertArrayEquals(pbr.toBytes(), out.bytes().toBytes());
-        out.close();
-    }
-
-    public void testWriteToChannel() throws IOException {
-        int length = randomIntBetween(10, PAGE_SIZE * 4);
-        BytesReference pbr = getRandomizedPagedBytesReference(length);
-        Path tFile = createTempFile();
-        try (FileChannel channel = FileChannel.open(tFile, StandardOpenOption.WRITE)) {
-            pbr.writeTo(channel);
-            assertEquals(pbr.length(), channel.position());
-        }
-        assertArrayEquals(pbr.toBytes(), Files.readAllBytes(tFile));
-    }
-
-    public void testSliceWriteToOutputStream() throws IOException {
-        int length = randomIntBetween(10, PAGE_SIZE * randomIntBetween(2, 5));
-        BytesReference pbr = getRandomizedPagedBytesReference(length);
-        int sliceOffset = randomIntBetween(1, length / 2);
-        int sliceLength = length - sliceOffset;
-        BytesReference slice = pbr.slice(sliceOffset, sliceLength);
-        BytesStreamOutput sliceOut = new BytesStreamOutput(sliceLength);
-        slice.writeTo(sliceOut);
-        assertEquals(slice.length(), sliceOut.size());
-        assertArrayEquals(slice.toBytes(), sliceOut.bytes().toBytes());
-        sliceOut.close();
-    }
-
-    public void testSliceWriteToChannel() throws IOException {
-        int length = randomIntBetween(10, PAGE_SIZE * randomIntBetween(2, 5));
-        BytesReference pbr = getRandomizedPagedBytesReference(length);
-        int sliceOffset = randomIntBetween(1, length / 2);
-        int sliceLength = length - sliceOffset;
-        BytesReference slice = pbr.slice(sliceOffset, sliceLength);
-        Path tFile = createTempFile();
-        try (FileChannel channel = FileChannel.open(tFile, StandardOpenOption.WRITE)) {
-            slice.writeTo(channel);
-            assertEquals(slice.length(), channel.position());
-        }
-        assertArrayEquals(slice.toBytes(), Files.readAllBytes(tFile));
-    }
-
-    public void testToBytes() {
-        int[] sizes = {0, randomInt(PAGE_SIZE), PAGE_SIZE, randomIntBetween(2, PAGE_SIZE * randomIntBetween(2, 5))};
-
-        for (int i = 0; i < sizes.length; i++) {
-            BytesReference pbr = getRandomizedPagedBytesReference(sizes[i]);
-            byte[] bytes = pbr.toBytes();
-            assertEquals(sizes[i], bytes.length);
-            // verify that toBytes() is cheap for small payloads
-            if (sizes[i] <= PAGE_SIZE) {
-                assertSame(bytes, pbr.toBytes());
-            } else {
-                assertNotSame(bytes, pbr.toBytes());
-            }
-        }
-    }
-
-    public void testToBytesArraySharedPage() {
-        int length = randomIntBetween(10, PAGE_SIZE);
-        BytesReference pbr = getRandomizedPagedBytesReference(length);
-        BytesArray ba = pbr.toBytesArray();
-        BytesArray ba2 = pbr.toBytesArray();
-        assertNotNull(ba);
-        assertNotNull(ba2);
-        assertEquals(pbr.length(), ba.length());
-        assertEquals(ba.length(), ba2.length());
-        // single-page optimization
-        assertSame(ba.array(), ba2.array());
-    }
-
-    public void testToBytesArrayMaterializedPages() {
-        // we need a length != (n * pagesize) to avoid page sharing at boundaries
-        int length = 0;
-        while ((length % PAGE_SIZE) == 0) {
-            length = randomIntBetween(PAGE_SIZE, PAGE_SIZE * randomIntBetween(2, 5));
-        }
-        BytesReference pbr = getRandomizedPagedBytesReference(length);
-        BytesArray ba = pbr.toBytesArray();
-        BytesArray ba2 = pbr.toBytesArray();
-        assertNotNull(ba);
-        assertNotNull(ba2);
-        assertEquals(pbr.length(), ba.length());
-        assertEquals(ba.length(), ba2.length());
-        // ensure no single-page optimization
-        assertNotSame(ba.array(), ba2.array());
-    }
-
-    public void testCopyBytesArray() {
-        // small PBR which would normally share the first page
-        int length = randomIntBetween(10, PAGE_SIZE);
-        BytesReference pbr = getRandomizedPagedBytesReference(length);
-        BytesArray ba = pbr.copyBytesArray();
-        BytesArray ba2 = pbr.copyBytesArray();
-        assertNotNull(ba);
-        assertNotSame(ba, ba2);
-        assertNotSame(ba.array(), ba2.array());
-    }
-
-    public void testSliceCopyBytesArray() {
-        int length = randomIntBetween(10, PAGE_SIZE * randomIntBetween(2, 8));
-        BytesReference pbr = getRandomizedPagedBytesReference(length);
-        int sliceOffset = randomIntBetween(0, pbr.length());
-        int sliceLength = randomIntBetween(pbr.length() - sliceOffset, pbr.length() - sliceOffset);
-        BytesReference slice = pbr.slice(sliceOffset, sliceLength);
-
-        BytesArray ba1 = slice.copyBytesArray();
-        BytesArray ba2 = slice.copyBytesArray();
-        assertNotNull(ba1);
-        assertNotNull(ba2);
-        assertNotSame(ba1.array(), ba2.array());
-        assertArrayEquals(slice.toBytes(), ba1.array());
-        assertArrayEquals(slice.toBytes(), ba2.array());
-        assertArrayEquals(ba1.array(), ba2.array());
-    }
-
-    public void testToChannelBuffer() {
-        int length = randomIntBetween(10, PAGE_SIZE * randomIntBetween(2, 8));
-        BytesReference pbr = getRandomizedPagedBytesReference(length);
-        ChannelBuffer cb = pbr.toChannelBuffer();
-        assertNotNull(cb);
-        byte[] bufferBytes = new byte[length];
-        cb.getBytes(0, bufferBytes);
-        assertArrayEquals(pbr.toBytes(), bufferBytes);
-    }
-
-    public void testEmptyToChannelBuffer() {
-        BytesReference pbr = getRandomizedPagedBytesReference(0);
-        ChannelBuffer cb = pbr.toChannelBuffer();
-        assertNotNull(cb);
-        assertEquals(0, pbr.length());
-        assertEquals(0, cb.capacity());
-    }
-
-    public void testSliceToChannelBuffer() {
-        int length = randomIntBetween(10, PAGE_SIZE * randomIntBetween(2, 8));
-        BytesReference pbr = getRandomizedPagedBytesReference(length);
-        int sliceOffset = randomIntBetween(0, pbr.length());
-        int sliceLength = randomIntBetween(pbr.length() - sliceOffset, pbr.length() - sliceOffset);
-        BytesReference slice = pbr.slice(sliceOffset, sliceLength);
-        ChannelBuffer cbSlice = slice.toChannelBuffer();
-        assertNotNull(cbSlice);
-        byte[] sliceBufferBytes = new byte[sliceLength];
-        cbSlice.getBytes(0, sliceBufferBytes);
-        assertArrayEquals(slice.toBytes(), sliceBufferBytes);
-    }
-
-    public void testHasArray() {
-        int length = randomIntBetween(10, PAGE_SIZE * randomIntBetween(1, 3));
-        BytesReference pbr = getRandomizedPagedBytesReference(length);
-        // must return true for <= pagesize
-        assertEquals(length <= PAGE_SIZE, pbr.hasArray());
-    }
-
-    public void testArray() {
-        int[] sizes = {0, randomInt(PAGE_SIZE), PAGE_SIZE, randomIntBetween(2, PAGE_SIZE * randomIntBetween(2, 5))};
-
-        for (int i = 0; i < sizes.length; i++) {
-            BytesReference pbr = getRandomizedPagedBytesReference(sizes[i]);
-            // verify that array() is cheap for small payloads
-            if (sizes[i] <= PAGE_SIZE) {
-                byte[] array = pbr.array();
-                assertNotNull(array);
-                assertEquals(sizes[i], array.length);
-                assertSame(array, pbr.array());
-            } else {
-                try {
-                    pbr.array();
-                    fail("expected IllegalStateException");
-                } catch (IllegalStateException isx) {
-                    // expected
-                }
-            }
-        }
-    }
-
-    public void testArrayOffset() {
-        int length = randomInt(PAGE_SIZE * randomIntBetween(2, 5));
-        BytesReference pbr = getRandomizedPagedBytesReference(length);
-        if (pbr.hasArray()) {
-            assertEquals(0, pbr.arrayOffset());
-        } else {
-            try {
-                pbr.arrayOffset();
-                fail("expected IllegalStateException");
-            } catch (IllegalStateException ise) {
-                // expected
-            }
-        }
-    }
-
-    public void testSliceArrayOffset() {
-        int length = randomInt(PAGE_SIZE * randomIntBetween(2, 5));
-        BytesReference pbr = getRandomizedPagedBytesReference(length);
-        int sliceOffset = randomIntBetween(0, pbr.length());
-        int sliceLength = randomIntBetween(pbr.length() - sliceOffset, pbr.length() - sliceOffset);
-        BytesReference slice = pbr.slice(sliceOffset, sliceLength);
-        if (slice.hasArray()) {
-            assertEquals(sliceOffset, slice.arrayOffset());
-        } else {
-            try {
-                slice.arrayOffset();
-                fail("expected IllegalStateException");
-            } catch (IllegalStateException ise) {
-                // expected
-            }
-        }
-    }
-
-    public void testToUtf8() throws IOException {
-        // test empty
-        BytesReference pbr = getRandomizedPagedBytesReference(0);
-        assertEquals("", pbr.toUtf8());
-        // TODO: good way to test?
-    }
-
-    public void testToBytesRef() {
-        int length = randomIntBetween(0, PAGE_SIZE);
-        BytesReference pbr = getRandomizedPagedBytesReference(length);
-        BytesRef ref = pbr.toBytesRef();
-        assertNotNull(ref);
-        assertEquals(pbr.arrayOffset(), ref.offset);
-        assertEquals(pbr.length(), ref.length);
-    }
-
-    public void testSliceToBytesRef() {
-        int length = randomIntBetween(0, PAGE_SIZE);
-        BytesReference pbr = getRandomizedPagedBytesReference(length);
-        // get a BytesRef from a slice
-        int sliceOffset = randomIntBetween(0, pbr.length());
-        int sliceLength = randomIntBetween(pbr.length() - sliceOffset, pbr.length() - sliceOffset);
-        BytesRef sliceRef = pbr.slice(sliceOffset, sliceLength).toBytesRef();
-        // note that these are only true if we have <= than a page, otherwise offset/length are shifted
-        assertEquals(sliceOffset, sliceRef.offset);
-        assertEquals(sliceLength, sliceRef.length);
-    }
-
-    public void testCopyBytesRef() {
-        int length = randomIntBetween(0, PAGE_SIZE * randomIntBetween(2, 5));
-        BytesReference pbr = getRandomizedPagedBytesReference(length);
-        BytesRef ref = pbr.copyBytesRef();
-        assertNotNull(ref);
-        assertEquals(pbr.length(), ref.length);
-    }
-
-    public void testHashCode() {
-        // empty content must have hash 1 (JDK compat)
-        BytesReference pbr = getRandomizedPagedBytesReference(0);
-        assertEquals(Arrays.hashCode(BytesRef.EMPTY_BYTES), pbr.hashCode());
-
-        // test with content
-        pbr = getRandomizedPagedBytesReference(randomIntBetween(0, PAGE_SIZE * randomIntBetween(2, 5)));
-        int jdkHash = Arrays.hashCode(pbr.toBytes());
-        int pbrHash = pbr.hashCode();
-        assertEquals(jdkHash, pbrHash);
-
-        // test hashes of slices
-        int sliceFrom = randomIntBetween(0, pbr.length());
-        int sliceLength = randomIntBetween(pbr.length() - sliceFrom, pbr.length() - sliceFrom);
-        BytesReference slice = pbr.slice(sliceFrom, sliceLength);
-        int sliceJdkHash = Arrays.hashCode(slice.toBytes());
-        int sliceHash = slice.hashCode();
-        assertEquals(sliceJdkHash, sliceHash);
-    }
-
-    public void testEquals() {
-        int length = randomIntBetween(100, PAGE_SIZE * randomIntBetween(2, 5));
-        ByteArray ba1 = bigarrays.newByteArray(length, false);
-        ByteArray ba2 = bigarrays.newByteArray(length, false);
-
-        // copy contents
-        for (long i = 0; i < length; i++) {
-            ba2.set(i, ba1.get(i));
-        }
-
-        // get refs & compare
-        BytesReference pbr = new PagedBytesReference(bigarrays, ba1, length);
-        BytesReference pbr2 = new PagedBytesReference(bigarrays, ba2, length);
-        assertEquals(pbr, pbr2);
-    }
-
-    public void testEqualsPeerClass() {
-        int length = randomIntBetween(100, PAGE_SIZE * randomIntBetween(2, 5));
-        BytesReference pbr = getRandomizedPagedBytesReference(length);
-        BytesReference ba = new BytesArray(pbr.toBytes());
-        assertEquals(pbr, ba);
-    }
-
-    public void testSliceEquals() {
-        int length = randomIntBetween(100, PAGE_SIZE * randomIntBetween(2, 5));
-        ByteArray ba1 = bigarrays.newByteArray(length, false);
-        BytesReference pbr = new PagedBytesReference(bigarrays, ba1, length);
-
-        // test equality of slices
-        int sliceFrom = randomIntBetween(0, pbr.length());
-        int sliceLength = randomIntBetween(pbr.length() - sliceFrom, pbr.length() - sliceFrom);
-        BytesReference slice1 = pbr.slice(sliceFrom, sliceLength);
-        BytesReference slice2 = pbr.slice(sliceFrom, sliceLength);
-        assertArrayEquals(slice1.toBytes(), slice2.toBytes());
-
-        // test a slice with same offset but different length,
-        // unless randomized testing gave us a 0-length slice.
-        if (sliceLength > 0) {
-            BytesReference slice3 = pbr.slice(sliceFrom, sliceLength / 2);
-            assertFalse(Arrays.equals(slice1.toBytes(), slice3.toBytes()));
-        }
-    }
-
-    private BytesReference getRandomizedPagedBytesReference(int length) {
-        // we know bytes stream output always creates a paged bytes reference, we use it to create randomized content
-        ReleasableBytesStreamOutput out = new ReleasableBytesStreamOutput(length, bigarrays);
-        try {
-            for (int i = 0; i < length; i++) {
-                out.writeByte((byte) getRandom().nextInt(1 << 8));
-            }
-        } catch (IOException e) {
-            fail("should not happen " + e.getMessage());
-        }
-        assertThat(out.size(), Matchers.equalTo(length));
-        BytesReference ref = out.bytes();
-        assertThat(ref.length(), Matchers.equalTo(length));
-        assertThat(ref, Matchers.instanceOf(PagedBytesReference.class));
-        return ref;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/common/bytes/PagedBytesReferenceTests.java b/core/src/test/java/org/elasticsearch/common/bytes/PagedBytesReferenceTests.java
new file mode 100644
index 0000000..802ea7c
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/bytes/PagedBytesReferenceTests.java
@@ -0,0 +1,582 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.bytes;
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefBuilder;
+import org.elasticsearch.common.io.stream.BytesStreamOutput;
+import org.elasticsearch.common.io.stream.ReleasableBytesStreamOutput;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.util.BigArrays;
+import org.elasticsearch.common.util.ByteArray;
+import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
+import org.elasticsearch.test.ESTestCase;
+import org.hamcrest.Matchers;
+import org.jboss.netty.buffer.ChannelBuffer;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import java.io.EOFException;
+import java.io.IOException;
+import java.nio.channels.FileChannel;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.StandardOpenOption;
+import java.util.Arrays;
+
+public class PagedBytesReferenceTests extends ESTestCase {
+
+    private static final int PAGE_SIZE = BigArrays.BYTE_PAGE_SIZE;
+
+    private BigArrays bigarrays;
+
+    @Override
+    @Before
+    public void setUp() throws Exception {
+        super.setUp();
+        bigarrays = new BigArrays(null, new NoneCircuitBreakerService());
+    }
+
+    @Override
+    @After
+    public void tearDown() throws Exception {
+        super.tearDown();
+    }
+
+    @Test
+    public void testGet() {
+        int length = randomIntBetween(1, PAGE_SIZE * 3);
+        BytesReference pbr = getRandomizedPagedBytesReference(length);
+        int sliceOffset = randomIntBetween(0, length / 2);
+        int sliceLength = Math.max(1, length - sliceOffset - 1);
+        BytesReference slice = pbr.slice(sliceOffset, sliceLength);
+        assertEquals(pbr.get(sliceOffset), slice.get(0));
+        assertEquals(pbr.get(sliceOffset + sliceLength - 1), slice.get(sliceLength - 1));
+    }
+
+    public void testLength() {
+        int[] sizes = {0, randomInt(PAGE_SIZE), PAGE_SIZE, randomInt(PAGE_SIZE * 3)};
+
+        for (int i = 0; i < sizes.length; i++) {
+            BytesReference pbr = getRandomizedPagedBytesReference(sizes[i]);
+            assertEquals(sizes[i], pbr.length());
+        }
+    }
+
+    public void testSlice() {
+        int length = randomInt(PAGE_SIZE * 3);
+        BytesReference pbr = getRandomizedPagedBytesReference(length);
+        int sliceOffset = randomIntBetween(0, length / 2);
+        int sliceLength = Math.max(0, length - sliceOffset - 1);
+        BytesReference slice = pbr.slice(sliceOffset, sliceLength);
+        assertEquals(sliceLength, slice.length());
+
+        if (slice.hasArray()) {
+            assertEquals(sliceOffset, slice.arrayOffset());
+        } else {
+            try {
+                slice.arrayOffset();
+                fail("expected IllegalStateException");
+            } catch (IllegalStateException ise) {
+                // expected
+            }
+        }
+    }
+
+    public void testStreamInput() throws IOException {
+        int length = randomIntBetween(10, scaledRandomIntBetween(PAGE_SIZE * 2, PAGE_SIZE * 20));
+        BytesReference pbr = getRandomizedPagedBytesReference(length);
+        StreamInput si = pbr.streamInput();
+        assertNotNull(si);
+
+        // read single bytes one by one
+        assertEquals(pbr.get(0), si.readByte());
+        assertEquals(pbr.get(1), si.readByte());
+        assertEquals(pbr.get(2), si.readByte());
+
+        // reset the stream for bulk reading
+        si.reset();
+
+        // buffer for bulk reads
+        byte[] origBuf = new byte[length];
+        getRandom().nextBytes(origBuf);
+        byte[] targetBuf = Arrays.copyOf(origBuf, origBuf.length);
+
+        // bulk-read 0 bytes: must not modify buffer
+        si.readBytes(targetBuf, 0, 0);
+        assertEquals(origBuf[0], targetBuf[0]);
+        si.reset();
+
+        // read a few few bytes as ints
+        int bytesToRead = randomIntBetween(1, length / 2);
+        for (int i = 0; i < bytesToRead; i++) {
+            int b = si.read();
+            assertEquals(pbr.get(i), b);
+        }
+        si.reset();
+
+        // bulk-read all
+        si.readFully(targetBuf);
+        assertArrayEquals(pbr.toBytes(), targetBuf);
+
+        // continuing to read should now fail with EOFException
+        try {
+            si.readByte();
+            fail("expected EOF");
+        } catch (EOFException eof) {
+            // yay
+        }
+
+        // try to read more than the stream contains
+        si.reset();
+        try {
+            si.readBytes(targetBuf, 0, length * 2);
+            fail("expected IndexOutOfBoundsException: le > stream.length");
+        } catch (IndexOutOfBoundsException ioob) {
+            // expected
+        }
+    }
+
+    public void testStreamInputBulkReadWithOffset() throws IOException {
+        int length = randomIntBetween(10, scaledRandomIntBetween(PAGE_SIZE * 2, PAGE_SIZE * 20));
+        BytesReference pbr = getRandomizedPagedBytesReference(length);
+        StreamInput si = pbr.streamInput();
+        assertNotNull(si);
+
+        // read a bunch of single bytes one by one
+        int offset = randomIntBetween(1, length / 2);
+        for (int i = 0; i < offset; i++) {
+            assertEquals(pbr.get(i), si.readByte());
+        }
+
+        // now do NOT reset the stream - keep the stream's offset!
+
+        // buffer to compare remaining bytes against bulk read
+        byte[] pbrBytesWithOffset = Arrays.copyOfRange(pbr.toBytes(), offset, length);
+        // randomized target buffer to ensure no stale slots
+        byte[] targetBytes = new byte[pbrBytesWithOffset.length];
+        getRandom().nextBytes(targetBytes);
+
+        // bulk-read all
+        si.readFully(targetBytes);
+        assertArrayEquals(pbrBytesWithOffset, targetBytes);
+    }
+
+    public void testRandomReads() throws IOException {
+        int length = randomIntBetween(10, scaledRandomIntBetween(PAGE_SIZE * 2, PAGE_SIZE * 20));
+        BytesReference pbr = getRandomizedPagedBytesReference(length);
+        StreamInput streamInput = pbr.streamInput();
+        BytesRefBuilder target = new BytesRefBuilder();
+        while (target.length() < pbr.length()) {
+            switch (randomIntBetween(0, 10)) {
+                case 6:
+                case 5:
+                    target.append(new BytesRef(new byte[]{streamInput.readByte()}));
+                    break;
+                case 4:
+                case 3:
+                    BytesRef bytesRef = streamInput.readBytesRef(scaledRandomIntBetween(1, pbr.length() - target.length()));
+                    target.append(bytesRef);
+                    break;
+                default:
+                    byte[] buffer = new byte[scaledRandomIntBetween(1, pbr.length() - target.length())];
+                    int offset = scaledRandomIntBetween(0, buffer.length - 1);
+                    int read = streamInput.read(buffer, offset, buffer.length - offset);
+                    target.append(new BytesRef(buffer, offset, read));
+                    break;
+            }
+        }
+        assertEquals(pbr.length(), target.length());
+        BytesRef targetBytes = target.get();
+        assertArrayEquals(pbr.toBytes(), Arrays.copyOfRange(targetBytes.bytes, targetBytes.offset, targetBytes.length));
+    }
+
+    public void testSliceStreamInput() throws IOException {
+        int length = randomIntBetween(10, scaledRandomIntBetween(PAGE_SIZE * 2, PAGE_SIZE * 20));
+        BytesReference pbr = getRandomizedPagedBytesReference(length);
+
+        // test stream input over slice (upper half of original)
+        int sliceOffset = randomIntBetween(1, length / 2);
+        int sliceLength = length - sliceOffset;
+        BytesReference slice = pbr.slice(sliceOffset, sliceLength);
+        StreamInput sliceInput = slice.streamInput();
+
+        // single reads
+        assertEquals(slice.get(0), sliceInput.readByte());
+        assertEquals(slice.get(1), sliceInput.readByte());
+        assertEquals(slice.get(2), sliceInput.readByte());
+
+        // reset the slice stream for bulk reading
+        sliceInput.reset();
+
+        // bulk read
+        byte[] sliceBytes = new byte[sliceLength];
+        sliceInput.readFully(sliceBytes);
+
+        // compare slice content with upper half of original
+        byte[] pbrSliceBytes = Arrays.copyOfRange(pbr.toBytes(), sliceOffset, length);
+        assertArrayEquals(pbrSliceBytes, sliceBytes);
+
+        // compare slice bytes with bytes read from slice via streamInput :D
+        byte[] sliceToBytes = slice.toBytes();
+        assertEquals(sliceBytes.length, sliceToBytes.length);
+        assertArrayEquals(sliceBytes, sliceToBytes);
+
+        sliceInput.reset();
+        byte[] buffer = new byte[sliceLength + scaledRandomIntBetween(1, 100)];
+        int offset = scaledRandomIntBetween(0, Math.max(1, buffer.length - sliceLength - 1));
+        int read = sliceInput.read(buffer, offset, sliceLength / 2);
+        sliceInput.read(buffer, offset + read, sliceLength);
+        assertArrayEquals(sliceBytes, Arrays.copyOfRange(buffer, offset, offset + sliceLength));
+    }
+
+    public void testWriteToOutputStream() throws IOException {
+        int length = randomIntBetween(10, PAGE_SIZE * 4);
+        BytesReference pbr = getRandomizedPagedBytesReference(length);
+        BytesStreamOutput out = new BytesStreamOutput();
+        pbr.writeTo(out);
+        assertEquals(pbr.length(), out.size());
+        assertArrayEquals(pbr.toBytes(), out.bytes().toBytes());
+        out.close();
+    }
+
+    public void testWriteToChannel() throws IOException {
+        int length = randomIntBetween(10, PAGE_SIZE * 4);
+        BytesReference pbr = getRandomizedPagedBytesReference(length);
+        Path tFile = createTempFile();
+        try (FileChannel channel = FileChannel.open(tFile, StandardOpenOption.WRITE)) {
+            pbr.writeTo(channel);
+            assertEquals(pbr.length(), channel.position());
+        }
+        assertArrayEquals(pbr.toBytes(), Files.readAllBytes(tFile));
+    }
+
+    public void testSliceWriteToOutputStream() throws IOException {
+        int length = randomIntBetween(10, PAGE_SIZE * randomIntBetween(2, 5));
+        BytesReference pbr = getRandomizedPagedBytesReference(length);
+        int sliceOffset = randomIntBetween(1, length / 2);
+        int sliceLength = length - sliceOffset;
+        BytesReference slice = pbr.slice(sliceOffset, sliceLength);
+        BytesStreamOutput sliceOut = new BytesStreamOutput(sliceLength);
+        slice.writeTo(sliceOut);
+        assertEquals(slice.length(), sliceOut.size());
+        assertArrayEquals(slice.toBytes(), sliceOut.bytes().toBytes());
+        sliceOut.close();
+    }
+
+    public void testSliceWriteToChannel() throws IOException {
+        int length = randomIntBetween(10, PAGE_SIZE * randomIntBetween(2, 5));
+        BytesReference pbr = getRandomizedPagedBytesReference(length);
+        int sliceOffset = randomIntBetween(1, length / 2);
+        int sliceLength = length - sliceOffset;
+        BytesReference slice = pbr.slice(sliceOffset, sliceLength);
+        Path tFile = createTempFile();
+        try (FileChannel channel = FileChannel.open(tFile, StandardOpenOption.WRITE)) {
+            slice.writeTo(channel);
+            assertEquals(slice.length(), channel.position());
+        }
+        assertArrayEquals(slice.toBytes(), Files.readAllBytes(tFile));
+    }
+
+    public void testToBytes() {
+        int[] sizes = {0, randomInt(PAGE_SIZE), PAGE_SIZE, randomIntBetween(2, PAGE_SIZE * randomIntBetween(2, 5))};
+
+        for (int i = 0; i < sizes.length; i++) {
+            BytesReference pbr = getRandomizedPagedBytesReference(sizes[i]);
+            byte[] bytes = pbr.toBytes();
+            assertEquals(sizes[i], bytes.length);
+            // verify that toBytes() is cheap for small payloads
+            if (sizes[i] <= PAGE_SIZE) {
+                assertSame(bytes, pbr.toBytes());
+            } else {
+                assertNotSame(bytes, pbr.toBytes());
+            }
+        }
+    }
+
+    public void testToBytesArraySharedPage() {
+        int length = randomIntBetween(10, PAGE_SIZE);
+        BytesReference pbr = getRandomizedPagedBytesReference(length);
+        BytesArray ba = pbr.toBytesArray();
+        BytesArray ba2 = pbr.toBytesArray();
+        assertNotNull(ba);
+        assertNotNull(ba2);
+        assertEquals(pbr.length(), ba.length());
+        assertEquals(ba.length(), ba2.length());
+        // single-page optimization
+        assertSame(ba.array(), ba2.array());
+    }
+
+    public void testToBytesArrayMaterializedPages() {
+        // we need a length != (n * pagesize) to avoid page sharing at boundaries
+        int length = 0;
+        while ((length % PAGE_SIZE) == 0) {
+            length = randomIntBetween(PAGE_SIZE, PAGE_SIZE * randomIntBetween(2, 5));
+        }
+        BytesReference pbr = getRandomizedPagedBytesReference(length);
+        BytesArray ba = pbr.toBytesArray();
+        BytesArray ba2 = pbr.toBytesArray();
+        assertNotNull(ba);
+        assertNotNull(ba2);
+        assertEquals(pbr.length(), ba.length());
+        assertEquals(ba.length(), ba2.length());
+        // ensure no single-page optimization
+        assertNotSame(ba.array(), ba2.array());
+    }
+
+    public void testCopyBytesArray() {
+        // small PBR which would normally share the first page
+        int length = randomIntBetween(10, PAGE_SIZE);
+        BytesReference pbr = getRandomizedPagedBytesReference(length);
+        BytesArray ba = pbr.copyBytesArray();
+        BytesArray ba2 = pbr.copyBytesArray();
+        assertNotNull(ba);
+        assertNotSame(ba, ba2);
+        assertNotSame(ba.array(), ba2.array());
+    }
+
+    public void testSliceCopyBytesArray() {
+        int length = randomIntBetween(10, PAGE_SIZE * randomIntBetween(2, 8));
+        BytesReference pbr = getRandomizedPagedBytesReference(length);
+        int sliceOffset = randomIntBetween(0, pbr.length());
+        int sliceLength = randomIntBetween(pbr.length() - sliceOffset, pbr.length() - sliceOffset);
+        BytesReference slice = pbr.slice(sliceOffset, sliceLength);
+
+        BytesArray ba1 = slice.copyBytesArray();
+        BytesArray ba2 = slice.copyBytesArray();
+        assertNotNull(ba1);
+        assertNotNull(ba2);
+        assertNotSame(ba1.array(), ba2.array());
+        assertArrayEquals(slice.toBytes(), ba1.array());
+        assertArrayEquals(slice.toBytes(), ba2.array());
+        assertArrayEquals(ba1.array(), ba2.array());
+    }
+
+    public void testToChannelBuffer() {
+        int length = randomIntBetween(10, PAGE_SIZE * randomIntBetween(2, 8));
+        BytesReference pbr = getRandomizedPagedBytesReference(length);
+        ChannelBuffer cb = pbr.toChannelBuffer();
+        assertNotNull(cb);
+        byte[] bufferBytes = new byte[length];
+        cb.getBytes(0, bufferBytes);
+        assertArrayEquals(pbr.toBytes(), bufferBytes);
+    }
+
+    public void testEmptyToChannelBuffer() {
+        BytesReference pbr = getRandomizedPagedBytesReference(0);
+        ChannelBuffer cb = pbr.toChannelBuffer();
+        assertNotNull(cb);
+        assertEquals(0, pbr.length());
+        assertEquals(0, cb.capacity());
+    }
+
+    public void testSliceToChannelBuffer() {
+        int length = randomIntBetween(10, PAGE_SIZE * randomIntBetween(2, 8));
+        BytesReference pbr = getRandomizedPagedBytesReference(length);
+        int sliceOffset = randomIntBetween(0, pbr.length());
+        int sliceLength = randomIntBetween(pbr.length() - sliceOffset, pbr.length() - sliceOffset);
+        BytesReference slice = pbr.slice(sliceOffset, sliceLength);
+        ChannelBuffer cbSlice = slice.toChannelBuffer();
+        assertNotNull(cbSlice);
+        byte[] sliceBufferBytes = new byte[sliceLength];
+        cbSlice.getBytes(0, sliceBufferBytes);
+        assertArrayEquals(slice.toBytes(), sliceBufferBytes);
+    }
+
+    public void testHasArray() {
+        int length = randomIntBetween(10, PAGE_SIZE * randomIntBetween(1, 3));
+        BytesReference pbr = getRandomizedPagedBytesReference(length);
+        // must return true for <= pagesize
+        assertEquals(length <= PAGE_SIZE, pbr.hasArray());
+    }
+
+    public void testArray() {
+        int[] sizes = {0, randomInt(PAGE_SIZE), PAGE_SIZE, randomIntBetween(2, PAGE_SIZE * randomIntBetween(2, 5))};
+
+        for (int i = 0; i < sizes.length; i++) {
+            BytesReference pbr = getRandomizedPagedBytesReference(sizes[i]);
+            // verify that array() is cheap for small payloads
+            if (sizes[i] <= PAGE_SIZE) {
+                byte[] array = pbr.array();
+                assertNotNull(array);
+                assertEquals(sizes[i], array.length);
+                assertSame(array, pbr.array());
+            } else {
+                try {
+                    pbr.array();
+                    fail("expected IllegalStateException");
+                } catch (IllegalStateException isx) {
+                    // expected
+                }
+            }
+        }
+    }
+
+    public void testArrayOffset() {
+        int length = randomInt(PAGE_SIZE * randomIntBetween(2, 5));
+        BytesReference pbr = getRandomizedPagedBytesReference(length);
+        if (pbr.hasArray()) {
+            assertEquals(0, pbr.arrayOffset());
+        } else {
+            try {
+                pbr.arrayOffset();
+                fail("expected IllegalStateException");
+            } catch (IllegalStateException ise) {
+                // expected
+            }
+        }
+    }
+
+    public void testSliceArrayOffset() {
+        int length = randomInt(PAGE_SIZE * randomIntBetween(2, 5));
+        BytesReference pbr = getRandomizedPagedBytesReference(length);
+        int sliceOffset = randomIntBetween(0, pbr.length());
+        int sliceLength = randomIntBetween(pbr.length() - sliceOffset, pbr.length() - sliceOffset);
+        BytesReference slice = pbr.slice(sliceOffset, sliceLength);
+        if (slice.hasArray()) {
+            assertEquals(sliceOffset, slice.arrayOffset());
+        } else {
+            try {
+                slice.arrayOffset();
+                fail("expected IllegalStateException");
+            } catch (IllegalStateException ise) {
+                // expected
+            }
+        }
+    }
+
+    public void testToUtf8() throws IOException {
+        // test empty
+        BytesReference pbr = getRandomizedPagedBytesReference(0);
+        assertEquals("", pbr.toUtf8());
+        // TODO: good way to test?
+    }
+
+    public void testToBytesRef() {
+        int length = randomIntBetween(0, PAGE_SIZE);
+        BytesReference pbr = getRandomizedPagedBytesReference(length);
+        BytesRef ref = pbr.toBytesRef();
+        assertNotNull(ref);
+        assertEquals(pbr.arrayOffset(), ref.offset);
+        assertEquals(pbr.length(), ref.length);
+    }
+
+    public void testSliceToBytesRef() {
+        int length = randomIntBetween(0, PAGE_SIZE);
+        BytesReference pbr = getRandomizedPagedBytesReference(length);
+        // get a BytesRef from a slice
+        int sliceOffset = randomIntBetween(0, pbr.length());
+        int sliceLength = randomIntBetween(pbr.length() - sliceOffset, pbr.length() - sliceOffset);
+        BytesRef sliceRef = pbr.slice(sliceOffset, sliceLength).toBytesRef();
+        // note that these are only true if we have <= than a page, otherwise offset/length are shifted
+        assertEquals(sliceOffset, sliceRef.offset);
+        assertEquals(sliceLength, sliceRef.length);
+    }
+
+    public void testCopyBytesRef() {
+        int length = randomIntBetween(0, PAGE_SIZE * randomIntBetween(2, 5));
+        BytesReference pbr = getRandomizedPagedBytesReference(length);
+        BytesRef ref = pbr.copyBytesRef();
+        assertNotNull(ref);
+        assertEquals(pbr.length(), ref.length);
+    }
+
+    public void testHashCode() {
+        // empty content must have hash 1 (JDK compat)
+        BytesReference pbr = getRandomizedPagedBytesReference(0);
+        assertEquals(Arrays.hashCode(BytesRef.EMPTY_BYTES), pbr.hashCode());
+
+        // test with content
+        pbr = getRandomizedPagedBytesReference(randomIntBetween(0, PAGE_SIZE * randomIntBetween(2, 5)));
+        int jdkHash = Arrays.hashCode(pbr.toBytes());
+        int pbrHash = pbr.hashCode();
+        assertEquals(jdkHash, pbrHash);
+
+        // test hashes of slices
+        int sliceFrom = randomIntBetween(0, pbr.length());
+        int sliceLength = randomIntBetween(pbr.length() - sliceFrom, pbr.length() - sliceFrom);
+        BytesReference slice = pbr.slice(sliceFrom, sliceLength);
+        int sliceJdkHash = Arrays.hashCode(slice.toBytes());
+        int sliceHash = slice.hashCode();
+        assertEquals(sliceJdkHash, sliceHash);
+    }
+
+    public void testEquals() {
+        int length = randomIntBetween(100, PAGE_SIZE * randomIntBetween(2, 5));
+        ByteArray ba1 = bigarrays.newByteArray(length, false);
+        ByteArray ba2 = bigarrays.newByteArray(length, false);
+
+        // copy contents
+        for (long i = 0; i < length; i++) {
+            ba2.set(i, ba1.get(i));
+        }
+
+        // get refs & compare
+        BytesReference pbr = new PagedBytesReference(bigarrays, ba1, length);
+        BytesReference pbr2 = new PagedBytesReference(bigarrays, ba2, length);
+        assertEquals(pbr, pbr2);
+    }
+
+    public void testEqualsPeerClass() {
+        int length = randomIntBetween(100, PAGE_SIZE * randomIntBetween(2, 5));
+        BytesReference pbr = getRandomizedPagedBytesReference(length);
+        BytesReference ba = new BytesArray(pbr.toBytes());
+        assertEquals(pbr, ba);
+    }
+
+    public void testSliceEquals() {
+        int length = randomIntBetween(100, PAGE_SIZE * randomIntBetween(2, 5));
+        ByteArray ba1 = bigarrays.newByteArray(length, false);
+        BytesReference pbr = new PagedBytesReference(bigarrays, ba1, length);
+
+        // test equality of slices
+        int sliceFrom = randomIntBetween(0, pbr.length());
+        int sliceLength = randomIntBetween(pbr.length() - sliceFrom, pbr.length() - sliceFrom);
+        BytesReference slice1 = pbr.slice(sliceFrom, sliceLength);
+        BytesReference slice2 = pbr.slice(sliceFrom, sliceLength);
+        assertArrayEquals(slice1.toBytes(), slice2.toBytes());
+
+        // test a slice with same offset but different length,
+        // unless randomized testing gave us a 0-length slice.
+        if (sliceLength > 0) {
+            BytesReference slice3 = pbr.slice(sliceFrom, sliceLength / 2);
+            assertFalse(Arrays.equals(slice1.toBytes(), slice3.toBytes()));
+        }
+    }
+
+    private BytesReference getRandomizedPagedBytesReference(int length) {
+        // we know bytes stream output always creates a paged bytes reference, we use it to create randomized content
+        ReleasableBytesStreamOutput out = new ReleasableBytesStreamOutput(length, bigarrays);
+        try {
+            for (int i = 0; i < length; i++) {
+                out.writeByte((byte) getRandom().nextInt(1 << 8));
+            }
+        } catch (IOException e) {
+            fail("should not happen " + e.getMessage());
+        }
+        assertThat(out.size(), Matchers.equalTo(length));
+        BytesReference ref = out.bytes();
+        assertThat(ref.length(), Matchers.equalTo(length));
+        assertThat(ref, Matchers.instanceOf(PagedBytesReference.class));
+        return ref;
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/common/compress/AbstractCompressedStreamTestCase.java b/core/src/test/java/org/elasticsearch/common/compress/AbstractCompressedStreamTestCase.java
new file mode 100644
index 0000000..88f1527
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/compress/AbstractCompressedStreamTestCase.java
@@ -0,0 +1,434 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.compress;
+
+import org.apache.lucene.util.LineFileDocs;
+import org.apache.lucene.util.TestUtil;
+import org.elasticsearch.common.io.stream.ByteBufferStreamInput;
+import org.elasticsearch.common.io.stream.OutputStreamStreamOutput;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.test.ESTestCase;
+
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.nio.charset.StandardCharsets;
+import java.util.Random;
+import java.util.concurrent.CountDownLatch;
+
+/**
+ * Test streaming compression (e.g. used for recovery)
+ */
+public abstract class AbstractCompressedStreamTestCase extends ESTestCase {
+
+    private final Compressor compressor;
+
+    protected AbstractCompressedStreamTestCase(Compressor compressor) {
+        this.compressor = compressor;
+    }
+
+    public void testRandom() throws IOException {
+        Random r = getRandom();
+        for (int i = 0; i < 10; i++) {
+            byte bytes[] = new byte[TestUtil.nextInt(r, 1, 100000)];
+            r.nextBytes(bytes);
+            doTest(bytes);
+        }
+    }
+
+    public void testRandomThreads() throws Exception {
+        final Random r = getRandom();
+        int threadCount = TestUtil.nextInt(r, 2, 6);
+        Thread[] threads = new Thread[threadCount];
+        final CountDownLatch startingGun = new CountDownLatch(1);
+        for (int tid=0; tid < threadCount; tid++) {
+            final long seed = r.nextLong();
+            threads[tid] = new Thread() {
+                @Override
+                public void run() {
+                    try {
+                        Random r = new Random(seed);
+                        startingGun.await();
+                        for (int i = 0; i < 10; i++) {
+                            byte bytes[] = new byte[TestUtil.nextInt(r, 1, 100000)];
+                            r.nextBytes(bytes);
+                            doTest(bytes);
+                        }
+                    } catch (Exception e) {
+                        throw new RuntimeException(e);
+                    }
+                }
+            };
+            threads[tid].start();
+        }
+        startingGun.countDown();
+        for (Thread t : threads) {
+            t.join();
+        }
+    }
+
+    public void testLineDocs() throws IOException {
+        Random r = getRandom();
+        LineFileDocs lineFileDocs = new LineFileDocs(r);
+        for (int i = 0; i < 10; i++) {
+            int numDocs = TestUtil.nextInt(r, 1, 200);
+            ByteArrayOutputStream bos = new ByteArrayOutputStream();
+            for (int j = 0; j < numDocs; j++) {
+                String s = lineFileDocs.nextDoc().get("body");
+                bos.write(s.getBytes(StandardCharsets.UTF_8));
+            }
+            doTest(bos.toByteArray());
+        }
+        lineFileDocs.close();
+    }
+
+    public void testLineDocsThreads() throws Exception {
+        final Random r = getRandom();
+        int threadCount = TestUtil.nextInt(r, 2, 6);
+        Thread[] threads = new Thread[threadCount];
+        final CountDownLatch startingGun = new CountDownLatch(1);
+        for (int tid=0; tid < threadCount; tid++) {
+            final long seed = r.nextLong();
+            threads[tid] = new Thread() {
+                @Override
+                public void run() {
+                    try {
+                        Random r = new Random(seed);
+                        startingGun.await();
+                        LineFileDocs lineFileDocs = new LineFileDocs(r);
+                        for (int i = 0; i < 10; i++) {
+                            int numDocs = TestUtil.nextInt(r, 1, 200);
+                            ByteArrayOutputStream bos = new ByteArrayOutputStream();
+                            for (int j = 0; j < numDocs; j++) {
+                                String s = lineFileDocs.nextDoc().get("body");
+                                bos.write(s.getBytes(StandardCharsets.UTF_8));
+                            }
+                            doTest(bos.toByteArray());
+                        }
+                        lineFileDocs.close();
+                    } catch (Exception e) {
+                        throw new RuntimeException(e);
+                    }
+                }
+            };
+            threads[tid].start();
+        }
+        startingGun.countDown();
+        for (Thread t : threads) {
+            t.join();
+        }
+    }
+
+    public void testRepetitionsL() throws IOException {
+        Random r = getRandom();
+        for (int i = 0; i < 10; i++) {
+            int numLongs = TestUtil.nextInt(r, 1, 10000);
+            ByteArrayOutputStream bos = new ByteArrayOutputStream();
+            long theValue = r.nextLong();
+            for (int j = 0; j < numLongs; j++) {
+                if (r.nextInt(10) == 0) {
+                    theValue = r.nextLong();
+                }
+                bos.write((byte) (theValue >>> 56));
+                bos.write((byte) (theValue >>> 48));
+                bos.write((byte) (theValue >>> 40));
+                bos.write((byte) (theValue >>> 32));
+                bos.write((byte) (theValue >>> 24));
+                bos.write((byte) (theValue >>> 16));
+                bos.write((byte) (theValue >>> 8));
+                bos.write((byte) theValue);
+            }
+            doTest(bos.toByteArray());
+        }
+    }
+
+    public void testRepetitionsLThreads() throws Exception {
+        final Random r = getRandom();
+        int threadCount = TestUtil.nextInt(r, 2, 6);
+        Thread[] threads = new Thread[threadCount];
+        final CountDownLatch startingGun = new CountDownLatch(1);
+        for (int tid=0; tid < threadCount; tid++) {
+            final long seed = r.nextLong();
+            threads[tid] = new Thread() {
+                @Override
+                public void run() {
+                    try {
+                        Random r = new Random(seed);
+                        startingGun.await();
+                        for (int i = 0; i < 10; i++) {
+                            int numLongs = TestUtil.nextInt(r, 1, 10000);
+                            ByteArrayOutputStream bos = new ByteArrayOutputStream();
+                            long theValue = r.nextLong();
+                            for (int j = 0; j < numLongs; j++) {
+                                if (r.nextInt(10) == 0) {
+                                    theValue = r.nextLong();
+                                }
+                                bos.write((byte) (theValue >>> 56));
+                                bos.write((byte) (theValue >>> 48));
+                                bos.write((byte) (theValue >>> 40));
+                                bos.write((byte) (theValue >>> 32));
+                                bos.write((byte) (theValue >>> 24));
+                                bos.write((byte) (theValue >>> 16));
+                                bos.write((byte) (theValue >>> 8));
+                                bos.write((byte) theValue);
+                            }
+                            doTest(bos.toByteArray());
+                        }
+                    } catch (Exception e) {
+                        throw new RuntimeException(e);
+                    }
+                }
+            };
+            threads[tid].start();
+        }
+        startingGun.countDown();
+        for (Thread t : threads) {
+            t.join();
+        }
+    }
+
+    public void testRepetitionsI() throws IOException {
+        Random r = getRandom();
+        for (int i = 0; i < 10; i++) {
+            int numInts = TestUtil.nextInt(r, 1, 20000);
+            ByteArrayOutputStream bos = new ByteArrayOutputStream();
+            int theValue = r.nextInt();
+            for (int j = 0; j < numInts; j++) {
+                if (r.nextInt(10) == 0) {
+                    theValue = r.nextInt();
+                }
+                bos.write((byte) (theValue >>> 24));
+                bos.write((byte) (theValue >>> 16));
+                bos.write((byte) (theValue >>> 8));
+                bos.write((byte) theValue);
+            }
+            doTest(bos.toByteArray());
+        }
+    }
+
+    public void testRepetitionsIThreads() throws Exception {
+        final Random r = getRandom();
+        int threadCount = TestUtil.nextInt(r, 2, 6);
+        Thread[] threads = new Thread[threadCount];
+        final CountDownLatch startingGun = new CountDownLatch(1);
+        for (int tid=0; tid < threadCount; tid++) {
+            final long seed = r.nextLong();
+            threads[tid] = new Thread() {
+                @Override
+                public void run() {
+                    try {
+                        Random r = new Random(seed);
+                        startingGun.await();
+                        for (int i = 0; i < 10; i++) {
+                            int numInts = TestUtil.nextInt(r, 1, 20000);
+                            ByteArrayOutputStream bos = new ByteArrayOutputStream();
+                            int theValue = r.nextInt();
+                            for (int j = 0; j < numInts; j++) {
+                                if (r.nextInt(10) == 0) {
+                                    theValue = r.nextInt();
+                                }
+                                bos.write((byte) (theValue >>> 24));
+                                bos.write((byte) (theValue >>> 16));
+                                bos.write((byte) (theValue >>> 8));
+                                bos.write((byte) theValue);
+                            }
+                            doTest(bos.toByteArray());
+                        }
+                    } catch (Exception e) {
+                        throw new RuntimeException(e);
+                    }
+                }
+            };
+            threads[tid].start();
+        }
+        startingGun.countDown();
+        for (Thread t : threads) {
+            t.join();
+        }
+    }
+
+    public void testRepetitionsS() throws IOException {
+        Random r = getRandom();
+        for (int i = 0; i < 10; i++) {
+            int numShorts = TestUtil.nextInt(r, 1, 40000);
+            ByteArrayOutputStream bos = new ByteArrayOutputStream();
+            short theValue = (short) r.nextInt(65535);
+            for (int j = 0; j < numShorts; j++) {
+                if (r.nextInt(10) == 0) {
+                    theValue = (short) r.nextInt(65535);
+                }
+                bos.write((byte) (theValue >>> 8));
+                bos.write((byte) theValue);
+            }
+            doTest(bos.toByteArray());
+        }
+    }
+
+    public void testMixed() throws IOException {
+        Random r = getRandom();
+        LineFileDocs lineFileDocs = new LineFileDocs(r);
+        for (int i = 0; i < 2; ++i) {
+            ByteArrayOutputStream bos = new ByteArrayOutputStream();
+            int prevInt = r.nextInt();
+            long prevLong = r.nextLong();
+            while (bos.size() < 400000) {
+                switch (r.nextInt(4)) {
+                case 0:
+                    addInt(r, prevInt, bos);
+                    break;
+                case 1:
+                    addLong(r, prevLong, bos);
+                    break;
+                case 2:
+                    addString(lineFileDocs, bos);
+                    break;
+                case 3:
+                    addBytes(r, bos);
+                    break;
+                default:
+                    throw new IllegalStateException("Random is broken");
+                }
+            }
+            doTest(bos.toByteArray());
+        }
+    }
+
+    private void addLong(Random r, long prev, ByteArrayOutputStream bos) {
+        long theValue = prev;
+        if (r.nextInt(10) != 0) {
+            theValue = r.nextLong();
+        }
+        bos.write((byte) (theValue >>> 56));
+        bos.write((byte) (theValue >>> 48));
+        bos.write((byte) (theValue >>> 40));
+        bos.write((byte) (theValue >>> 32));
+        bos.write((byte) (theValue >>> 24));
+        bos.write((byte) (theValue >>> 16));
+        bos.write((byte) (theValue >>> 8));
+        bos.write((byte) theValue);
+    }
+
+    private void addInt(Random r, int prev, ByteArrayOutputStream bos) {
+        int theValue = prev;
+        if (r.nextInt(10) != 0) {
+            theValue = r.nextInt();
+        }
+        bos.write((byte) (theValue >>> 24));
+        bos.write((byte) (theValue >>> 16));
+        bos.write((byte) (theValue >>> 8));
+        bos.write((byte) theValue);
+    }
+
+    private void addString(LineFileDocs lineFileDocs, ByteArrayOutputStream bos) throws IOException {
+        String s = lineFileDocs.nextDoc().get("body");
+        bos.write(s.getBytes(StandardCharsets.UTF_8));
+    }
+
+    private void addBytes(Random r, ByteArrayOutputStream bos) throws IOException {
+        byte bytes[] = new byte[TestUtil.nextInt(r, 1, 10000)];
+        r.nextBytes(bytes);
+        bos.write(bytes);
+    }
+
+    public void testRepetitionsSThreads() throws Exception {
+        final Random r = getRandom();
+        int threadCount = TestUtil.nextInt(r, 2, 6);
+        Thread[] threads = new Thread[threadCount];
+        final CountDownLatch startingGun = new CountDownLatch(1);
+        for (int tid=0; tid < threadCount; tid++) {
+            final long seed = r.nextLong();
+            threads[tid] = new Thread() {
+                @Override
+                public void run() {
+                    try {
+                        Random r = new Random(seed);
+                        startingGun.await();
+                        for (int i = 0; i < 10; i++) {
+                            int numShorts = TestUtil.nextInt(r, 1, 40000);
+                            ByteArrayOutputStream bos = new ByteArrayOutputStream();
+                            short theValue = (short) r.nextInt(65535);
+                            for (int j = 0; j < numShorts; j++) {
+                                if (r.nextInt(10) == 0) {
+                                    theValue = (short) r.nextInt(65535);
+                                }
+                                bos.write((byte) (theValue >>> 8));
+                                bos.write((byte) theValue);
+                            }
+                            doTest(bos.toByteArray());
+                        }
+                    } catch (Exception e) {
+                        throw new RuntimeException(e);
+                    }
+                }
+            };
+            threads[tid].start();
+        }
+        startingGun.countDown();
+        for (Thread t : threads) {
+            t.join();
+        }
+    }
+
+    private void doTest(byte bytes[]) throws IOException {
+        ByteBuffer bb = ByteBuffer.wrap(bytes);
+        StreamInput rawIn = new ByteBufferStreamInput(bb);
+        Compressor c = compressor;
+
+        ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        OutputStreamStreamOutput rawOs = new OutputStreamStreamOutput(bos);
+        StreamOutput os = c.streamOutput(rawOs);
+
+        Random r = getRandom();
+        int bufferSize = r.nextBoolean() ? 65535 : TestUtil.nextInt(getRandom(), 1, 70000);
+        int prepadding = r.nextInt(70000);
+        int postpadding = r.nextInt(70000);
+        byte buffer[] = new byte[prepadding + bufferSize + postpadding];
+        r.nextBytes(buffer); // fill block completely with junk
+        int len;
+        while ((len = rawIn.read(buffer, prepadding, bufferSize)) != -1) {
+            os.write(buffer, prepadding, len);
+        }
+        os.close();
+        rawIn.close();
+
+        // now we have compressed byte array
+
+        byte compressed[] = bos.toByteArray();
+        ByteBuffer bb2 = ByteBuffer.wrap(compressed);
+        StreamInput compressedIn = new ByteBufferStreamInput(bb2);
+        StreamInput in = c.streamInput(compressedIn);
+
+        // randomize constants again
+        bufferSize = r.nextBoolean() ? 65535 : TestUtil.nextInt(getRandom(), 1, 70000);
+        prepadding = r.nextInt(70000);
+        postpadding = r.nextInt(70000);
+        buffer = new byte[prepadding + bufferSize + postpadding];
+        r.nextBytes(buffer); // fill block completely with junk
+
+        ByteArrayOutputStream uncompressedOut = new ByteArrayOutputStream();
+        while ((len = in.read(buffer, prepadding, bufferSize)) != -1) {
+            uncompressedOut.write(buffer, prepadding, len);
+        }
+        uncompressedOut.close();
+
+        assertArrayEquals(bytes, uncompressedOut.toByteArray());
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/common/compress/AbstractCompressedStreamTests.java b/core/src/test/java/org/elasticsearch/common/compress/AbstractCompressedStreamTests.java
deleted file mode 100644
index b5800f3..0000000
--- a/core/src/test/java/org/elasticsearch/common/compress/AbstractCompressedStreamTests.java
+++ /dev/null
@@ -1,434 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common.compress;
-
-import org.apache.lucene.util.LineFileDocs;
-import org.apache.lucene.util.TestUtil;
-import org.elasticsearch.common.io.stream.ByteBufferStreamInput;
-import org.elasticsearch.common.io.stream.OutputStreamStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.nio.ByteBuffer;
-import java.nio.charset.StandardCharsets;
-import java.util.Random;
-import java.util.concurrent.CountDownLatch;
-
-/**
- * Test streaming compression (e.g. used for recovery)
- */
-public abstract class AbstractCompressedStreamTests extends ESTestCase {
-
-    private final Compressor compressor;
-
-    protected AbstractCompressedStreamTests(Compressor compressor) {
-        this.compressor = compressor;
-    }
-
-    public void testRandom() throws IOException {
-        Random r = getRandom();
-        for (int i = 0; i < 10; i++) {
-            byte bytes[] = new byte[TestUtil.nextInt(r, 1, 100000)];
-            r.nextBytes(bytes);
-            doTest(bytes);
-        }
-    }
-
-    public void testRandomThreads() throws Exception {
-        final Random r = getRandom();
-        int threadCount = TestUtil.nextInt(r, 2, 6);
-        Thread[] threads = new Thread[threadCount];
-        final CountDownLatch startingGun = new CountDownLatch(1);
-        for (int tid=0; tid < threadCount; tid++) {
-            final long seed = r.nextLong();
-            threads[tid] = new Thread() {
-                @Override
-                public void run() {
-                    try {
-                        Random r = new Random(seed);
-                        startingGun.await();
-                        for (int i = 0; i < 10; i++) {
-                            byte bytes[] = new byte[TestUtil.nextInt(r, 1, 100000)];
-                            r.nextBytes(bytes);
-                            doTest(bytes);
-                        }
-                    } catch (Exception e) {
-                        throw new RuntimeException(e);
-                    }
-                }
-            };
-            threads[tid].start();
-        }
-        startingGun.countDown();
-        for (Thread t : threads) {
-            t.join();
-        }
-    }
-
-    public void testLineDocs() throws IOException {
-        Random r = getRandom();
-        LineFileDocs lineFileDocs = new LineFileDocs(r);
-        for (int i = 0; i < 10; i++) {
-            int numDocs = TestUtil.nextInt(r, 1, 200);
-            ByteArrayOutputStream bos = new ByteArrayOutputStream();
-            for (int j = 0; j < numDocs; j++) {
-                String s = lineFileDocs.nextDoc().get("body");
-                bos.write(s.getBytes(StandardCharsets.UTF_8));
-            }
-            doTest(bos.toByteArray());
-        }
-        lineFileDocs.close();
-    }
-
-    public void testLineDocsThreads() throws Exception {
-        final Random r = getRandom();
-        int threadCount = TestUtil.nextInt(r, 2, 6);
-        Thread[] threads = new Thread[threadCount];
-        final CountDownLatch startingGun = new CountDownLatch(1);
-        for (int tid=0; tid < threadCount; tid++) {
-            final long seed = r.nextLong();
-            threads[tid] = new Thread() {
-                @Override
-                public void run() {
-                    try {
-                        Random r = new Random(seed);
-                        startingGun.await();
-                        LineFileDocs lineFileDocs = new LineFileDocs(r);
-                        for (int i = 0; i < 10; i++) {
-                            int numDocs = TestUtil.nextInt(r, 1, 200);
-                            ByteArrayOutputStream bos = new ByteArrayOutputStream();
-                            for (int j = 0; j < numDocs; j++) {
-                                String s = lineFileDocs.nextDoc().get("body");
-                                bos.write(s.getBytes(StandardCharsets.UTF_8));
-                            }
-                            doTest(bos.toByteArray());
-                        }
-                        lineFileDocs.close();
-                    } catch (Exception e) {
-                        throw new RuntimeException(e);
-                    }
-                }
-            };
-            threads[tid].start();
-        }
-        startingGun.countDown();
-        for (Thread t : threads) {
-            t.join();
-        }
-    }
-
-    public void testRepetitionsL() throws IOException {
-        Random r = getRandom();
-        for (int i = 0; i < 10; i++) {
-            int numLongs = TestUtil.nextInt(r, 1, 10000);
-            ByteArrayOutputStream bos = new ByteArrayOutputStream();
-            long theValue = r.nextLong();
-            for (int j = 0; j < numLongs; j++) {
-                if (r.nextInt(10) == 0) {
-                    theValue = r.nextLong();
-                }
-                bos.write((byte) (theValue >>> 56));
-                bos.write((byte) (theValue >>> 48));
-                bos.write((byte) (theValue >>> 40));
-                bos.write((byte) (theValue >>> 32));
-                bos.write((byte) (theValue >>> 24));
-                bos.write((byte) (theValue >>> 16));
-                bos.write((byte) (theValue >>> 8));
-                bos.write((byte) theValue);
-            }
-            doTest(bos.toByteArray());
-        }
-    }
-
-    public void testRepetitionsLThreads() throws Exception {
-        final Random r = getRandom();
-        int threadCount = TestUtil.nextInt(r, 2, 6);
-        Thread[] threads = new Thread[threadCount];
-        final CountDownLatch startingGun = new CountDownLatch(1);
-        for (int tid=0; tid < threadCount; tid++) {
-            final long seed = r.nextLong();
-            threads[tid] = new Thread() {
-                @Override
-                public void run() {
-                    try {
-                        Random r = new Random(seed);
-                        startingGun.await();
-                        for (int i = 0; i < 10; i++) {
-                            int numLongs = TestUtil.nextInt(r, 1, 10000);
-                            ByteArrayOutputStream bos = new ByteArrayOutputStream();
-                            long theValue = r.nextLong();
-                            for (int j = 0; j < numLongs; j++) {
-                                if (r.nextInt(10) == 0) {
-                                    theValue = r.nextLong();
-                                }
-                                bos.write((byte) (theValue >>> 56));
-                                bos.write((byte) (theValue >>> 48));
-                                bos.write((byte) (theValue >>> 40));
-                                bos.write((byte) (theValue >>> 32));
-                                bos.write((byte) (theValue >>> 24));
-                                bos.write((byte) (theValue >>> 16));
-                                bos.write((byte) (theValue >>> 8));
-                                bos.write((byte) theValue);
-                            }
-                            doTest(bos.toByteArray());
-                        }
-                    } catch (Exception e) {
-                        throw new RuntimeException(e);
-                    }
-                }
-            };
-            threads[tid].start();
-        }
-        startingGun.countDown();
-        for (Thread t : threads) {
-            t.join();
-        }
-    }
-
-    public void testRepetitionsI() throws IOException {
-        Random r = getRandom();
-        for (int i = 0; i < 10; i++) {
-            int numInts = TestUtil.nextInt(r, 1, 20000);
-            ByteArrayOutputStream bos = new ByteArrayOutputStream();
-            int theValue = r.nextInt();
-            for (int j = 0; j < numInts; j++) {
-                if (r.nextInt(10) == 0) {
-                    theValue = r.nextInt();
-                }
-                bos.write((byte) (theValue >>> 24));
-                bos.write((byte) (theValue >>> 16));
-                bos.write((byte) (theValue >>> 8));
-                bos.write((byte) theValue);
-            }
-            doTest(bos.toByteArray());
-        }
-    }
-
-    public void testRepetitionsIThreads() throws Exception {
-        final Random r = getRandom();
-        int threadCount = TestUtil.nextInt(r, 2, 6);
-        Thread[] threads = new Thread[threadCount];
-        final CountDownLatch startingGun = new CountDownLatch(1);
-        for (int tid=0; tid < threadCount; tid++) {
-            final long seed = r.nextLong();
-            threads[tid] = new Thread() {
-                @Override
-                public void run() {
-                    try {
-                        Random r = new Random(seed);
-                        startingGun.await();
-                        for (int i = 0; i < 10; i++) {
-                            int numInts = TestUtil.nextInt(r, 1, 20000);
-                            ByteArrayOutputStream bos = new ByteArrayOutputStream();
-                            int theValue = r.nextInt();
-                            for (int j = 0; j < numInts; j++) {
-                                if (r.nextInt(10) == 0) {
-                                    theValue = r.nextInt();
-                                }
-                                bos.write((byte) (theValue >>> 24));
-                                bos.write((byte) (theValue >>> 16));
-                                bos.write((byte) (theValue >>> 8));
-                                bos.write((byte) theValue);
-                            }
-                            doTest(bos.toByteArray());
-                        }
-                    } catch (Exception e) {
-                        throw new RuntimeException(e);
-                    }
-                }
-            };
-            threads[tid].start();
-        }
-        startingGun.countDown();
-        for (Thread t : threads) {
-            t.join();
-        }
-    }
-
-    public void testRepetitionsS() throws IOException {
-        Random r = getRandom();
-        for (int i = 0; i < 10; i++) {
-            int numShorts = TestUtil.nextInt(r, 1, 40000);
-            ByteArrayOutputStream bos = new ByteArrayOutputStream();
-            short theValue = (short) r.nextInt(65535);
-            for (int j = 0; j < numShorts; j++) {
-                if (r.nextInt(10) == 0) {
-                    theValue = (short) r.nextInt(65535);
-                }
-                bos.write((byte) (theValue >>> 8));
-                bos.write((byte) theValue);
-            }
-            doTest(bos.toByteArray());
-        }
-    }
-
-    public void testMixed() throws IOException {
-        Random r = getRandom();
-        LineFileDocs lineFileDocs = new LineFileDocs(r);
-        for (int i = 0; i < 2; ++i) {
-            ByteArrayOutputStream bos = new ByteArrayOutputStream();
-            int prevInt = r.nextInt();
-            long prevLong = r.nextLong();
-            while (bos.size() < 400000) {
-                switch (r.nextInt(4)) {
-                case 0:
-                    addInt(r, prevInt, bos);
-                    break;
-                case 1:
-                    addLong(r, prevLong, bos);
-                    break;
-                case 2:
-                    addString(lineFileDocs, bos);
-                    break;
-                case 3:
-                    addBytes(r, bos);
-                    break;
-                default:
-                    throw new IllegalStateException("Random is broken");
-                }
-            }
-            doTest(bos.toByteArray());
-        }
-    }
-
-    private void addLong(Random r, long prev, ByteArrayOutputStream bos) {
-        long theValue = prev;
-        if (r.nextInt(10) != 0) {
-            theValue = r.nextLong();
-        }
-        bos.write((byte) (theValue >>> 56));
-        bos.write((byte) (theValue >>> 48));
-        bos.write((byte) (theValue >>> 40));
-        bos.write((byte) (theValue >>> 32));
-        bos.write((byte) (theValue >>> 24));
-        bos.write((byte) (theValue >>> 16));
-        bos.write((byte) (theValue >>> 8));
-        bos.write((byte) theValue);
-    }
-
-    private void addInt(Random r, int prev, ByteArrayOutputStream bos) {
-        int theValue = prev;
-        if (r.nextInt(10) != 0) {
-            theValue = r.nextInt();
-        }
-        bos.write((byte) (theValue >>> 24));
-        bos.write((byte) (theValue >>> 16));
-        bos.write((byte) (theValue >>> 8));
-        bos.write((byte) theValue);
-    }
-
-    private void addString(LineFileDocs lineFileDocs, ByteArrayOutputStream bos) throws IOException {
-        String s = lineFileDocs.nextDoc().get("body");
-        bos.write(s.getBytes(StandardCharsets.UTF_8));
-    }
-
-    private void addBytes(Random r, ByteArrayOutputStream bos) throws IOException {
-        byte bytes[] = new byte[TestUtil.nextInt(r, 1, 10000)];
-        r.nextBytes(bytes);
-        bos.write(bytes);
-    }
-
-    public void testRepetitionsSThreads() throws Exception {
-        final Random r = getRandom();
-        int threadCount = TestUtil.nextInt(r, 2, 6);
-        Thread[] threads = new Thread[threadCount];
-        final CountDownLatch startingGun = new CountDownLatch(1);
-        for (int tid=0; tid < threadCount; tid++) {
-            final long seed = r.nextLong();
-            threads[tid] = new Thread() {
-                @Override
-                public void run() {
-                    try {
-                        Random r = new Random(seed);
-                        startingGun.await();
-                        for (int i = 0; i < 10; i++) {
-                            int numShorts = TestUtil.nextInt(r, 1, 40000);
-                            ByteArrayOutputStream bos = new ByteArrayOutputStream();
-                            short theValue = (short) r.nextInt(65535);
-                            for (int j = 0; j < numShorts; j++) {
-                                if (r.nextInt(10) == 0) {
-                                    theValue = (short) r.nextInt(65535);
-                                }
-                                bos.write((byte) (theValue >>> 8));
-                                bos.write((byte) theValue);
-                            }
-                            doTest(bos.toByteArray());
-                        }
-                    } catch (Exception e) {
-                        throw new RuntimeException(e);
-                    }
-                }
-            };
-            threads[tid].start();
-        }
-        startingGun.countDown();
-        for (Thread t : threads) {
-            t.join();
-        }
-    }
-
-    private void doTest(byte bytes[]) throws IOException {
-        ByteBuffer bb = ByteBuffer.wrap(bytes);
-        StreamInput rawIn = new ByteBufferStreamInput(bb);
-        Compressor c = compressor;
-
-        ByteArrayOutputStream bos = new ByteArrayOutputStream();
-        OutputStreamStreamOutput rawOs = new OutputStreamStreamOutput(bos);
-        StreamOutput os = c.streamOutput(rawOs);
-
-        Random r = getRandom();
-        int bufferSize = r.nextBoolean() ? 65535 : TestUtil.nextInt(getRandom(), 1, 70000);
-        int prepadding = r.nextInt(70000);
-        int postpadding = r.nextInt(70000);
-        byte buffer[] = new byte[prepadding + bufferSize + postpadding];
-        r.nextBytes(buffer); // fill block completely with junk
-        int len;
-        while ((len = rawIn.read(buffer, prepadding, bufferSize)) != -1) {
-            os.write(buffer, prepadding, len);
-        }
-        os.close();
-        rawIn.close();
-
-        // now we have compressed byte array
-
-        byte compressed[] = bos.toByteArray();
-        ByteBuffer bb2 = ByteBuffer.wrap(compressed);
-        StreamInput compressedIn = new ByteBufferStreamInput(bb2);
-        StreamInput in = c.streamInput(compressedIn);
-
-        // randomize constants again
-        bufferSize = r.nextBoolean() ? 65535 : TestUtil.nextInt(getRandom(), 1, 70000);
-        prepadding = r.nextInt(70000);
-        postpadding = r.nextInt(70000);
-        buffer = new byte[prepadding + bufferSize + postpadding];
-        r.nextBytes(buffer); // fill block completely with junk
-
-        ByteArrayOutputStream uncompressedOut = new ByteArrayOutputStream();
-        while ((len = in.read(buffer, prepadding, bufferSize)) != -1) {
-            uncompressedOut.write(buffer, prepadding, len);
-        }
-        uncompressedOut.close();
-
-        assertArrayEquals(bytes, uncompressedOut.toByteArray());
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/common/compress/AbstractCompressedXContentTestCase.java b/core/src/test/java/org/elasticsearch/common/compress/AbstractCompressedXContentTestCase.java
new file mode 100644
index 0000000..e5d627f
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/compress/AbstractCompressedXContentTestCase.java
@@ -0,0 +1,120 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.compress;
+
+import org.apache.lucene.util.TestUtil;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.io.stream.BytesStreamOutput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Assert;
+
+import java.io.IOException;
+import java.util.Random;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.not;
+
+/**
+ *
+ */
+public abstract class AbstractCompressedXContentTestCase extends ESTestCase {
+
+    private final Compressor compressor;
+
+    protected AbstractCompressedXContentTestCase(Compressor compressor) {
+        this.compressor = compressor;
+    }
+
+    private void assertEquals(CompressedXContent s1, CompressedXContent s2) {
+        Assert.assertEquals(s1, s2);
+        assertArrayEquals(s1.uncompressed(), s2.uncompressed());
+        assertEquals(s1.hashCode(), s2.hashCode());
+    }
+
+    public void simpleTests() throws IOException {
+        Compressor defaultCompressor = CompressorFactory.defaultCompressor();
+        try {
+            CompressorFactory.setDefaultCompressor(compressor);
+            String str = "---\nf:this is a simple string";
+            CompressedXContent cstr = new CompressedXContent(str);
+            assertThat(cstr.string(), equalTo(str));
+            assertThat(new CompressedXContent(str), equalTo(cstr));
+
+            String str2 = "---\nf:this is a simple string 2";
+            CompressedXContent cstr2 = new CompressedXContent(str2);
+            assertThat(cstr2.string(), not(equalTo(str)));
+            assertThat(new CompressedXContent(str2), not(equalTo(cstr)));
+            assertEquals(new CompressedXContent(str2), cstr2);
+        } finally {
+            CompressorFactory.setDefaultCompressor(defaultCompressor);
+        }
+    }
+
+    public void testRandom() throws IOException {
+        Compressor defaultCompressor = CompressorFactory.defaultCompressor();
+        try {
+            CompressorFactory.setDefaultCompressor(compressor);
+            Random r = getRandom();
+            for (int i = 0; i < 1000; i++) {
+                String string = TestUtil.randomUnicodeString(r, 10000);
+                // hack to make it detected as YAML
+                string = "---\n" + string;
+                CompressedXContent compressedXContent = new CompressedXContent(string);
+                assertThat(compressedXContent.string(), equalTo(string));
+            }
+        } finally {
+            CompressorFactory.setDefaultCompressor(defaultCompressor);
+        }
+    }
+
+    public void testDifferentCompressedRepresentation() throws Exception {
+        byte[] b = "---\nf:abcdefghijabcdefghij".getBytes("UTF-8");
+        BytesStreamOutput bout = new BytesStreamOutput();
+        StreamOutput out = compressor.streamOutput(bout);
+        out.writeBytes(b);
+        out.flush();
+        out.writeBytes(b);
+        out.close();
+        final BytesReference b1 = bout.bytes();
+
+        bout = new BytesStreamOutput();
+        out = compressor.streamOutput(bout);
+        out.writeBytes(b);
+        out.writeBytes(b);
+        out.close();
+        final BytesReference b2 = bout.bytes();
+
+        // because of the intermediate flush, the two compressed representations
+        // are different. It can also happen for other reasons like if hash tables
+        // of different size are being used
+        assertFalse(b1.equals(b2));
+        // we used the compressed representation directly and did not recompress
+        assertArrayEquals(b1.toBytes(), new CompressedXContent(b1).compressed());
+        assertArrayEquals(b2.toBytes(), new CompressedXContent(b2).compressed());
+        // but compressedstring instances are still equal
+        assertEquals(new CompressedXContent(b1), new CompressedXContent(b2));
+    }
+
+    public void testHashCode() throws IOException {
+        assertFalse(new CompressedXContent("{\"a\":\"b\"}").hashCode() == new CompressedXContent("{\"a\":\"c\"}").hashCode());
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/common/compress/AbstractCompressedXContentTests.java b/core/src/test/java/org/elasticsearch/common/compress/AbstractCompressedXContentTests.java
deleted file mode 100644
index 5b8bcf1..0000000
--- a/core/src/test/java/org/elasticsearch/common/compress/AbstractCompressedXContentTests.java
+++ /dev/null
@@ -1,120 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common.compress;
-
-import org.apache.lucene.util.TestUtil;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Assert;
-
-import java.io.IOException;
-import java.util.Random;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.not;
-
-/**
- *
- */
-public abstract class AbstractCompressedXContentTests extends ESTestCase {
-
-    private final Compressor compressor;
-
-    protected AbstractCompressedXContentTests(Compressor compressor) {
-        this.compressor = compressor;
-    }
-
-    private void assertEquals(CompressedXContent s1, CompressedXContent s2) {
-        Assert.assertEquals(s1, s2);
-        assertArrayEquals(s1.uncompressed(), s2.uncompressed());
-        assertEquals(s1.hashCode(), s2.hashCode());
-    }
-
-    public void simpleTests() throws IOException {
-        Compressor defaultCompressor = CompressorFactory.defaultCompressor();
-        try {
-            CompressorFactory.setDefaultCompressor(compressor);
-            String str = "---\nf:this is a simple string";
-            CompressedXContent cstr = new CompressedXContent(str);
-            assertThat(cstr.string(), equalTo(str));
-            assertThat(new CompressedXContent(str), equalTo(cstr));
-
-            String str2 = "---\nf:this is a simple string 2";
-            CompressedXContent cstr2 = new CompressedXContent(str2);
-            assertThat(cstr2.string(), not(equalTo(str)));
-            assertThat(new CompressedXContent(str2), not(equalTo(cstr)));
-            assertEquals(new CompressedXContent(str2), cstr2);
-        } finally {
-            CompressorFactory.setDefaultCompressor(defaultCompressor);
-        }
-    }
-
-    public void testRandom() throws IOException {
-        Compressor defaultCompressor = CompressorFactory.defaultCompressor();
-        try {
-            CompressorFactory.setDefaultCompressor(compressor);
-            Random r = getRandom();
-            for (int i = 0; i < 1000; i++) {
-                String string = TestUtil.randomUnicodeString(r, 10000);
-                // hack to make it detected as YAML
-                string = "---\n" + string;
-                CompressedXContent compressedXContent = new CompressedXContent(string);
-                assertThat(compressedXContent.string(), equalTo(string));
-            }
-        } finally {
-            CompressorFactory.setDefaultCompressor(defaultCompressor);
-        }
-    }
-
-    public void testDifferentCompressedRepresentation() throws Exception {
-        byte[] b = "---\nf:abcdefghijabcdefghij".getBytes("UTF-8");
-        BytesStreamOutput bout = new BytesStreamOutput();
-        StreamOutput out = compressor.streamOutput(bout);
-        out.writeBytes(b);
-        out.flush();
-        out.writeBytes(b);
-        out.close();
-        final BytesReference b1 = bout.bytes();
-
-        bout = new BytesStreamOutput();
-        out = compressor.streamOutput(bout);
-        out.writeBytes(b);
-        out.writeBytes(b);
-        out.close();
-        final BytesReference b2 = bout.bytes();
-
-        // because of the intermediate flush, the two compressed representations
-        // are different. It can also happen for other reasons like if hash tables
-        // of different size are being used
-        assertFalse(b1.equals(b2));
-        // we used the compressed representation directly and did not recompress
-        assertArrayEquals(b1.toBytes(), new CompressedXContent(b1).compressed());
-        assertArrayEquals(b2.toBytes(), new CompressedXContent(b2).compressed());
-        // but compressedstring instances are still equal
-        assertEquals(new CompressedXContent(b1), new CompressedXContent(b2));
-    }
-
-    public void testHashCode() throws IOException {
-        assertFalse(new CompressedXContent("{\"a\":\"b\"}").hashCode() == new CompressedXContent("{\"a\":\"c\"}").hashCode());
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/common/compress/deflate/DeflateCompressedStreamTests.java b/core/src/test/java/org/elasticsearch/common/compress/deflate/DeflateCompressedStreamTests.java
index 6607274..a6d3358 100644
--- a/core/src/test/java/org/elasticsearch/common/compress/deflate/DeflateCompressedStreamTests.java
+++ b/core/src/test/java/org/elasticsearch/common/compress/deflate/DeflateCompressedStreamTests.java
@@ -19,9 +19,9 @@
 
 package org.elasticsearch.common.compress.deflate;
 
-import org.elasticsearch.common.compress.AbstractCompressedStreamTests;
+import org.elasticsearch.common.compress.AbstractCompressedStreamTestCase;
 
-public class DeflateCompressedStreamTests extends AbstractCompressedStreamTests {
+public class DeflateCompressedStreamTests extends AbstractCompressedStreamTestCase {
 
     public DeflateCompressedStreamTests() {
         super(new DeflateCompressor());
diff --git a/core/src/test/java/org/elasticsearch/common/compress/deflate/DeflateXContentTests.java b/core/src/test/java/org/elasticsearch/common/compress/deflate/DeflateXContentTests.java
index 8b103c9..359a582 100644
--- a/core/src/test/java/org/elasticsearch/common/compress/deflate/DeflateXContentTests.java
+++ b/core/src/test/java/org/elasticsearch/common/compress/deflate/DeflateXContentTests.java
@@ -19,9 +19,9 @@
 
 package org.elasticsearch.common.compress.deflate;
 
-import org.elasticsearch.common.compress.AbstractCompressedXContentTests;
+import org.elasticsearch.common.compress.AbstractCompressedXContentTestCase;
 
-public class DeflateXContentTests extends AbstractCompressedXContentTests {
+public class DeflateXContentTests extends AbstractCompressedXContentTestCase {
 
     public DeflateXContentTests() {
         super(new DeflateCompressor());
diff --git a/core/src/test/java/org/elasticsearch/common/compress/lzf/LZFCompressedStreamTests.java b/core/src/test/java/org/elasticsearch/common/compress/lzf/LZFCompressedStreamTests.java
index 1d69fce..89ee148 100644
--- a/core/src/test/java/org/elasticsearch/common/compress/lzf/LZFCompressedStreamTests.java
+++ b/core/src/test/java/org/elasticsearch/common/compress/lzf/LZFCompressedStreamTests.java
@@ -19,9 +19,9 @@
 
 package org.elasticsearch.common.compress.lzf;
 
-import org.elasticsearch.common.compress.AbstractCompressedStreamTests;
+import org.elasticsearch.common.compress.AbstractCompressedStreamTestCase;
 
-public class LZFCompressedStreamTests extends AbstractCompressedStreamTests {
+public class LZFCompressedStreamTests extends AbstractCompressedStreamTestCase {
 
     public LZFCompressedStreamTests() {
         super(new LZFTestCompressor());
diff --git a/core/src/test/java/org/elasticsearch/common/compress/lzf/LZFXContentTests.java b/core/src/test/java/org/elasticsearch/common/compress/lzf/LZFXContentTests.java
index 698a033..05135f0 100644
--- a/core/src/test/java/org/elasticsearch/common/compress/lzf/LZFXContentTests.java
+++ b/core/src/test/java/org/elasticsearch/common/compress/lzf/LZFXContentTests.java
@@ -19,9 +19,9 @@
 
 package org.elasticsearch.common.compress.lzf;
 
-import org.elasticsearch.common.compress.AbstractCompressedXContentTests;
+import org.elasticsearch.common.compress.AbstractCompressedXContentTestCase;
 
-public class LZFXContentTests extends AbstractCompressedXContentTests {
+public class LZFXContentTests extends AbstractCompressedXContentTestCase {
 
     public LZFXContentTests() {
         super(new LZFTestCompressor());
diff --git a/core/src/test/java/org/elasticsearch/common/io/stream/BytesStreamsTests.java b/core/src/test/java/org/elasticsearch/common/io/stream/BytesStreamsTests.java
index afc17ce..d313dd7 100644
--- a/core/src/test/java/org/elasticsearch/common/io/stream/BytesStreamsTests.java
+++ b/core/src/test/java/org/elasticsearch/common/io/stream/BytesStreamsTests.java
@@ -26,7 +26,6 @@ import org.elasticsearch.test.ESTestCase;
 import org.junit.Test;
 
 import java.io.IOException;
-
 import java.util.Objects;
 
 import static org.hamcrest.Matchers.closeTo;
diff --git a/core/src/test/java/org/elasticsearch/common/lucene/LuceneTest.java b/core/src/test/java/org/elasticsearch/common/lucene/LuceneTest.java
deleted file mode 100644
index 8418bef..0000000
--- a/core/src/test/java/org/elasticsearch/common/lucene/LuceneTest.java
+++ /dev/null
@@ -1,325 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.common.lucene;
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.*;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.store.MockDirectoryWrapper;
-import org.apache.lucene.util.Version;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.*;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.atomic.AtomicBoolean;
-
-/**
- * 
- */
-public class LuceneTest extends ESTestCase {
-
-
-    /*
-     * simple test that ensures that we bump the version on Upgrade
-     */
-    @Test
-    public void testVersion() {
-        // note this is just a silly sanity check, we test it in lucene, and we point to it this way
-        assertEquals(Lucene.VERSION, Version.LATEST);
-    }
-
-    public void testWaitForIndex() throws Exception {
-        final MockDirectoryWrapper dir = newMockDirectory();
-
-        final AtomicBoolean succeeded = new AtomicBoolean(false);
-        final CountDownLatch latch = new CountDownLatch(1);
-
-        // Create a shadow Engine, which will freak out because there is no
-        // index yet
-        Thread t = new Thread(new Runnable() {
-            @Override
-            public void run() {
-                try {
-                    latch.await();
-                    if (Lucene.waitForIndex(dir, 5000)) {
-                        succeeded.set(true);
-                    } else {
-                        fail("index should have eventually existed!");
-                    }
-                } catch (InterruptedException e) {
-                    // ignore interruptions
-                } catch (Exception e) {
-                    fail("should have been able to create the engine! " + e.getMessage());
-                }
-            }
-        });
-        t.start();
-
-        // count down latch
-        // now shadow engine should try to be created
-        latch.countDown();
-
-        dir.setEnableVirusScanner(false);
-        IndexWriterConfig iwc = newIndexWriterConfig();
-        iwc.setIndexDeletionPolicy(NoDeletionPolicy.INSTANCE);
-        iwc.setMergePolicy(NoMergePolicy.INSTANCE);
-        iwc.setMaxBufferedDocs(2);
-        IndexWriter writer = new IndexWriter(dir, iwc);
-        Document doc = new Document();
-        doc.add(new TextField("id", "1", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-        writer.addDocument(doc);
-        writer.commit();
-
-        t.join();
-
-        writer.close();
-        dir.close();
-        assertTrue("index should have eventually existed", succeeded.get());
-    }
-
-    public void testCleanIndex() throws IOException {
-        MockDirectoryWrapper dir = newMockDirectory();
-        dir.setEnableVirusScanner(false);
-        IndexWriterConfig iwc = newIndexWriterConfig();
-        iwc.setIndexDeletionPolicy(NoDeletionPolicy.INSTANCE);
-        iwc.setMergePolicy(NoMergePolicy.INSTANCE);
-        iwc.setMaxBufferedDocs(2);
-        IndexWriter writer = new IndexWriter(dir, iwc);
-        Document doc = new Document();
-        doc.add(new TextField("id", "1", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-        writer.addDocument(doc);
-        writer.commit();
-
-        doc = new Document();
-        doc.add(new TextField("id", "2", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-        writer.addDocument(doc);
-
-        doc = new Document();
-        doc.add(new TextField("id", "3", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-        writer.addDocument(doc);
-
-        writer.commit();
-        doc = new Document();
-        doc.add(new TextField("id", "4", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-        writer.addDocument(doc);
-
-        writer.deleteDocuments(new Term("id", "2"));
-        writer.commit();
-        try (DirectoryReader open = DirectoryReader.open(writer, true)) {
-            assertEquals(3, open.numDocs());
-            assertEquals(1, open.numDeletedDocs());
-            assertEquals(4, open.maxDoc());
-        }
-        writer.close();
-        if (random().nextBoolean()) {
-            for (String file : dir.listAll()) {
-                if (file.startsWith("_1")) {
-                    // delete a random file
-                    dir.deleteFile(file);
-                    break;
-                }
-            }
-        }
-        Lucene.cleanLuceneIndex(dir);
-        if (dir.listAll().length > 0) {
-            for (String file : dir.listAll()) {
-                if (file.startsWith("extra") == false) {
-                    assertEquals(file, "write.lock");
-                }
-            }
-        }
-        dir.close();
-    }
-
-    public void testPruneUnreferencedFiles() throws IOException {
-        MockDirectoryWrapper dir = newMockDirectory();
-        dir.setEnableVirusScanner(false);
-        IndexWriterConfig iwc = newIndexWriterConfig();
-        iwc.setIndexDeletionPolicy(NoDeletionPolicy.INSTANCE);
-        iwc.setMergePolicy(NoMergePolicy.INSTANCE);
-        iwc.setMaxBufferedDocs(2);
-        IndexWriter writer = new IndexWriter(dir, iwc);
-        Document doc = new Document();
-        doc.add(new TextField("id", "1", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-        writer.addDocument(doc);
-        writer.commit();
-
-        doc = new Document();
-        doc.add(new TextField("id", "2", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-        writer.addDocument(doc);
-
-        doc = new Document();
-        doc.add(new TextField("id", "3", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-        writer.addDocument(doc);
-
-        writer.commit();
-        SegmentInfos segmentCommitInfos = Lucene.readSegmentInfos(dir);
-
-        doc = new Document();
-        doc.add(new TextField("id", "4", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-        writer.addDocument(doc);
-
-        writer.deleteDocuments(new Term("id", "2"));
-        writer.commit();
-        DirectoryReader open = DirectoryReader.open(writer, true);
-        assertEquals(3, open.numDocs());
-        assertEquals(1, open.numDeletedDocs());
-        assertEquals(4, open.maxDoc());
-        open.close();
-        writer.close();
-        SegmentInfos si = Lucene.pruneUnreferencedFiles(segmentCommitInfos.getSegmentsFileName(), dir);
-        assertEquals(si.getSegmentsFileName(), segmentCommitInfos.getSegmentsFileName());
-        open = DirectoryReader.open(dir);
-        assertEquals(3, open.numDocs());
-        assertEquals(0, open.numDeletedDocs());
-        assertEquals(3, open.maxDoc());
-
-        IndexSearcher s = new IndexSearcher(open);
-        assertEquals(s.search(new TermQuery(new Term("id", "1")), 1).totalHits, 1);
-        assertEquals(s.search(new TermQuery(new Term("id", "2")), 1).totalHits, 1);
-        assertEquals(s.search(new TermQuery(new Term("id", "3")), 1).totalHits, 1);
-        assertEquals(s.search(new TermQuery(new Term("id", "4")), 1).totalHits, 0);
-
-        for (String file : dir.listAll()) {
-            assertFalse("unexpected file: " + file, file.equals("segments_3") || file.startsWith("_2"));
-        }
-        open.close();
-        dir.close();
-
-    }
-
-    public void testFiles() throws IOException {
-        MockDirectoryWrapper dir = newMockDirectory();
-        dir.setEnableVirusScanner(false);
-        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));
-        iwc.setMergePolicy(NoMergePolicy.INSTANCE);
-        iwc.setMaxBufferedDocs(2);
-        iwc.setUseCompoundFile(true);
-        IndexWriter writer = new IndexWriter(dir, iwc);
-        Document doc = new Document();
-        doc.add(new TextField("id", "1", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-        writer.addDocument(doc);
-        writer.commit();
-        Set<String> files = new HashSet<>();
-        for (String f : Lucene.files(Lucene.readSegmentInfos(dir))) {
-            files.add(f);
-        }
-        final boolean simpleTextCFS = files.contains("_0.scf");
-        assertTrue(files.toString(), files.contains("segments_1"));
-        if (simpleTextCFS) {
-            assertFalse(files.toString(), files.contains("_0.cfs"));
-            assertFalse(files.toString(), files.contains("_0.cfe"));
-        } else {
-            assertTrue(files.toString(), files.contains("_0.cfs"));
-            assertTrue(files.toString(), files.contains("_0.cfe"));
-        }
-        assertTrue(files.toString(), files.contains("_0.si"));
-
-        doc = new Document();
-        doc.add(new TextField("id", "2", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-        writer.addDocument(doc);
-
-        doc = new Document();
-        doc.add(new TextField("id", "3", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-        writer.addDocument(doc);
-        writer.commit();
-
-        files.clear();
-        for (String f : Lucene.files(Lucene.readSegmentInfos(dir))) {
-            files.add(f);
-        }
-        assertFalse(files.toString(), files.contains("segments_1"));
-        assertTrue(files.toString(), files.contains("segments_2"));
-        if (simpleTextCFS) {
-            assertFalse(files.toString(), files.contains("_0.cfs"));
-            assertFalse(files.toString(), files.contains("_0.cfe"));
-        } else {
-            assertTrue(files.toString(), files.contains("_0.cfs"));
-            assertTrue(files.toString(), files.contains("_0.cfe"));
-        }
-        assertTrue(files.toString(), files.contains("_0.si"));
-
-
-        if (simpleTextCFS) {
-            assertFalse(files.toString(), files.contains("_1.cfs"));
-            assertFalse(files.toString(), files.contains("_1.cfe"));
-        } else {
-            assertTrue(files.toString(), files.contains("_1.cfs"));
-            assertTrue(files.toString(), files.contains("_1.cfe"));
-        }
-        assertTrue(files.toString(), files.contains("_1.si"));
-        writer.close();
-        dir.close();
-    }
-
-    public void testNumDocs() throws IOException {
-        MockDirectoryWrapper dir = newMockDirectory();
-        dir.setEnableVirusScanner(false);
-        IndexWriterConfig iwc = newIndexWriterConfig();
-        IndexWriter writer = new IndexWriter(dir, iwc);
-        Document doc = new Document();
-        doc.add(new TextField("id", "1", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-        writer.addDocument(doc);
-        writer.commit();
-        SegmentInfos segmentCommitInfos = Lucene.readSegmentInfos(dir);
-        assertEquals(1, Lucene.getNumDocs(segmentCommitInfos));
-
-        doc = new Document();
-        doc.add(new TextField("id", "2", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-        writer.addDocument(doc);
-
-        doc = new Document();
-        doc.add(new TextField("id", "3", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-        writer.addDocument(doc);
-        segmentCommitInfos = Lucene.readSegmentInfos(dir);
-        assertEquals(1, Lucene.getNumDocs(segmentCommitInfos));
-        writer.commit();
-        segmentCommitInfos = Lucene.readSegmentInfos(dir);
-        assertEquals(3, Lucene.getNumDocs(segmentCommitInfos));
-        writer.deleteDocuments(new Term("id", "2"));
-        writer.commit();
-        segmentCommitInfos = Lucene.readSegmentInfos(dir);
-        assertEquals(2, Lucene.getNumDocs(segmentCommitInfos));
-
-        int numDocsToIndex = randomIntBetween(10, 50);
-        List<Term> deleteTerms = new ArrayList<>();
-        for (int i = 0; i < numDocsToIndex; i++) {
-            doc = new Document();
-            doc.add(new TextField("id", "extra_" + i, random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-            deleteTerms.add(new Term("id", "extra_" + i));
-            writer.addDocument(doc);
-        }
-        int numDocsToDelete = randomIntBetween(0, numDocsToIndex);
-        Collections.shuffle(deleteTerms, random());
-        for (int i = 0; i < numDocsToDelete; i++) {
-            Term remove = deleteTerms.remove(0);
-            writer.deleteDocuments(remove);
-        }
-        writer.commit();
-        segmentCommitInfos = Lucene.readSegmentInfos(dir);
-        assertEquals(2 + deleteTerms.size(), Lucene.getNumDocs(segmentCommitInfos));
-        writer.close();
-        dir.close();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/common/lucene/LuceneTests.java b/core/src/test/java/org/elasticsearch/common/lucene/LuceneTests.java
new file mode 100644
index 0000000..2fb90c7
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/lucene/LuceneTests.java
@@ -0,0 +1,325 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.common.lucene;
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.*;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.util.Version;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.*;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+/**
+ * 
+ */
+public class LuceneTests extends ESTestCase {
+
+
+    /*
+     * simple test that ensures that we bump the version on Upgrade
+     */
+    @Test
+    public void testVersion() {
+        // note this is just a silly sanity check, we test it in lucene, and we point to it this way
+        assertEquals(Lucene.VERSION, Version.LATEST);
+    }
+
+    public void testWaitForIndex() throws Exception {
+        final MockDirectoryWrapper dir = newMockDirectory();
+
+        final AtomicBoolean succeeded = new AtomicBoolean(false);
+        final CountDownLatch latch = new CountDownLatch(1);
+
+        // Create a shadow Engine, which will freak out because there is no
+        // index yet
+        Thread t = new Thread(new Runnable() {
+            @Override
+            public void run() {
+                try {
+                    latch.await();
+                    if (Lucene.waitForIndex(dir, 5000)) {
+                        succeeded.set(true);
+                    } else {
+                        fail("index should have eventually existed!");
+                    }
+                } catch (InterruptedException e) {
+                    // ignore interruptions
+                } catch (Exception e) {
+                    fail("should have been able to create the engine! " + e.getMessage());
+                }
+            }
+        });
+        t.start();
+
+        // count down latch
+        // now shadow engine should try to be created
+        latch.countDown();
+
+        dir.setEnableVirusScanner(false);
+        IndexWriterConfig iwc = newIndexWriterConfig();
+        iwc.setIndexDeletionPolicy(NoDeletionPolicy.INSTANCE);
+        iwc.setMergePolicy(NoMergePolicy.INSTANCE);
+        iwc.setMaxBufferedDocs(2);
+        IndexWriter writer = new IndexWriter(dir, iwc);
+        Document doc = new Document();
+        doc.add(new TextField("id", "1", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+        writer.addDocument(doc);
+        writer.commit();
+
+        t.join();
+
+        writer.close();
+        dir.close();
+        assertTrue("index should have eventually existed", succeeded.get());
+    }
+
+    public void testCleanIndex() throws IOException {
+        MockDirectoryWrapper dir = newMockDirectory();
+        dir.setEnableVirusScanner(false);
+        IndexWriterConfig iwc = newIndexWriterConfig();
+        iwc.setIndexDeletionPolicy(NoDeletionPolicy.INSTANCE);
+        iwc.setMergePolicy(NoMergePolicy.INSTANCE);
+        iwc.setMaxBufferedDocs(2);
+        IndexWriter writer = new IndexWriter(dir, iwc);
+        Document doc = new Document();
+        doc.add(new TextField("id", "1", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+        writer.addDocument(doc);
+        writer.commit();
+
+        doc = new Document();
+        doc.add(new TextField("id", "2", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+        writer.addDocument(doc);
+
+        doc = new Document();
+        doc.add(new TextField("id", "3", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+        writer.addDocument(doc);
+
+        writer.commit();
+        doc = new Document();
+        doc.add(new TextField("id", "4", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+        writer.addDocument(doc);
+
+        writer.deleteDocuments(new Term("id", "2"));
+        writer.commit();
+        try (DirectoryReader open = DirectoryReader.open(writer, true)) {
+            assertEquals(3, open.numDocs());
+            assertEquals(1, open.numDeletedDocs());
+            assertEquals(4, open.maxDoc());
+        }
+        writer.close();
+        if (random().nextBoolean()) {
+            for (String file : dir.listAll()) {
+                if (file.startsWith("_1")) {
+                    // delete a random file
+                    dir.deleteFile(file);
+                    break;
+                }
+            }
+        }
+        Lucene.cleanLuceneIndex(dir);
+        if (dir.listAll().length > 0) {
+            for (String file : dir.listAll()) {
+                if (file.startsWith("extra") == false) {
+                    assertEquals(file, "write.lock");
+                }
+            }
+        }
+        dir.close();
+    }
+
+    public void testPruneUnreferencedFiles() throws IOException {
+        MockDirectoryWrapper dir = newMockDirectory();
+        dir.setEnableVirusScanner(false);
+        IndexWriterConfig iwc = newIndexWriterConfig();
+        iwc.setIndexDeletionPolicy(NoDeletionPolicy.INSTANCE);
+        iwc.setMergePolicy(NoMergePolicy.INSTANCE);
+        iwc.setMaxBufferedDocs(2);
+        IndexWriter writer = new IndexWriter(dir, iwc);
+        Document doc = new Document();
+        doc.add(new TextField("id", "1", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+        writer.addDocument(doc);
+        writer.commit();
+
+        doc = new Document();
+        doc.add(new TextField("id", "2", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+        writer.addDocument(doc);
+
+        doc = new Document();
+        doc.add(new TextField("id", "3", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+        writer.addDocument(doc);
+
+        writer.commit();
+        SegmentInfos segmentCommitInfos = Lucene.readSegmentInfos(dir);
+
+        doc = new Document();
+        doc.add(new TextField("id", "4", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+        writer.addDocument(doc);
+
+        writer.deleteDocuments(new Term("id", "2"));
+        writer.commit();
+        DirectoryReader open = DirectoryReader.open(writer, true);
+        assertEquals(3, open.numDocs());
+        assertEquals(1, open.numDeletedDocs());
+        assertEquals(4, open.maxDoc());
+        open.close();
+        writer.close();
+        SegmentInfos si = Lucene.pruneUnreferencedFiles(segmentCommitInfos.getSegmentsFileName(), dir);
+        assertEquals(si.getSegmentsFileName(), segmentCommitInfos.getSegmentsFileName());
+        open = DirectoryReader.open(dir);
+        assertEquals(3, open.numDocs());
+        assertEquals(0, open.numDeletedDocs());
+        assertEquals(3, open.maxDoc());
+
+        IndexSearcher s = new IndexSearcher(open);
+        assertEquals(s.search(new TermQuery(new Term("id", "1")), 1).totalHits, 1);
+        assertEquals(s.search(new TermQuery(new Term("id", "2")), 1).totalHits, 1);
+        assertEquals(s.search(new TermQuery(new Term("id", "3")), 1).totalHits, 1);
+        assertEquals(s.search(new TermQuery(new Term("id", "4")), 1).totalHits, 0);
+
+        for (String file : dir.listAll()) {
+            assertFalse("unexpected file: " + file, file.equals("segments_3") || file.startsWith("_2"));
+        }
+        open.close();
+        dir.close();
+
+    }
+
+    public void testFiles() throws IOException {
+        MockDirectoryWrapper dir = newMockDirectory();
+        dir.setEnableVirusScanner(false);
+        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));
+        iwc.setMergePolicy(NoMergePolicy.INSTANCE);
+        iwc.setMaxBufferedDocs(2);
+        iwc.setUseCompoundFile(true);
+        IndexWriter writer = new IndexWriter(dir, iwc);
+        Document doc = new Document();
+        doc.add(new TextField("id", "1", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+        writer.addDocument(doc);
+        writer.commit();
+        Set<String> files = new HashSet<>();
+        for (String f : Lucene.files(Lucene.readSegmentInfos(dir))) {
+            files.add(f);
+        }
+        final boolean simpleTextCFS = files.contains("_0.scf");
+        assertTrue(files.toString(), files.contains("segments_1"));
+        if (simpleTextCFS) {
+            assertFalse(files.toString(), files.contains("_0.cfs"));
+            assertFalse(files.toString(), files.contains("_0.cfe"));
+        } else {
+            assertTrue(files.toString(), files.contains("_0.cfs"));
+            assertTrue(files.toString(), files.contains("_0.cfe"));
+        }
+        assertTrue(files.toString(), files.contains("_0.si"));
+
+        doc = new Document();
+        doc.add(new TextField("id", "2", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+        writer.addDocument(doc);
+
+        doc = new Document();
+        doc.add(new TextField("id", "3", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+        writer.addDocument(doc);
+        writer.commit();
+
+        files.clear();
+        for (String f : Lucene.files(Lucene.readSegmentInfos(dir))) {
+            files.add(f);
+        }
+        assertFalse(files.toString(), files.contains("segments_1"));
+        assertTrue(files.toString(), files.contains("segments_2"));
+        if (simpleTextCFS) {
+            assertFalse(files.toString(), files.contains("_0.cfs"));
+            assertFalse(files.toString(), files.contains("_0.cfe"));
+        } else {
+            assertTrue(files.toString(), files.contains("_0.cfs"));
+            assertTrue(files.toString(), files.contains("_0.cfe"));
+        }
+        assertTrue(files.toString(), files.contains("_0.si"));
+
+
+        if (simpleTextCFS) {
+            assertFalse(files.toString(), files.contains("_1.cfs"));
+            assertFalse(files.toString(), files.contains("_1.cfe"));
+        } else {
+            assertTrue(files.toString(), files.contains("_1.cfs"));
+            assertTrue(files.toString(), files.contains("_1.cfe"));
+        }
+        assertTrue(files.toString(), files.contains("_1.si"));
+        writer.close();
+        dir.close();
+    }
+
+    public void testNumDocs() throws IOException {
+        MockDirectoryWrapper dir = newMockDirectory();
+        dir.setEnableVirusScanner(false);
+        IndexWriterConfig iwc = newIndexWriterConfig();
+        IndexWriter writer = new IndexWriter(dir, iwc);
+        Document doc = new Document();
+        doc.add(new TextField("id", "1", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+        writer.addDocument(doc);
+        writer.commit();
+        SegmentInfos segmentCommitInfos = Lucene.readSegmentInfos(dir);
+        assertEquals(1, Lucene.getNumDocs(segmentCommitInfos));
+
+        doc = new Document();
+        doc.add(new TextField("id", "2", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+        writer.addDocument(doc);
+
+        doc = new Document();
+        doc.add(new TextField("id", "3", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+        writer.addDocument(doc);
+        segmentCommitInfos = Lucene.readSegmentInfos(dir);
+        assertEquals(1, Lucene.getNumDocs(segmentCommitInfos));
+        writer.commit();
+        segmentCommitInfos = Lucene.readSegmentInfos(dir);
+        assertEquals(3, Lucene.getNumDocs(segmentCommitInfos));
+        writer.deleteDocuments(new Term("id", "2"));
+        writer.commit();
+        segmentCommitInfos = Lucene.readSegmentInfos(dir);
+        assertEquals(2, Lucene.getNumDocs(segmentCommitInfos));
+
+        int numDocsToIndex = randomIntBetween(10, 50);
+        List<Term> deleteTerms = new ArrayList<>();
+        for (int i = 0; i < numDocsToIndex; i++) {
+            doc = new Document();
+            doc.add(new TextField("id", "extra_" + i, random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+            deleteTerms.add(new Term("id", "extra_" + i));
+            writer.addDocument(doc);
+        }
+        int numDocsToDelete = randomIntBetween(0, numDocsToIndex);
+        Collections.shuffle(deleteTerms, random());
+        for (int i = 0; i < numDocsToDelete; i++) {
+            Term remove = deleteTerms.remove(0);
+            writer.deleteDocuments(remove);
+        }
+        writer.commit();
+        segmentCommitInfos = Lucene.readSegmentInfos(dir);
+        assertEquals(2 + deleteTerms.size(), Lucene.getNumDocs(segmentCommitInfos));
+        writer.close();
+        dir.close();
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/common/property/PropertyPlaceholderTest.java b/core/src/test/java/org/elasticsearch/common/property/PropertyPlaceholderTest.java
deleted file mode 100644
index 0c4ea86..0000000
--- a/core/src/test/java/org/elasticsearch/common/property/PropertyPlaceholderTest.java
+++ /dev/null
@@ -1,182 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common.property;
-
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.util.LinkedHashMap;
-import java.util.Map;
-
-public class PropertyPlaceholderTest extends ESTestCase {
-
-    @Test
-    public void testSimple() {
-        PropertyPlaceholder propertyPlaceholder = new PropertyPlaceholder("{", "}", false);
-        Map<String, String> map = new LinkedHashMap<>();
-        map.put("foo1", "bar1");
-        map.put("foo2", "bar2");
-        PropertyPlaceholder.PlaceholderResolver placeholderResolver = new SimplePlaceholderResolver(map, false, true);
-        assertEquals("bar1", propertyPlaceholder.replacePlaceholders("{foo1}", placeholderResolver));
-        assertEquals("a bar1b", propertyPlaceholder.replacePlaceholders("a {foo1}b", placeholderResolver));
-        assertEquals("bar1bar2", propertyPlaceholder.replacePlaceholders("{foo1}{foo2}", placeholderResolver));
-        assertEquals("a bar1 b bar2 c", propertyPlaceholder.replacePlaceholders("a {foo1} b {foo2} c", placeholderResolver));
-    }
-
-    @Test
-    public void testVariousPrefixSuffix() {
-        // Test various prefix/suffix lengths
-        PropertyPlaceholder ppEqualsPrefix = new PropertyPlaceholder("{", "}", false);
-        PropertyPlaceholder ppLongerPrefix = new PropertyPlaceholder("${", "}", false);
-        PropertyPlaceholder ppShorterPrefix = new PropertyPlaceholder("{", "}}", false);
-        Map<String, String> map = new LinkedHashMap<>();
-        map.put("foo", "bar");
-        PropertyPlaceholder.PlaceholderResolver placeholderResolver = new SimplePlaceholderResolver(map, false, true);
-        assertEquals("bar", ppEqualsPrefix.replacePlaceholders("{foo}", placeholderResolver));
-        assertEquals("bar", ppLongerPrefix.replacePlaceholders("${foo}", placeholderResolver));
-        assertEquals("bar", ppShorterPrefix.replacePlaceholders("{foo}}", placeholderResolver));
-    }
-
-    @Test
-    public void testDefaultValue() {
-        PropertyPlaceholder propertyPlaceholder = new PropertyPlaceholder("${", "}", false);
-        Map<String, String> map = new LinkedHashMap<>();
-        PropertyPlaceholder.PlaceholderResolver placeholderResolver = new SimplePlaceholderResolver(map, false, true);
-        assertEquals("bar", propertyPlaceholder.replacePlaceholders("${foo:bar}", placeholderResolver));
-        assertEquals("", propertyPlaceholder.replacePlaceholders("${foo:}", placeholderResolver));
-    }
-
-    @Test
-    public void testIgnoredUnresolvedPlaceholder() {
-        PropertyPlaceholder propertyPlaceholder = new PropertyPlaceholder("${", "}", true);
-        Map<String, String> map = new LinkedHashMap<>();
-        PropertyPlaceholder.PlaceholderResolver placeholderResolver = new SimplePlaceholderResolver(map, false, true);
-        assertEquals("${foo}", propertyPlaceholder.replacePlaceholders("${foo}", placeholderResolver));
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testNotIgnoredUnresolvedPlaceholder() {
-        PropertyPlaceholder propertyPlaceholder = new PropertyPlaceholder("${", "}", false);
-        Map<String, String> map = new LinkedHashMap<>();
-        PropertyPlaceholder.PlaceholderResolver placeholderResolver = new SimplePlaceholderResolver(map, false, true);
-        propertyPlaceholder.replacePlaceholders("${foo}", placeholderResolver);
-    }
-
-    @Test
-    public void testShouldIgnoreMissing() {
-        PropertyPlaceholder propertyPlaceholder = new PropertyPlaceholder("${", "}", false);
-        Map<String, String> map = new LinkedHashMap<>();
-        PropertyPlaceholder.PlaceholderResolver placeholderResolver = new SimplePlaceholderResolver(map, true, true);
-        assertEquals("bar", propertyPlaceholder.replacePlaceholders("bar${foo}", placeholderResolver));
-    }
-
-    @Test
-    public void testRecursive() {
-        PropertyPlaceholder propertyPlaceholder = new PropertyPlaceholder("${", "}", false);
-        Map<String, String> map = new LinkedHashMap<>();
-        map.put("foo", "${foo1}");
-        map.put("foo1", "${foo2}");
-        map.put("foo2", "bar");
-        PropertyPlaceholder.PlaceholderResolver placeholderResolver = new SimplePlaceholderResolver(map, false, true);
-        assertEquals("bar", propertyPlaceholder.replacePlaceholders("${foo}", placeholderResolver));
-        assertEquals("abarb", propertyPlaceholder.replacePlaceholders("a${foo}b", placeholderResolver));
-    }
-
-    @Test
-    public void testNestedLongerPrefix() {
-        PropertyPlaceholder propertyPlaceholder = new PropertyPlaceholder("${", "}", false);
-        Map<String, String> map = new LinkedHashMap<>();
-        map.put("foo", "${foo1}");
-        map.put("foo1", "${foo2}");
-        map.put("foo2", "bar");
-        map.put("barbar", "baz");
-        PropertyPlaceholder.PlaceholderResolver placeholderResolver = new SimplePlaceholderResolver(map, false, true);
-        assertEquals("baz", propertyPlaceholder.replacePlaceholders("${bar${foo}}", placeholderResolver));
-    }
-
-    @Test
-    public void testNestedSameLengthPrefixSuffix() {
-        PropertyPlaceholder propertyPlaceholder = new PropertyPlaceholder("{", "}", false);
-        Map<String, String> map = new LinkedHashMap<>();
-        map.put("foo", "{foo1}");
-        map.put("foo1", "{foo2}");
-        map.put("foo2", "bar");
-        map.put("barbar", "baz");
-        PropertyPlaceholder.PlaceholderResolver placeholderResolver = new SimplePlaceholderResolver(map, false, true);
-        assertEquals("baz", propertyPlaceholder.replacePlaceholders("{bar{foo}}", placeholderResolver));
-    }
-
-    @Test
-    public void testNestedShorterPrefix() {
-        PropertyPlaceholder propertyPlaceholder = new PropertyPlaceholder("{", "}}", false);
-        Map<String, String> map = new LinkedHashMap<>();
-        map.put("foo", "{foo1}}");
-        map.put("foo1", "{foo2}}");
-        map.put("foo2", "bar");
-        map.put("barbar", "baz");
-        PropertyPlaceholder.PlaceholderResolver placeholderResolver = new SimplePlaceholderResolver(map, false, true);
-        assertEquals("baz", propertyPlaceholder.replacePlaceholders("{bar{foo}}}}", placeholderResolver));
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testCircularReference() {
-        PropertyPlaceholder propertyPlaceholder = new PropertyPlaceholder("${", "}", false);
-        Map<String, String> map = new LinkedHashMap<>();
-        map.put("foo", "${bar}");
-        map.put("bar", "${foo}");
-        PropertyPlaceholder.PlaceholderResolver placeholderResolver = new SimplePlaceholderResolver(map, false, true);
-        propertyPlaceholder.replacePlaceholders("${foo}", placeholderResolver);
-    }
-
-    @Test
-    public void testShouldRemoveMissing() {
-        PropertyPlaceholder propertyPlaceholder = new PropertyPlaceholder("${", "}", false);
-        Map<String, String> map = new LinkedHashMap<>();
-        PropertyPlaceholder.PlaceholderResolver placeholderResolver = new SimplePlaceholderResolver(map, true, false);
-        assertEquals("bar${foo}", propertyPlaceholder.replacePlaceholders("bar${foo}", placeholderResolver));
-    }
-
-    private class SimplePlaceholderResolver implements PropertyPlaceholder.PlaceholderResolver {
-        private Map<String, String> map;
-        private boolean shouldIgnoreMissing;
-        private boolean shouldRemoveMissing;
-
-        SimplePlaceholderResolver(Map<String, String> map, boolean shouldIgnoreMissing, boolean shouldRemoveMissing) {
-            this.map = map;
-            this.shouldIgnoreMissing = shouldIgnoreMissing;
-            this.shouldRemoveMissing = shouldRemoveMissing;
-        }
-
-        @Override
-        public String resolvePlaceholder(String placeholderName) {
-            return map.get(placeholderName);
-        }
-
-        @Override
-        public boolean shouldIgnoreMissing(String placeholderName) {
-            return shouldIgnoreMissing;
-        }
-
-        @Override
-        public boolean shouldRemoveMissingPlaceholder(String placeholderName) {
-            return shouldRemoveMissing;
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/common/property/PropertyPlaceholderTests.java b/core/src/test/java/org/elasticsearch/common/property/PropertyPlaceholderTests.java
new file mode 100644
index 0000000..bfa08dd
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/property/PropertyPlaceholderTests.java
@@ -0,0 +1,182 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.property;
+
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.util.LinkedHashMap;
+import java.util.Map;
+
+public class PropertyPlaceholderTests extends ESTestCase {
+
+    @Test
+    public void testSimple() {
+        PropertyPlaceholder propertyPlaceholder = new PropertyPlaceholder("{", "}", false);
+        Map<String, String> map = new LinkedHashMap<>();
+        map.put("foo1", "bar1");
+        map.put("foo2", "bar2");
+        PropertyPlaceholder.PlaceholderResolver placeholderResolver = new SimplePlaceholderResolver(map, false, true);
+        assertEquals("bar1", propertyPlaceholder.replacePlaceholders("{foo1}", placeholderResolver));
+        assertEquals("a bar1b", propertyPlaceholder.replacePlaceholders("a {foo1}b", placeholderResolver));
+        assertEquals("bar1bar2", propertyPlaceholder.replacePlaceholders("{foo1}{foo2}", placeholderResolver));
+        assertEquals("a bar1 b bar2 c", propertyPlaceholder.replacePlaceholders("a {foo1} b {foo2} c", placeholderResolver));
+    }
+
+    @Test
+    public void testVariousPrefixSuffix() {
+        // Test various prefix/suffix lengths
+        PropertyPlaceholder ppEqualsPrefix = new PropertyPlaceholder("{", "}", false);
+        PropertyPlaceholder ppLongerPrefix = new PropertyPlaceholder("${", "}", false);
+        PropertyPlaceholder ppShorterPrefix = new PropertyPlaceholder("{", "}}", false);
+        Map<String, String> map = new LinkedHashMap<>();
+        map.put("foo", "bar");
+        PropertyPlaceholder.PlaceholderResolver placeholderResolver = new SimplePlaceholderResolver(map, false, true);
+        assertEquals("bar", ppEqualsPrefix.replacePlaceholders("{foo}", placeholderResolver));
+        assertEquals("bar", ppLongerPrefix.replacePlaceholders("${foo}", placeholderResolver));
+        assertEquals("bar", ppShorterPrefix.replacePlaceholders("{foo}}", placeholderResolver));
+    }
+
+    @Test
+    public void testDefaultValue() {
+        PropertyPlaceholder propertyPlaceholder = new PropertyPlaceholder("${", "}", false);
+        Map<String, String> map = new LinkedHashMap<>();
+        PropertyPlaceholder.PlaceholderResolver placeholderResolver = new SimplePlaceholderResolver(map, false, true);
+        assertEquals("bar", propertyPlaceholder.replacePlaceholders("${foo:bar}", placeholderResolver));
+        assertEquals("", propertyPlaceholder.replacePlaceholders("${foo:}", placeholderResolver));
+    }
+
+    @Test
+    public void testIgnoredUnresolvedPlaceholder() {
+        PropertyPlaceholder propertyPlaceholder = new PropertyPlaceholder("${", "}", true);
+        Map<String, String> map = new LinkedHashMap<>();
+        PropertyPlaceholder.PlaceholderResolver placeholderResolver = new SimplePlaceholderResolver(map, false, true);
+        assertEquals("${foo}", propertyPlaceholder.replacePlaceholders("${foo}", placeholderResolver));
+    }
+
+    @Test(expected = IllegalArgumentException.class)
+    public void testNotIgnoredUnresolvedPlaceholder() {
+        PropertyPlaceholder propertyPlaceholder = new PropertyPlaceholder("${", "}", false);
+        Map<String, String> map = new LinkedHashMap<>();
+        PropertyPlaceholder.PlaceholderResolver placeholderResolver = new SimplePlaceholderResolver(map, false, true);
+        propertyPlaceholder.replacePlaceholders("${foo}", placeholderResolver);
+    }
+
+    @Test
+    public void testShouldIgnoreMissing() {
+        PropertyPlaceholder propertyPlaceholder = new PropertyPlaceholder("${", "}", false);
+        Map<String, String> map = new LinkedHashMap<>();
+        PropertyPlaceholder.PlaceholderResolver placeholderResolver = new SimplePlaceholderResolver(map, true, true);
+        assertEquals("bar", propertyPlaceholder.replacePlaceholders("bar${foo}", placeholderResolver));
+    }
+
+    @Test
+    public void testRecursive() {
+        PropertyPlaceholder propertyPlaceholder = new PropertyPlaceholder("${", "}", false);
+        Map<String, String> map = new LinkedHashMap<>();
+        map.put("foo", "${foo1}");
+        map.put("foo1", "${foo2}");
+        map.put("foo2", "bar");
+        PropertyPlaceholder.PlaceholderResolver placeholderResolver = new SimplePlaceholderResolver(map, false, true);
+        assertEquals("bar", propertyPlaceholder.replacePlaceholders("${foo}", placeholderResolver));
+        assertEquals("abarb", propertyPlaceholder.replacePlaceholders("a${foo}b", placeholderResolver));
+    }
+
+    @Test
+    public void testNestedLongerPrefix() {
+        PropertyPlaceholder propertyPlaceholder = new PropertyPlaceholder("${", "}", false);
+        Map<String, String> map = new LinkedHashMap<>();
+        map.put("foo", "${foo1}");
+        map.put("foo1", "${foo2}");
+        map.put("foo2", "bar");
+        map.put("barbar", "baz");
+        PropertyPlaceholder.PlaceholderResolver placeholderResolver = new SimplePlaceholderResolver(map, false, true);
+        assertEquals("baz", propertyPlaceholder.replacePlaceholders("${bar${foo}}", placeholderResolver));
+    }
+
+    @Test
+    public void testNestedSameLengthPrefixSuffix() {
+        PropertyPlaceholder propertyPlaceholder = new PropertyPlaceholder("{", "}", false);
+        Map<String, String> map = new LinkedHashMap<>();
+        map.put("foo", "{foo1}");
+        map.put("foo1", "{foo2}");
+        map.put("foo2", "bar");
+        map.put("barbar", "baz");
+        PropertyPlaceholder.PlaceholderResolver placeholderResolver = new SimplePlaceholderResolver(map, false, true);
+        assertEquals("baz", propertyPlaceholder.replacePlaceholders("{bar{foo}}", placeholderResolver));
+    }
+
+    @Test
+    public void testNestedShorterPrefix() {
+        PropertyPlaceholder propertyPlaceholder = new PropertyPlaceholder("{", "}}", false);
+        Map<String, String> map = new LinkedHashMap<>();
+        map.put("foo", "{foo1}}");
+        map.put("foo1", "{foo2}}");
+        map.put("foo2", "bar");
+        map.put("barbar", "baz");
+        PropertyPlaceholder.PlaceholderResolver placeholderResolver = new SimplePlaceholderResolver(map, false, true);
+        assertEquals("baz", propertyPlaceholder.replacePlaceholders("{bar{foo}}}}", placeholderResolver));
+    }
+
+    @Test(expected = IllegalArgumentException.class)
+    public void testCircularReference() {
+        PropertyPlaceholder propertyPlaceholder = new PropertyPlaceholder("${", "}", false);
+        Map<String, String> map = new LinkedHashMap<>();
+        map.put("foo", "${bar}");
+        map.put("bar", "${foo}");
+        PropertyPlaceholder.PlaceholderResolver placeholderResolver = new SimplePlaceholderResolver(map, false, true);
+        propertyPlaceholder.replacePlaceholders("${foo}", placeholderResolver);
+    }
+
+    @Test
+    public void testShouldRemoveMissing() {
+        PropertyPlaceholder propertyPlaceholder = new PropertyPlaceholder("${", "}", false);
+        Map<String, String> map = new LinkedHashMap<>();
+        PropertyPlaceholder.PlaceholderResolver placeholderResolver = new SimplePlaceholderResolver(map, true, false);
+        assertEquals("bar${foo}", propertyPlaceholder.replacePlaceholders("bar${foo}", placeholderResolver));
+    }
+
+    private class SimplePlaceholderResolver implements PropertyPlaceholder.PlaceholderResolver {
+        private Map<String, String> map;
+        private boolean shouldIgnoreMissing;
+        private boolean shouldRemoveMissing;
+
+        SimplePlaceholderResolver(Map<String, String> map, boolean shouldIgnoreMissing, boolean shouldRemoveMissing) {
+            this.map = map;
+            this.shouldIgnoreMissing = shouldIgnoreMissing;
+            this.shouldRemoveMissing = shouldRemoveMissing;
+        }
+
+        @Override
+        public String resolvePlaceholder(String placeholderName) {
+            return map.get(placeholderName);
+        }
+
+        @Override
+        public boolean shouldIgnoreMissing(String placeholderName) {
+            return shouldIgnoreMissing;
+        }
+
+        @Override
+        public boolean shouldRemoveMissingPlaceholder(String placeholderName) {
+            return shouldRemoveMissing;
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/common/recycler/AbstractRecyclerTestCase.java b/core/src/test/java/org/elasticsearch/common/recycler/AbstractRecyclerTestCase.java
new file mode 100644
index 0000000..b48654a
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/recycler/AbstractRecyclerTestCase.java
@@ -0,0 +1,180 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.recycler;
+
+import org.elasticsearch.common.recycler.Recycler.V;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+
+public abstract class AbstractRecyclerTestCase extends ESTestCase {
+
+    // marker states for data
+    protected static final byte FRESH = 1;
+    protected static final byte RECYCLED = 2;
+    protected static final byte DEAD = 42;
+
+    protected static final Recycler.C<byte[]> RECYCLER_C = new AbstractRecyclerC<byte[]>() {
+
+        @Override
+        public byte[] newInstance(int sizing) {
+            byte[] value = new byte[10];
+            // "fresh" is intentionally not 0 to ensure we covered this code path
+            Arrays.fill(value, FRESH);
+            return value;
+        }
+
+        @Override
+        public void recycle(byte[] value) {
+            Arrays.fill(value, RECYCLED);
+        }
+
+        @Override
+        public void destroy(byte[] value) {
+            // we cannot really free the internals of a byte[], so mark it for verification
+            Arrays.fill(value, DEAD);
+        }
+
+    };
+
+    protected void assertFresh(byte[] data) {
+        assertNotNull(data);
+        for (int i = 0; i < data.length; ++i) {
+            assertEquals(FRESH, data[i]);
+        }
+    }
+
+    protected void assertRecycled(byte[] data) {
+        assertNotNull(data);
+        for (int i = 0; i < data.length; ++i) {
+            assertEquals(RECYCLED, data[i]);
+        }
+    }
+
+    protected void assertDead(byte[] data) {
+        assertNotNull(data);
+        for (int i = 0; i < data.length; ++i) {
+            assertEquals(DEAD, data[i]);
+        }
+    }
+
+    protected abstract Recycler<byte[]> newRecycler(int limit);
+
+    protected int limit = randomIntBetween(5, 10);
+
+    public void testReuse() {
+        Recycler<byte[]> r = newRecycler(limit);
+        Recycler.V<byte[]> o = r.obtain();
+        assertFalse(o.isRecycled());
+        final byte[] b1 = o.v();
+        assertFresh(b1);
+        o.close();
+        assertRecycled(b1);
+        o = r.obtain();
+        final byte[] b2 = o.v();
+        if (o.isRecycled()) {
+            assertRecycled(b2);
+            assertSame(b1, b2);
+        } else {
+            assertFresh(b2);
+            assertNotSame(b1, b2);
+        }
+        o.close();
+        r.close();
+    }
+
+    public void testRecycle() {
+        Recycler<byte[]> r = newRecycler(limit);
+        Recycler.V<byte[]> o = r.obtain();
+        assertFresh(o.v());
+        getRandom().nextBytes(o.v());
+        o.close();
+        o = r.obtain();
+        assertRecycled(o.v());
+        o.close();
+        r.close();
+    }
+
+    public void testDoubleRelease() {
+        final Recycler<byte[]> r = newRecycler(limit);
+        final Recycler.V<byte[]> v1 = r.obtain();
+        v1.close();
+        try {
+            v1.close();
+        } catch (IllegalStateException e) {
+            // impl has protection against double release: ok
+            return;
+        }
+        // otherwise ensure that the impl may not be returned twice
+        final Recycler.V<byte[]> v2 = r.obtain();
+        final Recycler.V<byte[]> v3 = r.obtain();
+        assertNotSame(v2.v(), v3.v());
+        r.close();
+    }
+
+    public void testDestroyWhenOverCapacity() {
+        Recycler<byte[]> r = newRecycler(limit);
+
+        // get & keep reference to new/recycled data
+        Recycler.V<byte[]> o = r.obtain();
+        byte[] data = o.v();
+        assertFresh(data);
+
+        // now exhaust the recycler
+        List<V<byte[]>> vals = new ArrayList<>(limit);
+        for (int i = 0; i < limit ; ++i) {
+            vals.add(r.obtain());
+        }
+        // Recycler size increases on release, not on obtain!
+        for (V<byte[]> v: vals) {
+            v.close();
+        }
+
+        // release first ref, verify for destruction
+        o.close();
+        assertDead(data);
+
+        // close the rest
+        r.close();
+    }
+
+    public void testClose() {
+        Recycler<byte[]> r = newRecycler(limit);
+
+        // get & keep reference to pooled data
+        Recycler.V<byte[]> o = r.obtain();
+        byte[] data = o.v();
+        assertFresh(data);
+
+        // randomize & return to pool
+        getRandom().nextBytes(data);
+        o.close();
+
+        // verify that recycle() ran
+        assertRecycled(data);
+
+        // closing the recycler should mark recycled instances via destroy()
+        r.close();
+        assertDead(data);
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/common/recycler/AbstractRecyclerTests.java b/core/src/test/java/org/elasticsearch/common/recycler/AbstractRecyclerTests.java
deleted file mode 100644
index 3937644..0000000
--- a/core/src/test/java/org/elasticsearch/common/recycler/AbstractRecyclerTests.java
+++ /dev/null
@@ -1,180 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common.recycler;
-
-import org.elasticsearch.common.recycler.Recycler.V;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-
-public abstract class AbstractRecyclerTests extends ESTestCase {
-
-    // marker states for data
-    protected static final byte FRESH = 1;
-    protected static final byte RECYCLED = 2;
-    protected static final byte DEAD = 42;
-
-    protected static final Recycler.C<byte[]> RECYCLER_C = new AbstractRecyclerC<byte[]>() {
-
-        @Override
-        public byte[] newInstance(int sizing) {
-            byte[] value = new byte[10];
-            // "fresh" is intentionally not 0 to ensure we covered this code path
-            Arrays.fill(value, FRESH);
-            return value;
-        }
-
-        @Override
-        public void recycle(byte[] value) {
-            Arrays.fill(value, RECYCLED);
-        }
-
-        @Override
-        public void destroy(byte[] value) {
-            // we cannot really free the internals of a byte[], so mark it for verification
-            Arrays.fill(value, DEAD);
-        }
-
-    };
-
-    protected void assertFresh(byte[] data) {
-        assertNotNull(data);
-        for (int i = 0; i < data.length; ++i) {
-            assertEquals(FRESH, data[i]);
-        }
-    }
-
-    protected void assertRecycled(byte[] data) {
-        assertNotNull(data);
-        for (int i = 0; i < data.length; ++i) {
-            assertEquals(RECYCLED, data[i]);
-        }
-    }
-
-    protected void assertDead(byte[] data) {
-        assertNotNull(data);
-        for (int i = 0; i < data.length; ++i) {
-            assertEquals(DEAD, data[i]);
-        }
-    }
-
-    protected abstract Recycler<byte[]> newRecycler(int limit);
-
-    protected int limit = randomIntBetween(5, 10);
-
-    public void testReuse() {
-        Recycler<byte[]> r = newRecycler(limit);
-        Recycler.V<byte[]> o = r.obtain();
-        assertFalse(o.isRecycled());
-        final byte[] b1 = o.v();
-        assertFresh(b1);
-        o.close();
-        assertRecycled(b1);
-        o = r.obtain();
-        final byte[] b2 = o.v();
-        if (o.isRecycled()) {
-            assertRecycled(b2);
-            assertSame(b1, b2);
-        } else {
-            assertFresh(b2);
-            assertNotSame(b1, b2);
-        }
-        o.close();
-        r.close();
-    }
-
-    public void testRecycle() {
-        Recycler<byte[]> r = newRecycler(limit);
-        Recycler.V<byte[]> o = r.obtain();
-        assertFresh(o.v());
-        getRandom().nextBytes(o.v());
-        o.close();
-        o = r.obtain();
-        assertRecycled(o.v());
-        o.close();
-        r.close();
-    }
-
-    public void testDoubleRelease() {
-        final Recycler<byte[]> r = newRecycler(limit);
-        final Recycler.V<byte[]> v1 = r.obtain();
-        v1.close();
-        try {
-            v1.close();
-        } catch (IllegalStateException e) {
-            // impl has protection against double release: ok
-            return;
-        }
-        // otherwise ensure that the impl may not be returned twice
-        final Recycler.V<byte[]> v2 = r.obtain();
-        final Recycler.V<byte[]> v3 = r.obtain();
-        assertNotSame(v2.v(), v3.v());
-        r.close();
-    }
-
-    public void testDestroyWhenOverCapacity() {
-        Recycler<byte[]> r = newRecycler(limit);
-
-        // get & keep reference to new/recycled data
-        Recycler.V<byte[]> o = r.obtain();
-        byte[] data = o.v();
-        assertFresh(data);
-
-        // now exhaust the recycler
-        List<V<byte[]>> vals = new ArrayList<>(limit);
-        for (int i = 0; i < limit ; ++i) {
-            vals.add(r.obtain());
-        }
-        // Recycler size increases on release, not on obtain!
-        for (V<byte[]> v: vals) {
-            v.close();
-        }
-
-        // release first ref, verify for destruction
-        o.close();
-        assertDead(data);
-
-        // close the rest
-        r.close();
-    }
-
-    public void testClose() {
-        Recycler<byte[]> r = newRecycler(limit);
-
-        // get & keep reference to pooled data
-        Recycler.V<byte[]> o = r.obtain();
-        byte[] data = o.v();
-        assertFresh(data);
-
-        // randomize & return to pool
-        getRandom().nextBytes(data);
-        o.close();
-
-        // verify that recycle() ran
-        assertRecycled(data);
-
-        // closing the recycler should mark recycled instances via destroy()
-        r.close();
-        assertDead(data);
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/common/recycler/ConcurrentRecyclerTests.java b/core/src/test/java/org/elasticsearch/common/recycler/ConcurrentRecyclerTests.java
index c8c4c2e..f4931b1 100644
--- a/core/src/test/java/org/elasticsearch/common/recycler/ConcurrentRecyclerTests.java
+++ b/core/src/test/java/org/elasticsearch/common/recycler/ConcurrentRecyclerTests.java
@@ -19,7 +19,7 @@
 
 package org.elasticsearch.common.recycler;
 
-public class ConcurrentRecyclerTests extends AbstractRecyclerTests {
+public class ConcurrentRecyclerTests extends AbstractRecyclerTestCase {
 
     @Override
     protected Recycler<byte[]> newRecycler(int limit) {
diff --git a/core/src/test/java/org/elasticsearch/common/recycler/LockedRecyclerTests.java b/core/src/test/java/org/elasticsearch/common/recycler/LockedRecyclerTests.java
index 7d56dff..ad7b294 100644
--- a/core/src/test/java/org/elasticsearch/common/recycler/LockedRecyclerTests.java
+++ b/core/src/test/java/org/elasticsearch/common/recycler/LockedRecyclerTests.java
@@ -19,7 +19,7 @@
 
 package org.elasticsearch.common.recycler;
 
-public class LockedRecyclerTests extends AbstractRecyclerTests {
+public class LockedRecyclerTests extends AbstractRecyclerTestCase {
 
     @Override
     protected Recycler<byte[]> newRecycler(int limit) {
diff --git a/core/src/test/java/org/elasticsearch/common/recycler/NoneRecyclerTests.java b/core/src/test/java/org/elasticsearch/common/recycler/NoneRecyclerTests.java
index d4acb54..f053702 100644
--- a/core/src/test/java/org/elasticsearch/common/recycler/NoneRecyclerTests.java
+++ b/core/src/test/java/org/elasticsearch/common/recycler/NoneRecyclerTests.java
@@ -19,7 +19,7 @@
 
 package org.elasticsearch.common.recycler;
 
-public class NoneRecyclerTests extends AbstractRecyclerTests {
+public class NoneRecyclerTests extends AbstractRecyclerTestCase {
 
     @Override
     protected Recycler<byte[]> newRecycler(int limit) {
diff --git a/core/src/test/java/org/elasticsearch/common/recycler/QueueRecyclerTests.java b/core/src/test/java/org/elasticsearch/common/recycler/QueueRecyclerTests.java
index 20e229a..649815db 100644
--- a/core/src/test/java/org/elasticsearch/common/recycler/QueueRecyclerTests.java
+++ b/core/src/test/java/org/elasticsearch/common/recycler/QueueRecyclerTests.java
@@ -19,7 +19,7 @@
 
 package org.elasticsearch.common.recycler;
 
-public class QueueRecyclerTests extends AbstractRecyclerTests {
+public class QueueRecyclerTests extends AbstractRecyclerTestCase {
 
     @Override
     protected Recycler<byte[]> newRecycler(int limit) {
diff --git a/core/src/test/java/org/elasticsearch/common/unit/FuzzinessTests.java b/core/src/test/java/org/elasticsearch/common/unit/FuzzinessTests.java
index 807b4a7..234e341 100644
--- a/core/src/test/java/org/elasticsearch/common/unit/FuzzinessTests.java
+++ b/core/src/test/java/org/elasticsearch/common/unit/FuzzinessTests.java
@@ -18,8 +18,6 @@
  */
 package org.elasticsearch.common.unit;
 
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.xcontent.XContent;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.XContentType;
@@ -164,29 +162,4 @@ public class FuzzinessTests extends ESTestCase {
         }
     }
 
-    @Test
-    public void testSerialization() throws IOException {
-        Fuzziness fuzziness = Fuzziness.AUTO;
-        Fuzziness deserializedFuzziness = doSerializeRoundtrip(fuzziness);
-        assertEquals(fuzziness, deserializedFuzziness);
-
-        fuzziness = Fuzziness.fromEdits(randomIntBetween(0, 2));
-        deserializedFuzziness = doSerializeRoundtrip(fuzziness);
-        assertEquals(fuzziness, deserializedFuzziness);
-    }
-
-    @Test
-    public void testSerializationAuto() throws IOException {
-        Fuzziness fuzziness = Fuzziness.AUTO;
-        Fuzziness deserializedFuzziness = doSerializeRoundtrip(fuzziness);
-        assertEquals(fuzziness, deserializedFuzziness);
-        assertEquals(fuzziness.asInt(), deserializedFuzziness.asInt());
-    }
-
-    private static Fuzziness doSerializeRoundtrip(Fuzziness in) throws IOException {
-        BytesStreamOutput output = new BytesStreamOutput();
-        in.writeTo(output);
-        StreamInput streamInput = StreamInput.wrap(output.bytes());
-        return Fuzziness.readFuzzinessFrom(streamInput);
-    }
 }
diff --git a/core/src/test/java/org/elasticsearch/common/util/CancellableThreadsTest.java b/core/src/test/java/org/elasticsearch/common/util/CancellableThreadsTest.java
deleted file mode 100644
index 85dd1c1..0000000
--- a/core/src/test/java/org/elasticsearch/common/util/CancellableThreadsTest.java
+++ /dev/null
@@ -1,141 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.common.util;
-
-import org.elasticsearch.common.util.CancellableThreads.Interruptable;
-import org.elasticsearch.test.ESTestCase;
-import org.hamcrest.Matchers;
-import org.junit.Test;
-
-import java.util.concurrent.CountDownLatch;
-
-public class CancellableThreadsTest extends ESTestCase {
-
-    public static class CustomException extends RuntimeException {
-
-        public CustomException(String msg) {
-            super(msg);
-        }
-    }
-
-    private class TestPlan {
-        public final int id;
-        public final boolean busySpin;
-        public final boolean exceptBeforeCancel;
-        public final boolean exitBeforeCancel;
-        public final boolean exceptAfterCancel;
-        public final boolean presetInterrupt;
-
-        private TestPlan(int id) {
-            this.id = id;
-            this.busySpin = randomBoolean();
-            this.exceptBeforeCancel = randomBoolean();
-            this.exitBeforeCancel = randomBoolean();
-            this.exceptAfterCancel = randomBoolean();
-            this.presetInterrupt = randomBoolean();
-        }
-    }
-
-
-    @Test
-    public void testCancellableThreads() throws InterruptedException {
-        Thread[] threads = new Thread[randomIntBetween(3, 10)];
-        final TestPlan[] plans = new TestPlan[threads.length];
-        final Throwable[] throwables = new Throwable[threads.length];
-        final boolean[] interrupted = new boolean[threads.length];
-        final CancellableThreads cancellableThreads = new CancellableThreads();
-        final CountDownLatch readyForCancel = new CountDownLatch(threads.length);
-        for (int i = 0; i < threads.length; i++) {
-            final TestPlan plan = new TestPlan(i);
-            plans[i] = plan;
-            threads[i] = new Thread(new Runnable() {
-                @Override
-                public void run() {
-                    try {
-                        if (plan.presetInterrupt) {
-                            Thread.currentThread().interrupt();
-                        }
-                        cancellableThreads.execute(new Interruptable() {
-                            @Override
-                            public void run() throws InterruptedException {
-                                assertFalse("interrupt thread should have been clear", Thread.currentThread().isInterrupted());
-                                if (plan.exceptBeforeCancel) {
-                                    throw new CustomException("thread [" + plan.id + "] pre-cancel exception");
-                                } else if (plan.exitBeforeCancel) {
-                                    return;
-                                }
-                                readyForCancel.countDown();
-                                try {
-                                    if (plan.busySpin) {
-                                        while (!Thread.currentThread().isInterrupted()) {
-                                        }
-                                    } else {
-                                        Thread.sleep(50000);
-                                    }
-                                } finally {
-                                    if (plan.exceptAfterCancel) {
-                                        throw new CustomException("thread [" + plan.id + "] post-cancel exception");
-                                    }
-                                }
-                            }
-                        });
-                    } catch (Throwable t) {
-                        throwables[plan.id] = t;
-                    }
-                    if (plan.exceptBeforeCancel || plan.exitBeforeCancel) {
-                        // we have to mark we're ready now (actually done).
-                        readyForCancel.countDown();
-                    }
-                    interrupted[plan.id] = Thread.currentThread().isInterrupted();
-
-                }
-            });
-            threads[i].setDaemon(true);
-            threads[i].start();
-        }
-
-        readyForCancel.await();
-        cancellableThreads.cancel("test");
-        for (Thread thread : threads) {
-            thread.join(20000);
-            assertFalse(thread.isAlive());
-        }
-        for (int i = 0; i < threads.length; i++) {
-            TestPlan plan = plans[i];
-            if (plan.exceptBeforeCancel) {
-                assertThat(throwables[i], Matchers.instanceOf(CustomException.class));
-            } else if (plan.exitBeforeCancel) {
-                assertNull(throwables[i]);
-            } else {
-                // in all other cases, we expect a cancellation exception.
-                assertThat(throwables[i], Matchers.instanceOf(CancellableThreads.ExecutionCancelledException.class));
-                if (plan.exceptAfterCancel) {
-                    assertThat(throwables[i].getSuppressed(),
-                            Matchers.arrayContaining(
-                                    Matchers.instanceOf(CustomException.class)
-                            ));
-                } else {
-                    assertThat(throwables[i].getSuppressed(), Matchers.emptyArray());
-                }
-            }
-            assertThat(interrupted[plan.id], Matchers.equalTo(plan.presetInterrupt));
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/common/util/CancellableThreadsTests.java b/core/src/test/java/org/elasticsearch/common/util/CancellableThreadsTests.java
new file mode 100644
index 0000000..2bdaea3
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/util/CancellableThreadsTests.java
@@ -0,0 +1,141 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.common.util;
+
+import org.elasticsearch.common.util.CancellableThreads.Interruptable;
+import org.elasticsearch.test.ESTestCase;
+import org.hamcrest.Matchers;
+import org.junit.Test;
+
+import java.util.concurrent.CountDownLatch;
+
+public class CancellableThreadsTests extends ESTestCase {
+
+    public static class CustomException extends RuntimeException {
+
+        public CustomException(String msg) {
+            super(msg);
+        }
+    }
+
+    private class TestPlan {
+        public final int id;
+        public final boolean busySpin;
+        public final boolean exceptBeforeCancel;
+        public final boolean exitBeforeCancel;
+        public final boolean exceptAfterCancel;
+        public final boolean presetInterrupt;
+
+        private TestPlan(int id) {
+            this.id = id;
+            this.busySpin = randomBoolean();
+            this.exceptBeforeCancel = randomBoolean();
+            this.exitBeforeCancel = randomBoolean();
+            this.exceptAfterCancel = randomBoolean();
+            this.presetInterrupt = randomBoolean();
+        }
+    }
+
+
+    @Test
+    public void testCancellableThreads() throws InterruptedException {
+        Thread[] threads = new Thread[randomIntBetween(3, 10)];
+        final TestPlan[] plans = new TestPlan[threads.length];
+        final Throwable[] throwables = new Throwable[threads.length];
+        final boolean[] interrupted = new boolean[threads.length];
+        final CancellableThreads cancellableThreads = new CancellableThreads();
+        final CountDownLatch readyForCancel = new CountDownLatch(threads.length);
+        for (int i = 0; i < threads.length; i++) {
+            final TestPlan plan = new TestPlan(i);
+            plans[i] = plan;
+            threads[i] = new Thread(new Runnable() {
+                @Override
+                public void run() {
+                    try {
+                        if (plan.presetInterrupt) {
+                            Thread.currentThread().interrupt();
+                        }
+                        cancellableThreads.execute(new Interruptable() {
+                            @Override
+                            public void run() throws InterruptedException {
+                                assertFalse("interrupt thread should have been clear", Thread.currentThread().isInterrupted());
+                                if (plan.exceptBeforeCancel) {
+                                    throw new CustomException("thread [" + plan.id + "] pre-cancel exception");
+                                } else if (plan.exitBeforeCancel) {
+                                    return;
+                                }
+                                readyForCancel.countDown();
+                                try {
+                                    if (plan.busySpin) {
+                                        while (!Thread.currentThread().isInterrupted()) {
+                                        }
+                                    } else {
+                                        Thread.sleep(50000);
+                                    }
+                                } finally {
+                                    if (plan.exceptAfterCancel) {
+                                        throw new CustomException("thread [" + plan.id + "] post-cancel exception");
+                                    }
+                                }
+                            }
+                        });
+                    } catch (Throwable t) {
+                        throwables[plan.id] = t;
+                    }
+                    if (plan.exceptBeforeCancel || plan.exitBeforeCancel) {
+                        // we have to mark we're ready now (actually done).
+                        readyForCancel.countDown();
+                    }
+                    interrupted[plan.id] = Thread.currentThread().isInterrupted();
+
+                }
+            });
+            threads[i].setDaemon(true);
+            threads[i].start();
+        }
+
+        readyForCancel.await();
+        cancellableThreads.cancel("test");
+        for (Thread thread : threads) {
+            thread.join(20000);
+            assertFalse(thread.isAlive());
+        }
+        for (int i = 0; i < threads.length; i++) {
+            TestPlan plan = plans[i];
+            if (plan.exceptBeforeCancel) {
+                assertThat(throwables[i], Matchers.instanceOf(CustomException.class));
+            } else if (plan.exitBeforeCancel) {
+                assertNull(throwables[i]);
+            } else {
+                // in all other cases, we expect a cancellation exception.
+                assertThat(throwables[i], Matchers.instanceOf(CancellableThreads.ExecutionCancelledException.class));
+                if (plan.exceptAfterCancel) {
+                    assertThat(throwables[i].getSuppressed(),
+                            Matchers.arrayContaining(
+                                    Matchers.instanceOf(CustomException.class)
+                            ));
+                } else {
+                    assertThat(throwables[i].getSuppressed(), Matchers.emptyArray());
+                }
+            }
+            assertThat(interrupted[plan.id], Matchers.equalTo(plan.presetInterrupt));
+        }
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/common/util/concurrent/CountDownTest.java b/core/src/test/java/org/elasticsearch/common/util/concurrent/CountDownTest.java
deleted file mode 100644
index 52fe724..0000000
--- a/core/src/test/java/org/elasticsearch/common/util/concurrent/CountDownTest.java
+++ /dev/null
@@ -1,105 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common.util.concurrent;
-
-import org.elasticsearch.test.ESTestCase;
-import org.hamcrest.Matchers;
-import org.junit.Test;
-
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.atomic.AtomicInteger;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-
-
-public class CountDownTest extends ESTestCase {
-
-    @Test
-    public void testConcurrent() throws InterruptedException {
-        final AtomicInteger count = new AtomicInteger(0);
-        final CountDown countDown = new CountDown(scaledRandomIntBetween(10, 1000));
-        Thread[] threads = new Thread[between(3, 10)];
-        final CountDownLatch latch = new CountDownLatch(1);
-        for (int i = 0; i < threads.length; i++) {
-            threads[i] = new Thread() {
-
-                @Override
-                public void run() {
-                    try {
-                        latch.await();
-                    } catch (InterruptedException e) {
-                        throw new RuntimeException();
-                    }
-                    while (true) {
-                        if(frequently()) {
-                            if (countDown.isCountedDown()) {
-                                break;
-                            }
-                        }
-                        if (countDown.countDown()) {
-                            count.incrementAndGet();
-                            break;
-                        }
-                    }
-                }
-            };
-            threads[i].start();
-        }
-        latch.countDown();
-        Thread.yield();
-        if (rarely()) {
-            if (countDown.fastForward()) {
-                count.incrementAndGet();
-            }
-            assertThat(countDown.isCountedDown(), equalTo(true));
-            assertThat(countDown.fastForward(), equalTo(false));
-
-        }
-
-        for (Thread thread : threads) {
-            thread.join();
-        }
-        assertThat(countDown.isCountedDown(), equalTo(true));
-        assertThat(count.get(), Matchers.equalTo(1));
-    }
-    
-    @Test
-    public void testSingleThreaded() {
-        int atLeast = scaledRandomIntBetween(10, 1000);
-        final CountDown countDown = new CountDown(atLeast);
-        while(!countDown.isCountedDown()) {
-            atLeast--;
-            if (countDown.countDown()) {
-                assertThat(atLeast, equalTo(0));
-                assertThat(countDown.isCountedDown(), equalTo(true));
-                assertThat(countDown.fastForward(), equalTo(false));
-                break;
-            }
-            if (rarely()) {
-                assertThat(countDown.fastForward(), equalTo(true));
-                assertThat(countDown.isCountedDown(), equalTo(true));
-                assertThat(countDown.fastForward(), equalTo(false));
-            }
-            assertThat(atLeast, greaterThan(0));
-        }
-
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/common/util/concurrent/CountDownTests.java b/core/src/test/java/org/elasticsearch/common/util/concurrent/CountDownTests.java
new file mode 100644
index 0000000..6a45bfb
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/util/concurrent/CountDownTests.java
@@ -0,0 +1,105 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.util.concurrent;
+
+import org.elasticsearch.test.ESTestCase;
+import org.hamcrest.Matchers;
+import org.junit.Test;
+
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.greaterThan;
+
+
+public class CountDownTests extends ESTestCase {
+
+    @Test
+    public void testConcurrent() throws InterruptedException {
+        final AtomicInteger count = new AtomicInteger(0);
+        final CountDown countDown = new CountDown(scaledRandomIntBetween(10, 1000));
+        Thread[] threads = new Thread[between(3, 10)];
+        final CountDownLatch latch = new CountDownLatch(1);
+        for (int i = 0; i < threads.length; i++) {
+            threads[i] = new Thread() {
+
+                @Override
+                public void run() {
+                    try {
+                        latch.await();
+                    } catch (InterruptedException e) {
+                        throw new RuntimeException();
+                    }
+                    while (true) {
+                        if(frequently()) {
+                            if (countDown.isCountedDown()) {
+                                break;
+                            }
+                        }
+                        if (countDown.countDown()) {
+                            count.incrementAndGet();
+                            break;
+                        }
+                    }
+                }
+            };
+            threads[i].start();
+        }
+        latch.countDown();
+        Thread.yield();
+        if (rarely()) {
+            if (countDown.fastForward()) {
+                count.incrementAndGet();
+            }
+            assertThat(countDown.isCountedDown(), equalTo(true));
+            assertThat(countDown.fastForward(), equalTo(false));
+
+        }
+
+        for (Thread thread : threads) {
+            thread.join();
+        }
+        assertThat(countDown.isCountedDown(), equalTo(true));
+        assertThat(count.get(), Matchers.equalTo(1));
+    }
+    
+    @Test
+    public void testSingleThreaded() {
+        int atLeast = scaledRandomIntBetween(10, 1000);
+        final CountDown countDown = new CountDown(atLeast);
+        while(!countDown.isCountedDown()) {
+            atLeast--;
+            if (countDown.countDown()) {
+                assertThat(atLeast, equalTo(0));
+                assertThat(countDown.isCountedDown(), equalTo(true));
+                assertThat(countDown.fastForward(), equalTo(false));
+                break;
+            }
+            if (rarely()) {
+                assertThat(countDown.fastForward(), equalTo(true));
+                assertThat(countDown.isCountedDown(), equalTo(true));
+                assertThat(countDown.fastForward(), equalTo(false));
+            }
+            assertThat(atLeast, greaterThan(0));
+        }
+
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/common/util/concurrent/RefCountedTest.java b/core/src/test/java/org/elasticsearch/common/util/concurrent/RefCountedTest.java
deleted file mode 100644
index 1301c9e..0000000
--- a/core/src/test/java/org/elasticsearch/common/util/concurrent/RefCountedTest.java
+++ /dev/null
@@ -1,153 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.common.util.concurrent;
-
-import org.apache.lucene.store.AlreadyClosedException;
-import org.elasticsearch.test.ESTestCase;
-import org.hamcrest.Matchers;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.concurrent.CopyOnWriteArrayList;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.atomic.AtomicBoolean;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.is;
-
-/**
- */
-public class RefCountedTest extends ESTestCase {
-
-    @Test
-    public void testRefCount() throws IOException {
-        MyRefCounted counted = new MyRefCounted();
-
-        int incs = randomIntBetween(1, 100);
-        for (int i = 0; i < incs; i++) {
-            if (randomBoolean()) {
-                counted.incRef();
-            } else {
-                assertTrue(counted.tryIncRef());
-            }
-            counted.ensureOpen();
-        }
-
-        for (int i = 0; i < incs; i++) {
-            counted.decRef();
-            counted.ensureOpen();
-        }
-
-        counted.incRef();
-        counted.decRef();
-        for (int i = 0; i < incs; i++) {
-            if (randomBoolean()) {
-                counted.incRef();
-            } else {
-                assertTrue(counted.tryIncRef());
-            }
-            counted.ensureOpen();
-        }
-
-        for (int i = 0; i < incs; i++) {
-            counted.decRef();
-            counted.ensureOpen();
-        }
-
-        counted.decRef();
-        assertFalse(counted.tryIncRef());
-        try {
-            counted.incRef();
-            fail(" expected exception");
-        } catch (AlreadyClosedException ex) {
-            assertThat(ex.getMessage(), equalTo("test is already closed can't increment refCount current count [0]"));
-        }
-
-        try {
-            counted.ensureOpen();
-            fail(" expected exception");
-        } catch (AlreadyClosedException ex) {
-            assertThat(ex.getMessage(), equalTo("closed"));
-        }
-    }
-
-    @Test
-    public void testMultiThreaded() throws InterruptedException {
-        final MyRefCounted counted = new MyRefCounted();
-        Thread[] threads = new Thread[randomIntBetween(2, 5)];
-        final CountDownLatch latch = new CountDownLatch(1);
-        final CopyOnWriteArrayList<Throwable> exceptions = new CopyOnWriteArrayList<>();
-        for (int i = 0; i < threads.length; i++) {
-            threads[i] = new Thread() {
-                @Override
-                public void run() {
-                    try {
-                        latch.await();
-                        for (int j = 0; j < 10000; j++) {
-                            counted.incRef();
-                            try {
-                                counted.ensureOpen();
-                            } finally {
-                                counted.decRef();
-                            }
-                        }
-                    } catch (Throwable e) {
-                        exceptions.add(e);
-                    }
-                }
-            };
-            threads[i].start();
-        }
-        latch.countDown();
-        for (int i = 0; i < threads.length; i++) {
-            threads[i].join();
-        }
-        counted.decRef();
-        try {
-            counted.ensureOpen();
-            fail("expected to be closed");
-        } catch (AlreadyClosedException ex) {
-            assertThat(ex.getMessage(), equalTo("closed"));
-        }
-        assertThat(counted.refCount(), is(0));
-        assertThat(exceptions, Matchers.emptyIterable());
-
-    }
-
-    private final class MyRefCounted extends AbstractRefCounted {
-
-        private final AtomicBoolean closed = new AtomicBoolean(false);
-
-        public MyRefCounted() {
-            super("test");
-        }
-
-        @Override
-        protected void closeInternal() {
-            this.closed.set(true);
-        }
-
-        public void ensureOpen() {
-            if (closed.get()) {
-                assert this.refCount() == 0;
-                throw new AlreadyClosedException("closed");
-            }
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/common/util/concurrent/RefCountedTests.java b/core/src/test/java/org/elasticsearch/common/util/concurrent/RefCountedTests.java
new file mode 100644
index 0000000..9b01b78
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/util/concurrent/RefCountedTests.java
@@ -0,0 +1,153 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.common.util.concurrent;
+
+import org.apache.lucene.store.AlreadyClosedException;
+import org.elasticsearch.test.ESTestCase;
+import org.hamcrest.Matchers;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.concurrent.CopyOnWriteArrayList;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.is;
+
+/**
+ */
+public class RefCountedTests extends ESTestCase {
+
+    @Test
+    public void testRefCount() throws IOException {
+        MyRefCounted counted = new MyRefCounted();
+
+        int incs = randomIntBetween(1, 100);
+        for (int i = 0; i < incs; i++) {
+            if (randomBoolean()) {
+                counted.incRef();
+            } else {
+                assertTrue(counted.tryIncRef());
+            }
+            counted.ensureOpen();
+        }
+
+        for (int i = 0; i < incs; i++) {
+            counted.decRef();
+            counted.ensureOpen();
+        }
+
+        counted.incRef();
+        counted.decRef();
+        for (int i = 0; i < incs; i++) {
+            if (randomBoolean()) {
+                counted.incRef();
+            } else {
+                assertTrue(counted.tryIncRef());
+            }
+            counted.ensureOpen();
+        }
+
+        for (int i = 0; i < incs; i++) {
+            counted.decRef();
+            counted.ensureOpen();
+        }
+
+        counted.decRef();
+        assertFalse(counted.tryIncRef());
+        try {
+            counted.incRef();
+            fail(" expected exception");
+        } catch (AlreadyClosedException ex) {
+            assertThat(ex.getMessage(), equalTo("test is already closed can't increment refCount current count [0]"));
+        }
+
+        try {
+            counted.ensureOpen();
+            fail(" expected exception");
+        } catch (AlreadyClosedException ex) {
+            assertThat(ex.getMessage(), equalTo("closed"));
+        }
+    }
+
+    @Test
+    public void testMultiThreaded() throws InterruptedException {
+        final MyRefCounted counted = new MyRefCounted();
+        Thread[] threads = new Thread[randomIntBetween(2, 5)];
+        final CountDownLatch latch = new CountDownLatch(1);
+        final CopyOnWriteArrayList<Throwable> exceptions = new CopyOnWriteArrayList<>();
+        for (int i = 0; i < threads.length; i++) {
+            threads[i] = new Thread() {
+                @Override
+                public void run() {
+                    try {
+                        latch.await();
+                        for (int j = 0; j < 10000; j++) {
+                            counted.incRef();
+                            try {
+                                counted.ensureOpen();
+                            } finally {
+                                counted.decRef();
+                            }
+                        }
+                    } catch (Throwable e) {
+                        exceptions.add(e);
+                    }
+                }
+            };
+            threads[i].start();
+        }
+        latch.countDown();
+        for (int i = 0; i < threads.length; i++) {
+            threads[i].join();
+        }
+        counted.decRef();
+        try {
+            counted.ensureOpen();
+            fail("expected to be closed");
+        } catch (AlreadyClosedException ex) {
+            assertThat(ex.getMessage(), equalTo("closed"));
+        }
+        assertThat(counted.refCount(), is(0));
+        assertThat(exceptions, Matchers.emptyIterable());
+
+    }
+
+    private final class MyRefCounted extends AbstractRefCounted {
+
+        private final AtomicBoolean closed = new AtomicBoolean(false);
+
+        public MyRefCounted() {
+            super("test");
+        }
+
+        @Override
+        protected void closeInternal() {
+            this.closed.set(true);
+        }
+
+        public void ensureOpen() {
+            if (closed.get()) {
+                assert this.refCount() == 0;
+                throw new AlreadyClosedException("closed");
+            }
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/AbstractFilteringJsonGeneratorTestCase.java b/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/AbstractFilteringJsonGeneratorTestCase.java
new file mode 100644
index 0000000..9669b09
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/AbstractFilteringJsonGeneratorTestCase.java
@@ -0,0 +1,524 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.xcontent.support.filtering;
+
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.xcontent.*;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.io.ByteArrayInputStream;
+import java.io.IOException;
+
+import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.nullValue;
+
+public abstract class AbstractFilteringJsonGeneratorTestCase extends ESTestCase {
+
+    protected abstract XContentType getXContentType();
+
+    protected abstract void assertXContentBuilder(XContentBuilder expected, XContentBuilder builder);
+
+    protected void assertString(XContentBuilder expected, XContentBuilder builder) {
+        assertNotNull(builder);
+        assertNotNull(expected);
+
+        // Verify that the result is equal to the expected string
+        assertThat(builder.bytes().toUtf8(), is(expected.bytes().toUtf8()));
+    }
+
+    protected void assertBinary(XContentBuilder expected, XContentBuilder builder) {
+        assertNotNull(builder);
+        assertNotNull(expected);
+
+        try {
+            XContent xContent = XContentFactory.xContent(builder.contentType());
+            XContentParser jsonParser = xContent.createParser(expected.bytes());
+            XContentParser testParser = xContent.createParser(builder.bytes());
+
+            while (true) {
+                XContentParser.Token token1 = jsonParser.nextToken();
+                XContentParser.Token token2 = testParser.nextToken();
+                if (token1 == null) {
+                    assertThat(token2, nullValue());
+                    return;
+                }
+                assertThat(token1, equalTo(token2));
+                switch (token1) {
+                    case FIELD_NAME:
+                        assertThat(jsonParser.currentName(), equalTo(testParser.currentName()));
+                        break;
+                    case VALUE_STRING:
+                        assertThat(jsonParser.text(), equalTo(testParser.text()));
+                        break;
+                    case VALUE_NUMBER:
+                        assertThat(jsonParser.numberType(), equalTo(testParser.numberType()));
+                        assertThat(jsonParser.numberValue(), equalTo(testParser.numberValue()));
+                        break;
+                }
+            }
+        } catch (Exception e) {
+            fail("Fail to verify the result of the XContentBuilder: " + e.getMessage());
+        }
+    }
+
+    private XContentBuilder newXContentBuilder(String... filters) throws IOException {
+        return XContentBuilder.builder(getXContentType().xContent(), filters);
+    }
+
+    /**
+     * Build a sample using a given XContentBuilder
+     */
+    private XContentBuilder sample(XContentBuilder builder) throws IOException {
+        assertNotNull(builder);
+        builder.startObject()
+                .field("title", "My awesome book")
+                .field("pages", 456)
+                .field("price", 27.99)
+                .field("timestamp", 1428582942867L)
+                .nullField("default")
+                .startArray("tags")
+                    .value("elasticsearch")
+                    .value("java")
+                .endArray()
+                .startArray("authors")
+                    .startObject()
+                        .field("name", "John Doe")
+                        .field("lastname", "John")
+                        .field("firstname", "Doe")
+                    .endObject()
+                    .startObject()
+                        .field("name", "William Smith")
+                        .field("lastname", "William")
+                        .field("firstname", "Smith")
+                    .endObject()
+                .endArray()
+                .startObject("properties")
+                    .field("weight", 0.8d)
+                    .startObject("language")
+                        .startObject("en")
+                            .field("lang", "English")
+                            .field("available", true)
+                            .startArray("distributors")
+                                .startObject()
+                                    .field("name", "The Book Shop")
+                                    .startArray("addresses")
+                                        .startObject()
+                                            .field("name", "address #1")
+                                            .field("street", "Hampton St")
+                                            .field("city", "London")
+                                        .endObject()
+                                        .startObject()
+                                            .field("name", "address #2")
+                                            .field("street", "Queen St")
+                                            .field("city", "Stornoway")
+                                        .endObject()
+                                    .endArray()
+                                .endObject()
+                                .startObject()
+                                    .field("name", "Sussex Books House")
+                                .endObject()
+                            .endArray()
+                        .endObject()
+                        .startObject("fr")
+                            .field("lang", "French")
+                            .field("available", false)
+                            .startArray("distributors")
+                                .startObject()
+                                    .field("name", "La Maison du Livre")
+                                    .startArray("addresses")
+                                        .startObject()
+                                            .field("name", "address #1")
+                                            .field("street", "Rue Mouffetard")
+                                            .field("city", "Paris")
+                                        .endObject()
+                                    .endArray()
+                                .endObject()
+                                .startObject()
+                                    .field("name", "Thetra")
+                                .endObject()
+                            .endArray()
+                        .endObject()
+                    .endObject()
+                .endObject()
+            .endObject();
+        return builder;
+    }
+
+    /**
+     * Instanciates a new XContentBuilder with the given filters and builds a sample with it.
+     */
+    private XContentBuilder sample(String... filters) throws IOException {
+        return sample(newXContentBuilder(filters));
+    }
+
+    @Test
+    public void testNoFiltering() throws Exception {
+        XContentBuilder expected = sample();
+
+        assertXContentBuilder(expected, sample());
+        assertXContentBuilder(expected, sample("*"));
+        assertXContentBuilder(expected, sample("**"));
+    }
+
+    @Test
+    public void testNoMatch() throws Exception {
+        XContentBuilder expected = newXContentBuilder().startObject().endObject();
+
+        assertXContentBuilder(expected, sample("xyz"));
+    }
+
+    @Test
+    public void testSimpleField() throws Exception {
+        XContentBuilder expected = newXContentBuilder().startObject()
+                                                            .field("title", "My awesome book")
+                                                        .endObject();
+
+        assertXContentBuilder(expected, sample("title"));
+    }
+
+    @Test
+    public void testSimpleFieldWithWildcard() throws Exception {
+        XContentBuilder expected = newXContentBuilder().startObject()
+                                                            .field("price", 27.99)
+                                                            .startObject("properties")
+                                                                .field("weight", 0.8d)
+                                                                .startObject("language")
+                                                                    .startObject("en")
+                                                                        .field("lang", "English")
+                                                                        .field("available", true)
+                                                                        .startArray("distributors")
+                                                                            .startObject()
+                                                                                .field("name", "The Book Shop")
+                                                                                .startArray("addresses")
+                                                                                    .startObject()
+                                                                                        .field("name", "address #1")
+                                                                                        .field("street", "Hampton St")
+                                                                                        .field("city", "London")
+                                                                                    .endObject()
+                                                                                    .startObject()
+                                                                                        .field("name", "address #2")
+                                                                                        .field("street", "Queen St")
+                                                                                        .field("city", "Stornoway")
+                                                                                    .endObject()
+                                                                                .endArray()
+                                                                            .endObject()
+                                                                            .startObject()
+                                                                                .field("name", "Sussex Books House")
+                                                                            .endObject()
+                                                                        .endArray()
+                                                                    .endObject()
+                                                                    .startObject("fr")
+                                                                        .field("lang", "French")
+                                                                        .field("available", false)
+                                                                        .startArray("distributors")
+                                                                            .startObject()
+                                                                                .field("name", "La Maison du Livre")
+                                                                                .startArray("addresses")
+                                                                                    .startObject()
+                                                                                        .field("name", "address #1")
+                                                                                        .field("street", "Rue Mouffetard")
+                                                                                        .field("city", "Paris")
+                                                                                    .endObject()
+                                                                                .endArray()
+                                                                            .endObject()
+                                                                            .startObject()
+                                                                                .field("name", "Thetra")
+                                                                            .endObject()
+                                                                        .endArray()
+                                                                    .endObject()
+                                                                .endObject()
+                                                            .endObject()
+                                                        .endObject();
+
+        assertXContentBuilder(expected, sample("pr*"));
+    }
+
+    @Test
+    public void testMultipleFields() throws Exception {
+        XContentBuilder expected = newXContentBuilder().startObject()
+                                                            .field("title", "My awesome book")
+                                                            .field("pages", 456)
+                                                        .endObject();
+
+        assertXContentBuilder(expected, sample("title", "pages"));
+    }
+
+    @Test
+    public void testSimpleArray() throws Exception {
+        XContentBuilder expected = newXContentBuilder().startObject()
+                                                        .startArray("tags")
+                                                            .value("elasticsearch")
+                                                            .value("java")
+                                                        .endArray()
+                                                    .endObject();
+
+        assertXContentBuilder(expected, sample("tags"));
+    }
+
+    @Test
+    public void testSimpleArrayOfObjects() throws Exception {
+        XContentBuilder expected = newXContentBuilder().startObject()
+                                                        .startArray("authors")
+                                                            .startObject()
+                                                                .field("name", "John Doe")
+                                                                .field("lastname", "John")
+                                                                .field("firstname", "Doe")
+                                                            .endObject()
+                                                            .startObject()
+                                                                .field("name", "William Smith")
+                                                                .field("lastname", "William")
+                                                                .field("firstname", "Smith")
+                                                            .endObject()
+                                                        .endArray()
+                                                    .endObject();
+
+        assertXContentBuilder(expected, sample("authors"));
+        assertXContentBuilder(expected, sample("authors.*"));
+        assertXContentBuilder(expected, sample("authors.*name"));
+    }
+
+    @Test
+    public void testSimpleArrayOfObjectsProperty() throws Exception {
+        XContentBuilder expected = newXContentBuilder().startObject()
+                                                            .startArray("authors")
+                                                                .startObject()
+                                                                    .field("lastname", "John")
+                                                                .endObject()
+                                                                .startObject()
+                                                                    .field("lastname", "William")
+                                                                .endObject()
+                                                            .endArray()
+                                                        .endObject();
+
+        assertXContentBuilder(expected, sample("authors.lastname"));
+        assertXContentBuilder(expected, sample("authors.l*"));
+    }
+
+    @Test
+    public void testRecurseField1() throws Exception {
+        XContentBuilder expected = newXContentBuilder().startObject()
+                                                            .startArray("authors")
+                                                                .startObject()
+                                                                    .field("name", "John Doe")
+                                                                .endObject()
+                                                                .startObject()
+                                                                    .field("name", "William Smith")
+                                                            .   endObject()
+                                                            .endArray()
+                                                            .startObject("properties")
+                                                                .startObject("language")
+                                                                    .startObject("en")
+                                                                        .startArray("distributors")
+                                                                            .startObject()
+                                                                                .field("name", "The Book Shop")
+                                                                                .startArray("addresses")
+                                                                                    .startObject()
+                                                                                        .field("name", "address #1")
+                                                                                    .endObject()
+                                                                                    .startObject()
+                                                                                        .field("name", "address #2")
+                                                                                    .endObject()
+                                                                                .endArray()
+                                                                            .endObject()
+                                                                            .startObject()
+                                                                                .field("name", "Sussex Books House")
+                                                                            .endObject()
+                                                                        .endArray()
+                                                                    .endObject()
+                                                                    .startObject("fr")
+                                                                        .startArray("distributors")
+                                                                            .startObject()
+                                                                                .field("name", "La Maison du Livre")
+                                                                                .startArray("addresses")
+                                                                                    .startObject()
+                                                                                        .field("name", "address #1")
+                                                                                    .endObject()
+                                                                                .endArray()
+                                                                            .endObject()
+                                                                            .startObject()
+                                                                                .field("name", "Thetra")
+                                                                            .endObject()
+                                                                        .endArray()
+                                                                    .endObject()
+                                                                .endObject()
+                                                            .endObject()
+                                                        .endObject();
+
+        assertXContentBuilder(expected, sample("**.name"));
+    }
+
+    @Test
+    public void testRecurseField2() throws Exception {
+        XContentBuilder expected = newXContentBuilder().startObject()
+                                                            .startObject("properties")
+                                                                .startObject("language")
+                                                                    .startObject("en")
+                                                                        .startArray("distributors")
+                                                                            .startObject()
+                                                                                .field("name", "The Book Shop")
+                                                                                .startArray("addresses")
+                                                                                    .startObject()
+                                                                                        .field("name", "address #1")
+                                                                                    .endObject()
+                                                                                    .startObject()
+                                                                                        .field("name", "address #2")
+                                                                                    .endObject()
+                                                                                .endArray()
+                                                                            .endObject()
+                                                                            .startObject()
+                                                                                .field("name", "Sussex Books House")
+                                                                            .endObject()
+                                                                        .endArray()
+                                                                    .endObject()
+                                                                    .startObject("fr")
+                                                                        .startArray("distributors")
+                                                                            .startObject()
+                                                                                .field("name", "La Maison du Livre")
+                                                                                .startArray("addresses")
+                                                                                    .startObject()
+                                                                                        .field("name", "address #1")
+                                                                                    .endObject()
+                                                                                .endArray()
+                                                                            .endObject()
+                                                                            .startObject()
+                                                                                .field("name", "Thetra")
+                                                                            .endObject()
+                                                                        .endArray()
+                                                                    .endObject()
+                                                                .endObject()
+                                                            .endObject()
+                                                        .endObject();
+
+        assertXContentBuilder(expected, sample("properties.**.name"));
+    }
+
+    @Test
+    public void testRecurseField3() throws Exception {
+        XContentBuilder expected = newXContentBuilder().startObject()
+                                                        .startObject("properties")
+                                                            .startObject("language")
+                                                                .startObject("en")
+                                                                    .startArray("distributors")
+                                                                        .startObject()
+                                                                            .field("name", "The Book Shop")
+                                                                            .startArray("addresses")
+                                                                                .startObject()
+                                                                                    .field("name", "address #1")
+                                                                                .endObject()
+                                                                                .startObject()
+                                                                                    .field("name", "address #2")
+                                                                                .endObject()
+                                                                            .endArray()
+                                                                        .endObject()
+                                                                        .startObject()
+                                                                            .field("name", "Sussex Books House")
+                                                                        .endObject()
+                                                                    .endArray()
+                                                                .endObject()
+                                                            .endObject()
+                                                        .endObject()
+                                                    .endObject();
+
+        assertXContentBuilder(expected, sample("properties.*.en.**.name"));
+    }
+
+    @Test
+    public void testRecurseField4() throws Exception {
+        XContentBuilder expected = newXContentBuilder().startObject()
+                                                            .startObject("properties")
+                                                                .startObject("language")
+                                                                    .startObject("en")
+                                                                        .startArray("distributors")
+                                                                            .startObject()
+                                                                                .field("name", "The Book Shop")
+                                                                            .endObject()
+                                                                            .startObject()
+                                                                                .field("name", "Sussex Books House")
+                                                                            .endObject()
+                                                                        .endArray()
+                                                                    .endObject()
+                                                                    .startObject("fr")
+                                                                        .startArray("distributors")
+                                                                            .startObject()
+                                                                                .field("name", "La Maison du Livre")
+                                                                            .endObject()
+                                                                            .startObject()
+                                                                                .field("name", "Thetra")
+                                                                            .endObject()
+                                                                        .endArray()
+                                                                    .endObject()
+                                                                .endObject()
+                                                            .endObject()
+                                                        .endObject();
+
+        assertXContentBuilder(expected, sample("properties.**.distributors.name"));
+    }
+
+    @Test
+    public void testRawField() throws Exception {
+
+        XContentBuilder expectedRawField = newXContentBuilder().startObject().field("foo", 0).startObject("raw").field("content", "hello world!").endObject().endObject();
+        XContentBuilder expectedRawFieldFiltered = newXContentBuilder().startObject().field("foo", 0).endObject();
+        XContentBuilder expectedRawFieldNotFiltered =newXContentBuilder().startObject().startObject("raw").field("content", "hello world!").endObject().endObject();
+
+        BytesReference raw = newXContentBuilder().startObject().field("content", "hello world!").endObject().bytes();
+
+        // Test method: rawField(String fieldName, BytesReference content)
+        assertXContentBuilder(expectedRawField, newXContentBuilder().startObject().field("foo", 0).rawField("raw", raw).endObject());
+        assertXContentBuilder(expectedRawFieldFiltered, newXContentBuilder("f*").startObject().field("foo", 0).rawField("raw", raw).endObject());
+        assertXContentBuilder(expectedRawFieldNotFiltered, newXContentBuilder("r*").startObject().field("foo", 0).rawField("raw", raw).endObject());
+
+        // Test method: rawField(String fieldName, byte[] content)
+        assertXContentBuilder(expectedRawField, newXContentBuilder().startObject().field("foo", 0).rawField("raw", raw.toBytes()).endObject());
+        assertXContentBuilder(expectedRawFieldFiltered, newXContentBuilder("f*").startObject().field("foo", 0).rawField("raw", raw.toBytes()).endObject());
+        assertXContentBuilder(expectedRawFieldNotFiltered, newXContentBuilder("r*").startObject().field("foo", 0).rawField("raw", raw.toBytes()).endObject());
+
+        // Test method: rawField(String fieldName, InputStream content)
+        assertXContentBuilder(expectedRawField, newXContentBuilder().startObject().field("foo", 0).rawField("raw", new ByteArrayInputStream(raw.toBytes())).endObject());
+        assertXContentBuilder(expectedRawFieldFiltered, newXContentBuilder("f*").startObject().field("foo", 0).rawField("raw", new ByteArrayInputStream(raw.toBytes())).endObject());
+        assertXContentBuilder(expectedRawFieldNotFiltered, newXContentBuilder("r*").startObject().field("foo", 0).rawField("raw", new ByteArrayInputStream(raw.toBytes())).endObject());
+    }
+
+    @Test
+    public void testArrays() throws Exception {
+        // Test: Array of values (no filtering)
+        XContentBuilder expected = newXContentBuilder().startObject().startArray("tags").value("lorem").value("ipsum").value("dolor").endArray().endObject();
+        assertXContentBuilder(expected, newXContentBuilder("t*").startObject().startArray("tags").value("lorem").value("ipsum").value("dolor").endArray().endObject());
+        assertXContentBuilder(expected, newXContentBuilder("tags").startObject().startArray("tags").value("lorem").value("ipsum").value("dolor").endArray().endObject());
+
+        // Test: Array of values (with filtering)
+        assertXContentBuilder(newXContentBuilder().startObject().endObject(), newXContentBuilder("foo").startObject().startArray("tags").value("lorem").value("ipsum").value("dolor").endArray().endObject());
+
+        // Test: Array of objects (no filtering)
+        expected = newXContentBuilder().startObject().startArray("tags").startObject().field("lastname", "lorem").endObject().startObject().field("firstname", "ipsum").endObject().endArray().endObject();
+        assertXContentBuilder(expected, newXContentBuilder("t*").startObject().startArray("tags").startObject().field("lastname", "lorem").endObject().startObject().field("firstname", "ipsum").endObject().endArray().endObject());
+        assertXContentBuilder(expected, newXContentBuilder("tags").startObject().startArray("tags").startObject().field("lastname", "lorem").endObject().startObject().field("firstname", "ipsum").endObject().endArray().endObject());
+
+        // Test: Array of objects (with filtering)
+        assertXContentBuilder(newXContentBuilder().startObject().endObject(), newXContentBuilder("foo").startObject().startArray("tags").startObject().field("lastname", "lorem").endObject().startObject().field("firstname", "ipsum").endObject().endArray().endObject());
+
+        // Test: Array of objects (with partial filtering)
+        expected = newXContentBuilder().startObject().startArray("tags").startObject().field("firstname", "ipsum").endObject().endArray().endObject();
+        assertXContentBuilder(expected, newXContentBuilder("t*.firstname").startObject().startArray("tags").startObject().field("lastname", "lorem").endObject().startObject().field("firstname", "ipsum").endObject().endArray().endObject());
+
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/AbstractFilteringJsonGeneratorTests.java b/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/AbstractFilteringJsonGeneratorTests.java
deleted file mode 100644
index 5ef19bd..0000000
--- a/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/AbstractFilteringJsonGeneratorTests.java
+++ /dev/null
@@ -1,524 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common.xcontent.support.filtering;
-
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.xcontent.*;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.io.ByteArrayInputStream;
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.is;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.nullValue;
-
-public abstract class AbstractFilteringJsonGeneratorTests extends ESTestCase {
-
-    protected abstract XContentType getXContentType();
-
-    protected abstract void assertXContentBuilder(XContentBuilder expected, XContentBuilder builder);
-
-    protected void assertString(XContentBuilder expected, XContentBuilder builder) {
-        assertNotNull(builder);
-        assertNotNull(expected);
-
-        // Verify that the result is equal to the expected string
-        assertThat(builder.bytes().toUtf8(), is(expected.bytes().toUtf8()));
-    }
-
-    protected void assertBinary(XContentBuilder expected, XContentBuilder builder) {
-        assertNotNull(builder);
-        assertNotNull(expected);
-
-        try {
-            XContent xContent = XContentFactory.xContent(builder.contentType());
-            XContentParser jsonParser = xContent.createParser(expected.bytes());
-            XContentParser testParser = xContent.createParser(builder.bytes());
-
-            while (true) {
-                XContentParser.Token token1 = jsonParser.nextToken();
-                XContentParser.Token token2 = testParser.nextToken();
-                if (token1 == null) {
-                    assertThat(token2, nullValue());
-                    return;
-                }
-                assertThat(token1, equalTo(token2));
-                switch (token1) {
-                    case FIELD_NAME:
-                        assertThat(jsonParser.currentName(), equalTo(testParser.currentName()));
-                        break;
-                    case VALUE_STRING:
-                        assertThat(jsonParser.text(), equalTo(testParser.text()));
-                        break;
-                    case VALUE_NUMBER:
-                        assertThat(jsonParser.numberType(), equalTo(testParser.numberType()));
-                        assertThat(jsonParser.numberValue(), equalTo(testParser.numberValue()));
-                        break;
-                }
-            }
-        } catch (Exception e) {
-            fail("Fail to verify the result of the XContentBuilder: " + e.getMessage());
-        }
-    }
-
-    private XContentBuilder newXContentBuilder(String... filters) throws IOException {
-        return XContentBuilder.builder(getXContentType().xContent(), filters);
-    }
-
-    /**
-     * Build a sample using a given XContentBuilder
-     */
-    private XContentBuilder sample(XContentBuilder builder) throws IOException {
-        assertNotNull(builder);
-        builder.startObject()
-                .field("title", "My awesome book")
-                .field("pages", 456)
-                .field("price", 27.99)
-                .field("timestamp", 1428582942867L)
-                .nullField("default")
-                .startArray("tags")
-                    .value("elasticsearch")
-                    .value("java")
-                .endArray()
-                .startArray("authors")
-                    .startObject()
-                        .field("name", "John Doe")
-                        .field("lastname", "John")
-                        .field("firstname", "Doe")
-                    .endObject()
-                    .startObject()
-                        .field("name", "William Smith")
-                        .field("lastname", "William")
-                        .field("firstname", "Smith")
-                    .endObject()
-                .endArray()
-                .startObject("properties")
-                    .field("weight", 0.8d)
-                    .startObject("language")
-                        .startObject("en")
-                            .field("lang", "English")
-                            .field("available", true)
-                            .startArray("distributors")
-                                .startObject()
-                                    .field("name", "The Book Shop")
-                                    .startArray("addresses")
-                                        .startObject()
-                                            .field("name", "address #1")
-                                            .field("street", "Hampton St")
-                                            .field("city", "London")
-                                        .endObject()
-                                        .startObject()
-                                            .field("name", "address #2")
-                                            .field("street", "Queen St")
-                                            .field("city", "Stornoway")
-                                        .endObject()
-                                    .endArray()
-                                .endObject()
-                                .startObject()
-                                    .field("name", "Sussex Books House")
-                                .endObject()
-                            .endArray()
-                        .endObject()
-                        .startObject("fr")
-                            .field("lang", "French")
-                            .field("available", false)
-                            .startArray("distributors")
-                                .startObject()
-                                    .field("name", "La Maison du Livre")
-                                    .startArray("addresses")
-                                        .startObject()
-                                            .field("name", "address #1")
-                                            .field("street", "Rue Mouffetard")
-                                            .field("city", "Paris")
-                                        .endObject()
-                                    .endArray()
-                                .endObject()
-                                .startObject()
-                                    .field("name", "Thetra")
-                                .endObject()
-                            .endArray()
-                        .endObject()
-                    .endObject()
-                .endObject()
-            .endObject();
-        return builder;
-    }
-
-    /**
-     * Instanciates a new XContentBuilder with the given filters and builds a sample with it.
-     */
-    private XContentBuilder sample(String... filters) throws IOException {
-        return sample(newXContentBuilder(filters));
-    }
-
-    @Test
-    public void testNoFiltering() throws Exception {
-        XContentBuilder expected = sample();
-
-        assertXContentBuilder(expected, sample());
-        assertXContentBuilder(expected, sample("*"));
-        assertXContentBuilder(expected, sample("**"));
-    }
-
-    @Test
-    public void testNoMatch() throws Exception {
-        XContentBuilder expected = newXContentBuilder().startObject().endObject();
-
-        assertXContentBuilder(expected, sample("xyz"));
-    }
-
-    @Test
-    public void testSimpleField() throws Exception {
-        XContentBuilder expected = newXContentBuilder().startObject()
-                                                            .field("title", "My awesome book")
-                                                        .endObject();
-
-        assertXContentBuilder(expected, sample("title"));
-    }
-
-    @Test
-    public void testSimpleFieldWithWildcard() throws Exception {
-        XContentBuilder expected = newXContentBuilder().startObject()
-                                                            .field("price", 27.99)
-                                                            .startObject("properties")
-                                                                .field("weight", 0.8d)
-                                                                .startObject("language")
-                                                                    .startObject("en")
-                                                                        .field("lang", "English")
-                                                                        .field("available", true)
-                                                                        .startArray("distributors")
-                                                                            .startObject()
-                                                                                .field("name", "The Book Shop")
-                                                                                .startArray("addresses")
-                                                                                    .startObject()
-                                                                                        .field("name", "address #1")
-                                                                                        .field("street", "Hampton St")
-                                                                                        .field("city", "London")
-                                                                                    .endObject()
-                                                                                    .startObject()
-                                                                                        .field("name", "address #2")
-                                                                                        .field("street", "Queen St")
-                                                                                        .field("city", "Stornoway")
-                                                                                    .endObject()
-                                                                                .endArray()
-                                                                            .endObject()
-                                                                            .startObject()
-                                                                                .field("name", "Sussex Books House")
-                                                                            .endObject()
-                                                                        .endArray()
-                                                                    .endObject()
-                                                                    .startObject("fr")
-                                                                        .field("lang", "French")
-                                                                        .field("available", false)
-                                                                        .startArray("distributors")
-                                                                            .startObject()
-                                                                                .field("name", "La Maison du Livre")
-                                                                                .startArray("addresses")
-                                                                                    .startObject()
-                                                                                        .field("name", "address #1")
-                                                                                        .field("street", "Rue Mouffetard")
-                                                                                        .field("city", "Paris")
-                                                                                    .endObject()
-                                                                                .endArray()
-                                                                            .endObject()
-                                                                            .startObject()
-                                                                                .field("name", "Thetra")
-                                                                            .endObject()
-                                                                        .endArray()
-                                                                    .endObject()
-                                                                .endObject()
-                                                            .endObject()
-                                                        .endObject();
-
-        assertXContentBuilder(expected, sample("pr*"));
-    }
-
-    @Test
-    public void testMultipleFields() throws Exception {
-        XContentBuilder expected = newXContentBuilder().startObject()
-                                                            .field("title", "My awesome book")
-                                                            .field("pages", 456)
-                                                        .endObject();
-
-        assertXContentBuilder(expected, sample("title", "pages"));
-    }
-
-    @Test
-    public void testSimpleArray() throws Exception {
-        XContentBuilder expected = newXContentBuilder().startObject()
-                                                        .startArray("tags")
-                                                            .value("elasticsearch")
-                                                            .value("java")
-                                                        .endArray()
-                                                    .endObject();
-
-        assertXContentBuilder(expected, sample("tags"));
-    }
-
-    @Test
-    public void testSimpleArrayOfObjects() throws Exception {
-        XContentBuilder expected = newXContentBuilder().startObject()
-                                                        .startArray("authors")
-                                                            .startObject()
-                                                                .field("name", "John Doe")
-                                                                .field("lastname", "John")
-                                                                .field("firstname", "Doe")
-                                                            .endObject()
-                                                            .startObject()
-                                                                .field("name", "William Smith")
-                                                                .field("lastname", "William")
-                                                                .field("firstname", "Smith")
-                                                            .endObject()
-                                                        .endArray()
-                                                    .endObject();
-
-        assertXContentBuilder(expected, sample("authors"));
-        assertXContentBuilder(expected, sample("authors.*"));
-        assertXContentBuilder(expected, sample("authors.*name"));
-    }
-
-    @Test
-    public void testSimpleArrayOfObjectsProperty() throws Exception {
-        XContentBuilder expected = newXContentBuilder().startObject()
-                                                            .startArray("authors")
-                                                                .startObject()
-                                                                    .field("lastname", "John")
-                                                                .endObject()
-                                                                .startObject()
-                                                                    .field("lastname", "William")
-                                                                .endObject()
-                                                            .endArray()
-                                                        .endObject();
-
-        assertXContentBuilder(expected, sample("authors.lastname"));
-        assertXContentBuilder(expected, sample("authors.l*"));
-    }
-
-    @Test
-    public void testRecurseField1() throws Exception {
-        XContentBuilder expected = newXContentBuilder().startObject()
-                                                            .startArray("authors")
-                                                                .startObject()
-                                                                    .field("name", "John Doe")
-                                                                .endObject()
-                                                                .startObject()
-                                                                    .field("name", "William Smith")
-                                                            .   endObject()
-                                                            .endArray()
-                                                            .startObject("properties")
-                                                                .startObject("language")
-                                                                    .startObject("en")
-                                                                        .startArray("distributors")
-                                                                            .startObject()
-                                                                                .field("name", "The Book Shop")
-                                                                                .startArray("addresses")
-                                                                                    .startObject()
-                                                                                        .field("name", "address #1")
-                                                                                    .endObject()
-                                                                                    .startObject()
-                                                                                        .field("name", "address #2")
-                                                                                    .endObject()
-                                                                                .endArray()
-                                                                            .endObject()
-                                                                            .startObject()
-                                                                                .field("name", "Sussex Books House")
-                                                                            .endObject()
-                                                                        .endArray()
-                                                                    .endObject()
-                                                                    .startObject("fr")
-                                                                        .startArray("distributors")
-                                                                            .startObject()
-                                                                                .field("name", "La Maison du Livre")
-                                                                                .startArray("addresses")
-                                                                                    .startObject()
-                                                                                        .field("name", "address #1")
-                                                                                    .endObject()
-                                                                                .endArray()
-                                                                            .endObject()
-                                                                            .startObject()
-                                                                                .field("name", "Thetra")
-                                                                            .endObject()
-                                                                        .endArray()
-                                                                    .endObject()
-                                                                .endObject()
-                                                            .endObject()
-                                                        .endObject();
-
-        assertXContentBuilder(expected, sample("**.name"));
-    }
-
-    @Test
-    public void testRecurseField2() throws Exception {
-        XContentBuilder expected = newXContentBuilder().startObject()
-                                                            .startObject("properties")
-                                                                .startObject("language")
-                                                                    .startObject("en")
-                                                                        .startArray("distributors")
-                                                                            .startObject()
-                                                                                .field("name", "The Book Shop")
-                                                                                .startArray("addresses")
-                                                                                    .startObject()
-                                                                                        .field("name", "address #1")
-                                                                                    .endObject()
-                                                                                    .startObject()
-                                                                                        .field("name", "address #2")
-                                                                                    .endObject()
-                                                                                .endArray()
-                                                                            .endObject()
-                                                                            .startObject()
-                                                                                .field("name", "Sussex Books House")
-                                                                            .endObject()
-                                                                        .endArray()
-                                                                    .endObject()
-                                                                    .startObject("fr")
-                                                                        .startArray("distributors")
-                                                                            .startObject()
-                                                                                .field("name", "La Maison du Livre")
-                                                                                .startArray("addresses")
-                                                                                    .startObject()
-                                                                                        .field("name", "address #1")
-                                                                                    .endObject()
-                                                                                .endArray()
-                                                                            .endObject()
-                                                                            .startObject()
-                                                                                .field("name", "Thetra")
-                                                                            .endObject()
-                                                                        .endArray()
-                                                                    .endObject()
-                                                                .endObject()
-                                                            .endObject()
-                                                        .endObject();
-
-        assertXContentBuilder(expected, sample("properties.**.name"));
-    }
-
-    @Test
-    public void testRecurseField3() throws Exception {
-        XContentBuilder expected = newXContentBuilder().startObject()
-                                                        .startObject("properties")
-                                                            .startObject("language")
-                                                                .startObject("en")
-                                                                    .startArray("distributors")
-                                                                        .startObject()
-                                                                            .field("name", "The Book Shop")
-                                                                            .startArray("addresses")
-                                                                                .startObject()
-                                                                                    .field("name", "address #1")
-                                                                                .endObject()
-                                                                                .startObject()
-                                                                                    .field("name", "address #2")
-                                                                                .endObject()
-                                                                            .endArray()
-                                                                        .endObject()
-                                                                        .startObject()
-                                                                            .field("name", "Sussex Books House")
-                                                                        .endObject()
-                                                                    .endArray()
-                                                                .endObject()
-                                                            .endObject()
-                                                        .endObject()
-                                                    .endObject();
-
-        assertXContentBuilder(expected, sample("properties.*.en.**.name"));
-    }
-
-    @Test
-    public void testRecurseField4() throws Exception {
-        XContentBuilder expected = newXContentBuilder().startObject()
-                                                            .startObject("properties")
-                                                                .startObject("language")
-                                                                    .startObject("en")
-                                                                        .startArray("distributors")
-                                                                            .startObject()
-                                                                                .field("name", "The Book Shop")
-                                                                            .endObject()
-                                                                            .startObject()
-                                                                                .field("name", "Sussex Books House")
-                                                                            .endObject()
-                                                                        .endArray()
-                                                                    .endObject()
-                                                                    .startObject("fr")
-                                                                        .startArray("distributors")
-                                                                            .startObject()
-                                                                                .field("name", "La Maison du Livre")
-                                                                            .endObject()
-                                                                            .startObject()
-                                                                                .field("name", "Thetra")
-                                                                            .endObject()
-                                                                        .endArray()
-                                                                    .endObject()
-                                                                .endObject()
-                                                            .endObject()
-                                                        .endObject();
-
-        assertXContentBuilder(expected, sample("properties.**.distributors.name"));
-    }
-
-    @Test
-    public void testRawField() throws Exception {
-
-        XContentBuilder expectedRawField = newXContentBuilder().startObject().field("foo", 0).startObject("raw").field("content", "hello world!").endObject().endObject();
-        XContentBuilder expectedRawFieldFiltered = newXContentBuilder().startObject().field("foo", 0).endObject();
-        XContentBuilder expectedRawFieldNotFiltered =newXContentBuilder().startObject().startObject("raw").field("content", "hello world!").endObject().endObject();
-
-        BytesReference raw = newXContentBuilder().startObject().field("content", "hello world!").endObject().bytes();
-
-        // Test method: rawField(String fieldName, BytesReference content)
-        assertXContentBuilder(expectedRawField, newXContentBuilder().startObject().field("foo", 0).rawField("raw", raw).endObject());
-        assertXContentBuilder(expectedRawFieldFiltered, newXContentBuilder("f*").startObject().field("foo", 0).rawField("raw", raw).endObject());
-        assertXContentBuilder(expectedRawFieldNotFiltered, newXContentBuilder("r*").startObject().field("foo", 0).rawField("raw", raw).endObject());
-
-        // Test method: rawField(String fieldName, byte[] content)
-        assertXContentBuilder(expectedRawField, newXContentBuilder().startObject().field("foo", 0).rawField("raw", raw.toBytes()).endObject());
-        assertXContentBuilder(expectedRawFieldFiltered, newXContentBuilder("f*").startObject().field("foo", 0).rawField("raw", raw.toBytes()).endObject());
-        assertXContentBuilder(expectedRawFieldNotFiltered, newXContentBuilder("r*").startObject().field("foo", 0).rawField("raw", raw.toBytes()).endObject());
-
-        // Test method: rawField(String fieldName, InputStream content)
-        assertXContentBuilder(expectedRawField, newXContentBuilder().startObject().field("foo", 0).rawField("raw", new ByteArrayInputStream(raw.toBytes())).endObject());
-        assertXContentBuilder(expectedRawFieldFiltered, newXContentBuilder("f*").startObject().field("foo", 0).rawField("raw", new ByteArrayInputStream(raw.toBytes())).endObject());
-        assertXContentBuilder(expectedRawFieldNotFiltered, newXContentBuilder("r*").startObject().field("foo", 0).rawField("raw", new ByteArrayInputStream(raw.toBytes())).endObject());
-    }
-
-    @Test
-    public void testArrays() throws Exception {
-        // Test: Array of values (no filtering)
-        XContentBuilder expected = newXContentBuilder().startObject().startArray("tags").value("lorem").value("ipsum").value("dolor").endArray().endObject();
-        assertXContentBuilder(expected, newXContentBuilder("t*").startObject().startArray("tags").value("lorem").value("ipsum").value("dolor").endArray().endObject());
-        assertXContentBuilder(expected, newXContentBuilder("tags").startObject().startArray("tags").value("lorem").value("ipsum").value("dolor").endArray().endObject());
-
-        // Test: Array of values (with filtering)
-        assertXContentBuilder(newXContentBuilder().startObject().endObject(), newXContentBuilder("foo").startObject().startArray("tags").value("lorem").value("ipsum").value("dolor").endArray().endObject());
-
-        // Test: Array of objects (no filtering)
-        expected = newXContentBuilder().startObject().startArray("tags").startObject().field("lastname", "lorem").endObject().startObject().field("firstname", "ipsum").endObject().endArray().endObject();
-        assertXContentBuilder(expected, newXContentBuilder("t*").startObject().startArray("tags").startObject().field("lastname", "lorem").endObject().startObject().field("firstname", "ipsum").endObject().endArray().endObject());
-        assertXContentBuilder(expected, newXContentBuilder("tags").startObject().startArray("tags").startObject().field("lastname", "lorem").endObject().startObject().field("firstname", "ipsum").endObject().endArray().endObject());
-
-        // Test: Array of objects (with filtering)
-        assertXContentBuilder(newXContentBuilder().startObject().endObject(), newXContentBuilder("foo").startObject().startArray("tags").startObject().field("lastname", "lorem").endObject().startObject().field("firstname", "ipsum").endObject().endArray().endObject());
-
-        // Test: Array of objects (with partial filtering)
-        expected = newXContentBuilder().startObject().startArray("tags").startObject().field("firstname", "ipsum").endObject().endArray().endObject();
-        assertXContentBuilder(expected, newXContentBuilder("t*.firstname").startObject().startArray("tags").startObject().field("lastname", "lorem").endObject().startObject().field("firstname", "ipsum").endObject().endArray().endObject());
-
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/JsonFilteringGeneratorTests.java b/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/JsonFilteringGeneratorTests.java
index 9468746..a518884 100644
--- a/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/JsonFilteringGeneratorTests.java
+++ b/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/JsonFilteringGeneratorTests.java
@@ -22,7 +22,7 @@ package org.elasticsearch.common.xcontent.support.filtering;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentType;
 
-public class JsonFilteringGeneratorTests extends AbstractFilteringJsonGeneratorTests {
+public class JsonFilteringGeneratorTests extends AbstractFilteringJsonGeneratorTestCase {
 
     @Override
     protected XContentType getXContentType() {
diff --git a/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/YamlFilteringGeneratorTests.java b/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/YamlFilteringGeneratorTests.java
index d7e3a93..c85fbc9 100644
--- a/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/YamlFilteringGeneratorTests.java
+++ b/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/YamlFilteringGeneratorTests.java
@@ -22,7 +22,7 @@ package org.elasticsearch.common.xcontent.support.filtering;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentType;
 
-public class YamlFilteringGeneratorTests extends AbstractFilteringJsonGeneratorTests {
+public class YamlFilteringGeneratorTests extends AbstractFilteringJsonGeneratorTestCase {
 
     @Override
     protected XContentType getXContentType() {
diff --git a/core/src/test/java/org/elasticsearch/discovery/zen/ElectMasterServiceTest.java b/core/src/test/java/org/elasticsearch/discovery/zen/ElectMasterServiceTest.java
deleted file mode 100644
index 884d238..0000000
--- a/core/src/test/java/org/elasticsearch/discovery/zen/ElectMasterServiceTest.java
+++ /dev/null
@@ -1,105 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.zen;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.transport.DummyTransportAddress;
-import org.elasticsearch.discovery.zen.elect.ElectMasterService;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.util.*;
-
-public class ElectMasterServiceTest extends ESTestCase {
-
-    ElectMasterService electMasterService() {
-        return new ElectMasterService(Settings.EMPTY, Version.CURRENT);
-    }
-
-    List<DiscoveryNode> generateRandomNodes() {
-        int count = scaledRandomIntBetween(1, 100);
-        ArrayList<DiscoveryNode> nodes = new ArrayList<>(count);
-
-        Map<String, String> master = new HashMap<>();
-        master.put("master", "true");
-        Map<String, String> nonMaster = new HashMap<>();
-        nonMaster.put("master", "false");
-
-        for (int i = 0; i < count; i++) {
-            Map<String, String> attributes = randomBoolean() ? master : nonMaster;
-            DiscoveryNode node = new DiscoveryNode("n_" + i, "n_" + i, DummyTransportAddress.INSTANCE, attributes, Version.CURRENT);
-            nodes.add(node);
-        }
-
-        Collections.shuffle(nodes, getRandom());
-        return nodes;
-    }
-
-    @Test
-    public void sortByMasterLikelihood() {
-        List<DiscoveryNode> nodes = generateRandomNodes();
-        List<DiscoveryNode> sortedNodes = electMasterService().sortByMasterLikelihood(nodes);
-        assertEquals(nodes.size(), sortedNodes.size());
-        DiscoveryNode prevNode = sortedNodes.get(0);
-        for (int i = 1; i < sortedNodes.size(); i++) {
-            DiscoveryNode node = sortedNodes.get(i);
-            if (!prevNode.masterNode()) {
-                assertFalse(node.masterNode());
-            } else if (node.masterNode()) {
-                assertTrue(prevNode.id().compareTo(node.id()) < 0);
-            }
-            prevNode = node;
-        }
-
-    }
-
-    @Test
-    public void electMaster() {
-        List<DiscoveryNode> nodes = generateRandomNodes();
-        ElectMasterService service = electMasterService();
-        int min_master_nodes = randomIntBetween(0, nodes.size());
-        service.minimumMasterNodes(min_master_nodes);
-
-        int master_nodes = 0;
-        for (DiscoveryNode node : nodes) {
-            if (node.masterNode()) {
-                master_nodes++;
-            }
-        }
-        DiscoveryNode master = null;
-        if (service.hasEnoughMasterNodes(nodes)) {
-            master = service.electMaster(nodes);
-        }
-
-        if (master_nodes == 0) {
-            assertNull(master);
-        } else if (min_master_nodes > 0 && master_nodes < min_master_nodes) {
-            assertNull(master);
-        } else {
-            for (DiscoveryNode node : nodes) {
-                if (node.masterNode()) {
-                    assertTrue(master.id().compareTo(node.id()) <= 0);
-                }
-            }
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/discovery/zen/ElectMasterServiceTests.java b/core/src/test/java/org/elasticsearch/discovery/zen/ElectMasterServiceTests.java
new file mode 100644
index 0000000..eddc4d9
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/discovery/zen/ElectMasterServiceTests.java
@@ -0,0 +1,105 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.zen;
+
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.transport.DummyTransportAddress;
+import org.elasticsearch.discovery.zen.elect.ElectMasterService;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.util.*;
+
+public class ElectMasterServiceTests extends ESTestCase {
+
+    ElectMasterService electMasterService() {
+        return new ElectMasterService(Settings.EMPTY, Version.CURRENT);
+    }
+
+    List<DiscoveryNode> generateRandomNodes() {
+        int count = scaledRandomIntBetween(1, 100);
+        ArrayList<DiscoveryNode> nodes = new ArrayList<>(count);
+
+        Map<String, String> master = new HashMap<>();
+        master.put("master", "true");
+        Map<String, String> nonMaster = new HashMap<>();
+        nonMaster.put("master", "false");
+
+        for (int i = 0; i < count; i++) {
+            Map<String, String> attributes = randomBoolean() ? master : nonMaster;
+            DiscoveryNode node = new DiscoveryNode("n_" + i, "n_" + i, DummyTransportAddress.INSTANCE, attributes, Version.CURRENT);
+            nodes.add(node);
+        }
+
+        Collections.shuffle(nodes, getRandom());
+        return nodes;
+    }
+
+    @Test
+    public void sortByMasterLikelihood() {
+        List<DiscoveryNode> nodes = generateRandomNodes();
+        List<DiscoveryNode> sortedNodes = electMasterService().sortByMasterLikelihood(nodes);
+        assertEquals(nodes.size(), sortedNodes.size());
+        DiscoveryNode prevNode = sortedNodes.get(0);
+        for (int i = 1; i < sortedNodes.size(); i++) {
+            DiscoveryNode node = sortedNodes.get(i);
+            if (!prevNode.masterNode()) {
+                assertFalse(node.masterNode());
+            } else if (node.masterNode()) {
+                assertTrue(prevNode.id().compareTo(node.id()) < 0);
+            }
+            prevNode = node;
+        }
+
+    }
+
+    @Test
+    public void electMaster() {
+        List<DiscoveryNode> nodes = generateRandomNodes();
+        ElectMasterService service = electMasterService();
+        int min_master_nodes = randomIntBetween(0, nodes.size());
+        service.minimumMasterNodes(min_master_nodes);
+
+        int master_nodes = 0;
+        for (DiscoveryNode node : nodes) {
+            if (node.masterNode()) {
+                master_nodes++;
+            }
+        }
+        DiscoveryNode master = null;
+        if (service.hasEnoughMasterNodes(nodes)) {
+            master = service.electMaster(nodes);
+        }
+
+        if (master_nodes == 0) {
+            assertNull(master);
+        } else if (min_master_nodes > 0 && master_nodes < min_master_nodes) {
+            assertNull(master);
+        } else {
+            for (DiscoveryNode node : nodes) {
+                if (node.masterNode()) {
+                    assertTrue(master.id().compareTo(node.id()) <= 0);
+                }
+            }
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryUnitTest.java b/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryUnitTest.java
deleted file mode 100644
index cc6bc2b..0000000
--- a/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryUnitTest.java
+++ /dev/null
@@ -1,143 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.zen;
-
-import org.elasticsearch.cluster.ClusterName;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Collections;
-import java.util.LinkedList;
-import java.util.Queue;
-
-import static org.elasticsearch.discovery.zen.ZenDiscovery.ProcessClusterState;
-import static org.elasticsearch.discovery.zen.ZenDiscovery.shouldIgnoreOrRejectNewClusterState;
-import static org.hamcrest.Matchers.*;
-import static org.hamcrest.core.IsNull.nullValue;
-
-/**
- */
-public class ZenDiscoveryUnitTest extends ESTestCase {
-
-    public void testShouldIgnoreNewClusterState() {
-        ClusterName clusterName = new ClusterName("abc");
-
-        DiscoveryNodes.Builder currentNodes = DiscoveryNodes.builder();
-        currentNodes.masterNodeId("a");
-        DiscoveryNodes.Builder newNodes = DiscoveryNodes.builder();
-        newNodes.masterNodeId("a");
-
-        ClusterState.Builder currentState = ClusterState.builder(clusterName);
-        currentState.nodes(currentNodes);
-        ClusterState.Builder newState = ClusterState.builder(clusterName);
-        newState.nodes(newNodes);
-
-        currentState.version(2);
-        newState.version(1);
-        assertTrue("should ignore, because new state's version is lower to current state's version", shouldIgnoreOrRejectNewClusterState(logger, currentState.build(), newState.build()));
-        currentState.version(1);
-        newState.version(1);
-        assertFalse("should not ignore, because new state's version is equal to current state's version", shouldIgnoreOrRejectNewClusterState(logger, currentState.build(), newState.build()));
-        currentState.version(1);
-        newState.version(2);
-        assertFalse("should not ignore, because new state's version is higher to current state's version", shouldIgnoreOrRejectNewClusterState(logger, currentState.build(), newState.build()));
-
-        currentNodes = DiscoveryNodes.builder();
-        currentNodes.masterNodeId("b");
-        // version isn't taken into account, so randomize it to ensure this.
-        if (randomBoolean()) {
-            currentState.version(2);
-            newState.version(1);
-        } else {
-            currentState.version(1);
-            newState.version(2);
-        }
-        currentState.nodes(currentNodes);
-        try {
-            shouldIgnoreOrRejectNewClusterState(logger, currentState.build(), newState.build());
-            fail("should ignore, because current state's master is not equal to new state's master");
-        } catch (IllegalStateException e) {
-            assertThat(e.getMessage(), containsString("cluster state from a different master than the current one, rejecting"));
-        }
-
-        currentNodes = DiscoveryNodes.builder();
-        currentNodes.masterNodeId(null);
-        currentState.nodes(currentNodes);
-        // version isn't taken into account, so randomize it to ensure this.
-        if (randomBoolean()) {
-            currentState.version(2);
-            newState.version(1);
-        } else {
-            currentState.version(1);
-            newState.version(2);
-        }
-        assertFalse("should not ignore, because current state doesn't have a master", shouldIgnoreOrRejectNewClusterState(logger, currentState.build(), newState.build()));
-    }
-
-    public void testSelectNextStateToProcess_empty() {
-        Queue<ProcessClusterState> queue = new LinkedList<>();
-        assertThat(ZenDiscovery.selectNextStateToProcess(queue), nullValue());
-    }
-
-    public void testSelectNextStateToProcess() {
-        ClusterName clusterName = new ClusterName("abc");
-        DiscoveryNodes nodes = DiscoveryNodes.builder().masterNodeId("a").build();
-
-        int numUpdates = scaledRandomIntBetween(50, 100);
-        LinkedList<ProcessClusterState> queue = new LinkedList<>();
-        for (int i = 0; i < numUpdates; i++) {
-            queue.add(new ProcessClusterState(ClusterState.builder(clusterName).version(i).nodes(nodes).build()));
-        }
-        ProcessClusterState mostRecent = queue.get(numUpdates - 1);
-        Collections.shuffle(queue, getRandom());
-
-        assertThat(ZenDiscovery.selectNextStateToProcess(queue), sameInstance(mostRecent.clusterState));
-        assertThat(mostRecent.processed, is(true));
-        assertThat(queue.size(), equalTo(0));
-    }
-
-    public void testSelectNextStateToProcess_differentMasters() {
-        ClusterName clusterName = new ClusterName("abc");
-        DiscoveryNodes nodes1 = DiscoveryNodes.builder().masterNodeId("a").build();
-        DiscoveryNodes nodes2 = DiscoveryNodes.builder().masterNodeId("b").build();
-
-        LinkedList<ProcessClusterState> queue = new LinkedList<>();
-        ProcessClusterState thirdMostRecent = new ProcessClusterState(ClusterState.builder(clusterName).version(1).nodes(nodes1).build());
-        queue.offer(thirdMostRecent);
-        ProcessClusterState secondMostRecent = new ProcessClusterState(ClusterState.builder(clusterName).version(2).nodes(nodes1).build());
-        queue.offer(secondMostRecent);
-        ProcessClusterState mostRecent = new ProcessClusterState(ClusterState.builder(clusterName).version(3).nodes(nodes1).build());
-        queue.offer(mostRecent);
-        Collections.shuffle(queue, getRandom());
-        queue.offer(new ProcessClusterState(ClusterState.builder(clusterName).version(4).nodes(nodes2).build()));
-        queue.offer(new ProcessClusterState(ClusterState.builder(clusterName).version(5).nodes(nodes1).build()));
-
-
-        assertThat(ZenDiscovery.selectNextStateToProcess(queue), sameInstance(mostRecent.clusterState));
-        assertThat(thirdMostRecent.processed, is(true));
-        assertThat(secondMostRecent.processed, is(true));
-        assertThat(mostRecent.processed, is(true));
-        assertThat(queue.size(), equalTo(2));
-        assertThat(queue.get(0).processed, is(false));
-        assertThat(queue.get(1).processed, is(false));
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryUnitTests.java b/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryUnitTests.java
new file mode 100644
index 0000000..6bd2bd7
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryUnitTests.java
@@ -0,0 +1,143 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.zen;
+
+import org.elasticsearch.cluster.ClusterName;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.node.DiscoveryNodes;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.Collections;
+import java.util.LinkedList;
+import java.util.Queue;
+
+import static org.elasticsearch.discovery.zen.ZenDiscovery.ProcessClusterState;
+import static org.elasticsearch.discovery.zen.ZenDiscovery.shouldIgnoreOrRejectNewClusterState;
+import static org.hamcrest.Matchers.*;
+import static org.hamcrest.core.IsNull.nullValue;
+
+/**
+ */
+public class ZenDiscoveryUnitTests extends ESTestCase {
+
+    public void testShouldIgnoreNewClusterState() {
+        ClusterName clusterName = new ClusterName("abc");
+
+        DiscoveryNodes.Builder currentNodes = DiscoveryNodes.builder();
+        currentNodes.masterNodeId("a");
+        DiscoveryNodes.Builder newNodes = DiscoveryNodes.builder();
+        newNodes.masterNodeId("a");
+
+        ClusterState.Builder currentState = ClusterState.builder(clusterName);
+        currentState.nodes(currentNodes);
+        ClusterState.Builder newState = ClusterState.builder(clusterName);
+        newState.nodes(newNodes);
+
+        currentState.version(2);
+        newState.version(1);
+        assertTrue("should ignore, because new state's version is lower to current state's version", shouldIgnoreOrRejectNewClusterState(logger, currentState.build(), newState.build()));
+        currentState.version(1);
+        newState.version(1);
+        assertFalse("should not ignore, because new state's version is equal to current state's version", shouldIgnoreOrRejectNewClusterState(logger, currentState.build(), newState.build()));
+        currentState.version(1);
+        newState.version(2);
+        assertFalse("should not ignore, because new state's version is higher to current state's version", shouldIgnoreOrRejectNewClusterState(logger, currentState.build(), newState.build()));
+
+        currentNodes = DiscoveryNodes.builder();
+        currentNodes.masterNodeId("b");
+        // version isn't taken into account, so randomize it to ensure this.
+        if (randomBoolean()) {
+            currentState.version(2);
+            newState.version(1);
+        } else {
+            currentState.version(1);
+            newState.version(2);
+        }
+        currentState.nodes(currentNodes);
+        try {
+            shouldIgnoreOrRejectNewClusterState(logger, currentState.build(), newState.build());
+            fail("should ignore, because current state's master is not equal to new state's master");
+        } catch (IllegalStateException e) {
+            assertThat(e.getMessage(), containsString("cluster state from a different master than the current one, rejecting"));
+        }
+
+        currentNodes = DiscoveryNodes.builder();
+        currentNodes.masterNodeId(null);
+        currentState.nodes(currentNodes);
+        // version isn't taken into account, so randomize it to ensure this.
+        if (randomBoolean()) {
+            currentState.version(2);
+            newState.version(1);
+        } else {
+            currentState.version(1);
+            newState.version(2);
+        }
+        assertFalse("should not ignore, because current state doesn't have a master", shouldIgnoreOrRejectNewClusterState(logger, currentState.build(), newState.build()));
+    }
+
+    public void testSelectNextStateToProcess_empty() {
+        Queue<ProcessClusterState> queue = new LinkedList<>();
+        assertThat(ZenDiscovery.selectNextStateToProcess(queue), nullValue());
+    }
+
+    public void testSelectNextStateToProcess() {
+        ClusterName clusterName = new ClusterName("abc");
+        DiscoveryNodes nodes = DiscoveryNodes.builder().masterNodeId("a").build();
+
+        int numUpdates = scaledRandomIntBetween(50, 100);
+        LinkedList<ProcessClusterState> queue = new LinkedList<>();
+        for (int i = 0; i < numUpdates; i++) {
+            queue.add(new ProcessClusterState(ClusterState.builder(clusterName).version(i).nodes(nodes).build()));
+        }
+        ProcessClusterState mostRecent = queue.get(numUpdates - 1);
+        Collections.shuffle(queue, getRandom());
+
+        assertThat(ZenDiscovery.selectNextStateToProcess(queue), sameInstance(mostRecent.clusterState));
+        assertThat(mostRecent.processed, is(true));
+        assertThat(queue.size(), equalTo(0));
+    }
+
+    public void testSelectNextStateToProcess_differentMasters() {
+        ClusterName clusterName = new ClusterName("abc");
+        DiscoveryNodes nodes1 = DiscoveryNodes.builder().masterNodeId("a").build();
+        DiscoveryNodes nodes2 = DiscoveryNodes.builder().masterNodeId("b").build();
+
+        LinkedList<ProcessClusterState> queue = new LinkedList<>();
+        ProcessClusterState thirdMostRecent = new ProcessClusterState(ClusterState.builder(clusterName).version(1).nodes(nodes1).build());
+        queue.offer(thirdMostRecent);
+        ProcessClusterState secondMostRecent = new ProcessClusterState(ClusterState.builder(clusterName).version(2).nodes(nodes1).build());
+        queue.offer(secondMostRecent);
+        ProcessClusterState mostRecent = new ProcessClusterState(ClusterState.builder(clusterName).version(3).nodes(nodes1).build());
+        queue.offer(mostRecent);
+        Collections.shuffle(queue, getRandom());
+        queue.offer(new ProcessClusterState(ClusterState.builder(clusterName).version(4).nodes(nodes2).build()));
+        queue.offer(new ProcessClusterState(ClusterState.builder(clusterName).version(5).nodes(nodes1).build()));
+
+
+        assertThat(ZenDiscovery.selectNextStateToProcess(queue), sameInstance(mostRecent.clusterState));
+        assertThat(thirdMostRecent.processed, is(true));
+        assertThat(secondMostRecent.processed, is(true));
+        assertThat(mostRecent.processed, is(true));
+        assertThat(queue.size(), equalTo(2));
+        assertThat(queue.get(0).processed, is(false));
+        assertThat(queue.get(1).processed, is(false));
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/gateway/MetaDataStateFormatTest.java b/core/src/test/java/org/elasticsearch/gateway/MetaDataStateFormatTest.java
deleted file mode 100644
index 8650fd5..0000000
--- a/core/src/test/java/org/elasticsearch/gateway/MetaDataStateFormatTest.java
+++ /dev/null
@@ -1,562 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.gateway;
-
-import com.google.common.collect.Iterators;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.MockDirectoryWrapper;
-import org.apache.lucene.store.SimpleFSDirectory;
-import org.apache.lucene.util.LuceneTestCase;
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.collect.ImmutableOpenMap;
-import org.elasticsearch.common.io.FileSystemUtils;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentType;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.io.InputStream;
-import java.io.IOException;
-import java.io.OutputStream;
-import java.net.URISyntaxException;
-import java.nio.ByteBuffer;
-import java.nio.channels.FileChannel;
-import java.nio.file.DirectoryStream;
-import java.nio.file.Files;
-import java.nio.file.Path;
-import java.nio.file.StandardOpenOption;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.not;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.startsWith;
-
-@LuceneTestCase.SuppressFileSystems("ExtrasFS") // TODO: fix test to work with ExtrasFS
-public class MetaDataStateFormatTest extends ESTestCase {
-
-
-    /**
-     * Ensure we can read a pre-generated cluster state.
-     */
-    public void testReadClusterState() throws URISyntaxException, IOException {
-        final MetaDataStateFormat<MetaData> format = new MetaDataStateFormat<MetaData>(randomFrom(XContentType.values()), "global-") {
-
-            @Override
-            public void toXContent(XContentBuilder builder, MetaData state) throws IOException {
-                fail("this test doesn't write");
-            }
-
-            @Override
-            public MetaData fromXContent(XContentParser parser) throws IOException {
-                return MetaData.Builder.fromXContent(parser);
-            }
-        };
-        Path tmp = createTempDir();
-        final InputStream resource = this.getClass().getResourceAsStream("global-3.st");
-        assertThat(resource, notNullValue());
-        Path dst = tmp.resolve("global-3.st");
-        Files.copy(resource, dst);
-        MetaData read = format.read(dst);
-        assertThat(read, notNullValue());
-        assertThat(read.clusterUUID(), equalTo("3O1tDF1IRB6fSJ-GrTMUtg"));
-        // indices are empty since they are serialized separately
-    }
-
-    public void testReadWriteState() throws IOException {
-        Path[] dirs = new Path[randomIntBetween(1, 5)];
-        for (int i = 0; i < dirs.length; i++) {
-            dirs[i] = createTempDir();
-        }
-        final long id = addDummyFiles("foo-", dirs);
-        Format format = new Format(randomFrom(XContentType.values()), "foo-");
-        DummyState state = new DummyState(randomRealisticUnicodeOfCodepointLengthBetween(1, 1000), randomInt(), randomLong(), randomDouble(), randomBoolean());
-        int version = between(0, Integer.MAX_VALUE/2);
-        format.write(state, version, dirs);
-        for (Path file : dirs) {
-            Path[] list = content("*", file);
-            assertEquals(list.length, 1);
-            assertThat(list[0].getFileName().toString(), equalTo(MetaDataStateFormat.STATE_DIR_NAME));
-            Path stateDir = list[0];
-            assertThat(Files.isDirectory(stateDir), is(true));
-            list = content("foo-*", stateDir);
-            assertEquals(list.length, 1);
-            assertThat(list[0].getFileName().toString(), equalTo("foo-" + id + ".st"));
-            DummyState read = format.read(list[0]);
-            assertThat(read, equalTo(state));
-        }
-        final int version2 = between(version, Integer.MAX_VALUE);
-        DummyState state2 = new DummyState(randomRealisticUnicodeOfCodepointLengthBetween(1, 1000), randomInt(), randomLong(), randomDouble(), randomBoolean());
-        format.write(state2, version2, dirs);
-
-        for (Path file : dirs) {
-            Path[] list = content("*", file);
-            assertEquals(list.length, 1);
-            assertThat(list[0].getFileName().toString(), equalTo(MetaDataStateFormat.STATE_DIR_NAME));
-            Path stateDir = list[0];
-            assertThat(Files.isDirectory(stateDir), is(true));
-            list = content("foo-*", stateDir);
-            assertEquals(list.length,1);
-            assertThat(list[0].getFileName().toString(), equalTo("foo-"+ (id+1) + ".st"));
-            DummyState read = format.read(list[0]);
-            assertThat(read, equalTo(state2));
-
-        }
-    }
-
-    @Test
-    public void testVersionMismatch() throws IOException {
-        Path[] dirs = new Path[randomIntBetween(1, 5)];
-        for (int i = 0; i < dirs.length; i++) {
-            dirs[i] = createTempDir();
-        }
-        final long id = addDummyFiles("foo-", dirs);
-
-        Format format = new Format(randomFrom(XContentType.values()), "foo-");
-        DummyState state = new DummyState(randomRealisticUnicodeOfCodepointLengthBetween(1, 1000), randomInt(), randomLong(), randomDouble(), randomBoolean());
-        int version = between(0, Integer.MAX_VALUE/2);
-        format.write(state, version, dirs);
-        for (Path file : dirs) {
-            Path[] list = content("*", file);
-            assertEquals(list.length, 1);
-            assertThat(list[0].getFileName().toString(), equalTo(MetaDataStateFormat.STATE_DIR_NAME));
-            Path stateDir = list[0];
-            assertThat(Files.isDirectory(stateDir), is(true));
-            list = content("foo-*", stateDir);
-            assertEquals(list.length, 1);
-            assertThat(list[0].getFileName().toString(), equalTo("foo-" + id + ".st"));
-            DummyState read = format.read(list[0]);
-            assertThat(read, equalTo(state));
-        }
-    }
-
-    public void testCorruption() throws IOException {
-        Path[] dirs = new Path[randomIntBetween(1, 5)];
-        for (int i = 0; i < dirs.length; i++) {
-            dirs[i] = createTempDir();
-        }
-        final long id = addDummyFiles("foo-", dirs);
-        Format format = new Format(randomFrom(XContentType.values()), "foo-");
-        DummyState state = new DummyState(randomRealisticUnicodeOfCodepointLengthBetween(1, 1000), randomInt(), randomLong(), randomDouble(), randomBoolean());
-        int version = between(0, Integer.MAX_VALUE/2);
-        format.write(state, version, dirs);
-        for (Path file : dirs) {
-            Path[] list = content("*", file);
-            assertEquals(list.length, 1);
-            assertThat(list[0].getFileName().toString(), equalTo(MetaDataStateFormat.STATE_DIR_NAME));
-            Path stateDir = list[0];
-            assertThat(Files.isDirectory(stateDir), is(true));
-            list = content("foo-*", stateDir);
-            assertEquals(list.length, 1);
-            assertThat(list[0].getFileName().toString(), equalTo("foo-" + id + ".st"));
-            DummyState read = format.read(list[0]);
-            assertThat(read, equalTo(state));
-            // now corrupt it
-            corruptFile(list[0], logger);
-            try {
-                format.read(list[0]);
-                fail("corrupted file");
-            } catch (CorruptStateException ex) {
-                // expected
-            }
-        }
-    }
-
-    public static void corruptFile(Path file, ESLogger logger) throws IOException {
-        Path fileToCorrupt = file;
-        try (final SimpleFSDirectory dir = new SimpleFSDirectory(fileToCorrupt.getParent())) {
-            long checksumBeforeCorruption;
-            try (IndexInput input = dir.openInput(fileToCorrupt.getFileName().toString(), IOContext.DEFAULT)) {
-                checksumBeforeCorruption = CodecUtil.retrieveChecksum(input);
-            }
-            try (FileChannel raf = FileChannel.open(fileToCorrupt, StandardOpenOption.READ, StandardOpenOption.WRITE)) {
-                raf.position(randomIntBetween(0, (int)Math.min(Integer.MAX_VALUE, raf.size()-1)));
-                long filePointer = raf.position();
-                ByteBuffer bb = ByteBuffer.wrap(new byte[1]);
-                raf.read(bb);
-                
-                bb.flip();
-                byte oldValue = bb.get(0);
-                byte newValue = (byte) ~oldValue;
-                bb.put(0, newValue);
-                raf.write(bb, filePointer);
-                logger.debug("Corrupting file {} --  flipping at position {} from {} to {} ", fileToCorrupt.getFileName().toString(), filePointer, Integer.toHexString(oldValue), Integer.toHexString(newValue));
-            }
-        long checksumAfterCorruption;
-        long actualChecksumAfterCorruption;
-        try (ChecksumIndexInput input = dir.openChecksumInput(fileToCorrupt.getFileName().toString(), IOContext.DEFAULT)) {
-            assertThat(input.getFilePointer(), is(0l));
-            input.seek(input.length() - 8); // one long is the checksum... 8 bytes
-            checksumAfterCorruption = input.getChecksum();
-            actualChecksumAfterCorruption = input.readLong();
-        }
-        StringBuilder msg = new StringBuilder();
-        msg.append("Checksum before: [").append(checksumBeforeCorruption).append("]");
-        msg.append(" after: [").append(checksumAfterCorruption).append("]");
-        msg.append(" checksum value after corruption: ").append(actualChecksumAfterCorruption).append("]");
-        msg.append(" file: ").append(fileToCorrupt.getFileName().toString()).append(" length: ").append(dir.fileLength(fileToCorrupt.getFileName().toString()));
-        logger.debug(msg.toString());
-        assumeTrue("Checksum collision - " + msg.toString(),
-                checksumAfterCorruption != checksumBeforeCorruption // collision
-                        || actualChecksumAfterCorruption != checksumBeforeCorruption); // checksum corrupted
-        }
-    }
-
-    // If the latest version doesn't use the legacy format while previous versions do, then fail hard
-    public void testLatestVersionDoesNotUseLegacy() throws IOException {
-        final ToXContent.Params params = ToXContent.EMPTY_PARAMS;
-        MetaDataStateFormat<MetaData> format = MetaStateService.globalStateFormat(randomFrom(XContentType.values()), params);
-        final Path[] dirs = new Path[2];
-        dirs[0] = createTempDir();
-        dirs[1] = createTempDir();
-        for (Path dir : dirs) {
-            Files.createDirectories(dir.resolve(MetaDataStateFormat.STATE_DIR_NAME));
-        }
-        final Path dir1 = randomFrom(dirs);
-        final int v1 = randomInt(10);
-        // write a first state file in the new format
-        format.write(randomMeta(), v1, dir1);
-
-        // write older state files in the old format but with a newer version
-        final int numLegacyFiles = randomIntBetween(1, 5);
-        for (int i = 0; i < numLegacyFiles; ++i) {
-            final Path dir2 = randomFrom(dirs);
-            final int v2 = v1 + 1 + randomInt(10);
-            try (XContentBuilder xcontentBuilder = XContentFactory.contentBuilder(format.format(), Files.newOutputStream(dir2.resolve(MetaDataStateFormat.STATE_DIR_NAME).resolve(MetaStateService.GLOBAL_STATE_FILE_PREFIX + v2)))) {
-                xcontentBuilder.startObject();
-                MetaData.Builder.toXContent(randomMeta(), xcontentBuilder, params);
-                xcontentBuilder.endObject();
-            }
-        }
-
-        try {
-            format.loadLatestState(logger, dirs);
-            fail("latest version can not be read");
-        } catch (IllegalStateException ex) {
-            assertThat(ex.getMessage(), startsWith("Could not find a state file to recover from among "));
-        }
-        // write the next state file in the new format and ensure it get's a higher ID
-        final MetaData meta = randomMeta();
-        format.write(meta, v1, dirs);
-        final MetaData metaData = format.loadLatestState(logger, dirs);
-        assertEquals(meta.clusterUUID(), metaData.clusterUUID());
-        final Path path = randomFrom(dirs);
-        final Path[] files = FileSystemUtils.files(path.resolve("_state"));
-        assertEquals(1, files.length);
-        assertEquals("global-" + format.findMaxStateId("global-", dirs) + ".st", files[0].getFileName().toString());
-
-    }
-
-    // If both the legacy and the new format are available for the latest version, prefer the new format
-    public void testPrefersNewerFormat() throws IOException {
-        final ToXContent.Params params = ToXContent.EMPTY_PARAMS;
-        MetaDataStateFormat<MetaData> format = MetaStateService.globalStateFormat(randomFrom(XContentType.values()), params);
-        final Path[] dirs = new Path[2];
-        dirs[0] = createTempDir();
-        dirs[1] = createTempDir();
-        for (Path dir : dirs) {
-            Files.createDirectories(dir.resolve(MetaDataStateFormat.STATE_DIR_NAME));
-        }
-        final long v = randomInt(10);
-
-        MetaData meta = randomMeta();
-        String uuid = meta.clusterUUID();
-
-        // write a first state file in the old format
-        final Path dir2 = randomFrom(dirs);
-        MetaData meta2 = randomMeta();
-        assertFalse(meta2.clusterUUID().equals(uuid));
-        try (XContentBuilder xcontentBuilder = XContentFactory.contentBuilder(format.format(), Files.newOutputStream(dir2.resolve(MetaDataStateFormat.STATE_DIR_NAME).resolve(MetaStateService.GLOBAL_STATE_FILE_PREFIX + v)))) {
-            xcontentBuilder.startObject();
-            MetaData.Builder.toXContent(randomMeta(), xcontentBuilder, params);
-            xcontentBuilder.endObject();
-        }
-
-        // write a second state file in the new format but with the same version
-        format.write(meta, v, dirs);
-
-        MetaData state = format.loadLatestState(logger, dirs);
-        final Path path = randomFrom(dirs);
-        assertTrue(Files.exists(path.resolve(MetaDataStateFormat.STATE_DIR_NAME).resolve("global-" + (v+1) + ".st")));
-        assertEquals(state.clusterUUID(), uuid);
-    }
-
-    @Test
-    public void testLoadState() throws IOException {
-        final ToXContent.Params params = ToXContent.EMPTY_PARAMS;
-        final Path[] dirs = new Path[randomIntBetween(1, 5)];
-        int numStates = randomIntBetween(1, 5);
-        int numLegacy = randomIntBetween(0, numStates);
-        List<MetaData> meta = new ArrayList<>();
-        for (int i = 0; i < numStates; i++) {
-            meta.add(randomMeta());
-        }
-        Set<Path> corruptedFiles = new HashSet<>();
-        MetaDataStateFormat<MetaData> format = MetaStateService.globalStateFormat(randomFrom(XContentType.values()), params);
-        for (int i = 0; i < dirs.length; i++) {
-            dirs[i] = createTempDir();
-            Files.createDirectories(dirs[i].resolve(MetaDataStateFormat.STATE_DIR_NAME));
-            for (int j = 0; j < numLegacy; j++) {
-                XContentType type = format.format();
-                if (randomBoolean() && (j < numStates - 1 || dirs.length > 0 && i != 0)) {
-                    Path file = dirs[i].resolve(MetaDataStateFormat.STATE_DIR_NAME).resolve("global-"+j);
-                    Files.createFile(file); // randomly create 0-byte files -- there is extra logic to skip them
-                } else {
-                    try (XContentBuilder xcontentBuilder = XContentFactory.contentBuilder(type, Files.newOutputStream(dirs[i].resolve(MetaDataStateFormat.STATE_DIR_NAME).resolve("global-" + j)))) {
-                        xcontentBuilder.startObject();
-                        MetaData.Builder.toXContent(meta.get(j), xcontentBuilder, params);
-                        xcontentBuilder.endObject();
-                    }
-                }
-            }
-            for (int j = numLegacy; j < numStates; j++) {
-                format.write(meta.get(j), j, dirs[i]);
-                if (randomBoolean() && (j < numStates - 1 || dirs.length > 0 && i != 0)) {  // corrupt a file that we do not necessarily need here....
-                    Path file = dirs[i].resolve(MetaDataStateFormat.STATE_DIR_NAME).resolve("global-" + j + ".st");
-                    corruptedFiles.add(file);
-                    MetaDataStateFormatTest.corruptFile(file, logger);
-                }
-            }
-
-        }
-        List<Path> dirList = Arrays.asList(dirs);
-        Collections.shuffle(dirList, getRandom());
-        MetaData loadedMetaData = format.loadLatestState(logger, dirList.toArray(new Path[0]));
-        MetaData latestMetaData = meta.get(numStates-1);
-        assertThat(loadedMetaData.clusterUUID(), not(equalTo("_na_")));
-        assertThat(loadedMetaData.clusterUUID(), equalTo(latestMetaData.clusterUUID()));
-        ImmutableOpenMap<String,IndexMetaData> indices = loadedMetaData.indices();
-        assertThat(indices.size(), equalTo(latestMetaData.indices().size()));
-        for (IndexMetaData original : latestMetaData) {
-            IndexMetaData deserialized = indices.get(original.getIndex());
-            assertThat(deserialized, notNullValue());
-            assertThat(deserialized.version(), equalTo(original.version()));
-            assertThat(deserialized.numberOfReplicas(), equalTo(original.numberOfReplicas()));
-            assertThat(deserialized.numberOfShards(), equalTo(original.numberOfShards()));
-        }
-
-        // now corrupt all the latest ones and make sure we fail to load the state
-        if (numStates > numLegacy) {
-            for (int i = 0; i < dirs.length; i++) {
-                Path file = dirs[i].resolve(MetaDataStateFormat.STATE_DIR_NAME).resolve("global-" + (numStates-1) + ".st");
-                if (corruptedFiles.contains(file)) {
-                    continue;
-                }
-                MetaDataStateFormatTest.corruptFile(file, logger);
-            }
-            try {
-                format.loadLatestState(logger, dirList.toArray(new Path[0]));
-                fail("latest version can not be read");
-            } catch (ElasticsearchException ex) {
-                assertThat(ex.getCause(), instanceOf(CorruptStateException.class));
-            }
-        }
-
-    }
-
-    private MetaData randomMeta() throws IOException {
-        int numIndices = randomIntBetween(1, 10);
-        MetaData.Builder mdBuilder = MetaData.builder();
-        mdBuilder.generateClusterUuidIfNeeded();
-        for (int i = 0; i < numIndices; i++) {
-            mdBuilder.put(indexBuilder(randomAsciiOfLength(10) + "idx-"+i));
-        }
-        return mdBuilder.build();
-    }
-
-    private IndexMetaData.Builder indexBuilder(String index) throws IOException {
-        return IndexMetaData.builder(index)
-                .settings(settings(Version.CURRENT).put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, randomIntBetween(1, 10)).put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, randomIntBetween(0, 5)));
-    }
-
-
-    private class Format extends MetaDataStateFormat<DummyState> {
-
-        Format(XContentType format, String prefix) {
-            super(format, prefix);
-        }
-
-        @Override
-        public void toXContent(XContentBuilder builder, DummyState state) throws IOException {
-            state.toXContent(builder, null);
-        }
-
-        @Override
-        public DummyState fromXContent(XContentParser parser) throws IOException {
-            return new DummyState().parse(parser);
-        }
-
-        @Override
-        protected Directory newDirectory(Path dir) throws IOException {
-            MockDirectoryWrapper  mock = new MockDirectoryWrapper(getRandom(), super.newDirectory(dir));
-            closeAfterSuite(mock);
-            return mock;
-        }
-    }
-
-    private static class DummyState implements ToXContent {
-        String string;
-        int aInt;
-        long aLong;
-        double aDouble;
-        boolean aBoolean;
-
-        @Override
-        public String toString() {
-            return "DummyState{" +
-                    "string='" + string + '\'' +
-                    ", aInt=" + aInt +
-                    ", aLong=" + aLong +
-                    ", aDouble=" + aDouble +
-                    ", aBoolean=" + aBoolean +
-                    '}';
-        }
-
-        public DummyState(String string, int aInt, long aLong, double aDouble, boolean aBoolean) {
-            this.string = string;
-            this.aInt = aInt;
-            this.aLong = aLong;
-            this.aDouble = aDouble;
-            this.aBoolean = aBoolean;
-        }
-
-        public DummyState() {
-
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.field("string", string);
-            builder.field("int", aInt);
-            builder.field("long", aLong);
-            builder.field("double", aDouble);
-            builder.field("boolean", aBoolean);
-            return builder;
-        }
-
-        @Override
-        public boolean equals(Object o) {
-            if (this == o) return true;
-            if (o == null || getClass() != o.getClass()) return false;
-
-            DummyState that = (DummyState) o;
-
-            if (aBoolean != that.aBoolean) return false;
-            if (Double.compare(that.aDouble, aDouble) != 0) return false;
-            if (aInt != that.aInt) return false;
-            if (aLong != that.aLong) return false;
-            return string.equals(that.string);
-
-        }
-
-        @Override
-        public int hashCode() {
-            int result;
-            long temp;
-            result = string.hashCode();
-            result = 31 * result + aInt;
-            result = 31 * result + (int) (aLong ^ (aLong >>> 32));
-            temp = Double.doubleToLongBits(aDouble);
-            result = 31 * result + (int) (temp ^ (temp >>> 32));
-            result = 31 * result + (aBoolean ? 1 : 0);
-            return result;
-        }
-
-        public DummyState parse(XContentParser parser) throws IOException {
-            String fieldName = null;
-            parser.nextToken();  // start object
-            while(parser.nextToken() != XContentParser.Token.END_OBJECT) {
-                XContentParser.Token token = parser.currentToken();
-                if (token == XContentParser.Token.FIELD_NAME) {
-                  fieldName = parser.currentName();
-                } else if (token == XContentParser.Token.VALUE_STRING) {
-                    assertTrue("string".equals(fieldName));
-                    string = parser.text();
-                } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                    switch (fieldName) {
-                        case "double":
-                            aDouble = parser.doubleValue();
-                            break;
-                        case "int":
-                            aInt = parser.intValue();
-                            break;
-                        case "long":
-                            aLong = parser.longValue();
-                            break;
-                        default:
-                            fail("unexpected numeric value " + token);
-                            break;
-                    }
-                }else if (token == XContentParser.Token.VALUE_BOOLEAN) {
-                    assertTrue("boolean".equals(fieldName));
-                    aBoolean = parser.booleanValue();
-                } else {
-                    fail("unexpected value " + token);
-                }
-            }
-            return this;
-        }
-    }
-
-    public Path[] content(String glob, Path dir) throws IOException {
-        try (DirectoryStream<Path> stream = Files.newDirectoryStream(dir, glob)) {
-            return Iterators.toArray(stream.iterator(), Path.class);
-        }
-    }
-
-    public long addDummyFiles(String prefix, Path... paths) throws IOException {
-        int realId = -1;
-        for (Path path : paths) {
-            if (randomBoolean()) {
-                Path stateDir = path.resolve(MetaDataStateFormat.STATE_DIR_NAME);
-                Files.createDirectories(stateDir);
-                String actualPrefix = prefix;
-                int id = randomIntBetween(0, 10);
-                if (randomBoolean()) {
-                    actualPrefix = "dummy-";
-                } else {
-                   realId = Math.max(realId, id);
-                }
-                try (OutputStream stream = Files.newOutputStream(stateDir.resolve(actualPrefix + id + MetaDataStateFormat.STATE_FILE_EXTENSION))) {
-                    stream.write(0);
-                }
-            }
-        }
-        return realId + 1;
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/gateway/MetaDataStateFormatTests.java b/core/src/test/java/org/elasticsearch/gateway/MetaDataStateFormatTests.java
new file mode 100644
index 0000000..98630ed
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/gateway/MetaDataStateFormatTests.java
@@ -0,0 +1,562 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.gateway;
+
+import com.google.common.collect.Iterators;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.store.ChecksumIndexInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.store.SimpleFSDirectory;
+import org.apache.lucene.util.LuceneTestCase;
+import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.cluster.metadata.MetaData;
+import org.elasticsearch.common.collect.ImmutableOpenMap;
+import org.elasticsearch.common.io.FileSystemUtils;
+import org.elasticsearch.common.logging.ESLogger;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.common.xcontent.XContentType;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.io.InputStream;
+import java.io.IOException;
+import java.io.OutputStream;
+import java.net.URISyntaxException;
+import java.nio.ByteBuffer;
+import java.nio.channels.FileChannel;
+import java.nio.file.DirectoryStream;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.StandardOpenOption;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.instanceOf;
+import static org.hamcrest.Matchers.is;
+import static org.hamcrest.Matchers.not;
+import static org.hamcrest.Matchers.notNullValue;
+import static org.hamcrest.Matchers.startsWith;
+
+@LuceneTestCase.SuppressFileSystems("ExtrasFS") // TODO: fix test to work with ExtrasFS
+public class MetaDataStateFormatTests extends ESTestCase {
+
+
+    /**
+     * Ensure we can read a pre-generated cluster state.
+     */
+    public void testReadClusterState() throws URISyntaxException, IOException {
+        final MetaDataStateFormat<MetaData> format = new MetaDataStateFormat<MetaData>(randomFrom(XContentType.values()), "global-") {
+
+            @Override
+            public void toXContent(XContentBuilder builder, MetaData state) throws IOException {
+                fail("this test doesn't write");
+            }
+
+            @Override
+            public MetaData fromXContent(XContentParser parser) throws IOException {
+                return MetaData.Builder.fromXContent(parser);
+            }
+        };
+        Path tmp = createTempDir();
+        final InputStream resource = this.getClass().getResourceAsStream("global-3.st");
+        assertThat(resource, notNullValue());
+        Path dst = tmp.resolve("global-3.st");
+        Files.copy(resource, dst);
+        MetaData read = format.read(dst);
+        assertThat(read, notNullValue());
+        assertThat(read.clusterUUID(), equalTo("3O1tDF1IRB6fSJ-GrTMUtg"));
+        // indices are empty since they are serialized separately
+    }
+
+    public void testReadWriteState() throws IOException {
+        Path[] dirs = new Path[randomIntBetween(1, 5)];
+        for (int i = 0; i < dirs.length; i++) {
+            dirs[i] = createTempDir();
+        }
+        final long id = addDummyFiles("foo-", dirs);
+        Format format = new Format(randomFrom(XContentType.values()), "foo-");
+        DummyState state = new DummyState(randomRealisticUnicodeOfCodepointLengthBetween(1, 1000), randomInt(), randomLong(), randomDouble(), randomBoolean());
+        int version = between(0, Integer.MAX_VALUE/2);
+        format.write(state, version, dirs);
+        for (Path file : dirs) {
+            Path[] list = content("*", file);
+            assertEquals(list.length, 1);
+            assertThat(list[0].getFileName().toString(), equalTo(MetaDataStateFormat.STATE_DIR_NAME));
+            Path stateDir = list[0];
+            assertThat(Files.isDirectory(stateDir), is(true));
+            list = content("foo-*", stateDir);
+            assertEquals(list.length, 1);
+            assertThat(list[0].getFileName().toString(), equalTo("foo-" + id + ".st"));
+            DummyState read = format.read(list[0]);
+            assertThat(read, equalTo(state));
+        }
+        final int version2 = between(version, Integer.MAX_VALUE);
+        DummyState state2 = new DummyState(randomRealisticUnicodeOfCodepointLengthBetween(1, 1000), randomInt(), randomLong(), randomDouble(), randomBoolean());
+        format.write(state2, version2, dirs);
+
+        for (Path file : dirs) {
+            Path[] list = content("*", file);
+            assertEquals(list.length, 1);
+            assertThat(list[0].getFileName().toString(), equalTo(MetaDataStateFormat.STATE_DIR_NAME));
+            Path stateDir = list[0];
+            assertThat(Files.isDirectory(stateDir), is(true));
+            list = content("foo-*", stateDir);
+            assertEquals(list.length,1);
+            assertThat(list[0].getFileName().toString(), equalTo("foo-"+ (id+1) + ".st"));
+            DummyState read = format.read(list[0]);
+            assertThat(read, equalTo(state2));
+
+        }
+    }
+
+    @Test
+    public void testVersionMismatch() throws IOException {
+        Path[] dirs = new Path[randomIntBetween(1, 5)];
+        for (int i = 0; i < dirs.length; i++) {
+            dirs[i] = createTempDir();
+        }
+        final long id = addDummyFiles("foo-", dirs);
+
+        Format format = new Format(randomFrom(XContentType.values()), "foo-");
+        DummyState state = new DummyState(randomRealisticUnicodeOfCodepointLengthBetween(1, 1000), randomInt(), randomLong(), randomDouble(), randomBoolean());
+        int version = between(0, Integer.MAX_VALUE/2);
+        format.write(state, version, dirs);
+        for (Path file : dirs) {
+            Path[] list = content("*", file);
+            assertEquals(list.length, 1);
+            assertThat(list[0].getFileName().toString(), equalTo(MetaDataStateFormat.STATE_DIR_NAME));
+            Path stateDir = list[0];
+            assertThat(Files.isDirectory(stateDir), is(true));
+            list = content("foo-*", stateDir);
+            assertEquals(list.length, 1);
+            assertThat(list[0].getFileName().toString(), equalTo("foo-" + id + ".st"));
+            DummyState read = format.read(list[0]);
+            assertThat(read, equalTo(state));
+        }
+    }
+
+    public void testCorruption() throws IOException {
+        Path[] dirs = new Path[randomIntBetween(1, 5)];
+        for (int i = 0; i < dirs.length; i++) {
+            dirs[i] = createTempDir();
+        }
+        final long id = addDummyFiles("foo-", dirs);
+        Format format = new Format(randomFrom(XContentType.values()), "foo-");
+        DummyState state = new DummyState(randomRealisticUnicodeOfCodepointLengthBetween(1, 1000), randomInt(), randomLong(), randomDouble(), randomBoolean());
+        int version = between(0, Integer.MAX_VALUE/2);
+        format.write(state, version, dirs);
+        for (Path file : dirs) {
+            Path[] list = content("*", file);
+            assertEquals(list.length, 1);
+            assertThat(list[0].getFileName().toString(), equalTo(MetaDataStateFormat.STATE_DIR_NAME));
+            Path stateDir = list[0];
+            assertThat(Files.isDirectory(stateDir), is(true));
+            list = content("foo-*", stateDir);
+            assertEquals(list.length, 1);
+            assertThat(list[0].getFileName().toString(), equalTo("foo-" + id + ".st"));
+            DummyState read = format.read(list[0]);
+            assertThat(read, equalTo(state));
+            // now corrupt it
+            corruptFile(list[0], logger);
+            try {
+                format.read(list[0]);
+                fail("corrupted file");
+            } catch (CorruptStateException ex) {
+                // expected
+            }
+        }
+    }
+
+    public static void corruptFile(Path file, ESLogger logger) throws IOException {
+        Path fileToCorrupt = file;
+        try (final SimpleFSDirectory dir = new SimpleFSDirectory(fileToCorrupt.getParent())) {
+            long checksumBeforeCorruption;
+            try (IndexInput input = dir.openInput(fileToCorrupt.getFileName().toString(), IOContext.DEFAULT)) {
+                checksumBeforeCorruption = CodecUtil.retrieveChecksum(input);
+            }
+            try (FileChannel raf = FileChannel.open(fileToCorrupt, StandardOpenOption.READ, StandardOpenOption.WRITE)) {
+                raf.position(randomIntBetween(0, (int)Math.min(Integer.MAX_VALUE, raf.size()-1)));
+                long filePointer = raf.position();
+                ByteBuffer bb = ByteBuffer.wrap(new byte[1]);
+                raf.read(bb);
+                
+                bb.flip();
+                byte oldValue = bb.get(0);
+                byte newValue = (byte) ~oldValue;
+                bb.put(0, newValue);
+                raf.write(bb, filePointer);
+                logger.debug("Corrupting file {} --  flipping at position {} from {} to {} ", fileToCorrupt.getFileName().toString(), filePointer, Integer.toHexString(oldValue), Integer.toHexString(newValue));
+            }
+        long checksumAfterCorruption;
+        long actualChecksumAfterCorruption;
+        try (ChecksumIndexInput input = dir.openChecksumInput(fileToCorrupt.getFileName().toString(), IOContext.DEFAULT)) {
+            assertThat(input.getFilePointer(), is(0l));
+            input.seek(input.length() - 8); // one long is the checksum... 8 bytes
+            checksumAfterCorruption = input.getChecksum();
+            actualChecksumAfterCorruption = input.readLong();
+        }
+        StringBuilder msg = new StringBuilder();
+        msg.append("Checksum before: [").append(checksumBeforeCorruption).append("]");
+        msg.append(" after: [").append(checksumAfterCorruption).append("]");
+        msg.append(" checksum value after corruption: ").append(actualChecksumAfterCorruption).append("]");
+        msg.append(" file: ").append(fileToCorrupt.getFileName().toString()).append(" length: ").append(dir.fileLength(fileToCorrupt.getFileName().toString()));
+        logger.debug(msg.toString());
+        assumeTrue("Checksum collision - " + msg.toString(),
+                checksumAfterCorruption != checksumBeforeCorruption // collision
+                        || actualChecksumAfterCorruption != checksumBeforeCorruption); // checksum corrupted
+        }
+    }
+
+    // If the latest version doesn't use the legacy format while previous versions do, then fail hard
+    public void testLatestVersionDoesNotUseLegacy() throws IOException {
+        final ToXContent.Params params = ToXContent.EMPTY_PARAMS;
+        MetaDataStateFormat<MetaData> format = MetaStateService.globalStateFormat(randomFrom(XContentType.values()), params);
+        final Path[] dirs = new Path[2];
+        dirs[0] = createTempDir();
+        dirs[1] = createTempDir();
+        for (Path dir : dirs) {
+            Files.createDirectories(dir.resolve(MetaDataStateFormat.STATE_DIR_NAME));
+        }
+        final Path dir1 = randomFrom(dirs);
+        final int v1 = randomInt(10);
+        // write a first state file in the new format
+        format.write(randomMeta(), v1, dir1);
+
+        // write older state files in the old format but with a newer version
+        final int numLegacyFiles = randomIntBetween(1, 5);
+        for (int i = 0; i < numLegacyFiles; ++i) {
+            final Path dir2 = randomFrom(dirs);
+            final int v2 = v1 + 1 + randomInt(10);
+            try (XContentBuilder xcontentBuilder = XContentFactory.contentBuilder(format.format(), Files.newOutputStream(dir2.resolve(MetaDataStateFormat.STATE_DIR_NAME).resolve(MetaStateService.GLOBAL_STATE_FILE_PREFIX + v2)))) {
+                xcontentBuilder.startObject();
+                MetaData.Builder.toXContent(randomMeta(), xcontentBuilder, params);
+                xcontentBuilder.endObject();
+            }
+        }
+
+        try {
+            format.loadLatestState(logger, dirs);
+            fail("latest version can not be read");
+        } catch (IllegalStateException ex) {
+            assertThat(ex.getMessage(), startsWith("Could not find a state file to recover from among "));
+        }
+        // write the next state file in the new format and ensure it get's a higher ID
+        final MetaData meta = randomMeta();
+        format.write(meta, v1, dirs);
+        final MetaData metaData = format.loadLatestState(logger, dirs);
+        assertEquals(meta.clusterUUID(), metaData.clusterUUID());
+        final Path path = randomFrom(dirs);
+        final Path[] files = FileSystemUtils.files(path.resolve("_state"));
+        assertEquals(1, files.length);
+        assertEquals("global-" + format.findMaxStateId("global-", dirs) + ".st", files[0].getFileName().toString());
+
+    }
+
+    // If both the legacy and the new format are available for the latest version, prefer the new format
+    public void testPrefersNewerFormat() throws IOException {
+        final ToXContent.Params params = ToXContent.EMPTY_PARAMS;
+        MetaDataStateFormat<MetaData> format = MetaStateService.globalStateFormat(randomFrom(XContentType.values()), params);
+        final Path[] dirs = new Path[2];
+        dirs[0] = createTempDir();
+        dirs[1] = createTempDir();
+        for (Path dir : dirs) {
+            Files.createDirectories(dir.resolve(MetaDataStateFormat.STATE_DIR_NAME));
+        }
+        final long v = randomInt(10);
+
+        MetaData meta = randomMeta();
+        String uuid = meta.clusterUUID();
+
+        // write a first state file in the old format
+        final Path dir2 = randomFrom(dirs);
+        MetaData meta2 = randomMeta();
+        assertFalse(meta2.clusterUUID().equals(uuid));
+        try (XContentBuilder xcontentBuilder = XContentFactory.contentBuilder(format.format(), Files.newOutputStream(dir2.resolve(MetaDataStateFormat.STATE_DIR_NAME).resolve(MetaStateService.GLOBAL_STATE_FILE_PREFIX + v)))) {
+            xcontentBuilder.startObject();
+            MetaData.Builder.toXContent(randomMeta(), xcontentBuilder, params);
+            xcontentBuilder.endObject();
+        }
+
+        // write a second state file in the new format but with the same version
+        format.write(meta, v, dirs);
+
+        MetaData state = format.loadLatestState(logger, dirs);
+        final Path path = randomFrom(dirs);
+        assertTrue(Files.exists(path.resolve(MetaDataStateFormat.STATE_DIR_NAME).resolve("global-" + (v+1) + ".st")));
+        assertEquals(state.clusterUUID(), uuid);
+    }
+
+    @Test
+    public void testLoadState() throws IOException {
+        final ToXContent.Params params = ToXContent.EMPTY_PARAMS;
+        final Path[] dirs = new Path[randomIntBetween(1, 5)];
+        int numStates = randomIntBetween(1, 5);
+        int numLegacy = randomIntBetween(0, numStates);
+        List<MetaData> meta = new ArrayList<>();
+        for (int i = 0; i < numStates; i++) {
+            meta.add(randomMeta());
+        }
+        Set<Path> corruptedFiles = new HashSet<>();
+        MetaDataStateFormat<MetaData> format = MetaStateService.globalStateFormat(randomFrom(XContentType.values()), params);
+        for (int i = 0; i < dirs.length; i++) {
+            dirs[i] = createTempDir();
+            Files.createDirectories(dirs[i].resolve(MetaDataStateFormat.STATE_DIR_NAME));
+            for (int j = 0; j < numLegacy; j++) {
+                XContentType type = format.format();
+                if (randomBoolean() && (j < numStates - 1 || dirs.length > 0 && i != 0)) {
+                    Path file = dirs[i].resolve(MetaDataStateFormat.STATE_DIR_NAME).resolve("global-"+j);
+                    Files.createFile(file); // randomly create 0-byte files -- there is extra logic to skip them
+                } else {
+                    try (XContentBuilder xcontentBuilder = XContentFactory.contentBuilder(type, Files.newOutputStream(dirs[i].resolve(MetaDataStateFormat.STATE_DIR_NAME).resolve("global-" + j)))) {
+                        xcontentBuilder.startObject();
+                        MetaData.Builder.toXContent(meta.get(j), xcontentBuilder, params);
+                        xcontentBuilder.endObject();
+                    }
+                }
+            }
+            for (int j = numLegacy; j < numStates; j++) {
+                format.write(meta.get(j), j, dirs[i]);
+                if (randomBoolean() && (j < numStates - 1 || dirs.length > 0 && i != 0)) {  // corrupt a file that we do not necessarily need here....
+                    Path file = dirs[i].resolve(MetaDataStateFormat.STATE_DIR_NAME).resolve("global-" + j + ".st");
+                    corruptedFiles.add(file);
+                    MetaDataStateFormatTests.corruptFile(file, logger);
+                }
+            }
+
+        }
+        List<Path> dirList = Arrays.asList(dirs);
+        Collections.shuffle(dirList, getRandom());
+        MetaData loadedMetaData = format.loadLatestState(logger, dirList.toArray(new Path[0]));
+        MetaData latestMetaData = meta.get(numStates-1);
+        assertThat(loadedMetaData.clusterUUID(), not(equalTo("_na_")));
+        assertThat(loadedMetaData.clusterUUID(), equalTo(latestMetaData.clusterUUID()));
+        ImmutableOpenMap<String,IndexMetaData> indices = loadedMetaData.indices();
+        assertThat(indices.size(), equalTo(latestMetaData.indices().size()));
+        for (IndexMetaData original : latestMetaData) {
+            IndexMetaData deserialized = indices.get(original.getIndex());
+            assertThat(deserialized, notNullValue());
+            assertThat(deserialized.version(), equalTo(original.version()));
+            assertThat(deserialized.numberOfReplicas(), equalTo(original.numberOfReplicas()));
+            assertThat(deserialized.numberOfShards(), equalTo(original.numberOfShards()));
+        }
+
+        // now corrupt all the latest ones and make sure we fail to load the state
+        if (numStates > numLegacy) {
+            for (int i = 0; i < dirs.length; i++) {
+                Path file = dirs[i].resolve(MetaDataStateFormat.STATE_DIR_NAME).resolve("global-" + (numStates-1) + ".st");
+                if (corruptedFiles.contains(file)) {
+                    continue;
+                }
+                MetaDataStateFormatTests.corruptFile(file, logger);
+            }
+            try {
+                format.loadLatestState(logger, dirList.toArray(new Path[0]));
+                fail("latest version can not be read");
+            } catch (ElasticsearchException ex) {
+                assertThat(ex.getCause(), instanceOf(CorruptStateException.class));
+            }
+        }
+
+    }
+
+    private MetaData randomMeta() throws IOException {
+        int numIndices = randomIntBetween(1, 10);
+        MetaData.Builder mdBuilder = MetaData.builder();
+        mdBuilder.generateClusterUuidIfNeeded();
+        for (int i = 0; i < numIndices; i++) {
+            mdBuilder.put(indexBuilder(randomAsciiOfLength(10) + "idx-"+i));
+        }
+        return mdBuilder.build();
+    }
+
+    private IndexMetaData.Builder indexBuilder(String index) throws IOException {
+        return IndexMetaData.builder(index)
+                .settings(settings(Version.CURRENT).put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, randomIntBetween(1, 10)).put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, randomIntBetween(0, 5)));
+    }
+
+
+    private class Format extends MetaDataStateFormat<DummyState> {
+
+        Format(XContentType format, String prefix) {
+            super(format, prefix);
+        }
+
+        @Override
+        public void toXContent(XContentBuilder builder, DummyState state) throws IOException {
+            state.toXContent(builder, null);
+        }
+
+        @Override
+        public DummyState fromXContent(XContentParser parser) throws IOException {
+            return new DummyState().parse(parser);
+        }
+
+        @Override
+        protected Directory newDirectory(Path dir) throws IOException {
+            MockDirectoryWrapper  mock = new MockDirectoryWrapper(getRandom(), super.newDirectory(dir));
+            closeAfterSuite(mock);
+            return mock;
+        }
+    }
+
+    private static class DummyState implements ToXContent {
+        String string;
+        int aInt;
+        long aLong;
+        double aDouble;
+        boolean aBoolean;
+
+        @Override
+        public String toString() {
+            return "DummyState{" +
+                    "string='" + string + '\'' +
+                    ", aInt=" + aInt +
+                    ", aLong=" + aLong +
+                    ", aDouble=" + aDouble +
+                    ", aBoolean=" + aBoolean +
+                    '}';
+        }
+
+        public DummyState(String string, int aInt, long aLong, double aDouble, boolean aBoolean) {
+            this.string = string;
+            this.aInt = aInt;
+            this.aLong = aLong;
+            this.aDouble = aDouble;
+            this.aBoolean = aBoolean;
+        }
+
+        public DummyState() {
+
+        }
+
+        @Override
+        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+            builder.field("string", string);
+            builder.field("int", aInt);
+            builder.field("long", aLong);
+            builder.field("double", aDouble);
+            builder.field("boolean", aBoolean);
+            return builder;
+        }
+
+        @Override
+        public boolean equals(Object o) {
+            if (this == o) return true;
+            if (o == null || getClass() != o.getClass()) return false;
+
+            DummyState that = (DummyState) o;
+
+            if (aBoolean != that.aBoolean) return false;
+            if (Double.compare(that.aDouble, aDouble) != 0) return false;
+            if (aInt != that.aInt) return false;
+            if (aLong != that.aLong) return false;
+            return string.equals(that.string);
+
+        }
+
+        @Override
+        public int hashCode() {
+            int result;
+            long temp;
+            result = string.hashCode();
+            result = 31 * result + aInt;
+            result = 31 * result + (int) (aLong ^ (aLong >>> 32));
+            temp = Double.doubleToLongBits(aDouble);
+            result = 31 * result + (int) (temp ^ (temp >>> 32));
+            result = 31 * result + (aBoolean ? 1 : 0);
+            return result;
+        }
+
+        public DummyState parse(XContentParser parser) throws IOException {
+            String fieldName = null;
+            parser.nextToken();  // start object
+            while(parser.nextToken() != XContentParser.Token.END_OBJECT) {
+                XContentParser.Token token = parser.currentToken();
+                if (token == XContentParser.Token.FIELD_NAME) {
+                  fieldName = parser.currentName();
+                } else if (token == XContentParser.Token.VALUE_STRING) {
+                    assertTrue("string".equals(fieldName));
+                    string = parser.text();
+                } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                    switch (fieldName) {
+                        case "double":
+                            aDouble = parser.doubleValue();
+                            break;
+                        case "int":
+                            aInt = parser.intValue();
+                            break;
+                        case "long":
+                            aLong = parser.longValue();
+                            break;
+                        default:
+                            fail("unexpected numeric value " + token);
+                            break;
+                    }
+                }else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                    assertTrue("boolean".equals(fieldName));
+                    aBoolean = parser.booleanValue();
+                } else {
+                    fail("unexpected value " + token);
+                }
+            }
+            return this;
+        }
+    }
+
+    public Path[] content(String glob, Path dir) throws IOException {
+        try (DirectoryStream<Path> stream = Files.newDirectoryStream(dir, glob)) {
+            return Iterators.toArray(stream.iterator(), Path.class);
+        }
+    }
+
+    public long addDummyFiles(String prefix, Path... paths) throws IOException {
+        int realId = -1;
+        for (Path path : paths) {
+            if (randomBoolean()) {
+                Path stateDir = path.resolve(MetaDataStateFormat.STATE_DIR_NAME);
+                Files.createDirectories(stateDir);
+                String actualPrefix = prefix;
+                int id = randomIntBetween(0, 10);
+                if (randomBoolean()) {
+                    actualPrefix = "dummy-";
+                } else {
+                   realId = Math.max(realId, id);
+                }
+                try (OutputStream stream = Files.newOutputStream(stateDir.resolve(actualPrefix + id + MetaDataStateFormat.STATE_FILE_EXTENSION))) {
+                    stream.write(0);
+                }
+            }
+        }
+        return realId + 1;
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/http/netty/NettyHttpServerPipeliningTest.java b/core/src/test/java/org/elasticsearch/http/netty/NettyHttpServerPipeliningTest.java
deleted file mode 100644
index e3a5861..0000000
--- a/core/src/test/java/org/elasticsearch/http/netty/NettyHttpServerPipeliningTest.java
+++ /dev/null
@@ -1,230 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.http.netty;
-
-import com.google.common.base.Charsets;
-import org.elasticsearch.cache.recycler.MockPageCacheRecycler;
-import org.elasticsearch.common.network.NetworkService;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.transport.InetSocketTransportAddress;
-import org.elasticsearch.common.util.MockBigArrays;
-import org.elasticsearch.http.HttpServerTransport;
-import org.elasticsearch.http.netty.pipelining.OrderedDownstreamChannelEvent;
-import org.elasticsearch.http.netty.pipelining.OrderedUpstreamMessageEvent;
-import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.jboss.netty.buffer.ChannelBuffer;
-import org.jboss.netty.buffer.ChannelBuffers;
-import org.jboss.netty.channel.ChannelHandlerContext;
-import org.jboss.netty.channel.ChannelPipeline;
-import org.jboss.netty.channel.ChannelPipelineFactory;
-import org.jboss.netty.channel.ExceptionEvent;
-import org.jboss.netty.channel.MessageEvent;
-import org.jboss.netty.channel.SimpleChannelUpstreamHandler;
-import org.jboss.netty.handler.codec.http.DefaultHttpResponse;
-import org.jboss.netty.handler.codec.http.HttpRequest;
-import org.jboss.netty.handler.codec.http.HttpResponse;
-import org.jboss.netty.handler.codec.http.QueryStringDecoder;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.List;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.Executors;
-
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.http.netty.NettyHttpClient.returnHttpResponseBodies;
-import static org.elasticsearch.http.netty.NettyHttpServerTransport.HttpChannelPipelineFactory;
-import static org.hamcrest.Matchers.*;
-import static org.jboss.netty.handler.codec.http.HttpHeaders.Names.CONTENT_LENGTH;
-import static org.jboss.netty.handler.codec.http.HttpResponseStatus.OK;
-import static org.jboss.netty.handler.codec.http.HttpVersion.HTTP_1_1;
-
-/**
- * This test just tests, if he pipelining works in general with out any connection the elasticsearch handler
- */
-public class NettyHttpServerPipeliningTest extends ESTestCase {
-
-    private NetworkService networkService;
-    private ThreadPool threadPool;
-    private MockPageCacheRecycler mockPageCacheRecycler;
-    private MockBigArrays bigArrays;
-    private CustomNettyHttpServerTransport httpServerTransport;
-
-    @Before
-    public void setup() throws Exception {
-        networkService = new NetworkService(Settings.EMPTY);
-        threadPool = new ThreadPool("test");
-        mockPageCacheRecycler = new MockPageCacheRecycler(Settings.EMPTY, threadPool);
-        bigArrays = new MockBigArrays(mockPageCacheRecycler, new NoneCircuitBreakerService());
-    }
-
-    @After
-    public void shutdown() throws Exception {
-        if (threadPool != null) {
-            threadPool.shutdownNow();
-        }
-        if (httpServerTransport != null) {
-            httpServerTransport.close();
-        }
-    }
-
-    @Test
-    public void testThatHttpPipeliningWorksWhenEnabled() throws Exception {
-        Settings settings = settingsBuilder().put("http.pipelining", true).build();
-        httpServerTransport = new CustomNettyHttpServerTransport(settings);
-        httpServerTransport.start();
-        InetSocketTransportAddress transportAddress = (InetSocketTransportAddress) httpServerTransport.boundAddress().boundAddress();
-
-        List<String> requests = Arrays.asList("/firstfast", "/slow?sleep=500", "/secondfast", "/slow?sleep=1000", "/thirdfast");
-        try (NettyHttpClient nettyHttpClient = new NettyHttpClient()) {
-            Collection<HttpResponse> responses = nettyHttpClient.sendRequests(transportAddress.address(), requests.toArray(new String[]{}));
-            Collection<String> responseBodies = returnHttpResponseBodies(responses);
-            assertThat(responseBodies, contains("/firstfast", "/slow?sleep=500", "/secondfast", "/slow?sleep=1000", "/thirdfast"));
-        }
-    }
-
-    @Test
-    public void testThatHttpPipeliningCanBeDisabled() throws Exception {
-        Settings settings = settingsBuilder().put("http.pipelining", false).build();
-        httpServerTransport = new CustomNettyHttpServerTransport(settings);
-        httpServerTransport.start();
-        InetSocketTransportAddress transportAddress = (InetSocketTransportAddress) httpServerTransport.boundAddress().boundAddress();
-
-        List<String> requests = Arrays.asList("/slow?sleep=1000", "/firstfast", "/secondfast", "/thirdfast", "/slow?sleep=500");
-        try (NettyHttpClient nettyHttpClient = new NettyHttpClient()) {
-            Collection<HttpResponse> responses = nettyHttpClient.sendRequests(transportAddress.address(), requests.toArray(new String[]{}));
-            List<String> responseBodies = new ArrayList<>(returnHttpResponseBodies(responses));
-            // we cannot be sure about the order of the fast requests, but the slow ones should have to be last
-            assertThat(responseBodies, hasSize(5));
-            assertThat(responseBodies.get(3), is("/slow?sleep=500"));
-            assertThat(responseBodies.get(4), is("/slow?sleep=1000"));
-        }
-    }
-
-    class CustomNettyHttpServerTransport extends NettyHttpServerTransport {
-
-        private final ExecutorService executorService;
-
-        public CustomNettyHttpServerTransport(Settings settings) {
-            super(settings, NettyHttpServerPipeliningTest.this.networkService, NettyHttpServerPipeliningTest.this.bigArrays);
-            this.executorService = Executors.newFixedThreadPool(5);
-        }
-
-        @Override
-        public ChannelPipelineFactory configureServerChannelPipelineFactory() {
-            return new CustomHttpChannelPipelineFactory(this, executorService);
-        }
-
-        @Override
-        public HttpServerTransport stop() {
-            executorService.shutdownNow();
-            return super.stop();
-        }
-    }
-
-    private class CustomHttpChannelPipelineFactory extends HttpChannelPipelineFactory {
-
-        private final ExecutorService executorService;
-
-        public CustomHttpChannelPipelineFactory(NettyHttpServerTransport transport, ExecutorService executorService) {
-            super(transport, randomBoolean());
-            this.executorService = executorService;
-        }
-
-        @Override
-        public ChannelPipeline getPipeline() throws Exception {
-            ChannelPipeline pipeline = super.getPipeline();
-            pipeline.replace("handler", "handler", new PossiblySlowUpstreamHandler(executorService));
-            return pipeline;
-        }
-    }
-
-    class PossiblySlowUpstreamHandler extends SimpleChannelUpstreamHandler {
-
-        private final ExecutorService executorService;
-
-        public PossiblySlowUpstreamHandler(ExecutorService executorService) {
-            this.executorService = executorService;
-        }
-
-        @Override
-        public void messageReceived(final ChannelHandlerContext ctx, final MessageEvent e) throws Exception {
-            executorService.submit(new PossiblySlowRunnable(ctx, e));
-        }
-
-        @Override
-        public void exceptionCaught(ChannelHandlerContext ctx, ExceptionEvent e) {
-            e.getCause().printStackTrace();
-            e.getChannel().close();
-        }
-    }
-
-    class PossiblySlowRunnable implements Runnable {
-
-        private ChannelHandlerContext ctx;
-        private MessageEvent e;
-
-        public PossiblySlowRunnable(ChannelHandlerContext ctx, MessageEvent e) {
-            this.ctx = ctx;
-            this.e = e;
-        }
-
-        @Override
-        public void run() {
-            HttpRequest request;
-            OrderedUpstreamMessageEvent oue = null;
-            if (e instanceof OrderedUpstreamMessageEvent) {
-                oue = (OrderedUpstreamMessageEvent) e;
-                request = (HttpRequest) oue.getMessage();
-            } else {
-                request = (HttpRequest) e.getMessage();
-            }
-
-            ChannelBuffer buffer = ChannelBuffers.copiedBuffer(request.getUri(), Charsets.UTF_8);
-
-            DefaultHttpResponse httpResponse = new DefaultHttpResponse(HTTP_1_1, OK);
-            httpResponse.headers().add(CONTENT_LENGTH, buffer.readableBytes());
-            httpResponse.setContent(buffer);
-
-            QueryStringDecoder decoder = new QueryStringDecoder(request.getUri());
-
-            final int timeout = request.getUri().startsWith("/slow") && decoder.getParameters().containsKey("sleep") ? Integer.valueOf(decoder.getParameters().get("sleep").get(0)) : 0;
-            if (timeout > 0) {
-                try {
-                    Thread.sleep(timeout);
-                } catch (InterruptedException e1) {
-                    Thread.currentThread().interrupt();
-                    throw new RuntimeException();
-                }
-            }
-
-            if (oue != null) {
-                ctx.sendDownstream(new OrderedDownstreamChannelEvent(oue, 0, true, httpResponse));
-            } else {
-                ctx.getChannel().write(httpResponse);
-            }
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/http/netty/NettyHttpServerPipeliningTests.java b/core/src/test/java/org/elasticsearch/http/netty/NettyHttpServerPipeliningTests.java
new file mode 100644
index 0000000..3274d4f
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/http/netty/NettyHttpServerPipeliningTests.java
@@ -0,0 +1,230 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.http.netty;
+
+import com.google.common.base.Charsets;
+import org.elasticsearch.cache.recycler.MockPageCacheRecycler;
+import org.elasticsearch.common.network.NetworkService;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.transport.InetSocketTransportAddress;
+import org.elasticsearch.common.util.MockBigArrays;
+import org.elasticsearch.http.HttpServerTransport;
+import org.elasticsearch.http.netty.pipelining.OrderedDownstreamChannelEvent;
+import org.elasticsearch.http.netty.pipelining.OrderedUpstreamMessageEvent;
+import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.jboss.netty.buffer.ChannelBuffer;
+import org.jboss.netty.buffer.ChannelBuffers;
+import org.jboss.netty.channel.ChannelHandlerContext;
+import org.jboss.netty.channel.ChannelPipeline;
+import org.jboss.netty.channel.ChannelPipelineFactory;
+import org.jboss.netty.channel.ExceptionEvent;
+import org.jboss.netty.channel.MessageEvent;
+import org.jboss.netty.channel.SimpleChannelUpstreamHandler;
+import org.jboss.netty.handler.codec.http.DefaultHttpResponse;
+import org.jboss.netty.handler.codec.http.HttpRequest;
+import org.jboss.netty.handler.codec.http.HttpResponse;
+import org.jboss.netty.handler.codec.http.QueryStringDecoder;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.List;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+
+import static org.elasticsearch.common.settings.Settings.settingsBuilder;
+import static org.elasticsearch.http.netty.NettyHttpClient.returnHttpResponseBodies;
+import static org.elasticsearch.http.netty.NettyHttpServerTransport.HttpChannelPipelineFactory;
+import static org.hamcrest.Matchers.*;
+import static org.jboss.netty.handler.codec.http.HttpHeaders.Names.CONTENT_LENGTH;
+import static org.jboss.netty.handler.codec.http.HttpResponseStatus.OK;
+import static org.jboss.netty.handler.codec.http.HttpVersion.HTTP_1_1;
+
+/**
+ * This test just tests, if he pipelining works in general with out any connection the elasticsearch handler
+ */
+public class NettyHttpServerPipeliningTests extends ESTestCase {
+
+    private NetworkService networkService;
+    private ThreadPool threadPool;
+    private MockPageCacheRecycler mockPageCacheRecycler;
+    private MockBigArrays bigArrays;
+    private CustomNettyHttpServerTransport httpServerTransport;
+
+    @Before
+    public void setup() throws Exception {
+        networkService = new NetworkService(Settings.EMPTY);
+        threadPool = new ThreadPool("test");
+        mockPageCacheRecycler = new MockPageCacheRecycler(Settings.EMPTY, threadPool);
+        bigArrays = new MockBigArrays(mockPageCacheRecycler, new NoneCircuitBreakerService());
+    }
+
+    @After
+    public void shutdown() throws Exception {
+        if (threadPool != null) {
+            threadPool.shutdownNow();
+        }
+        if (httpServerTransport != null) {
+            httpServerTransport.close();
+        }
+    }
+
+    @Test
+    public void testThatHttpPipeliningWorksWhenEnabled() throws Exception {
+        Settings settings = settingsBuilder().put("http.pipelining", true).build();
+        httpServerTransport = new CustomNettyHttpServerTransport(settings);
+        httpServerTransport.start();
+        InetSocketTransportAddress transportAddress = (InetSocketTransportAddress) httpServerTransport.boundAddress().boundAddress();
+
+        List<String> requests = Arrays.asList("/firstfast", "/slow?sleep=500", "/secondfast", "/slow?sleep=1000", "/thirdfast");
+        try (NettyHttpClient nettyHttpClient = new NettyHttpClient()) {
+            Collection<HttpResponse> responses = nettyHttpClient.sendRequests(transportAddress.address(), requests.toArray(new String[]{}));
+            Collection<String> responseBodies = returnHttpResponseBodies(responses);
+            assertThat(responseBodies, contains("/firstfast", "/slow?sleep=500", "/secondfast", "/slow?sleep=1000", "/thirdfast"));
+        }
+    }
+
+    @Test
+    public void testThatHttpPipeliningCanBeDisabled() throws Exception {
+        Settings settings = settingsBuilder().put("http.pipelining", false).build();
+        httpServerTransport = new CustomNettyHttpServerTransport(settings);
+        httpServerTransport.start();
+        InetSocketTransportAddress transportAddress = (InetSocketTransportAddress) httpServerTransport.boundAddress().boundAddress();
+
+        List<String> requests = Arrays.asList("/slow?sleep=1000", "/firstfast", "/secondfast", "/thirdfast", "/slow?sleep=500");
+        try (NettyHttpClient nettyHttpClient = new NettyHttpClient()) {
+            Collection<HttpResponse> responses = nettyHttpClient.sendRequests(transportAddress.address(), requests.toArray(new String[]{}));
+            List<String> responseBodies = new ArrayList<>(returnHttpResponseBodies(responses));
+            // we cannot be sure about the order of the fast requests, but the slow ones should have to be last
+            assertThat(responseBodies, hasSize(5));
+            assertThat(responseBodies.get(3), is("/slow?sleep=500"));
+            assertThat(responseBodies.get(4), is("/slow?sleep=1000"));
+        }
+    }
+
+    class CustomNettyHttpServerTransport extends NettyHttpServerTransport {
+
+        private final ExecutorService executorService;
+
+        public CustomNettyHttpServerTransport(Settings settings) {
+            super(settings, NettyHttpServerPipeliningTests.this.networkService, NettyHttpServerPipeliningTests.this.bigArrays);
+            this.executorService = Executors.newFixedThreadPool(5);
+        }
+
+        @Override
+        public ChannelPipelineFactory configureServerChannelPipelineFactory() {
+            return new CustomHttpChannelPipelineFactory(this, executorService);
+        }
+
+        @Override
+        public HttpServerTransport stop() {
+            executorService.shutdownNow();
+            return super.stop();
+        }
+    }
+
+    private class CustomHttpChannelPipelineFactory extends HttpChannelPipelineFactory {
+
+        private final ExecutorService executorService;
+
+        public CustomHttpChannelPipelineFactory(NettyHttpServerTransport transport, ExecutorService executorService) {
+            super(transport, randomBoolean());
+            this.executorService = executorService;
+        }
+
+        @Override
+        public ChannelPipeline getPipeline() throws Exception {
+            ChannelPipeline pipeline = super.getPipeline();
+            pipeline.replace("handler", "handler", new PossiblySlowUpstreamHandler(executorService));
+            return pipeline;
+        }
+    }
+
+    class PossiblySlowUpstreamHandler extends SimpleChannelUpstreamHandler {
+
+        private final ExecutorService executorService;
+
+        public PossiblySlowUpstreamHandler(ExecutorService executorService) {
+            this.executorService = executorService;
+        }
+
+        @Override
+        public void messageReceived(final ChannelHandlerContext ctx, final MessageEvent e) throws Exception {
+            executorService.submit(new PossiblySlowRunnable(ctx, e));
+        }
+
+        @Override
+        public void exceptionCaught(ChannelHandlerContext ctx, ExceptionEvent e) {
+            e.getCause().printStackTrace();
+            e.getChannel().close();
+        }
+    }
+
+    class PossiblySlowRunnable implements Runnable {
+
+        private ChannelHandlerContext ctx;
+        private MessageEvent e;
+
+        public PossiblySlowRunnable(ChannelHandlerContext ctx, MessageEvent e) {
+            this.ctx = ctx;
+            this.e = e;
+        }
+
+        @Override
+        public void run() {
+            HttpRequest request;
+            OrderedUpstreamMessageEvent oue = null;
+            if (e instanceof OrderedUpstreamMessageEvent) {
+                oue = (OrderedUpstreamMessageEvent) e;
+                request = (HttpRequest) oue.getMessage();
+            } else {
+                request = (HttpRequest) e.getMessage();
+            }
+
+            ChannelBuffer buffer = ChannelBuffers.copiedBuffer(request.getUri(), Charsets.UTF_8);
+
+            DefaultHttpResponse httpResponse = new DefaultHttpResponse(HTTP_1_1, OK);
+            httpResponse.headers().add(CONTENT_LENGTH, buffer.readableBytes());
+            httpResponse.setContent(buffer);
+
+            QueryStringDecoder decoder = new QueryStringDecoder(request.getUri());
+
+            final int timeout = request.getUri().startsWith("/slow") && decoder.getParameters().containsKey("sleep") ? Integer.valueOf(decoder.getParameters().get("sleep").get(0)) : 0;
+            if (timeout > 0) {
+                try {
+                    Thread.sleep(timeout);
+                } catch (InterruptedException e1) {
+                    Thread.currentThread().interrupt();
+                    throw new RuntimeException();
+                }
+            }
+
+            if (oue != null) {
+                ctx.sendDownstream(new OrderedDownstreamChannelEvent(oue, 0, true, httpResponse));
+            } else {
+                ctx.getChannel().write(httpResponse);
+            }
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/http/netty/pipelining/HttpPipeliningHandlerTest.java b/core/src/test/java/org/elasticsearch/http/netty/pipelining/HttpPipeliningHandlerTest.java
deleted file mode 100644
index f0368fb..0000000
--- a/core/src/test/java/org/elasticsearch/http/netty/pipelining/HttpPipeliningHandlerTest.java
+++ /dev/null
@@ -1,217 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.http.netty.pipelining;
-
-import org.elasticsearch.common.network.NetworkAddress;
-import org.elasticsearch.test.ESTestCase;
-import org.jboss.netty.bootstrap.ClientBootstrap;
-import org.jboss.netty.bootstrap.ServerBootstrap;
-import org.jboss.netty.channel.*;
-import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory;
-import org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory;
-import org.jboss.netty.handler.codec.http.*;
-import org.jboss.netty.util.HashedWheelTimer;
-import org.jboss.netty.util.Timeout;
-import org.jboss.netty.util.TimerTask;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.net.InetAddress;
-import java.net.InetSocketAddress;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.atomic.AtomicBoolean;
-
-import static java.util.concurrent.TimeUnit.MILLISECONDS;
-import static org.jboss.netty.buffer.ChannelBuffers.EMPTY_BUFFER;
-import static org.jboss.netty.buffer.ChannelBuffers.copiedBuffer;
-import static org.jboss.netty.handler.codec.http.HttpHeaders.Names.*;
-import static org.jboss.netty.handler.codec.http.HttpHeaders.Values.CHUNKED;
-import static org.jboss.netty.handler.codec.http.HttpHeaders.Values.KEEP_ALIVE;
-import static org.jboss.netty.handler.codec.http.HttpResponseStatus.OK;
-import static org.jboss.netty.handler.codec.http.HttpVersion.HTTP_1_1;
-import static org.jboss.netty.util.CharsetUtil.UTF_8;
-
-/**
- *
- */
-public class HttpPipeliningHandlerTest extends ESTestCase {
-
-    private static final long RESPONSE_TIMEOUT = 10000L;
-    private static final long CONNECTION_TIMEOUT = 10000L;
-    private static final String CONTENT_TYPE_TEXT = "text/plain; charset=UTF-8";
-    // TODO make me random
-    private static final InetSocketAddress HOST_ADDR = new InetSocketAddress(InetAddress.getLoopbackAddress(), 9080);
-    private static final String PATH1 = "/1";
-    private static final String PATH2 = "/2";
-    private static final String SOME_RESPONSE_TEXT = "some response for ";
-
-    private ClientBootstrap clientBootstrap;
-    private ServerBootstrap serverBootstrap;
-
-    private CountDownLatch responsesIn;
-    private final List<String> responses = new ArrayList<>(2);
-
-    private HashedWheelTimer timer;
-
-    @Before
-    public void startBootstraps() {
-        clientBootstrap = new ClientBootstrap(new NioClientSocketChannelFactory());
-
-        clientBootstrap.setPipelineFactory(new ChannelPipelineFactory() {
-            @Override
-            public ChannelPipeline getPipeline() throws Exception {
-                return Channels.pipeline(
-                        new HttpClientCodec(),
-                        new ClientHandler()
-                );
-            }
-        });
-
-        serverBootstrap = new ServerBootstrap(new NioServerSocketChannelFactory());
-
-        serverBootstrap.setPipelineFactory(new ChannelPipelineFactory() {
-            @Override
-            public ChannelPipeline getPipeline() throws Exception {
-                return Channels.pipeline(
-                        new HttpRequestDecoder(),
-                        new HttpResponseEncoder(),
-                        new HttpPipeliningHandler(10000),
-                        new ServerHandler()
-                );
-            }
-        });
-
-        serverBootstrap.bind(HOST_ADDR);
-
-        timer = new HashedWheelTimer();
-    }
-
-    @After
-    public void releaseResources() {
-        timer.stop();
-
-        serverBootstrap.shutdown();
-        serverBootstrap.releaseExternalResources();
-        clientBootstrap.shutdown();
-        clientBootstrap.releaseExternalResources();
-    }
-
-    @Test
-    public void shouldReturnMessagesInOrder() throws InterruptedException {
-        responsesIn = new CountDownLatch(1);
-        responses.clear();
-
-        final ChannelFuture connectionFuture = clientBootstrap.connect(HOST_ADDR);
-
-        assertTrue(connectionFuture.await(CONNECTION_TIMEOUT));
-        final Channel clientChannel = connectionFuture.getChannel();
-
-        // NetworkAddress.formatAddress makes a proper HOST header.
-        final HttpRequest request1 = new DefaultHttpRequest(
-                HTTP_1_1, HttpMethod.GET, PATH1);
-        request1.headers().add(HOST, NetworkAddress.formatAddress(HOST_ADDR));
-
-        final HttpRequest request2 = new DefaultHttpRequest(
-                HTTP_1_1, HttpMethod.GET, PATH2);
-        request2.headers().add(HOST, NetworkAddress.formatAddress(HOST_ADDR));
-
-        clientChannel.write(request1);
-        clientChannel.write(request2);
-
-        responsesIn.await(RESPONSE_TIMEOUT, MILLISECONDS);
-
-        assertTrue(responses.contains(SOME_RESPONSE_TEXT + PATH1));
-        assertTrue(responses.contains(SOME_RESPONSE_TEXT + PATH2));
-    }
-
-    public class ClientHandler extends SimpleChannelUpstreamHandler {
-        @Override
-        public void messageReceived(final ChannelHandlerContext ctx, final MessageEvent e) {
-            final Object message = e.getMessage();
-            if (message instanceof HttpChunk) {
-                final HttpChunk response = (HttpChunk) e.getMessage();
-                if (!response.isLast()) {
-                    final String content = response.getContent().toString(UTF_8);
-                    responses.add(content);
-                    if (content.equals(SOME_RESPONSE_TEXT + PATH2)) {
-                        responsesIn.countDown();
-                    }
-                }
-            }
-        }
-    }
-
-    public class ServerHandler extends SimpleChannelUpstreamHandler {
-        private final AtomicBoolean sendFinalChunk = new AtomicBoolean(false);
-
-        @Override
-        public void messageReceived(final ChannelHandlerContext ctx, final MessageEvent e) throws InterruptedException {
-            final HttpRequest request = (HttpRequest) e.getMessage();
-
-            final OrderedUpstreamMessageEvent oue = (OrderedUpstreamMessageEvent) e;
-            final String uri = request.getUri();
-
-            final HttpResponse initialChunk = new DefaultHttpResponse(HTTP_1_1, OK);
-            initialChunk.headers().add(CONTENT_TYPE, CONTENT_TYPE_TEXT);
-            initialChunk.headers().add(CONNECTION, KEEP_ALIVE);
-            initialChunk.headers().add(TRANSFER_ENCODING, CHUNKED);
-
-            ctx.sendDownstream(new OrderedDownstreamChannelEvent(oue, 0, false, initialChunk));
-
-            timer.newTimeout(new ChunkWriter(ctx, e, uri, oue, 1), 0, MILLISECONDS);
-        }
-
-        private class ChunkWriter implements TimerTask {
-            private final ChannelHandlerContext ctx;
-            private final MessageEvent e;
-            private final String uri;
-            private final OrderedUpstreamMessageEvent oue;
-            private final int subSequence;
-
-            public ChunkWriter(final ChannelHandlerContext ctx, final MessageEvent e, final String uri,
-                               final OrderedUpstreamMessageEvent oue, final int subSequence) {
-                this.ctx = ctx;
-                this.e = e;
-                this.uri = uri;
-                this.oue = oue;
-                this.subSequence = subSequence;
-            }
-
-            @Override
-            public void run(final Timeout timeout) {
-                if (sendFinalChunk.get() && subSequence > 1) {
-                    final HttpChunk finalChunk = new DefaultHttpChunk(EMPTY_BUFFER);
-                    ctx.sendDownstream(new OrderedDownstreamChannelEvent(oue, subSequence, true, finalChunk));
-                } else {
-                    final HttpChunk chunk = new DefaultHttpChunk(copiedBuffer(SOME_RESPONSE_TEXT + uri, UTF_8));
-                    ctx.sendDownstream(new OrderedDownstreamChannelEvent(oue, subSequence, false, chunk));
-
-                    timer.newTimeout(new ChunkWriter(ctx, e, uri, oue, subSequence + 1), 0, MILLISECONDS);
-
-                    if (uri.equals(PATH2)) {
-                        sendFinalChunk.set(true);
-                    }
-                }
-            }
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/http/netty/pipelining/HttpPipeliningHandlerTests.java b/core/src/test/java/org/elasticsearch/http/netty/pipelining/HttpPipeliningHandlerTests.java
new file mode 100644
index 0000000..f21153e
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/http/netty/pipelining/HttpPipeliningHandlerTests.java
@@ -0,0 +1,217 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.http.netty.pipelining;
+
+import org.elasticsearch.common.network.NetworkAddress;
+import org.elasticsearch.test.ESTestCase;
+import org.jboss.netty.bootstrap.ClientBootstrap;
+import org.jboss.netty.bootstrap.ServerBootstrap;
+import org.jboss.netty.channel.*;
+import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory;
+import org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory;
+import org.jboss.netty.handler.codec.http.*;
+import org.jboss.netty.util.HashedWheelTimer;
+import org.jboss.netty.util.Timeout;
+import org.jboss.netty.util.TimerTask;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import java.net.InetAddress;
+import java.net.InetSocketAddress;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+import static java.util.concurrent.TimeUnit.MILLISECONDS;
+import static org.jboss.netty.buffer.ChannelBuffers.EMPTY_BUFFER;
+import static org.jboss.netty.buffer.ChannelBuffers.copiedBuffer;
+import static org.jboss.netty.handler.codec.http.HttpHeaders.Names.*;
+import static org.jboss.netty.handler.codec.http.HttpHeaders.Values.CHUNKED;
+import static org.jboss.netty.handler.codec.http.HttpHeaders.Values.KEEP_ALIVE;
+import static org.jboss.netty.handler.codec.http.HttpResponseStatus.OK;
+import static org.jboss.netty.handler.codec.http.HttpVersion.HTTP_1_1;
+import static org.jboss.netty.util.CharsetUtil.UTF_8;
+
+/**
+ *
+ */
+public class HttpPipeliningHandlerTests extends ESTestCase {
+
+    private static final long RESPONSE_TIMEOUT = 10000L;
+    private static final long CONNECTION_TIMEOUT = 10000L;
+    private static final String CONTENT_TYPE_TEXT = "text/plain; charset=UTF-8";
+    // TODO make me random
+    private static final InetSocketAddress HOST_ADDR = new InetSocketAddress(InetAddress.getLoopbackAddress(), 9080);
+    private static final String PATH1 = "/1";
+    private static final String PATH2 = "/2";
+    private static final String SOME_RESPONSE_TEXT = "some response for ";
+
+    private ClientBootstrap clientBootstrap;
+    private ServerBootstrap serverBootstrap;
+
+    private CountDownLatch responsesIn;
+    private final List<String> responses = new ArrayList<>(2);
+
+    private HashedWheelTimer timer;
+
+    @Before
+    public void startBootstraps() {
+        clientBootstrap = new ClientBootstrap(new NioClientSocketChannelFactory());
+
+        clientBootstrap.setPipelineFactory(new ChannelPipelineFactory() {
+            @Override
+            public ChannelPipeline getPipeline() throws Exception {
+                return Channels.pipeline(
+                        new HttpClientCodec(),
+                        new ClientHandler()
+                );
+            }
+        });
+
+        serverBootstrap = new ServerBootstrap(new NioServerSocketChannelFactory());
+
+        serverBootstrap.setPipelineFactory(new ChannelPipelineFactory() {
+            @Override
+            public ChannelPipeline getPipeline() throws Exception {
+                return Channels.pipeline(
+                        new HttpRequestDecoder(),
+                        new HttpResponseEncoder(),
+                        new HttpPipeliningHandler(10000),
+                        new ServerHandler()
+                );
+            }
+        });
+
+        serverBootstrap.bind(HOST_ADDR);
+
+        timer = new HashedWheelTimer();
+    }
+
+    @After
+    public void releaseResources() {
+        timer.stop();
+
+        serverBootstrap.shutdown();
+        serverBootstrap.releaseExternalResources();
+        clientBootstrap.shutdown();
+        clientBootstrap.releaseExternalResources();
+    }
+
+    @Test
+    public void shouldReturnMessagesInOrder() throws InterruptedException {
+        responsesIn = new CountDownLatch(1);
+        responses.clear();
+
+        final ChannelFuture connectionFuture = clientBootstrap.connect(HOST_ADDR);
+
+        assertTrue(connectionFuture.await(CONNECTION_TIMEOUT));
+        final Channel clientChannel = connectionFuture.getChannel();
+
+        // NetworkAddress.formatAddress makes a proper HOST header.
+        final HttpRequest request1 = new DefaultHttpRequest(
+                HTTP_1_1, HttpMethod.GET, PATH1);
+        request1.headers().add(HOST, NetworkAddress.formatAddress(HOST_ADDR));
+
+        final HttpRequest request2 = new DefaultHttpRequest(
+                HTTP_1_1, HttpMethod.GET, PATH2);
+        request2.headers().add(HOST, NetworkAddress.formatAddress(HOST_ADDR));
+
+        clientChannel.write(request1);
+        clientChannel.write(request2);
+
+        responsesIn.await(RESPONSE_TIMEOUT, MILLISECONDS);
+
+        assertTrue(responses.contains(SOME_RESPONSE_TEXT + PATH1));
+        assertTrue(responses.contains(SOME_RESPONSE_TEXT + PATH2));
+    }
+
+    public class ClientHandler extends SimpleChannelUpstreamHandler {
+        @Override
+        public void messageReceived(final ChannelHandlerContext ctx, final MessageEvent e) {
+            final Object message = e.getMessage();
+            if (message instanceof HttpChunk) {
+                final HttpChunk response = (HttpChunk) e.getMessage();
+                if (!response.isLast()) {
+                    final String content = response.getContent().toString(UTF_8);
+                    responses.add(content);
+                    if (content.equals(SOME_RESPONSE_TEXT + PATH2)) {
+                        responsesIn.countDown();
+                    }
+                }
+            }
+        }
+    }
+
+    public class ServerHandler extends SimpleChannelUpstreamHandler {
+        private final AtomicBoolean sendFinalChunk = new AtomicBoolean(false);
+
+        @Override
+        public void messageReceived(final ChannelHandlerContext ctx, final MessageEvent e) throws InterruptedException {
+            final HttpRequest request = (HttpRequest) e.getMessage();
+
+            final OrderedUpstreamMessageEvent oue = (OrderedUpstreamMessageEvent) e;
+            final String uri = request.getUri();
+
+            final HttpResponse initialChunk = new DefaultHttpResponse(HTTP_1_1, OK);
+            initialChunk.headers().add(CONTENT_TYPE, CONTENT_TYPE_TEXT);
+            initialChunk.headers().add(CONNECTION, KEEP_ALIVE);
+            initialChunk.headers().add(TRANSFER_ENCODING, CHUNKED);
+
+            ctx.sendDownstream(new OrderedDownstreamChannelEvent(oue, 0, false, initialChunk));
+
+            timer.newTimeout(new ChunkWriter(ctx, e, uri, oue, 1), 0, MILLISECONDS);
+        }
+
+        private class ChunkWriter implements TimerTask {
+            private final ChannelHandlerContext ctx;
+            private final MessageEvent e;
+            private final String uri;
+            private final OrderedUpstreamMessageEvent oue;
+            private final int subSequence;
+
+            public ChunkWriter(final ChannelHandlerContext ctx, final MessageEvent e, final String uri,
+                               final OrderedUpstreamMessageEvent oue, final int subSequence) {
+                this.ctx = ctx;
+                this.e = e;
+                this.uri = uri;
+                this.oue = oue;
+                this.subSequence = subSequence;
+            }
+
+            @Override
+            public void run(final Timeout timeout) {
+                if (sendFinalChunk.get() && subSequence > 1) {
+                    final HttpChunk finalChunk = new DefaultHttpChunk(EMPTY_BUFFER);
+                    ctx.sendDownstream(new OrderedDownstreamChannelEvent(oue, subSequence, true, finalChunk));
+                } else {
+                    final HttpChunk chunk = new DefaultHttpChunk(copiedBuffer(SOME_RESPONSE_TEXT + uri, UTF_8));
+                    ctx.sendDownstream(new OrderedDownstreamChannelEvent(oue, subSequence, false, chunk));
+
+                    timer.newTimeout(new ChunkWriter(ctx, e, uri, oue, subSequence + 1), 0, MILLISECONDS);
+
+                    if (uri.equals(PATH2)) {
+                        sendFinalChunk.set(true);
+                    }
+                }
+            }
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/PatternAnalyzerTest.java b/core/src/test/java/org/elasticsearch/index/analysis/PatternAnalyzerTest.java
deleted file mode 100644
index 2f420e5..0000000
--- a/core/src/test/java/org/elasticsearch/index/analysis/PatternAnalyzerTest.java
+++ /dev/null
@@ -1,154 +0,0 @@
-package org.elasticsearch.index.analysis;
-
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-import java.io.IOException;
-import java.lang.Thread.UncaughtExceptionHandler;
-import java.util.Arrays;
-import java.util.regex.Pattern;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.core.StopAnalyzer;
-import org.elasticsearch.test.ESTokenStreamTestCase;
-
-/**
- * Verifies the behavior of PatternAnalyzer.
- */
-public class PatternAnalyzerTest extends ESTokenStreamTestCase {
-
-  /**
-   * Test PatternAnalyzer when it is configured with a non-word pattern.
-   */
-  public void testNonWordPattern() throws IOException {
-    // Split on non-letter pattern, do not lowercase, no stopwords
-    PatternAnalyzer a = new PatternAnalyzer(Pattern.compile("\\W+"), false, null);
-    assertAnalyzesTo(a, "The quick brown Fox,the abcd1234 (56.78) dc.", 
-                        new String[] { "The", "quick", "brown", "Fox", "the", "abcd1234", "56", "78", "dc" });
-
-    // split on non-letter pattern, lowercase, english stopwords
-    PatternAnalyzer b = new PatternAnalyzer(Pattern.compile("\\W+"), true, 
-                                            StopAnalyzer.ENGLISH_STOP_WORDS_SET);
-    assertAnalyzesTo(b, "The quick brown Fox,the abcd1234 (56.78) dc.", 
-                         new String[] { "quick", "brown", "fox", "abcd1234", "56", "78", "dc" });
-  }
-
-  /**
-   * Test PatternAnalyzer when it is configured with a whitespace pattern.
-   * Behavior can be similar to WhitespaceAnalyzer (depending upon options)
-   */
-  public void testWhitespacePattern() throws IOException {
-    // Split on whitespace patterns, do not lowercase, no stopwords
-    PatternAnalyzer a = new PatternAnalyzer(Pattern.compile("\\s+"), false, null);
-    assertAnalyzesTo(a, "The quick brown Fox,the abcd1234 (56.78) dc.", 
-                        new String[] { "The", "quick", "brown", "Fox,the", "abcd1234", "(56.78)", "dc." });
-
-    // Split on whitespace patterns, lowercase, english stopwords
-    PatternAnalyzer b = new PatternAnalyzer(Pattern.compile("\\s+"), true, 
-                                            StopAnalyzer.ENGLISH_STOP_WORDS_SET);
-    assertAnalyzesTo(b, "The quick brown Fox,the abcd1234 (56.78) dc.", 
-                         new String[] { "quick", "brown", "fox,the", "abcd1234", "(56.78)", "dc." });
-  }
-
-  /**
-   * Test PatternAnalyzer when it is configured with a custom pattern. In this
-   * case, text is tokenized on the comma ","
-   */
-  public void testCustomPattern() throws IOException {
-    // Split on comma, do not lowercase, no stopwords
-    PatternAnalyzer a = new PatternAnalyzer(Pattern.compile(","), false, null);
-    assertAnalyzesTo(a, "Here,Are,some,Comma,separated,words,", 
-                         new String[] { "Here", "Are", "some", "Comma", "separated", "words" });
-
-    // split on comma, lowercase, english stopwords
-    PatternAnalyzer b = new PatternAnalyzer(Pattern.compile(","), true,
-                                             StopAnalyzer.ENGLISH_STOP_WORDS_SET);
-    assertAnalyzesTo(b, "Here,Are,some,Comma,separated,words,", 
-                         new String[] { "here", "some", "comma", "separated", "words" });
-  }
-
-  /**
-   * Test PatternAnalyzer against a large document.
-   */
-  public void testHugeDocument() throws IOException {
-    StringBuilder document = new StringBuilder();
-    // 5000 a's
-    char largeWord[] = new char[5000];
-    Arrays.fill(largeWord, 'a');
-    document.append(largeWord);
-
-    // a space
-    document.append(' ');
-
-    // 2000 b's
-    char largeWord2[] = new char[2000];
-    Arrays.fill(largeWord2, 'b');
-    document.append(largeWord2);
-
-    // Split on whitespace patterns, do not lowercase, no stopwords
-    PatternAnalyzer a = new PatternAnalyzer(Pattern.compile("\\s+"), false, null);
-    assertAnalyzesTo(a, document.toString(), 
-                         new String[] { new String(largeWord), new String(largeWord2) });
-  }
-  
-  /** blast some random strings through the analyzer */
-  public void testRandomStrings() throws Exception {
-    Analyzer a = new PatternAnalyzer(Pattern.compile(","), true, StopAnalyzer.ENGLISH_STOP_WORDS_SET);
-    
-    // dodge jre bug http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7104012
-    final UncaughtExceptionHandler savedHandler = Thread.getDefaultUncaughtExceptionHandler();
-    Thread.setDefaultUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {
-      @Override
-      public void uncaughtException(Thread thread, Throwable throwable) {
-        assumeTrue("not failing due to jre bug ", !isJREBug7104012(throwable));
-        // otherwise its some other bug, pass to default handler
-        savedHandler.uncaughtException(thread, throwable);
-      }
-    });
-    
-    try {
-      Thread.getDefaultUncaughtExceptionHandler();
-      checkRandomData(random(), a, 10000*RANDOM_MULTIPLIER);
-    } catch (ArrayIndexOutOfBoundsException ex) {
-      assumeTrue("not failing due to jre bug ", !isJREBug7104012(ex));
-      throw ex; // otherwise rethrow
-    } finally {
-      Thread.setDefaultUncaughtExceptionHandler(savedHandler);
-    }
-  }
-  
-  static boolean isJREBug7104012(Throwable t) {
-    if (!(t instanceof ArrayIndexOutOfBoundsException)) {
-      // BaseTokenStreamTestCase now wraps exc in a new RuntimeException:
-      t = t.getCause();
-      if (!(t instanceof ArrayIndexOutOfBoundsException)) {
-        return false;
-      }
-    }
-    StackTraceElement trace[] = t.getStackTrace();
-    for (StackTraceElement st : trace) {
-      if ("java.text.RuleBasedBreakIterator".equals(st.getClassName()) || 
-          "sun.util.locale.provider.RuleBasedBreakIterator".equals(st.getClassName()) 
-          && "lookupBackwardState".equals(st.getMethodName())) {
-        return true;
-      }
-    }
-    return false;
-  }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/PatternAnalyzerTests.java b/core/src/test/java/org/elasticsearch/index/analysis/PatternAnalyzerTests.java
new file mode 100644
index 0000000..9c578ef
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/analysis/PatternAnalyzerTests.java
@@ -0,0 +1,154 @@
+package org.elasticsearch.index.analysis;
+
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+import java.io.IOException;
+import java.lang.Thread.UncaughtExceptionHandler;
+import java.util.Arrays;
+import java.util.regex.Pattern;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.core.StopAnalyzer;
+import org.elasticsearch.test.ESTokenStreamTestCase;
+
+/**
+ * Verifies the behavior of PatternAnalyzer.
+ */
+public class PatternAnalyzerTests extends ESTokenStreamTestCase {
+
+  /**
+   * Test PatternAnalyzer when it is configured with a non-word pattern.
+   */
+  public void testNonWordPattern() throws IOException {
+    // Split on non-letter pattern, do not lowercase, no stopwords
+    PatternAnalyzer a = new PatternAnalyzer(Pattern.compile("\\W+"), false, null);
+    assertAnalyzesTo(a, "The quick brown Fox,the abcd1234 (56.78) dc.", 
+                        new String[] { "The", "quick", "brown", "Fox", "the", "abcd1234", "56", "78", "dc" });
+
+    // split on non-letter pattern, lowercase, english stopwords
+    PatternAnalyzer b = new PatternAnalyzer(Pattern.compile("\\W+"), true, 
+                                            StopAnalyzer.ENGLISH_STOP_WORDS_SET);
+    assertAnalyzesTo(b, "The quick brown Fox,the abcd1234 (56.78) dc.", 
+                         new String[] { "quick", "brown", "fox", "abcd1234", "56", "78", "dc" });
+  }
+
+  /**
+   * Test PatternAnalyzer when it is configured with a whitespace pattern.
+   * Behavior can be similar to WhitespaceAnalyzer (depending upon options)
+   */
+  public void testWhitespacePattern() throws IOException {
+    // Split on whitespace patterns, do not lowercase, no stopwords
+    PatternAnalyzer a = new PatternAnalyzer(Pattern.compile("\\s+"), false, null);
+    assertAnalyzesTo(a, "The quick brown Fox,the abcd1234 (56.78) dc.", 
+                        new String[] { "The", "quick", "brown", "Fox,the", "abcd1234", "(56.78)", "dc." });
+
+    // Split on whitespace patterns, lowercase, english stopwords
+    PatternAnalyzer b = new PatternAnalyzer(Pattern.compile("\\s+"), true, 
+                                            StopAnalyzer.ENGLISH_STOP_WORDS_SET);
+    assertAnalyzesTo(b, "The quick brown Fox,the abcd1234 (56.78) dc.", 
+                         new String[] { "quick", "brown", "fox,the", "abcd1234", "(56.78)", "dc." });
+  }
+
+  /**
+   * Test PatternAnalyzer when it is configured with a custom pattern. In this
+   * case, text is tokenized on the comma ","
+   */
+  public void testCustomPattern() throws IOException {
+    // Split on comma, do not lowercase, no stopwords
+    PatternAnalyzer a = new PatternAnalyzer(Pattern.compile(","), false, null);
+    assertAnalyzesTo(a, "Here,Are,some,Comma,separated,words,", 
+                         new String[] { "Here", "Are", "some", "Comma", "separated", "words" });
+
+    // split on comma, lowercase, english stopwords
+    PatternAnalyzer b = new PatternAnalyzer(Pattern.compile(","), true,
+                                             StopAnalyzer.ENGLISH_STOP_WORDS_SET);
+    assertAnalyzesTo(b, "Here,Are,some,Comma,separated,words,", 
+                         new String[] { "here", "some", "comma", "separated", "words" });
+  }
+
+  /**
+   * Test PatternAnalyzer against a large document.
+   */
+  public void testHugeDocument() throws IOException {
+    StringBuilder document = new StringBuilder();
+    // 5000 a's
+    char largeWord[] = new char[5000];
+    Arrays.fill(largeWord, 'a');
+    document.append(largeWord);
+
+    // a space
+    document.append(' ');
+
+    // 2000 b's
+    char largeWord2[] = new char[2000];
+    Arrays.fill(largeWord2, 'b');
+    document.append(largeWord2);
+
+    // Split on whitespace patterns, do not lowercase, no stopwords
+    PatternAnalyzer a = new PatternAnalyzer(Pattern.compile("\\s+"), false, null);
+    assertAnalyzesTo(a, document.toString(), 
+                         new String[] { new String(largeWord), new String(largeWord2) });
+  }
+  
+  /** blast some random strings through the analyzer */
+  public void testRandomStrings() throws Exception {
+    Analyzer a = new PatternAnalyzer(Pattern.compile(","), true, StopAnalyzer.ENGLISH_STOP_WORDS_SET);
+    
+    // dodge jre bug http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7104012
+    final UncaughtExceptionHandler savedHandler = Thread.getDefaultUncaughtExceptionHandler();
+    Thread.setDefaultUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {
+      @Override
+      public void uncaughtException(Thread thread, Throwable throwable) {
+        assumeTrue("not failing due to jre bug ", !isJREBug7104012(throwable));
+        // otherwise its some other bug, pass to default handler
+        savedHandler.uncaughtException(thread, throwable);
+      }
+    });
+    
+    try {
+      Thread.getDefaultUncaughtExceptionHandler();
+      checkRandomData(random(), a, 10000*RANDOM_MULTIPLIER);
+    } catch (ArrayIndexOutOfBoundsException ex) {
+      assumeTrue("not failing due to jre bug ", !isJREBug7104012(ex));
+      throw ex; // otherwise rethrow
+    } finally {
+      Thread.setDefaultUncaughtExceptionHandler(savedHandler);
+    }
+  }
+  
+  static boolean isJREBug7104012(Throwable t) {
+    if (!(t instanceof ArrayIndexOutOfBoundsException)) {
+      // BaseTokenStreamTestCase now wraps exc in a new RuntimeException:
+      t = t.getCause();
+      if (!(t instanceof ArrayIndexOutOfBoundsException)) {
+        return false;
+      }
+    }
+    StackTraceElement trace[] = t.getStackTrace();
+    for (StackTraceElement st : trace) {
+      if ("java.text.RuleBasedBreakIterator".equals(st.getClassName()) || 
+          "sun.util.locale.provider.RuleBasedBreakIterator".equals(st.getClassName()) 
+          && "lookupBackwardState".equals(st.getMethodName())) {
+        return true;
+      }
+    }
+    return false;
+  }
+}
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/synonyms/SynonymsAnalysisTest.java b/core/src/test/java/org/elasticsearch/index/analysis/synonyms/SynonymsAnalysisTest.java
deleted file mode 100644
index 371a092..0000000
--- a/core/src/test/java/org/elasticsearch/index/analysis/synonyms/SynonymsAnalysisTest.java
+++ /dev/null
@@ -1,120 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.analysis.synonyms;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.inject.Injector;
-import org.elasticsearch.common.inject.ModulesBuilder;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.Loggers;
-import org.elasticsearch.common.lucene.all.AllEntries;
-import org.elasticsearch.common.lucene.all.AllTokenStream;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsModule;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.env.EnvironmentModule;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.IndexNameModule;
-import org.elasticsearch.index.analysis.AnalysisModule;
-import org.elasticsearch.index.analysis.AnalysisService;
-import org.elasticsearch.index.settings.IndexSettingsModule;
-import org.elasticsearch.indices.analysis.IndicesAnalysisService;
-import org.elasticsearch.test.ESTestCase;
-import org.hamcrest.MatcherAssert;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.io.InputStream;
-import java.nio.file.Files;
-import java.nio.file.Path;
-
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.hamcrest.Matchers.equalTo;
-
-/**
- */
-public class SynonymsAnalysisTest extends ESTestCase {
-
-    protected final ESLogger logger = Loggers.getLogger(getClass());
-    private AnalysisService analysisService;
-
-    @Test
-    public void testSynonymsAnalysis() throws IOException {
-        InputStream synonyms = getClass().getResourceAsStream("synonyms.txt");
-        InputStream synonymsWordnet = getClass().getResourceAsStream("synonyms_wordnet.txt");
-        Path home = createTempDir();
-        Path config = home.resolve("config");
-        Files.createDirectory(config);
-        Files.copy(synonyms, config.resolve("synonyms.txt"));
-        Files.copy(synonymsWordnet, config.resolve("synonyms_wordnet.txt"));
-
-        String json = "/org/elasticsearch/index/analysis/synonyms/synonyms.json";
-        Settings settings = settingsBuilder().
-            loadFromStream(json, getClass().getResourceAsStream(json))
-                .put("path.home", home)
-                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).build();
-
-        Index index = new Index("test");
-
-        Injector parentInjector = new ModulesBuilder().add(
-                new SettingsModule(settings),
-                new EnvironmentModule(new Environment(settings)))
-                .createInjector();
-        Injector injector = new ModulesBuilder().add(
-                new IndexSettingsModule(index, settings),
-                new IndexNameModule(index),
-                new AnalysisModule(settings, parentInjector.getInstance(IndicesAnalysisService.class)))
-                .createChildInjector(parentInjector);
-
-        analysisService = injector.getInstance(AnalysisService.class);
-
-        match("synonymAnalyzer", "kimchy is the dude abides", "shay is the elasticsearch man!");
-        match("synonymAnalyzer_file", "kimchy is the dude abides", "shay is the elasticsearch man!");
-        match("synonymAnalyzerWordnet", "abstain", "abstain refrain desist");
-        match("synonymAnalyzerWordnet_file", "abstain", "abstain refrain desist");
-        match("synonymAnalyzerWithsettings", "kimchy", "sha hay");
-
-    }
-
-    private void match(String analyzerName, String source, String target) throws IOException {
-
-        Analyzer analyzer = analysisService.analyzer(analyzerName).analyzer();
-
-        AllEntries allEntries = new AllEntries();
-        allEntries.addText("field", source, 1.0f);
-        allEntries.reset();
-
-        TokenStream stream = AllTokenStream.allTokenStream("_all", allEntries, analyzer);
-        stream.reset();
-        CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);
-
-        StringBuilder sb = new StringBuilder();
-        while (stream.incrementToken()) {
-            sb.append(termAtt.toString()).append(" ");
-        }
-
-        MatcherAssert.assertThat(target, equalTo(sb.toString().trim()));
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/synonyms/SynonymsAnalysisTests.java b/core/src/test/java/org/elasticsearch/index/analysis/synonyms/SynonymsAnalysisTests.java
new file mode 100644
index 0000000..f695b1b
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/analysis/synonyms/SynonymsAnalysisTests.java
@@ -0,0 +1,120 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.analysis.synonyms;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.common.inject.Injector;
+import org.elasticsearch.common.inject.ModulesBuilder;
+import org.elasticsearch.common.logging.ESLogger;
+import org.elasticsearch.common.logging.Loggers;
+import org.elasticsearch.common.lucene.all.AllEntries;
+import org.elasticsearch.common.lucene.all.AllTokenStream;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.settings.SettingsModule;
+import org.elasticsearch.env.Environment;
+import org.elasticsearch.env.EnvironmentModule;
+import org.elasticsearch.index.Index;
+import org.elasticsearch.index.IndexNameModule;
+import org.elasticsearch.index.analysis.AnalysisModule;
+import org.elasticsearch.index.analysis.AnalysisService;
+import org.elasticsearch.index.settings.IndexSettingsModule;
+import org.elasticsearch.indices.analysis.IndicesAnalysisService;
+import org.elasticsearch.test.ESTestCase;
+import org.hamcrest.MatcherAssert;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.nio.file.Files;
+import java.nio.file.Path;
+
+import static org.elasticsearch.common.settings.Settings.settingsBuilder;
+import static org.hamcrest.Matchers.equalTo;
+
+/**
+ */
+public class SynonymsAnalysisTests extends ESTestCase {
+
+    protected final ESLogger logger = Loggers.getLogger(getClass());
+    private AnalysisService analysisService;
+
+    @Test
+    public void testSynonymsAnalysis() throws IOException {
+        InputStream synonyms = getClass().getResourceAsStream("synonyms.txt");
+        InputStream synonymsWordnet = getClass().getResourceAsStream("synonyms_wordnet.txt");
+        Path home = createTempDir();
+        Path config = home.resolve("config");
+        Files.createDirectory(config);
+        Files.copy(synonyms, config.resolve("synonyms.txt"));
+        Files.copy(synonymsWordnet, config.resolve("synonyms_wordnet.txt"));
+
+        String json = "/org/elasticsearch/index/analysis/synonyms/synonyms.json";
+        Settings settings = settingsBuilder().
+            loadFromStream(json, getClass().getResourceAsStream(json))
+                .put("path.home", home)
+                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).build();
+
+        Index index = new Index("test");
+
+        Injector parentInjector = new ModulesBuilder().add(
+                new SettingsModule(settings),
+                new EnvironmentModule(new Environment(settings)))
+                .createInjector();
+        Injector injector = new ModulesBuilder().add(
+                new IndexSettingsModule(index, settings),
+                new IndexNameModule(index),
+                new AnalysisModule(settings, parentInjector.getInstance(IndicesAnalysisService.class)))
+                .createChildInjector(parentInjector);
+
+        analysisService = injector.getInstance(AnalysisService.class);
+
+        match("synonymAnalyzer", "kimchy is the dude abides", "shay is the elasticsearch man!");
+        match("synonymAnalyzer_file", "kimchy is the dude abides", "shay is the elasticsearch man!");
+        match("synonymAnalyzerWordnet", "abstain", "abstain refrain desist");
+        match("synonymAnalyzerWordnet_file", "abstain", "abstain refrain desist");
+        match("synonymAnalyzerWithsettings", "kimchy", "sha hay");
+
+    }
+
+    private void match(String analyzerName, String source, String target) throws IOException {
+
+        Analyzer analyzer = analysisService.analyzer(analyzerName).analyzer();
+
+        AllEntries allEntries = new AllEntries();
+        allEntries.addText("field", source, 1.0f);
+        allEntries.reset();
+
+        TokenStream stream = AllTokenStream.allTokenStream("_all", allEntries, analyzer);
+        stream.reset();
+        CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);
+
+        StringBuilder sb = new StringBuilder();
+        while (stream.incrementToken()) {
+            sb.append(termAtt.toString()).append(" ");
+        }
+
+        MatcherAssert.assertThat(target, equalTo(sb.toString().trim()));
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/index/cache/bitset/BitSetFilterCacheTest.java b/core/src/test/java/org/elasticsearch/index/cache/bitset/BitSetFilterCacheTest.java
deleted file mode 100644
index 389b209..0000000
--- a/core/src/test/java/org/elasticsearch/index/cache/bitset/BitSetFilterCacheTest.java
+++ /dev/null
@@ -1,105 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.cache.bitset;
-
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.LogByteSizeMergePolicy;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.QueryWrapperFilter;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.search.join.BitDocIdSetFilter;
-import org.apache.lucene.store.RAMDirectory;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import static org.hamcrest.Matchers.equalTo;
-
-/**
- */
-public class BitSetFilterCacheTest extends ESTestCase {
-
-    @Test
-    public void testInvalidateEntries() throws Exception {
-        IndexWriter writer = new IndexWriter(
-                new RAMDirectory(),
-                new IndexWriterConfig(new StandardAnalyzer()).setMergePolicy(new LogByteSizeMergePolicy())
-        );
-        Document document = new Document();
-        document.add(new StringField("field", "value", Field.Store.NO));
-        writer.addDocument(document);
-        writer.commit();
-
-        document = new Document();
-        document.add(new StringField("field", "value", Field.Store.NO));
-        writer.addDocument(document);
-        writer.commit();
-
-        document = new Document();
-        document.add(new StringField("field", "value", Field.Store.NO));
-        writer.addDocument(document);
-        writer.commit();
-
-        IndexReader reader = DirectoryReader.open(writer, false);
-        IndexSearcher searcher = new IndexSearcher(reader);
-
-        BitsetFilterCache cache = new BitsetFilterCache(new Index("test"), Settings.EMPTY);
-        BitDocIdSetFilter filter = cache.getBitDocIdSetFilter(new QueryWrapperFilter(new TermQuery(new Term("field", "value"))));
-        TopDocs docs = searcher.search(new ConstantScoreQuery(filter), 1);
-        assertThat(docs.totalHits, equalTo(3));
-
-        // now cached
-        docs = searcher.search(new ConstantScoreQuery(filter), 1);
-        assertThat(docs.totalHits, equalTo(3));
-        // There are 3 segments
-        assertThat(cache.getLoadedFilters().size(), equalTo(3l));
-
-        writer.forceMerge(1);
-        reader.close();
-        reader = DirectoryReader.open(writer, false);
-        searcher = new IndexSearcher(reader);
-
-        docs = searcher.search(new ConstantScoreQuery(filter), 1);
-        assertThat(docs.totalHits, equalTo(3));
-
-        // now cached
-        docs = searcher.search(new ConstantScoreQuery(filter), 1);
-        assertThat(docs.totalHits, equalTo(3));
-        // Only one segment now, so the size must be 1
-        assertThat(cache.getLoadedFilters().size(), equalTo(1l));
-
-        reader.close();
-        writer.close();
-        // There is no reference from readers and writer to any segment in the test index, so the size in the fbs cache must be 0
-        assertThat(cache.getLoadedFilters().size(), equalTo(0l));
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/index/cache/bitset/BitSetFilterCacheTests.java b/core/src/test/java/org/elasticsearch/index/cache/bitset/BitSetFilterCacheTests.java
new file mode 100644
index 0000000..7888704
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/cache/bitset/BitSetFilterCacheTests.java
@@ -0,0 +1,105 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.cache.bitset;
+
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.LogByteSizeMergePolicy;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.QueryWrapperFilter;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.search.join.BitDocIdSetFilter;
+import org.apache.lucene.store.RAMDirectory;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.Index;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import static org.hamcrest.Matchers.equalTo;
+
+/**
+ */
+public class BitSetFilterCacheTests extends ESTestCase {
+
+    @Test
+    public void testInvalidateEntries() throws Exception {
+        IndexWriter writer = new IndexWriter(
+                new RAMDirectory(),
+                new IndexWriterConfig(new StandardAnalyzer()).setMergePolicy(new LogByteSizeMergePolicy())
+        );
+        Document document = new Document();
+        document.add(new StringField("field", "value", Field.Store.NO));
+        writer.addDocument(document);
+        writer.commit();
+
+        document = new Document();
+        document.add(new StringField("field", "value", Field.Store.NO));
+        writer.addDocument(document);
+        writer.commit();
+
+        document = new Document();
+        document.add(new StringField("field", "value", Field.Store.NO));
+        writer.addDocument(document);
+        writer.commit();
+
+        IndexReader reader = DirectoryReader.open(writer, false);
+        IndexSearcher searcher = new IndexSearcher(reader);
+
+        BitsetFilterCache cache = new BitsetFilterCache(new Index("test"), Settings.EMPTY);
+        BitDocIdSetFilter filter = cache.getBitDocIdSetFilter(new QueryWrapperFilter(new TermQuery(new Term("field", "value"))));
+        TopDocs docs = searcher.search(new ConstantScoreQuery(filter), 1);
+        assertThat(docs.totalHits, equalTo(3));
+
+        // now cached
+        docs = searcher.search(new ConstantScoreQuery(filter), 1);
+        assertThat(docs.totalHits, equalTo(3));
+        // There are 3 segments
+        assertThat(cache.getLoadedFilters().size(), equalTo(3l));
+
+        writer.forceMerge(1);
+        reader.close();
+        reader = DirectoryReader.open(writer, false);
+        searcher = new IndexSearcher(reader);
+
+        docs = searcher.search(new ConstantScoreQuery(filter), 1);
+        assertThat(docs.totalHits, equalTo(3));
+
+        // now cached
+        docs = searcher.search(new ConstantScoreQuery(filter), 1);
+        assertThat(docs.totalHits, equalTo(3));
+        // Only one segment now, so the size must be 1
+        assertThat(cache.getLoadedFilters().size(), equalTo(1l));
+
+        reader.close();
+        writer.close();
+        // There is no reference from readers and writer to any segment in the test index, so the size in the fbs cache must be 0
+        assertThat(cache.getLoadedFilters().size(), equalTo(0l));
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/index/codec/postingformat/PostingsFormatTest.java b/core/src/test/java/org/elasticsearch/index/codec/postingformat/PostingsFormatTest.java
deleted file mode 100644
index efaa557..0000000
--- a/core/src/test/java/org/elasticsearch/index/codec/postingformat/PostingsFormatTest.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.codec.postingformat;
-
-import com.carrotsearch.randomizedtesting.annotations.Listeners;
-import com.carrotsearch.randomizedtesting.annotations.TimeoutSuite;
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BasePostingsFormatTestCase;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.TestUtil;
-import org.apache.lucene.util.TimeUnits;
-import org.elasticsearch.test.junit.listeners.ReproduceInfoPrinter;
-
-/** Runs elasticsearch postings format against lucene's standard postings format tests */
-@Listeners({
-        ReproduceInfoPrinter.class
-})
-@TimeoutSuite(millis = TimeUnits.HOUR)
-@LuceneTestCase.SuppressSysoutChecks(bugUrl = "we log a lot on purpose")
-public class PostingsFormatTest extends BasePostingsFormatTestCase {
-
-    @Override
-    protected Codec getCodec() {
-        return TestUtil.alwaysPostingsFormat(new Elasticsearch090RWPostingsFormat());
-    }
-    
-}
diff --git a/core/src/test/java/org/elasticsearch/index/codec/postingformat/PostingsFormatTests.java b/core/src/test/java/org/elasticsearch/index/codec/postingformat/PostingsFormatTests.java
new file mode 100644
index 0000000..f988445
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/codec/postingformat/PostingsFormatTests.java
@@ -0,0 +1,44 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.codec.postingformat;
+
+import com.carrotsearch.randomizedtesting.annotations.Listeners;
+import com.carrotsearch.randomizedtesting.annotations.TimeoutSuite;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.BasePostingsFormatTestCase;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.TestUtil;
+import org.apache.lucene.util.TimeUnits;
+import org.elasticsearch.test.junit.listeners.ReproduceInfoPrinter;
+
+/** Runs elasticsearch postings format against lucene's standard postings format tests */
+@Listeners({
+        ReproduceInfoPrinter.class
+})
+@TimeoutSuite(millis = TimeUnits.HOUR)
+@LuceneTestCase.SuppressSysoutChecks(bugUrl = "we log a lot on purpose")
+public class PostingsFormatTests extends BasePostingsFormatTestCase {
+
+    @Override
+    protected Codec getCodec() {
+        return TestUtil.alwaysPostingsFormat(new Elasticsearch090RWPostingsFormat());
+    }
+    
+}
diff --git a/core/src/test/java/org/elasticsearch/index/engine/InternalEngineSettingsTest.java b/core/src/test/java/org/elasticsearch/index/engine/InternalEngineSettingsTest.java
deleted file mode 100644
index 21211fe..0000000
--- a/core/src/test/java/org/elasticsearch/index/engine/InternalEngineSettingsTest.java
+++ /dev/null
@@ -1,142 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.engine;
-
-import org.apache.lucene.index.LiveIndexWriterConfig;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.IndexService;
-import org.elasticsearch.test.ESSingleNodeTestCase;
-
-import java.util.concurrent.TimeUnit;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.is;
-
-public class InternalEngineSettingsTest extends ESSingleNodeTestCase {
-
-    public void testSettingsUpdate() {
-        final IndexService service = createIndex("foo");
-        // INDEX_COMPOUND_ON_FLUSH
-        InternalEngine engine = ((InternalEngine)engine(service));
-        assertThat(engine.getCurrentIndexWriterConfig().getUseCompoundFile(), is(true));
-        client().admin().indices().prepareUpdateSettings("foo").setSettings(Settings.builder().put(EngineConfig.INDEX_COMPOUND_ON_FLUSH, false).build()).get();
-        assertThat(engine.getCurrentIndexWriterConfig().getUseCompoundFile(), is(false));
-        client().admin().indices().prepareUpdateSettings("foo").setSettings(Settings.builder().put(EngineConfig.INDEX_COMPOUND_ON_FLUSH, true).build()).get();
-        assertThat(engine.getCurrentIndexWriterConfig().getUseCompoundFile(), is(true));
-
-
-        // VERSION MAP SIZE
-        long indexBufferSize = engine.config().getIndexingBufferSize().bytes();
-        long versionMapSize = engine.config().getVersionMapSize().bytes();
-        assertThat(versionMapSize, equalTo((long) (indexBufferSize * 0.25)));
-
-        final int iters = between(1, 20);
-        for (int i = 0; i < iters; i++) {
-            boolean compoundOnFlush = randomBoolean();
-
-            // Tricky: TimeValue.parseTimeValue casts this long to a double, which steals 11 of the 64 bits for exponent, so we can't use
-            // the full long range here else the assert below fails:
-            long gcDeletes = random().nextLong() & (Long.MAX_VALUE >> 11);
-
-            boolean versionMapAsPercent = randomBoolean();
-            double versionMapPercent = randomIntBetween(0, 100);
-            long versionMapSizeInMB = randomIntBetween(10, 20);
-            String versionMapString = versionMapAsPercent ? versionMapPercent + "%" : versionMapSizeInMB + "mb";
-
-            Settings build = Settings.builder()
-                    .put(EngineConfig.INDEX_COMPOUND_ON_FLUSH, compoundOnFlush)
-                    .put(EngineConfig.INDEX_GC_DELETES_SETTING, gcDeletes, TimeUnit.MILLISECONDS)
-                    .put(EngineConfig.INDEX_VERSION_MAP_SIZE, versionMapString)
-                    .build();
-            assertEquals(gcDeletes, build.getAsTime(EngineConfig.INDEX_GC_DELETES_SETTING, null).millis());
-
-            client().admin().indices().prepareUpdateSettings("foo").setSettings(build).get();
-            LiveIndexWriterConfig currentIndexWriterConfig = engine.getCurrentIndexWriterConfig();
-            assertEquals(engine.config().isCompoundOnFlush(), compoundOnFlush);
-            assertEquals(currentIndexWriterConfig.getUseCompoundFile(), compoundOnFlush);
-
-
-            assertEquals(engine.config().getGcDeletesInMillis(), gcDeletes);
-            assertEquals(engine.getGcDeletesInMillis(), gcDeletes);
-
-            indexBufferSize = engine.config().getIndexingBufferSize().bytes();
-            versionMapSize = engine.config().getVersionMapSize().bytes();
-            if (versionMapAsPercent) {
-                assertThat(versionMapSize, equalTo((long) (indexBufferSize * (versionMapPercent / 100))));
-            } else {
-                assertThat(versionMapSize, equalTo(1024 * 1024 * versionMapSizeInMB));
-            }
-        }
-
-        Settings settings = Settings.builder()
-                .put(EngineConfig.INDEX_GC_DELETES_SETTING, 1000, TimeUnit.MILLISECONDS)
-                .build();
-        client().admin().indices().prepareUpdateSettings("foo").setSettings(settings).get();
-        assertEquals(engine.getGcDeletesInMillis(), 1000);
-        assertTrue(engine.config().isEnableGcDeletes());
-
-
-        settings = Settings.builder()
-                .put(EngineConfig.INDEX_GC_DELETES_SETTING, "0ms")
-                .build();
-
-        client().admin().indices().prepareUpdateSettings("foo").setSettings(settings).get();
-        assertEquals(engine.getGcDeletesInMillis(), 0);
-        assertTrue(engine.config().isEnableGcDeletes());
-
-        settings = Settings.builder()
-                .put(EngineConfig.INDEX_GC_DELETES_SETTING, 1000, TimeUnit.MILLISECONDS)
-                .build();
-        client().admin().indices().prepareUpdateSettings("foo").setSettings(settings).get();
-        assertEquals(engine.getGcDeletesInMillis(), 1000);
-        assertTrue(engine.config().isEnableGcDeletes());
-
-        settings = Settings.builder()
-                .put(EngineConfig.INDEX_VERSION_MAP_SIZE, "sdfasfd")
-                .build();
-        try {
-            client().admin().indices().prepareUpdateSettings("foo").setSettings(settings).get();
-            fail("settings update didn't fail, but should have");
-        } catch (IllegalArgumentException e) {
-            // good
-        }
-
-        settings = Settings.builder()
-                .put(EngineConfig.INDEX_VERSION_MAP_SIZE, "-12%")
-                .build();
-        try {
-            client().admin().indices().prepareUpdateSettings("foo").setSettings(settings).get();
-            fail("settings update didn't fail, but should have");
-        } catch (IllegalArgumentException e) {
-            // good
-        }
-
-        settings = Settings.builder()
-                .put(EngineConfig.INDEX_VERSION_MAP_SIZE, "130%")
-                .build();
-        try {
-            client().admin().indices().prepareUpdateSettings("foo").setSettings(settings).get();
-            fail("settings update didn't fail, but should have");
-        } catch (IllegalArgumentException e) {
-            // good
-        }
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/index/engine/InternalEngineSettingsTests.java b/core/src/test/java/org/elasticsearch/index/engine/InternalEngineSettingsTests.java
new file mode 100644
index 0000000..fa5db4c
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/engine/InternalEngineSettingsTests.java
@@ -0,0 +1,142 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.index.engine;
+
+import org.apache.lucene.index.LiveIndexWriterConfig;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.IndexService;
+import org.elasticsearch.test.ESSingleNodeTestCase;
+
+import java.util.concurrent.TimeUnit;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.is;
+
+public class InternalEngineSettingsTests extends ESSingleNodeTestCase {
+
+    public void testSettingsUpdate() {
+        final IndexService service = createIndex("foo");
+        // INDEX_COMPOUND_ON_FLUSH
+        InternalEngine engine = ((InternalEngine)engine(service));
+        assertThat(engine.getCurrentIndexWriterConfig().getUseCompoundFile(), is(true));
+        client().admin().indices().prepareUpdateSettings("foo").setSettings(Settings.builder().put(EngineConfig.INDEX_COMPOUND_ON_FLUSH, false).build()).get();
+        assertThat(engine.getCurrentIndexWriterConfig().getUseCompoundFile(), is(false));
+        client().admin().indices().prepareUpdateSettings("foo").setSettings(Settings.builder().put(EngineConfig.INDEX_COMPOUND_ON_FLUSH, true).build()).get();
+        assertThat(engine.getCurrentIndexWriterConfig().getUseCompoundFile(), is(true));
+
+
+        // VERSION MAP SIZE
+        long indexBufferSize = engine.config().getIndexingBufferSize().bytes();
+        long versionMapSize = engine.config().getVersionMapSize().bytes();
+        assertThat(versionMapSize, equalTo((long) (indexBufferSize * 0.25)));
+
+        final int iters = between(1, 20);
+        for (int i = 0; i < iters; i++) {
+            boolean compoundOnFlush = randomBoolean();
+
+            // Tricky: TimeValue.parseTimeValue casts this long to a double, which steals 11 of the 64 bits for exponent, so we can't use
+            // the full long range here else the assert below fails:
+            long gcDeletes = random().nextLong() & (Long.MAX_VALUE >> 11);
+
+            boolean versionMapAsPercent = randomBoolean();
+            double versionMapPercent = randomIntBetween(0, 100);
+            long versionMapSizeInMB = randomIntBetween(10, 20);
+            String versionMapString = versionMapAsPercent ? versionMapPercent + "%" : versionMapSizeInMB + "mb";
+
+            Settings build = Settings.builder()
+                    .put(EngineConfig.INDEX_COMPOUND_ON_FLUSH, compoundOnFlush)
+                    .put(EngineConfig.INDEX_GC_DELETES_SETTING, gcDeletes, TimeUnit.MILLISECONDS)
+                    .put(EngineConfig.INDEX_VERSION_MAP_SIZE, versionMapString)
+                    .build();
+            assertEquals(gcDeletes, build.getAsTime(EngineConfig.INDEX_GC_DELETES_SETTING, null).millis());
+
+            client().admin().indices().prepareUpdateSettings("foo").setSettings(build).get();
+            LiveIndexWriterConfig currentIndexWriterConfig = engine.getCurrentIndexWriterConfig();
+            assertEquals(engine.config().isCompoundOnFlush(), compoundOnFlush);
+            assertEquals(currentIndexWriterConfig.getUseCompoundFile(), compoundOnFlush);
+
+
+            assertEquals(engine.config().getGcDeletesInMillis(), gcDeletes);
+            assertEquals(engine.getGcDeletesInMillis(), gcDeletes);
+
+            indexBufferSize = engine.config().getIndexingBufferSize().bytes();
+            versionMapSize = engine.config().getVersionMapSize().bytes();
+            if (versionMapAsPercent) {
+                assertThat(versionMapSize, equalTo((long) (indexBufferSize * (versionMapPercent / 100))));
+            } else {
+                assertThat(versionMapSize, equalTo(1024 * 1024 * versionMapSizeInMB));
+            }
+        }
+
+        Settings settings = Settings.builder()
+                .put(EngineConfig.INDEX_GC_DELETES_SETTING, 1000, TimeUnit.MILLISECONDS)
+                .build();
+        client().admin().indices().prepareUpdateSettings("foo").setSettings(settings).get();
+        assertEquals(engine.getGcDeletesInMillis(), 1000);
+        assertTrue(engine.config().isEnableGcDeletes());
+
+
+        settings = Settings.builder()
+                .put(EngineConfig.INDEX_GC_DELETES_SETTING, "0ms")
+                .build();
+
+        client().admin().indices().prepareUpdateSettings("foo").setSettings(settings).get();
+        assertEquals(engine.getGcDeletesInMillis(), 0);
+        assertTrue(engine.config().isEnableGcDeletes());
+
+        settings = Settings.builder()
+                .put(EngineConfig.INDEX_GC_DELETES_SETTING, 1000, TimeUnit.MILLISECONDS)
+                .build();
+        client().admin().indices().prepareUpdateSettings("foo").setSettings(settings).get();
+        assertEquals(engine.getGcDeletesInMillis(), 1000);
+        assertTrue(engine.config().isEnableGcDeletes());
+
+        settings = Settings.builder()
+                .put(EngineConfig.INDEX_VERSION_MAP_SIZE, "sdfasfd")
+                .build();
+        try {
+            client().admin().indices().prepareUpdateSettings("foo").setSettings(settings).get();
+            fail("settings update didn't fail, but should have");
+        } catch (IllegalArgumentException e) {
+            // good
+        }
+
+        settings = Settings.builder()
+                .put(EngineConfig.INDEX_VERSION_MAP_SIZE, "-12%")
+                .build();
+        try {
+            client().admin().indices().prepareUpdateSettings("foo").setSettings(settings).get();
+            fail("settings update didn't fail, but should have");
+        } catch (IllegalArgumentException e) {
+            // good
+        }
+
+        settings = Settings.builder()
+                .put(EngineConfig.INDEX_VERSION_MAP_SIZE, "130%")
+                .build();
+        try {
+            client().admin().indices().prepareUpdateSettings("foo").setSettings(settings).get();
+            fail("settings update didn't fail, but should have");
+        } catch (IllegalArgumentException e) {
+            // good
+        }
+    }
+
+
+}
diff --git a/core/src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataImplTestCase.java b/core/src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataImplTestCase.java
new file mode 100644
index 0000000..a0f51a7
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataImplTestCase.java
@@ -0,0 +1,277 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.fielddata;
+
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.search.*;
+import org.apache.lucene.util.BytesRef;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.search.MultiValueMode;
+import org.junit.Test;
+
+import static org.hamcrest.Matchers.*;
+
+public abstract class AbstractFieldDataImplTestCase extends AbstractFieldDataTestCase {
+
+    protected String one() {
+        return "1";
+    }
+
+    protected String two() {
+        return "2";
+    }
+
+    protected String three() {
+        return "3";
+    }
+
+    protected String four() {
+        return "4";
+    }
+
+    protected String toString(Object value) {
+        if (value instanceof BytesRef) {
+            return ((BytesRef) value).utf8ToString();
+        }
+        return value.toString();
+    }
+
+    protected abstract void fillSingleValueAllSet() throws Exception;
+
+    protected abstract void add2SingleValuedDocumentsAndDeleteOneOfThem() throws Exception;
+
+    protected long minRamBytesUsed() {
+        // minimum number of bytes that this fielddata instance is expected to require
+        return 1;
+    }
+
+    @Test
+    public void testDeletedDocs() throws Exception {
+        add2SingleValuedDocumentsAndDeleteOneOfThem();
+        IndexFieldData indexFieldData = getForField("value");
+        LeafReaderContext readerContext = refreshReader();
+        AtomicFieldData fieldData = indexFieldData.load(readerContext);
+        SortedBinaryDocValues values = fieldData.getBytesValues();
+        for (int i = 0; i < readerContext.reader().maxDoc(); ++i) {
+            values.setDocument(i);
+            assertThat(values.count(), greaterThanOrEqualTo(1));
+        }
+    }
+
+    @Test
+    public void testSingleValueAllSet() throws Exception {
+        fillSingleValueAllSet();
+        IndexFieldData indexFieldData = getForField("value");
+        LeafReaderContext readerContext = refreshReader();
+        AtomicFieldData fieldData = indexFieldData.load(readerContext);
+        assertThat(fieldData.ramBytesUsed(), greaterThanOrEqualTo(minRamBytesUsed()));
+
+        SortedBinaryDocValues bytesValues = fieldData.getBytesValues();
+
+        bytesValues.setDocument(0);
+        assertThat(bytesValues.count(), equalTo(1));
+        assertThat(bytesValues.valueAt(0), equalTo(new BytesRef(two())));
+        bytesValues.setDocument(1);
+        assertThat(bytesValues.count(), equalTo(1));
+        assertThat(bytesValues.valueAt(0), equalTo(new BytesRef(one())));
+        bytesValues.setDocument(2);
+        assertThat(bytesValues.count(), equalTo(1));
+        assertThat(bytesValues.valueAt(0), equalTo(new BytesRef(three())));
+
+        assertValues(bytesValues, 0, two());
+        assertValues(bytesValues, 1, one());
+        assertValues(bytesValues, 2, three());
+
+        IndexSearcher searcher = new IndexSearcher(readerContext.reader());
+        TopFieldDocs topDocs;
+
+        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
+                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.MIN, null))));
+        assertThat(topDocs.totalHits, equalTo(3));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(1));
+        assertThat(toString(((FieldDoc) topDocs.scoreDocs[0]).fields[0]), equalTo(one()));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(0));
+        assertThat(toString(((FieldDoc) topDocs.scoreDocs[1]).fields[0]), equalTo(two()));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(2));
+        assertThat(toString(((FieldDoc) topDocs.scoreDocs[2]).fields[0]), equalTo(three()));
+
+        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
+                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.MAX, null), true)));
+        assertThat(topDocs.totalHits, equalTo(3));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(2));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(0));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(1));
+    }
+
+    protected abstract void fillSingleValueWithMissing() throws Exception;
+    
+    public void assertValues(SortedBinaryDocValues values, int docId, BytesRef... actualValues) {
+        values.setDocument(docId);
+        assertThat(values.count(), equalTo(actualValues.length));
+        for (int i = 0; i < actualValues.length; i++) {
+            assertThat(values.valueAt(i), equalTo(actualValues[i]));
+        }
+    }
+    
+    public void assertValues(SortedBinaryDocValues values, int docId, String... actualValues) {
+        values.setDocument(docId);
+        assertThat(values.count(), equalTo(actualValues.length));
+        for (int i = 0; i < actualValues.length; i++) {
+            assertThat(values.valueAt(i), equalTo(new BytesRef(actualValues[i])));
+        }
+    }
+
+
+    @Test
+    public void testSingleValueWithMissing() throws Exception {
+        fillSingleValueWithMissing();
+        IndexFieldData indexFieldData = getForField("value");
+        AtomicFieldData fieldData = indexFieldData.load(refreshReader());
+        assertThat(fieldData.ramBytesUsed(), greaterThanOrEqualTo(minRamBytesUsed()));
+
+        SortedBinaryDocValues bytesValues = fieldData
+                .getBytesValues();
+
+        assertValues(bytesValues, 0, two());
+        assertValues(bytesValues, 1, Strings.EMPTY_ARRAY);
+        assertValues(bytesValues, 2, three());
+    }
+
+    protected abstract void fillMultiValueAllSet() throws Exception;
+
+    @Test
+    public void testMultiValueAllSet() throws Exception {
+        fillMultiValueAllSet();
+        IndexFieldData indexFieldData = getForField("value");
+        AtomicFieldData fieldData = indexFieldData.load(refreshReader());
+        assertThat(fieldData.ramBytesUsed(), greaterThanOrEqualTo(minRamBytesUsed()));
+
+        SortedBinaryDocValues bytesValues = fieldData.getBytesValues();
+
+        assertValues(bytesValues, 0, two(), four());
+        assertValues(bytesValues, 1, one());
+        assertValues(bytesValues, 2, three());
+        
+        IndexSearcher searcher = new IndexSearcher(DirectoryReader.open(writer, true));
+        TopFieldDocs topDocs = searcher.search(new MatchAllDocsQuery(), 10, new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.MIN, null))));
+        assertThat(topDocs.totalHits, equalTo(3));
+        assertThat(topDocs.scoreDocs.length, equalTo(3));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(1));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(0));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(2));
+
+        topDocs = searcher.search(new MatchAllDocsQuery(), 10, new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.MAX, null), true)));
+        assertThat(topDocs.totalHits, equalTo(3));
+        assertThat(topDocs.scoreDocs.length, equalTo(3));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(0));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(2));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(1));
+    }
+
+    protected abstract void fillMultiValueWithMissing() throws Exception;
+
+    @Test
+    public void testMultiValueWithMissing() throws Exception {
+        fillMultiValueWithMissing();
+        IndexFieldData indexFieldData = getForField("value");
+        AtomicFieldData fieldData = indexFieldData.load(refreshReader());
+        assertThat(fieldData.ramBytesUsed(), greaterThanOrEqualTo(minRamBytesUsed()));
+
+        SortedBinaryDocValues bytesValues = fieldData.getBytesValues();
+
+        assertValues(bytesValues, 0, two(), four());
+        assertValues(bytesValues, 1, Strings.EMPTY_ARRAY);
+        assertValues(bytesValues, 2, three());
+    }
+
+    public void testMissingValueForAll() throws Exception {
+        fillAllMissing();
+        IndexFieldData indexFieldData = getForField("value");
+        AtomicFieldData fieldData = indexFieldData.load(refreshReader());
+        // Some impls (FST) return size 0 and some (PagedBytes) do take size in the case no actual data is loaded
+        assertThat(fieldData.ramBytesUsed(), greaterThanOrEqualTo(0l));
+
+        SortedBinaryDocValues bytesValues = fieldData.getBytesValues();
+
+        assertValues(bytesValues, 0, Strings.EMPTY_ARRAY);
+        assertValues(bytesValues, 1, Strings.EMPTY_ARRAY);
+        assertValues(bytesValues, 2, Strings.EMPTY_ARRAY);
+        SortedBinaryDocValues hashedBytesValues = fieldData.getBytesValues();
+
+        assertValues(hashedBytesValues, 0, Strings.EMPTY_ARRAY);
+        assertValues(hashedBytesValues, 1, Strings.EMPTY_ARRAY);
+        assertValues(hashedBytesValues, 2, Strings.EMPTY_ARRAY);
+    }
+
+    protected abstract void fillAllMissing() throws Exception;
+
+    @Test
+    public void testSortMultiValuesFields() throws Exception {
+        fillExtendedMvSet();
+        IndexFieldData indexFieldData = getForField("value");
+
+        IndexSearcher searcher = new IndexSearcher(DirectoryReader.open(writer, true));
+        TopFieldDocs topDocs = searcher.search(new MatchAllDocsQuery(), 10,
+                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.MIN, null))));
+        assertThat(topDocs.totalHits, equalTo(8));
+        assertThat(topDocs.scoreDocs.length, equalTo(8));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(7));
+        assertThat(((BytesRef) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).utf8ToString(), equalTo("!08"));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(0));
+        assertThat(((BytesRef) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).utf8ToString(), equalTo("02"));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(2));
+        assertThat(((BytesRef) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).utf8ToString(), equalTo("03"));
+        assertThat(topDocs.scoreDocs[3].doc, equalTo(3));
+        assertThat(((BytesRef) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).utf8ToString(), equalTo("04"));
+        assertThat(topDocs.scoreDocs[4].doc, equalTo(4));
+        assertThat(((BytesRef) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).utf8ToString(), equalTo("06"));
+        assertThat(topDocs.scoreDocs[5].doc, equalTo(6));
+        assertThat(((BytesRef) ((FieldDoc) topDocs.scoreDocs[5]).fields[0]).utf8ToString(), equalTo("08"));
+        assertThat(topDocs.scoreDocs[6].doc, equalTo(1));
+        assertThat((BytesRef) ((FieldDoc) topDocs.scoreDocs[6]).fields[0], equalTo(null));
+        assertThat(topDocs.scoreDocs[7].doc, equalTo(5));
+        assertThat((BytesRef) ((FieldDoc) topDocs.scoreDocs[7]).fields[0], equalTo(null));
+
+        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
+                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.MAX, null), true)));
+        assertThat(topDocs.totalHits, equalTo(8));
+        assertThat(topDocs.scoreDocs.length, equalTo(8));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(6));
+        assertThat(((BytesRef) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).utf8ToString(), equalTo("10"));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(4));
+        assertThat(((BytesRef) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).utf8ToString(), equalTo("08"));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(3));
+        assertThat(((BytesRef) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).utf8ToString(), equalTo("06"));
+        assertThat(topDocs.scoreDocs[3].doc, equalTo(0));
+        assertThat(((BytesRef) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).utf8ToString(), equalTo("04"));
+        assertThat(topDocs.scoreDocs[4].doc, equalTo(2));
+        assertThat(((BytesRef) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).utf8ToString(), equalTo("03"));
+        assertThat(topDocs.scoreDocs[5].doc, equalTo(7));
+        assertThat(((BytesRef) ((FieldDoc) topDocs.scoreDocs[5]).fields[0]).utf8ToString(), equalTo("!10"));
+        assertThat(topDocs.scoreDocs[6].doc, equalTo(1));
+        assertThat(((FieldDoc) topDocs.scoreDocs[6]).fields[0], equalTo(null));
+        assertThat(topDocs.scoreDocs[7].doc, equalTo(5));
+        assertThat(((FieldDoc) topDocs.scoreDocs[7]).fields[0], equalTo(null));
+    }
+
+    protected abstract void fillExtendedMvSet() throws Exception;
+
+}
diff --git a/core/src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataImplTests.java b/core/src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataImplTests.java
deleted file mode 100644
index 03328f2..0000000
--- a/core/src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataImplTests.java
+++ /dev/null
@@ -1,278 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.fielddata;
-
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.search.*;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.index.fielddata.IndexFieldData.XFieldComparatorSource;
-import org.elasticsearch.search.MultiValueMode;
-import org.junit.Test;
-
-import static org.hamcrest.Matchers.*;
-
-public abstract class AbstractFieldDataImplTests extends AbstractFieldDataTests {
-
-    protected String one() {
-        return "1";
-    }
-
-    protected String two() {
-        return "2";
-    }
-
-    protected String three() {
-        return "3";
-    }
-
-    protected String four() {
-        return "4";
-    }
-
-    protected String toString(Object value) {
-        if (value instanceof BytesRef) {
-            return ((BytesRef) value).utf8ToString();
-        }
-        return value.toString();
-    }
-
-    protected abstract void fillSingleValueAllSet() throws Exception;
-
-    protected abstract void add2SingleValuedDocumentsAndDeleteOneOfThem() throws Exception;
-
-    protected long minRamBytesUsed() {
-        // minimum number of bytes that this fielddata instance is expected to require
-        return 1;
-    }
-
-    @Test
-    public void testDeletedDocs() throws Exception {
-        add2SingleValuedDocumentsAndDeleteOneOfThem();
-        IndexFieldData indexFieldData = getForField("value");
-        LeafReaderContext readerContext = refreshReader();
-        AtomicFieldData fieldData = indexFieldData.load(readerContext);
-        SortedBinaryDocValues values = fieldData.getBytesValues();
-        for (int i = 0; i < readerContext.reader().maxDoc(); ++i) {
-            values.setDocument(i);
-            assertThat(values.count(), greaterThanOrEqualTo(1));
-        }
-    }
-
-    @Test
-    public void testSingleValueAllSet() throws Exception {
-        fillSingleValueAllSet();
-        IndexFieldData indexFieldData = getForField("value");
-        LeafReaderContext readerContext = refreshReader();
-        AtomicFieldData fieldData = indexFieldData.load(readerContext);
-        assertThat(fieldData.ramBytesUsed(), greaterThanOrEqualTo(minRamBytesUsed()));
-
-        SortedBinaryDocValues bytesValues = fieldData.getBytesValues();
-
-        bytesValues.setDocument(0);
-        assertThat(bytesValues.count(), equalTo(1));
-        assertThat(bytesValues.valueAt(0), equalTo(new BytesRef(two())));
-        bytesValues.setDocument(1);
-        assertThat(bytesValues.count(), equalTo(1));
-        assertThat(bytesValues.valueAt(0), equalTo(new BytesRef(one())));
-        bytesValues.setDocument(2);
-        assertThat(bytesValues.count(), equalTo(1));
-        assertThat(bytesValues.valueAt(0), equalTo(new BytesRef(three())));
-
-        assertValues(bytesValues, 0, two());
-        assertValues(bytesValues, 1, one());
-        assertValues(bytesValues, 2, three());
-
-        IndexSearcher searcher = new IndexSearcher(readerContext.reader());
-        TopFieldDocs topDocs;
-
-        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
-                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.MIN, null))));
-        assertThat(topDocs.totalHits, equalTo(3));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(1));
-        assertThat(toString(((FieldDoc) topDocs.scoreDocs[0]).fields[0]), equalTo(one()));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(0));
-        assertThat(toString(((FieldDoc) topDocs.scoreDocs[1]).fields[0]), equalTo(two()));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(2));
-        assertThat(toString(((FieldDoc) topDocs.scoreDocs[2]).fields[0]), equalTo(three()));
-
-        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
-                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.MAX, null), true)));
-        assertThat(topDocs.totalHits, equalTo(3));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(2));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(0));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(1));
-    }
-
-    protected abstract void fillSingleValueWithMissing() throws Exception;
-    
-    public void assertValues(SortedBinaryDocValues values, int docId, BytesRef... actualValues) {
-        values.setDocument(docId);
-        assertThat(values.count(), equalTo(actualValues.length));
-        for (int i = 0; i < actualValues.length; i++) {
-            assertThat(values.valueAt(i), equalTo(actualValues[i]));
-        }
-    }
-    
-    public void assertValues(SortedBinaryDocValues values, int docId, String... actualValues) {
-        values.setDocument(docId);
-        assertThat(values.count(), equalTo(actualValues.length));
-        for (int i = 0; i < actualValues.length; i++) {
-            assertThat(values.valueAt(i), equalTo(new BytesRef(actualValues[i])));
-        }
-    }
-
-
-    @Test
-    public void testSingleValueWithMissing() throws Exception {
-        fillSingleValueWithMissing();
-        IndexFieldData indexFieldData = getForField("value");
-        AtomicFieldData fieldData = indexFieldData.load(refreshReader());
-        assertThat(fieldData.ramBytesUsed(), greaterThanOrEqualTo(minRamBytesUsed()));
-
-        SortedBinaryDocValues bytesValues = fieldData
-                .getBytesValues();
-
-        assertValues(bytesValues, 0, two());
-        assertValues(bytesValues, 1, Strings.EMPTY_ARRAY);
-        assertValues(bytesValues, 2, three());
-    }
-
-    protected abstract void fillMultiValueAllSet() throws Exception;
-
-    @Test
-    public void testMultiValueAllSet() throws Exception {
-        fillMultiValueAllSet();
-        IndexFieldData indexFieldData = getForField("value");
-        AtomicFieldData fieldData = indexFieldData.load(refreshReader());
-        assertThat(fieldData.ramBytesUsed(), greaterThanOrEqualTo(minRamBytesUsed()));
-
-        SortedBinaryDocValues bytesValues = fieldData.getBytesValues();
-
-        assertValues(bytesValues, 0, two(), four());
-        assertValues(bytesValues, 1, one());
-        assertValues(bytesValues, 2, three());
-        
-        IndexSearcher searcher = new IndexSearcher(DirectoryReader.open(writer, true));
-        TopFieldDocs topDocs = searcher.search(new MatchAllDocsQuery(), 10, new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.MIN, null))));
-        assertThat(topDocs.totalHits, equalTo(3));
-        assertThat(topDocs.scoreDocs.length, equalTo(3));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(1));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(0));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(2));
-
-        topDocs = searcher.search(new MatchAllDocsQuery(), 10, new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.MAX, null), true)));
-        assertThat(topDocs.totalHits, equalTo(3));
-        assertThat(topDocs.scoreDocs.length, equalTo(3));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(0));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(2));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(1));
-    }
-
-    protected abstract void fillMultiValueWithMissing() throws Exception;
-
-    @Test
-    public void testMultiValueWithMissing() throws Exception {
-        fillMultiValueWithMissing();
-        IndexFieldData indexFieldData = getForField("value");
-        AtomicFieldData fieldData = indexFieldData.load(refreshReader());
-        assertThat(fieldData.ramBytesUsed(), greaterThanOrEqualTo(minRamBytesUsed()));
-
-        SortedBinaryDocValues bytesValues = fieldData.getBytesValues();
-
-        assertValues(bytesValues, 0, two(), four());
-        assertValues(bytesValues, 1, Strings.EMPTY_ARRAY);
-        assertValues(bytesValues, 2, three());
-    }
-
-    public void testMissingValueForAll() throws Exception {
-        fillAllMissing();
-        IndexFieldData indexFieldData = getForField("value");
-        AtomicFieldData fieldData = indexFieldData.load(refreshReader());
-        // Some impls (FST) return size 0 and some (PagedBytes) do take size in the case no actual data is loaded
-        assertThat(fieldData.ramBytesUsed(), greaterThanOrEqualTo(0l));
-
-        SortedBinaryDocValues bytesValues = fieldData.getBytesValues();
-
-        assertValues(bytesValues, 0, Strings.EMPTY_ARRAY);
-        assertValues(bytesValues, 1, Strings.EMPTY_ARRAY);
-        assertValues(bytesValues, 2, Strings.EMPTY_ARRAY);
-        SortedBinaryDocValues hashedBytesValues = fieldData.getBytesValues();
-
-        assertValues(hashedBytesValues, 0, Strings.EMPTY_ARRAY);
-        assertValues(hashedBytesValues, 1, Strings.EMPTY_ARRAY);
-        assertValues(hashedBytesValues, 2, Strings.EMPTY_ARRAY);
-    }
-
-    protected abstract void fillAllMissing() throws Exception;
-
-    @Test
-    public void testSortMultiValuesFields() throws Exception {
-        fillExtendedMvSet();
-        IndexFieldData indexFieldData = getForField("value");
-
-        IndexSearcher searcher = new IndexSearcher(DirectoryReader.open(writer, true));
-        TopFieldDocs topDocs = searcher.search(new MatchAllDocsQuery(), 10,
-                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.MIN, null))));
-        assertThat(topDocs.totalHits, equalTo(8));
-        assertThat(topDocs.scoreDocs.length, equalTo(8));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(7));
-        assertThat(((BytesRef) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).utf8ToString(), equalTo("!08"));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(0));
-        assertThat(((BytesRef) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).utf8ToString(), equalTo("02"));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(2));
-        assertThat(((BytesRef) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).utf8ToString(), equalTo("03"));
-        assertThat(topDocs.scoreDocs[3].doc, equalTo(3));
-        assertThat(((BytesRef) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).utf8ToString(), equalTo("04"));
-        assertThat(topDocs.scoreDocs[4].doc, equalTo(4));
-        assertThat(((BytesRef) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).utf8ToString(), equalTo("06"));
-        assertThat(topDocs.scoreDocs[5].doc, equalTo(6));
-        assertThat(((BytesRef) ((FieldDoc) topDocs.scoreDocs[5]).fields[0]).utf8ToString(), equalTo("08"));
-        assertThat(topDocs.scoreDocs[6].doc, equalTo(1));
-        assertThat((BytesRef) ((FieldDoc) topDocs.scoreDocs[6]).fields[0], equalTo(null));
-        assertThat(topDocs.scoreDocs[7].doc, equalTo(5));
-        assertThat((BytesRef) ((FieldDoc) topDocs.scoreDocs[7]).fields[0], equalTo(null));
-
-        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
-                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.MAX, null), true)));
-        assertThat(topDocs.totalHits, equalTo(8));
-        assertThat(topDocs.scoreDocs.length, equalTo(8));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(6));
-        assertThat(((BytesRef) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).utf8ToString(), equalTo("10"));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(4));
-        assertThat(((BytesRef) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).utf8ToString(), equalTo("08"));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(3));
-        assertThat(((BytesRef) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).utf8ToString(), equalTo("06"));
-        assertThat(topDocs.scoreDocs[3].doc, equalTo(0));
-        assertThat(((BytesRef) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).utf8ToString(), equalTo("04"));
-        assertThat(topDocs.scoreDocs[4].doc, equalTo(2));
-        assertThat(((BytesRef) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).utf8ToString(), equalTo("03"));
-        assertThat(topDocs.scoreDocs[5].doc, equalTo(7));
-        assertThat(((BytesRef) ((FieldDoc) topDocs.scoreDocs[5]).fields[0]).utf8ToString(), equalTo("!10"));
-        assertThat(topDocs.scoreDocs[6].doc, equalTo(1));
-        assertThat(((FieldDoc) topDocs.scoreDocs[6]).fields[0], equalTo(null));
-        assertThat(topDocs.scoreDocs[7].doc, equalTo(5));
-        assertThat(((FieldDoc) topDocs.scoreDocs[7]).fields[0], equalTo(null));
-    }
-
-    protected abstract void fillExtendedMvSet() throws Exception;
-
-}
diff --git a/core/src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataTestCase.java b/core/src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataTestCase.java
new file mode 100644
index 0000000..7489162
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataTestCase.java
@@ -0,0 +1,157 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.fielddata;
+
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.*;
+import org.apache.lucene.search.Filter;
+import org.apache.lucene.store.RAMDirectory;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.IndexService;
+import org.elasticsearch.index.cache.bitset.BitsetFilterCache;
+import org.elasticsearch.index.mapper.ContentPath;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.Mapper.BuilderContext;
+import org.elasticsearch.index.mapper.MapperBuilders;
+import org.elasticsearch.index.mapper.MapperService;
+import org.elasticsearch.index.mapper.internal.ParentFieldMapper;
+import org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache;
+import org.elasticsearch.test.ESSingleNodeTestCase;
+import org.junit.After;
+import org.junit.Before;
+
+import static org.elasticsearch.index.fielddata.IndexFieldData.XFieldComparatorSource.Nested;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.not;
+import static org.hamcrest.Matchers.sameInstance;
+
+public abstract class AbstractFieldDataTestCase extends ESSingleNodeTestCase {
+
+    protected IndexService indexService;
+    protected IndexFieldDataService ifdService;
+    protected MapperService mapperService;
+    protected IndexWriter writer;
+    protected LeafReaderContext readerContext;
+    protected IndexReader topLevelReader;
+    protected IndicesFieldDataCache indicesFieldDataCache;
+
+    protected abstract FieldDataType getFieldDataType();
+
+    protected boolean hasDocValues() {
+        return false;
+    }
+
+    public <IFD extends IndexFieldData<?>> IFD getForField(String fieldName) {
+        return getForField(getFieldDataType(), fieldName, hasDocValues());
+    }
+
+    public <IFD extends IndexFieldData<?>> IFD getForField(FieldDataType type, String fieldName) {
+        return getForField(type, fieldName, hasDocValues());
+    }
+
+    public <IFD extends IndexFieldData<?>> IFD getForField(FieldDataType type, String fieldName, boolean docValues) {
+        final MappedFieldType fieldType;
+        final BuilderContext context = new BuilderContext(indexService.settingsService().getSettings(), new ContentPath(1));
+        if (type.getType().equals("string")) {
+            fieldType = MapperBuilders.stringField(fieldName).tokenized(false).docValues(docValues).fieldDataSettings(type.getSettings()).build(context).fieldType();
+        } else if (type.getType().equals("float")) {
+            fieldType = MapperBuilders.floatField(fieldName).docValues(docValues).fieldDataSettings(type.getSettings()).build(context).fieldType();
+        } else if (type.getType().equals("double")) {
+            fieldType = MapperBuilders.doubleField(fieldName).docValues(docValues).fieldDataSettings(type.getSettings()).build(context).fieldType();
+        } else if (type.getType().equals("long")) {
+            fieldType = MapperBuilders.longField(fieldName).docValues(docValues).fieldDataSettings(type.getSettings()).build(context).fieldType();
+        } else if (type.getType().equals("int")) {
+            fieldType = MapperBuilders.integerField(fieldName).docValues(docValues).fieldDataSettings(type.getSettings()).build(context).fieldType();
+        } else if (type.getType().equals("short")) {
+            fieldType = MapperBuilders.shortField(fieldName).docValues(docValues).fieldDataSettings(type.getSettings()).build(context).fieldType();
+        } else if (type.getType().equals("byte")) {
+            fieldType = MapperBuilders.byteField(fieldName).docValues(docValues).fieldDataSettings(type.getSettings()).build(context).fieldType();
+        } else if (type.getType().equals("geo_point")) {
+            fieldType = MapperBuilders.geoPointField(fieldName).docValues(docValues).fieldDataSettings(type.getSettings()).build(context).fieldType();
+        } else if (type.getType().equals("_parent")) {
+            fieldType = new ParentFieldMapper.Builder().type(fieldName).build(context).fieldType();
+        } else if (type.getType().equals("binary")) {
+            fieldType = MapperBuilders.binaryField(fieldName).docValues(docValues).fieldDataSettings(type.getSettings()).build(context).fieldType();
+        } else {
+            throw new UnsupportedOperationException(type.getType());
+        }
+        return ifdService.getForField(fieldType);
+    }
+
+    @Before
+    public void setup() throws Exception {
+        Settings settings = Settings.builder().put("index.fielddata.cache", "none").build();
+        indexService = createIndex("test", settings);
+        mapperService = indexService.mapperService();
+        indicesFieldDataCache = indexService.injector().getInstance(IndicesFieldDataCache.class);
+        ifdService = indexService.fieldData();
+        // LogByteSizeMP to preserve doc ID order
+        writer = new IndexWriter(new RAMDirectory(), new IndexWriterConfig(new StandardAnalyzer()).setMergePolicy(new LogByteSizeMergePolicy()));
+    }
+
+    protected LeafReaderContext refreshReader() throws Exception {
+        if (readerContext != null) {
+            readerContext.reader().close();
+        }
+        LeafReader reader = SlowCompositeReaderWrapper.wrap(topLevelReader = DirectoryReader.open(writer, true));
+        readerContext = reader.getContext();
+        return readerContext;
+    }
+
+    @Override
+    @After
+    public void tearDown() throws Exception {
+        super.tearDown();
+        if (readerContext != null) {
+            readerContext.reader().close();
+        }
+        writer.close();
+    }
+
+    protected Nested createNested(Filter parentFilter, Filter childFilter) {
+        BitsetFilterCache s = indexService.bitsetFilterCache();
+        return new Nested(s.getBitDocIdSetFilter(parentFilter), s.getBitDocIdSetFilter(childFilter));
+    }
+
+    public void testEmpty() throws Exception {
+        Document d = new Document();
+        d.add(new StringField("field", "value", Field.Store.NO));
+        writer.addDocument(d);
+        refreshReader();
+
+        IndexFieldData fieldData = getForField("non_existing_field");
+        int max = randomInt(7);
+        AtomicFieldData previous = null;
+        for (int i = 0; i < max; i++) {
+            AtomicFieldData current = fieldData.load(readerContext);
+            assertThat(current.ramBytesUsed(), equalTo(0l));
+            if (previous != null) {
+                assertThat(current, not(sameInstance(previous)));
+            }
+            previous = current;
+        }
+
+
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataTests.java b/core/src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataTests.java
deleted file mode 100644
index 6908a67..0000000
--- a/core/src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataTests.java
+++ /dev/null
@@ -1,157 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.fielddata;
-
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.*;
-import org.apache.lucene.search.Filter;
-import org.apache.lucene.store.RAMDirectory;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.IndexService;
-import org.elasticsearch.index.cache.bitset.BitsetFilterCache;
-import org.elasticsearch.index.mapper.ContentPath;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.Mapper.BuilderContext;
-import org.elasticsearch.index.mapper.MapperBuilders;
-import org.elasticsearch.index.mapper.MapperService;
-import org.elasticsearch.index.mapper.internal.ParentFieldMapper;
-import org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache;
-import org.elasticsearch.test.ESSingleNodeTestCase;
-import org.junit.After;
-import org.junit.Before;
-
-import static org.elasticsearch.index.fielddata.IndexFieldData.XFieldComparatorSource.Nested;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.not;
-import static org.hamcrest.Matchers.sameInstance;
-
-public abstract class AbstractFieldDataTests extends ESSingleNodeTestCase {
-
-    protected IndexService indexService;
-    protected IndexFieldDataService ifdService;
-    protected MapperService mapperService;
-    protected IndexWriter writer;
-    protected LeafReaderContext readerContext;
-    protected IndexReader topLevelReader;
-    protected IndicesFieldDataCache indicesFieldDataCache;
-
-    protected abstract FieldDataType getFieldDataType();
-
-    protected boolean hasDocValues() {
-        return false;
-    }
-
-    public <IFD extends IndexFieldData<?>> IFD getForField(String fieldName) {
-        return getForField(getFieldDataType(), fieldName, hasDocValues());
-    }
-
-    public <IFD extends IndexFieldData<?>> IFD getForField(FieldDataType type, String fieldName) {
-        return getForField(type, fieldName, hasDocValues());
-    }
-
-    public <IFD extends IndexFieldData<?>> IFD getForField(FieldDataType type, String fieldName, boolean docValues) {
-        final MappedFieldType fieldType;
-        final BuilderContext context = new BuilderContext(indexService.settingsService().getSettings(), new ContentPath(1));
-        if (type.getType().equals("string")) {
-            fieldType = MapperBuilders.stringField(fieldName).tokenized(false).docValues(docValues).fieldDataSettings(type.getSettings()).build(context).fieldType();
-        } else if (type.getType().equals("float")) {
-            fieldType = MapperBuilders.floatField(fieldName).docValues(docValues).fieldDataSettings(type.getSettings()).build(context).fieldType();
-        } else if (type.getType().equals("double")) {
-            fieldType = MapperBuilders.doubleField(fieldName).docValues(docValues).fieldDataSettings(type.getSettings()).build(context).fieldType();
-        } else if (type.getType().equals("long")) {
-            fieldType = MapperBuilders.longField(fieldName).docValues(docValues).fieldDataSettings(type.getSettings()).build(context).fieldType();
-        } else if (type.getType().equals("int")) {
-            fieldType = MapperBuilders.integerField(fieldName).docValues(docValues).fieldDataSettings(type.getSettings()).build(context).fieldType();
-        } else if (type.getType().equals("short")) {
-            fieldType = MapperBuilders.shortField(fieldName).docValues(docValues).fieldDataSettings(type.getSettings()).build(context).fieldType();
-        } else if (type.getType().equals("byte")) {
-            fieldType = MapperBuilders.byteField(fieldName).docValues(docValues).fieldDataSettings(type.getSettings()).build(context).fieldType();
-        } else if (type.getType().equals("geo_point")) {
-            fieldType = MapperBuilders.geoPointField(fieldName).docValues(docValues).fieldDataSettings(type.getSettings()).build(context).fieldType();
-        } else if (type.getType().equals("_parent")) {
-            fieldType = new ParentFieldMapper.Builder().type(fieldName).build(context).fieldType();
-        } else if (type.getType().equals("binary")) {
-            fieldType = MapperBuilders.binaryField(fieldName).docValues(docValues).fieldDataSettings(type.getSettings()).build(context).fieldType();
-        } else {
-            throw new UnsupportedOperationException(type.getType());
-        }
-        return ifdService.getForField(fieldType);
-    }
-
-    @Before
-    public void setup() throws Exception {
-        Settings settings = Settings.builder().put("index.fielddata.cache", "none").build();
-        indexService = createIndex("test", settings);
-        mapperService = indexService.mapperService();
-        indicesFieldDataCache = indexService.injector().getInstance(IndicesFieldDataCache.class);
-        ifdService = indexService.fieldData();
-        // LogByteSizeMP to preserve doc ID order
-        writer = new IndexWriter(new RAMDirectory(), new IndexWriterConfig(new StandardAnalyzer()).setMergePolicy(new LogByteSizeMergePolicy()));
-    }
-
-    protected LeafReaderContext refreshReader() throws Exception {
-        if (readerContext != null) {
-            readerContext.reader().close();
-        }
-        LeafReader reader = SlowCompositeReaderWrapper.wrap(topLevelReader = DirectoryReader.open(writer, true));
-        readerContext = reader.getContext();
-        return readerContext;
-    }
-
-    @Override
-    @After
-    public void tearDown() throws Exception {
-        super.tearDown();
-        if (readerContext != null) {
-            readerContext.reader().close();
-        }
-        writer.close();
-    }
-
-    protected Nested createNested(Filter parentFilter, Filter childFilter) {
-        BitsetFilterCache s = indexService.bitsetFilterCache();
-        return new Nested(s.getBitDocIdSetFilter(parentFilter), s.getBitDocIdSetFilter(childFilter));
-    }
-
-    public void testEmpty() throws Exception {
-        Document d = new Document();
-        d.add(new StringField("field", "value", Field.Store.NO));
-        writer.addDocument(d);
-        refreshReader();
-
-        IndexFieldData fieldData = getForField("non_existing_field");
-        int max = randomInt(7);
-        AtomicFieldData previous = null;
-        for (int i = 0; i < max; i++) {
-            AtomicFieldData current = fieldData.load(readerContext);
-            assertThat(current.ramBytesUsed(), equalTo(0l));
-            if (previous != null) {
-                assertThat(current, not(sameInstance(previous)));
-            }
-            previous = current;
-        }
-
-
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/index/fielddata/AbstractNumericFieldDataTestCase.java b/core/src/test/java/org/elasticsearch/index/fielddata/AbstractNumericFieldDataTestCase.java
new file mode 100644
index 0000000..5c28a8f
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/fielddata/AbstractNumericFieldDataTestCase.java
@@ -0,0 +1,512 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.fielddata;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.SortedNumericDocValues;
+import org.apache.lucene.search.*;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.search.MultiValueMode;
+import org.junit.Test;
+
+import java.util.Locale;
+
+import static org.hamcrest.Matchers.equalTo;
+
+/**
+ */
+public abstract class AbstractNumericFieldDataTestCase extends AbstractFieldDataImplTestCase {
+
+    @Override
+    protected abstract FieldDataType getFieldDataType();
+
+    protected Settings.Builder getFieldDataSettings() {
+        Settings.Builder builder = Settings.builder();
+        IndexFieldData.CommonSettings.MemoryStorageFormat[] formats = IndexFieldData.CommonSettings.MemoryStorageFormat.values();
+        int i = randomInt(formats.length);
+        if (i < formats.length) {
+            builder.put(IndexFieldData.CommonSettings.SETTING_MEMORY_STORAGE_HINT, formats[i].name().toLowerCase(Locale.ROOT));
+        }
+        return builder;
+    }
+
+    @Test
+    public void testSingleValueAllSetNumber() throws Exception {
+        fillSingleValueAllSet();
+        IndexNumericFieldData indexFieldData = getForField("value");
+        AtomicNumericFieldData fieldData = indexFieldData.load(refreshReader());
+
+        SortedNumericDocValues longValues = fieldData.getLongValues();
+
+        assertThat(FieldData.isMultiValued(longValues), equalTo(false));
+
+        longValues.setDocument(0);
+        assertThat(longValues.count(), equalTo(1));
+        assertThat(longValues.valueAt(0), equalTo(2l));
+
+        longValues.setDocument(1);
+        assertThat(longValues.count(), equalTo(1));
+        assertThat(longValues.valueAt(0), equalTo(1l));
+
+        longValues.setDocument(2);
+        assertThat(longValues.count(), equalTo(1));
+        assertThat(longValues.valueAt(0), equalTo(3l));
+
+        SortedNumericDoubleValues doubleValues = fieldData.getDoubleValues();
+
+        assertThat(FieldData.isMultiValued(doubleValues), equalTo(false));
+
+        doubleValues.setDocument(0);
+        assertThat(1, equalTo(doubleValues.count()));
+        assertThat(doubleValues.valueAt(0), equalTo(2d));
+
+        doubleValues.setDocument(1);
+        assertThat(1, equalTo(doubleValues.count()));
+        assertThat(doubleValues.valueAt(0), equalTo(1d));
+
+        doubleValues.setDocument(2);
+        assertThat(1, equalTo(doubleValues.count()));
+        assertThat(doubleValues.valueAt(0), equalTo(3d));
+
+        IndexSearcher searcher = new IndexSearcher(readerContext.reader());
+        TopFieldDocs topDocs;
+
+        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
+                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.MIN, null))));
+        assertThat(topDocs.totalHits, equalTo(3));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(1));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(0));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(2));
+
+        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
+                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.MAX, null), true)));
+        assertThat(topDocs.totalHits, equalTo(3));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(2));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(0));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(1));
+    }
+
+    @Test
+    public void testSingleValueWithMissingNumber() throws Exception {
+        fillSingleValueWithMissing();
+        IndexNumericFieldData indexFieldData = getForField("value");
+        AtomicNumericFieldData fieldData = indexFieldData.load(refreshReader());
+
+        SortedNumericDocValues longValues = fieldData.getLongValues();
+
+        assertThat(FieldData.isMultiValued(longValues), equalTo(false));
+
+        longValues.setDocument(0);
+        assertThat(longValues.count(), equalTo(1));
+        assertThat(longValues.valueAt(0), equalTo(2l));
+
+        longValues.setDocument(1);
+        assertThat(longValues.count(), equalTo(0));
+
+        longValues.setDocument(2);
+        assertThat(longValues.count(), equalTo(1));
+        assertThat(longValues.valueAt(0), equalTo(3l));
+
+        SortedNumericDoubleValues doubleValues = fieldData.getDoubleValues();
+
+        assertThat(FieldData.isMultiValued(doubleValues), equalTo(false));
+
+        doubleValues.setDocument(0);
+        assertThat(1, equalTo(doubleValues.count()));
+        assertThat(doubleValues.valueAt(0), equalTo(2d));
+
+        doubleValues.setDocument(1);
+        assertThat(0, equalTo(doubleValues.count()));
+
+        doubleValues.setDocument(2);
+        assertThat(1, equalTo(doubleValues.count()));
+        assertThat(doubleValues.valueAt(0), equalTo(3d));
+
+        IndexSearcher searcher = new IndexSearcher(readerContext.reader());
+        TopFieldDocs topDocs;
+
+        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
+                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.MIN, null)))); // defaults to _last
+        assertThat(topDocs.totalHits, equalTo(3));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(0));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(2));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(1));
+
+        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
+                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.MAX, null), true))); // defaults to _last
+        assertThat(topDocs.totalHits, equalTo(3));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(2));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(0));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(1));
+
+        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
+                new Sort(new SortField("value", indexFieldData.comparatorSource("_first", MultiValueMode.MIN, null))));
+        assertThat(topDocs.totalHits, equalTo(3));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(1));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(0));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(2));
+
+        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
+                new Sort(new SortField("value", indexFieldData.comparatorSource("_first", MultiValueMode.MAX, null), true)));
+        assertThat(topDocs.totalHits, equalTo(3));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(1));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(2));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(0));
+
+        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
+                new Sort(new SortField("value", indexFieldData.comparatorSource("1", MultiValueMode.MIN, null))));
+        assertThat(topDocs.totalHits, equalTo(3));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(1));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(0));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(2));
+
+        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
+                new Sort(new SortField("value", indexFieldData.comparatorSource("1", MultiValueMode.MAX, null), true)));
+        assertThat(topDocs.totalHits, equalTo(3));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(2));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(0));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(1));
+    }
+
+    @Test
+    public void testMultiValueAllSetNumber() throws Exception {
+        fillMultiValueAllSet();
+        IndexNumericFieldData indexFieldData = getForField("value");
+        AtomicNumericFieldData fieldData = indexFieldData.load(refreshReader());
+
+        SortedNumericDocValues longValues = fieldData.getLongValues();
+
+        assertThat(FieldData.isMultiValued(longValues), equalTo(true));
+
+        longValues.setDocument(0);
+        assertThat(longValues.count(), equalTo(2));
+        assertThat(longValues.valueAt(0), equalTo(2l));
+        assertThat(longValues.valueAt(1), equalTo(4l));
+
+        longValues.setDocument(1);
+        assertThat(longValues.count(), equalTo(1));
+        assertThat(longValues.valueAt(0), equalTo(1l));
+
+        longValues.setDocument(2);
+        assertThat(longValues.count(), equalTo(1));
+        assertThat(longValues.valueAt(0), equalTo(3l));
+
+        SortedNumericDoubleValues doubleValues = fieldData.getDoubleValues();
+
+        assertThat(FieldData.isMultiValued(doubleValues), equalTo(true));
+
+        doubleValues.setDocument(0);
+        assertThat(2, equalTo(doubleValues.count()));
+        assertThat(doubleValues.valueAt(0), equalTo(2d));
+        assertThat(doubleValues.valueAt(1), equalTo(4d));
+
+        doubleValues.setDocument(1);
+        assertThat(1, equalTo(doubleValues.count()));
+        assertThat(doubleValues.valueAt(0), equalTo(1d));
+
+        doubleValues.setDocument(2);
+        assertThat(1, equalTo(doubleValues.count()));
+        assertThat(doubleValues.valueAt(0), equalTo(3d));
+    }
+
+    @Test
+    public void testMultiValueWithMissingNumber() throws Exception {
+        fillMultiValueWithMissing();
+        IndexNumericFieldData indexFieldData = getForField("value");
+        AtomicNumericFieldData fieldData = indexFieldData.load(refreshReader());
+
+        SortedNumericDocValues longValues = fieldData.getLongValues();
+
+        assertThat(FieldData.isMultiValued(longValues), equalTo(true));
+
+        longValues.setDocument(0);
+        assertThat(longValues.count(), equalTo(2));
+        assertThat(longValues.valueAt(0), equalTo(2l));
+        assertThat(longValues.valueAt(1), equalTo(4l));
+
+        longValues.setDocument(1);
+        assertThat(longValues.count(), equalTo(0));
+
+        longValues.setDocument(2);
+        assertThat(longValues.count(), equalTo(1));
+        assertThat(longValues.valueAt(0), equalTo(3l));
+
+        SortedNumericDoubleValues doubleValues = fieldData.getDoubleValues();
+
+        assertThat(FieldData.isMultiValued(doubleValues), equalTo(true));
+
+        doubleValues.setDocument(0);
+        assertThat(2, equalTo(doubleValues.count()));
+        assertThat(doubleValues.valueAt(0), equalTo(2d));
+        assertThat(doubleValues.valueAt(1), equalTo(4d));
+
+        doubleValues.setDocument(1);
+        assertThat(0, equalTo(doubleValues.count()));
+
+        doubleValues.setDocument(2);
+        assertThat(1, equalTo(doubleValues.count()));
+        assertThat(doubleValues.valueAt(0), equalTo(3d));
+
+    }
+
+    @Override
+    @Test
+    public void testMissingValueForAll() throws Exception {
+        fillAllMissing();
+        IndexNumericFieldData indexFieldData = getForField("value");
+        AtomicNumericFieldData fieldData = indexFieldData.load(refreshReader());
+
+        // long values
+
+        SortedNumericDocValues longValues = fieldData.getLongValues();
+
+        assertThat(FieldData.isMultiValued(longValues), equalTo(false));
+
+        for (int i = 0; i < 3; ++i) {
+            longValues.setDocument(0);
+            assertThat(longValues.count(), equalTo(0));
+        }
+
+        // double values
+
+        SortedNumericDoubleValues doubleValues = fieldData.getDoubleValues();
+
+        assertThat(FieldData.isMultiValued(doubleValues), equalTo(false));
+
+        doubleValues.setDocument(0);
+        assertThat(0, equalTo(doubleValues.count()));
+
+        doubleValues.setDocument(1);
+        assertThat(0, equalTo(doubleValues.count()));
+
+        doubleValues.setDocument(2);
+        assertThat(0, equalTo(doubleValues.count()));
+    }
+
+
+    @Override
+    protected void fillAllMissing() throws Exception {
+        Document d = new Document();
+        d.add(new StringField("_id", "1", Field.Store.NO));
+        writer.addDocument(d);
+
+        d = new Document();
+        d.add(new StringField("_id", "2", Field.Store.NO));
+        writer.addDocument(d);
+
+        d = new Document();
+        d.add(new StringField("_id", "3", Field.Store.NO));
+        writer.addDocument(d);
+    }
+
+    @Override
+    @Test
+    public void testSortMultiValuesFields() throws Exception {
+        fillExtendedMvSet();
+        IndexFieldData indexFieldData = getForField("value");
+
+        IndexSearcher searcher = new IndexSearcher(DirectoryReader.open(writer, true));
+        TopFieldDocs topDocs = searcher.search(new MatchAllDocsQuery(), 10,
+                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.MIN, null)))); // defaults to _last
+        assertThat(topDocs.totalHits, equalTo(8));
+        assertThat(topDocs.scoreDocs.length, equalTo(8));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(7));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).intValue(), equalTo(-10));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(0));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).intValue(), equalTo(2));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(2));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).intValue(), equalTo(3));
+        assertThat(topDocs.scoreDocs[3].doc, equalTo(3));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).intValue(), equalTo(4));
+        assertThat(topDocs.scoreDocs[4].doc, equalTo(4));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).intValue(), equalTo(6));
+        assertThat(topDocs.scoreDocs[5].doc, equalTo(6));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[5]).fields[0]).intValue(), equalTo(8));
+        assertThat(topDocs.scoreDocs[6].doc, equalTo(1));
+//        assertThat(((FieldDoc) topDocs.scoreDocs[6]).fields[0], equalTo(null));
+        assertThat(topDocs.scoreDocs[7].doc, equalTo(5));
+//        assertThat(((FieldDoc) topDocs.scoreDocs[7]).fields[0], equalTo(null));
+
+        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
+                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.MAX, null), true))); // defaults to _last
+        assertThat(topDocs.totalHits, equalTo(8));
+        assertThat(topDocs.scoreDocs.length, equalTo(8));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(6));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).intValue(), equalTo(10));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(4));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).intValue(), equalTo(8));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(3));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).intValue(), equalTo(6));
+        assertThat(topDocs.scoreDocs[3].doc, equalTo(0));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).intValue(), equalTo(4));
+        assertThat(topDocs.scoreDocs[4].doc, equalTo(2));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).intValue(), equalTo(3));
+        assertThat(topDocs.scoreDocs[5].doc, equalTo(7));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[5]).fields[0]).intValue(), equalTo(-8));
+        assertThat(topDocs.scoreDocs[6].doc, equalTo(1));
+//        assertThat(((FieldDoc) topDocs.scoreDocs[6]).fields[0], equalTo(null));
+        assertThat(topDocs.scoreDocs[7].doc, equalTo(5));
+//        assertThat(((FieldDoc) topDocs.scoreDocs[7]).fields[0], equalTo(null));
+
+        searcher = new IndexSearcher(DirectoryReader.open(writer, true));
+        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
+                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.SUM, null)))); // defaults to _last
+        assertThat(topDocs.totalHits, equalTo(8));
+        assertThat(topDocs.scoreDocs.length, equalTo(8));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(7));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).intValue(), equalTo(-27));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(2));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).intValue(), equalTo(3));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(0));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).intValue(), equalTo(6));
+        assertThat(topDocs.scoreDocs[3].doc, equalTo(3));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).intValue(), equalTo(15));
+        assertThat(topDocs.scoreDocs[4].doc, equalTo(4));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).intValue(), equalTo(21));
+        assertThat(topDocs.scoreDocs[5].doc, equalTo(6));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[5]).fields[0]).intValue(), equalTo(27));
+        assertThat(topDocs.scoreDocs[6].doc, equalTo(1));
+//        assertThat(((FieldDoc) topDocs.scoreDocs[6]).fields[0], equalTo(null));
+        assertThat(topDocs.scoreDocs[7].doc, equalTo(5));
+//        assertThat(((FieldDoc) topDocs.scoreDocs[7]).fields[0], equalTo(null));
+
+        searcher = new IndexSearcher(DirectoryReader.open(writer, true));
+        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
+                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.SUM, null), true))); // defaults to _last
+        assertThat(topDocs.totalHits, equalTo(8));
+        assertThat(topDocs.scoreDocs.length, equalTo(8));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(6));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).intValue(), equalTo(27));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(4));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).intValue(), equalTo(21));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(3));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).intValue(), equalTo(15));
+        assertThat(topDocs.scoreDocs[3].doc, equalTo(0));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).intValue(), equalTo(6));
+        assertThat(topDocs.scoreDocs[4].doc, equalTo(2));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).intValue(), equalTo(3));
+        assertThat(topDocs.scoreDocs[5].doc, equalTo(7));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[5]).fields[0]).intValue(), equalTo(-27));
+        assertThat(topDocs.scoreDocs[6].doc, equalTo(1));
+//        assertThat(((FieldDoc) topDocs.scoreDocs[6]).fields[0], equalTo(null));
+        assertThat(topDocs.scoreDocs[7].doc, equalTo(5));
+//        assertThat(((FieldDoc) topDocs.scoreDocs[7]).fields[0], equalTo(null));
+
+        searcher = new IndexSearcher(DirectoryReader.open(writer, true));
+        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
+                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.AVG, null)))); // defaults to _last
+        assertThat(topDocs.totalHits, equalTo(8));
+        assertThat(topDocs.scoreDocs.length, equalTo(8));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(7));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).intValue(), equalTo(-9));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(0));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).intValue(), equalTo(3));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(2));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).intValue(), equalTo(3));
+        assertThat(topDocs.scoreDocs[3].doc, equalTo(3));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).intValue(), equalTo(5));
+        assertThat(topDocs.scoreDocs[4].doc, equalTo(4));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).intValue(), equalTo(7));
+        assertThat(topDocs.scoreDocs[5].doc, equalTo(6));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[5]).fields[0]).intValue(), equalTo(9));
+        assertThat(topDocs.scoreDocs[6].doc, equalTo(1));
+//        assertThat(((FieldDoc) topDocs.scoreDocs[6]).fields[0], equalTo(null));
+        assertThat(topDocs.scoreDocs[7].doc, equalTo(5));
+//        assertThat(((FieldDoc) topDocs.scoreDocs[7]).fields[0], equalTo(null));
+
+        searcher = new IndexSearcher(DirectoryReader.open(writer, true));
+        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
+                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.AVG, null), true))); // defaults to _last
+        assertThat(topDocs.totalHits, equalTo(8));
+        assertThat(topDocs.scoreDocs.length, equalTo(8));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(6));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).intValue(), equalTo(9));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(4));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).intValue(), equalTo(7));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(3));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).intValue(), equalTo(5));
+        assertThat(topDocs.scoreDocs[3].doc, equalTo(0));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).intValue(), equalTo(3));
+        assertThat(topDocs.scoreDocs[4].doc, equalTo(2));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).intValue(), equalTo(3));
+        assertThat(topDocs.scoreDocs[5].doc, equalTo(7));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[5]).fields[0]).intValue(), equalTo(-9));
+        assertThat(topDocs.scoreDocs[6].doc, equalTo(1));
+//        assertThat(((FieldDoc) topDocs.scoreDocs[6]).fields[0], equalTo(null));
+        assertThat(topDocs.scoreDocs[7].doc, equalTo(5));
+//        assertThat(((FieldDoc) topDocs.scoreDocs[7]).fields[0], equalTo(null));
+
+        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
+                new Sort(new SortField("value", indexFieldData.comparatorSource("_first", MultiValueMode.MIN, null))));
+        assertThat(topDocs.totalHits, equalTo(8));
+        assertThat(topDocs.scoreDocs.length, equalTo(8));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(1));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(5));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(7));
+        assertThat(topDocs.scoreDocs[3].doc, equalTo(0));
+        assertThat(topDocs.scoreDocs[4].doc, equalTo(2));
+        assertThat(topDocs.scoreDocs[5].doc, equalTo(3));
+        assertThat(topDocs.scoreDocs[6].doc, equalTo(4));
+        assertThat(topDocs.scoreDocs[7].doc, equalTo(6));
+
+        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
+                new Sort(new SortField("value", indexFieldData.comparatorSource("_first", MultiValueMode.MAX, null), true)));
+        assertThat(topDocs.totalHits, equalTo(8));
+        assertThat(topDocs.scoreDocs.length, equalTo(8));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(1));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(5));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(6));
+        assertThat(topDocs.scoreDocs[3].doc, equalTo(4));
+        assertThat(topDocs.scoreDocs[4].doc, equalTo(3));
+        assertThat(topDocs.scoreDocs[5].doc, equalTo(0));
+        assertThat(topDocs.scoreDocs[6].doc, equalTo(2));
+        assertThat(topDocs.scoreDocs[7].doc, equalTo(7));
+
+        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
+                new Sort(new SortField("value", indexFieldData.comparatorSource("-9", MultiValueMode.MIN, null))));
+        assertThat(topDocs.totalHits, equalTo(8));
+        assertThat(topDocs.scoreDocs.length, equalTo(8));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(7));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(1));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(5));
+        assertThat(topDocs.scoreDocs[3].doc, equalTo(0));
+        assertThat(topDocs.scoreDocs[4].doc, equalTo(2));
+        assertThat(topDocs.scoreDocs[5].doc, equalTo(3));
+        assertThat(topDocs.scoreDocs[6].doc, equalTo(4));
+        assertThat(topDocs.scoreDocs[7].doc, equalTo(6));
+
+        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
+                new Sort(new SortField("value", indexFieldData.comparatorSource("9", MultiValueMode.MAX, null), true)));
+        assertThat(topDocs.totalHits, equalTo(8));
+        assertThat(topDocs.scoreDocs.length, equalTo(8));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(6));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(1));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(5));
+        assertThat(topDocs.scoreDocs[3].doc, equalTo(4));
+        assertThat(topDocs.scoreDocs[4].doc, equalTo(3));
+        assertThat(topDocs.scoreDocs[5].doc, equalTo(0));
+        assertThat(topDocs.scoreDocs[6].doc, equalTo(2));
+        assertThat(topDocs.scoreDocs[7].doc, equalTo(7));
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/index/fielddata/AbstractNumericFieldDataTests.java b/core/src/test/java/org/elasticsearch/index/fielddata/AbstractNumericFieldDataTests.java
deleted file mode 100644
index 271a042..0000000
--- a/core/src/test/java/org/elasticsearch/index/fielddata/AbstractNumericFieldDataTests.java
+++ /dev/null
@@ -1,512 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.fielddata;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.SortedNumericDocValues;
-import org.apache.lucene.search.*;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.search.MultiValueMode;
-import org.junit.Test;
-
-import java.util.Locale;
-
-import static org.hamcrest.Matchers.equalTo;
-
-/**
- */
-public abstract class AbstractNumericFieldDataTests extends AbstractFieldDataImplTests {
-
-    @Override
-    protected abstract FieldDataType getFieldDataType();
-
-    protected Settings.Builder getFieldDataSettings() {
-        Settings.Builder builder = Settings.builder();
-        IndexFieldData.CommonSettings.MemoryStorageFormat[] formats = IndexFieldData.CommonSettings.MemoryStorageFormat.values();
-        int i = randomInt(formats.length);
-        if (i < formats.length) {
-            builder.put(IndexFieldData.CommonSettings.SETTING_MEMORY_STORAGE_HINT, formats[i].name().toLowerCase(Locale.ROOT));
-        }
-        return builder;
-    }
-
-    @Test
-    public void testSingleValueAllSetNumber() throws Exception {
-        fillSingleValueAllSet();
-        IndexNumericFieldData indexFieldData = getForField("value");
-        AtomicNumericFieldData fieldData = indexFieldData.load(refreshReader());
-
-        SortedNumericDocValues longValues = fieldData.getLongValues();
-
-        assertThat(FieldData.isMultiValued(longValues), equalTo(false));
-
-        longValues.setDocument(0);
-        assertThat(longValues.count(), equalTo(1));
-        assertThat(longValues.valueAt(0), equalTo(2l));
-
-        longValues.setDocument(1);
-        assertThat(longValues.count(), equalTo(1));
-        assertThat(longValues.valueAt(0), equalTo(1l));
-
-        longValues.setDocument(2);
-        assertThat(longValues.count(), equalTo(1));
-        assertThat(longValues.valueAt(0), equalTo(3l));
-
-        SortedNumericDoubleValues doubleValues = fieldData.getDoubleValues();
-
-        assertThat(FieldData.isMultiValued(doubleValues), equalTo(false));
-
-        doubleValues.setDocument(0);
-        assertThat(1, equalTo(doubleValues.count()));
-        assertThat(doubleValues.valueAt(0), equalTo(2d));
-
-        doubleValues.setDocument(1);
-        assertThat(1, equalTo(doubleValues.count()));
-        assertThat(doubleValues.valueAt(0), equalTo(1d));
-
-        doubleValues.setDocument(2);
-        assertThat(1, equalTo(doubleValues.count()));
-        assertThat(doubleValues.valueAt(0), equalTo(3d));
-
-        IndexSearcher searcher = new IndexSearcher(readerContext.reader());
-        TopFieldDocs topDocs;
-
-        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
-                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.MIN, null))));
-        assertThat(topDocs.totalHits, equalTo(3));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(1));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(0));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(2));
-
-        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
-                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.MAX, null), true)));
-        assertThat(topDocs.totalHits, equalTo(3));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(2));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(0));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(1));
-    }
-
-    @Test
-    public void testSingleValueWithMissingNumber() throws Exception {
-        fillSingleValueWithMissing();
-        IndexNumericFieldData indexFieldData = getForField("value");
-        AtomicNumericFieldData fieldData = indexFieldData.load(refreshReader());
-
-        SortedNumericDocValues longValues = fieldData.getLongValues();
-
-        assertThat(FieldData.isMultiValued(longValues), equalTo(false));
-
-        longValues.setDocument(0);
-        assertThat(longValues.count(), equalTo(1));
-        assertThat(longValues.valueAt(0), equalTo(2l));
-
-        longValues.setDocument(1);
-        assertThat(longValues.count(), equalTo(0));
-
-        longValues.setDocument(2);
-        assertThat(longValues.count(), equalTo(1));
-        assertThat(longValues.valueAt(0), equalTo(3l));
-
-        SortedNumericDoubleValues doubleValues = fieldData.getDoubleValues();
-
-        assertThat(FieldData.isMultiValued(doubleValues), equalTo(false));
-
-        doubleValues.setDocument(0);
-        assertThat(1, equalTo(doubleValues.count()));
-        assertThat(doubleValues.valueAt(0), equalTo(2d));
-
-        doubleValues.setDocument(1);
-        assertThat(0, equalTo(doubleValues.count()));
-
-        doubleValues.setDocument(2);
-        assertThat(1, equalTo(doubleValues.count()));
-        assertThat(doubleValues.valueAt(0), equalTo(3d));
-
-        IndexSearcher searcher = new IndexSearcher(readerContext.reader());
-        TopFieldDocs topDocs;
-
-        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
-                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.MIN, null)))); // defaults to _last
-        assertThat(topDocs.totalHits, equalTo(3));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(0));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(2));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(1));
-
-        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
-                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.MAX, null), true))); // defaults to _last
-        assertThat(topDocs.totalHits, equalTo(3));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(2));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(0));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(1));
-
-        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
-                new Sort(new SortField("value", indexFieldData.comparatorSource("_first", MultiValueMode.MIN, null))));
-        assertThat(topDocs.totalHits, equalTo(3));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(1));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(0));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(2));
-
-        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
-                new Sort(new SortField("value", indexFieldData.comparatorSource("_first", MultiValueMode.MAX, null), true)));
-        assertThat(topDocs.totalHits, equalTo(3));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(1));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(2));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(0));
-
-        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
-                new Sort(new SortField("value", indexFieldData.comparatorSource("1", MultiValueMode.MIN, null))));
-        assertThat(topDocs.totalHits, equalTo(3));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(1));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(0));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(2));
-
-        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
-                new Sort(new SortField("value", indexFieldData.comparatorSource("1", MultiValueMode.MAX, null), true)));
-        assertThat(topDocs.totalHits, equalTo(3));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(2));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(0));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(1));
-    }
-
-    @Test
-    public void testMultiValueAllSetNumber() throws Exception {
-        fillMultiValueAllSet();
-        IndexNumericFieldData indexFieldData = getForField("value");
-        AtomicNumericFieldData fieldData = indexFieldData.load(refreshReader());
-
-        SortedNumericDocValues longValues = fieldData.getLongValues();
-
-        assertThat(FieldData.isMultiValued(longValues), equalTo(true));
-
-        longValues.setDocument(0);
-        assertThat(longValues.count(), equalTo(2));
-        assertThat(longValues.valueAt(0), equalTo(2l));
-        assertThat(longValues.valueAt(1), equalTo(4l));
-
-        longValues.setDocument(1);
-        assertThat(longValues.count(), equalTo(1));
-        assertThat(longValues.valueAt(0), equalTo(1l));
-
-        longValues.setDocument(2);
-        assertThat(longValues.count(), equalTo(1));
-        assertThat(longValues.valueAt(0), equalTo(3l));
-
-        SortedNumericDoubleValues doubleValues = fieldData.getDoubleValues();
-
-        assertThat(FieldData.isMultiValued(doubleValues), equalTo(true));
-
-        doubleValues.setDocument(0);
-        assertThat(2, equalTo(doubleValues.count()));
-        assertThat(doubleValues.valueAt(0), equalTo(2d));
-        assertThat(doubleValues.valueAt(1), equalTo(4d));
-
-        doubleValues.setDocument(1);
-        assertThat(1, equalTo(doubleValues.count()));
-        assertThat(doubleValues.valueAt(0), equalTo(1d));
-
-        doubleValues.setDocument(2);
-        assertThat(1, equalTo(doubleValues.count()));
-        assertThat(doubleValues.valueAt(0), equalTo(3d));
-    }
-
-    @Test
-    public void testMultiValueWithMissingNumber() throws Exception {
-        fillMultiValueWithMissing();
-        IndexNumericFieldData indexFieldData = getForField("value");
-        AtomicNumericFieldData fieldData = indexFieldData.load(refreshReader());
-
-        SortedNumericDocValues longValues = fieldData.getLongValues();
-
-        assertThat(FieldData.isMultiValued(longValues), equalTo(true));
-
-        longValues.setDocument(0);
-        assertThat(longValues.count(), equalTo(2));
-        assertThat(longValues.valueAt(0), equalTo(2l));
-        assertThat(longValues.valueAt(1), equalTo(4l));
-
-        longValues.setDocument(1);
-        assertThat(longValues.count(), equalTo(0));
-
-        longValues.setDocument(2);
-        assertThat(longValues.count(), equalTo(1));
-        assertThat(longValues.valueAt(0), equalTo(3l));
-
-        SortedNumericDoubleValues doubleValues = fieldData.getDoubleValues();
-
-        assertThat(FieldData.isMultiValued(doubleValues), equalTo(true));
-
-        doubleValues.setDocument(0);
-        assertThat(2, equalTo(doubleValues.count()));
-        assertThat(doubleValues.valueAt(0), equalTo(2d));
-        assertThat(doubleValues.valueAt(1), equalTo(4d));
-
-        doubleValues.setDocument(1);
-        assertThat(0, equalTo(doubleValues.count()));
-
-        doubleValues.setDocument(2);
-        assertThat(1, equalTo(doubleValues.count()));
-        assertThat(doubleValues.valueAt(0), equalTo(3d));
-
-    }
-
-    @Override
-    @Test
-    public void testMissingValueForAll() throws Exception {
-        fillAllMissing();
-        IndexNumericFieldData indexFieldData = getForField("value");
-        AtomicNumericFieldData fieldData = indexFieldData.load(refreshReader());
-
-        // long values
-
-        SortedNumericDocValues longValues = fieldData.getLongValues();
-
-        assertThat(FieldData.isMultiValued(longValues), equalTo(false));
-
-        for (int i = 0; i < 3; ++i) {
-            longValues.setDocument(0);
-            assertThat(longValues.count(), equalTo(0));
-        }
-
-        // double values
-
-        SortedNumericDoubleValues doubleValues = fieldData.getDoubleValues();
-
-        assertThat(FieldData.isMultiValued(doubleValues), equalTo(false));
-
-        doubleValues.setDocument(0);
-        assertThat(0, equalTo(doubleValues.count()));
-
-        doubleValues.setDocument(1);
-        assertThat(0, equalTo(doubleValues.count()));
-
-        doubleValues.setDocument(2);
-        assertThat(0, equalTo(doubleValues.count()));
-    }
-
-
-    @Override
-    protected void fillAllMissing() throws Exception {
-        Document d = new Document();
-        d.add(new StringField("_id", "1", Field.Store.NO));
-        writer.addDocument(d);
-
-        d = new Document();
-        d.add(new StringField("_id", "2", Field.Store.NO));
-        writer.addDocument(d);
-
-        d = new Document();
-        d.add(new StringField("_id", "3", Field.Store.NO));
-        writer.addDocument(d);
-    }
-
-    @Override
-    @Test
-    public void testSortMultiValuesFields() throws Exception {
-        fillExtendedMvSet();
-        IndexFieldData indexFieldData = getForField("value");
-
-        IndexSearcher searcher = new IndexSearcher(DirectoryReader.open(writer, true));
-        TopFieldDocs topDocs = searcher.search(new MatchAllDocsQuery(), 10,
-                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.MIN, null)))); // defaults to _last
-        assertThat(topDocs.totalHits, equalTo(8));
-        assertThat(topDocs.scoreDocs.length, equalTo(8));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(7));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).intValue(), equalTo(-10));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(0));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).intValue(), equalTo(2));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(2));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).intValue(), equalTo(3));
-        assertThat(topDocs.scoreDocs[3].doc, equalTo(3));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).intValue(), equalTo(4));
-        assertThat(topDocs.scoreDocs[4].doc, equalTo(4));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).intValue(), equalTo(6));
-        assertThat(topDocs.scoreDocs[5].doc, equalTo(6));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[5]).fields[0]).intValue(), equalTo(8));
-        assertThat(topDocs.scoreDocs[6].doc, equalTo(1));
-//        assertThat(((FieldDoc) topDocs.scoreDocs[6]).fields[0], equalTo(null));
-        assertThat(topDocs.scoreDocs[7].doc, equalTo(5));
-//        assertThat(((FieldDoc) topDocs.scoreDocs[7]).fields[0], equalTo(null));
-
-        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
-                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.MAX, null), true))); // defaults to _last
-        assertThat(topDocs.totalHits, equalTo(8));
-        assertThat(topDocs.scoreDocs.length, equalTo(8));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(6));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).intValue(), equalTo(10));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(4));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).intValue(), equalTo(8));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(3));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).intValue(), equalTo(6));
-        assertThat(topDocs.scoreDocs[3].doc, equalTo(0));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).intValue(), equalTo(4));
-        assertThat(topDocs.scoreDocs[4].doc, equalTo(2));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).intValue(), equalTo(3));
-        assertThat(topDocs.scoreDocs[5].doc, equalTo(7));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[5]).fields[0]).intValue(), equalTo(-8));
-        assertThat(topDocs.scoreDocs[6].doc, equalTo(1));
-//        assertThat(((FieldDoc) topDocs.scoreDocs[6]).fields[0], equalTo(null));
-        assertThat(topDocs.scoreDocs[7].doc, equalTo(5));
-//        assertThat(((FieldDoc) topDocs.scoreDocs[7]).fields[0], equalTo(null));
-
-        searcher = new IndexSearcher(DirectoryReader.open(writer, true));
-        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
-                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.SUM, null)))); // defaults to _last
-        assertThat(topDocs.totalHits, equalTo(8));
-        assertThat(topDocs.scoreDocs.length, equalTo(8));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(7));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).intValue(), equalTo(-27));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(2));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).intValue(), equalTo(3));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(0));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).intValue(), equalTo(6));
-        assertThat(topDocs.scoreDocs[3].doc, equalTo(3));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).intValue(), equalTo(15));
-        assertThat(topDocs.scoreDocs[4].doc, equalTo(4));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).intValue(), equalTo(21));
-        assertThat(topDocs.scoreDocs[5].doc, equalTo(6));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[5]).fields[0]).intValue(), equalTo(27));
-        assertThat(topDocs.scoreDocs[6].doc, equalTo(1));
-//        assertThat(((FieldDoc) topDocs.scoreDocs[6]).fields[0], equalTo(null));
-        assertThat(topDocs.scoreDocs[7].doc, equalTo(5));
-//        assertThat(((FieldDoc) topDocs.scoreDocs[7]).fields[0], equalTo(null));
-
-        searcher = new IndexSearcher(DirectoryReader.open(writer, true));
-        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
-                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.SUM, null), true))); // defaults to _last
-        assertThat(topDocs.totalHits, equalTo(8));
-        assertThat(topDocs.scoreDocs.length, equalTo(8));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(6));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).intValue(), equalTo(27));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(4));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).intValue(), equalTo(21));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(3));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).intValue(), equalTo(15));
-        assertThat(topDocs.scoreDocs[3].doc, equalTo(0));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).intValue(), equalTo(6));
-        assertThat(topDocs.scoreDocs[4].doc, equalTo(2));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).intValue(), equalTo(3));
-        assertThat(topDocs.scoreDocs[5].doc, equalTo(7));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[5]).fields[0]).intValue(), equalTo(-27));
-        assertThat(topDocs.scoreDocs[6].doc, equalTo(1));
-//        assertThat(((FieldDoc) topDocs.scoreDocs[6]).fields[0], equalTo(null));
-        assertThat(topDocs.scoreDocs[7].doc, equalTo(5));
-//        assertThat(((FieldDoc) topDocs.scoreDocs[7]).fields[0], equalTo(null));
-
-        searcher = new IndexSearcher(DirectoryReader.open(writer, true));
-        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
-                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.AVG, null)))); // defaults to _last
-        assertThat(topDocs.totalHits, equalTo(8));
-        assertThat(topDocs.scoreDocs.length, equalTo(8));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(7));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).intValue(), equalTo(-9));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(0));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).intValue(), equalTo(3));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(2));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).intValue(), equalTo(3));
-        assertThat(topDocs.scoreDocs[3].doc, equalTo(3));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).intValue(), equalTo(5));
-        assertThat(topDocs.scoreDocs[4].doc, equalTo(4));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).intValue(), equalTo(7));
-        assertThat(topDocs.scoreDocs[5].doc, equalTo(6));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[5]).fields[0]).intValue(), equalTo(9));
-        assertThat(topDocs.scoreDocs[6].doc, equalTo(1));
-//        assertThat(((FieldDoc) topDocs.scoreDocs[6]).fields[0], equalTo(null));
-        assertThat(topDocs.scoreDocs[7].doc, equalTo(5));
-//        assertThat(((FieldDoc) topDocs.scoreDocs[7]).fields[0], equalTo(null));
-
-        searcher = new IndexSearcher(DirectoryReader.open(writer, true));
-        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
-                new Sort(new SortField("value", indexFieldData.comparatorSource(null, MultiValueMode.AVG, null), true))); // defaults to _last
-        assertThat(topDocs.totalHits, equalTo(8));
-        assertThat(topDocs.scoreDocs.length, equalTo(8));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(6));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).intValue(), equalTo(9));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(4));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).intValue(), equalTo(7));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(3));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).intValue(), equalTo(5));
-        assertThat(topDocs.scoreDocs[3].doc, equalTo(0));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).intValue(), equalTo(3));
-        assertThat(topDocs.scoreDocs[4].doc, equalTo(2));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).intValue(), equalTo(3));
-        assertThat(topDocs.scoreDocs[5].doc, equalTo(7));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[5]).fields[0]).intValue(), equalTo(-9));
-        assertThat(topDocs.scoreDocs[6].doc, equalTo(1));
-//        assertThat(((FieldDoc) topDocs.scoreDocs[6]).fields[0], equalTo(null));
-        assertThat(topDocs.scoreDocs[7].doc, equalTo(5));
-//        assertThat(((FieldDoc) topDocs.scoreDocs[7]).fields[0], equalTo(null));
-
-        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
-                new Sort(new SortField("value", indexFieldData.comparatorSource("_first", MultiValueMode.MIN, null))));
-        assertThat(topDocs.totalHits, equalTo(8));
-        assertThat(topDocs.scoreDocs.length, equalTo(8));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(1));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(5));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(7));
-        assertThat(topDocs.scoreDocs[3].doc, equalTo(0));
-        assertThat(topDocs.scoreDocs[4].doc, equalTo(2));
-        assertThat(topDocs.scoreDocs[5].doc, equalTo(3));
-        assertThat(topDocs.scoreDocs[6].doc, equalTo(4));
-        assertThat(topDocs.scoreDocs[7].doc, equalTo(6));
-
-        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
-                new Sort(new SortField("value", indexFieldData.comparatorSource("_first", MultiValueMode.MAX, null), true)));
-        assertThat(topDocs.totalHits, equalTo(8));
-        assertThat(topDocs.scoreDocs.length, equalTo(8));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(1));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(5));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(6));
-        assertThat(topDocs.scoreDocs[3].doc, equalTo(4));
-        assertThat(topDocs.scoreDocs[4].doc, equalTo(3));
-        assertThat(topDocs.scoreDocs[5].doc, equalTo(0));
-        assertThat(topDocs.scoreDocs[6].doc, equalTo(2));
-        assertThat(topDocs.scoreDocs[7].doc, equalTo(7));
-
-        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
-                new Sort(new SortField("value", indexFieldData.comparatorSource("-9", MultiValueMode.MIN, null))));
-        assertThat(topDocs.totalHits, equalTo(8));
-        assertThat(topDocs.scoreDocs.length, equalTo(8));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(7));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(1));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(5));
-        assertThat(topDocs.scoreDocs[3].doc, equalTo(0));
-        assertThat(topDocs.scoreDocs[4].doc, equalTo(2));
-        assertThat(topDocs.scoreDocs[5].doc, equalTo(3));
-        assertThat(topDocs.scoreDocs[6].doc, equalTo(4));
-        assertThat(topDocs.scoreDocs[7].doc, equalTo(6));
-
-        topDocs = searcher.search(new MatchAllDocsQuery(), 10,
-                new Sort(new SortField("value", indexFieldData.comparatorSource("9", MultiValueMode.MAX, null), true)));
-        assertThat(topDocs.totalHits, equalTo(8));
-        assertThat(topDocs.scoreDocs.length, equalTo(8));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(6));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(1));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(5));
-        assertThat(topDocs.scoreDocs[3].doc, equalTo(4));
-        assertThat(topDocs.scoreDocs[4].doc, equalTo(3));
-        assertThat(topDocs.scoreDocs[5].doc, equalTo(0));
-        assertThat(topDocs.scoreDocs[6].doc, equalTo(2));
-        assertThat(topDocs.scoreDocs[7].doc, equalTo(7));
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/index/fielddata/AbstractStringFieldDataTestCase.java b/core/src/test/java/org/elasticsearch/index/fielddata/AbstractStringFieldDataTestCase.java
new file mode 100644
index 0000000..f17f20d
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/fielddata/AbstractStringFieldDataTestCase.java
@@ -0,0 +1,628 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.fielddata;
+
+import com.carrotsearch.randomizedtesting.generators.RandomPicks;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.Field.Store;
+import org.apache.lucene.document.SortedSetDocValuesField;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.index.RandomAccessOrds;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.search.Filter;
+import org.apache.lucene.search.FilteredQuery;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.QueryWrapperFilter;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.SortField;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopFieldDocs;
+import org.apache.lucene.search.join.BitDocIdSetCachingWrapperFilter;
+import org.apache.lucene.search.join.ScoreMode;
+import org.apache.lucene.search.join.ToParentBlockJoinQuery;
+import org.apache.lucene.util.Accountable;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.TestUtil;
+import org.apache.lucene.util.UnicodeUtil;
+import org.elasticsearch.common.lucene.search.Queries;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.fielddata.IndexFieldData.XFieldComparatorSource;
+import org.elasticsearch.index.fielddata.IndexFieldData.XFieldComparatorSource.Nested;
+import org.elasticsearch.index.fielddata.fieldcomparator.BytesRefFieldComparatorSource;
+import org.elasticsearch.index.fielddata.ordinals.GlobalOrdinalsIndexFieldData;
+import org.elasticsearch.search.MultiValueMode;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.instanceOf;
+import static org.hamcrest.Matchers.is;
+import static org.hamcrest.Matchers.not;
+import static org.hamcrest.Matchers.nullValue;
+import static org.hamcrest.Matchers.sameInstance;
+
+/**
+ */
+public abstract class AbstractStringFieldDataTestCase extends AbstractFieldDataImplTestCase {
+
+    private void addField(Document d, String name, String value) {
+        d.add(new StringField(name, value, Field.Store.YES));
+        d.add(new SortedSetDocValuesField(name, new BytesRef(value)));
+    }
+
+    @Override
+    protected void fillSingleValueAllSet() throws Exception {
+        Document d = new Document();
+        addField(d, "_id", "1");
+        addField(d, "value", "2");
+        writer.addDocument(d);
+
+        d = new Document();
+        addField(d, "_id", "1");
+        addField(d, "value", "1");
+        writer.addDocument(d);
+
+        d = new Document();
+        addField(d, "_id", "3");
+        addField(d, "value", "3");
+        writer.addDocument(d);
+    }
+
+    @Override
+    protected void add2SingleValuedDocumentsAndDeleteOneOfThem() throws Exception {
+        Document d = new Document();
+        addField(d, "_id", "1");
+        addField(d, "value", "2");
+        writer.addDocument(d);
+
+        d = new Document();
+        addField(d, "_id", "2");
+        addField(d, "value", "4");
+        writer.addDocument(d);
+
+        writer.commit();
+
+        writer.deleteDocuments(new Term("_id", "1"));
+    }
+
+    @Override
+    protected void fillSingleValueWithMissing() throws Exception {
+        Document d = new Document();
+        addField(d, "_id", "1");
+        addField(d, "value", "2");
+        writer.addDocument(d);
+
+        d = new Document();
+        addField(d, "_id", "2");
+        //d.add(new StringField("value", one(), Field.Store.NO)); // MISSING....
+        writer.addDocument(d);
+
+        d = new Document();
+        addField(d, "_id", "3");
+        addField(d, "value", "3");
+        writer.addDocument(d);
+    }
+
+    @Override
+    protected void fillMultiValueAllSet() throws Exception {
+        Document d = new Document();
+        addField(d, "_id", "1");
+        addField(d, "value", "2");
+        addField(d, "value", "4");
+        writer.addDocument(d);
+
+        d = new Document();
+        addField(d, "_id", "2");
+        addField(d, "value", "1");
+        writer.addDocument(d);
+        writer.commit(); // TODO: Have tests with more docs for sorting
+
+        d = new Document();
+        addField(d, "_id", "3");
+        addField(d, "value", "3");
+        writer.addDocument(d);
+    }
+
+    @Override
+    protected void fillMultiValueWithMissing() throws Exception {
+        Document d = new Document();
+        addField(d, "_id", "1");
+        addField(d, "value", "2");
+        addField(d, "value", "4");
+        writer.addDocument(d);
+
+        d = new Document();
+        addField(d, "_id", "2");
+        //d.add(new StringField("value", one(), Field.Store.NO)); // MISSING
+        writer.addDocument(d);
+
+        d = new Document();
+        addField(d, "_id", "3");
+        addField(d, "value", "3");
+        writer.addDocument(d);
+    }
+
+    @Override
+    protected void fillAllMissing() throws Exception {
+        Document d = new Document();
+        addField(d, "_id", "1");
+        writer.addDocument(d);
+
+        d = new Document();
+        addField(d, "_id", "2");
+        writer.addDocument(d);
+
+        d = new Document();
+        addField(d, "_id", "3");
+        writer.addDocument(d);
+    }
+
+    @Override
+    protected void fillExtendedMvSet() throws Exception {
+        Document d = new Document();
+        addField(d, "_id", "1");
+        addField(d, "value", "02");
+        addField(d, "value", "04");
+        writer.addDocument(d);
+
+        d = new Document();
+        addField(d, "_id", "2");
+        writer.addDocument(d);
+
+        d = new Document();
+        addField(d, "_id", "3");
+        addField(d, "value", "03");
+        writer.addDocument(d);
+        writer.commit();
+
+        d = new Document();
+        addField(d, "_id", "4");
+        addField(d, "value", "04");
+        addField(d, "value", "05");
+        addField(d, "value", "06");
+        writer.addDocument(d);
+
+        d = new Document();
+        addField(d, "_id", "5");
+        addField(d, "value", "06");
+        addField(d, "value", "07");
+        addField(d, "value", "08");
+        writer.addDocument(d);
+
+        d = new Document();
+        d.add(new StringField("_id", "6", Field.Store.NO));
+        writer.addDocument(d);
+
+        d = new Document();
+        addField(d, "_id", "7");
+        addField(d, "value", "08");
+        addField(d, "value", "09");
+        addField(d, "value", "10");
+        writer.addDocument(d);
+        writer.commit();
+
+        d = new Document();
+        addField(d, "_id", "8");
+        addField(d, "value", "!08");
+        addField(d, "value", "!09");
+        addField(d, "value", "!10");
+        writer.addDocument(d);
+    }
+
+    public void testActualMissingValue() throws IOException {
+        testActualMissingValue(false);
+    }
+
+    public void testActualMissingValueReverse() throws IOException {
+        testActualMissingValue(true);
+    }
+
+    public void testActualMissingValue(boolean reverse) throws IOException {
+        // missing value is set to an actual value
+        final String[] values = new String[randomIntBetween(2, 30)];
+        for (int i = 1; i < values.length; ++i) {
+            values[i] = TestUtil.randomUnicodeString(getRandom());
+        }
+        final int numDocs = scaledRandomIntBetween(10, 3072);
+        for (int i = 0; i < numDocs; ++i) {
+            final String value = RandomPicks.randomFrom(getRandom(), values);
+            if (value == null) {
+                writer.addDocument(new Document());
+            } else {
+                Document d = new Document();
+                addField(d, "value", value);
+                writer.addDocument(d);
+            }
+            if (randomInt(10) == 0) {
+                writer.commit();
+            }
+        }
+
+        final IndexFieldData indexFieldData = getForField("value");
+        final String missingValue = values[1];
+        IndexSearcher searcher = new IndexSearcher(DirectoryReader.open(writer, true));
+        XFieldComparatorSource comparator = indexFieldData.comparatorSource(missingValue, MultiValueMode.MIN, null);
+        TopFieldDocs topDocs = searcher.search(new MatchAllDocsQuery(), randomBoolean() ? numDocs : randomIntBetween(10, numDocs), new Sort(new SortField("value", comparator, reverse)));
+        assertEquals(numDocs, topDocs.totalHits);
+        BytesRef previousValue = reverse ? UnicodeUtil.BIG_TERM : new BytesRef();
+        for (int i = 0; i < topDocs.scoreDocs.length; ++i) {
+            final String docValue = searcher.doc(topDocs.scoreDocs[i].doc).get("value");
+            final BytesRef value = new BytesRef(docValue == null ? missingValue : docValue);
+            if (reverse) {
+                assertTrue(previousValue.compareTo(value) >= 0);
+            } else {
+                assertTrue(previousValue.compareTo(value) <= 0);
+            }
+            previousValue = value;
+        }
+        searcher.getIndexReader().close();
+    }
+
+    public void testSortMissingFirst() throws IOException {
+        testSortMissing(true, false);
+    }
+
+    public void testSortMissingFirstReverse() throws IOException {
+        testSortMissing(true, true);
+    }
+
+    public void testSortMissingLast() throws IOException {
+        testSortMissing(false, false);
+    }
+
+    public void testSortMissingLastReverse() throws IOException {
+        testSortMissing(false, true);
+    }
+
+    public void testSortMissing(boolean first, boolean reverse) throws IOException {
+        final String[] values = new String[randomIntBetween(2, 10)];
+        for (int i = 1; i < values.length; ++i) {
+            values[i] = TestUtil.randomUnicodeString(getRandom());
+        }
+        final int numDocs = scaledRandomIntBetween(10, 3072);
+        for (int i = 0; i < numDocs; ++i) {
+            final String value = RandomPicks.randomFrom(getRandom(), values);
+            if (value == null) {
+                writer.addDocument(new Document());
+            } else {
+                Document d = new Document();
+                addField(d, "value", value);
+                writer.addDocument(d);
+            }
+            if (randomInt(10) == 0) {
+                writer.commit();
+            }
+        }
+        final IndexFieldData indexFieldData = getForField("value");
+        IndexSearcher searcher = new IndexSearcher(DirectoryReader.open(writer, true));
+        XFieldComparatorSource comparator = indexFieldData.comparatorSource(first ? "_first" : "_last", MultiValueMode.MIN, null);
+        TopFieldDocs topDocs = searcher.search(new MatchAllDocsQuery(), randomBoolean() ? numDocs : randomIntBetween(10, numDocs), new Sort(new SortField("value", comparator, reverse)));
+        assertEquals(numDocs, topDocs.totalHits);
+        BytesRef previousValue = first ? null : reverse ? UnicodeUtil.BIG_TERM : new BytesRef();
+        for (int i = 0; i < topDocs.scoreDocs.length; ++i) {
+            final String docValue = searcher.doc(topDocs.scoreDocs[i].doc).get("value");
+            if (first && docValue == null) {
+                assertNull(previousValue);
+            } else if (!first && docValue != null) {
+                assertNotNull(previousValue);
+            }
+            final BytesRef value = docValue == null ? null : new BytesRef(docValue);
+            if (previousValue != null && value != null) {
+                if (reverse) {
+                    assertTrue(previousValue.compareTo(value) >= 0);
+                } else {
+                    assertTrue(previousValue.compareTo(value) <= 0);
+                }
+            }
+            previousValue = value;
+        }
+        searcher.getIndexReader().close();
+    }
+
+    public void testNestedSortingMin() throws IOException {
+        testNestedSorting(MultiValueMode.MIN);
+    }
+
+    public void testNestedSortingMax() throws IOException {
+        testNestedSorting(MultiValueMode.MAX);
+    }
+
+    public void testNestedSorting(MultiValueMode sortMode) throws IOException {
+        final String[] values = new String[randomIntBetween(2, 20)];
+        for (int i = 0; i < values.length; ++i) {
+            values[i] = TestUtil.randomSimpleString(getRandom());
+        }
+        final int numParents = scaledRandomIntBetween(10, 3072);
+        List<Document> docs = new ArrayList<>();
+        FixedBitSet parents = new FixedBitSet(64);
+        for (int i = 0; i < numParents; ++i) {
+            docs.clear();
+            final int numChildren = randomInt(4);
+            for (int j = 0; j < numChildren; ++j) {
+                final Document child = new Document();
+                final int numValues = randomInt(3);
+                for (int k = 0; k < numValues; ++k) {
+                    final String value = RandomPicks.randomFrom(getRandom(), values);
+                    addField(child, "text", value);
+                }
+                docs.add(child);
+            }
+            final Document parent = new Document();
+            parent.add(new StringField("type", "parent", Store.YES));
+            final String value = RandomPicks.randomFrom(getRandom(), values);
+            if (value != null) {
+                addField(parent, "text", value);
+            }
+            docs.add(parent);
+            int bit = parents.prevSetBit(parents.length() - 1) + docs.size();
+            parents = FixedBitSet.ensureCapacity(parents, bit);
+            parents.set(bit);
+            writer.addDocuments(docs);
+            if (randomInt(10) == 0) {
+                writer.commit();
+            }
+        }
+        IndexSearcher searcher = new IndexSearcher(DirectoryReader.open(writer, true));
+        IndexFieldData<?> fieldData = getForField("text");
+        final Object missingValue;
+        switch (randomInt(4)) {
+        case 0:
+            missingValue = "_first";
+            break;
+        case 1:
+            missingValue = "_last";
+            break;
+        case 2:
+            missingValue = new BytesRef(RandomPicks.randomFrom(getRandom(), values));
+            break;
+        default:
+            missingValue = new BytesRef(TestUtil.randomSimpleString(getRandom()));
+            break;
+        }
+        Filter parentFilter = new QueryWrapperFilter(new TermQuery(new Term("type", "parent")));
+        Filter childFilter = new QueryWrapperFilter(Queries.not(parentFilter));
+        Nested nested = createNested(parentFilter, childFilter);
+        BytesRefFieldComparatorSource nestedComparatorSource = new BytesRefFieldComparatorSource(fieldData, missingValue, sortMode, nested);
+        ToParentBlockJoinQuery query = new ToParentBlockJoinQuery(new FilteredQuery(new MatchAllDocsQuery(), childFilter), new BitDocIdSetCachingWrapperFilter(parentFilter), ScoreMode.None);
+        Sort sort = new Sort(new SortField("text", nestedComparatorSource));
+        TopFieldDocs topDocs = searcher.search(query, randomIntBetween(1, numParents), sort);
+        assertTrue(topDocs.scoreDocs.length > 0);
+        BytesRef previous = null;
+        for (int i = 0; i < topDocs.scoreDocs.length; ++i) {
+            final int docID = topDocs.scoreDocs[i].doc;
+            assertTrue("expected " + docID + " to be a parent", parents.get(docID));
+            BytesRef cmpValue = null;
+            for (int child = parents.prevSetBit(docID - 1) + 1; child < docID; ++child) {
+                String[] sVals = searcher.doc(child).getValues("text");
+                final BytesRef[] vals;
+                if (sVals.length == 0) {
+                    vals = new BytesRef[0];
+                } else {
+                    vals = new BytesRef[sVals.length];
+                    for (int j = 0; j < vals.length; ++j) {
+                        vals[j] = new BytesRef(sVals[j]);
+                    }
+                }
+                for (BytesRef value : vals) {
+                    if (cmpValue == null) {
+                        cmpValue = value;
+                    } else if (sortMode == MultiValueMode.MIN && value.compareTo(cmpValue) < 0) {
+                        cmpValue = value;
+                    } else if (sortMode == MultiValueMode.MAX && value.compareTo(cmpValue) > 0) {
+                        cmpValue = value;
+                    }
+                }
+            }
+            if (cmpValue == null) {
+                if ("_first".equals(missingValue)) {
+                    cmpValue = new BytesRef();
+                } else if ("_last".equals(missingValue) == false) {
+                    cmpValue = (BytesRef) missingValue;
+                }
+            }
+            if (previous != null && cmpValue != null) {
+                assertTrue(previous.utf8ToString() + "   /   " + cmpValue.utf8ToString(), previous.compareTo(cmpValue) <= 0);
+            }
+            previous = cmpValue;
+        }
+        searcher.getIndexReader().close();
+    }
+
+    private void assertIteratorConsistentWithRandomAccess(RandomAccessOrds ords, int maxDoc) {
+        for (int doc = 0; doc < maxDoc; ++doc) {
+            ords.setDocument(doc);
+            final int cardinality = ords.cardinality();
+            for (int i = 0; i < cardinality; ++i) {
+                assertEquals(ords.nextOrd(), ords.ordAt(i));
+            }
+            for (int i = 0; i < 3; ++i) {
+                assertEquals(ords.nextOrd(), -1);
+            }
+        }
+    }
+
+    @Test
+    public void testGlobalOrdinals() throws Exception {
+        fillExtendedMvSet();
+        refreshReader();
+        FieldDataType fieldDataType = new FieldDataType("string", Settings.builder().put("global_values", "fixed"));
+        IndexOrdinalsFieldData ifd = getForField(fieldDataType, "value", hasDocValues());
+        IndexOrdinalsFieldData globalOrdinals = ifd.loadGlobal(topLevelReader);
+        assertThat(topLevelReader.leaves().size(), equalTo(3));
+
+        // First segment
+        assertThat(globalOrdinals, instanceOf(GlobalOrdinalsIndexFieldData.class));
+        LeafReaderContext leaf = topLevelReader.leaves().get(0);
+        AtomicOrdinalsFieldData afd = globalOrdinals.load(leaf);
+        RandomAccessOrds values = afd.getOrdinalsValues();
+        assertIteratorConsistentWithRandomAccess(values, leaf.reader().maxDoc());
+        values.setDocument(0);
+        assertThat(values.cardinality(), equalTo(2));
+        long ord = values.nextOrd();
+        assertThat(ord, equalTo(3l));
+        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("02"));
+        ord = values.nextOrd();
+        assertThat(ord, equalTo(5l));
+        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("04"));
+        values.setDocument(1);
+        assertThat(values.cardinality(), equalTo(0));
+        values.setDocument(2);
+        assertThat(values.cardinality(), equalTo(1));
+        ord = values.nextOrd();
+        assertThat(ord, equalTo(4l));
+        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("03"));
+
+        // Second segment
+        leaf = topLevelReader.leaves().get(1);
+        afd = globalOrdinals.load(leaf);
+        values = afd.getOrdinalsValues();
+        assertIteratorConsistentWithRandomAccess(values, leaf.reader().maxDoc());
+        values.setDocument(0);
+        assertThat(values.cardinality(), equalTo(3));
+        ord = values.nextOrd();
+        assertThat(ord, equalTo(5l));
+        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("04"));
+        ord = values.nextOrd();
+        assertThat(ord, equalTo(6l));
+        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("05"));
+        ord = values.nextOrd();
+        assertThat(ord, equalTo(7l));
+        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("06"));
+        values.setDocument(1);
+        assertThat(values.cardinality(), equalTo(3));
+        ord = values.nextOrd();
+        assertThat(ord, equalTo(7l));
+        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("06"));
+        ord = values.nextOrd();
+        assertThat(ord, equalTo(8l));
+        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("07"));
+        ord = values.nextOrd();
+        assertThat(ord, equalTo(9l));
+        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("08"));
+        values.setDocument(2);
+        assertThat(values.cardinality(), equalTo(0));
+        values.setDocument(3);
+        assertThat(values.cardinality(), equalTo(3));
+        ord = values.nextOrd();
+        assertThat(ord, equalTo(9l));
+        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("08"));
+        ord = values.nextOrd();
+        assertThat(ord, equalTo(10l));
+        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("09"));
+        ord = values.nextOrd();
+        assertThat(ord, equalTo(11l));
+        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("10"));
+
+        // Third segment
+        leaf = topLevelReader.leaves().get(2);
+        afd = globalOrdinals.load(leaf);
+        values = afd.getOrdinalsValues();
+        assertIteratorConsistentWithRandomAccess(values, leaf.reader().maxDoc());
+        values.setDocument(0);
+        values.setDocument(0);
+        assertThat(values.cardinality(), equalTo(3));
+        ord = values.nextOrd();
+        assertThat(ord, equalTo(0l));
+        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("!08"));
+        ord = values.nextOrd();
+        assertThat(ord, equalTo(1l));
+        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("!09"));
+        ord = values.nextOrd();
+        assertThat(ord, equalTo(2l));
+        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("!10"));
+    }
+
+    @Test
+    public void testTermsEnum() throws Exception {
+        fillExtendedMvSet();
+        LeafReaderContext atomicReaderContext = refreshReader();
+
+        IndexOrdinalsFieldData ifd = getForField("value");
+        AtomicOrdinalsFieldData afd = ifd.load(atomicReaderContext);
+
+        TermsEnum termsEnum = afd.getOrdinalsValues().termsEnum();
+        int size = 0;
+        while (termsEnum.next() != null) {
+            size++;
+        }
+        assertThat(size, equalTo(12));
+
+        assertThat(termsEnum.seekExact(new BytesRef("10")), is(true));
+        assertThat(termsEnum.term().utf8ToString(), equalTo("10"));
+        assertThat(termsEnum.next(), nullValue());
+
+        assertThat(termsEnum.seekExact(new BytesRef("08")), is(true));
+        assertThat(termsEnum.term().utf8ToString(), equalTo("08"));
+        size = 0;
+        while (termsEnum.next() != null) {
+            size++;
+        }
+        assertThat(size, equalTo(2));
+
+        termsEnum.seekExact(8);
+        assertThat(termsEnum.term().utf8ToString(), equalTo("07"));
+        size = 0;
+        while (termsEnum.next() != null) {
+            size++;
+        }
+        assertThat(size, equalTo(3));
+    }
+
+    @Test
+    public void testGlobalOrdinalsGetRemovedOnceIndexReaderCloses() throws Exception {
+        fillExtendedMvSet();
+        refreshReader();
+        FieldDataType fieldDataType = new FieldDataType("string", Settings.builder().put("global_values", "fixed").put("cache", "node"));
+        IndexOrdinalsFieldData ifd = getForField(fieldDataType, "value", hasDocValues());
+        IndexOrdinalsFieldData globalOrdinals = ifd.loadGlobal(topLevelReader);
+        assertThat(ifd.loadGlobal(topLevelReader), sameInstance(globalOrdinals));
+        // 3 b/c 1 segment level caches and 1 top level cache
+        // in case of doc values, we don't cache atomic FD, so only the top-level cache is there
+        assertThat(indicesFieldDataCache.getCache().size(), equalTo(hasDocValues() ? 1L : 4L));
+
+        IndexOrdinalsFieldData cachedInstance = null;
+        for (Accountable ramUsage : indicesFieldDataCache.getCache().asMap().values()) {
+            if (ramUsage instanceof IndexOrdinalsFieldData) {
+                cachedInstance = (IndexOrdinalsFieldData) ramUsage;
+                break;
+            }
+        }
+        assertThat(cachedInstance, sameInstance(globalOrdinals));
+        topLevelReader.close();
+        // Now only 3 segment level entries, only the toplevel reader has been closed, but the segment readers are still used by IW
+        assertThat(indicesFieldDataCache.getCache().size(), equalTo(hasDocValues() ? 0L : 3L));
+
+        refreshReader();
+        assertThat(ifd.loadGlobal(topLevelReader), not(sameInstance(globalOrdinals)));
+
+        ifdService.clear();
+        assertThat(indicesFieldDataCache.getCache().size(), equalTo(0l));
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/index/fielddata/AbstractStringFieldDataTests.java b/core/src/test/java/org/elasticsearch/index/fielddata/AbstractStringFieldDataTests.java
deleted file mode 100644
index 8fcc949..0000000
--- a/core/src/test/java/org/elasticsearch/index/fielddata/AbstractStringFieldDataTests.java
+++ /dev/null
@@ -1,628 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.fielddata;
-
-import com.carrotsearch.randomizedtesting.generators.RandomPicks;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.Field.Store;
-import org.apache.lucene.document.SortedSetDocValuesField;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.index.RandomAccessOrds;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.search.Filter;
-import org.apache.lucene.search.FilteredQuery;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.QueryWrapperFilter;
-import org.apache.lucene.search.Sort;
-import org.apache.lucene.search.SortField;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TopFieldDocs;
-import org.apache.lucene.search.join.BitDocIdSetCachingWrapperFilter;
-import org.apache.lucene.search.join.ScoreMode;
-import org.apache.lucene.search.join.ToParentBlockJoinQuery;
-import org.apache.lucene.util.Accountable;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.FixedBitSet;
-import org.apache.lucene.util.TestUtil;
-import org.apache.lucene.util.UnicodeUtil;
-import org.elasticsearch.common.lucene.search.Queries;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.fielddata.IndexFieldData.XFieldComparatorSource;
-import org.elasticsearch.index.fielddata.IndexFieldData.XFieldComparatorSource.Nested;
-import org.elasticsearch.index.fielddata.fieldcomparator.BytesRefFieldComparatorSource;
-import org.elasticsearch.index.fielddata.ordinals.GlobalOrdinalsIndexFieldData;
-import org.elasticsearch.search.MultiValueMode;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.not;
-import static org.hamcrest.Matchers.nullValue;
-import static org.hamcrest.Matchers.sameInstance;
-
-/**
- */
-public abstract class AbstractStringFieldDataTests extends AbstractFieldDataImplTests {
-
-    private void addField(Document d, String name, String value) {
-        d.add(new StringField(name, value, Field.Store.YES));
-        d.add(new SortedSetDocValuesField(name, new BytesRef(value)));
-    }
-
-    @Override
-    protected void fillSingleValueAllSet() throws Exception {
-        Document d = new Document();
-        addField(d, "_id", "1");
-        addField(d, "value", "2");
-        writer.addDocument(d);
-
-        d = new Document();
-        addField(d, "_id", "1");
-        addField(d, "value", "1");
-        writer.addDocument(d);
-
-        d = new Document();
-        addField(d, "_id", "3");
-        addField(d, "value", "3");
-        writer.addDocument(d);
-    }
-
-    @Override
-    protected void add2SingleValuedDocumentsAndDeleteOneOfThem() throws Exception {
-        Document d = new Document();
-        addField(d, "_id", "1");
-        addField(d, "value", "2");
-        writer.addDocument(d);
-
-        d = new Document();
-        addField(d, "_id", "2");
-        addField(d, "value", "4");
-        writer.addDocument(d);
-
-        writer.commit();
-
-        writer.deleteDocuments(new Term("_id", "1"));
-    }
-
-    @Override
-    protected void fillSingleValueWithMissing() throws Exception {
-        Document d = new Document();
-        addField(d, "_id", "1");
-        addField(d, "value", "2");
-        writer.addDocument(d);
-
-        d = new Document();
-        addField(d, "_id", "2");
-        //d.add(new StringField("value", one(), Field.Store.NO)); // MISSING....
-        writer.addDocument(d);
-
-        d = new Document();
-        addField(d, "_id", "3");
-        addField(d, "value", "3");
-        writer.addDocument(d);
-    }
-
-    @Override
-    protected void fillMultiValueAllSet() throws Exception {
-        Document d = new Document();
-        addField(d, "_id", "1");
-        addField(d, "value", "2");
-        addField(d, "value", "4");
-        writer.addDocument(d);
-
-        d = new Document();
-        addField(d, "_id", "2");
-        addField(d, "value", "1");
-        writer.addDocument(d);
-        writer.commit(); // TODO: Have tests with more docs for sorting
-
-        d = new Document();
-        addField(d, "_id", "3");
-        addField(d, "value", "3");
-        writer.addDocument(d);
-    }
-
-    @Override
-    protected void fillMultiValueWithMissing() throws Exception {
-        Document d = new Document();
-        addField(d, "_id", "1");
-        addField(d, "value", "2");
-        addField(d, "value", "4");
-        writer.addDocument(d);
-
-        d = new Document();
-        addField(d, "_id", "2");
-        //d.add(new StringField("value", one(), Field.Store.NO)); // MISSING
-        writer.addDocument(d);
-
-        d = new Document();
-        addField(d, "_id", "3");
-        addField(d, "value", "3");
-        writer.addDocument(d);
-    }
-
-    @Override
-    protected void fillAllMissing() throws Exception {
-        Document d = new Document();
-        addField(d, "_id", "1");
-        writer.addDocument(d);
-
-        d = new Document();
-        addField(d, "_id", "2");
-        writer.addDocument(d);
-
-        d = new Document();
-        addField(d, "_id", "3");
-        writer.addDocument(d);
-    }
-
-    @Override
-    protected void fillExtendedMvSet() throws Exception {
-        Document d = new Document();
-        addField(d, "_id", "1");
-        addField(d, "value", "02");
-        addField(d, "value", "04");
-        writer.addDocument(d);
-
-        d = new Document();
-        addField(d, "_id", "2");
-        writer.addDocument(d);
-
-        d = new Document();
-        addField(d, "_id", "3");
-        addField(d, "value", "03");
-        writer.addDocument(d);
-        writer.commit();
-
-        d = new Document();
-        addField(d, "_id", "4");
-        addField(d, "value", "04");
-        addField(d, "value", "05");
-        addField(d, "value", "06");
-        writer.addDocument(d);
-
-        d = new Document();
-        addField(d, "_id", "5");
-        addField(d, "value", "06");
-        addField(d, "value", "07");
-        addField(d, "value", "08");
-        writer.addDocument(d);
-
-        d = new Document();
-        d.add(new StringField("_id", "6", Field.Store.NO));
-        writer.addDocument(d);
-
-        d = new Document();
-        addField(d, "_id", "7");
-        addField(d, "value", "08");
-        addField(d, "value", "09");
-        addField(d, "value", "10");
-        writer.addDocument(d);
-        writer.commit();
-
-        d = new Document();
-        addField(d, "_id", "8");
-        addField(d, "value", "!08");
-        addField(d, "value", "!09");
-        addField(d, "value", "!10");
-        writer.addDocument(d);
-    }
-
-    public void testActualMissingValue() throws IOException {
-        testActualMissingValue(false);
-    }
-
-    public void testActualMissingValueReverse() throws IOException {
-        testActualMissingValue(true);
-    }
-
-    public void testActualMissingValue(boolean reverse) throws IOException {
-        // missing value is set to an actual value
-        final String[] values = new String[randomIntBetween(2, 30)];
-        for (int i = 1; i < values.length; ++i) {
-            values[i] = TestUtil.randomUnicodeString(getRandom());
-        }
-        final int numDocs = scaledRandomIntBetween(10, 3072);
-        for (int i = 0; i < numDocs; ++i) {
-            final String value = RandomPicks.randomFrom(getRandom(), values);
-            if (value == null) {
-                writer.addDocument(new Document());
-            } else {
-                Document d = new Document();
-                addField(d, "value", value);
-                writer.addDocument(d);
-            }
-            if (randomInt(10) == 0) {
-                writer.commit();
-            }
-        }
-
-        final IndexFieldData indexFieldData = getForField("value");
-        final String missingValue = values[1];
-        IndexSearcher searcher = new IndexSearcher(DirectoryReader.open(writer, true));
-        XFieldComparatorSource comparator = indexFieldData.comparatorSource(missingValue, MultiValueMode.MIN, null);
-        TopFieldDocs topDocs = searcher.search(new MatchAllDocsQuery(), randomBoolean() ? numDocs : randomIntBetween(10, numDocs), new Sort(new SortField("value", comparator, reverse)));
-        assertEquals(numDocs, topDocs.totalHits);
-        BytesRef previousValue = reverse ? UnicodeUtil.BIG_TERM : new BytesRef();
-        for (int i = 0; i < topDocs.scoreDocs.length; ++i) {
-            final String docValue = searcher.doc(topDocs.scoreDocs[i].doc).get("value");
-            final BytesRef value = new BytesRef(docValue == null ? missingValue : docValue);
-            if (reverse) {
-                assertTrue(previousValue.compareTo(value) >= 0);
-            } else {
-                assertTrue(previousValue.compareTo(value) <= 0);
-            }
-            previousValue = value;
-        }
-        searcher.getIndexReader().close();
-    }
-
-    public void testSortMissingFirst() throws IOException {
-        testSortMissing(true, false);
-    }
-
-    public void testSortMissingFirstReverse() throws IOException {
-        testSortMissing(true, true);
-    }
-
-    public void testSortMissingLast() throws IOException {
-        testSortMissing(false, false);
-    }
-
-    public void testSortMissingLastReverse() throws IOException {
-        testSortMissing(false, true);
-    }
-
-    public void testSortMissing(boolean first, boolean reverse) throws IOException {
-        final String[] values = new String[randomIntBetween(2, 10)];
-        for (int i = 1; i < values.length; ++i) {
-            values[i] = TestUtil.randomUnicodeString(getRandom());
-        }
-        final int numDocs = scaledRandomIntBetween(10, 3072);
-        for (int i = 0; i < numDocs; ++i) {
-            final String value = RandomPicks.randomFrom(getRandom(), values);
-            if (value == null) {
-                writer.addDocument(new Document());
-            } else {
-                Document d = new Document();
-                addField(d, "value", value);
-                writer.addDocument(d);
-            }
-            if (randomInt(10) == 0) {
-                writer.commit();
-            }
-        }
-        final IndexFieldData indexFieldData = getForField("value");
-        IndexSearcher searcher = new IndexSearcher(DirectoryReader.open(writer, true));
-        XFieldComparatorSource comparator = indexFieldData.comparatorSource(first ? "_first" : "_last", MultiValueMode.MIN, null);
-        TopFieldDocs topDocs = searcher.search(new MatchAllDocsQuery(), randomBoolean() ? numDocs : randomIntBetween(10, numDocs), new Sort(new SortField("value", comparator, reverse)));
-        assertEquals(numDocs, topDocs.totalHits);
-        BytesRef previousValue = first ? null : reverse ? UnicodeUtil.BIG_TERM : new BytesRef();
-        for (int i = 0; i < topDocs.scoreDocs.length; ++i) {
-            final String docValue = searcher.doc(topDocs.scoreDocs[i].doc).get("value");
-            if (first && docValue == null) {
-                assertNull(previousValue);
-            } else if (!first && docValue != null) {
-                assertNotNull(previousValue);
-            }
-            final BytesRef value = docValue == null ? null : new BytesRef(docValue);
-            if (previousValue != null && value != null) {
-                if (reverse) {
-                    assertTrue(previousValue.compareTo(value) >= 0);
-                } else {
-                    assertTrue(previousValue.compareTo(value) <= 0);
-                }
-            }
-            previousValue = value;
-        }
-        searcher.getIndexReader().close();
-    }
-
-    public void testNestedSortingMin() throws IOException {
-        testNestedSorting(MultiValueMode.MIN);
-    }
-
-    public void testNestedSortingMax() throws IOException {
-        testNestedSorting(MultiValueMode.MAX);
-    }
-
-    public void testNestedSorting(MultiValueMode sortMode) throws IOException {
-        final String[] values = new String[randomIntBetween(2, 20)];
-        for (int i = 0; i < values.length; ++i) {
-            values[i] = TestUtil.randomSimpleString(getRandom());
-        }
-        final int numParents = scaledRandomIntBetween(10, 3072);
-        List<Document> docs = new ArrayList<>();
-        FixedBitSet parents = new FixedBitSet(64);
-        for (int i = 0; i < numParents; ++i) {
-            docs.clear();
-            final int numChildren = randomInt(4);
-            for (int j = 0; j < numChildren; ++j) {
-                final Document child = new Document();
-                final int numValues = randomInt(3);
-                for (int k = 0; k < numValues; ++k) {
-                    final String value = RandomPicks.randomFrom(getRandom(), values);
-                    addField(child, "text", value);
-                }
-                docs.add(child);
-            }
-            final Document parent = new Document();
-            parent.add(new StringField("type", "parent", Store.YES));
-            final String value = RandomPicks.randomFrom(getRandom(), values);
-            if (value != null) {
-                addField(parent, "text", value);
-            }
-            docs.add(parent);
-            int bit = parents.prevSetBit(parents.length() - 1) + docs.size();
-            parents = FixedBitSet.ensureCapacity(parents, bit);
-            parents.set(bit);
-            writer.addDocuments(docs);
-            if (randomInt(10) == 0) {
-                writer.commit();
-            }
-        }
-        IndexSearcher searcher = new IndexSearcher(DirectoryReader.open(writer, true));
-        IndexFieldData<?> fieldData = getForField("text");
-        final Object missingValue;
-        switch (randomInt(4)) {
-        case 0:
-            missingValue = "_first";
-            break;
-        case 1:
-            missingValue = "_last";
-            break;
-        case 2:
-            missingValue = new BytesRef(RandomPicks.randomFrom(getRandom(), values));
-            break;
-        default:
-            missingValue = new BytesRef(TestUtil.randomSimpleString(getRandom()));
-            break;
-        }
-        Filter parentFilter = new QueryWrapperFilter(new TermQuery(new Term("type", "parent")));
-        Filter childFilter = new QueryWrapperFilter(Queries.not(parentFilter));
-        Nested nested = createNested(parentFilter, childFilter);
-        BytesRefFieldComparatorSource nestedComparatorSource = new BytesRefFieldComparatorSource(fieldData, missingValue, sortMode, nested);
-        ToParentBlockJoinQuery query = new ToParentBlockJoinQuery(new FilteredQuery(new MatchAllDocsQuery(), childFilter), new BitDocIdSetCachingWrapperFilter(parentFilter), ScoreMode.None);
-        Sort sort = new Sort(new SortField("text", nestedComparatorSource));
-        TopFieldDocs topDocs = searcher.search(query, randomIntBetween(1, numParents), sort);
-        assertTrue(topDocs.scoreDocs.length > 0);
-        BytesRef previous = null;
-        for (int i = 0; i < topDocs.scoreDocs.length; ++i) {
-            final int docID = topDocs.scoreDocs[i].doc;
-            assertTrue("expected " + docID + " to be a parent", parents.get(docID));
-            BytesRef cmpValue = null;
-            for (int child = parents.prevSetBit(docID - 1) + 1; child < docID; ++child) {
-                String[] sVals = searcher.doc(child).getValues("text");
-                final BytesRef[] vals;
-                if (sVals.length == 0) {
-                    vals = new BytesRef[0];
-                } else {
-                    vals = new BytesRef[sVals.length];
-                    for (int j = 0; j < vals.length; ++j) {
-                        vals[j] = new BytesRef(sVals[j]);
-                    }
-                }
-                for (BytesRef value : vals) {
-                    if (cmpValue == null) {
-                        cmpValue = value;
-                    } else if (sortMode == MultiValueMode.MIN && value.compareTo(cmpValue) < 0) {
-                        cmpValue = value;
-                    } else if (sortMode == MultiValueMode.MAX && value.compareTo(cmpValue) > 0) {
-                        cmpValue = value;
-                    }
-                }
-            }
-            if (cmpValue == null) {
-                if ("_first".equals(missingValue)) {
-                    cmpValue = new BytesRef();
-                } else if ("_last".equals(missingValue) == false) {
-                    cmpValue = (BytesRef) missingValue;
-                }
-            }
-            if (previous != null && cmpValue != null) {
-                assertTrue(previous.utf8ToString() + "   /   " + cmpValue.utf8ToString(), previous.compareTo(cmpValue) <= 0);
-            }
-            previous = cmpValue;
-        }
-        searcher.getIndexReader().close();
-    }
-
-    private void assertIteratorConsistentWithRandomAccess(RandomAccessOrds ords, int maxDoc) {
-        for (int doc = 0; doc < maxDoc; ++doc) {
-            ords.setDocument(doc);
-            final int cardinality = ords.cardinality();
-            for (int i = 0; i < cardinality; ++i) {
-                assertEquals(ords.nextOrd(), ords.ordAt(i));
-            }
-            for (int i = 0; i < 3; ++i) {
-                assertEquals(ords.nextOrd(), -1);
-            }
-        }
-    }
-
-    @Test
-    public void testGlobalOrdinals() throws Exception {
-        fillExtendedMvSet();
-        refreshReader();
-        FieldDataType fieldDataType = new FieldDataType("string", Settings.builder().put("global_values", "fixed"));
-        IndexOrdinalsFieldData ifd = getForField(fieldDataType, "value", hasDocValues());
-        IndexOrdinalsFieldData globalOrdinals = ifd.loadGlobal(topLevelReader);
-        assertThat(topLevelReader.leaves().size(), equalTo(3));
-
-        // First segment
-        assertThat(globalOrdinals, instanceOf(GlobalOrdinalsIndexFieldData.class));
-        LeafReaderContext leaf = topLevelReader.leaves().get(0);
-        AtomicOrdinalsFieldData afd = globalOrdinals.load(leaf);
-        RandomAccessOrds values = afd.getOrdinalsValues();
-        assertIteratorConsistentWithRandomAccess(values, leaf.reader().maxDoc());
-        values.setDocument(0);
-        assertThat(values.cardinality(), equalTo(2));
-        long ord = values.nextOrd();
-        assertThat(ord, equalTo(3l));
-        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("02"));
-        ord = values.nextOrd();
-        assertThat(ord, equalTo(5l));
-        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("04"));
-        values.setDocument(1);
-        assertThat(values.cardinality(), equalTo(0));
-        values.setDocument(2);
-        assertThat(values.cardinality(), equalTo(1));
-        ord = values.nextOrd();
-        assertThat(ord, equalTo(4l));
-        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("03"));
-
-        // Second segment
-        leaf = topLevelReader.leaves().get(1);
-        afd = globalOrdinals.load(leaf);
-        values = afd.getOrdinalsValues();
-        assertIteratorConsistentWithRandomAccess(values, leaf.reader().maxDoc());
-        values.setDocument(0);
-        assertThat(values.cardinality(), equalTo(3));
-        ord = values.nextOrd();
-        assertThat(ord, equalTo(5l));
-        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("04"));
-        ord = values.nextOrd();
-        assertThat(ord, equalTo(6l));
-        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("05"));
-        ord = values.nextOrd();
-        assertThat(ord, equalTo(7l));
-        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("06"));
-        values.setDocument(1);
-        assertThat(values.cardinality(), equalTo(3));
-        ord = values.nextOrd();
-        assertThat(ord, equalTo(7l));
-        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("06"));
-        ord = values.nextOrd();
-        assertThat(ord, equalTo(8l));
-        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("07"));
-        ord = values.nextOrd();
-        assertThat(ord, equalTo(9l));
-        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("08"));
-        values.setDocument(2);
-        assertThat(values.cardinality(), equalTo(0));
-        values.setDocument(3);
-        assertThat(values.cardinality(), equalTo(3));
-        ord = values.nextOrd();
-        assertThat(ord, equalTo(9l));
-        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("08"));
-        ord = values.nextOrd();
-        assertThat(ord, equalTo(10l));
-        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("09"));
-        ord = values.nextOrd();
-        assertThat(ord, equalTo(11l));
-        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("10"));
-
-        // Third segment
-        leaf = topLevelReader.leaves().get(2);
-        afd = globalOrdinals.load(leaf);
-        values = afd.getOrdinalsValues();
-        assertIteratorConsistentWithRandomAccess(values, leaf.reader().maxDoc());
-        values.setDocument(0);
-        values.setDocument(0);
-        assertThat(values.cardinality(), equalTo(3));
-        ord = values.nextOrd();
-        assertThat(ord, equalTo(0l));
-        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("!08"));
-        ord = values.nextOrd();
-        assertThat(ord, equalTo(1l));
-        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("!09"));
-        ord = values.nextOrd();
-        assertThat(ord, equalTo(2l));
-        assertThat(values.lookupOrd(ord).utf8ToString(), equalTo("!10"));
-    }
-
-    @Test
-    public void testTermsEnum() throws Exception {
-        fillExtendedMvSet();
-        LeafReaderContext atomicReaderContext = refreshReader();
-
-        IndexOrdinalsFieldData ifd = getForField("value");
-        AtomicOrdinalsFieldData afd = ifd.load(atomicReaderContext);
-
-        TermsEnum termsEnum = afd.getOrdinalsValues().termsEnum();
-        int size = 0;
-        while (termsEnum.next() != null) {
-            size++;
-        }
-        assertThat(size, equalTo(12));
-
-        assertThat(termsEnum.seekExact(new BytesRef("10")), is(true));
-        assertThat(termsEnum.term().utf8ToString(), equalTo("10"));
-        assertThat(termsEnum.next(), nullValue());
-
-        assertThat(termsEnum.seekExact(new BytesRef("08")), is(true));
-        assertThat(termsEnum.term().utf8ToString(), equalTo("08"));
-        size = 0;
-        while (termsEnum.next() != null) {
-            size++;
-        }
-        assertThat(size, equalTo(2));
-
-        termsEnum.seekExact(8);
-        assertThat(termsEnum.term().utf8ToString(), equalTo("07"));
-        size = 0;
-        while (termsEnum.next() != null) {
-            size++;
-        }
-        assertThat(size, equalTo(3));
-    }
-
-    @Test
-    public void testGlobalOrdinalsGetRemovedOnceIndexReaderCloses() throws Exception {
-        fillExtendedMvSet();
-        refreshReader();
-        FieldDataType fieldDataType = new FieldDataType("string", Settings.builder().put("global_values", "fixed").put("cache", "node"));
-        IndexOrdinalsFieldData ifd = getForField(fieldDataType, "value", hasDocValues());
-        IndexOrdinalsFieldData globalOrdinals = ifd.loadGlobal(topLevelReader);
-        assertThat(ifd.loadGlobal(topLevelReader), sameInstance(globalOrdinals));
-        // 3 b/c 1 segment level caches and 1 top level cache
-        // in case of doc values, we don't cache atomic FD, so only the top-level cache is there
-        assertThat(indicesFieldDataCache.getCache().size(), equalTo(hasDocValues() ? 1L : 4L));
-
-        IndexOrdinalsFieldData cachedInstance = null;
-        for (Accountable ramUsage : indicesFieldDataCache.getCache().asMap().values()) {
-            if (ramUsage instanceof IndexOrdinalsFieldData) {
-                cachedInstance = (IndexOrdinalsFieldData) ramUsage;
-                break;
-            }
-        }
-        assertThat(cachedInstance, sameInstance(globalOrdinals));
-        topLevelReader.close();
-        // Now only 3 segment level entries, only the toplevel reader has been closed, but the segment readers are still used by IW
-        assertThat(indicesFieldDataCache.getCache().size(), equalTo(hasDocValues() ? 0L : 3L));
-
-        refreshReader();
-        assertThat(ifd.loadGlobal(topLevelReader), not(sameInstance(globalOrdinals)));
-
-        ifdService.clear();
-        assertThat(indicesFieldDataCache.getCache().size(), equalTo(0l));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/fielddata/BinaryDVFieldDataTests.java b/core/src/test/java/org/elasticsearch/index/fielddata/BinaryDVFieldDataTests.java
index ab63a5d..bc63ed9 100644
--- a/core/src/test/java/org/elasticsearch/index/fielddata/BinaryDVFieldDataTests.java
+++ b/core/src/test/java/org/elasticsearch/index/fielddata/BinaryDVFieldDataTests.java
@@ -35,7 +35,7 @@ import static org.hamcrest.Matchers.equalTo;
 /**
  *
  */
-public class BinaryDVFieldDataTests extends AbstractFieldDataTests {
+public class BinaryDVFieldDataTests extends AbstractFieldDataTestCase {
 
     @Override
     protected boolean hasDocValues() {
diff --git a/core/src/test/java/org/elasticsearch/index/fielddata/DoubleFieldDataTests.java b/core/src/test/java/org/elasticsearch/index/fielddata/DoubleFieldDataTests.java
index 242e014..6c93a2e 100644
--- a/core/src/test/java/org/elasticsearch/index/fielddata/DoubleFieldDataTests.java
+++ b/core/src/test/java/org/elasticsearch/index/fielddata/DoubleFieldDataTests.java
@@ -27,7 +27,7 @@ import org.apache.lucene.index.Term;
 
 /**
  */
-public class DoubleFieldDataTests extends AbstractNumericFieldDataTests {
+public class DoubleFieldDataTests extends AbstractNumericFieldDataTestCase {
 
     @Override
     protected FieldDataType getFieldDataType() {
diff --git a/core/src/test/java/org/elasticsearch/index/fielddata/DuelFieldDataTests.java b/core/src/test/java/org/elasticsearch/index/fielddata/DuelFieldDataTests.java
index 9a4250f..f02c286 100644
--- a/core/src/test/java/org/elasticsearch/index/fielddata/DuelFieldDataTests.java
+++ b/core/src/test/java/org/elasticsearch/index/fielddata/DuelFieldDataTests.java
@@ -56,7 +56,7 @@ import java.util.Set;
 
 import static org.hamcrest.Matchers.*;
 
-public class DuelFieldDataTests extends AbstractFieldDataTests {
+public class DuelFieldDataTests extends AbstractFieldDataTestCase {
 
     @Override
     protected FieldDataType getFieldDataType() {
diff --git a/core/src/test/java/org/elasticsearch/index/fielddata/FilterFieldDataTest.java b/core/src/test/java/org/elasticsearch/index/fielddata/FilterFieldDataTest.java
deleted file mode 100644
index 52807f3..0000000
--- a/core/src/test/java/org/elasticsearch/index/fielddata/FilterFieldDataTest.java
+++ /dev/null
@@ -1,187 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.fielddata;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.index.RandomAccessOrds;
-import org.elasticsearch.common.settings.Settings;
-import org.junit.Test;
-
-import java.util.Random;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public class FilterFieldDataTest extends AbstractFieldDataTests {
-
-    @Override
-    protected FieldDataType getFieldDataType() {
-        // TODO Auto-generated method stub
-        return null;
-    }
-
-    @Test
-    public void testFilterByFrequency() throws Exception {
-        Random random = getRandom();
-        for (int i = 0; i < 1000; i++) {
-            Document d = new Document();
-            d.add(new StringField("id", "" + i, Field.Store.NO));
-            if (i % 100 == 0) {
-                d.add(new StringField("high_freq", "100", Field.Store.NO));
-                d.add(new StringField("low_freq", "100", Field.Store.NO));
-                d.add(new StringField("med_freq", "100", Field.Store.NO));
-            }
-            if (i % 10 == 0) {
-                d.add(new StringField("high_freq", "10", Field.Store.NO));
-                d.add(new StringField("med_freq", "10", Field.Store.NO));
-            }
-            if (i % 5 == 0) {
-                d.add(new StringField("high_freq", "5", Field.Store.NO));
-            }
-            writer.addDocument(d);
-        }
-        writer.forceMerge(1, true);
-        LeafReaderContext context = refreshReader();
-        String[] formats = new String[] { "paged_bytes"};
-        
-        for (String format : formats) {
-            {
-                ifdService.clear();
-                FieldDataType fieldDataType = new FieldDataType("string", Settings.builder().put("format", format)
-                        .put("filter.frequency.min_segment_size", 100).put("filter.frequency.min", 0.0d).put("filter.frequency.max", random.nextBoolean() ? 100 : 0.5d));
-                IndexOrdinalsFieldData fieldData = getForField(fieldDataType, "high_freq");
-                AtomicOrdinalsFieldData loadDirect = fieldData.loadDirect(context);
-                RandomAccessOrds bytesValues = loadDirect.getOrdinalsValues();
-                assertThat(2L, equalTo(bytesValues.getValueCount()));
-                assertThat(bytesValues.lookupOrd(0).utf8ToString(), equalTo("10"));
-                assertThat(bytesValues.lookupOrd(1).utf8ToString(), equalTo("100"));
-            }
-            {
-                ifdService.clear();
-                FieldDataType fieldDataType = new FieldDataType("string", Settings.builder().put("format", format)
-                        .put("filter.frequency.min_segment_size", 100).put("filter.frequency.min",  random.nextBoolean() ? 101 : 101d/200.0d).put("filter.frequency.max", 201));
-                IndexOrdinalsFieldData fieldData = getForField(fieldDataType, "high_freq");
-                AtomicOrdinalsFieldData loadDirect = fieldData.loadDirect(context);
-                RandomAccessOrds bytesValues = loadDirect.getOrdinalsValues();
-                assertThat(1L, equalTo(bytesValues.getValueCount()));
-                assertThat(bytesValues.lookupOrd(0).utf8ToString(), equalTo("5"));
-            }
-            
-            {
-                ifdService.clear(); // test # docs with value
-                FieldDataType fieldDataType = new FieldDataType("string", Settings.builder().put("format", format)
-                        .put("filter.frequency.min_segment_size", 101).put("filter.frequency.min", random.nextBoolean() ? 101 : 101d/200.0d));
-                IndexOrdinalsFieldData fieldData = getForField(fieldDataType, "med_freq");
-                AtomicOrdinalsFieldData loadDirect = fieldData.loadDirect(context);
-                RandomAccessOrds bytesValues = loadDirect.getOrdinalsValues();
-                assertThat(2L, equalTo(bytesValues.getValueCount()));
-                assertThat(bytesValues.lookupOrd(0).utf8ToString(), equalTo("10"));
-                assertThat(bytesValues.lookupOrd(1).utf8ToString(), equalTo("100"));
-            }
-            
-            {
-                ifdService.clear();
-                FieldDataType fieldDataType = new FieldDataType("string", Settings.builder().put("format", format)
-                        .put("filter.frequency.min_segment_size", 101).put("filter.frequency.min", random.nextBoolean() ? 101 : 101d/200.0d));
-                IndexOrdinalsFieldData fieldData = getForField(fieldDataType, "med_freq");
-                AtomicOrdinalsFieldData loadDirect = fieldData.loadDirect(context);
-                RandomAccessOrds bytesValues = loadDirect.getOrdinalsValues();
-                assertThat(2L, equalTo(bytesValues.getValueCount()));
-                assertThat(bytesValues.lookupOrd(0).utf8ToString(), equalTo("10"));
-                assertThat(bytesValues.lookupOrd(1).utf8ToString(), equalTo("100"));
-            }
-            
-            {
-                ifdService.clear();
-                FieldDataType fieldDataType = new FieldDataType("string", Settings.builder().put("format", format)
-                        .put("filter.regex.pattern", "\\d{2,3}") // allows 10 & 100
-                        .put("filter.frequency.min_segment_size", 0)
-                        .put("filter.frequency.min", random.nextBoolean() ? 2 : 1d/200.0d) // 100, 10, 5
-                        .put("filter.frequency.max", random.nextBoolean() ? 99 : 99d/200.0d)); // 100
-                IndexOrdinalsFieldData fieldData = getForField(fieldDataType, "high_freq");
-                AtomicOrdinalsFieldData loadDirect = fieldData.loadDirect(context);
-                RandomAccessOrds bytesValues = loadDirect.getOrdinalsValues();
-                assertThat(1L, equalTo(bytesValues.getValueCount()));
-                assertThat(bytesValues.lookupOrd(0).utf8ToString(), equalTo("100"));
-            }
-        }
-
-    }
-    
-    @Test
-    public void testFilterByRegExp() throws Exception {
-
-        int hundred  = 0;
-        int ten  = 0;
-        int five  = 0;
-        for (int i = 0; i < 1000; i++) {
-            Document d = new Document();
-            d.add(new StringField("id", "" + i, Field.Store.NO));
-            if (i % 100 == 0) {
-                hundred++;
-                d.add(new StringField("high_freq", "100", Field.Store.NO));
-            }
-            if (i % 10 == 0) {
-                ten++;
-                d.add(new StringField("high_freq", "10", Field.Store.NO));
-            }
-            if (i % 5 == 0) {
-                five++;
-                d.add(new StringField("high_freq", "5", Field.Store.NO));
-
-            }
-            writer.addDocument(d);
-        }
-        logger.debug(hundred + " " + ten + " " + five);
-        writer.forceMerge(1, true);
-        LeafReaderContext context = refreshReader();
-        String[] formats = new String[] { "paged_bytes"};
-        for (String format : formats) {
-            {
-                ifdService.clear();
-                FieldDataType fieldDataType = new FieldDataType("string", Settings.builder().put("format", format)
-                        .put("filter.regex.pattern", "\\d"));
-                IndexOrdinalsFieldData fieldData = getForField(fieldDataType, "high_freq");
-                AtomicOrdinalsFieldData loadDirect = fieldData.loadDirect(context);
-                RandomAccessOrds bytesValues = loadDirect.getOrdinalsValues();
-                assertThat(1L, equalTo(bytesValues.getValueCount()));
-                assertThat(bytesValues.lookupOrd(0).utf8ToString(), equalTo("5"));
-            }
-            {
-                ifdService.clear();
-                FieldDataType fieldDataType = new FieldDataType("string", Settings.builder().put("format", format)
-                        .put("filter.regex.pattern", "\\d{1,2}"));
-                IndexOrdinalsFieldData fieldData = getForField(fieldDataType, "high_freq");
-                AtomicOrdinalsFieldData loadDirect = fieldData.loadDirect(context);
-                RandomAccessOrds bytesValues = loadDirect.getOrdinalsValues();
-                assertThat(2L, equalTo(bytesValues.getValueCount()));
-                assertThat(bytesValues.lookupOrd(0).utf8ToString(), equalTo("10"));
-                assertThat(bytesValues.lookupOrd(1).utf8ToString(), equalTo("5"));
-            }
-        }
-
-    }
-
-    @Override
-    public void testEmpty() throws Exception {
-        // No need to test empty usage here
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/fielddata/FilterFieldDataTests.java b/core/src/test/java/org/elasticsearch/index/fielddata/FilterFieldDataTests.java
new file mode 100644
index 0000000..4b9d0e1
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/fielddata/FilterFieldDataTests.java
@@ -0,0 +1,187 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.index.fielddata;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.index.RandomAccessOrds;
+import org.elasticsearch.common.settings.Settings;
+import org.junit.Test;
+
+import java.util.Random;
+
+import static org.hamcrest.Matchers.equalTo;
+
+public class FilterFieldDataTests extends AbstractFieldDataTestCase {
+
+    @Override
+    protected FieldDataType getFieldDataType() {
+        // TODO Auto-generated method stub
+        return null;
+    }
+
+    @Test
+    public void testFilterByFrequency() throws Exception {
+        Random random = getRandom();
+        for (int i = 0; i < 1000; i++) {
+            Document d = new Document();
+            d.add(new StringField("id", "" + i, Field.Store.NO));
+            if (i % 100 == 0) {
+                d.add(new StringField("high_freq", "100", Field.Store.NO));
+                d.add(new StringField("low_freq", "100", Field.Store.NO));
+                d.add(new StringField("med_freq", "100", Field.Store.NO));
+            }
+            if (i % 10 == 0) {
+                d.add(new StringField("high_freq", "10", Field.Store.NO));
+                d.add(new StringField("med_freq", "10", Field.Store.NO));
+            }
+            if (i % 5 == 0) {
+                d.add(new StringField("high_freq", "5", Field.Store.NO));
+            }
+            writer.addDocument(d);
+        }
+        writer.forceMerge(1, true);
+        LeafReaderContext context = refreshReader();
+        String[] formats = new String[] { "paged_bytes"};
+        
+        for (String format : formats) {
+            {
+                ifdService.clear();
+                FieldDataType fieldDataType = new FieldDataType("string", Settings.builder().put("format", format)
+                        .put("filter.frequency.min_segment_size", 100).put("filter.frequency.min", 0.0d).put("filter.frequency.max", random.nextBoolean() ? 100 : 0.5d));
+                IndexOrdinalsFieldData fieldData = getForField(fieldDataType, "high_freq");
+                AtomicOrdinalsFieldData loadDirect = fieldData.loadDirect(context);
+                RandomAccessOrds bytesValues = loadDirect.getOrdinalsValues();
+                assertThat(2L, equalTo(bytesValues.getValueCount()));
+                assertThat(bytesValues.lookupOrd(0).utf8ToString(), equalTo("10"));
+                assertThat(bytesValues.lookupOrd(1).utf8ToString(), equalTo("100"));
+            }
+            {
+                ifdService.clear();
+                FieldDataType fieldDataType = new FieldDataType("string", Settings.builder().put("format", format)
+                        .put("filter.frequency.min_segment_size", 100).put("filter.frequency.min",  random.nextBoolean() ? 101 : 101d/200.0d).put("filter.frequency.max", 201));
+                IndexOrdinalsFieldData fieldData = getForField(fieldDataType, "high_freq");
+                AtomicOrdinalsFieldData loadDirect = fieldData.loadDirect(context);
+                RandomAccessOrds bytesValues = loadDirect.getOrdinalsValues();
+                assertThat(1L, equalTo(bytesValues.getValueCount()));
+                assertThat(bytesValues.lookupOrd(0).utf8ToString(), equalTo("5"));
+            }
+            
+            {
+                ifdService.clear(); // test # docs with value
+                FieldDataType fieldDataType = new FieldDataType("string", Settings.builder().put("format", format)
+                        .put("filter.frequency.min_segment_size", 101).put("filter.frequency.min", random.nextBoolean() ? 101 : 101d/200.0d));
+                IndexOrdinalsFieldData fieldData = getForField(fieldDataType, "med_freq");
+                AtomicOrdinalsFieldData loadDirect = fieldData.loadDirect(context);
+                RandomAccessOrds bytesValues = loadDirect.getOrdinalsValues();
+                assertThat(2L, equalTo(bytesValues.getValueCount()));
+                assertThat(bytesValues.lookupOrd(0).utf8ToString(), equalTo("10"));
+                assertThat(bytesValues.lookupOrd(1).utf8ToString(), equalTo("100"));
+            }
+            
+            {
+                ifdService.clear();
+                FieldDataType fieldDataType = new FieldDataType("string", Settings.builder().put("format", format)
+                        .put("filter.frequency.min_segment_size", 101).put("filter.frequency.min", random.nextBoolean() ? 101 : 101d/200.0d));
+                IndexOrdinalsFieldData fieldData = getForField(fieldDataType, "med_freq");
+                AtomicOrdinalsFieldData loadDirect = fieldData.loadDirect(context);
+                RandomAccessOrds bytesValues = loadDirect.getOrdinalsValues();
+                assertThat(2L, equalTo(bytesValues.getValueCount()));
+                assertThat(bytesValues.lookupOrd(0).utf8ToString(), equalTo("10"));
+                assertThat(bytesValues.lookupOrd(1).utf8ToString(), equalTo("100"));
+            }
+            
+            {
+                ifdService.clear();
+                FieldDataType fieldDataType = new FieldDataType("string", Settings.builder().put("format", format)
+                        .put("filter.regex.pattern", "\\d{2,3}") // allows 10 & 100
+                        .put("filter.frequency.min_segment_size", 0)
+                        .put("filter.frequency.min", random.nextBoolean() ? 2 : 1d/200.0d) // 100, 10, 5
+                        .put("filter.frequency.max", random.nextBoolean() ? 99 : 99d/200.0d)); // 100
+                IndexOrdinalsFieldData fieldData = getForField(fieldDataType, "high_freq");
+                AtomicOrdinalsFieldData loadDirect = fieldData.loadDirect(context);
+                RandomAccessOrds bytesValues = loadDirect.getOrdinalsValues();
+                assertThat(1L, equalTo(bytesValues.getValueCount()));
+                assertThat(bytesValues.lookupOrd(0).utf8ToString(), equalTo("100"));
+            }
+        }
+
+    }
+    
+    @Test
+    public void testFilterByRegExp() throws Exception {
+
+        int hundred  = 0;
+        int ten  = 0;
+        int five  = 0;
+        for (int i = 0; i < 1000; i++) {
+            Document d = new Document();
+            d.add(new StringField("id", "" + i, Field.Store.NO));
+            if (i % 100 == 0) {
+                hundred++;
+                d.add(new StringField("high_freq", "100", Field.Store.NO));
+            }
+            if (i % 10 == 0) {
+                ten++;
+                d.add(new StringField("high_freq", "10", Field.Store.NO));
+            }
+            if (i % 5 == 0) {
+                five++;
+                d.add(new StringField("high_freq", "5", Field.Store.NO));
+
+            }
+            writer.addDocument(d);
+        }
+        logger.debug(hundred + " " + ten + " " + five);
+        writer.forceMerge(1, true);
+        LeafReaderContext context = refreshReader();
+        String[] formats = new String[] { "paged_bytes"};
+        for (String format : formats) {
+            {
+                ifdService.clear();
+                FieldDataType fieldDataType = new FieldDataType("string", Settings.builder().put("format", format)
+                        .put("filter.regex.pattern", "\\d"));
+                IndexOrdinalsFieldData fieldData = getForField(fieldDataType, "high_freq");
+                AtomicOrdinalsFieldData loadDirect = fieldData.loadDirect(context);
+                RandomAccessOrds bytesValues = loadDirect.getOrdinalsValues();
+                assertThat(1L, equalTo(bytesValues.getValueCount()));
+                assertThat(bytesValues.lookupOrd(0).utf8ToString(), equalTo("5"));
+            }
+            {
+                ifdService.clear();
+                FieldDataType fieldDataType = new FieldDataType("string", Settings.builder().put("format", format)
+                        .put("filter.regex.pattern", "\\d{1,2}"));
+                IndexOrdinalsFieldData fieldData = getForField(fieldDataType, "high_freq");
+                AtomicOrdinalsFieldData loadDirect = fieldData.loadDirect(context);
+                RandomAccessOrds bytesValues = loadDirect.getOrdinalsValues();
+                assertThat(2L, equalTo(bytesValues.getValueCount()));
+                assertThat(bytesValues.lookupOrd(0).utf8ToString(), equalTo("10"));
+                assertThat(bytesValues.lookupOrd(1).utf8ToString(), equalTo("5"));
+            }
+        }
+
+    }
+
+    @Override
+    public void testEmpty() throws Exception {
+        // No need to test empty usage here
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/index/fielddata/FloatFieldDataTests.java b/core/src/test/java/org/elasticsearch/index/fielddata/FloatFieldDataTests.java
index b81a8cd..2633673 100644
--- a/core/src/test/java/org/elasticsearch/index/fielddata/FloatFieldDataTests.java
+++ b/core/src/test/java/org/elasticsearch/index/fielddata/FloatFieldDataTests.java
@@ -26,7 +26,7 @@ import org.apache.lucene.index.Term;
 
 /**
  */
-public class FloatFieldDataTests extends AbstractNumericFieldDataTests {
+public class FloatFieldDataTests extends AbstractNumericFieldDataTestCase {
 
     @Override
     protected FieldDataType getFieldDataType() {
diff --git a/core/src/test/java/org/elasticsearch/index/fielddata/LongFieldDataTests.java b/core/src/test/java/org/elasticsearch/index/fielddata/LongFieldDataTests.java
index 09a24a4..f47b94d 100644
--- a/core/src/test/java/org/elasticsearch/index/fielddata/LongFieldDataTests.java
+++ b/core/src/test/java/org/elasticsearch/index/fielddata/LongFieldDataTests.java
@@ -42,7 +42,7 @@ import static org.hamcrest.Matchers.lessThan;
 /**
  * Tests for all integer types (byte, short, int, long).
  */
-public class LongFieldDataTests extends AbstractNumericFieldDataTests {
+public class LongFieldDataTests extends AbstractNumericFieldDataTestCase {
 
     @Override
     protected FieldDataType getFieldDataType() {
diff --git a/core/src/test/java/org/elasticsearch/index/fielddata/PagedBytesStringFieldDataTests.java b/core/src/test/java/org/elasticsearch/index/fielddata/PagedBytesStringFieldDataTests.java
index 1b8909e..7a8e879 100644
--- a/core/src/test/java/org/elasticsearch/index/fielddata/PagedBytesStringFieldDataTests.java
+++ b/core/src/test/java/org/elasticsearch/index/fielddata/PagedBytesStringFieldDataTests.java
@@ -24,7 +24,7 @@ import org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder;
 
 /**
  */
-public class PagedBytesStringFieldDataTests extends AbstractStringFieldDataTests {
+public class PagedBytesStringFieldDataTests extends AbstractStringFieldDataTestCase {
 
     @Override
     protected FieldDataType getFieldDataType() {
diff --git a/core/src/test/java/org/elasticsearch/index/fielddata/ParentChildFieldDataTests.java b/core/src/test/java/org/elasticsearch/index/fielddata/ParentChildFieldDataTests.java
index 90934bc..b265988 100644
--- a/core/src/test/java/org/elasticsearch/index/fielddata/ParentChildFieldDataTests.java
+++ b/core/src/test/java/org/elasticsearch/index/fielddata/ParentChildFieldDataTests.java
@@ -30,7 +30,6 @@ import org.apache.lucene.search.*;
 import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.action.admin.indices.mapping.put.PutMappingRequest;
 import org.elasticsearch.common.compress.CompressedXContent;
-import org.elasticsearch.index.fielddata.IndexFieldData.XFieldComparatorSource;
 import org.elasticsearch.index.fielddata.plain.ParentChildIndexFieldData;
 import org.elasticsearch.index.mapper.Uid;
 import org.elasticsearch.index.mapper.internal.ParentFieldMapper;
@@ -49,7 +48,7 @@ import static org.hamcrest.Matchers.nullValue;
 
 /**
  */
-public class ParentChildFieldDataTests extends AbstractFieldDataTests {
+public class ParentChildFieldDataTests extends AbstractFieldDataTestCase {
 
     private final String parentType = "parent";
     private final String childType = "child";
diff --git a/core/src/test/java/org/elasticsearch/index/fielddata/SortedSetDVStringFieldDataTests.java b/core/src/test/java/org/elasticsearch/index/fielddata/SortedSetDVStringFieldDataTests.java
index 013a7ec..0b2f174 100644
--- a/core/src/test/java/org/elasticsearch/index/fielddata/SortedSetDVStringFieldDataTests.java
+++ b/core/src/test/java/org/elasticsearch/index/fielddata/SortedSetDVStringFieldDataTests.java
@@ -22,7 +22,7 @@ package org.elasticsearch.index.fielddata;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder;
 
-public class SortedSetDVStringFieldDataTests extends AbstractStringFieldDataTests {
+public class SortedSetDVStringFieldDataTests extends AbstractStringFieldDataTestCase {
 
     @Override
     protected FieldDataType getFieldDataType() {
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/MapperServiceTest.java b/core/src/test/java/org/elasticsearch/index/mapper/MapperServiceTest.java
deleted file mode 100644
index d1c78c6..0000000
--- a/core/src/test/java/org/elasticsearch/index/mapper/MapperServiceTest.java
+++ /dev/null
@@ -1,90 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.mapper;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;
-import org.elasticsearch.test.ESSingleNodeTestCase;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.ExpectedException;
-
-import static org.elasticsearch.test.VersionUtils.getFirstVersion;
-import static org.elasticsearch.test.VersionUtils.getPreviousVersion;
-import static org.elasticsearch.test.VersionUtils.randomVersionBetween;
-import static org.hamcrest.CoreMatchers.containsString;
-import static org.hamcrest.CoreMatchers.is;
-import static org.hamcrest.Matchers.hasToString;
-
-public class MapperServiceTest extends ESSingleNodeTestCase {
-    @Rule
-    public ExpectedException expectedException = ExpectedException.none();
-
-    @Test
-    public void testTypeNameStartsWithIllegalDot() {
-        expectedException.expect(MapperParsingException.class);
-        expectedException.expect(hasToString(containsString("mapping type name [.test-type] must not start with a '.'")));
-        String index = "test-index";
-        String type = ".test-type";
-        String field = "field";
-        client()
-                .admin()
-                .indices()
-                .prepareCreate(index)
-                .addMapping(type, field, "type=string")
-                .execute()
-                .actionGet();
-    }
-
-    @Test
-    public void testThatLongTypeNameIsNotRejectedOnPreElasticsearchVersionTwo() {
-        String index = "text-index";
-        String field = "field";
-        String type = new String(new char[256]).replace("\0", "a");
-
-        CreateIndexResponse response =
-                client()
-                        .admin()
-                        .indices()
-                        .prepareCreate(index)
-                        .setSettings(settings(randomVersionBetween(random(), getFirstVersion(), getPreviousVersion(Version.V_2_0_0_beta1))))
-                        .addMapping(type, field, "type=string")
-                        .execute()
-                        .actionGet();
-        assertNotNull(response);
-    }
-
-    @Test
-    public void testTypeNameTooLong() {
-        String index = "text-index";
-        String field = "field";
-        String type = new String(new char[256]).replace("\0", "a");
-
-        expectedException.expect(MapperParsingException.class);
-        expectedException.expect(hasToString(containsString("mapping type name [" + type + "] is too long; limit is length 255 but was [256]")));
-        client()
-                .admin()
-                .indices()
-                .prepareCreate(index)
-                .addMapping(type, field, "type=string")
-                .execute()
-                .actionGet();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/MapperServiceTests.java b/core/src/test/java/org/elasticsearch/index/mapper/MapperServiceTests.java
new file mode 100644
index 0000000..add7ee6
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/mapper/MapperServiceTests.java
@@ -0,0 +1,90 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.mapper;
+
+import org.elasticsearch.Version;
+import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;
+import org.elasticsearch.test.ESSingleNodeTestCase;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.ExpectedException;
+
+import static org.elasticsearch.test.VersionUtils.getFirstVersion;
+import static org.elasticsearch.test.VersionUtils.getPreviousVersion;
+import static org.elasticsearch.test.VersionUtils.randomVersionBetween;
+import static org.hamcrest.CoreMatchers.containsString;
+import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.Matchers.hasToString;
+
+public class MapperServiceTests extends ESSingleNodeTestCase {
+    @Rule
+    public ExpectedException expectedException = ExpectedException.none();
+
+    @Test
+    public void testTypeNameStartsWithIllegalDot() {
+        expectedException.expect(MapperParsingException.class);
+        expectedException.expect(hasToString(containsString("mapping type name [.test-type] must not start with a '.'")));
+        String index = "test-index";
+        String type = ".test-type";
+        String field = "field";
+        client()
+                .admin()
+                .indices()
+                .prepareCreate(index)
+                .addMapping(type, field, "type=string")
+                .execute()
+                .actionGet();
+    }
+
+    @Test
+    public void testThatLongTypeNameIsNotRejectedOnPreElasticsearchVersionTwo() {
+        String index = "text-index";
+        String field = "field";
+        String type = new String(new char[256]).replace("\0", "a");
+
+        CreateIndexResponse response =
+                client()
+                        .admin()
+                        .indices()
+                        .prepareCreate(index)
+                        .setSettings(settings(randomVersionBetween(random(), getFirstVersion(), getPreviousVersion(Version.V_2_0_0_beta1))))
+                        .addMapping(type, field, "type=string")
+                        .execute()
+                        .actionGet();
+        assertNotNull(response);
+    }
+
+    @Test
+    public void testTypeNameTooLong() {
+        String index = "text-index";
+        String field = "field";
+        String type = new String(new char[256]).replace("\0", "a");
+
+        expectedException.expect(MapperParsingException.class);
+        expectedException.expect(hasToString(containsString("mapping type name [" + type + "] is too long; limit is length 255 but was [256]")));
+        client()
+                .admin()
+                .indices()
+                .prepareCreate(index)
+                .addMapping(type, field, "type=string")
+                .execute()
+                .actionGet();
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/lucene/DoubleIndexingDocTest.java b/core/src/test/java/org/elasticsearch/index/mapper/lucene/DoubleIndexingDocTest.java
deleted file mode 100644
index 5584501..0000000
--- a/core/src/test/java/org/elasticsearch/index/mapper/lucene/DoubleIndexingDocTest.java
+++ /dev/null
@@ -1,96 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.mapper.lucene;
-
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.store.Directory;
-import org.elasticsearch.common.lucene.Lucene;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.IndexService;
-import org.elasticsearch.index.mapper.DocumentMapper;
-import org.elasticsearch.index.mapper.ParsedDocument;
-import org.elasticsearch.test.ESSingleNodeTestCase;
-import org.junit.Test;
-
-import static org.hamcrest.Matchers.equalTo;
-
-/**
- *
- */
-public class DoubleIndexingDocTest extends ESSingleNodeTestCase {
-
-    @Test
-    public void testDoubleIndexingSameDoc() throws Exception {
-        Directory dir = newDirectory();
-        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(random(), Lucene.STANDARD_ANALYZER));
-
-        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
-                .startObject("properties").endObject()
-                .endObject().endObject().string();
-        IndexService index = createIndex("test");
-        client().admin().indices().preparePutMapping("test").setType("type").setSource(mapping).get();
-        DocumentMapper mapper = index.mapperService().documentMapper("type");
-
-        ParsedDocument doc = mapper.parse("test", "type", "1", XContentFactory.jsonBuilder()
-                .startObject()
-                .field("field1", "value1")
-                .field("field2", 1)
-                .field("field3", 1.1)
-                .field("field4", "2010-01-01")
-                .startArray("field5").value(1).value(2).value(3).endArray()
-                .endObject()
-                .bytes());
-        assertNotNull(doc.dynamicMappingsUpdate());
-        client().admin().indices().preparePutMapping("test").setType("type").setSource(doc.dynamicMappingsUpdate().toString()).get();
-
-        writer.addDocument(doc.rootDoc());
-        writer.addDocument(doc.rootDoc());
-
-        IndexReader reader = DirectoryReader.open(writer, true);
-        IndexSearcher searcher = new IndexSearcher(reader);
-
-        TopDocs topDocs = searcher.search(mapper.mappers().smartNameFieldMapper("field1").fieldType().termQuery("value1", null), 10);
-        assertThat(topDocs.totalHits, equalTo(2));
-
-        topDocs = searcher.search(mapper.mappers().smartNameFieldMapper("field2").fieldType().termQuery("1", null), 10);
-        assertThat(topDocs.totalHits, equalTo(2));
-
-        topDocs = searcher.search(mapper.mappers().smartNameFieldMapper("field3").fieldType().termQuery("1.1", null), 10);
-        assertThat(topDocs.totalHits, equalTo(2));
-
-        topDocs = searcher.search(mapper.mappers().smartNameFieldMapper("field4").fieldType().termQuery("2010-01-01", null), 10);
-        assertThat(topDocs.totalHits, equalTo(2));
-
-        topDocs = searcher.search(mapper.mappers().smartNameFieldMapper("field5").fieldType().termQuery("1", null), 10);
-        assertThat(topDocs.totalHits, equalTo(2));
-
-        topDocs = searcher.search(mapper.mappers().smartNameFieldMapper("field5").fieldType().termQuery("2", null), 10);
-        assertThat(topDocs.totalHits, equalTo(2));
-
-        topDocs = searcher.search(mapper.mappers().smartNameFieldMapper("field5").fieldType().termQuery("3", null), 10);
-        assertThat(topDocs.totalHits, equalTo(2));
-        writer.close();
-        reader.close();
-        dir.close();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/lucene/DoubleIndexingDocTests.java b/core/src/test/java/org/elasticsearch/index/mapper/lucene/DoubleIndexingDocTests.java
new file mode 100644
index 0000000..9aade61
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/mapper/lucene/DoubleIndexingDocTests.java
@@ -0,0 +1,96 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.index.mapper.lucene;
+
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.elasticsearch.common.lucene.Lucene;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.index.IndexService;
+import org.elasticsearch.index.mapper.DocumentMapper;
+import org.elasticsearch.index.mapper.ParsedDocument;
+import org.elasticsearch.test.ESSingleNodeTestCase;
+import org.junit.Test;
+
+import static org.hamcrest.Matchers.equalTo;
+
+/**
+ *
+ */
+public class DoubleIndexingDocTests extends ESSingleNodeTestCase {
+
+    @Test
+    public void testDoubleIndexingSameDoc() throws Exception {
+        Directory dir = newDirectory();
+        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(random(), Lucene.STANDARD_ANALYZER));
+
+        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
+                .startObject("properties").endObject()
+                .endObject().endObject().string();
+        IndexService index = createIndex("test");
+        client().admin().indices().preparePutMapping("test").setType("type").setSource(mapping).get();
+        DocumentMapper mapper = index.mapperService().documentMapper("type");
+
+        ParsedDocument doc = mapper.parse("test", "type", "1", XContentFactory.jsonBuilder()
+                .startObject()
+                .field("field1", "value1")
+                .field("field2", 1)
+                .field("field3", 1.1)
+                .field("field4", "2010-01-01")
+                .startArray("field5").value(1).value(2).value(3).endArray()
+                .endObject()
+                .bytes());
+        assertNotNull(doc.dynamicMappingsUpdate());
+        client().admin().indices().preparePutMapping("test").setType("type").setSource(doc.dynamicMappingsUpdate().toString()).get();
+
+        writer.addDocument(doc.rootDoc());
+        writer.addDocument(doc.rootDoc());
+
+        IndexReader reader = DirectoryReader.open(writer, true);
+        IndexSearcher searcher = new IndexSearcher(reader);
+
+        TopDocs topDocs = searcher.search(mapper.mappers().smartNameFieldMapper("field1").fieldType().termQuery("value1", null), 10);
+        assertThat(topDocs.totalHits, equalTo(2));
+
+        topDocs = searcher.search(mapper.mappers().smartNameFieldMapper("field2").fieldType().termQuery("1", null), 10);
+        assertThat(topDocs.totalHits, equalTo(2));
+
+        topDocs = searcher.search(mapper.mappers().smartNameFieldMapper("field3").fieldType().termQuery("1.1", null), 10);
+        assertThat(topDocs.totalHits, equalTo(2));
+
+        topDocs = searcher.search(mapper.mappers().smartNameFieldMapper("field4").fieldType().termQuery("2010-01-01", null), 10);
+        assertThat(topDocs.totalHits, equalTo(2));
+
+        topDocs = searcher.search(mapper.mappers().smartNameFieldMapper("field5").fieldType().termQuery("1", null), 10);
+        assertThat(topDocs.totalHits, equalTo(2));
+
+        topDocs = searcher.search(mapper.mappers().smartNameFieldMapper("field5").fieldType().termQuery("2", null), 10);
+        assertThat(topDocs.totalHits, equalTo(2));
+
+        topDocs = searcher.search(mapper.mappers().smartNameFieldMapper("field5").fieldType().termQuery("3", null), 10);
+        assertThat(topDocs.totalHits, equalTo(2));
+        writer.close();
+        reader.close();
+        dir.close();
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/lucene/StoredNumericValuesTest.java b/core/src/test/java/org/elasticsearch/index/mapper/lucene/StoredNumericValuesTest.java
deleted file mode 100644
index 558ba25..0000000
--- a/core/src/test/java/org/elasticsearch/index/mapper/lucene/StoredNumericValuesTest.java
+++ /dev/null
@@ -1,124 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.mapper.lucene;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.StoredField;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.Numbers;
-import org.elasticsearch.common.lucene.Lucene;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.fieldvisitor.CustomFieldsVisitor;
-import org.elasticsearch.index.mapper.DocumentMapper;
-import org.elasticsearch.index.mapper.ParsedDocument;
-import org.elasticsearch.test.ESSingleNodeTestCase;
-import org.junit.Test;
-
-import java.util.Arrays;
-import java.util.HashSet;
-import java.util.Set;
-
-import static org.hamcrest.Matchers.equalTo;
-
-/**
- *
- */
-public class StoredNumericValuesTest extends ESSingleNodeTestCase {
-
-    @Test
-    public void testBytesAndNumericRepresentation() throws Exception {
-        IndexWriter writer = new IndexWriter(new RAMDirectory(), new IndexWriterConfig(Lucene.STANDARD_ANALYZER));
-
-        String mapping = XContentFactory.jsonBuilder()
-                .startObject()
-                    .startObject("type")
-                        .startObject("properties")
-                            .startObject("field1").field("type", "integer").field("store", "yes").endObject()
-                            .startObject("field2").field("type", "float").field("store", "yes").endObject()
-                            .startObject("field3").field("type", "long").field("store", "yes").endObject()
-                        .endObject()
-                    .endObject()
-                .endObject()
-                .string();
-        DocumentMapper mapper = createIndex("test").mapperService().documentMapperParser().parse(mapping);
-
-        ParsedDocument doc = mapper.parse("test", "type", "1", XContentFactory.jsonBuilder()
-                .startObject()
-                    .field("field1", 1)
-                    .field("field2", 1.1)
-                    .startArray("field3").value(1).value(2).value(3).endArray()
-                .endObject()
-                .bytes());
-
-        writer.addDocument(doc.rootDoc());
-
-        // Indexing a doc in the old way
-        FieldType fieldType = new FieldType();
-        fieldType.setStored(true);
-        fieldType.setNumericType(FieldType.NumericType.INT);
-        Document doc2 = new Document();
-        doc2.add(new StoredField("field1", new BytesRef(Numbers.intToBytes(1))));
-        doc2.add(new StoredField("field2", new BytesRef(Numbers.floatToBytes(1.1f))));
-        doc2.add(new StoredField("field3", new BytesRef(Numbers.longToBytes(1l))));
-        doc2.add(new StoredField("field3", new BytesRef(Numbers.longToBytes(2l))));
-        doc2.add(new StoredField("field3", new BytesRef(Numbers.longToBytes(3l))));
-        writer.addDocument(doc2);
-
-        DirectoryReader reader = DirectoryReader.open(writer, true);
-        IndexSearcher searcher = new IndexSearcher(reader);
-
-        Set<String> fields = new HashSet<>(Arrays.asList("field1", "field2", "field3"));
-        CustomFieldsVisitor fieldsVisitor = new CustomFieldsVisitor(fields, false);
-        searcher.doc(0, fieldsVisitor);
-        fieldsVisitor.postProcess(mapper);
-        assertThat(fieldsVisitor.fields().size(), equalTo(3));
-        assertThat(fieldsVisitor.fields().get("field1").size(), equalTo(1));
-        assertThat((Integer) fieldsVisitor.fields().get("field1").get(0), equalTo(1));
-        assertThat(fieldsVisitor.fields().get("field2").size(), equalTo(1));
-        assertThat((Float) fieldsVisitor.fields().get("field2").get(0), equalTo(1.1f));
-        assertThat(fieldsVisitor.fields().get("field3").size(), equalTo(3));
-        assertThat((Long) fieldsVisitor.fields().get("field3").get(0), equalTo(1l));
-        assertThat((Long) fieldsVisitor.fields().get("field3").get(1), equalTo(2l));
-        assertThat((Long) fieldsVisitor.fields().get("field3").get(2), equalTo(3l));
-
-        // Make sure the doc gets loaded as if it was stored in the new way
-        fieldsVisitor.reset();
-        searcher.doc(1, fieldsVisitor);
-        fieldsVisitor.postProcess(mapper);
-        assertThat(fieldsVisitor.fields().size(), equalTo(3));
-        assertThat(fieldsVisitor.fields().get("field1").size(), equalTo(1));
-        assertThat((Integer) fieldsVisitor.fields().get("field1").get(0), equalTo(1));
-        assertThat(fieldsVisitor.fields().get("field2").size(), equalTo(1));
-        assertThat((Float) fieldsVisitor.fields().get("field2").get(0), equalTo(1.1f));
-        assertThat(fieldsVisitor.fields().get("field3").size(), equalTo(3));
-        assertThat((Long) fieldsVisitor.fields().get("field3").get(0), equalTo(1l));
-        assertThat((Long) fieldsVisitor.fields().get("field3").get(1), equalTo(2l));
-        assertThat((Long) fieldsVisitor.fields().get("field3").get(2), equalTo(3l));
-
-        reader.close();
-        writer.close();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/lucene/StoredNumericValuesTests.java b/core/src/test/java/org/elasticsearch/index/mapper/lucene/StoredNumericValuesTests.java
new file mode 100644
index 0000000..380e8e3
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/mapper/lucene/StoredNumericValuesTests.java
@@ -0,0 +1,124 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.mapper.lucene;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.StoredField;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.BytesRef;
+import org.elasticsearch.common.Numbers;
+import org.elasticsearch.common.lucene.Lucene;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.index.fieldvisitor.CustomFieldsVisitor;
+import org.elasticsearch.index.mapper.DocumentMapper;
+import org.elasticsearch.index.mapper.ParsedDocument;
+import org.elasticsearch.test.ESSingleNodeTestCase;
+import org.junit.Test;
+
+import java.util.Arrays;
+import java.util.HashSet;
+import java.util.Set;
+
+import static org.hamcrest.Matchers.equalTo;
+
+/**
+ *
+ */
+public class StoredNumericValuesTests extends ESSingleNodeTestCase {
+
+    @Test
+    public void testBytesAndNumericRepresentation() throws Exception {
+        IndexWriter writer = new IndexWriter(new RAMDirectory(), new IndexWriterConfig(Lucene.STANDARD_ANALYZER));
+
+        String mapping = XContentFactory.jsonBuilder()
+                .startObject()
+                    .startObject("type")
+                        .startObject("properties")
+                            .startObject("field1").field("type", "integer").field("store", "yes").endObject()
+                            .startObject("field2").field("type", "float").field("store", "yes").endObject()
+                            .startObject("field3").field("type", "long").field("store", "yes").endObject()
+                        .endObject()
+                    .endObject()
+                .endObject()
+                .string();
+        DocumentMapper mapper = createIndex("test").mapperService().documentMapperParser().parse(mapping);
+
+        ParsedDocument doc = mapper.parse("test", "type", "1", XContentFactory.jsonBuilder()
+                .startObject()
+                    .field("field1", 1)
+                    .field("field2", 1.1)
+                    .startArray("field3").value(1).value(2).value(3).endArray()
+                .endObject()
+                .bytes());
+
+        writer.addDocument(doc.rootDoc());
+
+        // Indexing a doc in the old way
+        FieldType fieldType = new FieldType();
+        fieldType.setStored(true);
+        fieldType.setNumericType(FieldType.NumericType.INT);
+        Document doc2 = new Document();
+        doc2.add(new StoredField("field1", new BytesRef(Numbers.intToBytes(1))));
+        doc2.add(new StoredField("field2", new BytesRef(Numbers.floatToBytes(1.1f))));
+        doc2.add(new StoredField("field3", new BytesRef(Numbers.longToBytes(1l))));
+        doc2.add(new StoredField("field3", new BytesRef(Numbers.longToBytes(2l))));
+        doc2.add(new StoredField("field3", new BytesRef(Numbers.longToBytes(3l))));
+        writer.addDocument(doc2);
+
+        DirectoryReader reader = DirectoryReader.open(writer, true);
+        IndexSearcher searcher = new IndexSearcher(reader);
+
+        Set<String> fields = new HashSet<>(Arrays.asList("field1", "field2", "field3"));
+        CustomFieldsVisitor fieldsVisitor = new CustomFieldsVisitor(fields, false);
+        searcher.doc(0, fieldsVisitor);
+        fieldsVisitor.postProcess(mapper);
+        assertThat(fieldsVisitor.fields().size(), equalTo(3));
+        assertThat(fieldsVisitor.fields().get("field1").size(), equalTo(1));
+        assertThat((Integer) fieldsVisitor.fields().get("field1").get(0), equalTo(1));
+        assertThat(fieldsVisitor.fields().get("field2").size(), equalTo(1));
+        assertThat((Float) fieldsVisitor.fields().get("field2").get(0), equalTo(1.1f));
+        assertThat(fieldsVisitor.fields().get("field3").size(), equalTo(3));
+        assertThat((Long) fieldsVisitor.fields().get("field3").get(0), equalTo(1l));
+        assertThat((Long) fieldsVisitor.fields().get("field3").get(1), equalTo(2l));
+        assertThat((Long) fieldsVisitor.fields().get("field3").get(2), equalTo(3l));
+
+        // Make sure the doc gets loaded as if it was stored in the new way
+        fieldsVisitor.reset();
+        searcher.doc(1, fieldsVisitor);
+        fieldsVisitor.postProcess(mapper);
+        assertThat(fieldsVisitor.fields().size(), equalTo(3));
+        assertThat(fieldsVisitor.fields().get("field1").size(), equalTo(1));
+        assertThat((Integer) fieldsVisitor.fields().get("field1").get(0), equalTo(1));
+        assertThat(fieldsVisitor.fields().get("field2").size(), equalTo(1));
+        assertThat((Float) fieldsVisitor.fields().get("field2").get(0), equalTo(1.1f));
+        assertThat(fieldsVisitor.fields().get("field3").size(), equalTo(3));
+        assertThat((Long) fieldsVisitor.fields().get("field3").get(0), equalTo(1l));
+        assertThat((Long) fieldsVisitor.fields().get("field3").get(1), equalTo(2l));
+        assertThat((Long) fieldsVisitor.fields().get("field3").get(2), equalTo(3l));
+
+        reader.close();
+        writer.close();
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/index/query/AndQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/AndQueryBuilderTest.java
deleted file mode 100644
index b91a1ea..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/AndQueryBuilderTest.java
+++ /dev/null
@@ -1,124 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.*;
-
-import static org.hamcrest.CoreMatchers.*;
-
-@SuppressWarnings("deprecation")
-public class AndQueryBuilderTest extends BaseQueryTestCase<AndQueryBuilder> {
-
-    /**
-     * @return a AndQueryBuilder with random limit between 0 and 20
-     */
-    @Override
-    protected AndQueryBuilder doCreateTestQueryBuilder() {
-        AndQueryBuilder query = new AndQueryBuilder();
-        int subQueries = randomIntBetween(1, 5);
-        for (int i = 0; i < subQueries; i++ ) {
-            query.add(RandomQueryBuilder.createQuery(random()));
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(AndQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (queryBuilder.innerQueries().isEmpty()) {
-            assertThat(query, nullValue());
-        } else {
-            List<Query> clauses = new ArrayList<>();
-            for (QueryBuilder innerFilter : queryBuilder.innerQueries()) {
-                Query clause = innerFilter.toQuery(context);
-                if (clause != null) {
-                    clauses.add(clause);
-                }
-            }
-            if (clauses.isEmpty()) {
-                assertThat(query, nullValue());
-            } else {
-                assertThat(query, instanceOf(BooleanQuery.class));
-                BooleanQuery booleanQuery = (BooleanQuery) query;
-                assertThat(booleanQuery.clauses().size(), equalTo(clauses.size()));
-                Iterator<Query> queryIterator = clauses.iterator();
-                for (BooleanClause booleanClause : booleanQuery) {
-                    assertThat(booleanClause.getOccur(), equalTo(BooleanClause.Occur.MUST));
-                    assertThat(booleanClause.getQuery(), equalTo(queryIterator.next()));
-                }
-            }
-        }
-    }
-
-    /**
-     * test corner case where no inner queries exist
-     */
-    @Test
-    public void testNoInnerQueries() throws QueryShardException, IOException {
-        AndQueryBuilder andQuery = new AndQueryBuilder();
-        assertNull(andQuery.toQuery(createShardContext()));
-    }
-
-    @Test(expected=QueryParsingException.class)
-    public void testMissingFiltersSection() throws IOException {
-        parseQuery("{ \"and\" : {}");
-    }
-
-    @Test
-    public void testValidate() {
-        AndQueryBuilder andQuery = new AndQueryBuilder();
-        int iters = randomIntBetween(0, 5);
-        int totalExpectedErrors = 0;
-        for (int i = 0; i < iters; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    andQuery.add(RandomQueryBuilder.createInvalidQuery(random()));
-                } else {
-                    andQuery.add(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                andQuery.add(RandomQueryBuilder.createQuery(random()));
-            }
-        }
-        assertValidate(andQuery, totalExpectedErrors);
-    }
-
-    @Override
-    protected Map<String, AndQueryBuilder> getAlternateVersions() {
-        Map<String, AndQueryBuilder> alternateVersions = new HashMap<>();
-        QueryBuilder innerQuery = createTestQueryBuilder().innerQueries().get(0);
-        AndQueryBuilder expectedQuery = new AndQueryBuilder(innerQuery);
-        String contentString =  "{ \"and\" : [ " + innerQuery + "] }";
-        alternateVersions.put(contentString, expectedQuery);
-        return alternateVersions;
-    }
-
-    @Test(expected=QueryParsingException.class)
-    public void testParsingExceptionNonFiltersElementArray() throws IOException {
-        String queryString = "{ \"and\" : { \"whatever_filters\" : [ { \"match_all\" : {} } ] } }";
-        parseQuery(queryString);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java b/core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java
deleted file mode 100644
index 5bed617..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java
+++ /dev/null
@@ -1,533 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.elasticsearch.Version;
-import org.elasticsearch.action.admin.indices.mapping.put.PutMappingRequest;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.collect.Tuple;
-import org.elasticsearch.common.compress.CompressedXContent;
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Injector;
-import org.elasticsearch.common.inject.ModulesBuilder;
-import org.elasticsearch.common.inject.multibindings.Multibinder;
-import org.elasticsearch.common.inject.util.Providers;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.NamedWriteableAwareStreamInput;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsModule;
-import org.elasticsearch.common.unit.Fuzziness;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.env.EnvironmentModule;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.IndexNameModule;
-import org.elasticsearch.index.analysis.AnalysisModule;
-import org.elasticsearch.index.cache.IndexCacheModule;
-import org.elasticsearch.index.mapper.MapperService;
-import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
-import org.elasticsearch.index.query.support.QueryParsers;
-import org.elasticsearch.index.settings.IndexSettingsModule;
-import org.elasticsearch.index.similarity.SimilarityModule;
-import org.elasticsearch.indices.IndicesModule;
-import org.elasticsearch.indices.analysis.IndicesAnalysisService;
-import org.elasticsearch.indices.breaker.CircuitBreakerService;
-import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
-import org.elasticsearch.script.ScriptModule;
-import org.elasticsearch.search.internal.SearchContext;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.TestSearchContext;
-import org.elasticsearch.test.VersionUtils;
-import org.elasticsearch.test.cluster.TestClusterService;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.threadpool.ThreadPoolModule;
-import org.joda.time.DateTime;
-import org.joda.time.DateTimeZone;
-import org.junit.*;
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.*;
-
-public abstract class BaseQueryTestCase<QB extends AbstractQueryBuilder<QB>> extends ESTestCase {
-
-    protected static final String STRING_FIELD_NAME = "mapped_string";
-    protected static final String INT_FIELD_NAME = "mapped_int";
-    protected static final String DOUBLE_FIELD_NAME = "mapped_double";
-    protected static final String BOOLEAN_FIELD_NAME = "mapped_boolean";
-    protected static final String DATE_FIELD_NAME = "mapped_date";
-    protected static final String OBJECT_FIELD_NAME = "mapped_object";
-    protected static final String[] MAPPED_FIELD_NAMES = new String[] { STRING_FIELD_NAME, INT_FIELD_NAME,
-            DOUBLE_FIELD_NAME, BOOLEAN_FIELD_NAME, DATE_FIELD_NAME, OBJECT_FIELD_NAME };
-    protected static final String[] MAPPED_LEAF_FIELD_NAMES = new String[] { STRING_FIELD_NAME, INT_FIELD_NAME,
-            DOUBLE_FIELD_NAME, BOOLEAN_FIELD_NAME, DATE_FIELD_NAME };
-
-    private static Injector injector;
-    private static IndexQueryParserService queryParserService;
-
-    protected static IndexQueryParserService queryParserService() {
-        return queryParserService;
-    }
-
-    private static Index index;
-
-    protected static Index getIndex() {
-        return index;
-    }
-
-    private static String[] currentTypes;
-
-    protected static String[] getCurrentTypes() {
-        return currentTypes;
-    }
-
-    private static NamedWriteableRegistry namedWriteableRegistry;
-
-    /**
-     * Setup for the whole base test class.
-     * @throws IOException
-     */
-    @BeforeClass
-    public static void init() throws IOException {
-        Version version = VersionUtils.randomVersionBetween(random(), Version.V_1_0_0, Version.CURRENT);
-        Settings settings = Settings.settingsBuilder()
-                .put("name", BaseQueryTestCase.class.toString())
-                .put("path.home", createTempDir())
-                .build();
-        Settings indexSettings = Settings.settingsBuilder()
-                .put(IndexMetaData.SETTING_VERSION_CREATED, version).build();
-        index = new Index(randomAsciiOfLengthBetween(1, 10));
-        final TestClusterService clusterService = new TestClusterService();
-        clusterService.setState(new ClusterState.Builder(clusterService.state()).metaData(new MetaData.Builder().put(
-                new IndexMetaData.Builder(index.name()).settings(indexSettings).numberOfShards(1).numberOfReplicas(0))));
-        injector = new ModulesBuilder().add(
-                new EnvironmentModule(new Environment(settings)),
-                new SettingsModule(settings),
-                new ThreadPoolModule(new ThreadPool(settings)),
-                new IndicesModule(settings) {
-                    @Override
-                    public void configure() {
-                        // skip services
-                        bindQueryParsersExtension();
-                    }
-                },
-                new ScriptModule(settings),
-                new IndexSettingsModule(index, indexSettings),
-                new IndexCacheModule(indexSettings),
-                new AnalysisModule(indexSettings, new IndicesAnalysisService(indexSettings)),
-                new SimilarityModule(indexSettings),
-                new IndexNameModule(index),
-                new AbstractModule() {
-                    @Override
-                    protected void configure() {
-                        Multibinder.newSetBinder(binder(), ScoreFunctionParser.class);
-                        bind(ClusterService.class).toProvider(Providers.of(clusterService));
-                        bind(CircuitBreakerService.class).to(NoneCircuitBreakerService.class);
-                        bind(NamedWriteableRegistry.class).asEagerSingleton();
-                    }
-                }
-        ).createInjector();
-        queryParserService = injector.getInstance(IndexQueryParserService.class);
-        MapperService mapperService = queryParserService.mapperService;
-        //create some random type with some default field, those types will stick around for all of the subclasses
-        currentTypes = new String[randomIntBetween(0, 5)];
-        for (int i = 0; i < currentTypes.length; i++) {
-            String type = randomAsciiOfLengthBetween(1, 10);
-            mapperService.merge(type, new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef(type,
-                    STRING_FIELD_NAME, "type=string",
-                    INT_FIELD_NAME, "type=integer",
-                    DOUBLE_FIELD_NAME, "type=double",
-                    BOOLEAN_FIELD_NAME, "type=boolean",
-                    DATE_FIELD_NAME, "type=date",
-                    OBJECT_FIELD_NAME, "type=object"
-            ).string()), false, false);
-            // also add mappings for two inner field in the object field
-            mapperService.merge(type, new CompressedXContent("{\"properties\":{\""+OBJECT_FIELD_NAME+"\":{\"type\":\"object\","
-                    + "\"properties\":{\""+DATE_FIELD_NAME+"\":{\"type\":\"date\"},\""+INT_FIELD_NAME+"\":{\"type\":\"integer\"}}}}}"), false, false);
-            currentTypes[i] = type;
-        }
-        namedWriteableRegistry = injector.getInstance(NamedWriteableRegistry.class);
-    }
-
-    @AfterClass
-    public static void afterClass() throws Exception {
-        terminate(injector.getInstance(ThreadPool.class));
-        injector = null;
-        index = null;
-        queryParserService = null;
-        currentTypes = null;
-        namedWriteableRegistry = null;
-    }
-
-    @Before
-    public void beforeTest() {
-        //set some random types to be queried as part the search request, before each test
-        String[] types = getRandomTypes();
-        //some query (e.g. range query) have a different behaviour depending on whether the current search context is set or not
-        //which is why we randomly set the search context, which will internally also do QueryParseContext.setTypes(types)
-        if (randomBoolean()) {
-            QueryShardContext.setTypes(types);
-        } else {
-            TestSearchContext testSearchContext = new TestSearchContext();
-            testSearchContext.setTypes(types);
-            SearchContext.setCurrent(testSearchContext);
-        }
-    }
-
-    @After
-    public void afterTest() {
-        QueryShardContext.removeTypes();
-        SearchContext.removeCurrent();
-    }
-
-    protected final QB createTestQueryBuilder() {
-        QB query = doCreateTestQueryBuilder();
-        if (supportsBoostAndQueryName()) {
-            if (randomBoolean()) {
-                query.boost(2.0f / randomIntBetween(1, 20));
-            }
-            if (randomBoolean()) {
-                query.queryName(randomAsciiOfLengthBetween(1, 10));
-            }
-        }
-        return query;
-    }
-
-    /**
-     * Create the query that is being tested
-     */
-    protected abstract QB doCreateTestQueryBuilder();
-
-    /**
-     * Generic test that creates new query from the test query and checks both for equality
-     * and asserts equality on the two queries.
-     */
-    @Test
-    public void testFromXContent() throws IOException {
-        QB testQuery = createTestQueryBuilder();
-        assertParsedQuery(testQuery.toString(), testQuery);
-        for (Map.Entry<String, QB> alternateVersion : getAlternateVersions().entrySet()) {
-            assertParsedQuery(alternateVersion.getKey(), alternateVersion.getValue());
-        }
-    }
-
-    /**
-     * Returns alternate string representation of the query that need to be tested as they are never used as output
-     * of {@link QueryBuilder#toXContent(XContentBuilder, ToXContent.Params)}. By default there are no alternate versions.
-     */
-    protected Map<String, QB> getAlternateVersions() {
-        return Collections.emptyMap();
-    }
-
-    /**
-     * Parses the query provided as string argument and compares it with the expected result provided as argument as a {@link QueryBuilder}
-     */
-    protected void assertParsedQuery(String queryAsString, QueryBuilder<?> expectedQuery) throws IOException {
-        QueryBuilder<?> newQuery = parseQuery(queryAsString);
-        assertNotSame(newQuery, expectedQuery);
-        assertEquals(expectedQuery, newQuery);
-        assertEquals(expectedQuery.hashCode(), newQuery.hashCode());
-    }
-
-    protected QueryBuilder<?> parseQuery(String queryAsString) throws IOException {
-        XContentParser parser = XContentFactory.xContent(queryAsString).createParser(queryAsString);
-        QueryParseContext context = createParseContext();
-        context.reset(parser);
-        return context.parseInnerQueryBuilder();
-    }
-
-    /**
-     * Test creates the {@link Query} from the {@link QueryBuilder} under test and delegates the
-     * assertions being made on the result to the implementing subclass.
-     */
-    @Test
-    public void testToQuery() throws IOException {
-        QueryShardContext context = createShardContext();
-        context.setAllowUnmappedFields(true);
-
-        QB firstQuery = createTestQueryBuilder();
-        Query firstLuceneQuery = firstQuery.toQuery(context);
-        assertLuceneQuery(firstQuery, firstLuceneQuery, context);
-
-        QB secondQuery = copyQuery(firstQuery);
-        //query _name never should affect the result of toQuery, we randomly set it to make sure
-        if (randomBoolean()) {
-            secondQuery.queryName(secondQuery.queryName() == null ? randomAsciiOfLengthBetween(1, 30) : secondQuery.queryName() + randomAsciiOfLengthBetween(1, 10));
-        }
-        Query secondLuceneQuery = secondQuery.toQuery(context);
-        assertLuceneQuery(secondQuery, secondLuceneQuery, context);
-        assertThat("two equivalent query builders lead to different lucene queries", secondLuceneQuery, equalTo(firstLuceneQuery));
-
-        //if the initial lucene query is null, changing its boost won't have any effect, we shouldn't test that
-        //few queries also don't support boost e.g. wrapper query and filter query
-        //otherwise makes sure that boost is taken into account in toQuery
-        if (firstLuceneQuery != null && supportsBoostAndQueryName()) {
-            secondQuery.boost(firstQuery.boost() + 1f + randomFloat());
-            Query thirdLuceneQuery = secondQuery.toQuery(context);
-            assertThat("modifying the boost doesn't affect the corresponding lucene query", firstLuceneQuery, not(equalTo(thirdLuceneQuery)));
-        }
-    }
-
-    /**
-     * Few queries allow you to set the boost and queryName but don't do anything with it. This method allows
-     * to disable boost and queryName related tests for those queries. Those queries are easy to identify: their parsers
-     * don't parse `boost` and `_name` as they don't apply to the specific query e.g. filter query or wrapper query
-     */
-    protected boolean supportsBoostAndQueryName() {
-        return true;
-    }
-
-    /**
-     * Checks the result of {@link QueryBuilder#toQuery(QueryShardContext)} given the original {@link QueryBuilder} and {@link QueryShardContext}.
-     * Verifies that named queries and boost are properly handled and delegates to {@link #doAssertLuceneQuery(AbstractQueryBuilder, Query, QueryShardContext)}
-     * for query specific checks.
-     */
-    protected final void assertLuceneQuery(QB queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (queryBuilder.queryName() != null) {
-            Query namedQuery = context.copyNamedQueries().get(queryBuilder.queryName());
-            assertThat(namedQuery, equalTo(query));
-        }
-        if (query != null) {
-            assertThat(query.getBoost(), equalTo(queryBuilder.boost()));
-        }
-        doAssertLuceneQuery(queryBuilder, query, context);
-    }
-
-    /**
-     * Checks the result of {@link QueryBuilder#toQuery(QueryShardContext)} given the original {@link QueryBuilder} and {@link QueryShardContext}.
-     * Contains the query specific checks to be implemented by subclasses.
-     */
-    protected abstract void doAssertLuceneQuery(QB queryBuilder, Query query, QueryShardContext context) throws IOException;
-
-    /**
-     * Test serialization and deserialization of the test query.
-     */
-    @Test
-    public void testSerialization() throws IOException {
-        QB testQuery = createTestQueryBuilder();
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            testQuery.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                QueryBuilder<?> prototype = queryParser(testQuery.getName()).getBuilderPrototype();
-                QueryBuilder deserializedQuery = prototype.readFrom(in);
-                assertEquals(deserializedQuery, testQuery);
-                assertEquals(deserializedQuery.hashCode(), testQuery.hashCode());
-                assertNotSame(deserializedQuery, testQuery);
-            }
-        }
-    }
-
-    @Test
-    public void testEqualsAndHashcode() throws IOException {
-        QB firstQuery = createTestQueryBuilder();
-        assertFalse("query is equal to null", firstQuery.equals(null));
-        assertFalse("query is equal to incompatible type", firstQuery.equals(""));
-        assertTrue("query is not equal to self", firstQuery.equals(firstQuery));
-        assertThat("same query's hashcode returns different values if called multiple times", firstQuery.hashCode(), equalTo(firstQuery.hashCode()));
-
-        QB secondQuery = copyQuery(firstQuery);
-        assertTrue("query is not equal to self", secondQuery.equals(secondQuery));
-        assertTrue("query is not equal to its copy", firstQuery.equals(secondQuery));
-        assertTrue("equals is not symmetric", secondQuery.equals(firstQuery));
-        assertThat("query copy's hashcode is different from original hashcode", secondQuery.hashCode(), equalTo(firstQuery.hashCode()));
-
-        QB thirdQuery = copyQuery(secondQuery);
-        assertTrue("query is not equal to self", thirdQuery.equals(thirdQuery));
-        assertTrue("query is not equal to its copy", secondQuery.equals(thirdQuery));
-        assertThat("query copy's hashcode is different from original hashcode", secondQuery.hashCode(), equalTo(thirdQuery.hashCode()));
-        assertTrue("equals is not transitive", firstQuery.equals(thirdQuery));
-        assertThat("query copy's hashcode is different from original hashcode", firstQuery.hashCode(), equalTo(thirdQuery.hashCode()));
-        assertTrue("equals is not symmetric", thirdQuery.equals(secondQuery));
-        assertTrue("equals is not symmetric", thirdQuery.equals(firstQuery));
-
-        if (randomBoolean()) {
-            secondQuery.queryName(secondQuery.queryName() == null ? randomAsciiOfLengthBetween(1, 30) : secondQuery.queryName() + randomAsciiOfLengthBetween(1, 10));
-        } else {
-            secondQuery.boost(firstQuery.boost() + 1f + randomFloat());
-        }
-        assertThat("different queries should not be equal", secondQuery, not(equalTo(firstQuery)));
-        assertThat("different queries should have different hashcode", secondQuery.hashCode(), not(equalTo(firstQuery.hashCode())));
-    }
-
-    private QueryParser<?> queryParser(String queryId) {
-        return queryParserService.indicesQueriesRegistry().queryParsers().get(queryId);
-    }
-
-    //we use the streaming infra to create a copy of the query provided as argument
-    private QB copyQuery(QB query) throws IOException {
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            query.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                QueryBuilder<?> prototype = queryParser(query.getName()).getBuilderPrototype();
-                @SuppressWarnings("unchecked")
-                QB secondQuery = (QB)prototype.readFrom(in);
-                return secondQuery;
-            }
-        }
-    }
-
-    /**
-     * @return a new {@link QueryShardContext} based on the base test index and queryParserService
-     */
-    protected static QueryShardContext createShardContext() {
-        QueryShardContext queryCreationContext = new QueryShardContext(index, queryParserService);
-        queryCreationContext.parseFieldMatcher(ParseFieldMatcher.EMPTY);
-        return queryCreationContext;
-    }
-
-    /**
-     * @return a new {@link QueryParseContext} based on the base test index and queryParserService
-     */
-    protected static QueryParseContext createParseContext() {
-        return createShardContext().parseContext();
-    }
-
-    protected static void assertValidate(QueryBuilder queryBuilder, int totalExpectedErrors) {
-        QueryValidationException queryValidationException = queryBuilder.validate();
-        if (totalExpectedErrors > 0) {
-            assertThat(queryValidationException, notNullValue());
-            assertThat(queryValidationException.validationErrors().size(), equalTo(totalExpectedErrors));
-        } else {
-            assertThat(queryValidationException, nullValue());
-        }
-    }
-
-    /**
-     * create a random value for either {@link BaseQueryTestCase#BOOLEAN_FIELD_NAME}, {@link BaseQueryTestCase#INT_FIELD_NAME},
-     * {@link BaseQueryTestCase#DOUBLE_FIELD_NAME}, {@link BaseQueryTestCase#STRING_FIELD_NAME} or
-     * {@link BaseQueryTestCase#DATE_FIELD_NAME}, or a String value by default
-     */
-    protected static Object getRandomValueForFieldName(String fieldName) {
-        Object value;
-        switch (fieldName) {
-            case STRING_FIELD_NAME:
-                value = rarely() ? randomUnicodeOfLength(10) : randomAsciiOfLengthBetween(1, 10); // unicode in 10% cases
-                break;
-            case INT_FIELD_NAME:
-                value = randomIntBetween(0, 10);
-                break;
-            case DOUBLE_FIELD_NAME:
-                value = randomDouble() * 10;
-                break;
-            case BOOLEAN_FIELD_NAME:
-                value = randomBoolean();
-                break;
-            case DATE_FIELD_NAME:
-                value = new DateTime(System.currentTimeMillis(), DateTimeZone.UTC).toString();
-                break;
-            default:
-                value = randomAsciiOfLengthBetween(1, 10);
-        }
-        return value;
-    }
-
-    /**
-     * Helper method to return a mapped or a random field
-     */
-    protected String getRandomFieldName() {
-        // if no type is set then return a random field name
-        if (currentTypes == null || currentTypes.length == 0 || randomBoolean()) {
-            return randomAsciiOfLengthBetween(1, 10);
-        }
-        return randomFrom(MAPPED_LEAF_FIELD_NAMES);
-    }
-
-    /**
-     * Helper method to return a random field (mapped or unmapped) and a value
-     */
-    protected Tuple<String, Object> getRandomFieldNameAndValue() {
-        String fieldName = getRandomFieldName();
-        return new Tuple<>(fieldName, getRandomValueForFieldName(fieldName));
-    }
-
-    /**
-     * Helper method to return a random rewrite method
-     */
-    protected static String getRandomRewriteMethod() {
-        String rewrite;
-        if (randomBoolean()) {
-            rewrite = randomFrom(QueryParsers.CONSTANT_SCORE,
-                    QueryParsers.SCORING_BOOLEAN,
-                    QueryParsers.CONSTANT_SCORE_BOOLEAN).getPreferredName();
-        } else {
-            rewrite = randomFrom(QueryParsers.TOP_TERMS,
-                    QueryParsers.TOP_TERMS_BOOST,
-                    QueryParsers.TOP_TERMS_BLENDED_FREQS).getPreferredName() + "1";
-        }
-        return rewrite;
-    }
-
-    protected String[] getRandomTypes() {
-        String[] types;
-        if (currentTypes.length > 0 && randomBoolean()) {
-            int numberOfQueryTypes = randomIntBetween(1, currentTypes.length);
-            types = new String[numberOfQueryTypes];
-            for (int i = 0; i < numberOfQueryTypes; i++) {
-                types[i] = randomFrom(currentTypes);
-            }
-        } else {
-            if (randomBoolean()) {
-                types = new String[] { MetaData.ALL };
-            } else {
-                types = new String[0];
-            }
-        }
-        return types;
-    }
-
-    protected String getRandomType() {
-        return (currentTypes.length == 0) ? MetaData.ALL : randomFrom(currentTypes);
-    }
-
-    protected static Fuzziness randomFuzziness(String fieldName) {
-        Fuzziness fuzziness = Fuzziness.AUTO;
-        switch (fieldName) {
-            case INT_FIELD_NAME:
-                fuzziness = Fuzziness.build(randomIntBetween(3, 100));
-                break;
-            case DOUBLE_FIELD_NAME:
-                fuzziness = Fuzziness.build(1 + randomFloat() * 10);
-                break;
-            case DATE_FIELD_NAME:
-                fuzziness = Fuzziness.build(randomTimeValue());
-                break;
-        }
-        if (randomBoolean()) {
-            fuzziness = Fuzziness.fromEdits(randomIntBetween(0, 2));
-        }
-        return fuzziness;
-    }
-
-    protected static boolean isNumericFieldName(String fieldName) {
-        return INT_FIELD_NAME.equals(fieldName) || DOUBLE_FIELD_NAME.equals(fieldName);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/BaseTermQueryTestCase.java b/core/src/test/java/org/elasticsearch/index/query/BaseTermQueryTestCase.java
deleted file mode 100644
index c83a3f5..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/BaseTermQueryTestCase.java
+++ /dev/null
@@ -1,110 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.junit.Test;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.is;
-
-public abstract class BaseTermQueryTestCase<QB extends BaseTermQueryBuilder<QB>> extends BaseQueryTestCase<QB> {
-
-    @Override
-    protected final QB doCreateTestQueryBuilder() {
-        String fieldName = null;
-        Object value;
-        switch (randomIntBetween(0, 3)) {
-            case 0:
-                if (randomBoolean()) {
-                    fieldName = BOOLEAN_FIELD_NAME;
-                }
-                value = randomBoolean();
-                break;
-            case 1:
-                if (randomBoolean()) {
-                    fieldName = STRING_FIELD_NAME;
-                }
-                if (frequently()) {
-                    value = randomAsciiOfLengthBetween(1, 10);
-                } else {
-                    // generate unicode string in 10% of cases
-                    value = randomUnicodeOfLength(10);
-                }
-                break;
-            case 2:
-                if (randomBoolean()) {
-                    fieldName = INT_FIELD_NAME;
-                }
-                value = randomInt(10000);
-                break;
-            case 3:
-                if (randomBoolean()) {
-                    fieldName = DOUBLE_FIELD_NAME;
-                }
-                value = randomDouble();
-                break;
-            default:
-                throw new UnsupportedOperationException();
-        }
-
-        if (fieldName == null) {
-            fieldName = randomAsciiOfLengthBetween(1, 10);
-        }
-        return createQueryBuilder(fieldName, value);
-    }
-
-    protected abstract QB createQueryBuilder(String fieldName, Object value);
-
-    @Test
-    public void testValidate() throws QueryShardException {
-        QB queryBuilder = createQueryBuilder(randomAsciiOfLengthBetween(1, 30), randomAsciiOfLengthBetween(1, 30));
-        assertNull(queryBuilder.validate());
-
-        queryBuilder = createQueryBuilder(null, randomAsciiOfLengthBetween(1, 30));
-        assertNotNull(queryBuilder.validate());
-        assertThat(queryBuilder.validate().validationErrors().size(), is(1));
-
-        queryBuilder = createQueryBuilder("", randomAsciiOfLengthBetween(1, 30));
-        assertNotNull(queryBuilder.validate());
-        assertThat(queryBuilder.validate().validationErrors().size(), is(1));
-
-        queryBuilder = createQueryBuilder("", null);
-        assertNotNull(queryBuilder.validate());
-        assertThat(queryBuilder.validate().validationErrors().size(), is(2));
-    }
-
-    @Override
-    protected Map<String, QB> getAlternateVersions() {
-        HashMap<String, QB> alternateVersions = new HashMap<>();
-        QB tempQuery = createTestQueryBuilder();
-        QB testQuery = createQueryBuilder(tempQuery.fieldName(), tempQuery.value());
-        boolean isString = testQuery.value() instanceof String;
-        String value = (isString ? "\"" : "") + testQuery.value() + (isString ? "\"" : "");
-        String contentString = "{\n" +
-                "    \"" + testQuery.getName() + "\" : {\n" +
-                "        \"" + testQuery.fieldName() + "\" : " + value + "\n" +
-                "    }\n" +
-                "}";
-        alternateVersions.put(contentString, testQuery);
-        return alternateVersions;
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTest.java
deleted file mode 100644
index a235ba1..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTest.java
+++ /dev/null
@@ -1,206 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.*;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class BoolQueryBuilderTest extends BaseQueryTestCase<BoolQueryBuilder> {
-
-    @Override
-    protected BoolQueryBuilder doCreateTestQueryBuilder() {
-        BoolQueryBuilder query = new BoolQueryBuilder();
-        if (randomBoolean()) {
-            query.adjustPureNegative(randomBoolean());
-        }
-        if (randomBoolean()) {
-            query.disableCoord(randomBoolean());
-        }
-        if (randomBoolean()) {
-            query.minimumNumberShouldMatch(randomIntBetween(1, 10));
-        }
-        int mustClauses = randomIntBetween(0, 3);
-        for (int i = 0; i < mustClauses; i++) {
-            query.must(RandomQueryBuilder.createQuery(random()));
-        }
-        int mustNotClauses = randomIntBetween(0, 3);
-        for (int i = 0; i < mustNotClauses; i++) {
-            query.mustNot(RandomQueryBuilder.createQuery(random()));
-        }
-        int shouldClauses = randomIntBetween(0, 3);
-        for (int i = 0; i < shouldClauses; i++) {
-            query.should(RandomQueryBuilder.createQuery(random()));
-        }
-        int filterClauses = randomIntBetween(0, 3);
-        for (int i = 0; i < filterClauses; i++) {
-            query.filter(RandomQueryBuilder.createQuery(random()));
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(BoolQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (!queryBuilder.hasClauses()) {
-            assertThat(query, instanceOf(MatchAllDocsQuery.class));
-        } else {
-            List<BooleanClause> clauses = new ArrayList<>();
-            clauses.addAll(getBooleanClauses(queryBuilder.must(), BooleanClause.Occur.MUST, context));
-            clauses.addAll(getBooleanClauses(queryBuilder.mustNot(), BooleanClause.Occur.MUST_NOT, context));
-            clauses.addAll(getBooleanClauses(queryBuilder.should(), BooleanClause.Occur.SHOULD, context));
-            clauses.addAll(getBooleanClauses(queryBuilder.filter(), BooleanClause.Occur.FILTER, context));
-
-            if (clauses.isEmpty()) {
-                assertThat(query, instanceOf(MatchAllDocsQuery.class));
-            } else {
-                assertThat(query, instanceOf(BooleanQuery.class));
-                BooleanQuery booleanQuery = (BooleanQuery) query;
-                if (queryBuilder.adjustPureNegative()) {
-                    boolean isNegative = true;
-                    for (BooleanClause clause : clauses) {
-                        if (clause.isProhibited() == false) {
-                            isNegative = false;
-                            break;
-                        }
-                    }
-                    if (isNegative) {
-                        clauses.add(new BooleanClause(new MatchAllDocsQuery(), BooleanClause.Occur.MUST));
-                    }
-                }
-                assertThat(booleanQuery.clauses().size(), equalTo(clauses.size()));
-                Iterator<BooleanClause> clauseIterator = clauses.iterator();
-                for (BooleanClause booleanClause : booleanQuery.getClauses()) {
-                    assertThat(booleanClause, equalTo(clauseIterator.next()));
-                }
-            }
-        }
-    }
-
-    private static List<BooleanClause> getBooleanClauses(List<QueryBuilder> queryBuilders, BooleanClause.Occur occur, QueryShardContext context) throws IOException {
-        List<BooleanClause> clauses = new ArrayList<>();
-        for (QueryBuilder query : queryBuilders) {
-            Query innerQuery = query.toQuery(context);
-            if (innerQuery != null) {
-                clauses.add(new BooleanClause(innerQuery, occur));
-            }
-        }
-        return clauses;
-    }
-
-    @Override
-    protected Map<String, BoolQueryBuilder> getAlternateVersions() {
-        Map<String, BoolQueryBuilder> alternateVersions = new HashMap<>();
-        BoolQueryBuilder tempQueryBuilder = createTestQueryBuilder();
-        BoolQueryBuilder expectedQuery = new BoolQueryBuilder();
-        String contentString = "{\n" +
-                "    \"bool\" : {\n";
-        if (tempQueryBuilder.must().size() > 0) {
-            QueryBuilder must = tempQueryBuilder.must().get(0);
-            contentString += "must: " + must.toString() + ",";
-            expectedQuery.must(must);
-        }
-        if (tempQueryBuilder.mustNot().size() > 0) {
-            QueryBuilder mustNot = tempQueryBuilder.mustNot().get(0);
-            contentString += (randomBoolean() ? "must_not: " : "mustNot: ") + mustNot.toString() + ",";
-            expectedQuery.mustNot(mustNot);
-        }
-        if (tempQueryBuilder.should().size() > 0) {
-            QueryBuilder should = tempQueryBuilder.should().get(0);
-            contentString += "should: " + should.toString() + ",";
-            expectedQuery.should(should);
-        }
-        if (tempQueryBuilder.filter().size() > 0) {
-            QueryBuilder filter = tempQueryBuilder.filter().get(0);
-            contentString += "filter: " + filter.toString() + ",";
-            expectedQuery.filter(filter);
-        }
-        contentString = contentString.substring(0, contentString.length() - 1);
-        contentString += "    }    \n" + "}";
-        alternateVersions.put(contentString, expectedQuery);
-        return alternateVersions;
-    }
-
-    @Test
-    public void testValidate() {
-        BoolQueryBuilder booleanQuery = new BoolQueryBuilder();
-        int iters = randomIntBetween(0, 3);
-        int totalExpectedErrors = 0;
-        for (int i = 0; i < iters; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    booleanQuery.must(RandomQueryBuilder.createInvalidQuery(random()));
-                } else {
-                    booleanQuery.must(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                booleanQuery.must(RandomQueryBuilder.createQuery(random()));
-            }
-        }
-        iters = randomIntBetween(0, 3);
-        for (int i = 0; i < iters; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    booleanQuery.should(RandomQueryBuilder.createInvalidQuery(random()));
-                } else {
-                    booleanQuery.should(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                booleanQuery.should(RandomQueryBuilder.createQuery(random()));
-            }
-        }
-        iters = randomIntBetween(0, 3);
-        for (int i = 0; i < iters; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    booleanQuery.mustNot(RandomQueryBuilder.createInvalidQuery(random()));
-                } else {
-                    booleanQuery.mustNot(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                booleanQuery.mustNot(RandomQueryBuilder.createQuery(random()));
-            }
-        }
-        iters = randomIntBetween(0, 3);
-        for (int i = 0; i < iters; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    booleanQuery.filter(RandomQueryBuilder.createInvalidQuery(random()));
-                } else {
-                    booleanQuery.filter(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                booleanQuery.filter(RandomQueryBuilder.createQuery(random()));
-            }
-        }
-        assertValidate(booleanQuery, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/BoostingQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/BoostingQueryBuilderTest.java
deleted file mode 100644
index 2b7ca52..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/BoostingQueryBuilderTest.java
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.queries.BoostingQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.instanceOf;
-import static org.hamcrest.CoreMatchers.nullValue;
-
-public class BoostingQueryBuilderTest extends BaseQueryTestCase<BoostingQueryBuilder> {
-
-    @Override
-    protected BoostingQueryBuilder doCreateTestQueryBuilder() {
-        BoostingQueryBuilder query = new BoostingQueryBuilder(RandomQueryBuilder.createQuery(random()), RandomQueryBuilder.createQuery(random()));
-        query.negativeBoost(2.0f / randomIntBetween(1, 20));
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(BoostingQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Query positive = queryBuilder.positiveQuery().toQuery(context);
-        Query negative = queryBuilder.negativeQuery().toQuery(context);
-        if (positive == null || negative == null) {
-            assertThat(query, nullValue());
-        } else {
-            assertThat(query, instanceOf(BoostingQuery.class));
-        }
-    }
-
-    @Test
-    public void testValidate() {
-        int totalExpectedErrors = 0;
-        QueryBuilder positive = null;
-        QueryBuilder negative = null;
-        if (frequently()) {
-            if (randomBoolean()) {
-                negative = RandomQueryBuilder.createInvalidQuery(random());
-            }
-            totalExpectedErrors++;
-        } else {
-            negative = RandomQueryBuilder.createQuery(random());
-        }
-        if (frequently()) {
-            if (randomBoolean()) {
-                positive = RandomQueryBuilder.createInvalidQuery(random());
-            }
-            totalExpectedErrors++;
-        } else {
-            positive = RandomQueryBuilder.createQuery(random());
-        }
-        BoostingQueryBuilder boostingQuery = new BoostingQueryBuilder(positive, negative);
-        if (frequently()) {
-            boostingQuery.negativeBoost(0.5f);
-        } else {
-            boostingQuery.negativeBoost(-0.5f);
-            totalExpectedErrors++;
-        }
-        assertValidate(boostingQuery, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/CommonTermsQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/CommonTermsQueryBuilderTest.java
deleted file mode 100644
index ab5cb07..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/CommonTermsQueryBuilderTest.java
+++ /dev/null
@@ -1,105 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.queries.ExtendedCommonTermsQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-
-public class CommonTermsQueryBuilderTest extends BaseQueryTestCase<CommonTermsQueryBuilder> {
-
-    @Override
-    protected CommonTermsQueryBuilder doCreateTestQueryBuilder() {
-        CommonTermsQueryBuilder query;
-
-        // mapped or unmapped field
-        String text = randomAsciiOfLengthBetween(1, 10);
-        if (randomBoolean()) {
-            query = new CommonTermsQueryBuilder(STRING_FIELD_NAME, text);
-        } else {
-            query = new CommonTermsQueryBuilder(randomAsciiOfLengthBetween(1, 10), text);
-        }
-
-        if (randomBoolean()) {
-            query.cutoffFrequency((float) randomIntBetween(1, 10));
-        }
-
-        if (randomBoolean()) {
-            query.lowFreqOperator(randomFrom(Operator.values()));
-        }
-
-        // number of low frequency terms that must match
-        if (randomBoolean()) {
-            query.lowFreqMinimumShouldMatch("" + randomIntBetween(1, 5));
-        }
-
-        if (randomBoolean()) {
-            query.highFreqOperator(randomFrom(Operator.values()));
-        }
-
-        // number of high frequency terms that must match
-        if (randomBoolean()) {
-            query.highFreqMinimumShouldMatch("" + randomIntBetween(1, 5));
-        }
-
-        if (randomBoolean()) {
-            query.analyzer(randomFrom("simple", "keyword", "whitespace"));
-        }
-
-        if (randomBoolean()) {
-            query.disableCoord(randomBoolean());
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(CommonTermsQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(ExtendedCommonTermsQuery.class));
-        ExtendedCommonTermsQuery extendedCommonTermsQuery = (ExtendedCommonTermsQuery) query;
-        assertThat(extendedCommonTermsQuery.getHighFreqMinimumNumberShouldMatchSpec(), equalTo(queryBuilder.highFreqMinimumShouldMatch()));
-        assertThat(extendedCommonTermsQuery.getLowFreqMinimumNumberShouldMatchSpec(), equalTo(queryBuilder.lowFreqMinimumShouldMatch()));
-    }
-
-    @Test
-    public void testValidate() {
-        CommonTermsQueryBuilder commonTermsQueryBuilder = new CommonTermsQueryBuilder("", "text");
-        assertThat(commonTermsQueryBuilder.validate().validationErrors().size(), is(1));
-
-        commonTermsQueryBuilder = new CommonTermsQueryBuilder("field", null);
-        assertThat(commonTermsQueryBuilder.validate().validationErrors().size(), is(1));
-
-        commonTermsQueryBuilder = new CommonTermsQueryBuilder("field", "text");
-        assertNull(commonTermsQueryBuilder.validate());
-    }
-
-    @Test
-    public void testNoTermsFromQueryString() throws IOException {
-        CommonTermsQueryBuilder builder = new CommonTermsQueryBuilder(STRING_FIELD_NAME, "");
-        QueryShardContext context = createShardContext();
-        context.setAllowUnmappedFields(true);
-        assertNull(builder.toQuery(context));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/CommonTermsQueryParserTest.java b/core/src/test/java/org/elasticsearch/index/query/CommonTermsQueryParserTest.java
deleted file mode 100644
index c95477e..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/CommonTermsQueryParserTest.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.test.ESSingleNodeTestCase;
-import org.junit.Test;
-
-import java.io.IOException;
-
-public class CommonTermsQueryParserTest extends ESSingleNodeTestCase {
-    @Test
-    public void testWhenParsedQueryIsNullNoNullPointerExceptionIsThrown() throws IOException {
-        final String index = "test-index";
-        final String type = "test-type";
-        client()
-                .admin()
-                .indices()
-                .prepareCreate(index)
-                .addMapping(type, "name", "type=string,analyzer=stop")
-                .execute()
-                .actionGet();
-        ensureGreen();
-
-        CommonTermsQueryBuilder commonTermsQueryBuilder =
-                new CommonTermsQueryBuilder("name", "the").queryName("query-name");
-
-        // the named query parses to null; we are testing this does not cause a NullPointerException
-        SearchResponse response =
-                client().prepareSearch(index).setTypes(type).setQuery(commonTermsQueryBuilder).execute().actionGet();
-
-        assertNotNull(response);
-        assertEquals(response.getHits().hits().length, 0);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/CommonTermsQueryParserTests.java b/core/src/test/java/org/elasticsearch/index/query/CommonTermsQueryParserTests.java
new file mode 100644
index 0000000..d339a5d
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/query/CommonTermsQueryParserTests.java
@@ -0,0 +1,52 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.query;
+
+import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.test.ESSingleNodeTestCase;
+import org.junit.Test;
+
+import java.io.IOException;
+
+public class CommonTermsQueryParserTests extends ESSingleNodeTestCase {
+    @Test
+    public void testWhenParsedQueryIsNullNoNullPointerExceptionIsThrown() throws IOException {
+        final String index = "test-index";
+        final String type = "test-type";
+        client()
+                .admin()
+                .indices()
+                .prepareCreate(index)
+                .addMapping(type, "name", "type=string,analyzer=stop")
+                .execute()
+                .actionGet();
+        ensureGreen();
+
+        CommonTermsQueryBuilder commonTermsQueryBuilder =
+                new CommonTermsQueryBuilder("name", "the").queryName("query-name");
+
+        // the named query parses to null; we are testing this does not cause a NullPointerException
+        SearchResponse response =
+                client().prepareSearch(index).setTypes(type).setQuery(commonTermsQueryBuilder).execute().actionGet();
+
+        assertNotNull(response);
+        assertEquals(response.getHits().hits().length, 0);
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTest.java
deleted file mode 100644
index 95cbe66..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTest.java
+++ /dev/null
@@ -1,76 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.*;
-
-public class ConstantScoreQueryBuilderTest extends BaseQueryTestCase<ConstantScoreQueryBuilder> {
-
-    /**
-     * @return a {@link ConstantScoreQueryBuilder} with random boost between 0.1f and 2.0f
-     */
-    @Override
-    protected ConstantScoreQueryBuilder doCreateTestQueryBuilder() {
-        return new ConstantScoreQueryBuilder(RandomQueryBuilder.createQuery(random()));
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(ConstantScoreQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Query innerQuery = queryBuilder.innerQuery().toQuery(context);
-        if (innerQuery == null) {
-            assertThat(query, nullValue());
-        } else {
-            assertThat(query, instanceOf(ConstantScoreQuery.class));
-            ConstantScoreQuery constantScoreQuery = (ConstantScoreQuery) query;
-            assertThat(constantScoreQuery.getQuery(), equalTo(innerQuery));
-        }
-    }
-
-    /**
-     * test that missing "filter" element causes {@link QueryParsingException}
-     */
-    @Test(expected=QueryParsingException.class)
-    public void testFilterElement() throws IOException {
-        String queryString = "{ \"" + ConstantScoreQueryBuilder.NAME + "\" : {}";
-        parseQuery(queryString);
-    }
-
-    @Test
-    public void testValidate() {
-        QueryBuilder innerQuery = null;
-        int totalExpectedErrors = 0;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                innerQuery = RandomQueryBuilder.createInvalidQuery(random());
-            }
-            totalExpectedErrors++;
-        } else {
-            innerQuery = RandomQueryBuilder.createQuery(random());
-        }
-        ConstantScoreQueryBuilder constantScoreQuery = new ConstantScoreQueryBuilder(innerQuery);
-        assertValidate(constantScoreQuery, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/DisMaxQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/DisMaxQueryBuilderTest.java
deleted file mode 100644
index 6911911..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/DisMaxQueryBuilderTest.java
+++ /dev/null
@@ -1,128 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.DisjunctionMaxQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.Collection;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.*;
-
-public class DisMaxQueryBuilderTest extends BaseQueryTestCase<DisMaxQueryBuilder> {
-
-    /**
-     * @return a {@link DisMaxQueryBuilder} with random inner queries
-     */
-    @Override
-    protected DisMaxQueryBuilder doCreateTestQueryBuilder() {
-        DisMaxQueryBuilder dismax = new DisMaxQueryBuilder();
-        int clauses = randomIntBetween(1, 5);
-        for (int i = 0; i < clauses; i++) {
-            dismax.add(RandomQueryBuilder.createQuery(random()));
-        }
-        if (randomBoolean()) {
-            dismax.tieBreaker(2.0f / randomIntBetween(1, 20));
-        }
-        return dismax;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(DisMaxQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Collection<Query> queries = AbstractQueryBuilder.toQueries(queryBuilder.innerQueries(), context);
-        if (queries.isEmpty()) {
-            assertThat(query, nullValue());
-        } else {
-            assertThat(query, instanceOf(DisjunctionMaxQuery.class));
-            DisjunctionMaxQuery disjunctionMaxQuery = (DisjunctionMaxQuery) query;
-            assertThat(disjunctionMaxQuery.getTieBreakerMultiplier(), equalTo(queryBuilder.tieBreaker()));
-            assertThat(disjunctionMaxQuery.getDisjuncts().size(), equalTo(queries.size()));
-            Iterator<Query> queryIterator = queries.iterator();
-            for (int i = 0; i < disjunctionMaxQuery.getDisjuncts().size(); i++) {
-                assertThat(disjunctionMaxQuery.getDisjuncts().get(i), equalTo(queryIterator.next()));
-            }
-        }
-    }
-
-    @Override
-    protected Map<String, DisMaxQueryBuilder> getAlternateVersions() {
-        Map<String, DisMaxQueryBuilder> alternateVersions = new HashMap<>();
-        QueryBuilder innerQuery = createTestQueryBuilder().innerQueries().get(0);
-        DisMaxQueryBuilder expectedQuery = new DisMaxQueryBuilder();
-        expectedQuery.add(innerQuery);
-        String contentString = "{\n" +
-                "    \"dis_max\" : {\n" +
-                "        \"queries\" : " + innerQuery.toString() +
-                "    }\n" +
-                "}";
-        alternateVersions.put(contentString, expectedQuery);
-        return alternateVersions;
-    }
-
-    /**
-     * test `null`return value for missing inner queries
-     * @throws IOException
-     * @throws QueryParsingException
-     */
-    @Test
-    public void testNoInnerQueries() throws QueryParsingException, IOException {
-        DisMaxQueryBuilder disMaxBuilder = new DisMaxQueryBuilder();
-        assertNull(disMaxBuilder.toQuery(createShardContext()));
-        assertNull(disMaxBuilder.validate());
-    }
-
-    /**
-     * Test inner query parsing to null. Current DSL allows inner filter element to parse to <tt>null</tt>.
-     * Those should be ignored upstream. To test this, we use inner {@link ConstantScoreQueryBuilder}
-     * with empty inner filter.
-     */
-    @Test
-    public void testInnerQueryReturnsNull() throws IOException {
-        String queryString = "{ \"" + ConstantScoreQueryBuilder.NAME + "\" : { \"filter\" : { } } }";
-        QueryBuilder<?> innerQueryBuilder = parseQuery(queryString);
-        DisMaxQueryBuilder disMaxBuilder = new DisMaxQueryBuilder().add(innerQueryBuilder);
-        assertNull(disMaxBuilder.toQuery(createShardContext()));
-    }
-
-    @Test
-    public void testValidate() {
-        DisMaxQueryBuilder disMaxQuery = new DisMaxQueryBuilder();
-        int iters = randomIntBetween(0, 5);
-        int totalExpectedErrors = 0;
-        for (int i = 0; i < iters; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    disMaxQuery.add(RandomQueryBuilder.createInvalidQuery(random()));
-                } else {
-                    disMaxQuery.add(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                disMaxQuery.add(RandomQueryBuilder.createQuery(random()));
-            }
-        }
-        assertValidate(disMaxQuery, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/ExistsQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/ExistsQueryBuilderTest.java
deleted file mode 100644
index 35400e0..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/ExistsQueryBuilderTest.java
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.index.mapper.object.ObjectMapper;
-
-import java.io.IOException;
-import java.util.Collection;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class ExistsQueryBuilderTest extends BaseQueryTestCase<ExistsQueryBuilder> {
-
-    @Override
-    protected ExistsQueryBuilder doCreateTestQueryBuilder() {
-        String fieldPattern;
-        if (randomBoolean()) {
-            fieldPattern = randomFrom(MAPPED_FIELD_NAMES);
-        } else {
-            fieldPattern = randomAsciiOfLengthBetween(1, 10);
-        }
-        // also sometimes test wildcard patterns
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                fieldPattern = fieldPattern + "*";
-            } else {
-                fieldPattern = MetaData.ALL;
-            }
-        }
-        return new ExistsQueryBuilder(fieldPattern);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(ExistsQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        String fieldPattern = queryBuilder.fieldName();
-        ObjectMapper objectMapper = context.getObjectMapper(fieldPattern);
-        if (objectMapper != null) {
-            // automatic make the object mapper pattern
-            fieldPattern = fieldPattern + ".*";
-        }
-        Collection<String> fields = context.simpleMatchToIndexNames(fieldPattern);
-        if (getCurrentTypes().length == 0 || fields.size() == 0) {
-            assertThat(query, instanceOf(BooleanQuery.class));
-            BooleanQuery booleanQuery = (BooleanQuery) query;
-            assertThat(booleanQuery.clauses().size(), equalTo(0));
-        } else {
-            assertThat(query, instanceOf(ConstantScoreQuery.class));
-            ConstantScoreQuery constantScoreQuery = (ConstantScoreQuery) query;
-            assertThat(constantScoreQuery.getQuery(), instanceOf(BooleanQuery.class));
-            BooleanQuery booleanQuery = (BooleanQuery) constantScoreQuery.getQuery();
-            assertThat(booleanQuery.clauses().size(), equalTo(fields.size()));
-            for (int i = 0; i < fields.size(); i++) {
-                BooleanClause booleanClause = booleanQuery.clauses().get(i);
-                assertThat(booleanClause.getOccur(), equalTo(BooleanClause.Occur.SHOULD));
-            }
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/FQueryFilterBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/FQueryFilterBuilderTest.java
deleted file mode 100644
index a739d50..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/FQueryFilterBuilderTest.java
+++ /dev/null
@@ -1,91 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.*;
-
-@SuppressWarnings("deprecation")
-public class FQueryFilterBuilderTest extends BaseQueryTestCase<FQueryFilterBuilder> {
-
-    /**
-     * @return a FQueryFilterBuilder with random inner query
-     */
-    @Override
-    protected FQueryFilterBuilder doCreateTestQueryBuilder() {
-        QueryBuilder innerQuery = RandomQueryBuilder.createQuery(random());
-        return new FQueryFilterBuilder(innerQuery);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(FQueryFilterBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Query innerQuery = queryBuilder.innerQuery().toQuery(context);
-        if (innerQuery == null) {
-            assertThat(query, nullValue());
-        } else {
-            assertThat(query, instanceOf(ConstantScoreQuery.class));
-            ConstantScoreQuery constantScoreQuery = (ConstantScoreQuery) query;
-            assertThat(constantScoreQuery.getQuery(), equalTo(innerQuery));
-        }
-    }
-
-    /**
-     * test corner case where no inner query exist
-     */
-    @Test
-    public void testNoInnerQuery() throws QueryParsingException, IOException {
-        FQueryFilterBuilder queryFilterQuery = new FQueryFilterBuilder(EmptyQueryBuilder.PROTOTYPE);
-        assertNull(queryFilterQuery.toQuery(createShardContext()));
-    }
-
-    /**
-     * test wrapping an inner filter that returns null also returns <tt>null</null> to pass on upwards
-     */
-    @Test
-    public void testInnerQueryReturnsNull() throws IOException {
-        // create inner filter
-        String queryString = "{ \"constant_score\" : { \"filter\" : {} } }";
-        QueryBuilder innerQuery = parseQuery(queryString);
-        // check that when wrapping this filter, toQuery() returns null
-        FQueryFilterBuilder queryFilterQuery = new FQueryFilterBuilder(innerQuery);
-        assertNull(queryFilterQuery.toQuery(createShardContext()));
-    }
-
-    @Test
-    public void testValidate() {
-        QueryBuilder innerQuery = null;
-        int totalExpectedErrors = 0;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                innerQuery = RandomQueryBuilder.createInvalidQuery(random());
-            }
-            totalExpectedErrors++;
-        } else {
-            innerQuery = RandomQueryBuilder.createQuery(random());
-        }
-        FQueryFilterBuilder fQueryFilter = new FQueryFilterBuilder(innerQuery);
-        assertValidate(fQueryFilter, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilderTest.java
deleted file mode 100644
index e2c4f85..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilderTest.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.FieldMaskingSpanQuery;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class FieldMaskingSpanQueryBuilderTest extends BaseQueryTestCase<FieldMaskingSpanQueryBuilder> {
-
-    @Override
-    protected FieldMaskingSpanQueryBuilder doCreateTestQueryBuilder() {
-        String fieldName;
-        if (randomBoolean()) {
-            fieldName = randomFrom(MAPPED_FIELD_NAMES);
-        } else {
-            fieldName = randomAsciiOfLengthBetween(1, 10);
-        }
-        SpanTermQueryBuilder innerQuery = new SpanTermQueryBuilderTest().createTestQueryBuilder();
-        return new FieldMaskingSpanQueryBuilder(innerQuery, fieldName);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(FieldMaskingSpanQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        String fieldInQuery = queryBuilder.fieldName();
-        MappedFieldType fieldType = context.fieldMapper(fieldInQuery);
-        if (fieldType != null) {
-            fieldInQuery = fieldType.names().indexName();
-        }
-        assertThat(query, instanceOf(FieldMaskingSpanQuery.class));
-        FieldMaskingSpanQuery fieldMaskingSpanQuery = (FieldMaskingSpanQuery) query;
-        assertThat(fieldMaskingSpanQuery.getField(), equalTo(fieldInQuery));
-        assertThat(fieldMaskingSpanQuery.getMaskedQuery(), equalTo(queryBuilder.innerQuery().toQuery(context)));
-    }
-
-    @Test
-    public void testValidate() {
-        String fieldName = null;
-        SpanQueryBuilder spanQueryBuilder = null;
-        int totalExpectedErrors = 0;
-        if (randomBoolean()) {
-            fieldName = "fieldName";
-        } else {
-            if (randomBoolean()) {
-                fieldName = "";
-            }
-            totalExpectedErrors++;
-        }
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                spanQueryBuilder = new SpanTermQueryBuilder("", "test");
-            }
-            totalExpectedErrors++;
-        } else {
-            spanQueryBuilder = new SpanTermQueryBuilder("name", "value");
-        }
-        FieldMaskingSpanQueryBuilder queryBuilder = new FieldMaskingSpanQueryBuilder(spanQueryBuilder, fieldName);
-        assertValidate(queryBuilder, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/FilteredQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/FilteredQueryBuilderTest.java
deleted file mode 100644
index 9eb7f5e..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/FilteredQueryBuilderTest.java
+++ /dev/null
@@ -1,113 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.lucene.search.Queries;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-import static org.hamcrest.CoreMatchers.nullValue;
-
-@SuppressWarnings("deprecation")
-public class FilteredQueryBuilderTest extends BaseQueryTestCase<FilteredQueryBuilder> {
-
-    @Override
-    protected FilteredQueryBuilder doCreateTestQueryBuilder() {
-        QueryBuilder queryBuilder = RandomQueryBuilder.createQuery(random());
-        QueryBuilder filterBuilder = RandomQueryBuilder.createQuery(random());
-        return new FilteredQueryBuilder(queryBuilder, filterBuilder);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(FilteredQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Query innerQuery = queryBuilder.innerQuery().toQuery(context);
-        if (innerQuery == null) {
-            assertThat(query, nullValue());
-        } else {
-            Query innerFilter = queryBuilder.innerFilter().toQuery(context);
-            if (innerFilter == null || Queries.isConstantMatchAllQuery(innerFilter)) {
-                innerQuery.setBoost(queryBuilder.boost());
-                assertThat(query, equalTo(innerQuery));
-            } else if (Queries.isConstantMatchAllQuery(innerQuery)) {
-                assertThat(query, instanceOf(ConstantScoreQuery.class));
-                assertThat(((ConstantScoreQuery)query).getQuery(), equalTo(innerFilter));
-            } else {
-                assertThat(query, instanceOf(BooleanQuery.class));
-                BooleanQuery booleanQuery = (BooleanQuery) query;
-                assertThat(booleanQuery.clauses().size(), equalTo(2));
-                assertThat(booleanQuery.clauses().get(0).getOccur(), equalTo(BooleanClause.Occur.MUST));
-                assertThat(booleanQuery.clauses().get(0).getQuery(), equalTo(innerQuery));
-                assertThat(booleanQuery.clauses().get(1).getOccur(), equalTo(BooleanClause.Occur.FILTER));
-                assertThat(booleanQuery.clauses().get(1).getQuery(), equalTo(innerFilter));
-            }
-        }
-    }
-
-    @Test
-    public void testValidation() {
-        QueryBuilder valid = RandomQueryBuilder.createQuery(random());
-        QueryBuilder invalid = RandomQueryBuilder.createInvalidQuery(random());
-
-        // invalid cases
-        FilteredQueryBuilder qb = new FilteredQueryBuilder(invalid);
-        QueryValidationException result = qb.validate();
-        assertNotNull(result);
-        assertEquals(1, result.validationErrors().size());
-
-        qb = new FilteredQueryBuilder(valid, invalid);
-        result = qb.validate();
-        assertNotNull(result);
-        assertEquals(1, result.validationErrors().size());
-
-        qb = new FilteredQueryBuilder(invalid, valid);
-        result = qb.validate();
-        assertNotNull(result);
-        assertEquals(1, result.validationErrors().size());
-
-        qb = new FilteredQueryBuilder(invalid, invalid);
-        result = qb.validate();
-        assertNotNull(result);
-        assertEquals(2, result.validationErrors().size());
-
-        // valid cases
-        qb = new FilteredQueryBuilder(valid);
-        assertNull(qb.validate());
-
-        qb = new FilteredQueryBuilder(null);
-        assertNull(qb.validate());
-
-        qb = new FilteredQueryBuilder(null, valid);
-        assertNull(qb.validate());
-
-        qb = new FilteredQueryBuilder(valid, null);
-        assertNull(qb.validate());
-
-        qb = new FilteredQueryBuilder(valid, valid);
-        assertNull(qb.validate());
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/FuzzyQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/FuzzyQueryBuilderTest.java
deleted file mode 100644
index 5f296be..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/FuzzyQueryBuilderTest.java
+++ /dev/null
@@ -1,98 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.FuzzyQuery;
-import org.apache.lucene.search.NumericRangeQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.collect.Tuple;
-import org.elasticsearch.common.unit.Fuzziness;
-import org.hamcrest.Matchers;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-
-public class FuzzyQueryBuilderTest extends BaseQueryTestCase<FuzzyQueryBuilder> {
-
-    @Override
-    protected FuzzyQueryBuilder doCreateTestQueryBuilder() {
-        Tuple<String, Object> fieldAndValue = getRandomFieldNameAndValue();
-        FuzzyQueryBuilder query = new FuzzyQueryBuilder(fieldAndValue.v1(), fieldAndValue.v2());
-        if (randomBoolean()) {
-            query.fuzziness(randomFuzziness(query.fieldName()));
-        }
-        if (randomBoolean()) {
-            query.prefixLength(randomIntBetween(0, 10));
-        }
-        if (randomBoolean()) {
-            query.maxExpansions(randomIntBetween(1, 10));
-        }
-        if (randomBoolean()) {
-            query.transpositions(randomBoolean());
-        }
-        if (randomBoolean()) {
-            query.rewrite(getRandomRewriteMethod());
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(FuzzyQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (isNumericFieldName(queryBuilder.fieldName()) || queryBuilder.fieldName().equals(DATE_FIELD_NAME)) {
-            assertThat(query, instanceOf(NumericRangeQuery.class));
-        } else {
-            assertThat(query, instanceOf(FuzzyQuery.class));
-        }
-    }
-
-    @Test
-    public void testValidate() {
-        FuzzyQueryBuilder fuzzyQueryBuilder = new FuzzyQueryBuilder("", "text");
-        assertThat(fuzzyQueryBuilder.validate().validationErrors().size(), is(1));
-
-        fuzzyQueryBuilder = new FuzzyQueryBuilder("field", null);
-        assertThat(fuzzyQueryBuilder.validate().validationErrors().size(), is(1));
-
-        fuzzyQueryBuilder = new FuzzyQueryBuilder("field", "text");
-        assertNull(fuzzyQueryBuilder.validate());
-
-        fuzzyQueryBuilder = new FuzzyQueryBuilder(null, null);
-        assertThat(fuzzyQueryBuilder.validate().validationErrors().size(), is(2));
-    }
-    
-    @Test
-    public void testUnsupportedFuzzinessForStringType() throws IOException {
-        QueryShardContext context = createShardContext();
-        context.setAllowUnmappedFields(true);
-        
-        FuzzyQueryBuilder fuzzyQueryBuilder = new FuzzyQueryBuilder(STRING_FIELD_NAME, "text");
-        fuzzyQueryBuilder.fuzziness(Fuzziness.build(randomFrom("a string which is not auto", "3h", "200s")));
-
-        try {
-            fuzzyQueryBuilder.toQuery(context);
-            fail("should have failed with NumberFormatException");
-        } catch (NumberFormatException e) {
-            assertThat(e.getMessage(), Matchers.containsString("For input string"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTest.java
deleted file mode 100644
index 73913f8..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTest.java
+++ /dev/null
@@ -1,124 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-
-import org.apache.lucene.queries.TermsQuery;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class IdsQueryBuilderTest extends BaseQueryTestCase<IdsQueryBuilder> {
-
-    /**
-     * check that parser throws exception on missing values field
-     * @throws IOException
-     */
-    @Test(expected=QueryParsingException.class)
-    public void testIdsNotProvided() throws IOException {
-        String noIdsFieldQuery = "{\"ids\" : { \"type\" : \"my_type\"  }";
-        parseQuery(noIdsFieldQuery);
-    }
-
-    @Override
-    protected IdsQueryBuilder doCreateTestQueryBuilder() {
-        String[] types;
-        if (getCurrentTypes().length > 0 && randomBoolean()) {
-            int numberOfTypes = randomIntBetween(1, getCurrentTypes().length);
-            types = new String[numberOfTypes];
-            for (int i = 0; i < numberOfTypes; i++) {
-                if (frequently()) {
-                    types[i] = randomFrom(getCurrentTypes());
-                } else {
-                    types[i] = randomAsciiOfLengthBetween(1, 10);
-                }
-            }
-        } else {
-            if (randomBoolean()) {
-                types = new String[]{MetaData.ALL};
-            } else {
-                types = new String[0];
-            }
-        }
-        int numberOfIds = randomIntBetween(0, 10);
-        String[] ids = new String[numberOfIds];
-        for (int i = 0; i < numberOfIds; i++) {
-            ids[i] = randomAsciiOfLengthBetween(1, 10);
-        }
-        IdsQueryBuilder query;
-        if (types.length > 0 || randomBoolean()) {
-            query = new IdsQueryBuilder(types);
-            query.addIds(ids);
-        } else {
-            query = new IdsQueryBuilder();
-            query.addIds(ids);
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(IdsQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (queryBuilder.ids().size() == 0) {
-            assertThat(query, instanceOf(BooleanQuery.class));
-            assertThat(((BooleanQuery)query).clauses().size(), equalTo(0));
-        } else {
-            assertThat(query, instanceOf(TermsQuery.class));
-        }
-    }
-
-    @Override
-    protected Map<String, IdsQueryBuilder> getAlternateVersions() {
-        Map<String, IdsQueryBuilder> alternateVersions = new HashMap<>();
-
-        IdsQueryBuilder tempQuery = createTestQueryBuilder();
-        if (tempQuery.types() != null && tempQuery.types().length > 0) {
-            String type = tempQuery.types()[0];
-            IdsQueryBuilder testQuery = new IdsQueryBuilder(type);
-
-            //single value type can also be called _type
-            String contentString1 = "{\n" +
-                        "    \"ids\" : {\n" +
-                        "        \"_type\" : \"" + type + "\",\n" +
-                        "        \"values\" : []\n" +
-                        "    }\n" +
-                        "}";
-            alternateVersions.put(contentString1, testQuery);
-
-            //array of types can also be called type rather than types
-            String contentString2 = "{\n" +
-                        "    \"ids\" : {\n" +
-                        "        \"type\" : [\"" + type + "\"],\n" +
-                        "        \"values\" : []\n" +
-                        "    }\n" +
-                        "}";
-            alternateVersions.put(contentString2, testQuery);
-        }
-
-        return alternateVersions;
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeTimezoneTests.java b/core/src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeTimezoneTests.java
index 6222f3b..d581aa6 100644
--- a/core/src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeTimezoneTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeTimezoneTests.java
@@ -83,7 +83,7 @@ public class IndexQueryParserFilterDateRangeTimezoneTests extends ESSingleNodeTe
             SearchContext.setCurrent(new TestSearchContext());
             queryParser.parse(query).query();
             fail("A Range Filter on a numeric field with a TimeZone should raise a QueryParsingException");
-        } catch (QueryShardException e) {
+        } catch (QueryParsingException e) {
             // We expect it
         } finally {
             SearchContext.removeCurrent();
@@ -120,7 +120,7 @@ public class IndexQueryParserFilterDateRangeTimezoneTests extends ESSingleNodeTe
             SearchContext.setCurrent(new TestSearchContext());
             queryParser.parse(query).query();
             fail("A Range Query on a numeric field with a TimeZone should raise a QueryParsingException");
-        } catch (QueryShardException e) {
+        } catch (QueryParsingException e) {
             // We expect it
         } finally {
             SearchContext.removeCurrent();
diff --git a/core/src/test/java/org/elasticsearch/index/query/IndicesQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/IndicesQueryBuilderTest.java
deleted file mode 100644
index c045bb0..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/IndicesQueryBuilderTest.java
+++ /dev/null
@@ -1,98 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-
-public class IndicesQueryBuilderTest extends BaseQueryTestCase<IndicesQueryBuilder> {
-
-    @Override
-    protected IndicesQueryBuilder doCreateTestQueryBuilder() {
-        String[] indices;
-        if (randomBoolean()) {
-            indices = new String[]{getIndex().getName()};
-        } else {
-            indices = generateRandomStringArray(5, 10, false, false);
-        }
-        IndicesQueryBuilder query = new IndicesQueryBuilder(RandomQueryBuilder.createQuery(random()), indices);
-
-        switch (randomInt(2)) {
-            case 0:
-                query.noMatchQuery(RandomQueryBuilder.createQuery(random()));
-                break;
-            case 1:
-                query.noMatchQuery(randomFrom(QueryBuilders.matchAllQuery(), new MatchNoneQueryBuilder()));
-                break;
-            default:
-                // do not set noMatchQuery
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(IndicesQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Query expected;
-        if (queryBuilder.indices().length == 1 && getIndex().getName().equals(queryBuilder.indices()[0])) {
-            expected = queryBuilder.innerQuery().toQuery(context);
-        } else {
-            expected = queryBuilder.noMatchQuery().toQuery(context);
-        }
-        if (expected != null) {
-            expected.setBoost(queryBuilder.boost());
-        }
-        assertEquals(query, expected);
-    }
-
-    @Test
-    public void testValidate() {
-        int expectedErrors = 0;
-
-        // inner query
-        QueryBuilder innerQuery;
-        if (randomBoolean()) {
-            // setting innerQuery to null would be caught in the builder already and make validation fail
-            innerQuery = RandomQueryBuilder.createInvalidQuery(random());
-            expectedErrors++;
-        } else {
-            innerQuery = RandomQueryBuilder.createQuery(random());
-        }
-        // indices
-        String[] indices;
-        if (randomBoolean()) {
-            indices = randomBoolean() ? null : new String[0];
-            expectedErrors++;
-        } else {
-            indices = new String[]{"index"};
-        }
-        // no match query
-        QueryBuilder noMatchQuery;
-        if (randomBoolean()) {
-            noMatchQuery = RandomQueryBuilder.createInvalidQuery(random());
-            expectedErrors++;
-        } else {
-            noMatchQuery = RandomQueryBuilder.createQuery(random());
-        }
-
-        assertValidate(new IndicesQueryBuilder(innerQuery, indices).noMatchQuery(noMatchQuery), expectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/LimitQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/LimitQueryBuilderTest.java
deleted file mode 100644
index 59bb644..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/LimitQueryBuilderTest.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class LimitQueryBuilderTest extends BaseQueryTestCase<LimitQueryBuilder> {
-
-    /**
-     * @return a LimitQueryBuilder with random limit between 0 and 20
-     */
-    @Override
-    protected LimitQueryBuilder doCreateTestQueryBuilder() {
-        return new LimitQueryBuilder(randomIntBetween(0, 20));
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(LimitQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(MatchAllDocsQuery.class));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/MatchAllQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/MatchAllQueryBuilderTest.java
deleted file mode 100644
index 277717c..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/MatchAllQueryBuilderTest.java
+++ /dev/null
@@ -1,40 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class MatchAllQueryBuilderTest extends BaseQueryTestCase<MatchAllQueryBuilder> {
-
-    @Override
-    protected MatchAllQueryBuilder doCreateTestQueryBuilder() {
-        return new MatchAllQueryBuilder();
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(MatchAllQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(MatchAllDocsQuery.class));
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/index/query/MatchNoneQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/MatchNoneQueryBuilderTest.java
deleted file mode 100644
index 1a78992..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/MatchNoneQueryBuilderTest.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.lucene.search.Queries;
-
-import java.io.IOException;
-
-public class MatchNoneQueryBuilderTest extends BaseQueryTestCase {
-
-    @Override
-    protected boolean supportsBoostAndQueryName() {
-        return false;
-    }
-
-    @Override
-    protected AbstractQueryBuilder doCreateTestQueryBuilder() {
-        return new MatchNoneQueryBuilder();
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(AbstractQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertEquals(query, Queries.newMatchNoDocsQuery());
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/MissingQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/MissingQueryBuilderTest.java
deleted file mode 100644
index 54b6a58..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/MissingQueryBuilderTest.java
+++ /dev/null
@@ -1,73 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.is;
-
-public class MissingQueryBuilderTest extends BaseQueryTestCase<MissingQueryBuilder> {
-
-    @Override
-    protected MissingQueryBuilder doCreateTestQueryBuilder() {
-        MissingQueryBuilder query  = new MissingQueryBuilder(randomBoolean() ? randomFrom(MAPPED_FIELD_NAMES) : randomAsciiOfLengthBetween(1, 10));
-        if (randomBoolean()) {
-            query.nullValue(randomBoolean());
-        }
-        if (randomBoolean()) {
-            query.existence(randomBoolean());
-        }
-        // cannot set both to false
-        if ((query.nullValue() == false) && (query.existence() == false)) {
-            query.existence(!query.existence());
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(MissingQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        //too many mapping dependent cases to test, we don't want to end up duplication the toQuery method
-    }
-
-    @Test
-    public void testValidate() {
-        MissingQueryBuilder missingQueryBuilder = new MissingQueryBuilder("");
-        assertThat(missingQueryBuilder.validate().validationErrors().size(), is(1));
-
-        missingQueryBuilder = new MissingQueryBuilder(null);
-        assertThat(missingQueryBuilder.validate().validationErrors().size(), is(1));
-
-        missingQueryBuilder = new MissingQueryBuilder("field").existence(false).nullValue(false);
-        assertThat(missingQueryBuilder.validate().validationErrors().size(), is(1));
-
-        missingQueryBuilder = new MissingQueryBuilder("field");
-        assertNull(missingQueryBuilder.validate());
-    }
-
-    @Test(expected = QueryShardException.class)
-    public void testBothNullValueAndExistenceFalse() throws IOException {
-        QueryShardContext context = createShardContext();
-        context.setAllowUnmappedFields(true);
-        MissingQueryBuilder.newFilter(context, "field", false, false);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/NotQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/NotQueryBuilderTest.java
deleted file mode 100644
index b117b9b..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/NotQueryBuilderTest.java
+++ /dev/null
@@ -1,108 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.*;
-
-public class NotQueryBuilderTest extends BaseQueryTestCase<NotQueryBuilder> {
-
-    /**
-     * @return a NotQueryBuilder with random limit between 0 and 20
-     */
-    @Override
-    protected NotQueryBuilder doCreateTestQueryBuilder() {
-        return new NotQueryBuilder(RandomQueryBuilder.createQuery(random()));
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(NotQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Query filter = queryBuilder.innerQuery().toQuery(context);
-        if (filter == null) {
-            assertThat(query, nullValue());
-        } else {
-            assertThat(query, instanceOf(BooleanQuery.class));
-            BooleanQuery booleanQuery = (BooleanQuery) query;
-            assertThat(booleanQuery.clauses().size(), equalTo(2));
-            assertThat(booleanQuery.clauses().get(0).getOccur(), equalTo(BooleanClause.Occur.MUST));
-            assertThat(booleanQuery.clauses().get(0).getQuery(), instanceOf(MatchAllDocsQuery.class));
-            assertThat(booleanQuery.clauses().get(1).getOccur(), equalTo(BooleanClause.Occur.MUST_NOT));
-            assertThat(booleanQuery.clauses().get(1).getQuery(), equalTo(filter));
-        }
-    }
-
-    /**
-     * @throws IOException
-     */
-    @Test(expected=QueryParsingException.class)
-    public void testMissingFilterSection() throws IOException {
-        String queryString = "{ \"not\" : {}";
-        parseQuery(queryString);
-    }
-
-    @Override
-    protected Map<String, NotQueryBuilder> getAlternateVersions() {
-        Map<String, NotQueryBuilder> alternateVersions = new HashMap<>();
-
-        NotQueryBuilder testQuery1 = new NotQueryBuilder(createTestQueryBuilder().innerQuery());
-        String contentString1 = "{\n" +
-                "    \"not\" : {\n" +
-                "        \"filter\" : " + testQuery1.innerQuery().toString() + "\n" +
-                "    }\n" +
-                "}";
-        alternateVersions.put(contentString1, testQuery1);
-
-        QueryBuilder innerQuery = createTestQueryBuilder().innerQuery();
-        //not doesn't support empty query when query/filter element is not specified
-        if (innerQuery != EmptyQueryBuilder.PROTOTYPE) {
-            NotQueryBuilder testQuery2 = new NotQueryBuilder(innerQuery);
-            String contentString2 = "{\n" +
-                    "    \"not\" : " + testQuery2.innerQuery().toString() +  "\n}";
-            alternateVersions.put(contentString2, testQuery2);
-        }
-
-        return alternateVersions;
-    }
-
-    @Test
-    public void testValidate() {
-        QueryBuilder innerQuery = null;
-        int totalExpectedErrors = 0;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                innerQuery = RandomQueryBuilder.createInvalidQuery(random());
-            }
-            totalExpectedErrors++;
-        } else {
-            innerQuery = RandomQueryBuilder.createQuery(random());
-        }
-        NotQueryBuilder notQuery = new NotQueryBuilder(innerQuery);
-        assertValidate(notQuery, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/OrQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/OrQueryBuilderTest.java
deleted file mode 100644
index ba1cc1c..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/OrQueryBuilderTest.java
+++ /dev/null
@@ -1,120 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.*;
-
-import static org.hamcrest.CoreMatchers.*;
-
-@SuppressWarnings("deprecation")
-public class OrQueryBuilderTest extends BaseQueryTestCase<OrQueryBuilder> {
-
-    /**
-     * @return an OrQueryBuilder with random limit between 0 and 20
-     */
-    @Override
-    protected OrQueryBuilder doCreateTestQueryBuilder() {
-        OrQueryBuilder query = new OrQueryBuilder();
-        int subQueries = randomIntBetween(1, 5);
-        for (int i = 0; i < subQueries; i++ ) {
-            query.add(RandomQueryBuilder.createQuery(random()));
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(OrQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (queryBuilder.innerQueries().isEmpty()) {
-            assertThat(query, nullValue());
-        } else {
-            List<Query> innerQueries = new ArrayList<>();
-            for (QueryBuilder subQuery : queryBuilder.innerQueries()) {
-                Query innerQuery = subQuery.toQuery(context);
-                // ignore queries that are null
-                if (innerQuery != null) {
-                    innerQueries.add(innerQuery);
-                }
-            }
-            if (innerQueries.isEmpty()) {
-                assertThat(query, nullValue());
-            } else {
-                assertThat(query, instanceOf(BooleanQuery.class));
-                BooleanQuery booleanQuery = (BooleanQuery) query;
-                assertThat(booleanQuery.clauses().size(), equalTo(innerQueries.size()));
-                Iterator<Query> queryIterator = innerQueries.iterator();
-                for (BooleanClause booleanClause : booleanQuery) {
-                    assertThat(booleanClause.getOccur(), equalTo(BooleanClause.Occur.SHOULD));
-                    assertThat(booleanClause.getQuery(), equalTo(queryIterator.next()));
-                }
-            }
-        }
-    }
-
-    /**
-     * test corner case where no inner queries exist
-     */
-    @Test
-    public void testNoInnerQueries() throws QueryShardException, IOException {
-        OrQueryBuilder orQuery = new OrQueryBuilder();
-        assertNull(orQuery.toQuery(createShardContext()));
-    }
-
-    @Override
-    protected Map<String, OrQueryBuilder> getAlternateVersions() {
-        Map<String, OrQueryBuilder> alternateVersions = new HashMap<>();
-        QueryBuilder innerQuery = createTestQueryBuilder().innerQueries().get(0);
-        OrQueryBuilder expectedQuery = new OrQueryBuilder(innerQuery);
-        String contentString =  "{ \"or\" : [ " + innerQuery + "] }";
-        alternateVersions.put(contentString, expectedQuery);
-        return alternateVersions;
-    }
-
-    @Test(expected=QueryParsingException.class)
-    public void testMissingFiltersSection() throws IOException {
-        String queryString = "{ \"or\" : {}";
-        parseQuery(queryString);
-    }
-
-    @Test
-    public void testValidate() {
-        OrQueryBuilder orQuery = new OrQueryBuilder();
-        int iters = randomIntBetween(0, 5);
-        int totalExpectedErrors = 0;
-        for (int i = 0; i < iters; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    orQuery.add(RandomQueryBuilder.createInvalidQuery(random()));
-                } else {
-                    orQuery.add(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                orQuery.add(RandomQueryBuilder.createQuery(random()));
-            }
-        }
-        assertValidate(orQuery, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/PrefixQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/PrefixQueryBuilderTest.java
deleted file mode 100644
index 42fe5ab..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/PrefixQueryBuilderTest.java
+++ /dev/null
@@ -1,67 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.PrefixQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-
-public class PrefixQueryBuilderTest extends BaseQueryTestCase<PrefixQueryBuilder> {
-
-    @Override
-    protected PrefixQueryBuilder doCreateTestQueryBuilder() {
-        String fieldName = randomBoolean() ? STRING_FIELD_NAME : randomAsciiOfLengthBetween(1, 10);
-        String value = randomAsciiOfLengthBetween(1, 10);
-        PrefixQueryBuilder query = new PrefixQueryBuilder(fieldName, value);
-
-        if (randomBoolean()) {
-            query.rewrite(getRandomRewriteMethod());
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(PrefixQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(PrefixQuery.class));
-        PrefixQuery prefixQuery = (PrefixQuery) query;
-        assertThat(prefixQuery.getPrefix().field(), equalTo(queryBuilder.fieldName()));
-    }
-
-    @Test
-    public void testValidate() {
-        PrefixQueryBuilder prefixQueryBuilder = new PrefixQueryBuilder("", "prefix");
-        assertThat(prefixQueryBuilder.validate().validationErrors().size(), is(1));
-
-        prefixQueryBuilder = new PrefixQueryBuilder("field", null);
-        assertThat(prefixQueryBuilder.validate().validationErrors().size(), is(1));
-
-        prefixQueryBuilder = new PrefixQueryBuilder("field", "prefix");
-        assertNull(prefixQueryBuilder.validate());
-
-        prefixQueryBuilder = new PrefixQueryBuilder(null, null);
-        assertThat(prefixQueryBuilder.validate().validationErrors().size(), is(2));
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/index/query/QueryFilterBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/QueryFilterBuilderTest.java
deleted file mode 100644
index 0841923..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/QueryFilterBuilderTest.java
+++ /dev/null
@@ -1,84 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.*;
-
-@SuppressWarnings("deprecation")
-public class QueryFilterBuilderTest extends BaseQueryTestCase<QueryFilterBuilder> {
-
-    @Override
-    protected QueryFilterBuilder doCreateTestQueryBuilder() {
-        QueryBuilder innerQuery = RandomQueryBuilder.createQuery(random());
-        return new QueryFilterBuilder(innerQuery);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(QueryFilterBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Query innerQuery = queryBuilder.innerQuery().toQuery(context);
-        if (innerQuery == null) {
-            assertThat(query, nullValue());
-        } else {
-            assertThat(query, instanceOf(ConstantScoreQuery.class));
-            ConstantScoreQuery constantScoreQuery = (ConstantScoreQuery) query;
-            assertThat(constantScoreQuery.getQuery(), equalTo(innerQuery));
-        }
-    }
-
-    @Override
-    protected boolean supportsBoostAndQueryName() {
-        return false;
-    }
-
-    /**
-     * test wrapping an inner filter that returns null also returns <tt>null</null> to pass on upwards
-     */
-    @Test
-    public void testInnerQueryReturnsNull() throws IOException {
-        // create inner filter
-        String queryString = "{ \"constant_score\" : { \"filter\" : {} } }";
-        QueryBuilder<?> innerQuery = parseQuery(queryString);
-        // check that when wrapping this filter, toQuery() returns null
-        QueryFilterBuilder queryFilterQuery = new QueryFilterBuilder(innerQuery);
-        assertNull(queryFilterQuery.toQuery(createShardContext()));
-    }
-
-    @Test
-    public void testValidate() {
-        QueryBuilder innerQuery = null;
-        int totalExpectedErrors = 0;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                innerQuery = RandomQueryBuilder.createInvalidQuery(random());
-            }
-            totalExpectedErrors++;
-        } else {
-            innerQuery = RandomQueryBuilder.createQuery(random());
-        }
-        QueryFilterBuilder fQueryFilter = new QueryFilterBuilder(innerQuery);
-        assertValidate(fQueryFilter, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/RandomQueryBuilder.java b/core/src/test/java/org/elasticsearch/index/query/RandomQueryBuilder.java
deleted file mode 100644
index e86a0ec..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/RandomQueryBuilder.java
+++ /dev/null
@@ -1,91 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import com.carrotsearch.randomizedtesting.generators.RandomInts;
-import com.carrotsearch.randomizedtesting.generators.RandomStrings;
-
-import java.util.Random;
-
-/**
- * Utility class for creating random QueryBuilders.
- * So far only leaf queries like {@link MatchAllQueryBuilder}, {@link TermQueryBuilder} or
- * {@link IdsQueryBuilder} are returned.
- */
-public class RandomQueryBuilder {
-
-    /**
-     * Create a new query of a random type
-     * @param r random seed
-     * @return a random {@link QueryBuilder}
-     */
-    public static QueryBuilder createQuery(Random r) {
-        switch (RandomInts.randomIntBetween(r, 0, 4)) {
-            case 0:
-                return new MatchAllQueryBuilderTest().createTestQueryBuilder();
-            case 1:
-                return new TermQueryBuilderTest().createTestQueryBuilder();
-            case 2:
-                return new IdsQueryBuilderTest().createTestQueryBuilder();
-            case 3:
-                return createMultiTermQuery(r);
-            case 4:
-                return EmptyQueryBuilder.PROTOTYPE;
-            default:
-                throw new UnsupportedOperationException();
-        }
-    }
-
-    /**
-     * Create a new multi term query of a random type
-     * @param r random seed
-     * @return a random {@link MultiTermQueryBuilder}
-     */
-    public static MultiTermQueryBuilder createMultiTermQuery(Random r) {
-        // for now, only use String Rangequeries for MultiTerm test, numeric and date makes little sense
-        // see issue #12123 for discussion
-        // Prefix / Fuzzy / RegEx / Wildcard can go here later once refactored and they have random query generators
-        RangeQueryBuilder query = new RangeQueryBuilder(BaseQueryTestCase.STRING_FIELD_NAME);
-        query.from("a" + RandomStrings.randomAsciiOfLengthBetween(r, 1, 10));
-        query.to("z" + RandomStrings.randomAsciiOfLengthBetween(r, 1, 10));
-        return query;
-    }
-
-    /**
-     * Create a new invalid query of a random type
-     * @param r random seed
-     * @return a random {@link QueryBuilder} that is invalid, meaning that calling validate against it
-     * will return an error. We can rely on the fact that a single error will be returned per query.
-     */
-    public static QueryBuilder createInvalidQuery(Random r) {
-        switch (RandomInts.randomIntBetween(r, 0, 3)) {
-            case 0:
-                return new TermQueryBuilder("", "test");
-            case 1:
-                return new BoostingQueryBuilder(new MatchAllQueryBuilder(), new MatchAllQueryBuilder()).negativeBoost(-1f);
-            case 2:
-                return new CommonTermsQueryBuilder("", "text");
-            case 3:
-                return new SimpleQueryStringBuilder(null);
-            default:
-                throw new UnsupportedOperationException();
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTest.java
deleted file mode 100644
index 00753d8..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTest.java
+++ /dev/null
@@ -1,144 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.NumericRangeQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermRangeQuery;
-import org.joda.time.DateTime;
-import org.joda.time.DateTimeZone;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-
-public class RangeQueryBuilderTest extends BaseQueryTestCase<RangeQueryBuilder> {
-
-    private static final List<String> TIMEZONE_IDS = new ArrayList<>(DateTimeZone.getAvailableIDs());
-
-    @Override
-    protected RangeQueryBuilder doCreateTestQueryBuilder() {
-        RangeQueryBuilder query;
-        // switch between numeric and date ranges
-        switch (randomIntBetween(0, 2)) {
-            case 0:
-                if (randomBoolean()) {
-                    // use mapped integer field for numeric range queries
-                    query = new RangeQueryBuilder(INT_FIELD_NAME);
-                    query.from(randomIntBetween(1, 100));
-                    query.to(randomIntBetween(101, 200));
-                } else {
-                    // use unmapped field for numeric range queries
-                    query = new RangeQueryBuilder(randomAsciiOfLengthBetween(1, 10));
-                    query.from(0.0 - randomDouble());
-                    query.to(randomDouble());
-                }
-                break;
-            case 1:
-                // use mapped date field, using date string representation
-                query = new RangeQueryBuilder(DATE_FIELD_NAME);
-                query.from(new DateTime(System.currentTimeMillis() - randomIntBetween(0, 1000000), DateTimeZone.UTC).toString());
-                query.to(new DateTime(System.currentTimeMillis() + randomIntBetween(0, 1000000), DateTimeZone.UTC).toString());
-                // Create timestamp option only then we have a date mapper,
-                // otherwise we could trigger exception.
-                if (createShardContext().mapperService().smartNameFieldType(DATE_FIELD_NAME) != null) {
-                    if (randomBoolean()) {
-                        query.timeZone(TIMEZONE_IDS.get(randomIntBetween(0, TIMEZONE_IDS.size() - 1)));
-                    }
-                    if (randomBoolean()) {
-                        query.format("yyyy-MM-dd'T'HH:mm:ss.SSSZZ");
-                    }
-                }
-                break;
-            case 2:
-            default:
-                query = new RangeQueryBuilder(STRING_FIELD_NAME);
-                query.from("a" + randomAsciiOfLengthBetween(1, 10));
-                query.to("z" + randomAsciiOfLengthBetween(1, 10));
-                break;
-        }
-        query.includeLower(randomBoolean()).includeUpper(randomBoolean());
-        if (randomBoolean()) {
-            query.from(null);
-        }
-        if (randomBoolean()) {
-            query.to(null);
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(RangeQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (getCurrentTypes().length == 0 || (queryBuilder.fieldName().equals(DATE_FIELD_NAME) == false && queryBuilder.fieldName().equals(INT_FIELD_NAME) == false)) {
-            assertThat(query, instanceOf(TermRangeQuery.class));
-        } else if (queryBuilder.fieldName().equals(DATE_FIELD_NAME)) {
-            //we can't properly test unmapped dates because LateParsingQuery is package private
-        } else if (queryBuilder.fieldName().equals(INT_FIELD_NAME)) {
-            assertThat(query, instanceOf(NumericRangeQuery.class));
-        } else {
-            throw new UnsupportedOperationException();
-        }
-    }
-
-    @Test
-    public void testValidate() {
-        RangeQueryBuilder rangeQueryBuilder = new RangeQueryBuilder("");
-        assertThat(rangeQueryBuilder.validate().validationErrors().size(), is(1));
-
-        rangeQueryBuilder = new RangeQueryBuilder("okay").timeZone("UTC");
-        assertNull(rangeQueryBuilder.validate());
-
-        rangeQueryBuilder.timeZone("blab");
-        assertThat(rangeQueryBuilder.validate().validationErrors().size(), is(1));
-
-        rangeQueryBuilder.timeZone("UTC").format("basicDate");
-        assertNull(rangeQueryBuilder.validate());
-
-        rangeQueryBuilder.timeZone("UTC").format("broken_xx");
-        assertThat(rangeQueryBuilder.validate().validationErrors().size(), is(1));
-
-        rangeQueryBuilder.timeZone("xXx").format("broken_xx");
-        assertThat(rangeQueryBuilder.validate().validationErrors().size(), is(2));
-    }
-
-    /**
-     * Specifying a timezone together with a numeric range query should throw an exception.
-     */
-    @Test(expected=QueryShardException.class)
-    public void testToQueryNonDateWithTimezone() throws QueryShardException, IOException {
-        RangeQueryBuilder query = new RangeQueryBuilder(INT_FIELD_NAME);
-        query.from(1).to(10).timeZone("UTC");
-        query.toQuery(createShardContext());
-    }
-
-    /**
-     * Specifying a timezone together with an unmapped field should throw an exception.
-     */
-    @Test(expected=QueryShardException.class)
-    public void testToQueryUnmappedWithTimezone() throws QueryShardException, IOException {
-        RangeQueryBuilder query = new RangeQueryBuilder("bogus_field");
-        query.from(1).to(10).timeZone("UTC");
-        query.toQuery(createShardContext());
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/RegexpQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/RegexpQueryBuilderTest.java
deleted file mode 100644
index 9328609..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/RegexpQueryBuilderTest.java
+++ /dev/null
@@ -1,78 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.RegexpQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-
-public class RegexpQueryBuilderTest extends BaseQueryTestCase<RegexpQueryBuilder> {
-
-    @Override
-    protected RegexpQueryBuilder doCreateTestQueryBuilder() {
-        // mapped or unmapped fields
-        String fieldName = randomBoolean() ? STRING_FIELD_NAME : randomAsciiOfLengthBetween(1, 10);
-        String value = randomAsciiOfLengthBetween(1, 10);
-        RegexpQueryBuilder query = new RegexpQueryBuilder(fieldName, value);
-
-        if (randomBoolean()) {
-            List<RegexpFlag> flags = new ArrayList<>();
-            int iter = randomInt(5);
-            for (int i = 0; i < iter; i++) {
-                flags.add(randomFrom(RegexpFlag.values()));
-            }
-            query.flags(flags.toArray(new RegexpFlag[flags.size()]));
-        }
-        if (randomBoolean()) {
-            query.maxDeterminizedStates(randomInt(50000));
-        }
-        if (randomBoolean()) {
-            query.rewrite(randomFrom(getRandomRewriteMethod()));
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(RegexpQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(RegexpQuery.class));
-    }
-
-    @Test
-    public void testValidate() {
-        RegexpQueryBuilder regexQueryBuilder = new RegexpQueryBuilder("", "regex");
-        assertThat(regexQueryBuilder.validate().validationErrors().size(), is(1));
-
-        regexQueryBuilder = new RegexpQueryBuilder("field", null);
-        assertThat(regexQueryBuilder.validate().validationErrors().size(), is(1));
-
-        regexQueryBuilder = new RegexpQueryBuilder("field", "regex");
-        assertNull(regexQueryBuilder.validate());
-
-        regexQueryBuilder = new RegexpQueryBuilder(null, null);
-        assertThat(regexQueryBuilder.validate().validationErrors().size(), is(2));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/ScriptQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/ScriptQueryBuilderTest.java
deleted file mode 100644
index dcc74d1..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/ScriptQueryBuilderTest.java
+++ /dev/null
@@ -1,60 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-
-public class ScriptQueryBuilderTest extends BaseQueryTestCase<ScriptQueryBuilder> {
-
-    @Override
-    protected ScriptQueryBuilder doCreateTestQueryBuilder() {
-        String script;
-        Map<String, Object> params = null;
-        if (randomBoolean()) {
-            script = "5 * 2 > param";
-            params = new HashMap<>();
-            params.put("param", 1);
-        } else {
-            script = "5 * 2 > 2";
-        }
-        return new ScriptQueryBuilder(new Script(script, ScriptType.INLINE, "expression", params));
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(ScriptQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(ScriptQueryBuilder.ScriptQuery.class));
-    }
-
-    @Test
-    public void testValidate() {
-        ScriptQueryBuilder scriptQueryBuilder = new ScriptQueryBuilder(null);
-        assertThat(scriptQueryBuilder.validate().validationErrors().size(), is(1));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java b/core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java
index 260fa85..559d5d1 100644
--- a/core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java
@@ -67,6 +67,7 @@ import org.elasticsearch.action.termvectors.MultiTermVectorsRequest;
 import org.elasticsearch.action.termvectors.MultiTermVectorsResponse;
 import org.elasticsearch.action.termvectors.TermVectorsRequest;
 import org.elasticsearch.action.termvectors.TermVectorsResponse;
+import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.compress.CompressedXContent;
@@ -101,6 +102,7 @@ import java.io.IOException;
 import java.util.Arrays;
 import java.util.EnumSet;
 import java.util.List;
+import java.util.Locale;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.*;
@@ -998,7 +1000,7 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
     @Test
     public void testBoostingQueryBuilder() throws IOException {
         IndexQueryParserService queryParser = queryParser();
-        Query parsedQuery = queryParser.parse(boostingQuery(termQuery("field1", "value1"), termQuery("field1", "value2")).negativeBoost(0.2f)).query();
+        Query parsedQuery = queryParser.parse(boostingQuery().positive(termQuery("field1", "value1")).negative(termQuery("field1", "value2")).negativeBoost(0.2f)).query();
         assertThat(parsedQuery, instanceOf(BoostingQuery.class));
     }
 
@@ -1380,7 +1382,7 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
     @Test
     public void testSpanNotQueryBuilder() throws IOException {
         IndexQueryParserService queryParser = queryParser();
-        Query parsedQuery = queryParser.parse(spanNotQuery(spanTermQuery("age", 34), spanTermQuery("age", 35))).query();
+        Query parsedQuery = queryParser.parse(spanNotQuery().include(spanTermQuery("age", 34)).exclude(spanTermQuery("age", 35))).query();
         assertThat(parsedQuery, instanceOf(SpanNotQuery.class));
         SpanNotQuery spanNotQuery = (SpanNotQuery) parsedQuery;
         // since age is automatically registered in data, we encode it as numeric
@@ -1405,7 +1407,9 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
         IndexQueryParserService queryParser = queryParser();
         Query expectedQuery = new SpanWithinQuery(new SpanTermQuery(new Term("age", longToPrefixCoded(34, 0))),
                                                   new SpanTermQuery(new Term("age", longToPrefixCoded(35, 0))));
-        Query actualQuery = queryParser.parse(spanWithinQuery(spanTermQuery("age", 34), spanTermQuery("age", 35)))
+        Query actualQuery = queryParser.parse(spanWithinQuery()
+                                              .big(spanTermQuery("age", 34))
+                                              .little(spanTermQuery("age", 35)))
                                               .query();
         assertEquals(expectedQuery, actualQuery);
     }
@@ -1425,7 +1429,10 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
         IndexQueryParserService queryParser = queryParser();
         Query expectedQuery = new SpanContainingQuery(new SpanTermQuery(new Term("age", longToPrefixCoded(34, 0))),
                                                       new SpanTermQuery(new Term("age", longToPrefixCoded(35, 0))));
-        Query actualQuery = queryParser.parse(spanContainingQuery(spanTermQuery("age", 34), spanTermQuery("age", 35))).query();
+        Query actualQuery = queryParser.parse(spanContainingQuery()
+                                              .big(spanTermQuery("age", 34))
+                                              .little(spanTermQuery("age", 35)))
+                                              .query();
         assertEquals(expectedQuery, actualQuery);
     }
 
@@ -1465,7 +1472,7 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
     @Test
     public void testSpanNearQueryBuilder() throws IOException {
         IndexQueryParserService queryParser = queryParser();
-        Query parsedQuery = queryParser.parse(spanNearQuery(12).clause(spanTermQuery("age", 34)).clause(spanTermQuery("age", 35)).clause(spanTermQuery("age", 36)).inOrder(false).collectPayloads(false)).query();
+        Query parsedQuery = queryParser.parse(spanNearQuery().clause(spanTermQuery("age", 34)).clause(spanTermQuery("age", 35)).clause(spanTermQuery("age", 36)).slop(12).inOrder(false).collectPayloads(false)).query();
         assertThat(parsedQuery, instanceOf(SpanNearQuery.class));
         SpanNearQuery spanNearQuery = (SpanNearQuery) parsedQuery;
         assertThat(spanNearQuery.getClauses().length, equalTo(3));
@@ -1673,7 +1680,7 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
 
     @Test
     public void testMoreLikeThisIds() throws Exception {
-        MoreLikeThisQueryParser parser = (MoreLikeThisQueryParser) queryParser.indicesQueriesRegistry().queryParsers().get("more_like_this");
+        MoreLikeThisQueryParser parser = (MoreLikeThisQueryParser) queryParser.queryParser("more_like_this");
         parser.setFetchService(new MockMoreLikeThisFetchService());
 
         IndexQueryParserService queryParser = queryParser();
@@ -1699,7 +1706,7 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
     @Test
     public void testMLTMinimumShouldMatch() throws Exception {
         // setup for mocking fetching items
-        MoreLikeThisQueryParser parser = (MoreLikeThisQueryParser) queryParser.indicesQueriesRegistry().queryParsers().get("more_like_this");
+        MoreLikeThisQueryParser parser = (MoreLikeThisQueryParser) queryParser.queryParser("more_like_this");
         parser.setFetchService(new MockMoreLikeThisFetchService());
 
         // parsing the ES query
@@ -2340,6 +2347,14 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
     }
 
     @Test
+    public void testSimpleQueryString() throws Exception {
+        IndexQueryParserService queryParser = queryParser();
+        String query = copyToStringFromClasspath("/org/elasticsearch/index/query/simple-query-string.json");
+        Query parsedQuery = queryParser.parse(query).query();
+        assertThat(parsedQuery, instanceOf(BooleanQuery.class));
+    }
+
+    @Test
     public void testMatchWithFuzzyTranspositions() throws Exception {
         IndexQueryParserService queryParser = queryParser();
         String query = copyToStringFromClasspath("/org/elasticsearch/index/query/match-with-fuzzy-transpositions.json");
@@ -2469,8 +2484,8 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
         assertThat(((ConstantScoreQuery) parsedQuery).getQuery().toString(), equalTo("ToParentBlockJoinQuery (+*:* #QueryWrapperFilter(_type:__nested))"));
         SearchContext.removeCurrent();
     }
-
-    /**
+    
+    /** 
      * helper to extract term from TermQuery. */
     private Term getTerm(Query query) {
         while (query instanceof QueryWrapperFilter) {
@@ -2522,4 +2537,19 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
             assertThat(prefixQuery.getRewriteMethod(), instanceOf(MultiTermQuery.TopTermsBlendedFreqScoringRewrite.class));
         }
     }
+
+    @Test
+    public void testSimpleQueryStringNoFields() throws Exception {
+        IndexQueryParserService queryParser = queryParser();
+        String queryText = randomAsciiOfLengthBetween(1, 10).toLowerCase(Locale.ROOT);
+        String query = "{\n" +
+                "    \"simple_query_string\" : {\n" +
+                "        \"query\" : \"" + queryText + "\"\n" +
+                "    }\n" +
+                "}";
+        Query parsedQuery = queryParser.parse(query).query();
+        assertThat(parsedQuery, instanceOf(TermQuery.class));
+        TermQuery termQuery = (TermQuery) parsedQuery;
+        assertThat(termQuery.getTerm(), equalTo(new Term(MetaData.ALL, queryText)));
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTest.java
deleted file mode 100644
index f497f6c..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTest.java
+++ /dev/null
@@ -1,323 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.*;
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.*;
-
-import static org.hamcrest.Matchers.*;
-
-public class SimpleQueryStringBuilderTest extends BaseQueryTestCase<SimpleQueryStringBuilder> {
-
-    private static final String[] MINIMUM_SHOULD_MATCH = new String[] { "1", "-1", "75%", "-25%", "2<75%", "2<-25%" };
-
-    @Override
-    protected SimpleQueryStringBuilder doCreateTestQueryBuilder() {
-        SimpleQueryStringBuilder result = new SimpleQueryStringBuilder(randomAsciiOfLengthBetween(1, 10));
-        if (randomBoolean()) {
-            result.analyzeWildcard(randomBoolean());
-        }
-        if (randomBoolean()) {
-            result.lenient(randomBoolean());
-        }
-        if (randomBoolean()) {
-            result.lowercaseExpandedTerms(randomBoolean());
-        }
-        if (randomBoolean()) {
-            result.locale(randomLocale(getRandom()));
-        }
-        if (randomBoolean()) {
-            result.minimumShouldMatch(randomFrom(MINIMUM_SHOULD_MATCH));
-        }
-        if (randomBoolean()) {
-            result.analyzer("simple");
-        }
-        if (randomBoolean()) {
-            result.defaultOperator(randomFrom(Operator.AND, Operator.OR));
-        }
-        if (randomBoolean()) {
-            Set<SimpleQueryStringFlag> flagSet = new HashSet<>();
-            int size = randomIntBetween(0, SimpleQueryStringFlag.values().length);
-            for (int i = 0; i < size; i++) {
-                flagSet.add(randomFrom(SimpleQueryStringFlag.values()));
-            }
-            if (flagSet.size() > 0) {
-                result.flags(flagSet.toArray(new SimpleQueryStringFlag[flagSet.size()]));
-            }
-        }
-
-        int fieldCount = randomIntBetween(0, 10);
-        Map<String, Float> fields = new TreeMap<>();
-        for (int i = 0; i < fieldCount; i++) {
-            if (randomBoolean()) {
-                fields.put(randomAsciiOfLengthBetween(1, 10), AbstractQueryBuilder.DEFAULT_BOOST);
-            } else {
-                fields.put(randomAsciiOfLengthBetween(1, 10), 2.0f / randomIntBetween(1, 20));
-            }
-        }
-        result.fields(fields);
-
-        return result;
-    }
-
-    @Test
-    public void testDefaults() {
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder("The quick brown fox.");
-
-        assertEquals("Wrong default default boost.", AbstractQueryBuilder.DEFAULT_BOOST, qb.boost(), 0.001);
-        assertEquals("Wrong default default boost field.", AbstractQueryBuilder.DEFAULT_BOOST, SimpleQueryStringBuilder.DEFAULT_BOOST,
-                0.001);
-
-        assertEquals("Wrong default flags.", SimpleQueryStringFlag.ALL.value, qb.flags());
-        assertEquals("Wrong default flags field.", SimpleQueryStringFlag.ALL.value(), SimpleQueryStringBuilder.DEFAULT_FLAGS);
-
-        assertEquals("Wrong default default operator.", Operator.OR, qb.defaultOperator());
-        assertEquals("Wrong default default operator field.", Operator.OR, SimpleQueryStringBuilder.DEFAULT_OPERATOR);
-
-        assertEquals("Wrong default default locale.", Locale.ROOT, qb.locale());
-        assertEquals("Wrong default default locale field.", Locale.ROOT, SimpleQueryStringBuilder.DEFAULT_LOCALE);
-
-        assertEquals("Wrong default default analyze_wildcard.", false, qb.analyzeWildcard());
-        assertEquals("Wrong default default analyze_wildcard field.", false, SimpleQueryStringBuilder.DEFAULT_ANALYZE_WILDCARD);
-
-        assertEquals("Wrong default default lowercase_expanded_terms.", true, qb.lowercaseExpandedTerms());
-        assertEquals("Wrong default default lowercase_expanded_terms field.", true,
-                SimpleQueryStringBuilder.DEFAULT_LOWERCASE_EXPANDED_TERMS);
-
-        assertEquals("Wrong default default lenient.", false, qb.lenient());
-        assertEquals("Wrong default default lenient field.", false, SimpleQueryStringBuilder.DEFAULT_LENIENT);
-
-        assertEquals("Wrong default default locale.", Locale.ROOT, qb.locale());
-        assertEquals("Wrong default default locale field.", Locale.ROOT, SimpleQueryStringBuilder.DEFAULT_LOCALE);
-    }
-
-    @Test
-    public void testDefaultNullLocale() {
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder("The quick brown fox.");
-        qb.locale(null);
-        assertEquals("Setting locale to null should result in returning to default value.", SimpleQueryStringBuilder.DEFAULT_LOCALE,
-                qb.locale());
-    }
-
-    @Test
-    public void testDefaultNullComplainFlags() {
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder("The quick brown fox.");
-        qb.flags((SimpleQueryStringFlag[]) null);
-        assertEquals("Setting flags to null should result in returning to default value.", SimpleQueryStringBuilder.DEFAULT_FLAGS,
-                qb.flags());
-    }
-
-    @Test
-    public void testDefaultEmptyComplainFlags() {
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder("The quick brown fox.");
-        qb.flags(new SimpleQueryStringFlag[] {});
-        assertEquals("Setting flags to empty should result in returning to default value.", SimpleQueryStringBuilder.DEFAULT_FLAGS,
-                qb.flags());
-    }
-
-    @Test
-    public void testDefaultNullComplainOp() {
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder("The quick brown fox.");
-        qb.defaultOperator(null);
-        assertEquals("Setting operator to null should result in returning to default value.", SimpleQueryStringBuilder.DEFAULT_OPERATOR,
-                qb.defaultOperator());
-    }
-
-    // Check operator handling, and default field handling.
-    @Test
-    public void testDefaultOperatorHandling() throws IOException {
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder("The quick brown fox.").field(STRING_FIELD_NAME);
-        QueryShardContext shardContext = createShardContext();
-        shardContext.setAllowUnmappedFields(true); // to avoid occasional cases
-                                                   // in setup where we didn't
-                                                   // add types but strict field
-                                                   // resolution
-        BooleanQuery boolQuery = (BooleanQuery) qb.toQuery(shardContext);
-        assertThat(shouldClauses(boolQuery), is(4));
-
-        qb.defaultOperator(Operator.AND);
-        boolQuery = (BooleanQuery) qb.toQuery(shardContext);
-        assertThat(shouldClauses(boolQuery), is(0));
-
-        qb.defaultOperator(Operator.OR);
-        boolQuery = (BooleanQuery) qb.toQuery(shardContext);
-        assertThat(shouldClauses(boolQuery), is(4));
-    }
-
-    @Test
-    public void testValidation() {
-        SimpleQueryStringBuilder qb = createTestQueryBuilder();
-        assertNull(qb.validate());
-    }
-
-    @Test
-    public void testNullQueryTextGeneratesException() {
-        SimpleQueryStringBuilder builder = new SimpleQueryStringBuilder(null);
-        QueryValidationException exception = builder.validate();
-        assertThat(exception, notNullValue());
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testFieldCannotBeNull() {
-        SimpleQueryStringBuilder qb = createTestQueryBuilder();
-        qb.field(null);
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testFieldCannotBeNullAndWeighted() {
-        SimpleQueryStringBuilder qb = createTestQueryBuilder();
-        qb.field(null, AbstractQueryBuilder.DEFAULT_BOOST);
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testFieldCannotBeEmpty() {
-        SimpleQueryStringBuilder qb = createTestQueryBuilder();
-        qb.field("");
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testFieldCannotBeEmptyAndWeighted() {
-        SimpleQueryStringBuilder qb = createTestQueryBuilder();
-        qb.field("", AbstractQueryBuilder.DEFAULT_BOOST);
-    }
-
-    /**
-     * The following should fail fast - never silently set the map containing
-     * fields and weights to null but refuse to accept null instead.
-     * */
-    @Test(expected = NullPointerException.class)
-    public void testFieldsCannotBeSetToNull() {
-        SimpleQueryStringBuilder qb = createTestQueryBuilder();
-        qb.fields(null);
-    }
-
-    @Test
-    public void testDefaultFieldParsing() throws IOException {
-        QueryParseContext context = createParseContext();
-        String query = randomAsciiOfLengthBetween(1, 10).toLowerCase(Locale.ROOT);
-        String contentString = "{\n" +
-                "    \"simple_query_string\" : {\n" +
-                "      \"query\" : \"" + query + "\"" +
-                "    }\n" +
-                "}";
-        XContentParser parser = XContentFactory.xContent(contentString).createParser(contentString);
-        context.reset(parser);
-        SimpleQueryStringBuilder queryBuilder = new SimpleQueryStringParser().fromXContent(context);
-        assertThat(queryBuilder.value(), equalTo(query));
-        assertThat(queryBuilder.fields(), notNullValue());
-        assertThat(queryBuilder.fields().size(), equalTo(0));
-        QueryShardContext shardContext = createShardContext();
-
-        // the remaining tests requires either a mapping that we register with types in base test setup
-        // no strict field resolution (version before V_1_4_0_Beta1)
-        if (getCurrentTypes().length > 0 || shardContext.indexQueryParserService().getIndexCreatedVersion().before(Version.V_1_4_0_Beta1)) {
-            Query luceneQuery = queryBuilder.toQuery(shardContext);
-            assertThat(luceneQuery, instanceOf(TermQuery.class));
-            TermQuery termQuery = (TermQuery) luceneQuery;
-            assertThat(termQuery.getTerm(), equalTo(new Term(MetaData.ALL, query)));
-        }
-    }
-
-    /*
-     * This assumes that Lucene query parsing is being checked already, adding
-     * checks only for our parsing extensions.
-     * 
-     * Also this relies on {@link SimpleQueryStringTests} to test most of the
-     * actual functionality of query parsing.
-     */
-    @Override
-    protected void doAssertLuceneQuery(SimpleQueryStringBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, notNullValue());
-
-        if ("".equals(queryBuilder.value())) {
-            assertTrue("Query should have been MatchNoDocsQuery but was " + query.getClass().getName(), query instanceof MatchNoDocsQuery);
-        } else if (queryBuilder.fields().size() > 1) {
-            assertTrue("Query should have been BooleanQuery but was " + query.getClass().getName(), query instanceof BooleanQuery);
-
-            BooleanQuery boolQuery = (BooleanQuery) query;
-            if (queryBuilder.lowercaseExpandedTerms()) {
-                for (BooleanClause clause : boolQuery.clauses()) {
-                    if (clause.getQuery() instanceof TermQuery) {
-                        TermQuery inner = (TermQuery) clause.getQuery();
-                        assertThat(inner.getTerm().bytes().toString(), is(inner.getTerm().bytes().toString().toLowerCase(Locale.ROOT)));
-                    }
-                }
-            }
-
-            assertThat(boolQuery.clauses().size(), equalTo(queryBuilder.fields().size()));
-            Iterator<String> fields = queryBuilder.fields().keySet().iterator();
-            for (BooleanClause booleanClause : boolQuery) {
-                assertThat(booleanClause.getQuery(), instanceOf(TermQuery.class));
-                TermQuery termQuery = (TermQuery) booleanClause.getQuery();
-                assertThat(termQuery.getTerm(), equalTo(new Term(fields.next(), queryBuilder.value().toLowerCase(Locale.ROOT))));
-            }
-
-            if (queryBuilder.minimumShouldMatch() != null) {
-                Collection<String> minMatchAlways = Arrays.asList("1", "-1", "75%", "-25%");
-                Collection<String> minMatchLarger = Arrays.asList("2<75%", "2<-25%");
-
-                if (minMatchAlways.contains(queryBuilder.minimumShouldMatch())) {
-                    assertThat(boolQuery.getMinimumNumberShouldMatch(), greaterThan(0));
-                } else if (minMatchLarger.contains(queryBuilder.minimumShouldMatch())) {
-                    if (shouldClauses(boolQuery) > 2) {
-                        assertThat(boolQuery.getMinimumNumberShouldMatch(), greaterThan(0));
-                    }
-                } else {
-                    assertEquals(0, boolQuery.getMinimumNumberShouldMatch());
-                }
-            }
-        } else if (queryBuilder.fields().size() <= 1) {
-            assertTrue("Query should have been TermQuery but was " + query.getClass().getName(), query instanceof TermQuery);
-
-            TermQuery termQuery = (TermQuery) query;
-            String field;
-            if (queryBuilder.fields().size() == 0) {
-                field = MetaData.ALL;
-            } else {
-                field = queryBuilder.fields().keySet().iterator().next();
-            }
-            assertThat(termQuery.getTerm(), equalTo(new Term(field, queryBuilder.value().toLowerCase(Locale.ROOT))));
-
-            if (queryBuilder.lowercaseExpandedTerms()) {
-                assertThat(termQuery.getTerm().bytes().toString(), is(termQuery.getTerm().bytes().toString().toLowerCase(Locale.ROOT)));
-            }
-        } else {
-            fail("Encountered lucene query type we do not have a validation implementation for in our SimpleQueryStringBuilderTest");
-        }
-    }
-
-    private int shouldClauses(BooleanQuery query) {
-        int result = 0;
-        for (BooleanClause c : query.clauses()) {
-            if (c.getOccur() == BooleanClause.Occur.SHOULD) {
-                result++;
-            }
-        }
-        return result;
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanContainingQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SpanContainingQueryBuilderTest.java
deleted file mode 100644
index 7429023..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanContainingQueryBuilderTest.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanContainingQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SpanContainingQueryBuilderTest extends BaseQueryTestCase<SpanContainingQueryBuilder> {
-
-    @Override
-    protected SpanContainingQueryBuilder doCreateTestQueryBuilder() {
-        SpanTermQueryBuilder[] spanTermQueries = new SpanTermQueryBuilderTest().createSpanTermQueryBuilders(2);
-        return new SpanContainingQueryBuilder(spanTermQueries[0], spanTermQueries[1]);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanContainingQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanContainingQuery.class));
-    }
-
-    @Test
-    public void testValidate() {
-        int totalExpectedErrors = 0;
-        SpanQueryBuilder bigSpanQueryBuilder;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                bigSpanQueryBuilder = new SpanTermQueryBuilder("", "test");
-            } else {
-                bigSpanQueryBuilder = null;
-            }
-            totalExpectedErrors++;
-        } else {
-            bigSpanQueryBuilder = new SpanTermQueryBuilder("name", "value");
-        }
-        SpanQueryBuilder littleSpanQueryBuilder;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                littleSpanQueryBuilder = new SpanTermQueryBuilder("", "test");
-            } else {
-                littleSpanQueryBuilder = null;
-            }
-            totalExpectedErrors++;
-        } else {
-            littleSpanQueryBuilder = new SpanTermQueryBuilder("name", "value");
-        }
-        SpanContainingQueryBuilder queryBuilder = new SpanContainingQueryBuilder(bigSpanQueryBuilder, littleSpanQueryBuilder);
-        assertValidate(queryBuilder, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanFirstQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SpanFirstQueryBuilderTest.java
deleted file mode 100644
index 0011012..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanFirstQueryBuilderTest.java
+++ /dev/null
@@ -1,108 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanFirstQuery;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.elasticsearch.index.query.QueryBuilders.spanTermQuery;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SpanFirstQueryBuilderTest extends BaseQueryTestCase<SpanFirstQueryBuilder> {
-
-    @Override
-    protected SpanFirstQueryBuilder doCreateTestQueryBuilder() {
-        SpanTermQueryBuilder[] spanTermQueries = new SpanTermQueryBuilderTest().createSpanTermQueryBuilders(1);
-        return new SpanFirstQueryBuilder(spanTermQueries[0], randomIntBetween(0, 1000));
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanFirstQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanFirstQuery.class));
-    }
-
-    @Test
-    public void testValidate() {
-        int totalExpectedErrors = 0;
-        SpanQueryBuilder innerSpanQueryBuilder;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                innerSpanQueryBuilder = new SpanTermQueryBuilder("", "test");
-            } else {
-                innerSpanQueryBuilder = null;
-            }
-            totalExpectedErrors++;
-        } else {
-            innerSpanQueryBuilder = new SpanTermQueryBuilder("name", "value");
-        }
-        int end = randomIntBetween(0, 10);
-        if (randomBoolean()) {
-            end = randomIntBetween(-10, -1);
-            totalExpectedErrors++;
-        }
-        SpanFirstQueryBuilder queryBuilder = new SpanFirstQueryBuilder(innerSpanQueryBuilder, end);
-        assertValidate(queryBuilder, totalExpectedErrors);
-    }
-
-    /**
-     * test exception on missing `end` and `match` parameter in parser
-     */
-    @Test
-    public void testParseEnd() throws IOException {
-
-        {
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            builder.startObject(SpanFirstQueryBuilder.NAME);
-            builder.field("match");
-            spanTermQuery("description", "jumped").toXContent(builder, null);
-            builder.endObject();
-            builder.endObject();
-
-            try {
-                parseQuery(builder.string());
-                fail("missing [end] parameter should raise exception");
-            } catch (QueryParsingException e) {
-                assertTrue(e.getMessage().contains("spanFirst must have [end] set"));
-            }
-        }
-
-        {
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            builder.startObject(SpanFirstQueryBuilder.NAME);
-            builder.field("end", 10);
-            builder.endObject();
-            builder.endObject();
-
-            try {
-                parseQuery(builder.string());
-                fail("missing [match] parameter should raise exception");
-            } catch (QueryParsingException e) {
-                assertTrue(e.getMessage().contains("spanFirst must have [match] span query clause"));
-            }
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilderTest.java
deleted file mode 100644
index b112a08..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilderTest.java
+++ /dev/null
@@ -1,87 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanMultiTermQueryWrapper;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SpanMultiTermQueryBuilderTest extends BaseQueryTestCase<SpanMultiTermQueryBuilder> {
-
-    @Override
-    protected SpanMultiTermQueryBuilder doCreateTestQueryBuilder() {
-        MultiTermQueryBuilder multiTermQueryBuilder = RandomQueryBuilder.createMultiTermQuery(random());
-        return new SpanMultiTermQueryBuilder(multiTermQueryBuilder);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanMultiTermQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanMultiTermQueryWrapper.class));
-        SpanMultiTermQueryWrapper spanMultiTermQueryWrapper = (SpanMultiTermQueryWrapper) query;
-        Query multiTermQuery = queryBuilder.innerQuery().toQuery(context);
-        assertThat(multiTermQuery, instanceOf(MultiTermQuery.class));
-        assertThat(spanMultiTermQueryWrapper.getWrappedQuery(), equalTo(new SpanMultiTermQueryWrapper<>((MultiTermQuery)multiTermQuery).getWrappedQuery()));
-    }
-
-    @Test
-    public void testValidate() {
-        int totalExpectedErrors = 0;
-        MultiTermQueryBuilder multiTermQueryBuilder;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                multiTermQueryBuilder = new RangeQueryBuilder("");
-            } else {
-                multiTermQueryBuilder = null;
-            }
-            totalExpectedErrors++;
-        } else {
-            multiTermQueryBuilder = new RangeQueryBuilder("field");
-        }
-        SpanMultiTermQueryBuilder queryBuilder = new SpanMultiTermQueryBuilder(multiTermQueryBuilder);
-        assertValidate(queryBuilder, totalExpectedErrors);
-    }
-
-    /**
-     * test checks that we throw an {@link UnsupportedOperationException} if the query wrapped
-     * by {@link SpanMultiTermQueryBuilder} does not generate a lucene {@link MultiTermQuery}.
-     * This is currently the case for {@link RangeQueryBuilder} when the target field is mapped
-     * to a date.
-     */
-    @Test
-    public void testUnsupportedInnerQueryType() throws IOException {
-        QueryShardContext context = createShardContext();
-        // test makes only sense if we have at least one type registered with date field mapping
-        if (getCurrentTypes().length > 0 && context.fieldMapper(DATE_FIELD_NAME) != null) {
-            try {
-                RangeQueryBuilder query = new RangeQueryBuilder(DATE_FIELD_NAME);
-                new SpanMultiTermQueryBuilder(query).toQuery(createShardContext());
-                fail("Exception expected, range query on date fields should not generate a lucene " + MultiTermQuery.class.getName());
-            } catch (UnsupportedOperationException e) {
-                assert(e.getMessage().contains("unsupported inner query, should be " + MultiTermQuery.class.getName()));
-            }
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanNearQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SpanNearQueryBuilderTest.java
deleted file mode 100644
index d2eb1a0..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanNearQueryBuilderTest.java
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanNearQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.Iterator;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SpanNearQueryBuilderTest extends BaseQueryTestCase<SpanNearQueryBuilder> {
-
-    @Override
-    protected SpanNearQueryBuilder doCreateTestQueryBuilder() {
-        SpanNearQueryBuilder queryBuilder = new SpanNearQueryBuilder(randomIntBetween(-10, 10));
-        SpanTermQueryBuilder[] spanTermQueries = new SpanTermQueryBuilderTest().createSpanTermQueryBuilders(randomIntBetween(1, 6));
-        for (SpanTermQueryBuilder clause : spanTermQueries) {
-            queryBuilder.clause(clause);
-        }
-        queryBuilder.inOrder(randomBoolean());
-        queryBuilder.collectPayloads(randomBoolean());
-        return queryBuilder;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanNearQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanNearQuery.class));
-        SpanNearQuery spanNearQuery = (SpanNearQuery) query;
-        assertThat(spanNearQuery.getSlop(), equalTo(queryBuilder.slop()));
-        assertThat(spanNearQuery.isInOrder(), equalTo(queryBuilder.inOrder()));
-        assertThat(spanNearQuery.getClauses().length, equalTo(queryBuilder.clauses().size()));
-        Iterator<SpanQueryBuilder> spanQueryBuilderIterator = queryBuilder.clauses().iterator();
-        for (SpanQuery spanQuery : spanNearQuery.getClauses()) {
-            assertThat(spanQuery, equalTo(spanQueryBuilderIterator.next().toQuery(context)));
-        }
-    }
-
-    @Test
-    public void testValidate() {
-        SpanNearQueryBuilder queryBuilder = new SpanNearQueryBuilder(1);
-        assertValidate(queryBuilder, 1); // empty clause list
-
-        int totalExpectedErrors = 0;
-        int clauses = randomIntBetween(1, 10);
-        for (int i = 0; i < clauses; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    queryBuilder.clause(new SpanTermQueryBuilder("", "test"));
-                } else {
-                    queryBuilder.clause(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                queryBuilder.clause(new SpanTermQueryBuilder("name", "value"));
-            }
-        }
-        assertValidate(queryBuilder, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanNotQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SpanNotQueryBuilderTest.java
deleted file mode 100644
index 5fb56c5..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanNotQueryBuilderTest.java
+++ /dev/null
@@ -1,205 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanNotQuery;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.elasticsearch.index.query.QueryBuilders.spanNearQuery;
-import static org.elasticsearch.index.query.QueryBuilders.spanTermQuery;
-import static org.hamcrest.Matchers.*;
-
-public class SpanNotQueryBuilderTest extends BaseQueryTestCase<SpanNotQueryBuilder> {
-
-    @Override
-    protected SpanNotQueryBuilder doCreateTestQueryBuilder() {
-        SpanTermQueryBuilder[] spanTermQueries = new SpanTermQueryBuilderTest().createSpanTermQueryBuilders(2);
-        SpanNotQueryBuilder queryBuilder = new SpanNotQueryBuilder(spanTermQueries[0], spanTermQueries[1]);
-        if (randomBoolean()) {
-            // also test negative values, they should implicitly be changed to 0
-            queryBuilder.dist(randomIntBetween(-2, 10));
-        } else {
-            if (randomBoolean()) {
-                queryBuilder.pre(randomIntBetween(-2, 10));
-            }
-            if (randomBoolean()) {
-                queryBuilder.post(randomIntBetween(-2, 10));
-            }
-        }
-        return queryBuilder;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanNotQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanNotQuery.class));
-        SpanNotQuery spanNotQuery = (SpanNotQuery) query;
-        assertThat(spanNotQuery.getExclude(), equalTo(queryBuilder.excludeQuery().toQuery(context)));
-        assertThat(spanNotQuery.getInclude(), equalTo(queryBuilder.includeQuery().toQuery(context)));
-    }
-
-    @Test
-    public void testValidate() {
-        int totalExpectedErrors = 0;
-        SpanQueryBuilder include;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                include = new SpanTermQueryBuilder("", "test");
-            } else {
-                include = null;
-            }
-            totalExpectedErrors++;
-        } else {
-            include = new SpanTermQueryBuilder("name", "value");
-        }
-        SpanQueryBuilder exclude;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                exclude = new SpanTermQueryBuilder("", "test");
-            } else {
-                exclude = null;
-            }
-            totalExpectedErrors++;
-        } else {
-            exclude = new SpanTermQueryBuilder("name", "value");
-        }
-        SpanNotQueryBuilder queryBuilder = new SpanNotQueryBuilder(include, exclude);
-        assertValidate(queryBuilder, totalExpectedErrors);
-    }
-
-    @Test
-    public void testDist() {
-        SpanNotQueryBuilder builder = new SpanNotQueryBuilder(new SpanTermQueryBuilder("name1", "value1"), new SpanTermQueryBuilder("name2", "value2"));
-        assertThat(builder.pre(), equalTo(0));
-        assertThat(builder.post(), equalTo(0));
-        builder.dist(-4);
-        assertThat(builder.pre(), equalTo(0));
-        assertThat(builder.post(), equalTo(0));
-        builder.dist(4);
-        assertThat(builder.pre(), equalTo(4));
-        assertThat(builder.post(), equalTo(4));
-    }
-
-    @Test
-    public void testPrePost() {
-        SpanNotQueryBuilder builder = new SpanNotQueryBuilder(new SpanTermQueryBuilder("name1", "value1"), new SpanTermQueryBuilder("name2", "value2"));
-        assertThat(builder.pre(), equalTo(0));
-        assertThat(builder.post(), equalTo(0));
-        builder.pre(-4).post(-4);
-        assertThat(builder.pre(), equalTo(0));
-        assertThat(builder.post(), equalTo(0));
-        builder.pre(1).post(2);
-        assertThat(builder.pre(), equalTo(1));
-        assertThat(builder.post(), equalTo(2));
-    }
-
-    /**
-     * test correct parsing of `dist` parameter, this should create builder with pre/post set to same value
-     */
-    @Test
-    public void testParseDist() throws IOException {
-        XContentBuilder builder = XContentFactory.jsonBuilder();
-        builder.startObject();
-        builder.startObject(SpanNotQueryBuilder.NAME);
-        builder.field("exclude");
-        spanTermQuery("description", "jumped").toXContent(builder, null);
-        builder.field("include");
-        spanNearQuery(1).clause(QueryBuilders.spanTermQuery("description", "quick"))
-                .clause(QueryBuilders.spanTermQuery("description", "fox")).toXContent(builder, null);
-        builder.field("dist", 3);
-        builder.endObject();
-        builder.endObject();
-        SpanNotQueryBuilder query = (SpanNotQueryBuilder)parseQuery(builder.string());
-        assertThat(query.pre(), equalTo(3));
-        assertThat(query.post(), equalTo(3));
-        assertNotNull(query.includeQuery());
-        assertNotNull(query.excludeQuery());
-    }
-
-    /**
-     * test exceptions for three types of broken json, missing include / exclude and both dist and pre/post specified
-     */
-    @Test
-    public void testParserExceptions() throws IOException {
-
-        {
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            builder.startObject(SpanNotQueryBuilder.NAME);
-            builder.field("exclude");
-            spanTermQuery("description", "jumped").toXContent(builder, null);
-            builder.field("dist", 2);
-            builder.endObject();
-            builder.endObject();
-
-            try {
-                parseQuery(builder.string());
-                fail("QueryParsingException should have been caught");
-            } catch (QueryParsingException e) {
-                assertThat("QueryParsingException should have been caught", e.getDetailedMessage(), containsString("spanNot must have [include]"));
-            }
-        }
-
-        {
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            builder.startObject(SpanNotQueryBuilder.NAME);
-            builder.field("include");
-            spanNearQuery(1).clause(QueryBuilders.spanTermQuery("description", "quick"))
-                    .clause(QueryBuilders.spanTermQuery("description", "fox")).toXContent(builder, null);
-            builder.field("dist", 2);
-            builder.endObject();
-            builder.endObject();
-
-            try {
-                parseQuery(builder.string());
-                fail("QueryParsingException should have been caught");
-            } catch (QueryParsingException e) {
-                assertThat("QueryParsingException should have been caught", e.getDetailedMessage(), containsString("spanNot must have [exclude]"));
-            }
-        }
-
-        {
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            builder.startObject(SpanNotQueryBuilder.NAME);
-            builder.field("include");
-            spanNearQuery(1).clause(QueryBuilders.spanTermQuery("description", "quick"))
-                    .clause(QueryBuilders.spanTermQuery("description", "fox")).toXContent(builder, null);
-            builder.field("exclude");
-            spanTermQuery("description", "jumped").toXContent(builder, null);
-            builder.field("dist", 2);
-            builder.field("pre", 2);
-            builder.endObject();
-            builder.endObject();
-
-            try {
-                parseQuery(builder.string());
-                fail("QueryParsingException should have been caught");
-            } catch (QueryParsingException e) {
-                assertThat("QueryParsingException should have been caught", e.getDetailedMessage(), containsString("spanNot can either use [dist] or [pre] & [post] (or none)"));
-            }
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanOrQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SpanOrQueryBuilderTest.java
deleted file mode 100644
index 051e6fd..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanOrQueryBuilderTest.java
+++ /dev/null
@@ -1,77 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanOrQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.Iterator;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SpanOrQueryBuilderTest extends BaseQueryTestCase<SpanOrQueryBuilder> {
-
-    @Override
-    protected SpanOrQueryBuilder doCreateTestQueryBuilder() {
-        SpanOrQueryBuilder queryBuilder = new SpanOrQueryBuilder();
-        SpanTermQueryBuilder[] spanTermQueries = new SpanTermQueryBuilderTest().createSpanTermQueryBuilders(randomIntBetween(1, 6));
-        for (SpanTermQueryBuilder clause : spanTermQueries) {
-            queryBuilder.clause(clause);
-        }
-        return queryBuilder;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanOrQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanOrQuery.class));
-        SpanOrQuery spanOrQuery = (SpanOrQuery) query;
-        assertThat(spanOrQuery.getClauses().length, equalTo(queryBuilder.clauses().size()));
-        Iterator<SpanQueryBuilder> spanQueryBuilderIterator = queryBuilder.clauses().iterator();
-        for (SpanQuery spanQuery : spanOrQuery.getClauses()) {
-            assertThat(spanQuery, equalTo(spanQueryBuilderIterator.next().toQuery(context)));
-        }
-    }
-
-    @Test
-    public void testValidate() {
-        SpanOrQueryBuilder queryBuilder = new SpanOrQueryBuilder();
-        assertValidate(queryBuilder, 1); // empty clause list
-
-        int totalExpectedErrors = 0;
-        int clauses = randomIntBetween(1, 10);
-        for (int i = 0; i < clauses; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    queryBuilder.clause(new SpanTermQueryBuilder("", "test"));
-                } else {
-                    queryBuilder.clause(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                queryBuilder.clause(new SpanTermQueryBuilder("name", "value"));
-            }
-        }
-        assertValidate(queryBuilder, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanTermQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SpanTermQueryBuilderTest.java
deleted file mode 100644
index 4fc369e..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanTermQueryBuilderTest.java
+++ /dev/null
@@ -1,75 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanTermQuery;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.lucene.BytesRefs;
-import org.elasticsearch.index.mapper.MappedFieldType;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SpanTermQueryBuilderTest extends BaseTermQueryTestCase<SpanTermQueryBuilder> {
-
-    @Override
-    protected SpanTermQueryBuilder createQueryBuilder(String fieldName, Object value) {
-        return new SpanTermQueryBuilder(fieldName, value);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanTermQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanTermQuery.class));
-        SpanTermQuery spanTermQuery = (SpanTermQuery) query;
-        assertThat(spanTermQuery.getTerm().field(), equalTo(queryBuilder.fieldName()));
-        MappedFieldType mapper = context.fieldMapper(queryBuilder.fieldName());
-        if (mapper != null) {
-            BytesRef bytesRef = mapper.indexedValueForSearch(queryBuilder.value());
-            assertThat(spanTermQuery.getTerm().bytes(), equalTo(bytesRef));
-        } else {
-            assertThat(spanTermQuery.getTerm().bytes(), equalTo(BytesRefs.toBytesRef(queryBuilder.value())));
-        }
-    }
-
-    /**
-     * @param amount the number of clauses that will be returned
-     * @return an array of random {@link SpanTermQueryBuilder} with same field name
-     */
-    public SpanTermQueryBuilder[] createSpanTermQueryBuilders(int amount) {
-        SpanTermQueryBuilder[] clauses = new SpanTermQueryBuilder[amount];
-        SpanTermQueryBuilder first = createTestQueryBuilder();
-        clauses[0] = first;
-        for (int i = 1; i < amount; i++) {
-            // we need same field name in all clauses, so we only randomize value
-            SpanTermQueryBuilder spanTermQuery = new SpanTermQueryBuilder(first.fieldName(), getRandomValueForFieldName(first.fieldName()));
-            if (randomBoolean()) {
-                spanTermQuery.boost(2.0f / randomIntBetween(1, 20));
-            }
-            if (randomBoolean()) {
-                spanTermQuery.queryName(randomAsciiOfLengthBetween(1, 10));
-            }
-            clauses[i] = spanTermQuery;
-        }
-        return clauses;
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanWithinQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SpanWithinQueryBuilderTest.java
deleted file mode 100644
index ffc518d..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanWithinQueryBuilderTest.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanWithinQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SpanWithinQueryBuilderTest extends BaseQueryTestCase<SpanWithinQueryBuilder> {
-
-    @Override
-    protected SpanWithinQueryBuilder doCreateTestQueryBuilder() {
-        SpanTermQueryBuilder[] spanTermQueries = new SpanTermQueryBuilderTest().createSpanTermQueryBuilders(2);
-        return new SpanWithinQueryBuilder(spanTermQueries[0], spanTermQueries[1]);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanWithinQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanWithinQuery.class));
-    }
-
-    @Test
-    public void testValidate() {
-        int totalExpectedErrors = 0;
-        SpanQueryBuilder bigSpanQueryBuilder;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                bigSpanQueryBuilder = new SpanTermQueryBuilder("", "test");
-            } else {
-                bigSpanQueryBuilder = null;
-            }
-            totalExpectedErrors++;
-        } else {
-            bigSpanQueryBuilder = new SpanTermQueryBuilder("name", "value");
-        }
-        SpanQueryBuilder littleSpanQueryBuilder;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                littleSpanQueryBuilder = new SpanTermQueryBuilder("", "test");
-            } else {
-                littleSpanQueryBuilder = null;
-            }
-            totalExpectedErrors++;
-        } else {
-            littleSpanQueryBuilder = new SpanTermQueryBuilder("name", "value");
-        }
-        SpanWithinQueryBuilder queryBuilder = new SpanWithinQueryBuilder(bigSpanQueryBuilder, littleSpanQueryBuilder);
-        assertValidate(queryBuilder, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTest.java
deleted file mode 100644
index c744119..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTest.java
+++ /dev/null
@@ -1,51 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.query;
-
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.script.Template;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- * Test building and serialising a template search request.
- * */
-public class TemplateQueryBuilderTest extends ESTestCase {
-
-    @Test
-    public void testJSONGeneration() throws IOException {
-        Map<String, Object> vars = new HashMap<>();
-        vars.put("template", "filled");
-        TemplateQueryBuilder builder = new TemplateQueryBuilder(
-                new Template("I am a $template string", ScriptType.INLINE, null, null, vars));
-        XContentBuilder content = XContentFactory.jsonBuilder();
-        content.startObject();
-        builder.doXContent(content, null);
-        content.endObject();
-        content.close();
-        assertEquals("{\"template\":{\"inline\":\"I am a $template string\",\"params\":{\"template\":\"filled\"}}}", content.string());
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java
new file mode 100644
index 0000000..647ac44
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java
@@ -0,0 +1,51 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.index.query;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.script.ScriptService.ScriptType;
+import org.elasticsearch.script.Template;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+/**
+ * Test building and serialising a template search request.
+ * */
+public class TemplateQueryBuilderTests extends ESTestCase {
+
+    @Test
+    public void testJSONGeneration() throws IOException {
+        Map<String, Object> vars = new HashMap<>();
+        vars.put("template", "filled");
+        TemplateQueryBuilder builder = new TemplateQueryBuilder(
+                new Template("I am a $template string", ScriptType.INLINE, null, null, vars));
+        XContentBuilder content = XContentFactory.jsonBuilder();
+        content.startObject();
+        builder.doXContent(content, null);
+        content.endObject();
+        content.close();
+        assertEquals("{\"template\":{\"inline\":\"I am a $template string\",\"params\":{\"template\":\"filled\"}}}", content.string());
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTest.java b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTest.java
deleted file mode 100644
index 045a21b..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTest.java
+++ /dev/null
@@ -1,169 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Injector;
-import org.elasticsearch.common.inject.ModulesBuilder;
-import org.elasticsearch.common.inject.multibindings.Multibinder;
-import org.elasticsearch.common.inject.util.Providers;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsModule;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.env.EnvironmentModule;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.IndexNameModule;
-import org.elasticsearch.index.analysis.AnalysisModule;
-import org.elasticsearch.index.cache.IndexCacheModule;
-import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
-import org.elasticsearch.index.settings.IndexSettingsModule;
-import org.elasticsearch.index.similarity.SimilarityModule;
-import org.elasticsearch.indices.IndicesModule;
-import org.elasticsearch.indices.analysis.IndicesAnalysisService;
-import org.elasticsearch.indices.breaker.CircuitBreakerService;
-import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
-import org.elasticsearch.script.ScriptModule;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.threadpool.ThreadPoolModule;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.io.IOException;
-
-/**
- * Test parsing and executing a template request.
- */
-// NOTE: this can't be migrated to ESSingleNodeTestCase because of the custom path.conf
-public class TemplateQueryParserTest extends ESTestCase {
-
-    private Injector injector;
-    private QueryShardContext context;
-
-    @Before
-    public void setup() throws IOException {
-        Settings settings = Settings.settingsBuilder()
-                .put("path.home", createTempDir().toString())
-                .put("path.conf", this.getDataPath("config"))
-                .put("name", getClass().getName())
-                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
-                .build();
-
-        Index index = new Index("test");
-        injector = new ModulesBuilder().add(
-                new EnvironmentModule(new Environment(settings)),
-                new SettingsModule(settings),
-                new ThreadPoolModule(new ThreadPool(settings)),
-                new IndicesModule(settings) {
-                    @Override
-                    public void configure() {
-                        // skip services
-                        bindQueryParsersExtension();
-                    }
-                },
-                new ScriptModule(settings),
-                new IndexSettingsModule(index, settings),
-                new IndexCacheModule(settings),
-                new AnalysisModule(settings, new IndicesAnalysisService(settings)),
-                new SimilarityModule(settings),
-                new IndexNameModule(index),
-                new AbstractModule() {
-                    @Override
-                    protected void configure() {
-                        Multibinder.newSetBinder(binder(), ScoreFunctionParser.class);
-                        bind(ClusterService.class).toProvider(Providers.of((ClusterService) null));
-                        bind(CircuitBreakerService.class).to(NoneCircuitBreakerService.class);
-                    }
-                }
-        ).createInjector();
-
-        IndexQueryParserService queryParserService = injector.getInstance(IndexQueryParserService.class);
-        context = new QueryShardContext(index, queryParserService);
-    }
-
-    @Override
-    @After
-    public void tearDown() throws Exception {
-        super.tearDown();
-        terminate(injector.getInstance(ThreadPool.class));
-    }
-
-    @Test
-    public void testParser() throws IOException {
-        String templateString = "{" + "\"query\":{\"match_{{template}}\": {}}," + "\"params\":{\"template\":\"all\"}" + "}";
-
-        XContentParser templateSourceParser = XContentFactory.xContent(templateString).createParser(templateString);
-        context.reset(templateSourceParser);
-        templateSourceParser.nextToken();
-
-        TemplateQueryParser parser = injector.getInstance(TemplateQueryParser.class);
-        Query query = parser.parse(context);
-        assertTrue("Parsing template query failed.", query instanceof MatchAllDocsQuery);
-    }
-
-    @Test
-    public void testParseTemplateAsSingleStringWithConditionalClause() throws IOException {
-        String templateString = "{" + "  \"inline\" : \"{ \\\"match_{{#use_it}}{{template}}{{/use_it}}\\\":{} }\"," + "  \"params\":{"
-                + "    \"template\":\"all\"," + "    \"use_it\": true" + "  }" + "}";
-        XContentParser templateSourceParser = XContentFactory.xContent(templateString).createParser(templateString);
-        context.reset(templateSourceParser);
-
-        TemplateQueryParser parser = injector.getInstance(TemplateQueryParser.class);
-        Query query = parser.parse(context);
-        assertTrue("Parsing template query failed.", query instanceof MatchAllDocsQuery);
-    }
-
-    /**
-     * Test that the template query parser can parse and evaluate template
-     * expressed as a single string but still it expects only the query
-     * specification (thus this test should fail with specific exception).
-     */
-    @Test(expected = QueryParsingException.class)
-    public void testParseTemplateFailsToParseCompleteQueryAsSingleString() throws IOException {
-        String templateString = "{" + "  \"inline\" : \"{ \\\"size\\\": \\\"{{size}}\\\", \\\"query\\\":{\\\"match_all\\\":{}}}\","
-                + "  \"params\":{" + "    \"size\":2" + "  }\n" + "}";
-
-        XContentParser templateSourceParser = XContentFactory.xContent(templateString).createParser(templateString);
-        context.reset(templateSourceParser);
-
-        TemplateQueryParser parser = injector.getInstance(TemplateQueryParser.class);
-        parser.parse(context);
-    }
-
-    @Test
-    public void testParserCanExtractTemplateNames() throws Exception {
-        String templateString = "{ \"file\": \"storedTemplate\" ,\"params\":{\"template\":\"all\" } } ";
-
-        XContentParser templateSourceParser = XContentFactory.xContent(templateString).createParser(templateString);
-        context.reset(templateSourceParser);
-        templateSourceParser.nextToken();
-
-        TemplateQueryParser parser = injector.getInstance(TemplateQueryParser.class);
-        Query query = parser.parse(context);
-        assertTrue("Parsing template query failed.", query instanceof MatchAllDocsQuery);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTests.java b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTests.java
new file mode 100644
index 0000000..df65adc
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTests.java
@@ -0,0 +1,169 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.index.query;
+
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.common.inject.AbstractModule;
+import org.elasticsearch.common.inject.Injector;
+import org.elasticsearch.common.inject.ModulesBuilder;
+import org.elasticsearch.common.inject.multibindings.Multibinder;
+import org.elasticsearch.common.inject.util.Providers;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.settings.SettingsModule;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.env.Environment;
+import org.elasticsearch.env.EnvironmentModule;
+import org.elasticsearch.index.Index;
+import org.elasticsearch.index.IndexNameModule;
+import org.elasticsearch.index.analysis.AnalysisModule;
+import org.elasticsearch.index.cache.IndexCacheModule;
+import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
+import org.elasticsearch.index.settings.IndexSettingsModule;
+import org.elasticsearch.index.similarity.SimilarityModule;
+import org.elasticsearch.indices.IndicesModule;
+import org.elasticsearch.indices.analysis.IndicesAnalysisService;
+import org.elasticsearch.indices.breaker.CircuitBreakerService;
+import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
+import org.elasticsearch.script.ScriptModule;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.elasticsearch.threadpool.ThreadPoolModule;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import java.io.IOException;
+
+/**
+ * Test parsing and executing a template request.
+ */
+// NOTE: this can't be migrated to ESSingleNodeTestCase because of the custom path.conf
+public class TemplateQueryParserTests extends ESTestCase {
+
+    private Injector injector;
+    private QueryParseContext context;
+
+    @Before
+    public void setup() throws IOException {
+        Settings settings = Settings.settingsBuilder()
+                .put("path.home", createTempDir().toString())
+                .put("path.conf", this.getDataPath("config"))
+                .put("name", getClass().getName())
+                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
+                .build();
+
+        Index index = new Index("test");
+        injector = new ModulesBuilder().add(
+                new EnvironmentModule(new Environment(settings)),
+                new SettingsModule(settings),
+                new ThreadPoolModule(new ThreadPool(settings)),
+                new IndicesModule(settings) {
+                    @Override
+                    public void configure() {
+                        // skip services
+                        bindQueryParsersExtension();
+                    }
+                },
+                new ScriptModule(settings),
+                new IndexSettingsModule(index, settings),
+                new IndexCacheModule(settings),
+                new AnalysisModule(settings, new IndicesAnalysisService(settings)),
+                new SimilarityModule(settings),
+                new IndexNameModule(index),
+                new AbstractModule() {
+                    @Override
+                    protected void configure() {
+                        Multibinder.newSetBinder(binder(), ScoreFunctionParser.class);
+                        bind(ClusterService.class).toProvider(Providers.of((ClusterService) null));
+                        bind(CircuitBreakerService.class).to(NoneCircuitBreakerService.class);
+                    }
+                }
+        ).createInjector();
+
+        IndexQueryParserService queryParserService = injector.getInstance(IndexQueryParserService.class);
+        context = new QueryParseContext(index, queryParserService);
+    }
+
+    @Override
+    @After
+    public void tearDown() throws Exception {
+        super.tearDown();
+        terminate(injector.getInstance(ThreadPool.class));
+    }
+
+    @Test
+    public void testParser() throws IOException {
+        String templateString = "{" + "\"query\":{\"match_{{template}}\": {}}," + "\"params\":{\"template\":\"all\"}" + "}";
+
+        XContentParser templateSourceParser = XContentFactory.xContent(templateString).createParser(templateString);
+        context.reset(templateSourceParser);
+        templateSourceParser.nextToken();
+
+        TemplateQueryParser parser = injector.getInstance(TemplateQueryParser.class);
+        Query query = parser.parse(context);
+        assertTrue("Parsing template query failed.", query instanceof MatchAllDocsQuery);
+    }
+
+    @Test
+    public void testParseTemplateAsSingleStringWithConditionalClause() throws IOException {
+        String templateString = "{" + "  \"inline\" : \"{ \\\"match_{{#use_it}}{{template}}{{/use_it}}\\\":{} }\"," + "  \"params\":{"
+                + "    \"template\":\"all\"," + "    \"use_it\": true" + "  }" + "}";
+        XContentParser templateSourceParser = XContentFactory.xContent(templateString).createParser(templateString);
+        context.reset(templateSourceParser);
+
+        TemplateQueryParser parser = injector.getInstance(TemplateQueryParser.class);
+        Query query = parser.parse(context);
+        assertTrue("Parsing template query failed.", query instanceof MatchAllDocsQuery);
+    }
+
+    /**
+     * Test that the template query parser can parse and evaluate template
+     * expressed as a single string but still it expects only the query
+     * specification (thus this test should fail with specific exception).
+     */
+    @Test(expected = QueryParsingException.class)
+    public void testParseTemplateFailsToParseCompleteQueryAsSingleString() throws IOException {
+        String templateString = "{" + "  \"inline\" : \"{ \\\"size\\\": \\\"{{size}}\\\", \\\"query\\\":{\\\"match_all\\\":{}}}\","
+                + "  \"params\":{" + "    \"size\":2" + "  }\n" + "}";
+
+        XContentParser templateSourceParser = XContentFactory.xContent(templateString).createParser(templateString);
+        context.reset(templateSourceParser);
+
+        TemplateQueryParser parser = injector.getInstance(TemplateQueryParser.class);
+        parser.parse(context);
+    }
+
+    @Test
+    public void testParserCanExtractTemplateNames() throws Exception {
+        String templateString = "{ \"file\": \"storedTemplate\" ,\"params\":{\"template\":\"all\" } } ";
+
+        XContentParser templateSourceParser = XContentFactory.xContent(templateString).createParser(templateString);
+        context.reset(templateSourceParser);
+        templateSourceParser.nextToken();
+
+        TemplateQueryParser parser = injector.getInstance(TemplateQueryParser.class);
+        Query query = parser.parse(context);
+        assertTrue("Parsing template query failed.", query instanceof MatchAllDocsQuery);
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/index/query/TermQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/TermQueryBuilderTest.java
deleted file mode 100644
index f84d1c0..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/TermQueryBuilderTest.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.lucene.BytesRefs;
-import org.elasticsearch.index.mapper.MappedFieldType;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class TermQueryBuilderTest extends BaseTermQueryTestCase<TermQueryBuilder> {
-
-    /**
-     * @return a TermQuery with random field name and value, optional random boost and queryname
-     */
-    @Override
-    protected TermQueryBuilder createQueryBuilder(String fieldName, Object value) {
-        return new TermQueryBuilder(fieldName, value);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(TermQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(TermQuery.class));
-        TermQuery termQuery = (TermQuery) query;
-        assertThat(termQuery.getTerm().field(), equalTo(queryBuilder.fieldName()));
-        MappedFieldType mapper = context.fieldMapper(queryBuilder.fieldName());
-        if (mapper != null) {
-            BytesRef bytesRef = mapper.indexedValueForSearch(queryBuilder.value());
-            assertThat(termQuery.getTerm().bytes(), equalTo(bytesRef));
-        } else {
-            assertThat(termQuery.getTerm().bytes(), equalTo(BytesRefs.toBytesRef(queryBuilder.value())));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/TermsQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/TermsQueryBuilderTest.java
deleted file mode 100644
index 26991d8..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/TermsQueryBuilderTest.java
+++ /dev/null
@@ -1,228 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.search.termslookup.TermsLookupFetchService;
-import org.elasticsearch.indices.cache.query.terms.TermsLookup;
-import org.hamcrest.Matchers;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-
-import static org.hamcrest.Matchers.*;
-
-public class TermsQueryBuilderTest extends BaseQueryTestCase<TermsQueryBuilder> {
-
-    private MockTermsLookupFetchService termsLookupFetchService;
-
-    @Before
-    public void mockTermsLookupFetchService() {
-        termsLookupFetchService = new MockTermsLookupFetchService();
-        queryParserService().setTermsLookupFetchService(termsLookupFetchService);
-    }
-
-    @Override
-    protected TermsQueryBuilder doCreateTestQueryBuilder() {
-        TermsQueryBuilder query;
-        // terms query or lookup query
-        if (randomBoolean()) {
-            // make between 0 and 5 different values of the same type
-            String fieldName = getRandomFieldName();
-            Object[] values = new Object[randomInt(5)];
-            for (int i = 0; i < values.length; i++) {
-                values[i] = getRandomValueForFieldName(fieldName);
-            }
-            query = new TermsQueryBuilder(fieldName, values);
-        } else {
-            // right now the mock service returns us a list of strings
-            query = new TermsQueryBuilder(randomBoolean() ? randomAsciiOfLengthBetween(1,10) : STRING_FIELD_NAME);
-            query.termsLookup(randomTermsLookup());
-        }
-        if (randomBoolean()) {
-            query.minimumShouldMatch(randomInt(100) + "%");
-        }
-        if (randomBoolean()) {
-            query.disableCoord(randomBoolean());
-        }
-        return query;
-    }
-
-    private TermsLookup randomTermsLookup() {
-        return new TermsLookup(
-                randomBoolean() ? randomAsciiOfLength(10) : null,
-                randomAsciiOfLength(10),
-                randomAsciiOfLength(10),
-                randomAsciiOfLength(10)
-        ).routing(randomBoolean() ? randomAsciiOfLength(10) : null);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(TermsQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(BooleanQuery.class));
-        BooleanQuery booleanQuery = (BooleanQuery) query;
-
-        // we only do the check below for string fields (otherwise we'd have to decode the values)
-        if (queryBuilder.fieldName().equals(INT_FIELD_NAME) || queryBuilder.fieldName().equals(DOUBLE_FIELD_NAME)
-                || queryBuilder.fieldName().equals(BOOLEAN_FIELD_NAME) || queryBuilder.fieldName().equals(DATE_FIELD_NAME)) {
-            return;
-        }
-
-        // expected returned terms depending on whether we have a terms query or a terms lookup query
-        List<Object> terms;
-        if (queryBuilder.termsLookup() != null) {
-            terms = termsLookupFetchService.getRandomTerms();
-        } else {
-            terms = queryBuilder.values();
-        }
-
-        // compare whether we have the expected list of terms returned
-        Iterator<Object> iter = terms.iterator();
-        for (BooleanClause booleanClause : booleanQuery) {
-            assertThat(booleanClause.getQuery(), instanceOf(TermQuery.class));
-            Term term = ((TermQuery) booleanClause.getQuery()).getTerm();
-            Object next = iter.next();
-            if (next == null) {
-                continue;
-            }
-            assertThat(term, equalTo(new Term(queryBuilder.fieldName(), next.toString())));
-        }
-    }
-
-    @Test
-    public void testValidate() {
-        TermsQueryBuilder termsQueryBuilder = new TermsQueryBuilder(null, "term");
-        assertThat(termsQueryBuilder.validate().validationErrors().size(), is(1));
-
-        termsQueryBuilder = new TermsQueryBuilder("field", "term").termsLookup(randomTermsLookup());
-        assertThat(termsQueryBuilder.validate().validationErrors().size(), is(1));
-
-        termsQueryBuilder = new TermsQueryBuilder(null, "term").termsLookup(randomTermsLookup());
-        assertThat(termsQueryBuilder.validate().validationErrors().size(), is(2));
-
-        termsQueryBuilder = new TermsQueryBuilder("field", "term");
-        assertNull(termsQueryBuilder.validate());
-    }
-
-    @Test
-    public void testValidateLookupQuery() {
-        TermsQueryBuilder termsQuery = new TermsQueryBuilder("field").termsLookup(new TermsLookup());
-        int totalExpectedErrors = 3;
-        if (randomBoolean()) {
-            termsQuery.lookupId("id");
-            totalExpectedErrors--;
-        }
-        if (randomBoolean()) {
-            termsQuery.lookupType("type");
-            totalExpectedErrors--;
-        }
-        if (randomBoolean()) {
-            termsQuery.lookupPath("path");
-            totalExpectedErrors--;
-        }
-        assertValidate(termsQuery, totalExpectedErrors);
-    }
-
-    @Test
-    public void testNullValues() {
-        try {
-            switch (randomInt(6)) {
-                case 0:
-                    new TermsQueryBuilder("field", (String[]) null);
-                    break;
-                case 1:
-                    new TermsQueryBuilder("field", (int[]) null);
-                    break;
-                case 2:
-                    new TermsQueryBuilder("field", (long[]) null);
-                    break;
-                case 3:
-                    new TermsQueryBuilder("field", (float[]) null);
-                    break;
-                case 4:
-                    new TermsQueryBuilder("field", (double[]) null);
-                    break;
-                case 5:
-                    new TermsQueryBuilder("field", (Object[]) null);
-                    break;
-                default:
-                    new TermsQueryBuilder("field", (Iterable<?>) null);
-                    break;
-            }
-            fail("should have failed with IllegalArgumentException");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), Matchers.containsString("No value specified for terms query"));
-        }
-    }
-
-    @Test
-    public void testBothValuesAndLookupSet() throws IOException {
-        String query = "{\n" +
-                "  \"terms\": {\n" +
-                "    \"field\": [\n" +
-                "      \"blue\",\n" +
-                "      \"pill\"\n" +
-                "    ],\n" +
-                "    \"field_lookup\": {\n" +
-                "      \"index\": \"pills\",\n" +
-                "      \"type\": \"red\",\n" +
-                "      \"id\": \"3\",\n" +
-                "      \"path\": \"white rabbit\"\n" +
-                "    }\n" +
-                "  }\n" +
-                "}";
-        QueryBuilder termsQueryBuilder = parseQuery(query);
-        assertThat(termsQueryBuilder.validate().validationErrors().size(), is(1));
-    }
-
-    private static class MockTermsLookupFetchService extends TermsLookupFetchService {
-
-        private List<Object> randomTerms = new ArrayList<>();
-
-        MockTermsLookupFetchService() {
-            super(null, Settings.Builder.EMPTY_SETTINGS);
-            String[] strings = generateRandomStringArray(10, 10, false, true);
-            for (String string : strings) {
-                randomTerms.add(string);
-                if (rarely()) {
-                    randomTerms.add(null);
-                }
-            }
-        }
-
-        @Override
-        public List<Object> fetch(TermsLookup termsLookup) {
-            return randomTerms;
-        }
-
-        List<Object> getRandomTerms() {
-            return randomTerms;
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/TypeQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/TypeQueryBuilderTest.java
deleted file mode 100644
index 18c5534..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/TypeQueryBuilderTest.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.*;
-
-public class TypeQueryBuilderTest extends BaseQueryTestCase<TypeQueryBuilder> {
-
-    @Override
-    protected TypeQueryBuilder doCreateTestQueryBuilder() {
-        return new TypeQueryBuilder(getRandomType());
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(TypeQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, either(instanceOf(TermQuery.class)).or(instanceOf(ConstantScoreQuery.class)));
-        if (query instanceof ConstantScoreQuery) {
-            query = ((ConstantScoreQuery) query).getQuery();
-            assertThat(query, instanceOf(TermQuery.class));
-        }
-        TermQuery termQuery = (TermQuery) query;
-        assertThat(termQuery.getTerm().field(), equalTo(TypeFieldMapper.NAME));
-        assertThat(termQuery.getTerm().text(), equalTo(queryBuilder.type()));
-    }
-
-    @Test
-    public void testValidate() {
-        TypeQueryBuilder typeQueryBuilder = new TypeQueryBuilder((String) null);
-        assertThat(typeQueryBuilder.validate().validationErrors().size(), is(1));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/WildcardQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/WildcardQueryBuilderTest.java
deleted file mode 100644
index ba23249..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/WildcardQueryBuilderTest.java
+++ /dev/null
@@ -1,78 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.WildcardQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-
-public class WildcardQueryBuilderTest extends BaseQueryTestCase<WildcardQueryBuilder> {
-
-    @Override
-    protected WildcardQueryBuilder doCreateTestQueryBuilder() {
-        WildcardQueryBuilder query;
-
-        // mapped or unmapped field
-        String text = randomAsciiOfLengthBetween(1, 10);
-        if (randomBoolean()) {
-            query = new WildcardQueryBuilder(STRING_FIELD_NAME, text);
-        } else {
-            query = new WildcardQueryBuilder(randomAsciiOfLengthBetween(1, 10), text);
-        }
-        if (randomBoolean()) {
-            query.rewrite(randomFrom(getRandomRewriteMethod()));
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(WildcardQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(WildcardQuery.class));
-    }
-
-    @Test
-    public void testValidate() {
-        WildcardQueryBuilder wildcardQueryBuilder = new WildcardQueryBuilder("", "text");
-        assertThat(wildcardQueryBuilder.validate().validationErrors().size(), is(1));
-
-        wildcardQueryBuilder = new WildcardQueryBuilder("field", null);
-        assertThat(wildcardQueryBuilder.validate().validationErrors().size(), is(1));
-
-        wildcardQueryBuilder = new WildcardQueryBuilder(null, null);
-        assertThat(wildcardQueryBuilder.validate().validationErrors().size(), is(2));
-
-        wildcardQueryBuilder = new WildcardQueryBuilder("field", "text");
-        assertNull(wildcardQueryBuilder.validate());
-    }
-
-    @Test
-    public void testEmptyValue() throws IOException {
-        QueryShardContext context = createShardContext();
-        context.setAllowUnmappedFields(true);
-
-        WildcardQueryBuilder wildcardQueryBuilder = new WildcardQueryBuilder(getRandomType(), "");
-        assertEquals(wildcardQueryBuilder.toQuery(context).getClass(), WildcardQuery.class);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/WrapperQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/WrapperQueryBuilderTest.java
deleted file mode 100644
index d18204f..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/WrapperQueryBuilderTest.java
+++ /dev/null
@@ -1,77 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.is;
-
-public class WrapperQueryBuilderTest extends BaseQueryTestCase<WrapperQueryBuilder> {
-
-    @Override
-    protected boolean supportsBoostAndQueryName() {
-        return false;
-    }
-
-    @Override
-    protected WrapperQueryBuilder doCreateTestQueryBuilder() {
-        QueryBuilder wrappedQuery = RandomQueryBuilder.createQuery(random());
-        switch (randomInt(2)) {
-            case 0:
-                return new WrapperQueryBuilder(wrappedQuery.toString());
-            case 1:
-                return new WrapperQueryBuilder(wrappedQuery.buildAsBytes().toBytes());
-            case 2:
-                return new WrapperQueryBuilder(wrappedQuery.buildAsBytes());
-            default:
-                throw new UnsupportedOperationException();
-        }
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(WrapperQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        try (XContentParser qSourceParser = XContentFactory.xContent(queryBuilder.source()).createParser(queryBuilder.source())) {
-            final QueryShardContext contextCopy = new QueryShardContext(context.index(), context.indexQueryParserService());
-            contextCopy.reset(qSourceParser);
-            QueryBuilder result = contextCopy.parseContext().parseInnerQueryBuilder();
-            context.combineNamedQueries(contextCopy);
-            Query expected = result.toQuery(context);
-            if (expected != null) {
-                expected.setBoost(AbstractQueryBuilder.DEFAULT_BOOST);
-            }
-            assertThat(query, equalTo(expected));
-        }
-    }
-
-    @Test
-    public void testValidate() {
-        WrapperQueryBuilder wrapperQueryBuilder = new WrapperQueryBuilder((byte[]) null);
-        assertThat(wrapperQueryBuilder.validate().validationErrors().size(), is(1));
-
-        wrapperQueryBuilder = new WrapperQueryBuilder("");
-        assertThat(wrapperQueryBuilder.validate().validationErrors().size(), is(1));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java b/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java
index 2311f1c..dbbc358 100644
--- a/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java
+++ b/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java
@@ -25,7 +25,10 @@ import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Weight;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.*;
+import org.elasticsearch.index.query.QueryBuilder;
+import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.query.QueryParser;
+import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.indices.IndicesModule;
 import org.elasticsearch.plugins.Plugin;
 
@@ -47,41 +50,24 @@ public class DummyQueryParserPlugin extends Plugin {
         module.registerQueryParser(DummyQueryParser.class);
     }
 
-    public static class DummyQueryBuilder extends AbstractQueryBuilder<DummyQueryBuilder> {
-        private static final String NAME = "dummy";
-
+    public static class DummyQueryBuilder extends QueryBuilder {
         @Override
         protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAME).endObject();
-        }
-
-        @Override
-        protected Query doToQuery(QueryShardContext context) throws IOException {
-            return new DummyQuery(context.isFilter());
-        }
-
-        @Override
-        public String getWriteableName() {
-            return NAME;
+            builder.startObject("dummy").endObject();
         }
     }
 
-    public static class DummyQueryParser extends BaseQueryParser {
+    public static class DummyQueryParser implements QueryParser {
         @Override
         public String[] names() {
-            return new String[]{DummyQueryBuilder.NAME};
+            return new String[]{"dummy"};
         }
 
         @Override
-        public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+        public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
             XContentParser.Token token = parseContext.parser().nextToken();
             assert token == XContentParser.Token.END_OBJECT;
-            return new DummyQueryBuilder();
-        }
-
-        @Override
-        public DummyQueryBuilder getBuilderPrototype() {
-            return new DummyQueryBuilder();
+            return new DummyQuery(parseContext.isFilter());
         }
     }
 
diff --git a/core/src/test/java/org/elasticsearch/index/query/simple-query-string.json b/core/src/test/java/org/elasticsearch/index/query/simple-query-string.json
new file mode 100644
index 0000000..9208e88
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/query/simple-query-string.json
@@ -0,0 +1,8 @@
+{
+  "simple_query_string": {
+    "query": "foo bar",
+    "analyzer": "keyword",
+    "fields": ["body^5","_all"],
+    "default_operator": "and"
+  }
+}
diff --git a/core/src/test/java/org/elasticsearch/index/search/child/AbstractChildTestCase.java b/core/src/test/java/org/elasticsearch/index/search/child/AbstractChildTestCase.java
index f0ed5c7..c7dd274 100644
--- a/core/src/test/java/org/elasticsearch/index/search/child/AbstractChildTestCase.java
+++ b/core/src/test/java/org/elasticsearch/index/search/child/AbstractChildTestCase.java
@@ -19,19 +19,14 @@
 
 package org.elasticsearch.index.search.child;
 
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.Filter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.search.*;
 import org.apache.lucene.search.join.BitDocIdSetFilter;
 import org.apache.lucene.util.BitDocIdSet;
 import org.apache.lucene.util.BitSet;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.indices.mapping.put.PutMappingRequest;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.compress.CompressedXContent;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentParser;
@@ -40,7 +35,7 @@ import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.internal.UidFieldMapper;
 import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.test.ESSingleNodeTestCase;
 import org.hamcrest.Description;
@@ -71,7 +66,7 @@ public abstract class AbstractChildTestCase extends ESSingleNodeTestCase {
         mapperService.merge(childType, new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef(childType, "_parent", "type=" + parentType, CHILD_SCORE_NAME, "type=double,doc_values=false").string()), true, false);
         return createSearchContext(indexService);
     }
-
+    
     static void assertBitSet(BitSet actual, BitSet expected, IndexSearcher searcher) throws IOException {
         assertBitSet(new BitDocIdSet(actual), new BitDocIdSet(expected), searcher);
     }
@@ -88,7 +83,7 @@ public abstract class AbstractChildTestCase extends ESSingleNodeTestCase {
             throw new java.lang.AssertionError(description.toString());
         }
     }
-
+    
     static boolean equals(BitDocIdSet expected, BitDocIdSet actual) {
         if (actual == null && expected == null) {
             return true;
@@ -140,10 +135,10 @@ public abstract class AbstractChildTestCase extends ESSingleNodeTestCase {
     }
 
     static Query parseQuery(QueryBuilder queryBuilder) throws IOException {
-        QueryShardContext context = new QueryShardContext(new Index("test"), SearchContext.current().queryParserService());
+        QueryParseContext context = new QueryParseContext(new Index("test"), SearchContext.current().queryParserService());
         XContentParser parser = XContentHelper.createParser(queryBuilder.buildAsBytes());
         context.reset(parser);
-        return context.parseContext().parseInnerQueryBuilder().toQuery(context);
+        return context.parseInnerQuery();
     }
 
 }
diff --git a/core/src/test/java/org/elasticsearch/index/search/nested/AbstractNumberNestedSortingTestCase.java b/core/src/test/java/org/elasticsearch/index/search/nested/AbstractNumberNestedSortingTestCase.java
new file mode 100644
index 0000000..1581693
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/search/nested/AbstractNumberNestedSortingTestCase.java
@@ -0,0 +1,355 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.index.search.nested;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexableField;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.FieldDoc;
+import org.apache.lucene.search.Filter;
+import org.apache.lucene.search.FilteredQuery;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.QueryWrapperFilter;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.SortField;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.search.TopFieldDocs;
+import org.apache.lucene.search.join.BitDocIdSetCachingWrapperFilter;
+import org.apache.lucene.search.join.ScoreMode;
+import org.apache.lucene.search.join.ToParentBlockJoinQuery;
+import org.elasticsearch.common.lucene.search.Queries;
+import org.elasticsearch.index.fielddata.AbstractFieldDataTestCase;
+import org.elasticsearch.index.fielddata.IndexFieldData;
+import org.elasticsearch.index.fielddata.IndexFieldData.XFieldComparatorSource;
+import org.elasticsearch.index.fielddata.IndexFieldData.XFieldComparatorSource.Nested;
+import org.elasticsearch.search.MultiValueMode;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import static org.hamcrest.Matchers.equalTo;
+
+/**
+ */
+public abstract class AbstractNumberNestedSortingTestCase extends AbstractFieldDataTestCase {
+
+    @Test
+    public void testNestedSorting() throws Exception {
+        List<Document> docs = new ArrayList<>();
+        Document document = new Document();
+        document.add(createField("field2", 3, Field.Store.NO));
+        document.add(new StringField("filter_1", "T", Field.Store.NO));
+        docs.add(document);
+        document = new Document();
+        document.add(createField("field2", 3, Field.Store.NO));
+        document.add(new StringField("filter_1", "T", Field.Store.NO));
+        docs.add(document);
+        document = new Document();
+        document.add(createField("field2", 3, Field.Store.NO));
+        document.add(new StringField("filter_1", "T", Field.Store.NO));
+        docs.add(document);
+        document = new Document();
+        document.add(new StringField("__type", "parent", Field.Store.NO));
+        document.add(createField("field1", 1, Field.Store.NO));
+        docs.add(document);
+        writer.addDocuments(docs);
+        writer.commit();
+
+        docs.clear();
+        document = new Document();
+        document.add(createField("field2", 3, Field.Store.NO));
+        document.add(new StringField("filter_1", "T", Field.Store.NO));
+        docs.add(document);
+        document = new Document();
+        document.add(createField("field2", 3, Field.Store.NO));
+        document.add(new StringField("filter_1", "T", Field.Store.NO));
+        docs.add(document);
+        document = new Document();
+        document.add(createField("field2", 2, Field.Store.NO));
+        document.add(new StringField("filter_1", "T", Field.Store.NO));
+        docs.add(document);
+        document = new Document();
+        document.add(new StringField("__type", "parent", Field.Store.NO));
+        document.add(createField("field1", 2, Field.Store.NO));
+        docs.add(document);
+        writer.addDocuments(docs);
+
+        docs.clear();
+        document = new Document();
+        document.add(createField("field2", 3, Field.Store.NO));
+        document.add(new StringField("filter_1", "T", Field.Store.NO));
+        docs.add(document);
+        document = new Document();
+        document.add(createField("field2", 3, Field.Store.NO));
+        document.add(new StringField("filter_1", "T", Field.Store.NO));
+        docs.add(document);
+        document = new Document();
+        document.add(createField("field2", 1, Field.Store.NO));
+        document.add(new StringField("filter_1", "T", Field.Store.NO));
+        docs.add(document);
+        document = new Document();
+        document.add(new StringField("__type", "parent", Field.Store.NO));
+        document.add(createField("field1", 3, Field.Store.NO));
+        docs.add(document);
+        writer.addDocuments(docs);
+
+        docs.clear();
+        document = new Document();
+        document.add(createField("field2", 3, Field.Store.NO));
+        document.add(new StringField("filter_1", "T", Field.Store.NO));
+        docs.add(document);
+        document = new Document();
+        document.add(createField("field2", 3, Field.Store.NO));
+        document.add(new StringField("filter_1", "F", Field.Store.NO));
+        docs.add(document);
+        document = new Document();
+        document.add(createField("field2", 4, Field.Store.NO));
+        document.add(new StringField("filter_1", "F", Field.Store.NO));
+        docs.add(document);
+        document = new Document();
+        document.add(new StringField("__type", "parent", Field.Store.NO));
+        document.add(createField("field1", 4, Field.Store.NO));
+        docs.add(document);
+        writer.addDocuments(docs);
+        writer.commit();
+
+        docs.clear();
+        document = new Document();
+        document.add(createField("field2", 3, Field.Store.NO));
+        document.add(new StringField("filter_1", "F", Field.Store.NO));
+        docs.add(document);
+        document = new Document();
+        document.add(createField("field2", 3, Field.Store.NO));
+        document.add(new StringField("filter_1", "F", Field.Store.NO));
+        docs.add(document);
+        document = new Document();
+        document.add(createField("field2", 5, Field.Store.NO));
+        document.add(new StringField("filter_1", "F", Field.Store.NO));
+        docs.add(document);
+        document = new Document();
+        document.add(new StringField("__type", "parent", Field.Store.NO));
+        document.add(createField("field1", 5, Field.Store.NO));
+        docs.add(document);
+        writer.addDocuments(docs);
+        writer.commit();
+
+        docs.clear();
+        document = new Document();
+        document.add(createField("field2", 3, Field.Store.NO));
+        document.add(new StringField("filter_1", "T", Field.Store.NO));
+        docs.add(document);
+        document = new Document();
+        document.add(createField("field2", 3, Field.Store.NO));
+        document.add(new StringField("filter_1", "T", Field.Store.NO));
+        docs.add(document);
+        document = new Document();
+        document.add(createField("field2", 6, Field.Store.NO));
+        document.add(new StringField("filter_1", "T", Field.Store.NO));
+        docs.add(document);
+        document = new Document();
+        document.add(new StringField("__type", "parent", Field.Store.NO));
+        document.add(createField("field1", 6, Field.Store.NO));
+        docs.add(document);
+        writer.addDocuments(docs);
+        writer.commit();
+
+        // This doc will not be included, because it doesn't have nested docs
+        document = new Document();
+        document.add(new StringField("__type", "parent", Field.Store.NO));
+        document.add(createField("field1", 7, Field.Store.NO));
+        writer.addDocument(document);
+        writer.commit();
+
+        docs.clear();
+        document = new Document();
+        document.add(createField("field2", 3, Field.Store.NO));
+        document.add(new StringField("filter_1", "T", Field.Store.NO));
+        docs.add(document);
+        document = new Document();
+        document.add(createField("field2", 3, Field.Store.NO));
+        document.add(new StringField("filter_1", "F", Field.Store.NO));
+        docs.add(document);
+        document = new Document();
+        document.add(createField("field2", 7, Field.Store.NO));
+        document.add(new StringField("filter_1", "F", Field.Store.NO));
+        docs.add(document);
+        document = new Document();
+        document.add(new StringField("__type", "parent", Field.Store.NO));
+        document.add(createField("field1", 8, Field.Store.NO));
+        docs.add(document);
+        writer.addDocuments(docs);
+        writer.commit();
+
+        // Some garbage docs, just to check if the NestedFieldComparator can deal with this.
+        document = new Document();
+        document.add(new StringField("fieldXXX", "x", Field.Store.NO));
+        writer.addDocument(document);
+        document = new Document();
+        document.add(new StringField("fieldXXX", "x", Field.Store.NO));
+        writer.addDocument(document);
+        document = new Document();
+        document.add(new StringField("fieldXXX", "x", Field.Store.NO));
+        writer.addDocument(document);
+
+        MultiValueMode sortMode = MultiValueMode.SUM;
+        IndexSearcher searcher = new IndexSearcher(DirectoryReader.open(writer, false));
+        Filter parentFilter = new QueryWrapperFilter(new TermQuery(new Term("__type", "parent")));
+        Filter childFilter = new QueryWrapperFilter(Queries.not(parentFilter));
+        XFieldComparatorSource nestedComparatorSource = createFieldComparator("field2", sortMode, null, createNested(parentFilter, childFilter));
+        ToParentBlockJoinQuery query = new ToParentBlockJoinQuery(new FilteredQuery(new MatchAllDocsQuery(), childFilter), new BitDocIdSetCachingWrapperFilter(parentFilter), ScoreMode.None);
+
+        Sort sort = new Sort(new SortField("field2", nestedComparatorSource));
+        TopFieldDocs topDocs = searcher.search(query, 5, sort);
+        assertThat(topDocs.totalHits, equalTo(7));
+        assertThat(topDocs.scoreDocs.length, equalTo(5));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(11));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).intValue(), equalTo(7));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(7));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).intValue(), equalTo(8));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(3));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).intValue(), equalTo(9));
+        assertThat(topDocs.scoreDocs[3].doc, equalTo(15));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).intValue(), equalTo(10));
+        assertThat(topDocs.scoreDocs[4].doc, equalTo(19));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).intValue(), equalTo(11));
+
+        sort = new Sort(new SortField("field2", nestedComparatorSource, true));
+        topDocs = searcher.search(query, 5, sort);
+        assertThat(topDocs.totalHits, equalTo(7));
+        assertThat(topDocs.scoreDocs.length, equalTo(5));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(28));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).intValue(), equalTo(13));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(23));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).intValue(), equalTo(12));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(19));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).intValue(), equalTo(11));
+        assertThat(topDocs.scoreDocs[3].doc, equalTo(15));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).intValue(), equalTo(10));
+        assertThat(topDocs.scoreDocs[4].doc, equalTo(3));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).intValue(), equalTo(9));
+
+        childFilter = new QueryWrapperFilter(new TermQuery(new Term("filter_1", "T")));
+        nestedComparatorSource = createFieldComparator("field2", sortMode, null, createNested(parentFilter, childFilter));
+        query = new ToParentBlockJoinQuery(
+                new FilteredQuery(new MatchAllDocsQuery(), childFilter),
+                new BitDocIdSetCachingWrapperFilter(parentFilter),
+                ScoreMode.None
+        );
+        sort = new Sort(new SortField("field2", nestedComparatorSource, true));
+        topDocs = searcher.search(query, 5, sort);
+        assertThat(topDocs.totalHits, equalTo(6));
+        assertThat(topDocs.scoreDocs.length, equalTo(5));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(23));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).intValue(), equalTo(12));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(3));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).intValue(), equalTo(9));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(7));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).intValue(), equalTo(8));
+        assertThat(topDocs.scoreDocs[3].doc, equalTo(11));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).intValue(), equalTo(7));
+        assertThat(topDocs.scoreDocs[4].doc, equalTo(15));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).intValue(), equalTo(3));
+
+        sort = new Sort(new SortField("field2", nestedComparatorSource));
+        topDocs = searcher.search(query, 5, sort);
+        assertThat(topDocs.totalHits, equalTo(6));
+        assertThat(topDocs.scoreDocs.length, equalTo(5));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(15));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).intValue(), equalTo(3));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(28));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).intValue(), equalTo(3));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(11));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).intValue(), equalTo(7));
+        assertThat(topDocs.scoreDocs[3].doc, equalTo(7));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).intValue(), equalTo(8));
+        assertThat(topDocs.scoreDocs[4].doc, equalTo(3));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).intValue(), equalTo(9));
+
+        nestedComparatorSource = createFieldComparator("field2", sortMode, 127, createNested(parentFilter, childFilter));
+        sort = new Sort(new SortField("field2", nestedComparatorSource, true));
+        topDocs = searcher.search(new TermQuery(new Term("__type", "parent")), 5, sort);
+        assertThat(topDocs.totalHits, equalTo(8));
+        assertThat(topDocs.scoreDocs.length, equalTo(5));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(19));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).intValue(), equalTo(127));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(24));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).intValue(), equalTo(127));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(23));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).intValue(), equalTo(12));
+        assertThat(topDocs.scoreDocs[3].doc, equalTo(3));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).intValue(), equalTo(9));
+        assertThat(topDocs.scoreDocs[4].doc, equalTo(7));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).intValue(), equalTo(8));
+
+        nestedComparatorSource = createFieldComparator("field2", sortMode, -127, createNested(parentFilter, childFilter));
+        sort = new Sort(new SortField("field2", nestedComparatorSource));
+        topDocs = searcher.search(new TermQuery(new Term("__type", "parent")), 5, sort);
+        assertThat(topDocs.totalHits, equalTo(8));
+        assertThat(topDocs.scoreDocs.length, equalTo(5));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(19));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).intValue(), equalTo(-127));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(24));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).intValue(), equalTo(-127));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(15));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).intValue(), equalTo(3));
+        assertThat(topDocs.scoreDocs[3].doc, equalTo(28));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).intValue(), equalTo(3));
+        assertThat(topDocs.scoreDocs[4].doc, equalTo(11));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).intValue(), equalTo(7));
+
+        // Moved to method, because floating point based XFieldComparatorSource have different outcome for SortMode avg,
+        // than integral number based implementations...
+        assertAvgScoreMode(parentFilter, searcher);
+        searcher.getIndexReader().close();
+    }
+
+    protected void assertAvgScoreMode(Filter parentFilter, IndexSearcher searcher) throws IOException {
+        MultiValueMode sortMode = MultiValueMode.AVG;
+        Filter childFilter = new QueryWrapperFilter(Queries.not(parentFilter));
+        XFieldComparatorSource nestedComparatorSource = createFieldComparator("field2", sortMode, -127, createNested(parentFilter, childFilter));
+        Query query = new ToParentBlockJoinQuery(new FilteredQuery(new MatchAllDocsQuery(), childFilter), new BitDocIdSetCachingWrapperFilter(parentFilter), ScoreMode.None);
+        Sort sort = new Sort(new SortField("field2", nestedComparatorSource));
+        TopDocs topDocs = searcher.search(query, 5, sort);
+        assertThat(topDocs.totalHits, equalTo(7));
+        assertThat(topDocs.scoreDocs.length, equalTo(5));
+        assertThat(topDocs.scoreDocs[0].doc, equalTo(11));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).intValue(), equalTo(2));
+        assertThat(topDocs.scoreDocs[1].doc, equalTo(3));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).intValue(), equalTo(3));
+        assertThat(topDocs.scoreDocs[2].doc, equalTo(7));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).intValue(), equalTo(3));
+        assertThat(topDocs.scoreDocs[3].doc, equalTo(15));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).intValue(), equalTo(3));
+        assertThat(topDocs.scoreDocs[4].doc, equalTo(19));
+        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).intValue(), equalTo(4));
+    }
+
+    protected abstract IndexableField createField(String name, int value, Field.Store store);
+
+    protected abstract IndexFieldData.XFieldComparatorSource createFieldComparator(String fieldName, MultiValueMode sortMode, Object missingValue, Nested nested);
+
+}
diff --git a/core/src/test/java/org/elasticsearch/index/search/nested/AbstractNumberNestedSortingTests.java b/core/src/test/java/org/elasticsearch/index/search/nested/AbstractNumberNestedSortingTests.java
deleted file mode 100644
index 940e10e..0000000
--- a/core/src/test/java/org/elasticsearch/index/search/nested/AbstractNumberNestedSortingTests.java
+++ /dev/null
@@ -1,355 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.search.nested;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexableField;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.FieldDoc;
-import org.apache.lucene.search.Filter;
-import org.apache.lucene.search.FilteredQuery;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.QueryWrapperFilter;
-import org.apache.lucene.search.Sort;
-import org.apache.lucene.search.SortField;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.search.TopFieldDocs;
-import org.apache.lucene.search.join.BitDocIdSetCachingWrapperFilter;
-import org.apache.lucene.search.join.ScoreMode;
-import org.apache.lucene.search.join.ToParentBlockJoinQuery;
-import org.elasticsearch.common.lucene.search.Queries;
-import org.elasticsearch.index.fielddata.AbstractFieldDataTests;
-import org.elasticsearch.index.fielddata.IndexFieldData;
-import org.elasticsearch.index.fielddata.IndexFieldData.XFieldComparatorSource;
-import org.elasticsearch.index.fielddata.IndexFieldData.XFieldComparatorSource.Nested;
-import org.elasticsearch.search.MultiValueMode;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import static org.hamcrest.Matchers.equalTo;
-
-/**
- */
-public abstract class AbstractNumberNestedSortingTests extends AbstractFieldDataTests {
-
-    @Test
-    public void testNestedSorting() throws Exception {
-        List<Document> docs = new ArrayList<>();
-        Document document = new Document();
-        document.add(createField("field2", 3, Field.Store.NO));
-        document.add(new StringField("filter_1", "T", Field.Store.NO));
-        docs.add(document);
-        document = new Document();
-        document.add(createField("field2", 3, Field.Store.NO));
-        document.add(new StringField("filter_1", "T", Field.Store.NO));
-        docs.add(document);
-        document = new Document();
-        document.add(createField("field2", 3, Field.Store.NO));
-        document.add(new StringField("filter_1", "T", Field.Store.NO));
-        docs.add(document);
-        document = new Document();
-        document.add(new StringField("__type", "parent", Field.Store.NO));
-        document.add(createField("field1", 1, Field.Store.NO));
-        docs.add(document);
-        writer.addDocuments(docs);
-        writer.commit();
-
-        docs.clear();
-        document = new Document();
-        document.add(createField("field2", 3, Field.Store.NO));
-        document.add(new StringField("filter_1", "T", Field.Store.NO));
-        docs.add(document);
-        document = new Document();
-        document.add(createField("field2", 3, Field.Store.NO));
-        document.add(new StringField("filter_1", "T", Field.Store.NO));
-        docs.add(document);
-        document = new Document();
-        document.add(createField("field2", 2, Field.Store.NO));
-        document.add(new StringField("filter_1", "T", Field.Store.NO));
-        docs.add(document);
-        document = new Document();
-        document.add(new StringField("__type", "parent", Field.Store.NO));
-        document.add(createField("field1", 2, Field.Store.NO));
-        docs.add(document);
-        writer.addDocuments(docs);
-
-        docs.clear();
-        document = new Document();
-        document.add(createField("field2", 3, Field.Store.NO));
-        document.add(new StringField("filter_1", "T", Field.Store.NO));
-        docs.add(document);
-        document = new Document();
-        document.add(createField("field2", 3, Field.Store.NO));
-        document.add(new StringField("filter_1", "T", Field.Store.NO));
-        docs.add(document);
-        document = new Document();
-        document.add(createField("field2", 1, Field.Store.NO));
-        document.add(new StringField("filter_1", "T", Field.Store.NO));
-        docs.add(document);
-        document = new Document();
-        document.add(new StringField("__type", "parent", Field.Store.NO));
-        document.add(createField("field1", 3, Field.Store.NO));
-        docs.add(document);
-        writer.addDocuments(docs);
-
-        docs.clear();
-        document = new Document();
-        document.add(createField("field2", 3, Field.Store.NO));
-        document.add(new StringField("filter_1", "T", Field.Store.NO));
-        docs.add(document);
-        document = new Document();
-        document.add(createField("field2", 3, Field.Store.NO));
-        document.add(new StringField("filter_1", "F", Field.Store.NO));
-        docs.add(document);
-        document = new Document();
-        document.add(createField("field2", 4, Field.Store.NO));
-        document.add(new StringField("filter_1", "F", Field.Store.NO));
-        docs.add(document);
-        document = new Document();
-        document.add(new StringField("__type", "parent", Field.Store.NO));
-        document.add(createField("field1", 4, Field.Store.NO));
-        docs.add(document);
-        writer.addDocuments(docs);
-        writer.commit();
-
-        docs.clear();
-        document = new Document();
-        document.add(createField("field2", 3, Field.Store.NO));
-        document.add(new StringField("filter_1", "F", Field.Store.NO));
-        docs.add(document);
-        document = new Document();
-        document.add(createField("field2", 3, Field.Store.NO));
-        document.add(new StringField("filter_1", "F", Field.Store.NO));
-        docs.add(document);
-        document = new Document();
-        document.add(createField("field2", 5, Field.Store.NO));
-        document.add(new StringField("filter_1", "F", Field.Store.NO));
-        docs.add(document);
-        document = new Document();
-        document.add(new StringField("__type", "parent", Field.Store.NO));
-        document.add(createField("field1", 5, Field.Store.NO));
-        docs.add(document);
-        writer.addDocuments(docs);
-        writer.commit();
-
-        docs.clear();
-        document = new Document();
-        document.add(createField("field2", 3, Field.Store.NO));
-        document.add(new StringField("filter_1", "T", Field.Store.NO));
-        docs.add(document);
-        document = new Document();
-        document.add(createField("field2", 3, Field.Store.NO));
-        document.add(new StringField("filter_1", "T", Field.Store.NO));
-        docs.add(document);
-        document = new Document();
-        document.add(createField("field2", 6, Field.Store.NO));
-        document.add(new StringField("filter_1", "T", Field.Store.NO));
-        docs.add(document);
-        document = new Document();
-        document.add(new StringField("__type", "parent", Field.Store.NO));
-        document.add(createField("field1", 6, Field.Store.NO));
-        docs.add(document);
-        writer.addDocuments(docs);
-        writer.commit();
-
-        // This doc will not be included, because it doesn't have nested docs
-        document = new Document();
-        document.add(new StringField("__type", "parent", Field.Store.NO));
-        document.add(createField("field1", 7, Field.Store.NO));
-        writer.addDocument(document);
-        writer.commit();
-
-        docs.clear();
-        document = new Document();
-        document.add(createField("field2", 3, Field.Store.NO));
-        document.add(new StringField("filter_1", "T", Field.Store.NO));
-        docs.add(document);
-        document = new Document();
-        document.add(createField("field2", 3, Field.Store.NO));
-        document.add(new StringField("filter_1", "F", Field.Store.NO));
-        docs.add(document);
-        document = new Document();
-        document.add(createField("field2", 7, Field.Store.NO));
-        document.add(new StringField("filter_1", "F", Field.Store.NO));
-        docs.add(document);
-        document = new Document();
-        document.add(new StringField("__type", "parent", Field.Store.NO));
-        document.add(createField("field1", 8, Field.Store.NO));
-        docs.add(document);
-        writer.addDocuments(docs);
-        writer.commit();
-
-        // Some garbage docs, just to check if the NestedFieldComparator can deal with this.
-        document = new Document();
-        document.add(new StringField("fieldXXX", "x", Field.Store.NO));
-        writer.addDocument(document);
-        document = new Document();
-        document.add(new StringField("fieldXXX", "x", Field.Store.NO));
-        writer.addDocument(document);
-        document = new Document();
-        document.add(new StringField("fieldXXX", "x", Field.Store.NO));
-        writer.addDocument(document);
-
-        MultiValueMode sortMode = MultiValueMode.SUM;
-        IndexSearcher searcher = new IndexSearcher(DirectoryReader.open(writer, false));
-        Filter parentFilter = new QueryWrapperFilter(new TermQuery(new Term("__type", "parent")));
-        Filter childFilter = new QueryWrapperFilter(Queries.not(parentFilter));
-        XFieldComparatorSource nestedComparatorSource = createFieldComparator("field2", sortMode, null, createNested(parentFilter, childFilter));
-        ToParentBlockJoinQuery query = new ToParentBlockJoinQuery(new FilteredQuery(new MatchAllDocsQuery(), childFilter), new BitDocIdSetCachingWrapperFilter(parentFilter), ScoreMode.None);
-
-        Sort sort = new Sort(new SortField("field2", nestedComparatorSource));
-        TopFieldDocs topDocs = searcher.search(query, 5, sort);
-        assertThat(topDocs.totalHits, equalTo(7));
-        assertThat(topDocs.scoreDocs.length, equalTo(5));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(11));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).intValue(), equalTo(7));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(7));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).intValue(), equalTo(8));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(3));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).intValue(), equalTo(9));
-        assertThat(topDocs.scoreDocs[3].doc, equalTo(15));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).intValue(), equalTo(10));
-        assertThat(topDocs.scoreDocs[4].doc, equalTo(19));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).intValue(), equalTo(11));
-
-        sort = new Sort(new SortField("field2", nestedComparatorSource, true));
-        topDocs = searcher.search(query, 5, sort);
-        assertThat(topDocs.totalHits, equalTo(7));
-        assertThat(topDocs.scoreDocs.length, equalTo(5));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(28));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).intValue(), equalTo(13));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(23));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).intValue(), equalTo(12));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(19));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).intValue(), equalTo(11));
-        assertThat(topDocs.scoreDocs[3].doc, equalTo(15));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).intValue(), equalTo(10));
-        assertThat(topDocs.scoreDocs[4].doc, equalTo(3));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).intValue(), equalTo(9));
-
-        childFilter = new QueryWrapperFilter(new TermQuery(new Term("filter_1", "T")));
-        nestedComparatorSource = createFieldComparator("field2", sortMode, null, createNested(parentFilter, childFilter));
-        query = new ToParentBlockJoinQuery(
-                new FilteredQuery(new MatchAllDocsQuery(), childFilter),
-                new BitDocIdSetCachingWrapperFilter(parentFilter),
-                ScoreMode.None
-        );
-        sort = new Sort(new SortField("field2", nestedComparatorSource, true));
-        topDocs = searcher.search(query, 5, sort);
-        assertThat(topDocs.totalHits, equalTo(6));
-        assertThat(topDocs.scoreDocs.length, equalTo(5));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(23));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).intValue(), equalTo(12));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(3));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).intValue(), equalTo(9));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(7));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).intValue(), equalTo(8));
-        assertThat(topDocs.scoreDocs[3].doc, equalTo(11));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).intValue(), equalTo(7));
-        assertThat(topDocs.scoreDocs[4].doc, equalTo(15));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).intValue(), equalTo(3));
-
-        sort = new Sort(new SortField("field2", nestedComparatorSource));
-        topDocs = searcher.search(query, 5, sort);
-        assertThat(topDocs.totalHits, equalTo(6));
-        assertThat(topDocs.scoreDocs.length, equalTo(5));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(15));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).intValue(), equalTo(3));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(28));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).intValue(), equalTo(3));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(11));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).intValue(), equalTo(7));
-        assertThat(topDocs.scoreDocs[3].doc, equalTo(7));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).intValue(), equalTo(8));
-        assertThat(topDocs.scoreDocs[4].doc, equalTo(3));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).intValue(), equalTo(9));
-
-        nestedComparatorSource = createFieldComparator("field2", sortMode, 127, createNested(parentFilter, childFilter));
-        sort = new Sort(new SortField("field2", nestedComparatorSource, true));
-        topDocs = searcher.search(new TermQuery(new Term("__type", "parent")), 5, sort);
-        assertThat(topDocs.totalHits, equalTo(8));
-        assertThat(topDocs.scoreDocs.length, equalTo(5));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(19));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).intValue(), equalTo(127));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(24));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).intValue(), equalTo(127));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(23));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).intValue(), equalTo(12));
-        assertThat(topDocs.scoreDocs[3].doc, equalTo(3));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).intValue(), equalTo(9));
-        assertThat(topDocs.scoreDocs[4].doc, equalTo(7));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).intValue(), equalTo(8));
-
-        nestedComparatorSource = createFieldComparator("field2", sortMode, -127, createNested(parentFilter, childFilter));
-        sort = new Sort(new SortField("field2", nestedComparatorSource));
-        topDocs = searcher.search(new TermQuery(new Term("__type", "parent")), 5, sort);
-        assertThat(topDocs.totalHits, equalTo(8));
-        assertThat(topDocs.scoreDocs.length, equalTo(5));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(19));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).intValue(), equalTo(-127));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(24));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).intValue(), equalTo(-127));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(15));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).intValue(), equalTo(3));
-        assertThat(topDocs.scoreDocs[3].doc, equalTo(28));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).intValue(), equalTo(3));
-        assertThat(topDocs.scoreDocs[4].doc, equalTo(11));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).intValue(), equalTo(7));
-
-        // Moved to method, because floating point based XFieldComparatorSource have different outcome for SortMode avg,
-        // than integral number based implementations...
-        assertAvgScoreMode(parentFilter, searcher);
-        searcher.getIndexReader().close();
-    }
-
-    protected void assertAvgScoreMode(Filter parentFilter, IndexSearcher searcher) throws IOException {
-        MultiValueMode sortMode = MultiValueMode.AVG;
-        Filter childFilter = new QueryWrapperFilter(Queries.not(parentFilter));
-        XFieldComparatorSource nestedComparatorSource = createFieldComparator("field2", sortMode, -127, createNested(parentFilter, childFilter));
-        Query query = new ToParentBlockJoinQuery(new FilteredQuery(new MatchAllDocsQuery(), childFilter), new BitDocIdSetCachingWrapperFilter(parentFilter), ScoreMode.None);
-        Sort sort = new Sort(new SortField("field2", nestedComparatorSource));
-        TopDocs topDocs = searcher.search(query, 5, sort);
-        assertThat(topDocs.totalHits, equalTo(7));
-        assertThat(topDocs.scoreDocs.length, equalTo(5));
-        assertThat(topDocs.scoreDocs[0].doc, equalTo(11));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[0]).fields[0]).intValue(), equalTo(2));
-        assertThat(topDocs.scoreDocs[1].doc, equalTo(3));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[1]).fields[0]).intValue(), equalTo(3));
-        assertThat(topDocs.scoreDocs[2].doc, equalTo(7));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[2]).fields[0]).intValue(), equalTo(3));
-        assertThat(topDocs.scoreDocs[3].doc, equalTo(15));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[3]).fields[0]).intValue(), equalTo(3));
-        assertThat(topDocs.scoreDocs[4].doc, equalTo(19));
-        assertThat(((Number) ((FieldDoc) topDocs.scoreDocs[4]).fields[0]).intValue(), equalTo(4));
-    }
-
-    protected abstract IndexableField createField(String name, int value, Field.Store store);
-
-    protected abstract IndexFieldData.XFieldComparatorSource createFieldComparator(String fieldName, MultiValueMode sortMode, Object missingValue, Nested nested);
-
-}
diff --git a/core/src/test/java/org/elasticsearch/index/search/nested/DoubleNestedSortingTests.java b/core/src/test/java/org/elasticsearch/index/search/nested/DoubleNestedSortingTests.java
index 12776ce..fde2aa5 100644
--- a/core/src/test/java/org/elasticsearch/index/search/nested/DoubleNestedSortingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/search/nested/DoubleNestedSortingTests.java
@@ -49,7 +49,7 @@ import static org.hamcrest.Matchers.equalTo;
 
 /**
  */
-public class DoubleNestedSortingTests extends AbstractNumberNestedSortingTests {
+public class DoubleNestedSortingTests extends AbstractNumberNestedSortingTestCase {
 
     @Override
     protected FieldDataType getFieldDataType() {
diff --git a/core/src/test/java/org/elasticsearch/index/search/nested/LongNestedSortingTests.java b/core/src/test/java/org/elasticsearch/index/search/nested/LongNestedSortingTests.java
index 927aa67..9113222 100644
--- a/core/src/test/java/org/elasticsearch/index/search/nested/LongNestedSortingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/search/nested/LongNestedSortingTests.java
@@ -30,7 +30,7 @@ import org.elasticsearch.search.MultiValueMode;
 
 /**
  */
-public class LongNestedSortingTests extends AbstractNumberNestedSortingTests {
+public class LongNestedSortingTests extends AbstractNumberNestedSortingTestCase {
 
     @Override
     protected FieldDataType getFieldDataType() {
diff --git a/core/src/test/java/org/elasticsearch/index/search/nested/NestedSortingTests.java b/core/src/test/java/org/elasticsearch/index/search/nested/NestedSortingTests.java
index 287170d..7d30eb5 100644
--- a/core/src/test/java/org/elasticsearch/index/search/nested/NestedSortingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/search/nested/NestedSortingTests.java
@@ -46,7 +46,7 @@ import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.TestUtil;
 import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.fielddata.AbstractFieldDataTests;
+import org.elasticsearch.index.fielddata.AbstractFieldDataTestCase;
 import org.elasticsearch.index.fielddata.FieldDataType;
 import org.elasticsearch.index.fielddata.IndexFieldData;
 import org.elasticsearch.index.fielddata.IndexFieldData.XFieldComparatorSource;
@@ -65,7 +65,7 @@ import static org.hamcrest.Matchers.equalTo;
 
 /**
  */
-public class NestedSortingTests extends AbstractFieldDataTests {
+public class NestedSortingTests extends AbstractFieldDataTestCase {
 
     @Override
     protected FieldDataType getFieldDataType() {
diff --git a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
index ea72e49..0bedb81 100644
--- a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
+++ b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
@@ -18,7 +18,10 @@
  */
 package org.elasticsearch.index.shard;
 
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.Term;
 import org.apache.lucene.store.LockObtainFailedException;
 import org.apache.lucene.util.Constants;
 import org.apache.lucene.util.IOUtils;
@@ -38,6 +41,7 @@ import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.ShardRoutingState;
 import org.elasticsearch.cluster.routing.TestShardRouting;
 import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.logging.ESLogger;
@@ -49,6 +53,12 @@ import org.elasticsearch.env.NodeEnvironment;
 import org.elasticsearch.env.ShardLock;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.engine.Engine;
+import org.elasticsearch.index.indexing.IndexingOperationListener;
+import org.elasticsearch.index.indexing.ShardIndexingService;
+import org.elasticsearch.index.mapper.Mapping;
+import org.elasticsearch.index.mapper.ParseContext;
+import org.elasticsearch.index.mapper.ParsedDocument;
+import org.elasticsearch.index.mapper.internal.UidFieldMapper;
 import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.settings.IndexSettingsService;
 import org.elasticsearch.index.store.Store;
@@ -64,18 +74,22 @@ import java.io.IOException;
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.nio.file.StandardCopyOption;
+import java.util.Arrays;
+import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Set;
 import java.util.concurrent.ExecutionException;
+import java.util.concurrent.atomic.AtomicBoolean;
 
 import static org.elasticsearch.cluster.metadata.IndexMetaData.EMPTY_PARAMS;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_VERSION_CREATED;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.ToXContent.EMPTY_PARAMS;
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
 import static org.hamcrest.Matchers.equalTo;
 
 /**
@@ -584,4 +598,93 @@ public class IndexShardTests extends ESSingleNodeTestCase {
         assertTrue(xContent.contains(expectedSubSequence));
     }
 
+    private ParsedDocument testParsedDocument(String uid, String id, String type, String routing, long timestamp, long ttl, ParseContext.Document document, BytesReference source, Mapping mappingUpdate) {
+        Field uidField = new Field("_uid", uid, UidFieldMapper.Defaults.FIELD_TYPE);
+        Field versionField = new NumericDocValuesField("_version", 0);
+        document.add(uidField);
+        document.add(versionField);
+        return new ParsedDocument(uidField, versionField, id, type, routing, timestamp, ttl, Arrays.asList(document), source, mappingUpdate);
+    }
+
+    public void testPreIndex() throws IOException {
+        createIndex("testpreindex");
+        ensureGreen();
+        IndicesService indicesService = getInstanceFromNode(IndicesService.class);
+        IndexService test = indicesService.indexService("testpreindex");
+        IndexShard shard = test.shard(0);
+        ShardIndexingService shardIndexingService = shard.indexingService();
+        final AtomicBoolean preIndexCalled = new AtomicBoolean(false);
+
+        shardIndexingService.addListener(new IndexingOperationListener() {
+            @Override
+            public Engine.Index preIndex(Engine.Index index) {
+                preIndexCalled.set(true);
+                return super.preIndex(index);
+            }
+        });
+
+        ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, new ParseContext.Document(), new BytesArray(new byte[]{1}), null);
+        Engine.Index index = new Engine.Index(new Term("_uid", "1"), doc);
+        shard.index(index);
+        assertTrue(preIndexCalled.get());
+    }
+
+    public void testPostIndex() throws IOException {
+        createIndex("testpostindex");
+        ensureGreen();
+        IndicesService indicesService = getInstanceFromNode(IndicesService.class);
+        IndexService test = indicesService.indexService("testpostindex");
+        IndexShard shard = test.shard(0);
+        ShardIndexingService shardIndexingService = shard.indexingService();
+        final AtomicBoolean postIndexCalled = new AtomicBoolean(false);
+
+        shardIndexingService.addListener(new IndexingOperationListener() {
+            @Override
+            public void postIndex(Engine.Index index) {
+                postIndexCalled.set(true);
+                super.postIndex(index);
+            }
+        });
+
+        ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, new ParseContext.Document(), new BytesArray(new byte[]{1}), null);
+        Engine.Index index = new Engine.Index(new Term("_uid", "1"), doc);
+        shard.index(index);
+        assertTrue(postIndexCalled.get());
+    }
+
+    public void testPostIndexWithException() throws IOException {
+        createIndex("testpostindexwithexception");
+        ensureGreen();
+        IndicesService indicesService = getInstanceFromNode(IndicesService.class);
+        IndexService test = indicesService.indexService("testpostindexwithexception");
+        IndexShard shard = test.shard(0);
+        ShardIndexingService shardIndexingService = shard.indexingService();
+
+        shard.close("Unexpected close", true);
+        shard.state = IndexShardState.STARTED; // It will generate exception
+
+        final AtomicBoolean postIndexWithExceptionCalled = new AtomicBoolean(false);
+
+        shardIndexingService.addListener(new IndexingOperationListener() {
+            @Override
+            public void postIndex(Engine.Index index, Throwable ex) {
+                assertNotNull(ex);
+                postIndexWithExceptionCalled.set(true);
+                super.postIndex(index, ex);
+            }
+        });
+
+        ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, new ParseContext.Document(), new BytesArray(new byte[]{1}), null);
+        Engine.Index index = new Engine.Index(new Term("_uid", "1"), doc);
+
+        try {
+            shard.index(index);
+            fail();
+        }catch (IllegalIndexShardStateException e){
+
+        }
+
+        assertTrue(postIndexWithExceptionCalled.get());
+    }
+
 }
diff --git a/core/src/test/java/org/elasticsearch/index/shard/MergePolicySettingsTest.java b/core/src/test/java/org/elasticsearch/index/shard/MergePolicySettingsTest.java
deleted file mode 100644
index 49ec5c2..0000000
--- a/core/src/test/java/org/elasticsearch/index/shard/MergePolicySettingsTest.java
+++ /dev/null
@@ -1,142 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.shard;
-
-import org.apache.lucene.index.NoMergePolicy;
-import org.apache.lucene.index.TieredMergePolicy;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.ByteSizeUnit;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.settings.IndexSettingsService;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.elasticsearch.common.settings.Settings.Builder.EMPTY_SETTINGS;
-import static org.hamcrest.Matchers.equalTo;
-
-public class MergePolicySettingsTest extends ESTestCase {
-
-    protected final ShardId shardId = new ShardId(new Index("index"), 1);
-
-    @Test
-    public void testCompoundFileSettings() throws IOException {
-
-        assertThat(new MergePolicyConfig(logger, EMPTY_SETTINGS).getMergePolicy().getNoCFSRatio(), equalTo(0.1));
-        assertThat(new MergePolicyConfig(logger, build(true)).getMergePolicy().getNoCFSRatio(), equalTo(1.0));
-        assertThat(new MergePolicyConfig(logger, build(0.5)).getMergePolicy().getNoCFSRatio(), equalTo(0.5));
-        assertThat(new MergePolicyConfig(logger, build(1.0)).getMergePolicy().getNoCFSRatio(), equalTo(1.0));
-        assertThat(new MergePolicyConfig(logger, build("true")).getMergePolicy().getNoCFSRatio(), equalTo(1.0));
-        assertThat(new MergePolicyConfig(logger, build("True")).getMergePolicy().getNoCFSRatio(), equalTo(1.0));
-        assertThat(new MergePolicyConfig(logger, build("False")).getMergePolicy().getNoCFSRatio(), equalTo(0.0));
-        assertThat(new MergePolicyConfig(logger, build("false")).getMergePolicy().getNoCFSRatio(), equalTo(0.0));
-        assertThat(new MergePolicyConfig(logger, build(false)).getMergePolicy().getNoCFSRatio(), equalTo(0.0));
-        assertThat(new MergePolicyConfig(logger, build(0)).getMergePolicy().getNoCFSRatio(), equalTo(0.0));
-        assertThat(new MergePolicyConfig(logger, build(0.0)).getMergePolicy().getNoCFSRatio(), equalTo(0.0));
-    }
-
-    public void testNoMerges() {
-        MergePolicyConfig mp = new MergePolicyConfig(logger, Settings.builder().put(MergePolicyConfig.INDEX_MERGE_ENABLED, false).build());
-        assertTrue(mp.getMergePolicy() instanceof NoMergePolicy);
-    }
-
-    @Test
-    public void testUpdateSettings() throws IOException {
-        {
-            IndexSettingsService service = new IndexSettingsService(new Index("test"), EMPTY_SETTINGS);
-            MergePolicyConfig mp = new MergePolicyConfig(logger, EMPTY_SETTINGS);
-            assertThat(((TieredMergePolicy) mp.getMergePolicy()).getNoCFSRatio(), equalTo(0.1));
-
-            mp.onRefreshSettings(build(1.0));
-            assertThat(((TieredMergePolicy) mp.getMergePolicy()).getNoCFSRatio(), equalTo(1.0));
-
-            mp.onRefreshSettings(build(0.1));
-            assertThat(((TieredMergePolicy) mp.getMergePolicy()).getNoCFSRatio(), equalTo(0.1));
-
-            mp.onRefreshSettings(build(0.0));
-            assertThat(((TieredMergePolicy) mp.getMergePolicy()).getNoCFSRatio(), equalTo(0.0));
-        }
-
-
-    }
-
-
-    public void testTieredMergePolicySettingsUpdate() throws IOException {
-        MergePolicyConfig mp = new MergePolicyConfig(logger, EMPTY_SETTINGS);
-        assertThat(mp.getMergePolicy().getNoCFSRatio(), equalTo(0.1));
-
-        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getForceMergeDeletesPctAllowed(), MergePolicyConfig.DEFAULT_EXPUNGE_DELETES_ALLOWED, 0.0d);
-        mp.onRefreshSettings(Settings.builder().put(MergePolicyConfig.INDEX_MERGE_POLICY_EXPUNGE_DELETES_ALLOWED, MergePolicyConfig.DEFAULT_EXPUNGE_DELETES_ALLOWED + 1.0d).build());
-        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getForceMergeDeletesPctAllowed(), MergePolicyConfig.DEFAULT_EXPUNGE_DELETES_ALLOWED + 1.0d, 0.0d);
-
-        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getFloorSegmentMB(), MergePolicyConfig.DEFAULT_FLOOR_SEGMENT.mbFrac(), 0);
-        mp.onRefreshSettings(Settings.builder().put(MergePolicyConfig.INDEX_MERGE_POLICY_FLOOR_SEGMENT, new ByteSizeValue(MergePolicyConfig.DEFAULT_FLOOR_SEGMENT.mb() + 1, ByteSizeUnit.MB)).build());
-        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getFloorSegmentMB(), new ByteSizeValue(MergePolicyConfig.DEFAULT_FLOOR_SEGMENT.mb() + 1, ByteSizeUnit.MB).mbFrac(), 0.001);
-
-        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getMaxMergeAtOnce(), MergePolicyConfig.DEFAULT_MAX_MERGE_AT_ONCE);
-        mp.onRefreshSettings(Settings.builder().put(MergePolicyConfig.INDEX_MERGE_POLICY_MAX_MERGE_AT_ONCE, MergePolicyConfig.DEFAULT_MAX_MERGE_AT_ONCE - 1).build());
-        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getMaxMergeAtOnce(), MergePolicyConfig.DEFAULT_MAX_MERGE_AT_ONCE-1);
-
-        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getMaxMergeAtOnceExplicit(), MergePolicyConfig.DEFAULT_MAX_MERGE_AT_ONCE_EXPLICIT);
-        mp.onRefreshSettings(Settings.builder().put(MergePolicyConfig.INDEX_MERGE_POLICY_MAX_MERGE_AT_ONCE_EXPLICIT, MergePolicyConfig.DEFAULT_MAX_MERGE_AT_ONCE_EXPLICIT - 1).build());
-        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getMaxMergeAtOnceExplicit(), MergePolicyConfig.DEFAULT_MAX_MERGE_AT_ONCE_EXPLICIT-1);
-
-        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getMaxMergedSegmentMB(), MergePolicyConfig.DEFAULT_MAX_MERGED_SEGMENT.mbFrac(), 0.0001);
-        mp.onRefreshSettings(Settings.builder().put(MergePolicyConfig.INDEX_MERGE_POLICY_MAX_MERGED_SEGMENT, new ByteSizeValue(MergePolicyConfig.DEFAULT_MAX_MERGED_SEGMENT.bytes() + 1)).build());
-        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getMaxMergedSegmentMB(), new ByteSizeValue(MergePolicyConfig.DEFAULT_MAX_MERGED_SEGMENT.bytes() + 1).mbFrac(), 0.0001);
-
-        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getReclaimDeletesWeight(), MergePolicyConfig.DEFAULT_RECLAIM_DELETES_WEIGHT, 0);
-        mp.onRefreshSettings(Settings.builder().put(MergePolicyConfig.INDEX_MERGE_POLICY_RECLAIM_DELETES_WEIGHT, MergePolicyConfig.DEFAULT_RECLAIM_DELETES_WEIGHT + 1).build());
-        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getReclaimDeletesWeight(), MergePolicyConfig.DEFAULT_RECLAIM_DELETES_WEIGHT + 1, 0);
-
-        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getSegmentsPerTier(), MergePolicyConfig.DEFAULT_SEGMENTS_PER_TIER, 0);
-        mp.onRefreshSettings(Settings.builder().put(MergePolicyConfig.INDEX_MERGE_POLICY_SEGMENTS_PER_TIER, MergePolicyConfig.DEFAULT_SEGMENTS_PER_TIER + 1).build());
-        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getSegmentsPerTier(), MergePolicyConfig.DEFAULT_SEGMENTS_PER_TIER + 1, 0);
-
-        mp.onRefreshSettings(EMPTY_SETTINGS); // update without the settings and see if we stick to the values
-
-        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getForceMergeDeletesPctAllowed(), MergePolicyConfig.DEFAULT_EXPUNGE_DELETES_ALLOWED + 1.0d, 0.0d);
-        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getFloorSegmentMB(), new ByteSizeValue(MergePolicyConfig.DEFAULT_FLOOR_SEGMENT.mb() + 1, ByteSizeUnit.MB).mbFrac(), 0.001);
-        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getMaxMergeAtOnce(), MergePolicyConfig.DEFAULT_MAX_MERGE_AT_ONCE-1);
-        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getMaxMergeAtOnceExplicit(), MergePolicyConfig.DEFAULT_MAX_MERGE_AT_ONCE_EXPLICIT-1);
-        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getMaxMergedSegmentMB(), new ByteSizeValue(MergePolicyConfig.DEFAULT_MAX_MERGED_SEGMENT.bytes() + 1).mbFrac(), 0.0001);
-        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getReclaimDeletesWeight(), MergePolicyConfig.DEFAULT_RECLAIM_DELETES_WEIGHT + 1, 0);
-        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getSegmentsPerTier(), MergePolicyConfig.DEFAULT_SEGMENTS_PER_TIER + 1, 0);
-    }
-
-    public Settings build(String value) {
-        return Settings.builder().put(MergePolicyConfig.INDEX_COMPOUND_FORMAT, value).build();
-    }
-
-    public Settings build(double value) {
-        return Settings.builder().put(MergePolicyConfig.INDEX_COMPOUND_FORMAT, value).build();
-    }
-
-    public Settings build(int value) {
-        return Settings.builder().put(MergePolicyConfig.INDEX_COMPOUND_FORMAT, value).build();
-    }
-
-    public Settings build(boolean value) {
-        return Settings.builder().put(MergePolicyConfig.INDEX_COMPOUND_FORMAT, value).build();
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/index/shard/MergePolicySettingsTests.java b/core/src/test/java/org/elasticsearch/index/shard/MergePolicySettingsTests.java
new file mode 100644
index 0000000..0f6b2bd
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/shard/MergePolicySettingsTests.java
@@ -0,0 +1,142 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.index.shard;
+
+import org.apache.lucene.index.NoMergePolicy;
+import org.apache.lucene.index.TieredMergePolicy;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.ByteSizeUnit;
+import org.elasticsearch.common.unit.ByteSizeValue;
+import org.elasticsearch.index.Index;
+import org.elasticsearch.index.settings.IndexSettingsService;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.io.IOException;
+
+import static org.elasticsearch.common.settings.Settings.Builder.EMPTY_SETTINGS;
+import static org.hamcrest.Matchers.equalTo;
+
+public class MergePolicySettingsTests extends ESTestCase {
+
+    protected final ShardId shardId = new ShardId(new Index("index"), 1);
+
+    @Test
+    public void testCompoundFileSettings() throws IOException {
+
+        assertThat(new MergePolicyConfig(logger, EMPTY_SETTINGS).getMergePolicy().getNoCFSRatio(), equalTo(0.1));
+        assertThat(new MergePolicyConfig(logger, build(true)).getMergePolicy().getNoCFSRatio(), equalTo(1.0));
+        assertThat(new MergePolicyConfig(logger, build(0.5)).getMergePolicy().getNoCFSRatio(), equalTo(0.5));
+        assertThat(new MergePolicyConfig(logger, build(1.0)).getMergePolicy().getNoCFSRatio(), equalTo(1.0));
+        assertThat(new MergePolicyConfig(logger, build("true")).getMergePolicy().getNoCFSRatio(), equalTo(1.0));
+        assertThat(new MergePolicyConfig(logger, build("True")).getMergePolicy().getNoCFSRatio(), equalTo(1.0));
+        assertThat(new MergePolicyConfig(logger, build("False")).getMergePolicy().getNoCFSRatio(), equalTo(0.0));
+        assertThat(new MergePolicyConfig(logger, build("false")).getMergePolicy().getNoCFSRatio(), equalTo(0.0));
+        assertThat(new MergePolicyConfig(logger, build(false)).getMergePolicy().getNoCFSRatio(), equalTo(0.0));
+        assertThat(new MergePolicyConfig(logger, build(0)).getMergePolicy().getNoCFSRatio(), equalTo(0.0));
+        assertThat(new MergePolicyConfig(logger, build(0.0)).getMergePolicy().getNoCFSRatio(), equalTo(0.0));
+    }
+
+    public void testNoMerges() {
+        MergePolicyConfig mp = new MergePolicyConfig(logger, Settings.builder().put(MergePolicyConfig.INDEX_MERGE_ENABLED, false).build());
+        assertTrue(mp.getMergePolicy() instanceof NoMergePolicy);
+    }
+
+    @Test
+    public void testUpdateSettings() throws IOException {
+        {
+            IndexSettingsService service = new IndexSettingsService(new Index("test"), EMPTY_SETTINGS);
+            MergePolicyConfig mp = new MergePolicyConfig(logger, EMPTY_SETTINGS);
+            assertThat(((TieredMergePolicy) mp.getMergePolicy()).getNoCFSRatio(), equalTo(0.1));
+
+            mp.onRefreshSettings(build(1.0));
+            assertThat(((TieredMergePolicy) mp.getMergePolicy()).getNoCFSRatio(), equalTo(1.0));
+
+            mp.onRefreshSettings(build(0.1));
+            assertThat(((TieredMergePolicy) mp.getMergePolicy()).getNoCFSRatio(), equalTo(0.1));
+
+            mp.onRefreshSettings(build(0.0));
+            assertThat(((TieredMergePolicy) mp.getMergePolicy()).getNoCFSRatio(), equalTo(0.0));
+        }
+
+
+    }
+
+
+    public void testTieredMergePolicySettingsUpdate() throws IOException {
+        MergePolicyConfig mp = new MergePolicyConfig(logger, EMPTY_SETTINGS);
+        assertThat(mp.getMergePolicy().getNoCFSRatio(), equalTo(0.1));
+
+        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getForceMergeDeletesPctAllowed(), MergePolicyConfig.DEFAULT_EXPUNGE_DELETES_ALLOWED, 0.0d);
+        mp.onRefreshSettings(Settings.builder().put(MergePolicyConfig.INDEX_MERGE_POLICY_EXPUNGE_DELETES_ALLOWED, MergePolicyConfig.DEFAULT_EXPUNGE_DELETES_ALLOWED + 1.0d).build());
+        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getForceMergeDeletesPctAllowed(), MergePolicyConfig.DEFAULT_EXPUNGE_DELETES_ALLOWED + 1.0d, 0.0d);
+
+        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getFloorSegmentMB(), MergePolicyConfig.DEFAULT_FLOOR_SEGMENT.mbFrac(), 0);
+        mp.onRefreshSettings(Settings.builder().put(MergePolicyConfig.INDEX_MERGE_POLICY_FLOOR_SEGMENT, new ByteSizeValue(MergePolicyConfig.DEFAULT_FLOOR_SEGMENT.mb() + 1, ByteSizeUnit.MB)).build());
+        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getFloorSegmentMB(), new ByteSizeValue(MergePolicyConfig.DEFAULT_FLOOR_SEGMENT.mb() + 1, ByteSizeUnit.MB).mbFrac(), 0.001);
+
+        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getMaxMergeAtOnce(), MergePolicyConfig.DEFAULT_MAX_MERGE_AT_ONCE);
+        mp.onRefreshSettings(Settings.builder().put(MergePolicyConfig.INDEX_MERGE_POLICY_MAX_MERGE_AT_ONCE, MergePolicyConfig.DEFAULT_MAX_MERGE_AT_ONCE - 1).build());
+        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getMaxMergeAtOnce(), MergePolicyConfig.DEFAULT_MAX_MERGE_AT_ONCE-1);
+
+        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getMaxMergeAtOnceExplicit(), MergePolicyConfig.DEFAULT_MAX_MERGE_AT_ONCE_EXPLICIT);
+        mp.onRefreshSettings(Settings.builder().put(MergePolicyConfig.INDEX_MERGE_POLICY_MAX_MERGE_AT_ONCE_EXPLICIT, MergePolicyConfig.DEFAULT_MAX_MERGE_AT_ONCE_EXPLICIT - 1).build());
+        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getMaxMergeAtOnceExplicit(), MergePolicyConfig.DEFAULT_MAX_MERGE_AT_ONCE_EXPLICIT-1);
+
+        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getMaxMergedSegmentMB(), MergePolicyConfig.DEFAULT_MAX_MERGED_SEGMENT.mbFrac(), 0.0001);
+        mp.onRefreshSettings(Settings.builder().put(MergePolicyConfig.INDEX_MERGE_POLICY_MAX_MERGED_SEGMENT, new ByteSizeValue(MergePolicyConfig.DEFAULT_MAX_MERGED_SEGMENT.bytes() + 1)).build());
+        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getMaxMergedSegmentMB(), new ByteSizeValue(MergePolicyConfig.DEFAULT_MAX_MERGED_SEGMENT.bytes() + 1).mbFrac(), 0.0001);
+
+        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getReclaimDeletesWeight(), MergePolicyConfig.DEFAULT_RECLAIM_DELETES_WEIGHT, 0);
+        mp.onRefreshSettings(Settings.builder().put(MergePolicyConfig.INDEX_MERGE_POLICY_RECLAIM_DELETES_WEIGHT, MergePolicyConfig.DEFAULT_RECLAIM_DELETES_WEIGHT + 1).build());
+        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getReclaimDeletesWeight(), MergePolicyConfig.DEFAULT_RECLAIM_DELETES_WEIGHT + 1, 0);
+
+        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getSegmentsPerTier(), MergePolicyConfig.DEFAULT_SEGMENTS_PER_TIER, 0);
+        mp.onRefreshSettings(Settings.builder().put(MergePolicyConfig.INDEX_MERGE_POLICY_SEGMENTS_PER_TIER, MergePolicyConfig.DEFAULT_SEGMENTS_PER_TIER + 1).build());
+        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getSegmentsPerTier(), MergePolicyConfig.DEFAULT_SEGMENTS_PER_TIER + 1, 0);
+
+        mp.onRefreshSettings(EMPTY_SETTINGS); // update without the settings and see if we stick to the values
+
+        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getForceMergeDeletesPctAllowed(), MergePolicyConfig.DEFAULT_EXPUNGE_DELETES_ALLOWED + 1.0d, 0.0d);
+        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getFloorSegmentMB(), new ByteSizeValue(MergePolicyConfig.DEFAULT_FLOOR_SEGMENT.mb() + 1, ByteSizeUnit.MB).mbFrac(), 0.001);
+        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getMaxMergeAtOnce(), MergePolicyConfig.DEFAULT_MAX_MERGE_AT_ONCE-1);
+        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getMaxMergeAtOnceExplicit(), MergePolicyConfig.DEFAULT_MAX_MERGE_AT_ONCE_EXPLICIT-1);
+        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getMaxMergedSegmentMB(), new ByteSizeValue(MergePolicyConfig.DEFAULT_MAX_MERGED_SEGMENT.bytes() + 1).mbFrac(), 0.0001);
+        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getReclaimDeletesWeight(), MergePolicyConfig.DEFAULT_RECLAIM_DELETES_WEIGHT + 1, 0);
+        assertEquals(((TieredMergePolicy) mp.getMergePolicy()).getSegmentsPerTier(), MergePolicyConfig.DEFAULT_SEGMENTS_PER_TIER + 1, 0);
+    }
+
+    public Settings build(String value) {
+        return Settings.builder().put(MergePolicyConfig.INDEX_COMPOUND_FORMAT, value).build();
+    }
+
+    public Settings build(double value) {
+        return Settings.builder().put(MergePolicyConfig.INDEX_COMPOUND_FORMAT, value).build();
+    }
+
+    public Settings build(int value) {
+        return Settings.builder().put(MergePolicyConfig.INDEX_COMPOUND_FORMAT, value).build();
+    }
+
+    public Settings build(boolean value) {
+        return Settings.builder().put(MergePolicyConfig.INDEX_COMPOUND_FORMAT, value).build();
+    }
+
+
+}
diff --git a/core/src/test/java/org/elasticsearch/index/shard/NewPathForShardTest.java b/core/src/test/java/org/elasticsearch/index/shard/NewPathForShardTest.java
deleted file mode 100644
index d89c328..0000000
--- a/core/src/test/java/org/elasticsearch/index/shard/NewPathForShardTest.java
+++ /dev/null
@@ -1,236 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.shard;
-
-import com.carrotsearch.randomizedtesting.annotations.Repeat;
-
-import org.apache.lucene.mockfile.FilterFileSystem;
-import org.apache.lucene.mockfile.FilterFileSystemProvider;
-import org.apache.lucene.mockfile.FilterPath;
-import org.apache.lucene.util.Constants;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.SuppressForbidden;
-import org.elasticsearch.common.io.PathUtils;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.env.NodeEnvironment.NodePath;
-import org.elasticsearch.env.NodeEnvironment;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-import java.io.File;
-import java.io.IOException;
-import java.lang.reflect.Field;
-import java.nio.file.FileStore;
-import java.nio.file.FileSystem;
-import java.nio.file.FileSystems;
-import java.nio.file.Files;
-import java.nio.file.Path;
-import java.nio.file.attribute.FileAttributeView;
-import java.nio.file.attribute.FileStoreAttributeView;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-
-/** Separate test class from ShardPathTests because we need static (BeforeClass) setup to install mock filesystems... */
-@SuppressForbidden(reason = "ProviderMismatchException if I try to use PathUtils.getDefault instead")
-public class NewPathForShardTest extends ESTestCase {
-
-    // Sneakiness to install mock file stores so we can pretend how much free space we have on each path.data:
-    private static MockFileStore aFileStore = new MockFileStore("mocka");
-    private static MockFileStore bFileStore = new MockFileStore("mockb");
-    private static FileSystem origFileSystem;
-    private static String aPathPart = File.separator + 'a' + File.separator;
-    private static String bPathPart = File.separator + 'b' + File.separator;
-
-    @BeforeClass
-    public static void installMockUsableSpaceFS() throws Exception {
-        // Necessary so when Environment.clinit runs, to gather all FileStores, it sees ours:
-        origFileSystem = FileSystems.getDefault();
-
-        Field field = PathUtils.class.getDeclaredField("DEFAULT");
-        field.setAccessible(true);
-        FileSystem mock = new MockUsableSpaceFileSystemProvider().getFileSystem(getBaseTempDirForTestClass().toUri());
-        field.set(null, mock);
-        assertEquals(mock, PathUtils.getDefaultFileSystem());
-    }
-
-    @AfterClass
-    public static void removeMockUsableSpaceFS() throws Exception {
-        Field field = PathUtils.class.getDeclaredField("DEFAULT");
-        field.setAccessible(true);
-        field.set(null, origFileSystem);
-        origFileSystem = null;
-        aFileStore = null;
-        bFileStore = null;
-    }
-
-    /** Mock file system that fakes usable space for each FileStore */
-    @SuppressForbidden(reason = "ProviderMismatchException if I try to use PathUtils.getDefault instead")
-    static class MockUsableSpaceFileSystemProvider extends FilterFileSystemProvider {
-    
-        public MockUsableSpaceFileSystemProvider() {
-            super("mockusablespace://", FileSystems.getDefault());
-            final List<FileStore> fileStores = new ArrayList<>();
-            fileStores.add(aFileStore);
-            fileStores.add(bFileStore);
-            fileSystem = new FilterFileSystem(this, origFileSystem) {
-                    @Override
-                    public Iterable<FileStore> getFileStores() {
-                        return fileStores;
-                    }
-                };
-        }
-
-        @Override
-        public FileStore getFileStore(Path path) throws IOException {
-            if (path.toString().contains(aPathPart)) {
-                return aFileStore;
-            } else {
-                return bFileStore;
-            }
-        }
-    }
-
-    static class MockFileStore extends FileStore {
-
-        public long usableSpace;
-
-        private final String desc;
-
-        public MockFileStore(String desc) {
-            this.desc = desc;
-        }
-    
-        @Override
-        public String type() {
-            return "mock";
-        }
-
-        @Override
-        public String name() {
-            return desc;
-        }
-
-        @Override
-        public String toString() {
-            return desc;
-        }
-
-        @Override
-        public boolean isReadOnly() {
-            return false;
-        }
-
-        @Override
-        public long getTotalSpace() throws IOException {
-            return usableSpace*3;
-        }
-
-        @Override
-        public long getUsableSpace() throws IOException {
-            return usableSpace;
-        }
-
-        @Override
-        public long getUnallocatedSpace() throws IOException {
-            return usableSpace*2;
-        }
-
-        @Override
-        public boolean supportsFileAttributeView(Class<? extends FileAttributeView> type) {
-            return false;
-        }
-
-        @Override
-        public boolean supportsFileAttributeView(String name) {
-            return false;
-        }
-
-        @Override
-        public <V extends FileStoreAttributeView> V getFileStoreAttributeView(Class<V> type) {
-            return null;
-        }
-
-        @Override
-        public Object getAttribute(String attribute) throws IOException {
-            return null;
-        }
-    }
-
-    public void testSelectNewPathForShard() throws Exception {
-        assumeFalse("Consistenty fails on windows ('could not remove the following files')", Constants.WINDOWS);
-        Path path = PathUtils.get(createTempDir().toString());
-
-        // Use 2 data paths:
-        String[] paths = new String[] {path.resolve("a").toString(),
-                                       path.resolve("b").toString()};
-
-        Settings settings = Settings.builder()
-            .put("path.home", path)
-            .putArray("path.data", paths).build();
-        NodeEnvironment nodeEnv = new NodeEnvironment(settings, new Environment(settings));
-
-        // Make sure all our mocking above actually worked:
-        NodePath[] nodePaths = nodeEnv.nodePaths();
-        assertEquals(2, nodePaths.length);
-
-        assertEquals("mocka", nodePaths[0].fileStore.name());
-        assertEquals("mockb", nodePaths[1].fileStore.name());
-
-        // Path a has lots of free space, but b has little, so new shard should go to a:
-        aFileStore.usableSpace = 100000;
-        bFileStore.usableSpace = 1000;
-
-        ShardId shardId = new ShardId("index", 0);
-        ShardPath result = ShardPath.selectNewPathForShard(nodeEnv, shardId, Settings.EMPTY, 100, Collections.<Path,Integer>emptyMap());
-        assertTrue(result.getDataPath().toString().contains(aPathPart));
-
-        // Test the reverse: b has lots of free space, but a has little, so new shard should go to b:
-        aFileStore.usableSpace = 1000;
-        bFileStore.usableSpace = 100000;
-
-        shardId = new ShardId("index", 0);
-        result = ShardPath.selectNewPathForShard(nodeEnv, shardId, Settings.EMPTY, 100, Collections.<Path,Integer>emptyMap());
-        assertTrue(result.getDataPath().toString().contains(bPathPart));
-
-        // Now a and be have equal usable space; we allocate two shards to the node, and each should go to different paths:
-        aFileStore.usableSpace = 100000;
-        bFileStore.usableSpace = 100000;
-
-        Map<Path,Integer> dataPathToShardCount = new HashMap<>();
-        ShardPath result1 = ShardPath.selectNewPathForShard(nodeEnv, shardId, Settings.EMPTY, 100, dataPathToShardCount);
-        dataPathToShardCount.put(NodeEnvironment.shardStatePathToDataPath(result1.getDataPath()), 1);
-        ShardPath result2 = ShardPath.selectNewPathForShard(nodeEnv, shardId, Settings.EMPTY, 100, dataPathToShardCount);
-
-        // #11122: this was the original failure: on a node with 2 disks that have nearly equal
-        // free space, we would always allocate all N incoming shards to the one path that
-        // had the most free space, never using the other drive unless new shards arrive
-        // after the first shards started using storage:
-        assertNotEquals(result1.getDataPath(), result2.getDataPath());
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/shard/NewPathForShardTests.java b/core/src/test/java/org/elasticsearch/index/shard/NewPathForShardTests.java
new file mode 100644
index 0000000..0780cd7
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/shard/NewPathForShardTests.java
@@ -0,0 +1,236 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.index.shard;
+
+import com.carrotsearch.randomizedtesting.annotations.Repeat;
+
+import org.apache.lucene.mockfile.FilterFileSystem;
+import org.apache.lucene.mockfile.FilterFileSystemProvider;
+import org.apache.lucene.mockfile.FilterPath;
+import org.apache.lucene.util.Constants;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.common.SuppressForbidden;
+import org.elasticsearch.common.io.PathUtils;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
+import org.elasticsearch.env.NodeEnvironment.NodePath;
+import org.elasticsearch.env.NodeEnvironment;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import java.io.File;
+import java.io.IOException;
+import java.lang.reflect.Field;
+import java.nio.file.FileStore;
+import java.nio.file.FileSystem;
+import java.nio.file.FileSystems;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.attribute.FileAttributeView;
+import java.nio.file.attribute.FileStoreAttributeView;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import static org.elasticsearch.common.settings.Settings.settingsBuilder;
+
+/** Separate test class from ShardPathTests because we need static (BeforeClass) setup to install mock filesystems... */
+@SuppressForbidden(reason = "ProviderMismatchException if I try to use PathUtils.getDefault instead")
+public class NewPathForShardTests extends ESTestCase {
+
+    // Sneakiness to install mock file stores so we can pretend how much free space we have on each path.data:
+    private static MockFileStore aFileStore = new MockFileStore("mocka");
+    private static MockFileStore bFileStore = new MockFileStore("mockb");
+    private static FileSystem origFileSystem;
+    private static String aPathPart = File.separator + 'a' + File.separator;
+    private static String bPathPart = File.separator + 'b' + File.separator;
+
+    @BeforeClass
+    public static void installMockUsableSpaceFS() throws Exception {
+        // Necessary so when Environment.clinit runs, to gather all FileStores, it sees ours:
+        origFileSystem = FileSystems.getDefault();
+
+        Field field = PathUtils.class.getDeclaredField("DEFAULT");
+        field.setAccessible(true);
+        FileSystem mock = new MockUsableSpaceFileSystemProvider().getFileSystem(getBaseTempDirForTestClass().toUri());
+        field.set(null, mock);
+        assertEquals(mock, PathUtils.getDefaultFileSystem());
+    }
+
+    @AfterClass
+    public static void removeMockUsableSpaceFS() throws Exception {
+        Field field = PathUtils.class.getDeclaredField("DEFAULT");
+        field.setAccessible(true);
+        field.set(null, origFileSystem);
+        origFileSystem = null;
+        aFileStore = null;
+        bFileStore = null;
+    }
+
+    /** Mock file system that fakes usable space for each FileStore */
+    @SuppressForbidden(reason = "ProviderMismatchException if I try to use PathUtils.getDefault instead")
+    static class MockUsableSpaceFileSystemProvider extends FilterFileSystemProvider {
+    
+        public MockUsableSpaceFileSystemProvider() {
+            super("mockusablespace://", FileSystems.getDefault());
+            final List<FileStore> fileStores = new ArrayList<>();
+            fileStores.add(aFileStore);
+            fileStores.add(bFileStore);
+            fileSystem = new FilterFileSystem(this, origFileSystem) {
+                    @Override
+                    public Iterable<FileStore> getFileStores() {
+                        return fileStores;
+                    }
+                };
+        }
+
+        @Override
+        public FileStore getFileStore(Path path) throws IOException {
+            if (path.toString().contains(aPathPart)) {
+                return aFileStore;
+            } else {
+                return bFileStore;
+            }
+        }
+    }
+
+    static class MockFileStore extends FileStore {
+
+        public long usableSpace;
+
+        private final String desc;
+
+        public MockFileStore(String desc) {
+            this.desc = desc;
+        }
+    
+        @Override
+        public String type() {
+            return "mock";
+        }
+
+        @Override
+        public String name() {
+            return desc;
+        }
+
+        @Override
+        public String toString() {
+            return desc;
+        }
+
+        @Override
+        public boolean isReadOnly() {
+            return false;
+        }
+
+        @Override
+        public long getTotalSpace() throws IOException {
+            return usableSpace*3;
+        }
+
+        @Override
+        public long getUsableSpace() throws IOException {
+            return usableSpace;
+        }
+
+        @Override
+        public long getUnallocatedSpace() throws IOException {
+            return usableSpace*2;
+        }
+
+        @Override
+        public boolean supportsFileAttributeView(Class<? extends FileAttributeView> type) {
+            return false;
+        }
+
+        @Override
+        public boolean supportsFileAttributeView(String name) {
+            return false;
+        }
+
+        @Override
+        public <V extends FileStoreAttributeView> V getFileStoreAttributeView(Class<V> type) {
+            return null;
+        }
+
+        @Override
+        public Object getAttribute(String attribute) throws IOException {
+            return null;
+        }
+    }
+
+    public void testSelectNewPathForShard() throws Exception {
+        assumeFalse("Consistenty fails on windows ('could not remove the following files')", Constants.WINDOWS);
+        Path path = PathUtils.get(createTempDir().toString());
+
+        // Use 2 data paths:
+        String[] paths = new String[] {path.resolve("a").toString(),
+                                       path.resolve("b").toString()};
+
+        Settings settings = Settings.builder()
+            .put("path.home", path)
+            .putArray("path.data", paths).build();
+        NodeEnvironment nodeEnv = new NodeEnvironment(settings, new Environment(settings));
+
+        // Make sure all our mocking above actually worked:
+        NodePath[] nodePaths = nodeEnv.nodePaths();
+        assertEquals(2, nodePaths.length);
+
+        assertEquals("mocka", nodePaths[0].fileStore.name());
+        assertEquals("mockb", nodePaths[1].fileStore.name());
+
+        // Path a has lots of free space, but b has little, so new shard should go to a:
+        aFileStore.usableSpace = 100000;
+        bFileStore.usableSpace = 1000;
+
+        ShardId shardId = new ShardId("index", 0);
+        ShardPath result = ShardPath.selectNewPathForShard(nodeEnv, shardId, Settings.EMPTY, 100, Collections.<Path,Integer>emptyMap());
+        assertTrue(result.getDataPath().toString().contains(aPathPart));
+
+        // Test the reverse: b has lots of free space, but a has little, so new shard should go to b:
+        aFileStore.usableSpace = 1000;
+        bFileStore.usableSpace = 100000;
+
+        shardId = new ShardId("index", 0);
+        result = ShardPath.selectNewPathForShard(nodeEnv, shardId, Settings.EMPTY, 100, Collections.<Path,Integer>emptyMap());
+        assertTrue(result.getDataPath().toString().contains(bPathPart));
+
+        // Now a and be have equal usable space; we allocate two shards to the node, and each should go to different paths:
+        aFileStore.usableSpace = 100000;
+        bFileStore.usableSpace = 100000;
+
+        Map<Path,Integer> dataPathToShardCount = new HashMap<>();
+        ShardPath result1 = ShardPath.selectNewPathForShard(nodeEnv, shardId, Settings.EMPTY, 100, dataPathToShardCount);
+        dataPathToShardCount.put(NodeEnvironment.shardStatePathToDataPath(result1.getDataPath()), 1);
+        ShardPath result2 = ShardPath.selectNewPathForShard(nodeEnv, shardId, Settings.EMPTY, 100, dataPathToShardCount);
+
+        // #11122: this was the original failure: on a node with 2 disks that have nearly equal
+        // free space, we would always allocate all N incoming shards to the one path that
+        // had the most free space, never using the other drive unless new shards arrive
+        // after the first shards started using storage:
+        assertNotEquals(result1.getDataPath(), result2.getDataPath());
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/index/shard/VersionFieldUpgraderTest.java b/core/src/test/java/org/elasticsearch/index/shard/VersionFieldUpgraderTest.java
deleted file mode 100644
index cf25c1c..0000000
--- a/core/src/test/java/org/elasticsearch/index/shard/VersionFieldUpgraderTest.java
+++ /dev/null
@@ -1,144 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.shard;
-
-import org.apache.lucene.analysis.CannedTokenStream;
-import org.apache.lucene.analysis.Token;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.CodecReader;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.DocValuesType;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.TestUtil;
-import org.elasticsearch.common.Numbers;
-import org.elasticsearch.index.mapper.internal.UidFieldMapper;
-import org.elasticsearch.index.mapper.internal.VersionFieldMapper;
-import org.elasticsearch.test.ESTestCase;
-
-/** Tests upgrading old document versions from _uid payloads to _version docvalues */
-public class VersionFieldUpgraderTest extends ESTestCase {
-    
-    /** Simple test: one doc in the old format, check that it looks correct */
-    public void testUpgradeOneDocument() throws Exception {
-        Directory dir = newDirectory();
-        IndexWriter iw = new IndexWriter(dir, new IndexWriterConfig(null));
-        
-        // add a document with a _uid having a payload of 3
-        Document doc = new Document();
-        Token token = new Token("1", 0, 1);
-        token.setPayload(new BytesRef(Numbers.longToBytes(3)));
-        doc.add(new TextField(UidFieldMapper.NAME, new CannedTokenStream(token)));
-        iw.addDocument(doc);
-        iw.commit();
-        
-        CodecReader reader = getOnlySegmentReader(DirectoryReader.open(iw, true));
-        CodecReader upgraded = VersionFieldUpgrader.wrap(reader);
-        // we need to be upgraded, should be a different instance
-        assertNotSame(reader, upgraded);
-        
-        // make sure we can see our numericdocvalues in fieldinfos
-        FieldInfo versionField = upgraded.getFieldInfos().fieldInfo(VersionFieldMapper.NAME);
-        assertNotNull(versionField);
-        assertEquals(DocValuesType.NUMERIC, versionField.getDocValuesType());
-        // should have a value of 3, and be visible in docsWithField
-        assertEquals(3, upgraded.getNumericDocValues(VersionFieldMapper.NAME).get(0));
-        assertTrue(upgraded.getDocsWithField(VersionFieldMapper.NAME).get(0));
-        
-        // verify filterreader with checkindex
-        TestUtil.checkReader(upgraded);
-        
-        reader.close();
-        iw.close();
-        dir.close();
-    }
-    
-    /** test that we are a non-op if the segment already has the version field */
-    public void testAlreadyUpgraded() throws Exception {
-        Directory dir = newDirectory();
-        IndexWriter iw = new IndexWriter(dir, new IndexWriterConfig(null));
-        
-        // add a document with a _uid having a payload of 3
-        Document doc = new Document();
-        Token token = new Token("1", 0, 1);
-        token.setPayload(new BytesRef(Numbers.longToBytes(3)));
-        doc.add(new TextField(UidFieldMapper.NAME, new CannedTokenStream(token)));
-        doc.add(new NumericDocValuesField(VersionFieldMapper.NAME, 3));
-        iw.addDocument(doc);
-        iw.commit();
-        
-        CodecReader reader = getOnlySegmentReader(DirectoryReader.open(iw, true));
-        CodecReader upgraded = VersionFieldUpgrader.wrap(reader);
-        // we already upgraded: should be same instance
-        assertSame(reader, upgraded);
-        
-        reader.close();
-        iw.close();
-        dir.close();
-    }
-    
-    /** Test upgrading two documents */
-    public void testUpgradeTwoDocuments() throws Exception {
-        Directory dir = newDirectory();
-        IndexWriter iw = new IndexWriter(dir, new IndexWriterConfig(null));
-        
-        // add a document with a _uid having a payload of 3
-        Document doc = new Document();
-        Token token = new Token("1", 0, 1);
-        token.setPayload(new BytesRef(Numbers.longToBytes(3)));
-        doc.add(new TextField(UidFieldMapper.NAME, new CannedTokenStream(token)));
-        iw.addDocument(doc);
-        
-        doc = new Document();
-        token = new Token("2", 0, 1);
-        token.setPayload(new BytesRef(Numbers.longToBytes(4)));
-        doc.add(new TextField(UidFieldMapper.NAME, new CannedTokenStream(token)));
-        iw.addDocument(doc);
-
-        iw.commit();
-        
-        CodecReader reader = getOnlySegmentReader(DirectoryReader.open(iw, true));
-        CodecReader upgraded = VersionFieldUpgrader.wrap(reader);
-        // we need to be upgraded, should be a different instance
-        assertNotSame(reader, upgraded);
-        
-        // make sure we can see our numericdocvalues in fieldinfos
-        FieldInfo versionField = upgraded.getFieldInfos().fieldInfo(VersionFieldMapper.NAME);
-        assertNotNull(versionField);
-        assertEquals(DocValuesType.NUMERIC, versionField.getDocValuesType());
-        // should have a values of 3 and 4, and be visible in docsWithField
-        assertEquals(3, upgraded.getNumericDocValues(VersionFieldMapper.NAME).get(0));
-        assertEquals(4, upgraded.getNumericDocValues(VersionFieldMapper.NAME).get(1));
-        assertTrue(upgraded.getDocsWithField(VersionFieldMapper.NAME).get(0));
-        assertTrue(upgraded.getDocsWithField(VersionFieldMapper.NAME).get(1));
-        
-        // verify filterreader with checkindex
-        TestUtil.checkReader(upgraded);
-        
-        reader.close();
-        iw.close();
-        dir.close();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/shard/VersionFieldUpgraderTests.java b/core/src/test/java/org/elasticsearch/index/shard/VersionFieldUpgraderTests.java
new file mode 100644
index 0000000..2fc02fb
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/shard/VersionFieldUpgraderTests.java
@@ -0,0 +1,144 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.shard;
+
+import org.apache.lucene.analysis.CannedTokenStream;
+import org.apache.lucene.analysis.Token;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.CodecReader;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.DocValuesType;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.TestUtil;
+import org.elasticsearch.common.Numbers;
+import org.elasticsearch.index.mapper.internal.UidFieldMapper;
+import org.elasticsearch.index.mapper.internal.VersionFieldMapper;
+import org.elasticsearch.test.ESTestCase;
+
+/** Tests upgrading old document versions from _uid payloads to _version docvalues */
+public class VersionFieldUpgraderTests extends ESTestCase {
+    
+    /** Simple test: one doc in the old format, check that it looks correct */
+    public void testUpgradeOneDocument() throws Exception {
+        Directory dir = newDirectory();
+        IndexWriter iw = new IndexWriter(dir, new IndexWriterConfig(null));
+        
+        // add a document with a _uid having a payload of 3
+        Document doc = new Document();
+        Token token = new Token("1", 0, 1);
+        token.setPayload(new BytesRef(Numbers.longToBytes(3)));
+        doc.add(new TextField(UidFieldMapper.NAME, new CannedTokenStream(token)));
+        iw.addDocument(doc);
+        iw.commit();
+        
+        CodecReader reader = getOnlySegmentReader(DirectoryReader.open(iw, true));
+        CodecReader upgraded = VersionFieldUpgrader.wrap(reader);
+        // we need to be upgraded, should be a different instance
+        assertNotSame(reader, upgraded);
+        
+        // make sure we can see our numericdocvalues in fieldinfos
+        FieldInfo versionField = upgraded.getFieldInfos().fieldInfo(VersionFieldMapper.NAME);
+        assertNotNull(versionField);
+        assertEquals(DocValuesType.NUMERIC, versionField.getDocValuesType());
+        // should have a value of 3, and be visible in docsWithField
+        assertEquals(3, upgraded.getNumericDocValues(VersionFieldMapper.NAME).get(0));
+        assertTrue(upgraded.getDocsWithField(VersionFieldMapper.NAME).get(0));
+        
+        // verify filterreader with checkindex
+        TestUtil.checkReader(upgraded);
+        
+        reader.close();
+        iw.close();
+        dir.close();
+    }
+    
+    /** test that we are a non-op if the segment already has the version field */
+    public void testAlreadyUpgraded() throws Exception {
+        Directory dir = newDirectory();
+        IndexWriter iw = new IndexWriter(dir, new IndexWriterConfig(null));
+        
+        // add a document with a _uid having a payload of 3
+        Document doc = new Document();
+        Token token = new Token("1", 0, 1);
+        token.setPayload(new BytesRef(Numbers.longToBytes(3)));
+        doc.add(new TextField(UidFieldMapper.NAME, new CannedTokenStream(token)));
+        doc.add(new NumericDocValuesField(VersionFieldMapper.NAME, 3));
+        iw.addDocument(doc);
+        iw.commit();
+        
+        CodecReader reader = getOnlySegmentReader(DirectoryReader.open(iw, true));
+        CodecReader upgraded = VersionFieldUpgrader.wrap(reader);
+        // we already upgraded: should be same instance
+        assertSame(reader, upgraded);
+        
+        reader.close();
+        iw.close();
+        dir.close();
+    }
+    
+    /** Test upgrading two documents */
+    public void testUpgradeTwoDocuments() throws Exception {
+        Directory dir = newDirectory();
+        IndexWriter iw = new IndexWriter(dir, new IndexWriterConfig(null));
+        
+        // add a document with a _uid having a payload of 3
+        Document doc = new Document();
+        Token token = new Token("1", 0, 1);
+        token.setPayload(new BytesRef(Numbers.longToBytes(3)));
+        doc.add(new TextField(UidFieldMapper.NAME, new CannedTokenStream(token)));
+        iw.addDocument(doc);
+        
+        doc = new Document();
+        token = new Token("2", 0, 1);
+        token.setPayload(new BytesRef(Numbers.longToBytes(4)));
+        doc.add(new TextField(UidFieldMapper.NAME, new CannedTokenStream(token)));
+        iw.addDocument(doc);
+
+        iw.commit();
+        
+        CodecReader reader = getOnlySegmentReader(DirectoryReader.open(iw, true));
+        CodecReader upgraded = VersionFieldUpgrader.wrap(reader);
+        // we need to be upgraded, should be a different instance
+        assertNotSame(reader, upgraded);
+        
+        // make sure we can see our numericdocvalues in fieldinfos
+        FieldInfo versionField = upgraded.getFieldInfos().fieldInfo(VersionFieldMapper.NAME);
+        assertNotNull(versionField);
+        assertEquals(DocValuesType.NUMERIC, versionField.getDocValuesType());
+        // should have a values of 3 and 4, and be visible in docsWithField
+        assertEquals(3, upgraded.getNumericDocValues(VersionFieldMapper.NAME).get(0));
+        assertEquals(4, upgraded.getNumericDocValues(VersionFieldMapper.NAME).get(1));
+        assertTrue(upgraded.getDocsWithField(VersionFieldMapper.NAME).get(0));
+        assertTrue(upgraded.getDocsWithField(VersionFieldMapper.NAME).get(1));
+        
+        // verify filterreader with checkindex
+        TestUtil.checkReader(upgraded);
+        
+        reader.close();
+        iw.close();
+        dir.close();
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/index/snapshots/blobstore/FileInfoTest.java b/core/src/test/java/org/elasticsearch/index/snapshots/blobstore/FileInfoTest.java
deleted file mode 100644
index 297c228..0000000
--- a/core/src/test/java/org/elasticsearch/index/snapshots/blobstore/FileInfoTest.java
+++ /dev/null
@@ -1,140 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.snapshots.blobstore;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Version;
-import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.xcontent.*;
-import org.elasticsearch.index.store.StoreFileMetaData;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot.FileInfo.Fields;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.is;
-
-/**
- */
-public class FileInfoTest extends ESTestCase {
-
-    @Test
-    public void testToFromXContent() throws IOException {
-        final int iters = scaledRandomIntBetween(1, 10);
-        for (int iter = 0; iter < iters; iter++) {
-            final BytesRef hash = new BytesRef(scaledRandomIntBetween(0, 1024 * 1024));
-            hash.length = hash.bytes.length;
-            for (int i = 0; i < hash.length; i++) {
-                hash.bytes[i] = randomByte();
-            }
-            StoreFileMetaData meta = new StoreFileMetaData("foobar", Math.abs(randomLong()), randomAsciiOfLengthBetween(1, 10), Version.LATEST, hash);
-            ByteSizeValue size = new ByteSizeValue(Math.abs(randomLong()));
-            BlobStoreIndexShardSnapshot.FileInfo info = new BlobStoreIndexShardSnapshot.FileInfo("_foobar", meta, size);
-            XContentBuilder builder = XContentFactory.contentBuilder(XContentType.JSON).prettyPrint();
-            BlobStoreIndexShardSnapshot.FileInfo.toXContent(info, builder, ToXContent.EMPTY_PARAMS);
-            byte[] xcontent = builder.bytes().toBytes();
-
-            final BlobStoreIndexShardSnapshot.FileInfo parsedInfo;
-            try (XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(xcontent)) {
-                parser.nextToken();
-                parsedInfo = BlobStoreIndexShardSnapshot.FileInfo.fromXContent(parser);
-            }
-            assertThat(info.name(), equalTo(parsedInfo.name()));
-            assertThat(info.physicalName(), equalTo(parsedInfo.physicalName()));
-            assertThat(info.length(), equalTo(parsedInfo.length()));
-            assertThat(info.checksum(), equalTo(parsedInfo.checksum()));
-            assertThat(info.partBytes(), equalTo(parsedInfo.partBytes()));
-            assertThat(parsedInfo.metadata().hash().length, equalTo(hash.length));
-            assertThat(parsedInfo.metadata().hash(), equalTo(hash));
-            assertThat(parsedInfo.metadata().writtenBy(), equalTo(Version.LATEST));
-            assertThat(parsedInfo.isSame(info.metadata()), is(true));
-        }
-    }
-
-    @Test
-    public void testInvalidFieldsInFromXContent() throws IOException {
-        final int iters = scaledRandomIntBetween(1, 10);
-        for (int iter = 0; iter < iters; iter++) {
-            final BytesRef hash = new BytesRef(scaledRandomIntBetween(0, 1024 * 1024));
-            hash.length = hash.bytes.length;
-            for (int i = 0; i < hash.length; i++) {
-                hash.bytes[i] = randomByte();
-            }
-            String name = "foobar";
-            String physicalName = "_foobar";
-            String failure = null;
-            long length = Math.max(0,Math.abs(randomLong()));
-            // random corruption
-            switch (randomIntBetween(0, 3)) {
-                case 0:
-                    name = "foo,bar";
-                    failure = "missing or invalid file name";
-                    break;
-                case 1:
-                    physicalName = "_foo,bar";
-                    failure = "missing or invalid physical file name";
-                    break;
-                case 2:
-                    length = -Math.abs(randomLong());
-                    failure = "missing or invalid file length";
-                    break;
-                case 3:
-                    break;
-                default:
-                    fail("shouldn't be here");
-            }
-
-            XContentBuilder builder = XContentFactory.contentBuilder(XContentType.JSON);
-            builder.startObject();
-            builder.field(Fields.NAME, name);
-            builder.field(Fields.PHYSICAL_NAME, physicalName);
-            builder.field(Fields.LENGTH, length);
-            builder.endObject();
-            byte[] xContent = builder.bytes().toBytes();
-
-            if (failure == null) {
-                // No failures should read as usual
-                final BlobStoreIndexShardSnapshot.FileInfo parsedInfo;
-                try (XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(xContent)) {
-                    parser.nextToken();
-                    parsedInfo = BlobStoreIndexShardSnapshot.FileInfo.fromXContent(parser);
-                }
-                assertThat(name, equalTo(parsedInfo.name()));
-                assertThat(physicalName, equalTo(parsedInfo.physicalName()));
-                assertThat(length, equalTo(parsedInfo.length()));
-                assertNull(parsedInfo.checksum());
-                assertNull(parsedInfo.metadata().checksum());
-                assertNull(parsedInfo.metadata().writtenBy());
-            } else {
-                try (XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(xContent)) {
-                    parser.nextToken();
-                    BlobStoreIndexShardSnapshot.FileInfo.fromXContent(parser);
-                    fail("Should have failed with [" + failure + "]");
-                } catch (ElasticsearchParseException ex) {
-                    assertThat(ex.getMessage(), containsString(failure));
-                }
-
-            }
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/snapshots/blobstore/FileInfoTests.java b/core/src/test/java/org/elasticsearch/index/snapshots/blobstore/FileInfoTests.java
new file mode 100644
index 0000000..8a72a77
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/snapshots/blobstore/FileInfoTests.java
@@ -0,0 +1,140 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.index.snapshots.blobstore;
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Version;
+import org.elasticsearch.ElasticsearchParseException;
+import org.elasticsearch.common.unit.ByteSizeValue;
+import org.elasticsearch.common.xcontent.*;
+import org.elasticsearch.index.store.StoreFileMetaData;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot.FileInfo.Fields;
+import org.junit.Test;
+
+import java.io.IOException;
+
+import static org.hamcrest.Matchers.containsString;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.is;
+
+/**
+ */
+public class FileInfoTests extends ESTestCase {
+
+    @Test
+    public void testToFromXContent() throws IOException {
+        final int iters = scaledRandomIntBetween(1, 10);
+        for (int iter = 0; iter < iters; iter++) {
+            final BytesRef hash = new BytesRef(scaledRandomIntBetween(0, 1024 * 1024));
+            hash.length = hash.bytes.length;
+            for (int i = 0; i < hash.length; i++) {
+                hash.bytes[i] = randomByte();
+            }
+            StoreFileMetaData meta = new StoreFileMetaData("foobar", Math.abs(randomLong()), randomAsciiOfLengthBetween(1, 10), Version.LATEST, hash);
+            ByteSizeValue size = new ByteSizeValue(Math.abs(randomLong()));
+            BlobStoreIndexShardSnapshot.FileInfo info = new BlobStoreIndexShardSnapshot.FileInfo("_foobar", meta, size);
+            XContentBuilder builder = XContentFactory.contentBuilder(XContentType.JSON).prettyPrint();
+            BlobStoreIndexShardSnapshot.FileInfo.toXContent(info, builder, ToXContent.EMPTY_PARAMS);
+            byte[] xcontent = builder.bytes().toBytes();
+
+            final BlobStoreIndexShardSnapshot.FileInfo parsedInfo;
+            try (XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(xcontent)) {
+                parser.nextToken();
+                parsedInfo = BlobStoreIndexShardSnapshot.FileInfo.fromXContent(parser);
+            }
+            assertThat(info.name(), equalTo(parsedInfo.name()));
+            assertThat(info.physicalName(), equalTo(parsedInfo.physicalName()));
+            assertThat(info.length(), equalTo(parsedInfo.length()));
+            assertThat(info.checksum(), equalTo(parsedInfo.checksum()));
+            assertThat(info.partBytes(), equalTo(parsedInfo.partBytes()));
+            assertThat(parsedInfo.metadata().hash().length, equalTo(hash.length));
+            assertThat(parsedInfo.metadata().hash(), equalTo(hash));
+            assertThat(parsedInfo.metadata().writtenBy(), equalTo(Version.LATEST));
+            assertThat(parsedInfo.isSame(info.metadata()), is(true));
+        }
+    }
+
+    @Test
+    public void testInvalidFieldsInFromXContent() throws IOException {
+        final int iters = scaledRandomIntBetween(1, 10);
+        for (int iter = 0; iter < iters; iter++) {
+            final BytesRef hash = new BytesRef(scaledRandomIntBetween(0, 1024 * 1024));
+            hash.length = hash.bytes.length;
+            for (int i = 0; i < hash.length; i++) {
+                hash.bytes[i] = randomByte();
+            }
+            String name = "foobar";
+            String physicalName = "_foobar";
+            String failure = null;
+            long length = Math.max(0,Math.abs(randomLong()));
+            // random corruption
+            switch (randomIntBetween(0, 3)) {
+                case 0:
+                    name = "foo,bar";
+                    failure = "missing or invalid file name";
+                    break;
+                case 1:
+                    physicalName = "_foo,bar";
+                    failure = "missing or invalid physical file name";
+                    break;
+                case 2:
+                    length = -Math.abs(randomLong());
+                    failure = "missing or invalid file length";
+                    break;
+                case 3:
+                    break;
+                default:
+                    fail("shouldn't be here");
+            }
+
+            XContentBuilder builder = XContentFactory.contentBuilder(XContentType.JSON);
+            builder.startObject();
+            builder.field(Fields.NAME, name);
+            builder.field(Fields.PHYSICAL_NAME, physicalName);
+            builder.field(Fields.LENGTH, length);
+            builder.endObject();
+            byte[] xContent = builder.bytes().toBytes();
+
+            if (failure == null) {
+                // No failures should read as usual
+                final BlobStoreIndexShardSnapshot.FileInfo parsedInfo;
+                try (XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(xContent)) {
+                    parser.nextToken();
+                    parsedInfo = BlobStoreIndexShardSnapshot.FileInfo.fromXContent(parser);
+                }
+                assertThat(name, equalTo(parsedInfo.name()));
+                assertThat(physicalName, equalTo(parsedInfo.physicalName()));
+                assertThat(length, equalTo(parsedInfo.length()));
+                assertNull(parsedInfo.checksum());
+                assertNull(parsedInfo.metadata().checksum());
+                assertNull(parsedInfo.metadata().writtenBy());
+            } else {
+                try (XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(xContent)) {
+                    parser.nextToken();
+                    BlobStoreIndexShardSnapshot.FileInfo.fromXContent(parser);
+                    fail("Should have failed with [" + failure + "]");
+                } catch (ElasticsearchParseException ex) {
+                    assertThat(ex.getMessage(), containsString(failure));
+                }
+
+            }
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/index/snapshots/blobstore/SlicedInputStreamTest.java b/core/src/test/java/org/elasticsearch/index/snapshots/blobstore/SlicedInputStreamTest.java
deleted file mode 100644
index 8adc7f6..0000000
--- a/core/src/test/java/org/elasticsearch/index/snapshots/blobstore/SlicedInputStreamTest.java
+++ /dev/null
@@ -1,133 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.snapshots.blobstore;
-
-import com.carrotsearch.randomizedtesting.generators.RandomInts;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.io.*;
-import java.util.Random;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public class SlicedInputStreamTest extends ESTestCase {
-
-    @Test
-    public void readRandom() throws IOException {
-        int parts = randomIntBetween(1, 20);
-        ByteArrayOutputStream stream = new ByteArrayOutputStream();
-        int numWriteOps = scaledRandomIntBetween(1000, 10000);
-        final long seed = randomLong();
-        Random random = new Random(seed);
-        for (int i = 0; i < numWriteOps; i++) {
-            switch(random.nextInt(5)) {
-                case 1:
-                    stream.write(random.nextInt(Byte.MAX_VALUE));
-                    break;
-                default:
-                    stream.write(randomBytes(random));
-                    break;
-            }
-        }
-
-        final CheckClosedInputStream[] streams = new CheckClosedInputStream[parts];
-        byte[] bytes = stream.toByteArray();
-        int slice = bytes.length / parts;
-        int offset = 0;
-        int length;
-        for (int i = 0; i < parts; i++) {
-            length = i == parts-1 ? bytes.length-offset : slice;
-            streams[i] = new CheckClosedInputStream(new ByteArrayInputStream(bytes, offset, length));
-            offset += length;
-        }
-
-        SlicedInputStream input = new SlicedInputStream(parts) {
-
-            @Override
-            protected InputStream openSlice(long slice) throws IOException {
-                return streams[(int)slice];
-            }
-        };
-        random = new Random(seed);
-        assertThat(input.available(), equalTo(streams[0].available()));
-        for (int i = 0; i < numWriteOps; i++) {
-            switch(random.nextInt(5)) {
-                case 1:
-                    assertThat(random.nextInt(Byte.MAX_VALUE), equalTo(input.read()));
-                    break;
-                default:
-                    byte[] b = randomBytes(random);
-                    byte[] buffer = new byte[b.length];
-                    int read = readFully(input, buffer);
-                    assertThat(b.length, equalTo(read));
-                    assertArrayEquals(b, buffer);
-                    break;
-            }
-        }
-
-        assertThat(input.available(), equalTo(0));
-        for (int i =0; i < streams.length-1; i++) {
-            assertTrue(streams[i].closed);
-        }
-        input.close();
-
-        for (int i =0; i < streams.length; i++) {
-            assertTrue(streams[i].closed);
-        }
-
-    }
-
-    private int readFully(InputStream stream, byte[] buffer) throws IOException {
-        for (int i = 0; i < buffer.length;) {
-            int read = stream.read(buffer, i, buffer.length-i);
-            if (read == -1) {
-              if (i == 0) {
-                  return -1;
-              } else {
-                  return i;
-              }
-            }
-            i+= read;
-        }
-        return buffer.length;
-    }
-
-    private byte[] randomBytes(Random random) {
-        int length = RandomInts.randomIntBetween(random, 1, 10);
-        byte[] data = new byte[length];
-        random.nextBytes(data);
-        return data;
-    }
-
-    private static final class CheckClosedInputStream extends FilterInputStream {
-
-        public boolean closed = false;
-
-        public CheckClosedInputStream(InputStream in) {
-            super(in);
-        }
-
-        @Override
-        public void close() throws IOException {
-            closed = true;
-            super.close();
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/snapshots/blobstore/SlicedInputStreamTests.java b/core/src/test/java/org/elasticsearch/index/snapshots/blobstore/SlicedInputStreamTests.java
new file mode 100644
index 0000000..e9deadb
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/snapshots/blobstore/SlicedInputStreamTests.java
@@ -0,0 +1,133 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.index.snapshots.blobstore;
+
+import com.carrotsearch.randomizedtesting.generators.RandomInts;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.io.*;
+import java.util.Random;
+
+import static org.hamcrest.Matchers.equalTo;
+
+public class SlicedInputStreamTests extends ESTestCase {
+
+    @Test
+    public void readRandom() throws IOException {
+        int parts = randomIntBetween(1, 20);
+        ByteArrayOutputStream stream = new ByteArrayOutputStream();
+        int numWriteOps = scaledRandomIntBetween(1000, 10000);
+        final long seed = randomLong();
+        Random random = new Random(seed);
+        for (int i = 0; i < numWriteOps; i++) {
+            switch(random.nextInt(5)) {
+                case 1:
+                    stream.write(random.nextInt(Byte.MAX_VALUE));
+                    break;
+                default:
+                    stream.write(randomBytes(random));
+                    break;
+            }
+        }
+
+        final CheckClosedInputStream[] streams = new CheckClosedInputStream[parts];
+        byte[] bytes = stream.toByteArray();
+        int slice = bytes.length / parts;
+        int offset = 0;
+        int length;
+        for (int i = 0; i < parts; i++) {
+            length = i == parts-1 ? bytes.length-offset : slice;
+            streams[i] = new CheckClosedInputStream(new ByteArrayInputStream(bytes, offset, length));
+            offset += length;
+        }
+
+        SlicedInputStream input = new SlicedInputStream(parts) {
+
+            @Override
+            protected InputStream openSlice(long slice) throws IOException {
+                return streams[(int)slice];
+            }
+        };
+        random = new Random(seed);
+        assertThat(input.available(), equalTo(streams[0].available()));
+        for (int i = 0; i < numWriteOps; i++) {
+            switch(random.nextInt(5)) {
+                case 1:
+                    assertThat(random.nextInt(Byte.MAX_VALUE), equalTo(input.read()));
+                    break;
+                default:
+                    byte[] b = randomBytes(random);
+                    byte[] buffer = new byte[b.length];
+                    int read = readFully(input, buffer);
+                    assertThat(b.length, equalTo(read));
+                    assertArrayEquals(b, buffer);
+                    break;
+            }
+        }
+
+        assertThat(input.available(), equalTo(0));
+        for (int i =0; i < streams.length-1; i++) {
+            assertTrue(streams[i].closed);
+        }
+        input.close();
+
+        for (int i =0; i < streams.length; i++) {
+            assertTrue(streams[i].closed);
+        }
+
+    }
+
+    private int readFully(InputStream stream, byte[] buffer) throws IOException {
+        for (int i = 0; i < buffer.length;) {
+            int read = stream.read(buffer, i, buffer.length-i);
+            if (read == -1) {
+              if (i == 0) {
+                  return -1;
+              } else {
+                  return i;
+              }
+            }
+            i+= read;
+        }
+        return buffer.length;
+    }
+
+    private byte[] randomBytes(Random random) {
+        int length = RandomInts.randomIntBetween(random, 1, 10);
+        byte[] data = new byte[length];
+        random.nextBytes(data);
+        return data;
+    }
+
+    private static final class CheckClosedInputStream extends FilterInputStream {
+
+        public boolean closed = false;
+
+        public CheckClosedInputStream(InputStream in) {
+            super(in);
+        }
+
+        @Override
+        public void close() throws IOException {
+            closed = true;
+            super.close();
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/index/store/DirectoryUtilsTest.java b/core/src/test/java/org/elasticsearch/index/store/DirectoryUtilsTest.java
deleted file mode 100644
index 3cfdaa2..0000000
--- a/core/src/test/java/org/elasticsearch/index/store/DirectoryUtilsTest.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.store;
-
-import org.apache.lucene.store.*;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.nio.file.Path;
-import java.util.Collections;
-import java.util.Set;
-
-import static org.hamcrest.CoreMatchers.*;
-
-public class DirectoryUtilsTest extends ESTestCase {
-
-    @Test
-    public void testGetLeave() throws IOException {
-        Path file = createTempDir();
-        final int iters = scaledRandomIntBetween(10, 100);
-        for (int i = 0; i < iters; i++) {
-            {
-                BaseDirectoryWrapper dir = newFSDirectory(file);
-                FSDirectory directory = DirectoryUtils.getLeaf(new FilterDirectory(dir) {}, FSDirectory.class, null);
-                assertThat(directory, notNullValue());
-                assertThat(directory, sameInstance(DirectoryUtils.getLeafDirectory(dir, null)));
-                dir.close();
-            }
-
-            {
-                BaseDirectoryWrapper dir = newFSDirectory(file);
-                FSDirectory directory = DirectoryUtils.getLeaf(dir, FSDirectory.class, null);
-                assertThat(directory, notNullValue());
-                assertThat(directory, sameInstance(DirectoryUtils.getLeafDirectory(dir, null)));
-                dir.close();
-            }
-
-            {
-                Set<String> stringSet = Collections.emptySet();
-                BaseDirectoryWrapper dir = newFSDirectory(file);
-                FSDirectory directory = DirectoryUtils.getLeaf(new FileSwitchDirectory(stringSet, dir, dir, random().nextBoolean()), FSDirectory.class, null);
-                assertThat(directory, notNullValue());
-                assertThat(directory, sameInstance(DirectoryUtils.getLeafDirectory(dir, null)));
-                dir.close();
-            }
-
-            {
-                Set<String> stringSet = Collections.emptySet();
-                BaseDirectoryWrapper dir = newFSDirectory(file);
-                FSDirectory directory = DirectoryUtils.getLeaf(new FilterDirectory(new FileSwitchDirectory(stringSet, dir, dir, random().nextBoolean())) {}, FSDirectory.class, null);
-                assertThat(directory, notNullValue());
-                assertThat(directory, sameInstance(DirectoryUtils.getLeafDirectory(dir, null)));
-                dir.close();
-            }
-
-            {
-                Set<String> stringSet = Collections.emptySet();
-                BaseDirectoryWrapper dir = newFSDirectory(file);
-                RAMDirectory directory = DirectoryUtils.getLeaf(new FilterDirectory(new FileSwitchDirectory(stringSet, dir, dir, random().nextBoolean())) {}, RAMDirectory.class, null);
-                assertThat(directory, nullValue());
-                dir.close();
-            }
-
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/store/DirectoryUtilsTests.java b/core/src/test/java/org/elasticsearch/index/store/DirectoryUtilsTests.java
new file mode 100644
index 0000000..04ae467
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/store/DirectoryUtilsTests.java
@@ -0,0 +1,83 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.index.store;
+
+import org.apache.lucene.store.*;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.nio.file.Path;
+import java.util.Collections;
+import java.util.Set;
+
+import static org.hamcrest.CoreMatchers.*;
+
+public class DirectoryUtilsTests extends ESTestCase {
+
+    @Test
+    public void testGetLeave() throws IOException {
+        Path file = createTempDir();
+        final int iters = scaledRandomIntBetween(10, 100);
+        for (int i = 0; i < iters; i++) {
+            {
+                BaseDirectoryWrapper dir = newFSDirectory(file);
+                FSDirectory directory = DirectoryUtils.getLeaf(new FilterDirectory(dir) {}, FSDirectory.class, null);
+                assertThat(directory, notNullValue());
+                assertThat(directory, sameInstance(DirectoryUtils.getLeafDirectory(dir, null)));
+                dir.close();
+            }
+
+            {
+                BaseDirectoryWrapper dir = newFSDirectory(file);
+                FSDirectory directory = DirectoryUtils.getLeaf(dir, FSDirectory.class, null);
+                assertThat(directory, notNullValue());
+                assertThat(directory, sameInstance(DirectoryUtils.getLeafDirectory(dir, null)));
+                dir.close();
+            }
+
+            {
+                Set<String> stringSet = Collections.emptySet();
+                BaseDirectoryWrapper dir = newFSDirectory(file);
+                FSDirectory directory = DirectoryUtils.getLeaf(new FileSwitchDirectory(stringSet, dir, dir, random().nextBoolean()), FSDirectory.class, null);
+                assertThat(directory, notNullValue());
+                assertThat(directory, sameInstance(DirectoryUtils.getLeafDirectory(dir, null)));
+                dir.close();
+            }
+
+            {
+                Set<String> stringSet = Collections.emptySet();
+                BaseDirectoryWrapper dir = newFSDirectory(file);
+                FSDirectory directory = DirectoryUtils.getLeaf(new FilterDirectory(new FileSwitchDirectory(stringSet, dir, dir, random().nextBoolean())) {}, FSDirectory.class, null);
+                assertThat(directory, notNullValue());
+                assertThat(directory, sameInstance(DirectoryUtils.getLeafDirectory(dir, null)));
+                dir.close();
+            }
+
+            {
+                Set<String> stringSet = Collections.emptySet();
+                BaseDirectoryWrapper dir = newFSDirectory(file);
+                RAMDirectory directory = DirectoryUtils.getLeaf(new FilterDirectory(new FileSwitchDirectory(stringSet, dir, dir, random().nextBoolean())) {}, RAMDirectory.class, null);
+                assertThat(directory, nullValue());
+                dir.close();
+            }
+
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/index/store/IndexStoreBWCTest.java b/core/src/test/java/org/elasticsearch/index/store/IndexStoreBWCTest.java
deleted file mode 100644
index 7bb1a92..0000000
--- a/core/src/test/java/org/elasticsearch/index/store/IndexStoreBWCTest.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.store;
-
-import com.carrotsearch.randomizedtesting.generators.RandomPicks;
-import org.apache.lucene.store.*;
-import org.apache.lucene.util.Constants;
-import org.elasticsearch.Version;
-import org.elasticsearch.action.admin.indices.settings.get.GetSettingsResponse;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.index.shard.ShardPath;
-import org.elasticsearch.test.ESSingleNodeTestCase;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-import java.nio.file.Path;
-import java.util.Arrays;
-import java.util.Locale;
-
-/**
- */
-public class IndexStoreBWCTest extends ESSingleNodeTestCase {
-
-
-    public void testOldCoreTypesFail() {
-        try {
-            createIndex("test", Settings.builder().put(IndexStoreModule.STORE_TYPE, "nio_fs").build());
-            fail();
-        } catch (Exception ex) {
-        }
-        try {
-            createIndex("test", Settings.builder().put(IndexStoreModule.STORE_TYPE, "mmap_fs").build());
-            fail();
-        } catch (Exception ex) {
-        }
-        try {
-            createIndex("test", Settings.builder().put(IndexStoreModule.STORE_TYPE, "simple_fs").build());
-            fail();
-        } catch (Exception ex) {
-        }
-    }
-
-    public void testUpgradeCoreTypes() throws IOException {
-        String type = RandomPicks.randomFrom(random(), Arrays.asList("nio", "mmap", "simple"));
-        createIndex("test", Settings.builder()
-                .put(IndexStoreModule.STORE_TYPE, type+"fs")
-                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.V_1_7_0)
-                .build());
-
-        client().admin().indices().prepareClose("test").get();
-        client().admin().indices().prepareUpdateSettings("test").setSettings(Settings.builder()
-                .put(IndexStoreModule.STORE_TYPE, type + "_fs").build()).get();
-        GetSettingsResponse getSettingsResponse = client().admin().indices().prepareGetSettings("test").get();
-        String actualType = getSettingsResponse.getSetting("test", IndexStoreModule.STORE_TYPE);
-        assertEquals(type + "_fs", actualType);
-
-        // now reopen and upgrade
-        client().admin().indices().prepareOpen("test").get();
-
-        getSettingsResponse = client().admin().indices().prepareGetSettings("test").get();
-        actualType = getSettingsResponse.getSetting("test", IndexStoreModule.STORE_TYPE);
-        assertEquals(type+"fs", actualType);
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/index/store/IndexStoreBWCTests.java b/core/src/test/java/org/elasticsearch/index/store/IndexStoreBWCTests.java
new file mode 100644
index 0000000..e53358c
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/store/IndexStoreBWCTests.java
@@ -0,0 +1,83 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.index.store;
+
+import com.carrotsearch.randomizedtesting.generators.RandomPicks;
+import org.apache.lucene.store.*;
+import org.apache.lucene.util.Constants;
+import org.elasticsearch.Version;
+import org.elasticsearch.action.admin.indices.settings.get.GetSettingsResponse;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.index.shard.ShardPath;
+import org.elasticsearch.test.ESSingleNodeTestCase;
+import org.elasticsearch.test.ESTestCase;
+
+import java.io.IOException;
+import java.nio.file.Path;
+import java.util.Arrays;
+import java.util.Locale;
+
+/**
+ */
+public class IndexStoreBWCTests extends ESSingleNodeTestCase {
+
+
+    public void testOldCoreTypesFail() {
+        try {
+            createIndex("test", Settings.builder().put(IndexStoreModule.STORE_TYPE, "nio_fs").build());
+            fail();
+        } catch (Exception ex) {
+        }
+        try {
+            createIndex("test", Settings.builder().put(IndexStoreModule.STORE_TYPE, "mmap_fs").build());
+            fail();
+        } catch (Exception ex) {
+        }
+        try {
+            createIndex("test", Settings.builder().put(IndexStoreModule.STORE_TYPE, "simple_fs").build());
+            fail();
+        } catch (Exception ex) {
+        }
+    }
+
+    public void testUpgradeCoreTypes() throws IOException {
+        String type = RandomPicks.randomFrom(random(), Arrays.asList("nio", "mmap", "simple"));
+        createIndex("test", Settings.builder()
+                .put(IndexStoreModule.STORE_TYPE, type+"fs")
+                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.V_1_7_0)
+                .build());
+
+        client().admin().indices().prepareClose("test").get();
+        client().admin().indices().prepareUpdateSettings("test").setSettings(Settings.builder()
+                .put(IndexStoreModule.STORE_TYPE, type + "_fs").build()).get();
+        GetSettingsResponse getSettingsResponse = client().admin().indices().prepareGetSettings("test").get();
+        String actualType = getSettingsResponse.getSetting("test", IndexStoreModule.STORE_TYPE);
+        assertEquals(type + "_fs", actualType);
+
+        // now reopen and upgrade
+        client().admin().indices().prepareOpen("test").get();
+
+        getSettingsResponse = client().admin().indices().prepareGetSettings("test").get();
+        actualType = getSettingsResponse.getSetting("test", IndexStoreModule.STORE_TYPE);
+        assertEquals(type+"fs", actualType);
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/index/store/StoreTest.java b/core/src/test/java/org/elasticsearch/index/store/StoreTest.java
deleted file mode 100644
index 5757764..0000000
--- a/core/src/test/java/org/elasticsearch/index/store/StoreTest.java
+++ /dev/null
@@ -1,1270 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.store;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.FilterCodec;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.lucene50.Lucene50SegmentInfoFormat;
-import org.apache.lucene.codecs.lucene53.Lucene53Codec;
-import org.apache.lucene.document.*;
-import org.apache.lucene.index.*;
-import org.apache.lucene.store.*;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.TestUtil;
-import org.apache.lucene.util.Version;
-import org.elasticsearch.ExceptionsHelper;
-import org.elasticsearch.common.io.stream.InputStreamStreamInput;
-import org.elasticsearch.common.io.stream.OutputStreamStreamOutput;
-import org.elasticsearch.common.lucene.Lucene;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.env.ShardLock;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.deletionpolicy.KeepOnlyLastDeletionPolicy;
-import org.elasticsearch.index.deletionpolicy.SnapshotDeletionPolicy;
-import org.elasticsearch.index.engine.Engine;
-import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.index.translog.Translog;
-import org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData;
-import org.elasticsearch.test.DummyShardLock;
-import org.elasticsearch.test.ESTestCase;
-import org.hamcrest.Matchers;
-import org.junit.Test;
-
-import java.io.ByteArrayInputStream;
-import java.io.ByteArrayOutputStream;
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.nio.file.NoSuchFileException;
-import java.nio.file.Path;
-import java.util.*;
-import java.util.concurrent.atomic.AtomicBoolean;
-import java.util.concurrent.atomic.AtomicInteger;
-import java.util.zip.Adler32;
-
-import static org.elasticsearch.common.settings.Settings.Builder.EMPTY_SETTINGS;
-import static org.elasticsearch.test.VersionUtils.randomVersion;
-import static org.hamcrest.Matchers.*;
-
-public class StoreTest extends ESTestCase {
-
-    @Test
-    public void testRefCount() throws IOException {
-        final ShardId shardId = new ShardId(new Index("index"), 1);
-        DirectoryService directoryService = new LuceneManagedDirectoryService(random());
-        Store store = new Store(shardId, Settings.EMPTY, directoryService, new DummyShardLock(shardId));
-        int incs = randomIntBetween(1, 100);
-        for (int i = 0; i < incs; i++) {
-            if (randomBoolean()) {
-                store.incRef();
-            } else {
-                assertTrue(store.tryIncRef());
-            }
-            store.ensureOpen();
-        }
-
-        for (int i = 0; i < incs; i++) {
-            store.decRef();
-            store.ensureOpen();
-        }
-
-        store.incRef();
-        final AtomicBoolean called = new AtomicBoolean(false);
-        store.close();
-        for (int i = 0; i < incs; i++) {
-            if (randomBoolean()) {
-                store.incRef();
-            } else {
-                assertTrue(store.tryIncRef());
-            }
-            store.ensureOpen();
-        }
-
-        for (int i = 0; i < incs; i++) {
-            store.decRef();
-            store.ensureOpen();
-        }
-
-        store.decRef();
-        assertThat(store.refCount(), Matchers.equalTo(0));
-        assertFalse(store.tryIncRef());
-        try {
-            store.incRef();
-            fail(" expected exception");
-        } catch (AlreadyClosedException ex) {
-
-        }
-        try {
-            store.ensureOpen();
-            fail(" expected exception");
-        } catch (AlreadyClosedException ex) {
-
-        }
-    }
-
-    @Test
-    public void testVerifyingIndexOutput() throws IOException {
-        Directory dir = newDirectory();
-        IndexOutput output = dir.createOutput("foo.bar", IOContext.DEFAULT);
-        int iters = scaledRandomIntBetween(10, 100);
-        for (int i = 0; i < iters; i++) {
-            BytesRef bytesRef = new BytesRef(TestUtil.randomRealisticUnicodeString(random(), 10, 1024));
-            output.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
-        }
-        CodecUtil.writeFooter(output);
-        output.close();
-        IndexInput indexInput = dir.openInput("foo.bar", IOContext.DEFAULT);
-        String checksum = Store.digestToString(CodecUtil.retrieveChecksum(indexInput));
-        indexInput.seek(0);
-        BytesRef ref = new BytesRef(scaledRandomIntBetween(1, 1024));
-        long length = indexInput.length();
-        IndexOutput verifyingOutput = new Store.LuceneVerifyingIndexOutput(new StoreFileMetaData("foo1.bar", length, checksum), dir.createOutput("foo1.bar", IOContext.DEFAULT));
-        while (length > 0) {
-            if (random().nextInt(10) == 0) {
-                verifyingOutput.writeByte(indexInput.readByte());
-                length--;
-            } else {
-                int min = (int) Math.min(length, ref.bytes.length);
-                indexInput.readBytes(ref.bytes, ref.offset, min);
-                verifyingOutput.writeBytes(ref.bytes, ref.offset, min);
-                length -= min;
-            }
-        }
-        Store.verify(verifyingOutput);
-        verifyingOutput.writeByte((byte) 0x0);
-        try {
-            Store.verify(verifyingOutput);
-            fail("should be a corrupted index");
-        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {
-            // ok
-        }
-        IOUtils.close(indexInput, verifyingOutput, dir);
-    }
-
-    @Test
-    public void testVerifyingIndexOutputWithBogusInput() throws IOException {
-        Directory dir = newDirectory();
-        int length = scaledRandomIntBetween(10, 1024);
-        IndexOutput verifyingOutput = new Store.LuceneVerifyingIndexOutput(new StoreFileMetaData("foo1.bar", length, ""), dir.createOutput("foo1.bar", IOContext.DEFAULT));
-        try {
-            while (length > 0) {
-                verifyingOutput.writeByte((byte) random().nextInt());
-                length--;
-            }
-            fail("should be a corrupted index");
-        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {
-            // ok
-        }
-        IOUtils.close(verifyingOutput, dir);
-    }
-
-    // TODO: remove this, its too fragile. just use a static old index instead.
-    private static final class OldSIMockingCodec extends FilterCodec {
-
-        protected OldSIMockingCodec() {
-            super(new Lucene53Codec().getName(), new Lucene53Codec());
-        }
-
-        @Override
-        public SegmentInfoFormat segmentInfoFormat() {
-            final SegmentInfoFormat segmentInfoFormat = super.segmentInfoFormat();
-            return new SegmentInfoFormat() {
-                @Override
-                public SegmentInfo read(Directory directory, String segmentName, byte[] segmentID, IOContext context) throws IOException {
-                    return segmentInfoFormat.read(directory, segmentName, segmentID, context);
-                }
-
-                // this sucks it's a full copy of Lucene50SegmentInfoFormat but hey I couldn't find a way to make it write 4_5_0 versions
-                // somebody was too paranoid when implementing this. ey rmuir, was that you? - go fix it :P
-                @Override
-                public void write(Directory dir, SegmentInfo si, IOContext ioContext) throws IOException {
-                    final String fileName = IndexFileNames.segmentFileName(si.name, "", Lucene50SegmentInfoFormat.SI_EXTENSION);
-                    si.addFile(fileName);
-
-                    boolean success = false;
-                    try (IndexOutput output = dir.createOutput(fileName, ioContext)) {
-                        CodecUtil.writeIndexHeader(output,
-                                "Lucene50SegmentInfo",
-                                0,
-                                si.getId(),
-                                "");
-                        Version version = Version.LUCENE_4_5_0; // FOOOOOO!!
-                        // Write the Lucene version that created this segment, since 3.1
-                        output.writeInt(version.major);
-                        output.writeInt(version.minor);
-                        output.writeInt(version.bugfix);
-                        assert version.prerelease == 0;
-                        output.writeInt(si.maxDoc());
-
-                        output.writeByte((byte) (si.getUseCompoundFile() ? SegmentInfo.YES : SegmentInfo.NO));
-                        output.writeStringStringMap(si.getDiagnostics());
-                        Set<String> files = si.files();
-                        for (String file : files) {
-                            if (!IndexFileNames.parseSegmentName(file).equals(si.name)) {
-                                throw new IllegalArgumentException("invalid files: expected segment=" + si.name + ", got=" + files);
-                            }
-                        }
-                        output.writeStringSet(files);
-                        output.writeStringStringMap(si.getAttributes());
-                        CodecUtil.writeFooter(output);
-                        success = true;
-                    } finally {
-                        if (!success) {
-                            // TODO: are we doing this outside of the tracking wrapper? why must SIWriter cleanup like this?
-                            IOUtils.deleteFilesIgnoringExceptions(si.dir, fileName);
-                        }
-                    }
-                }
-            };
-        }
-    }
-
-    // IF THIS TEST FAILS ON UPGRADE GO LOOK AT THE OldSIMockingCodec!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
-    @AwaitsFix(bugUrl="Fails with seed E1394B038144F6E")
-    // The test currently fails because the segment infos and the index don't
-    // agree on the oldest version of a segment. We should fix this test by
-    // switching to a static bw index
-    @Test
-    public void testWriteLegacyChecksums() throws IOException {
-        final ShardId shardId = new ShardId(new Index("index"), 1);
-        DirectoryService directoryService = new LuceneManagedDirectoryService(random());
-        Store store = new Store(shardId, Settings.EMPTY, directoryService, new DummyShardLock(shardId));
-        // set default codec - all segments need checksums
-        final boolean usesOldCodec = randomBoolean();
-        IndexWriter writer = new IndexWriter(store.directory(), newIndexWriterConfig(random(), new MockAnalyzer(random())).setCodec(usesOldCodec ? new OldSIMockingCodec() : TestUtil.getDefaultCodec()));
-        int docs = 1 + random().nextInt(100);
-
-        for (int i = 0; i < docs; i++) {
-            Document doc = new Document();
-            doc.add(new TextField("id", "" + i, random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-            doc.add(new TextField("body", TestUtil.randomRealisticUnicodeString(random()), random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-            doc.add(new SortedDocValuesField("dv", new BytesRef(TestUtil.randomRealisticUnicodeString(random()))));
-            writer.addDocument(doc);
-        }
-        if (random().nextBoolean()) {
-            for (int i = 0; i < docs; i++) {
-                if (random().nextBoolean()) {
-                    Document doc = new Document();
-                    doc.add(new TextField("id", "" + i, random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-                    doc.add(new TextField("body", TestUtil.randomRealisticUnicodeString(random()), random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-                    writer.updateDocument(new Term("id", "" + i), doc);
-                }
-            }
-        }
-        if (random().nextBoolean()) {
-            DirectoryReader.open(writer, random().nextBoolean()).close(); // flush
-        }
-        Store.MetadataSnapshot metadata;
-        // check before we committed
-        try {
-            store.getMetadata();
-            fail("no index present - expected exception");
-        } catch (IndexNotFoundException ex) {
-            // expected
-        }
-        assertThat(store.getMetadataOrEmpty(), is(Store.MetadataSnapshot.EMPTY)); // nothing committed
-
-        writer.close();
-        Store.LegacyChecksums checksums = new Store.LegacyChecksums();
-        Map<String, StoreFileMetaData> legacyMeta = new HashMap<>();
-        for (String file : store.directory().listAll()) {
-            if (file.equals("write.lock") || file.equals(IndexFileNames.OLD_SEGMENTS_GEN) || file.startsWith("extra")) {
-                continue;
-            }
-            BytesRef hash = new BytesRef();
-            if (file.startsWith("segments")) {
-                hash = Store.MetadataSnapshot.hashFile(store.directory(), file);
-            }
-            StoreFileMetaData storeFileMetaData = new StoreFileMetaData(file, store.directory().fileLength(file), file + "checksum", null, hash);
-            legacyMeta.put(file, storeFileMetaData);
-            checksums.add(storeFileMetaData);
-        }
-        checksums.write(store);
-
-        metadata = store.getMetadata();
-        Map<String, StoreFileMetaData> stringStoreFileMetaDataMap = metadata.asMap();
-        assertThat(legacyMeta.size(), equalTo(stringStoreFileMetaDataMap.size()));
-        if (usesOldCodec) {
-            for (StoreFileMetaData meta : legacyMeta.values()) {
-                assertTrue(meta.toString(), stringStoreFileMetaDataMap.containsKey(meta.name()));
-                assertEquals(meta.name() + "checksum", meta.checksum());
-                assertTrue(meta + " vs. " + stringStoreFileMetaDataMap.get(meta.name()), stringStoreFileMetaDataMap.get(meta.name()).isSame(meta));
-            }
-        } else {
-
-            // even if we have a legacy checksum - if we use a new codec we should reuse
-            for (StoreFileMetaData meta : legacyMeta.values()) {
-                assertTrue(meta.toString(), stringStoreFileMetaDataMap.containsKey(meta.name()));
-                assertFalse(meta + " vs. " + stringStoreFileMetaDataMap.get(meta.name()), stringStoreFileMetaDataMap.get(meta.name()).isSame(meta));
-                StoreFileMetaData storeFileMetaData = metadata.get(meta.name());
-                try (IndexInput input = store.openVerifyingInput(meta.name(), IOContext.DEFAULT, storeFileMetaData)) {
-                    assertTrue(storeFileMetaData.toString(), input instanceof Store.VerifyingIndexInput);
-                    input.seek(meta.length());
-                    Store.verify(input);
-                }
-            }
-        }
-        assertDeleteContent(store, directoryService);
-        IOUtils.close(store);
-
-    }
-
-    @Test
-    public void testNewChecksums() throws IOException {
-        final ShardId shardId = new ShardId(new Index("index"), 1);
-        DirectoryService directoryService = new LuceneManagedDirectoryService(random());
-        Store store = new Store(shardId, Settings.EMPTY, directoryService, new DummyShardLock(shardId));
-        // set default codec - all segments need checksums
-        IndexWriter writer = new IndexWriter(store.directory(), newIndexWriterConfig(random(), new MockAnalyzer(random())).setCodec(TestUtil.getDefaultCodec()));
-        int docs = 1 + random().nextInt(100);
-
-        for (int i = 0; i < docs; i++) {
-            Document doc = new Document();
-            doc.add(new TextField("id", "" + i, random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-            doc.add(new TextField("body", TestUtil.randomRealisticUnicodeString(random()), random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-            doc.add(new SortedDocValuesField("dv", new BytesRef(TestUtil.randomRealisticUnicodeString(random()))));
-            writer.addDocument(doc);
-        }
-        if (random().nextBoolean()) {
-            for (int i = 0; i < docs; i++) {
-                if (random().nextBoolean()) {
-                    Document doc = new Document();
-                    doc.add(new TextField("id", "" + i, random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-                    doc.add(new TextField("body", TestUtil.randomRealisticUnicodeString(random()), random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-                    writer.updateDocument(new Term("id", "" + i), doc);
-                }
-            }
-        }
-        if (random().nextBoolean()) {
-            DirectoryReader.open(writer, random().nextBoolean()).close(); // flush
-        }
-        Store.MetadataSnapshot metadata;
-        // check before we committed
-        try {
-            store.getMetadata();
-            fail("no index present - expected exception");
-        } catch (IndexNotFoundException ex) {
-            // expected
-        }
-        assertThat(store.getMetadataOrEmpty(), is(Store.MetadataSnapshot.EMPTY)); // nothing committed
-        writer.commit();
-        writer.close();
-        metadata = store.getMetadata();
-        assertThat(metadata.asMap().isEmpty(), is(false));
-        for (StoreFileMetaData meta : metadata) {
-            try (IndexInput input = store.directory().openInput(meta.name(), IOContext.DEFAULT)) {
-                String checksum = Store.digestToString(CodecUtil.retrieveChecksum(input));
-                assertThat("File: " + meta.name() + " has a different checksum", meta.checksum(), equalTo(checksum));
-                assertThat(meta.hasLegacyChecksum(), equalTo(false));
-                assertThat(meta.writtenBy(), equalTo(Version.LATEST));
-                if (meta.name().endsWith(".si") || meta.name().startsWith("segments_")) {
-                    assertThat(meta.hash().length, greaterThan(0));
-                }
-            }
-        }
-        assertConsistent(store, metadata);
-
-        TestUtil.checkIndex(store.directory());
-        assertDeleteContent(store, directoryService);
-        IOUtils.close(store);
-    }
-
-    @Test
-    public void testMixedChecksums() throws IOException {
-        final ShardId shardId = new ShardId(new Index("index"), 1);
-        DirectoryService directoryService = new LuceneManagedDirectoryService(random());
-        Store store = new Store(shardId, Settings.EMPTY, directoryService, new DummyShardLock(shardId));
-        // this time random codec....
-        IndexWriter writer = new IndexWriter(store.directory(), newIndexWriterConfig(random(), new MockAnalyzer(random())).setCodec(TestUtil.getDefaultCodec()));
-        int docs = 1 + random().nextInt(100);
-
-        for (int i = 0; i < docs; i++) {
-            Document doc = new Document();
-            doc.add(new TextField("id", "" + i, random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-            doc.add(new TextField("body", TestUtil.randomRealisticUnicodeString(random()), random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-            doc.add(new SortedDocValuesField("dv", new BytesRef(TestUtil.randomRealisticUnicodeString(random()))));
-            writer.addDocument(doc);
-        }
-        if (random().nextBoolean()) {
-            for (int i = 0; i < docs; i++) {
-                if (random().nextBoolean()) {
-                    Document doc = new Document();
-                    doc.add(new TextField("id", "" + i, random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-                    doc.add(new TextField("body", TestUtil.randomRealisticUnicodeString(random()), random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-                    writer.updateDocument(new Term("id", "" + i), doc);
-                }
-            }
-        }
-        if (random().nextBoolean()) {
-            DirectoryReader.open(writer, random().nextBoolean()).close(); // flush
-        }
-        Store.MetadataSnapshot metadata;
-        // check before we committed
-        try {
-            store.getMetadata();
-            fail("no index present - expected exception");
-        } catch (IndexNotFoundException ex) {
-            // expected
-        }
-        assertThat(store.getMetadataOrEmpty(), is(Store.MetadataSnapshot.EMPTY)); // nothing committed
-        writer.commit();
-        writer.close();
-        Store.LegacyChecksums checksums = new Store.LegacyChecksums();
-        metadata = store.getMetadata();
-        assertThat(metadata.asMap().isEmpty(), is(false));
-        for (StoreFileMetaData meta : metadata) {
-            try (IndexInput input = store.directory().openInput(meta.name(), IOContext.DEFAULT)) {
-                if (meta.checksum() == null) {
-                    String checksum = null;
-                    try {
-                        CodecUtil.retrieveChecksum(input);
-                        fail("expected a corrupt index - posting format has not checksums");
-                    } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {
-                        try (ChecksumIndexInput checksumIndexInput = store.directory().openChecksumInput(meta.name(), IOContext.DEFAULT)) {
-                            checksumIndexInput.seek(meta.length());
-                            checksum = Store.digestToString(checksumIndexInput.getChecksum());
-                        }
-                        // fine - it's a postings format without checksums
-                        checksums.add(new StoreFileMetaData(meta.name(), meta.length(), checksum, null));
-                    }
-                } else {
-                    String checksum = Store.digestToString(CodecUtil.retrieveChecksum(input));
-                    assertThat("File: " + meta.name() + " has a different checksum", meta.checksum(), equalTo(checksum));
-                    assertThat(meta.hasLegacyChecksum(), equalTo(false));
-                    assertThat(meta.writtenBy(), equalTo(Version.LATEST));
-                }
-            }
-        }
-        assertConsistent(store, metadata);
-        checksums.write(store);
-        metadata = store.getMetadata();
-        assertThat(metadata.asMap().isEmpty(), is(false));
-        for (StoreFileMetaData meta : metadata) {
-            assertThat("file: " + meta.name() + " has a null checksum", meta.checksum(), not(nullValue()));
-            if (meta.hasLegacyChecksum()) {
-                try (ChecksumIndexInput checksumIndexInput = store.directory().openChecksumInput(meta.name(), IOContext.DEFAULT)) {
-                    checksumIndexInput.seek(meta.length());
-                    assertThat(meta.checksum(), equalTo(Store.digestToString(checksumIndexInput.getChecksum())));
-                }
-            } else {
-                try (IndexInput input = store.directory().openInput(meta.name(), IOContext.DEFAULT)) {
-                    String checksum = Store.digestToString(CodecUtil.retrieveChecksum(input));
-                    assertThat("File: " + meta.name() + " has a different checksum", meta.checksum(), equalTo(checksum));
-                    assertThat(meta.hasLegacyChecksum(), equalTo(false));
-                    assertThat(meta.writtenBy(), equalTo(Version.LATEST));
-                }
-            }
-        }
-        assertConsistent(store, metadata);
-        TestUtil.checkIndex(store.directory());
-        assertDeleteContent(store, directoryService);
-        IOUtils.close(store);
-    }
-
-    @Test
-    public void testRenameFile() throws IOException {
-        final ShardId shardId = new ShardId(new Index("index"), 1);
-        DirectoryService directoryService = new LuceneManagedDirectoryService(random(), false);
-        Store store = new Store(shardId, Settings.EMPTY, directoryService, new DummyShardLock(shardId));
-        {
-            IndexOutput output = store.directory().createOutput("foo.bar", IOContext.DEFAULT);
-            int iters = scaledRandomIntBetween(10, 100);
-            for (int i = 0; i < iters; i++) {
-                BytesRef bytesRef = new BytesRef(TestUtil.randomRealisticUnicodeString(random(), 10, 1024));
-                output.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
-            }
-            CodecUtil.writeFooter(output);
-            output.close();
-        }
-        store.renameFile("foo.bar", "bar.foo");
-        assertThat(numNonExtraFiles(store), is(1));
-        final long lastChecksum;
-        try (IndexInput input = store.directory().openInput("bar.foo", IOContext.DEFAULT)) {
-            lastChecksum = CodecUtil.checksumEntireFile(input);
-        }
-
-        try {
-            store.directory().openInput("foo.bar", IOContext.DEFAULT);
-            fail("file was renamed");
-        } catch (FileNotFoundException | NoSuchFileException ex) {
-            // expected
-        }
-        {
-            IndexOutput output = store.directory().createOutput("foo.bar", IOContext.DEFAULT);
-            int iters = scaledRandomIntBetween(10, 100);
-            for (int i = 0; i < iters; i++) {
-                BytesRef bytesRef = new BytesRef(TestUtil.randomRealisticUnicodeString(random(), 10, 1024));
-                output.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
-            }
-            CodecUtil.writeFooter(output);
-            output.close();
-        }
-        store.renameFile("foo.bar", "bar.foo");
-        assertThat(numNonExtraFiles(store), is(1));
-        assertDeleteContent(store, directoryService);
-        IOUtils.close(store);
-    }
-
-    public void testCheckIntegrity() throws IOException {
-        Directory dir = newDirectory();
-        long luceneFileLength = 0;
-
-        try (IndexOutput output = dir.createOutput("lucene_checksum.bin", IOContext.DEFAULT)) {
-            int iters = scaledRandomIntBetween(10, 100);
-            for (int i = 0; i < iters; i++) {
-                BytesRef bytesRef = new BytesRef(TestUtil.randomRealisticUnicodeString(random(), 10, 1024));
-                output.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
-                luceneFileLength += bytesRef.length;
-            }
-            CodecUtil.writeFooter(output);
-            luceneFileLength += CodecUtil.footerLength();
-
-        }
-
-        final Adler32 adler32 = new Adler32();
-        long legacyFileLength = 0;
-        try (IndexOutput output = dir.createOutput("legacy.bin", IOContext.DEFAULT)) {
-            int iters = scaledRandomIntBetween(10, 100);
-            for (int i = 0; i < iters; i++) {
-                BytesRef bytesRef = new BytesRef(TestUtil.randomRealisticUnicodeString(random(), 10, 1024));
-                output.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
-                adler32.update(bytesRef.bytes, bytesRef.offset, bytesRef.length);
-                legacyFileLength += bytesRef.length;
-            }
-        }
-        final long luceneChecksum;
-        final long adler32LegacyChecksum = adler32.getValue();
-        try (IndexInput indexInput = dir.openInput("lucene_checksum.bin", IOContext.DEFAULT)) {
-            assertEquals(luceneFileLength, indexInput.length());
-            luceneChecksum = CodecUtil.retrieveChecksum(indexInput);
-        }
-
-        { // positive check
-            StoreFileMetaData lucene = new StoreFileMetaData("lucene_checksum.bin", luceneFileLength, Store.digestToString(luceneChecksum), Version.LUCENE_4_8_0);
-            StoreFileMetaData legacy = new StoreFileMetaData("legacy.bin", legacyFileLength, Store.digestToString(adler32LegacyChecksum));
-            assertTrue(legacy.hasLegacyChecksum());
-            assertFalse(lucene.hasLegacyChecksum());
-            assertTrue(Store.checkIntegrityNoException(lucene, dir));
-            assertTrue(Store.checkIntegrityNoException(legacy, dir));
-        }
-
-        { // negative check - wrong checksum
-            StoreFileMetaData lucene = new StoreFileMetaData("lucene_checksum.bin", luceneFileLength, Store.digestToString(luceneChecksum + 1), Version.LUCENE_4_8_0);
-            StoreFileMetaData legacy = new StoreFileMetaData("legacy.bin", legacyFileLength, Store.digestToString(adler32LegacyChecksum + 1));
-            assertTrue(legacy.hasLegacyChecksum());
-            assertFalse(lucene.hasLegacyChecksum());
-            assertFalse(Store.checkIntegrityNoException(lucene, dir));
-            assertFalse(Store.checkIntegrityNoException(legacy, dir));
-        }
-
-        { // negative check - wrong length
-            StoreFileMetaData lucene = new StoreFileMetaData("lucene_checksum.bin", luceneFileLength + 1, Store.digestToString(luceneChecksum), Version.LUCENE_4_8_0);
-            StoreFileMetaData legacy = new StoreFileMetaData("legacy.bin", legacyFileLength + 1, Store.digestToString(adler32LegacyChecksum));
-            assertTrue(legacy.hasLegacyChecksum());
-            assertFalse(lucene.hasLegacyChecksum());
-            assertFalse(Store.checkIntegrityNoException(lucene, dir));
-            assertFalse(Store.checkIntegrityNoException(legacy, dir));
-        }
-
-        { // negative check - wrong file
-            StoreFileMetaData lucene = new StoreFileMetaData("legacy.bin", luceneFileLength, Store.digestToString(luceneChecksum), Version.LUCENE_4_8_0);
-            StoreFileMetaData legacy = new StoreFileMetaData("lucene_checksum.bin", legacyFileLength, Store.digestToString(adler32LegacyChecksum));
-            assertTrue(legacy.hasLegacyChecksum());
-            assertFalse(lucene.hasLegacyChecksum());
-            assertFalse(Store.checkIntegrityNoException(lucene, dir));
-            assertFalse(Store.checkIntegrityNoException(legacy, dir));
-        }
-        dir.close();
-
-    }
-
-    @Test
-    public void testVerifyingIndexInput() throws IOException {
-        Directory dir = newDirectory();
-        IndexOutput output = dir.createOutput("foo.bar", IOContext.DEFAULT);
-        int iters = scaledRandomIntBetween(10, 100);
-        for (int i = 0; i < iters; i++) {
-            BytesRef bytesRef = new BytesRef(TestUtil.randomRealisticUnicodeString(random(), 10, 1024));
-            output.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
-        }
-        CodecUtil.writeFooter(output);
-        output.close();
-
-        // Check file
-        IndexInput indexInput = dir.openInput("foo.bar", IOContext.DEFAULT);
-        long checksum = CodecUtil.retrieveChecksum(indexInput);
-        indexInput.seek(0);
-        IndexInput verifyingIndexInput = new Store.VerifyingIndexInput(dir.openInput("foo.bar", IOContext.DEFAULT));
-        readIndexInputFullyWithRandomSeeks(verifyingIndexInput);
-        Store.verify(verifyingIndexInput);
-        assertThat(checksum, equalTo(((ChecksumIndexInput) verifyingIndexInput).getChecksum()));
-        IOUtils.close(indexInput, verifyingIndexInput);
-
-        // Corrupt file and check again
-        corruptFile(dir, "foo.bar", "foo1.bar");
-        verifyingIndexInput = new Store.VerifyingIndexInput(dir.openInput("foo1.bar", IOContext.DEFAULT));
-        readIndexInputFullyWithRandomSeeks(verifyingIndexInput);
-        try {
-            Store.verify(verifyingIndexInput);
-            fail("should be a corrupted index");
-        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {
-            // ok
-        }
-        IOUtils.close(verifyingIndexInput);
-        IOUtils.close(dir);
-    }
-
-    private void readIndexInputFullyWithRandomSeeks(IndexInput indexInput) throws IOException {
-        BytesRef ref = new BytesRef(scaledRandomIntBetween(1, 1024));
-        long pos = 0;
-        while (pos < indexInput.length()) {
-            assertEquals(pos, indexInput.getFilePointer());
-            int op = random().nextInt(5);
-            if (op == 0) {
-                int shift = 100 - randomIntBetween(0, 200);
-                pos = Math.min(indexInput.length() - 1, Math.max(0, pos + shift));
-                indexInput.seek(pos);
-            } else if (op == 1) {
-                indexInput.readByte();
-                pos++;
-            } else {
-                int min = (int) Math.min(indexInput.length() - pos, ref.bytes.length);
-                indexInput.readBytes(ref.bytes, ref.offset, min);
-                pos += min;
-            }
-        }
-    }
-
-    private void corruptFile(Directory dir, String fileIn, String fileOut) throws IOException {
-        IndexInput input = dir.openInput(fileIn, IOContext.READONCE);
-        IndexOutput output = dir.createOutput(fileOut, IOContext.DEFAULT);
-        long len = input.length();
-        byte[] b = new byte[1024];
-        long broken = randomInt((int) len-1);
-        long pos = 0;
-        while (pos < len) {
-            int min = (int) Math.min(input.length() - pos, b.length);
-            input.readBytes(b, 0, min);
-            if (broken >= pos && broken < pos + min) {
-                // Flip one byte
-                int flipPos = (int) (broken - pos);
-                b[flipPos] = (byte) (b[flipPos] ^ 42);
-            }
-            output.writeBytes(b, min);
-            pos += min;
-        }
-        IOUtils.close(input, output);
-
-    }
-
-    public void assertDeleteContent(Store store, DirectoryService service) throws IOException {
-        deleteContent(store.directory());
-        assertThat(Arrays.toString(store.directory().listAll()), store.directory().listAll().length, equalTo(0));
-        assertThat(store.stats().sizeInBytes(), equalTo(0l));
-        assertThat(service.newDirectory().listAll().length, equalTo(0));
-    }
-
-    private static final class LuceneManagedDirectoryService extends DirectoryService {
-        private final Directory dir;
-        private final Random random;
-
-        public LuceneManagedDirectoryService(Random random) {
-            this(random, true);
-        }
-
-        public LuceneManagedDirectoryService(Random random, boolean preventDoubleWrite) {
-            super(new ShardId("fake", 1), Settings.EMPTY);
-            dir = StoreTest.newDirectory(random);
-            if (dir instanceof MockDirectoryWrapper) {
-                ((MockDirectoryWrapper) dir).setPreventDoubleWrite(preventDoubleWrite);
-                // TODO: fix this test to handle virus checker
-                ((MockDirectoryWrapper) dir).setEnableVirusScanner(false);
-            }
-            this.random = random;
-        }
-
-        @Override
-        public Directory newDirectory() throws IOException {
-            return dir;
-        }
-
-        @Override
-        public long throttleTimeInNanos() {
-            return random.nextInt(1000);
-        }
-    }
-
-    public static void assertConsistent(Store store, Store.MetadataSnapshot metadata) throws IOException {
-        for (String file : store.directory().listAll()) {
-            if (!IndexWriter.WRITE_LOCK_NAME.equals(file) && !IndexFileNames.OLD_SEGMENTS_GEN.equals(file) && !Store.isChecksum(file) && file.startsWith("extra") == false) {
-                assertTrue(file + " is not in the map: " + metadata.asMap().size() + " vs. " + store.directory().listAll().length, metadata.asMap().containsKey(file));
-            } else {
-                assertFalse(file + " is not in the map: " + metadata.asMap().size() + " vs. " + store.directory().listAll().length, metadata.asMap().containsKey(file));
-            }
-        }
-    }
-
-    /**
-     * Legacy indices without lucene CRC32 did never write or calculate checksums for segments_N files
-     * but for other files
-     */
-    @Test
-    public void testRecoveryDiffWithLegacyCommit() {
-        Map<String, StoreFileMetaData> metaDataMap = new HashMap<>();
-        metaDataMap.put("segments_1", new StoreFileMetaData("segments_1", 50, null, null, new BytesRef(new byte[]{1})));
-        metaDataMap.put("_0_1.del", new StoreFileMetaData("_0_1.del", 42, "foobarbaz", null, new BytesRef()));
-        Store.MetadataSnapshot first = new Store.MetadataSnapshot(metaDataMap, Collections.EMPTY_MAP, 0);
-
-        Store.MetadataSnapshot second = new Store.MetadataSnapshot(metaDataMap, Collections.EMPTY_MAP, 0);
-        Store.RecoveryDiff recoveryDiff = first.recoveryDiff(second);
-        assertEquals(recoveryDiff.toString(), recoveryDiff.different.size(), 2);
-    }
-
-
-    @Test
-    public void testRecoveryDiff() throws IOException, InterruptedException {
-        int numDocs = 2 + random().nextInt(100);
-        List<Document> docs = new ArrayList<>();
-        for (int i = 0; i < numDocs; i++) {
-            Document doc = new Document();
-            doc.add(new StringField("id", "" + i, random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-            doc.add(new TextField("body", TestUtil.randomRealisticUnicodeString(random()), random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-            doc.add(new SortedDocValuesField("dv", new BytesRef(TestUtil.randomRealisticUnicodeString(random()))));
-            docs.add(doc);
-        }
-        long seed = random().nextLong();
-        Store.MetadataSnapshot first;
-        {
-            Random random = new Random(seed);
-            IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random)).setCodec(TestUtil.getDefaultCodec());
-            iwc.setMergePolicy(NoMergePolicy.INSTANCE);
-            iwc.setUseCompoundFile(random.nextBoolean());
-            final ShardId shardId = new ShardId(new Index("index"), 1);
-            DirectoryService directoryService = new LuceneManagedDirectoryService(random);
-            Store store = new Store(shardId, Settings.EMPTY, directoryService, new DummyShardLock(shardId));
-            IndexWriter writer = new IndexWriter(store.directory(), iwc);
-            final boolean lotsOfSegments = rarely(random);
-            for (Document d : docs) {
-                writer.addDocument(d);
-                if (lotsOfSegments && random.nextBoolean()) {
-                    writer.commit();
-                } else if (rarely(random)) {
-                    writer.commit();
-                }
-            }
-            writer.commit();
-            writer.close();
-            first = store.getMetadata();
-            assertDeleteContent(store, directoryService);
-            store.close();
-        }
-        long time = new Date().getTime();
-        while (time == new Date().getTime()) {
-            Thread.sleep(10); // bump the time
-        }
-        Store.MetadataSnapshot second;
-        Store store;
-        {
-            Random random = new Random(seed);
-            IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random)).setCodec(TestUtil.getDefaultCodec());
-            iwc.setMergePolicy(NoMergePolicy.INSTANCE);
-            iwc.setUseCompoundFile(random.nextBoolean());
-            final ShardId shardId = new ShardId(new Index("index"), 1);
-            DirectoryService directoryService = new LuceneManagedDirectoryService(random);
-            store = new Store(shardId, Settings.EMPTY, directoryService, new DummyShardLock(shardId));
-            IndexWriter writer = new IndexWriter(store.directory(), iwc);
-            final boolean lotsOfSegments = rarely(random);
-            for (Document d : docs) {
-                writer.addDocument(d);
-                if (lotsOfSegments && random.nextBoolean()) {
-                    writer.commit();
-                } else if (rarely(random)) {
-                    writer.commit();
-                }
-            }
-            writer.commit();
-            writer.close();
-            second = store.getMetadata();
-        }
-        Store.RecoveryDiff diff = first.recoveryDiff(second);
-        assertThat(first.size(), equalTo(second.size()));
-        for (StoreFileMetaData md : first) {
-            assertThat(second.get(md.name()), notNullValue());
-            // si files are different - containing timestamps etc
-            assertThat(second.get(md.name()).isSame(md), equalTo(false));
-        }
-        assertThat(diff.different.size(), equalTo(first.size()));
-        assertThat(diff.identical.size(), equalTo(0)); // in lucene 5 nothing is identical - we use random ids in file headers
-        assertThat(diff.missing, empty());
-
-        // check the self diff
-        Store.RecoveryDiff selfDiff = first.recoveryDiff(first);
-        assertThat(selfDiff.identical.size(), equalTo(first.size()));
-        assertThat(selfDiff.different, empty());
-        assertThat(selfDiff.missing, empty());
-
-
-        // lets add some deletes
-        Random random = new Random(seed);
-        IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random)).setCodec(TestUtil.getDefaultCodec());
-        iwc.setMergePolicy(NoMergePolicy.INSTANCE);
-        iwc.setUseCompoundFile(random.nextBoolean());
-        iwc.setOpenMode(IndexWriterConfig.OpenMode.APPEND);
-        IndexWriter writer = new IndexWriter(store.directory(), iwc);
-        writer.deleteDocuments(new Term("id", Integer.toString(random().nextInt(numDocs))));
-        writer.commit();
-        writer.close();
-        Store.MetadataSnapshot metadata = store.getMetadata();
-        StoreFileMetaData delFile = null;
-        for (StoreFileMetaData md : metadata) {
-            if (md.name().endsWith(".liv")) {
-                delFile = md;
-                break;
-            }
-        }
-        Store.RecoveryDiff afterDeleteDiff = metadata.recoveryDiff(second);
-        if (delFile != null) {
-            assertThat(afterDeleteDiff.identical.size(), equalTo(metadata.size() - 2)); // segments_N + del file
-            assertThat(afterDeleteDiff.different.size(), equalTo(0));
-            assertThat(afterDeleteDiff.missing.size(), equalTo(2));
-        } else {
-            // an entire segment must be missing (single doc segment got dropped)
-            assertThat(afterDeleteDiff.identical.size(), greaterThan(0));
-            assertThat(afterDeleteDiff.different.size(), equalTo(0));
-            assertThat(afterDeleteDiff.missing.size(), equalTo(1)); // the commit file is different
-        }
-
-        // check the self diff
-        selfDiff = metadata.recoveryDiff(metadata);
-        assertThat(selfDiff.identical.size(), equalTo(metadata.size()));
-        assertThat(selfDiff.different, empty());
-        assertThat(selfDiff.missing, empty());
-
-        // add a new commit
-        iwc = new IndexWriterConfig(new MockAnalyzer(random)).setCodec(TestUtil.getDefaultCodec());
-        iwc.setMergePolicy(NoMergePolicy.INSTANCE);
-        iwc.setUseCompoundFile(true); // force CFS - easier to test here since we know it will add 3 files
-        iwc.setOpenMode(IndexWriterConfig.OpenMode.APPEND);
-        writer = new IndexWriter(store.directory(), iwc);
-        writer.addDocument(docs.get(0));
-        writer.close();
-
-        Store.MetadataSnapshot newCommitMetaData = store.getMetadata();
-        Store.RecoveryDiff newCommitDiff = newCommitMetaData.recoveryDiff(metadata);
-        if (delFile != null) {
-            assertThat(newCommitDiff.identical.size(), equalTo(newCommitMetaData.size() - 5)); // segments_N, del file, cfs, cfe, si for the new segment
-            assertThat(newCommitDiff.different.size(), equalTo(1)); // the del file must be different
-            assertThat(newCommitDiff.different.get(0).name(), endsWith(".liv"));
-            assertThat(newCommitDiff.missing.size(), equalTo(4)); // segments_N,cfs, cfe, si for the new segment
-        } else {
-            assertThat(newCommitDiff.identical.size(), equalTo(newCommitMetaData.size() - 4)); // segments_N, cfs, cfe, si for the new segment
-            assertThat(newCommitDiff.different.size(), equalTo(0));
-            assertThat(newCommitDiff.missing.size(), equalTo(4)); // an entire segment must be missing (single doc segment got dropped)  plus the commit is different
-        }
-
-        deleteContent(store.directory());
-        IOUtils.close(store);
-    }
-
-    @Test
-    public void testCleanupFromSnapshot() throws IOException {
-        final ShardId shardId = new ShardId(new Index("index"), 1);
-        DirectoryService directoryService = new LuceneManagedDirectoryService(random());
-        Store store = new Store(shardId, Settings.EMPTY, directoryService, new DummyShardLock(shardId));
-        // this time random codec....
-        IndexWriterConfig indexWriterConfig = newIndexWriterConfig(random(), new MockAnalyzer(random())).setCodec(TestUtil.getDefaultCodec());
-        // we keep all commits and that allows us clean based on multiple snapshots
-        indexWriterConfig.setIndexDeletionPolicy(NoDeletionPolicy.INSTANCE);
-        IndexWriter writer = new IndexWriter(store.directory(), indexWriterConfig);
-        int docs = 1 + random().nextInt(100);
-        int numCommits = 0;
-        for (int i = 0; i < docs; i++) {
-            if (i > 0 && randomIntBetween(0, 10) == 0) {
-                writer.commit();
-                numCommits++;
-            }
-            Document doc = new Document();
-            doc.add(new TextField("id", "" + i, random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-            doc.add(new TextField("body", TestUtil.randomRealisticUnicodeString(random()), random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-            doc.add(new SortedDocValuesField("dv", new BytesRef(TestUtil.randomRealisticUnicodeString(random()))));
-            writer.addDocument(doc);
-
-        }
-        if (numCommits < 1) {
-            writer.commit();
-            Document doc = new Document();
-            doc.add(new TextField("id", "" + docs++, random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-            doc.add(new TextField("body", TestUtil.randomRealisticUnicodeString(random()), random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-            doc.add(new SortedDocValuesField("dv", new BytesRef(TestUtil.randomRealisticUnicodeString(random()))));
-            writer.addDocument(doc);
-        }
-
-        Store.MetadataSnapshot firstMeta = store.getMetadata();
-
-        if (random().nextBoolean()) {
-            for (int i = 0; i < docs; i++) {
-                if (random().nextBoolean()) {
-                    Document doc = new Document();
-                    doc.add(new TextField("id", "" + i, random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-                    doc.add(new TextField("body", TestUtil.randomRealisticUnicodeString(random()), random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-                    writer.updateDocument(new Term("id", "" + i), doc);
-                }
-            }
-        }
-        writer.commit();
-        writer.close();
-
-        Store.MetadataSnapshot secondMeta = store.getMetadata();
-
-        Store.LegacyChecksums checksums = new Store.LegacyChecksums();
-        Map<String, StoreFileMetaData> legacyMeta = new HashMap<>();
-        for (String file : store.directory().listAll()) {
-            if (file.equals("write.lock") || file.equals(IndexFileNames.OLD_SEGMENTS_GEN) || file.startsWith("extra")) {
-                continue;
-            }
-            BytesRef hash = new BytesRef();
-            if (file.startsWith("segments")) {
-                hash = Store.MetadataSnapshot.hashFile(store.directory(), file);
-            }
-            StoreFileMetaData storeFileMetaData = new StoreFileMetaData(file, store.directory().fileLength(file), file + "checksum", null, hash);
-            legacyMeta.put(file, storeFileMetaData);
-            checksums.add(storeFileMetaData);
-        }
-        checksums.write(store); // write one checksum file here - we expect it to survive all the cleanups
-
-        if (randomBoolean()) {
-            store.cleanupAndVerify("test", firstMeta);
-            String[] strings = store.directory().listAll();
-            int numChecksums = 0;
-            int numNotFound = 0;
-            for (String file : strings) {
-                if (file.startsWith("extra")) {
-                    continue;
-                }
-                assertTrue(firstMeta.contains(file) || Store.isChecksum(file) || file.equals("write.lock"));
-                if (Store.isChecksum(file)) {
-                    numChecksums++;
-                } else if (secondMeta.contains(file) == false) {
-                    numNotFound++;
-                }
-
-            }
-            assertTrue("at least one file must not be in here since we have two commits?", numNotFound > 0);
-            assertEquals("we wrote one checksum but it's gone now? - checksums are supposed to be kept", numChecksums, 1);
-        } else {
-            store.cleanupAndVerify("test", secondMeta);
-            String[] strings = store.directory().listAll();
-            int numChecksums = 0;
-            int numNotFound = 0;
-            for (String file : strings) {
-                if (file.startsWith("extra")) {
-                    continue;
-                }
-                assertTrue(file, secondMeta.contains(file) || Store.isChecksum(file) || file.equals("write.lock"));
-                if (Store.isChecksum(file)) {
-                    numChecksums++;
-                } else if (firstMeta.contains(file) == false) {
-                    numNotFound++;
-                }
-
-            }
-            assertTrue("at least one file must not be in here since we have two commits?", numNotFound > 0);
-            assertEquals("we wrote one checksum but it's gone now? - checksums are supposed to be kept", numChecksums, 1);
-        }
-
-        deleteContent(store.directory());
-        IOUtils.close(store);
-    }
-
-    @Test
-    public void testCleanUpWithLegacyChecksums() throws IOException {
-        Map<String, StoreFileMetaData> metaDataMap = new HashMap<>();
-        metaDataMap.put("segments_1", new StoreFileMetaData("segments_1", 50, null, null, new BytesRef(new byte[]{1})));
-        metaDataMap.put("_0_1.del", new StoreFileMetaData("_0_1.del", 42, "foobarbaz", null, new BytesRef()));
-        Store.MetadataSnapshot snapshot = new Store.MetadataSnapshot(metaDataMap, Collections.EMPTY_MAP, 0);
-
-        final ShardId shardId = new ShardId(new Index("index"), 1);
-        DirectoryService directoryService = new LuceneManagedDirectoryService(random());
-        Store store = new Store(shardId, Settings.EMPTY, directoryService, new DummyShardLock(shardId));
-        for (String file : metaDataMap.keySet()) {
-            try (IndexOutput output = store.directory().createOutput(file, IOContext.DEFAULT)) {
-                BytesRef bytesRef = new BytesRef(TestUtil.randomRealisticUnicodeString(random(), 10, 1024));
-                output.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
-                CodecUtil.writeFooter(output);
-            }
-        }
-
-        store.verifyAfterCleanup(snapshot, snapshot);
-        deleteContent(store.directory());
-        IOUtils.close(store);
-    }
-
-    public void testOnCloseCallback() throws IOException {
-        final ShardId shardId = new ShardId(new Index(randomRealisticUnicodeOfCodepointLengthBetween(1, 10)), randomIntBetween(0, 100));
-        DirectoryService directoryService = new LuceneManagedDirectoryService(random());
-        final AtomicInteger count = new AtomicInteger(0);
-        final ShardLock lock = new DummyShardLock(shardId);
-
-        Store store = new Store(shardId, Settings.EMPTY, directoryService, lock, new Store.OnClose() {
-            @Override
-            public void handle(ShardLock theLock) {
-                assertEquals(shardId, theLock.getShardId());
-                assertEquals(lock, theLock);
-                count.incrementAndGet();
-            }
-        });
-        assertEquals(count.get(), 0);
-
-        final int iters = randomIntBetween(1, 10);
-        for (int i = 0; i < iters; i++) {
-            store.close();
-        }
-
-        assertEquals(count.get(), 1);
-    }
-
-    @Test
-    public void testStoreStats() throws IOException {
-        final ShardId shardId = new ShardId(new Index("index"), 1);
-        DirectoryService directoryService = new LuceneManagedDirectoryService(random());
-        Settings settings = Settings.builder().put(Store.INDEX_STORE_STATS_REFRESH_INTERVAL, TimeValue.timeValueMinutes(0)).build();
-        Store store = new Store(shardId, settings, directoryService, new DummyShardLock(shardId));
-        long initialStoreSize = 0;
-        for (String extraFiles : store.directory().listAll()) {
-            assertTrue("expected extraFS file but got: " + extraFiles, extraFiles.startsWith("extra"));
-            initialStoreSize += store.directory().fileLength(extraFiles);
-        }
-        StoreStats stats = store.stats();
-        assertEquals(stats.getSize().bytes(), initialStoreSize);
-
-        Directory dir = store.directory();
-        final long length;
-        try (IndexOutput output = dir.createOutput("foo.bar", IOContext.DEFAULT)) {
-            int iters = scaledRandomIntBetween(10, 100);
-            for (int i = 0; i < iters; i++) {
-                BytesRef bytesRef = new BytesRef(TestUtil.randomRealisticUnicodeString(random(), 10, 1024));
-                output.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
-            }
-            length = output.getFilePointer();
-        }
-
-        assertTrue(numNonExtraFiles(store) > 0);
-        stats = store.stats();
-        assertEquals(stats.getSizeInBytes(), length + initialStoreSize);
-
-        deleteContent(store.directory());
-        IOUtils.close(store);
-    }
-
-
-    public static void deleteContent(Directory directory) throws IOException {
-        final String[] files = directory.listAll();
-        final List<IOException> exceptions = new ArrayList<>();
-        for (String file : files) {
-            try {
-                directory.deleteFile(file);
-            } catch (NoSuchFileException | FileNotFoundException e) {
-                // ignore
-            } catch (IOException e) {
-                exceptions.add(e);
-            }
-        }
-        ExceptionsHelper.rethrowAndSuppress(exceptions);
-    }
-
-    public int numNonExtraFiles(Store store) throws IOException {
-        int numNonExtra = 0;
-        for (String file : store.directory().listAll()) {
-            if (file.startsWith("extra") == false) {
-                numNonExtra++;
-            }
-        }
-        return numNonExtra;
-    }
-
-    @Test
-    public void testMetadataSnapshotStreaming() throws Exception {
-
-        Store.MetadataSnapshot outMetadataSnapshot = createMetaDataSnapshot();
-        org.elasticsearch.Version targetNodeVersion = randomVersion(random());
-
-        ByteArrayOutputStream outBuffer = new ByteArrayOutputStream();
-        OutputStreamStreamOutput out = new OutputStreamStreamOutput(outBuffer);
-        out.setVersion(targetNodeVersion);
-        outMetadataSnapshot.writeTo(out);
-
-        ByteArrayInputStream inBuffer = new ByteArrayInputStream(outBuffer.toByteArray());
-        InputStreamStreamInput in = new InputStreamStreamInput(inBuffer);
-        in.setVersion(targetNodeVersion);
-        Store.MetadataSnapshot inMetadataSnapshot = new Store.MetadataSnapshot(in);
-        Map<String, StoreFileMetaData> origEntries = new HashMap<>();
-        origEntries.putAll(outMetadataSnapshot.asMap());
-        for (Map.Entry<String, StoreFileMetaData> entry : inMetadataSnapshot.asMap().entrySet()) {
-            assertThat(entry.getValue().name(), equalTo(origEntries.remove(entry.getKey()).name()));
-        }
-        assertThat(origEntries.size(), equalTo(0));
-        assertThat(inMetadataSnapshot.getCommitUserData(), equalTo(outMetadataSnapshot.getCommitUserData()));
-    }
-
-    protected Store.MetadataSnapshot createMetaDataSnapshot() {
-        StoreFileMetaData storeFileMetaData1 = new StoreFileMetaData("segments", 1);
-        StoreFileMetaData storeFileMetaData2 = new StoreFileMetaData("no_segments", 1);
-        Map<String, StoreFileMetaData> storeFileMetaDataMap = new HashMap<>();
-        storeFileMetaDataMap.put(storeFileMetaData1.name(), storeFileMetaData1);
-        storeFileMetaDataMap.put(storeFileMetaData2.name(), storeFileMetaData2);
-        Map<String, String> commitUserData = new HashMap<>();
-        commitUserData.put("userdata_1", "test");
-        commitUserData.put("userdata_2", "test");
-        return new Store.MetadataSnapshot(storeFileMetaDataMap, commitUserData, 0);
-    }
-
-    @Test
-    public void testUserDataRead() throws IOException {
-        final ShardId shardId = new ShardId(new Index("index"), 1);
-        DirectoryService directoryService = new LuceneManagedDirectoryService(random());
-        Store store = new Store(shardId, Settings.EMPTY, directoryService, new DummyShardLock(shardId));
-        IndexWriterConfig config = newIndexWriterConfig(random(), new MockAnalyzer(random())).setCodec(TestUtil.getDefaultCodec());
-        SnapshotDeletionPolicy deletionPolicy = new SnapshotDeletionPolicy(new KeepOnlyLastDeletionPolicy(shardId, EMPTY_SETTINGS));
-        config.setIndexDeletionPolicy(deletionPolicy);
-        IndexWriter writer = new IndexWriter(store.directory(), config);
-        Document doc = new Document();
-        doc.add(new TextField("id", "1", Field.Store.NO));
-        writer.addDocument(doc);
-        Map<String, String> commitData = new HashMap<>(2);
-        String syncId = "a sync id";
-        String translogId = "a translog id";
-        commitData.put(Engine.SYNC_COMMIT_ID, syncId);
-        commitData.put(Translog.TRANSLOG_GENERATION_KEY, translogId);
-        writer.setCommitData(commitData);
-        writer.commit();
-        writer.close();
-        Store.MetadataSnapshot metadata;
-        if (randomBoolean()) {
-            metadata = store.getMetadata();
-        } else {
-            metadata = store.getMetadata(deletionPolicy.snapshot());
-        }
-        assertFalse(metadata.asMap().isEmpty());
-        // do not check for correct files, we have enough tests for that above
-        assertThat(metadata.getCommitUserData().get(Engine.SYNC_COMMIT_ID), equalTo(syncId));
-        assertThat(metadata.getCommitUserData().get(Translog.TRANSLOG_GENERATION_KEY), equalTo(translogId));
-        TestUtil.checkIndex(store.directory());
-        assertDeleteContent(store, directoryService);
-        IOUtils.close(store);
-    }
-
-    @Test
-    public void testStreamStoreFilesMetaData() throws Exception {
-        Store.MetadataSnapshot metadataSnapshot = createMetaDataSnapshot();
-        TransportNodesListShardStoreMetaData.StoreFilesMetaData outStoreFileMetaData = new TransportNodesListShardStoreMetaData.StoreFilesMetaData(randomBoolean(), new ShardId("test", 0),metadataSnapshot);
-        ByteArrayOutputStream outBuffer = new ByteArrayOutputStream();
-        OutputStreamStreamOutput out = new OutputStreamStreamOutput(outBuffer);
-        org.elasticsearch.Version targetNodeVersion = randomVersion(random());
-        out.setVersion(targetNodeVersion);
-        outStoreFileMetaData.writeTo(out);
-        ByteArrayInputStream inBuffer = new ByteArrayInputStream(outBuffer.toByteArray());
-        InputStreamStreamInput in = new InputStreamStreamInput(inBuffer);
-        in.setVersion(targetNodeVersion);
-        TransportNodesListShardStoreMetaData.StoreFilesMetaData inStoreFileMetaData = TransportNodesListShardStoreMetaData.StoreFilesMetaData.readStoreFilesMetaData(in);
-        Iterator<StoreFileMetaData> outFiles = outStoreFileMetaData.iterator();
-        for (StoreFileMetaData inFile : inStoreFileMetaData) {
-            assertThat(inFile.name(), equalTo(outFiles.next().name()));
-        }
-        assertThat(outStoreFileMetaData.syncId(), equalTo(inStoreFileMetaData.syncId()));
-    }
-
-    public void testMarkCorruptedOnTruncatedSegmentsFile() throws IOException {
-        IndexWriterConfig iwc = newIndexWriterConfig();
-        final ShardId shardId = new ShardId(new Index("index"), 1);
-        DirectoryService directoryService = new LuceneManagedDirectoryService(random());
-        Store store = new Store(shardId, Settings.EMPTY, directoryService, new DummyShardLock(shardId));
-        IndexWriter writer = new IndexWriter(store.directory(), iwc);
-
-        int numDocs = 1 + random().nextInt(10);
-        List<Document> docs = new ArrayList<>();
-        for (int i = 0; i < numDocs; i++) {
-            Document doc = new Document();
-            doc.add(new StringField("id", "" + i, random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-            doc.add(new TextField("body", TestUtil.randomRealisticUnicodeString(random()), random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-            doc.add(new SortedDocValuesField("dv", new BytesRef(TestUtil.randomRealisticUnicodeString(random()))));
-            docs.add(doc);
-        }
-        for (Document d : docs) {
-            writer.addDocument(d);
-        }
-        writer.commit();
-        writer.close();
-        MockDirectoryWrapper leaf = DirectoryUtils.getLeaf(store.directory(), MockDirectoryWrapper.class);
-        if (leaf != null) {
-            leaf.setPreventDoubleWrite(false); // I do this on purpose
-        }
-        SegmentInfos segmentCommitInfos = store.readLastCommittedSegmentsInfo();
-        try (IndexOutput out = store.directory().createOutput(segmentCommitInfos.getSegmentsFileName(), IOContext.DEFAULT)) {
-            // empty file
-        }
-
-        try {
-            if (randomBoolean()) {
-                store.getMetadata();
-            } else {
-                store.readLastCommittedSegmentsInfo();
-            }
-            fail("corrupted segments_N file");
-        } catch (CorruptIndexException ex) {
-            // expected
-        }
-        assertTrue(store.isMarkedCorrupted());
-        Lucene.cleanLuceneIndex(store.directory()); // we have to remove the index since it's corrupted and might fail the MocKDirWrapper checkindex call
-        store.close();
-    }
-
-    public void testCanOpenIndex() throws IOException {
-        IndexWriterConfig iwc = newIndexWriterConfig();
-        Path tempDir = createTempDir();
-        final BaseDirectoryWrapper dir = newFSDirectory(tempDir);
-        assertFalse(Store.canOpenIndex(logger, tempDir));
-        IndexWriter writer = new IndexWriter(dir, iwc);
-        Document doc = new Document();
-        doc.add(new StringField("id", "1", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
-        writer.addDocument(doc);
-        writer.commit();
-        writer.close();
-        assertTrue(Store.canOpenIndex(logger, tempDir));
-
-        final ShardId shardId = new ShardId(new Index("index"), 1);
-        DirectoryService directoryService = new DirectoryService(shardId, Settings.EMPTY) {
-            @Override
-            public long throttleTimeInNanos() {
-                return 0;
-            }
-
-            @Override
-            public Directory newDirectory() throws IOException {
-                return dir;
-            }
-        };
-        Store store = new Store(shardId, Settings.EMPTY, directoryService, new DummyShardLock(shardId));
-        store.markStoreCorrupted(new CorruptIndexException("foo", "bar"));
-        assertFalse(Store.canOpenIndex(logger, tempDir));
-        store.close();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/store/StoreTests.java b/core/src/test/java/org/elasticsearch/index/store/StoreTests.java
new file mode 100644
index 0000000..b2e49f3
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/store/StoreTests.java
@@ -0,0 +1,1270 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.index.store;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.FilterCodec;
+import org.apache.lucene.codecs.SegmentInfoFormat;
+import org.apache.lucene.codecs.lucene50.Lucene50SegmentInfoFormat;
+import org.apache.lucene.codecs.lucene53.Lucene53Codec;
+import org.apache.lucene.document.*;
+import org.apache.lucene.index.*;
+import org.apache.lucene.store.*;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.TestUtil;
+import org.apache.lucene.util.Version;
+import org.elasticsearch.ExceptionsHelper;
+import org.elasticsearch.common.io.stream.InputStreamStreamInput;
+import org.elasticsearch.common.io.stream.OutputStreamStreamOutput;
+import org.elasticsearch.common.lucene.Lucene;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.env.ShardLock;
+import org.elasticsearch.index.Index;
+import org.elasticsearch.index.deletionpolicy.KeepOnlyLastDeletionPolicy;
+import org.elasticsearch.index.deletionpolicy.SnapshotDeletionPolicy;
+import org.elasticsearch.index.engine.Engine;
+import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.index.translog.Translog;
+import org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData;
+import org.elasticsearch.test.DummyShardLock;
+import org.elasticsearch.test.ESTestCase;
+import org.hamcrest.Matchers;
+import org.junit.Test;
+
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.nio.file.NoSuchFileException;
+import java.nio.file.Path;
+import java.util.*;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.zip.Adler32;
+
+import static org.elasticsearch.common.settings.Settings.Builder.EMPTY_SETTINGS;
+import static org.elasticsearch.test.VersionUtils.randomVersion;
+import static org.hamcrest.Matchers.*;
+
+public class StoreTests extends ESTestCase {
+
+    @Test
+    public void testRefCount() throws IOException {
+        final ShardId shardId = new ShardId(new Index("index"), 1);
+        DirectoryService directoryService = new LuceneManagedDirectoryService(random());
+        Store store = new Store(shardId, Settings.EMPTY, directoryService, new DummyShardLock(shardId));
+        int incs = randomIntBetween(1, 100);
+        for (int i = 0; i < incs; i++) {
+            if (randomBoolean()) {
+                store.incRef();
+            } else {
+                assertTrue(store.tryIncRef());
+            }
+            store.ensureOpen();
+        }
+
+        for (int i = 0; i < incs; i++) {
+            store.decRef();
+            store.ensureOpen();
+        }
+
+        store.incRef();
+        final AtomicBoolean called = new AtomicBoolean(false);
+        store.close();
+        for (int i = 0; i < incs; i++) {
+            if (randomBoolean()) {
+                store.incRef();
+            } else {
+                assertTrue(store.tryIncRef());
+            }
+            store.ensureOpen();
+        }
+
+        for (int i = 0; i < incs; i++) {
+            store.decRef();
+            store.ensureOpen();
+        }
+
+        store.decRef();
+        assertThat(store.refCount(), Matchers.equalTo(0));
+        assertFalse(store.tryIncRef());
+        try {
+            store.incRef();
+            fail(" expected exception");
+        } catch (AlreadyClosedException ex) {
+
+        }
+        try {
+            store.ensureOpen();
+            fail(" expected exception");
+        } catch (AlreadyClosedException ex) {
+
+        }
+    }
+
+    @Test
+    public void testVerifyingIndexOutput() throws IOException {
+        Directory dir = newDirectory();
+        IndexOutput output = dir.createOutput("foo.bar", IOContext.DEFAULT);
+        int iters = scaledRandomIntBetween(10, 100);
+        for (int i = 0; i < iters; i++) {
+            BytesRef bytesRef = new BytesRef(TestUtil.randomRealisticUnicodeString(random(), 10, 1024));
+            output.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
+        }
+        CodecUtil.writeFooter(output);
+        output.close();
+        IndexInput indexInput = dir.openInput("foo.bar", IOContext.DEFAULT);
+        String checksum = Store.digestToString(CodecUtil.retrieveChecksum(indexInput));
+        indexInput.seek(0);
+        BytesRef ref = new BytesRef(scaledRandomIntBetween(1, 1024));
+        long length = indexInput.length();
+        IndexOutput verifyingOutput = new Store.LuceneVerifyingIndexOutput(new StoreFileMetaData("foo1.bar", length, checksum), dir.createOutput("foo1.bar", IOContext.DEFAULT));
+        while (length > 0) {
+            if (random().nextInt(10) == 0) {
+                verifyingOutput.writeByte(indexInput.readByte());
+                length--;
+            } else {
+                int min = (int) Math.min(length, ref.bytes.length);
+                indexInput.readBytes(ref.bytes, ref.offset, min);
+                verifyingOutput.writeBytes(ref.bytes, ref.offset, min);
+                length -= min;
+            }
+        }
+        Store.verify(verifyingOutput);
+        verifyingOutput.writeByte((byte) 0x0);
+        try {
+            Store.verify(verifyingOutput);
+            fail("should be a corrupted index");
+        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {
+            // ok
+        }
+        IOUtils.close(indexInput, verifyingOutput, dir);
+    }
+
+    @Test
+    public void testVerifyingIndexOutputWithBogusInput() throws IOException {
+        Directory dir = newDirectory();
+        int length = scaledRandomIntBetween(10, 1024);
+        IndexOutput verifyingOutput = new Store.LuceneVerifyingIndexOutput(new StoreFileMetaData("foo1.bar", length, ""), dir.createOutput("foo1.bar", IOContext.DEFAULT));
+        try {
+            while (length > 0) {
+                verifyingOutput.writeByte((byte) random().nextInt());
+                length--;
+            }
+            fail("should be a corrupted index");
+        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {
+            // ok
+        }
+        IOUtils.close(verifyingOutput, dir);
+    }
+
+    // TODO: remove this, its too fragile. just use a static old index instead.
+    private static final class OldSIMockingCodec extends FilterCodec {
+
+        protected OldSIMockingCodec() {
+            super(new Lucene53Codec().getName(), new Lucene53Codec());
+        }
+
+        @Override
+        public SegmentInfoFormat segmentInfoFormat() {
+            final SegmentInfoFormat segmentInfoFormat = super.segmentInfoFormat();
+            return new SegmentInfoFormat() {
+                @Override
+                public SegmentInfo read(Directory directory, String segmentName, byte[] segmentID, IOContext context) throws IOException {
+                    return segmentInfoFormat.read(directory, segmentName, segmentID, context);
+                }
+
+                // this sucks it's a full copy of Lucene50SegmentInfoFormat but hey I couldn't find a way to make it write 4_5_0 versions
+                // somebody was too paranoid when implementing this. ey rmuir, was that you? - go fix it :P
+                @Override
+                public void write(Directory dir, SegmentInfo si, IOContext ioContext) throws IOException {
+                    final String fileName = IndexFileNames.segmentFileName(si.name, "", Lucene50SegmentInfoFormat.SI_EXTENSION);
+                    si.addFile(fileName);
+
+                    boolean success = false;
+                    try (IndexOutput output = dir.createOutput(fileName, ioContext)) {
+                        CodecUtil.writeIndexHeader(output,
+                                "Lucene50SegmentInfo",
+                                0,
+                                si.getId(),
+                                "");
+                        Version version = Version.LUCENE_4_5_0; // FOOOOOO!!
+                        // Write the Lucene version that created this segment, since 3.1
+                        output.writeInt(version.major);
+                        output.writeInt(version.minor);
+                        output.writeInt(version.bugfix);
+                        assert version.prerelease == 0;
+                        output.writeInt(si.maxDoc());
+
+                        output.writeByte((byte) (si.getUseCompoundFile() ? SegmentInfo.YES : SegmentInfo.NO));
+                        output.writeStringStringMap(si.getDiagnostics());
+                        Set<String> files = si.files();
+                        for (String file : files) {
+                            if (!IndexFileNames.parseSegmentName(file).equals(si.name)) {
+                                throw new IllegalArgumentException("invalid files: expected segment=" + si.name + ", got=" + files);
+                            }
+                        }
+                        output.writeStringSet(files);
+                        output.writeStringStringMap(si.getAttributes());
+                        CodecUtil.writeFooter(output);
+                        success = true;
+                    } finally {
+                        if (!success) {
+                            // TODO: are we doing this outside of the tracking wrapper? why must SIWriter cleanup like this?
+                            IOUtils.deleteFilesIgnoringExceptions(si.dir, fileName);
+                        }
+                    }
+                }
+            };
+        }
+    }
+
+    // IF THIS TEST FAILS ON UPGRADE GO LOOK AT THE OldSIMockingCodec!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+    @AwaitsFix(bugUrl="Fails with seed E1394B038144F6E")
+    // The test currently fails because the segment infos and the index don't
+    // agree on the oldest version of a segment. We should fix this test by
+    // switching to a static bw index
+    @Test
+    public void testWriteLegacyChecksums() throws IOException {
+        final ShardId shardId = new ShardId(new Index("index"), 1);
+        DirectoryService directoryService = new LuceneManagedDirectoryService(random());
+        Store store = new Store(shardId, Settings.EMPTY, directoryService, new DummyShardLock(shardId));
+        // set default codec - all segments need checksums
+        final boolean usesOldCodec = randomBoolean();
+        IndexWriter writer = new IndexWriter(store.directory(), newIndexWriterConfig(random(), new MockAnalyzer(random())).setCodec(usesOldCodec ? new OldSIMockingCodec() : TestUtil.getDefaultCodec()));
+        int docs = 1 + random().nextInt(100);
+
+        for (int i = 0; i < docs; i++) {
+            Document doc = new Document();
+            doc.add(new TextField("id", "" + i, random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+            doc.add(new TextField("body", TestUtil.randomRealisticUnicodeString(random()), random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+            doc.add(new SortedDocValuesField("dv", new BytesRef(TestUtil.randomRealisticUnicodeString(random()))));
+            writer.addDocument(doc);
+        }
+        if (random().nextBoolean()) {
+            for (int i = 0; i < docs; i++) {
+                if (random().nextBoolean()) {
+                    Document doc = new Document();
+                    doc.add(new TextField("id", "" + i, random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+                    doc.add(new TextField("body", TestUtil.randomRealisticUnicodeString(random()), random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+                    writer.updateDocument(new Term("id", "" + i), doc);
+                }
+            }
+        }
+        if (random().nextBoolean()) {
+            DirectoryReader.open(writer, random().nextBoolean()).close(); // flush
+        }
+        Store.MetadataSnapshot metadata;
+        // check before we committed
+        try {
+            store.getMetadata();
+            fail("no index present - expected exception");
+        } catch (IndexNotFoundException ex) {
+            // expected
+        }
+        assertThat(store.getMetadataOrEmpty(), is(Store.MetadataSnapshot.EMPTY)); // nothing committed
+
+        writer.close();
+        Store.LegacyChecksums checksums = new Store.LegacyChecksums();
+        Map<String, StoreFileMetaData> legacyMeta = new HashMap<>();
+        for (String file : store.directory().listAll()) {
+            if (file.equals("write.lock") || file.equals(IndexFileNames.OLD_SEGMENTS_GEN) || file.startsWith("extra")) {
+                continue;
+            }
+            BytesRef hash = new BytesRef();
+            if (file.startsWith("segments")) {
+                hash = Store.MetadataSnapshot.hashFile(store.directory(), file);
+            }
+            StoreFileMetaData storeFileMetaData = new StoreFileMetaData(file, store.directory().fileLength(file), file + "checksum", null, hash);
+            legacyMeta.put(file, storeFileMetaData);
+            checksums.add(storeFileMetaData);
+        }
+        checksums.write(store);
+
+        metadata = store.getMetadata();
+        Map<String, StoreFileMetaData> stringStoreFileMetaDataMap = metadata.asMap();
+        assertThat(legacyMeta.size(), equalTo(stringStoreFileMetaDataMap.size()));
+        if (usesOldCodec) {
+            for (StoreFileMetaData meta : legacyMeta.values()) {
+                assertTrue(meta.toString(), stringStoreFileMetaDataMap.containsKey(meta.name()));
+                assertEquals(meta.name() + "checksum", meta.checksum());
+                assertTrue(meta + " vs. " + stringStoreFileMetaDataMap.get(meta.name()), stringStoreFileMetaDataMap.get(meta.name()).isSame(meta));
+            }
+        } else {
+
+            // even if we have a legacy checksum - if we use a new codec we should reuse
+            for (StoreFileMetaData meta : legacyMeta.values()) {
+                assertTrue(meta.toString(), stringStoreFileMetaDataMap.containsKey(meta.name()));
+                assertFalse(meta + " vs. " + stringStoreFileMetaDataMap.get(meta.name()), stringStoreFileMetaDataMap.get(meta.name()).isSame(meta));
+                StoreFileMetaData storeFileMetaData = metadata.get(meta.name());
+                try (IndexInput input = store.openVerifyingInput(meta.name(), IOContext.DEFAULT, storeFileMetaData)) {
+                    assertTrue(storeFileMetaData.toString(), input instanceof Store.VerifyingIndexInput);
+                    input.seek(meta.length());
+                    Store.verify(input);
+                }
+            }
+        }
+        assertDeleteContent(store, directoryService);
+        IOUtils.close(store);
+
+    }
+
+    @Test
+    public void testNewChecksums() throws IOException {
+        final ShardId shardId = new ShardId(new Index("index"), 1);
+        DirectoryService directoryService = new LuceneManagedDirectoryService(random());
+        Store store = new Store(shardId, Settings.EMPTY, directoryService, new DummyShardLock(shardId));
+        // set default codec - all segments need checksums
+        IndexWriter writer = new IndexWriter(store.directory(), newIndexWriterConfig(random(), new MockAnalyzer(random())).setCodec(TestUtil.getDefaultCodec()));
+        int docs = 1 + random().nextInt(100);
+
+        for (int i = 0; i < docs; i++) {
+            Document doc = new Document();
+            doc.add(new TextField("id", "" + i, random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+            doc.add(new TextField("body", TestUtil.randomRealisticUnicodeString(random()), random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+            doc.add(new SortedDocValuesField("dv", new BytesRef(TestUtil.randomRealisticUnicodeString(random()))));
+            writer.addDocument(doc);
+        }
+        if (random().nextBoolean()) {
+            for (int i = 0; i < docs; i++) {
+                if (random().nextBoolean()) {
+                    Document doc = new Document();
+                    doc.add(new TextField("id", "" + i, random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+                    doc.add(new TextField("body", TestUtil.randomRealisticUnicodeString(random()), random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+                    writer.updateDocument(new Term("id", "" + i), doc);
+                }
+            }
+        }
+        if (random().nextBoolean()) {
+            DirectoryReader.open(writer, random().nextBoolean()).close(); // flush
+        }
+        Store.MetadataSnapshot metadata;
+        // check before we committed
+        try {
+            store.getMetadata();
+            fail("no index present - expected exception");
+        } catch (IndexNotFoundException ex) {
+            // expected
+        }
+        assertThat(store.getMetadataOrEmpty(), is(Store.MetadataSnapshot.EMPTY)); // nothing committed
+        writer.commit();
+        writer.close();
+        metadata = store.getMetadata();
+        assertThat(metadata.asMap().isEmpty(), is(false));
+        for (StoreFileMetaData meta : metadata) {
+            try (IndexInput input = store.directory().openInput(meta.name(), IOContext.DEFAULT)) {
+                String checksum = Store.digestToString(CodecUtil.retrieveChecksum(input));
+                assertThat("File: " + meta.name() + " has a different checksum", meta.checksum(), equalTo(checksum));
+                assertThat(meta.hasLegacyChecksum(), equalTo(false));
+                assertThat(meta.writtenBy(), equalTo(Version.LATEST));
+                if (meta.name().endsWith(".si") || meta.name().startsWith("segments_")) {
+                    assertThat(meta.hash().length, greaterThan(0));
+                }
+            }
+        }
+        assertConsistent(store, metadata);
+
+        TestUtil.checkIndex(store.directory());
+        assertDeleteContent(store, directoryService);
+        IOUtils.close(store);
+    }
+
+    @Test
+    public void testMixedChecksums() throws IOException {
+        final ShardId shardId = new ShardId(new Index("index"), 1);
+        DirectoryService directoryService = new LuceneManagedDirectoryService(random());
+        Store store = new Store(shardId, Settings.EMPTY, directoryService, new DummyShardLock(shardId));
+        // this time random codec....
+        IndexWriter writer = new IndexWriter(store.directory(), newIndexWriterConfig(random(), new MockAnalyzer(random())).setCodec(TestUtil.getDefaultCodec()));
+        int docs = 1 + random().nextInt(100);
+
+        for (int i = 0; i < docs; i++) {
+            Document doc = new Document();
+            doc.add(new TextField("id", "" + i, random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+            doc.add(new TextField("body", TestUtil.randomRealisticUnicodeString(random()), random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+            doc.add(new SortedDocValuesField("dv", new BytesRef(TestUtil.randomRealisticUnicodeString(random()))));
+            writer.addDocument(doc);
+        }
+        if (random().nextBoolean()) {
+            for (int i = 0; i < docs; i++) {
+                if (random().nextBoolean()) {
+                    Document doc = new Document();
+                    doc.add(new TextField("id", "" + i, random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+                    doc.add(new TextField("body", TestUtil.randomRealisticUnicodeString(random()), random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+                    writer.updateDocument(new Term("id", "" + i), doc);
+                }
+            }
+        }
+        if (random().nextBoolean()) {
+            DirectoryReader.open(writer, random().nextBoolean()).close(); // flush
+        }
+        Store.MetadataSnapshot metadata;
+        // check before we committed
+        try {
+            store.getMetadata();
+            fail("no index present - expected exception");
+        } catch (IndexNotFoundException ex) {
+            // expected
+        }
+        assertThat(store.getMetadataOrEmpty(), is(Store.MetadataSnapshot.EMPTY)); // nothing committed
+        writer.commit();
+        writer.close();
+        Store.LegacyChecksums checksums = new Store.LegacyChecksums();
+        metadata = store.getMetadata();
+        assertThat(metadata.asMap().isEmpty(), is(false));
+        for (StoreFileMetaData meta : metadata) {
+            try (IndexInput input = store.directory().openInput(meta.name(), IOContext.DEFAULT)) {
+                if (meta.checksum() == null) {
+                    String checksum = null;
+                    try {
+                        CodecUtil.retrieveChecksum(input);
+                        fail("expected a corrupt index - posting format has not checksums");
+                    } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {
+                        try (ChecksumIndexInput checksumIndexInput = store.directory().openChecksumInput(meta.name(), IOContext.DEFAULT)) {
+                            checksumIndexInput.seek(meta.length());
+                            checksum = Store.digestToString(checksumIndexInput.getChecksum());
+                        }
+                        // fine - it's a postings format without checksums
+                        checksums.add(new StoreFileMetaData(meta.name(), meta.length(), checksum, null));
+                    }
+                } else {
+                    String checksum = Store.digestToString(CodecUtil.retrieveChecksum(input));
+                    assertThat("File: " + meta.name() + " has a different checksum", meta.checksum(), equalTo(checksum));
+                    assertThat(meta.hasLegacyChecksum(), equalTo(false));
+                    assertThat(meta.writtenBy(), equalTo(Version.LATEST));
+                }
+            }
+        }
+        assertConsistent(store, metadata);
+        checksums.write(store);
+        metadata = store.getMetadata();
+        assertThat(metadata.asMap().isEmpty(), is(false));
+        for (StoreFileMetaData meta : metadata) {
+            assertThat("file: " + meta.name() + " has a null checksum", meta.checksum(), not(nullValue()));
+            if (meta.hasLegacyChecksum()) {
+                try (ChecksumIndexInput checksumIndexInput = store.directory().openChecksumInput(meta.name(), IOContext.DEFAULT)) {
+                    checksumIndexInput.seek(meta.length());
+                    assertThat(meta.checksum(), equalTo(Store.digestToString(checksumIndexInput.getChecksum())));
+                }
+            } else {
+                try (IndexInput input = store.directory().openInput(meta.name(), IOContext.DEFAULT)) {
+                    String checksum = Store.digestToString(CodecUtil.retrieveChecksum(input));
+                    assertThat("File: " + meta.name() + " has a different checksum", meta.checksum(), equalTo(checksum));
+                    assertThat(meta.hasLegacyChecksum(), equalTo(false));
+                    assertThat(meta.writtenBy(), equalTo(Version.LATEST));
+                }
+            }
+        }
+        assertConsistent(store, metadata);
+        TestUtil.checkIndex(store.directory());
+        assertDeleteContent(store, directoryService);
+        IOUtils.close(store);
+    }
+
+    @Test
+    public void testRenameFile() throws IOException {
+        final ShardId shardId = new ShardId(new Index("index"), 1);
+        DirectoryService directoryService = new LuceneManagedDirectoryService(random(), false);
+        Store store = new Store(shardId, Settings.EMPTY, directoryService, new DummyShardLock(shardId));
+        {
+            IndexOutput output = store.directory().createOutput("foo.bar", IOContext.DEFAULT);
+            int iters = scaledRandomIntBetween(10, 100);
+            for (int i = 0; i < iters; i++) {
+                BytesRef bytesRef = new BytesRef(TestUtil.randomRealisticUnicodeString(random(), 10, 1024));
+                output.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
+            }
+            CodecUtil.writeFooter(output);
+            output.close();
+        }
+        store.renameFile("foo.bar", "bar.foo");
+        assertThat(numNonExtraFiles(store), is(1));
+        final long lastChecksum;
+        try (IndexInput input = store.directory().openInput("bar.foo", IOContext.DEFAULT)) {
+            lastChecksum = CodecUtil.checksumEntireFile(input);
+        }
+
+        try {
+            store.directory().openInput("foo.bar", IOContext.DEFAULT);
+            fail("file was renamed");
+        } catch (FileNotFoundException | NoSuchFileException ex) {
+            // expected
+        }
+        {
+            IndexOutput output = store.directory().createOutput("foo.bar", IOContext.DEFAULT);
+            int iters = scaledRandomIntBetween(10, 100);
+            for (int i = 0; i < iters; i++) {
+                BytesRef bytesRef = new BytesRef(TestUtil.randomRealisticUnicodeString(random(), 10, 1024));
+                output.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
+            }
+            CodecUtil.writeFooter(output);
+            output.close();
+        }
+        store.renameFile("foo.bar", "bar.foo");
+        assertThat(numNonExtraFiles(store), is(1));
+        assertDeleteContent(store, directoryService);
+        IOUtils.close(store);
+    }
+
+    public void testCheckIntegrity() throws IOException {
+        Directory dir = newDirectory();
+        long luceneFileLength = 0;
+
+        try (IndexOutput output = dir.createOutput("lucene_checksum.bin", IOContext.DEFAULT)) {
+            int iters = scaledRandomIntBetween(10, 100);
+            for (int i = 0; i < iters; i++) {
+                BytesRef bytesRef = new BytesRef(TestUtil.randomRealisticUnicodeString(random(), 10, 1024));
+                output.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
+                luceneFileLength += bytesRef.length;
+            }
+            CodecUtil.writeFooter(output);
+            luceneFileLength += CodecUtil.footerLength();
+
+        }
+
+        final Adler32 adler32 = new Adler32();
+        long legacyFileLength = 0;
+        try (IndexOutput output = dir.createOutput("legacy.bin", IOContext.DEFAULT)) {
+            int iters = scaledRandomIntBetween(10, 100);
+            for (int i = 0; i < iters; i++) {
+                BytesRef bytesRef = new BytesRef(TestUtil.randomRealisticUnicodeString(random(), 10, 1024));
+                output.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
+                adler32.update(bytesRef.bytes, bytesRef.offset, bytesRef.length);
+                legacyFileLength += bytesRef.length;
+            }
+        }
+        final long luceneChecksum;
+        final long adler32LegacyChecksum = adler32.getValue();
+        try (IndexInput indexInput = dir.openInput("lucene_checksum.bin", IOContext.DEFAULT)) {
+            assertEquals(luceneFileLength, indexInput.length());
+            luceneChecksum = CodecUtil.retrieveChecksum(indexInput);
+        }
+
+        { // positive check
+            StoreFileMetaData lucene = new StoreFileMetaData("lucene_checksum.bin", luceneFileLength, Store.digestToString(luceneChecksum), Version.LUCENE_4_8_0);
+            StoreFileMetaData legacy = new StoreFileMetaData("legacy.bin", legacyFileLength, Store.digestToString(adler32LegacyChecksum));
+            assertTrue(legacy.hasLegacyChecksum());
+            assertFalse(lucene.hasLegacyChecksum());
+            assertTrue(Store.checkIntegrityNoException(lucene, dir));
+            assertTrue(Store.checkIntegrityNoException(legacy, dir));
+        }
+
+        { // negative check - wrong checksum
+            StoreFileMetaData lucene = new StoreFileMetaData("lucene_checksum.bin", luceneFileLength, Store.digestToString(luceneChecksum + 1), Version.LUCENE_4_8_0);
+            StoreFileMetaData legacy = new StoreFileMetaData("legacy.bin", legacyFileLength, Store.digestToString(adler32LegacyChecksum + 1));
+            assertTrue(legacy.hasLegacyChecksum());
+            assertFalse(lucene.hasLegacyChecksum());
+            assertFalse(Store.checkIntegrityNoException(lucene, dir));
+            assertFalse(Store.checkIntegrityNoException(legacy, dir));
+        }
+
+        { // negative check - wrong length
+            StoreFileMetaData lucene = new StoreFileMetaData("lucene_checksum.bin", luceneFileLength + 1, Store.digestToString(luceneChecksum), Version.LUCENE_4_8_0);
+            StoreFileMetaData legacy = new StoreFileMetaData("legacy.bin", legacyFileLength + 1, Store.digestToString(adler32LegacyChecksum));
+            assertTrue(legacy.hasLegacyChecksum());
+            assertFalse(lucene.hasLegacyChecksum());
+            assertFalse(Store.checkIntegrityNoException(lucene, dir));
+            assertFalse(Store.checkIntegrityNoException(legacy, dir));
+        }
+
+        { // negative check - wrong file
+            StoreFileMetaData lucene = new StoreFileMetaData("legacy.bin", luceneFileLength, Store.digestToString(luceneChecksum), Version.LUCENE_4_8_0);
+            StoreFileMetaData legacy = new StoreFileMetaData("lucene_checksum.bin", legacyFileLength, Store.digestToString(adler32LegacyChecksum));
+            assertTrue(legacy.hasLegacyChecksum());
+            assertFalse(lucene.hasLegacyChecksum());
+            assertFalse(Store.checkIntegrityNoException(lucene, dir));
+            assertFalse(Store.checkIntegrityNoException(legacy, dir));
+        }
+        dir.close();
+
+    }
+
+    @Test
+    public void testVerifyingIndexInput() throws IOException {
+        Directory dir = newDirectory();
+        IndexOutput output = dir.createOutput("foo.bar", IOContext.DEFAULT);
+        int iters = scaledRandomIntBetween(10, 100);
+        for (int i = 0; i < iters; i++) {
+            BytesRef bytesRef = new BytesRef(TestUtil.randomRealisticUnicodeString(random(), 10, 1024));
+            output.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
+        }
+        CodecUtil.writeFooter(output);
+        output.close();
+
+        // Check file
+        IndexInput indexInput = dir.openInput("foo.bar", IOContext.DEFAULT);
+        long checksum = CodecUtil.retrieveChecksum(indexInput);
+        indexInput.seek(0);
+        IndexInput verifyingIndexInput = new Store.VerifyingIndexInput(dir.openInput("foo.bar", IOContext.DEFAULT));
+        readIndexInputFullyWithRandomSeeks(verifyingIndexInput);
+        Store.verify(verifyingIndexInput);
+        assertThat(checksum, equalTo(((ChecksumIndexInput) verifyingIndexInput).getChecksum()));
+        IOUtils.close(indexInput, verifyingIndexInput);
+
+        // Corrupt file and check again
+        corruptFile(dir, "foo.bar", "foo1.bar");
+        verifyingIndexInput = new Store.VerifyingIndexInput(dir.openInput("foo1.bar", IOContext.DEFAULT));
+        readIndexInputFullyWithRandomSeeks(verifyingIndexInput);
+        try {
+            Store.verify(verifyingIndexInput);
+            fail("should be a corrupted index");
+        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {
+            // ok
+        }
+        IOUtils.close(verifyingIndexInput);
+        IOUtils.close(dir);
+    }
+
+    private void readIndexInputFullyWithRandomSeeks(IndexInput indexInput) throws IOException {
+        BytesRef ref = new BytesRef(scaledRandomIntBetween(1, 1024));
+        long pos = 0;
+        while (pos < indexInput.length()) {
+            assertEquals(pos, indexInput.getFilePointer());
+            int op = random().nextInt(5);
+            if (op == 0) {
+                int shift = 100 - randomIntBetween(0, 200);
+                pos = Math.min(indexInput.length() - 1, Math.max(0, pos + shift));
+                indexInput.seek(pos);
+            } else if (op == 1) {
+                indexInput.readByte();
+                pos++;
+            } else {
+                int min = (int) Math.min(indexInput.length() - pos, ref.bytes.length);
+                indexInput.readBytes(ref.bytes, ref.offset, min);
+                pos += min;
+            }
+        }
+    }
+
+    private void corruptFile(Directory dir, String fileIn, String fileOut) throws IOException {
+        IndexInput input = dir.openInput(fileIn, IOContext.READONCE);
+        IndexOutput output = dir.createOutput(fileOut, IOContext.DEFAULT);
+        long len = input.length();
+        byte[] b = new byte[1024];
+        long broken = randomInt((int) len-1);
+        long pos = 0;
+        while (pos < len) {
+            int min = (int) Math.min(input.length() - pos, b.length);
+            input.readBytes(b, 0, min);
+            if (broken >= pos && broken < pos + min) {
+                // Flip one byte
+                int flipPos = (int) (broken - pos);
+                b[flipPos] = (byte) (b[flipPos] ^ 42);
+            }
+            output.writeBytes(b, min);
+            pos += min;
+        }
+        IOUtils.close(input, output);
+
+    }
+
+    public void assertDeleteContent(Store store, DirectoryService service) throws IOException {
+        deleteContent(store.directory());
+        assertThat(Arrays.toString(store.directory().listAll()), store.directory().listAll().length, equalTo(0));
+        assertThat(store.stats().sizeInBytes(), equalTo(0l));
+        assertThat(service.newDirectory().listAll().length, equalTo(0));
+    }
+
+    private static final class LuceneManagedDirectoryService extends DirectoryService {
+        private final Directory dir;
+        private final Random random;
+
+        public LuceneManagedDirectoryService(Random random) {
+            this(random, true);
+        }
+
+        public LuceneManagedDirectoryService(Random random, boolean preventDoubleWrite) {
+            super(new ShardId("fake", 1), Settings.EMPTY);
+            dir = StoreTests.newDirectory(random);
+            if (dir instanceof MockDirectoryWrapper) {
+                ((MockDirectoryWrapper) dir).setPreventDoubleWrite(preventDoubleWrite);
+                // TODO: fix this test to handle virus checker
+                ((MockDirectoryWrapper) dir).setEnableVirusScanner(false);
+            }
+            this.random = random;
+        }
+
+        @Override
+        public Directory newDirectory() throws IOException {
+            return dir;
+        }
+
+        @Override
+        public long throttleTimeInNanos() {
+            return random.nextInt(1000);
+        }
+    }
+
+    public static void assertConsistent(Store store, Store.MetadataSnapshot metadata) throws IOException {
+        for (String file : store.directory().listAll()) {
+            if (!IndexWriter.WRITE_LOCK_NAME.equals(file) && !IndexFileNames.OLD_SEGMENTS_GEN.equals(file) && !Store.isChecksum(file) && file.startsWith("extra") == false) {
+                assertTrue(file + " is not in the map: " + metadata.asMap().size() + " vs. " + store.directory().listAll().length, metadata.asMap().containsKey(file));
+            } else {
+                assertFalse(file + " is not in the map: " + metadata.asMap().size() + " vs. " + store.directory().listAll().length, metadata.asMap().containsKey(file));
+            }
+        }
+    }
+
+    /**
+     * Legacy indices without lucene CRC32 did never write or calculate checksums for segments_N files
+     * but for other files
+     */
+    @Test
+    public void testRecoveryDiffWithLegacyCommit() {
+        Map<String, StoreFileMetaData> metaDataMap = new HashMap<>();
+        metaDataMap.put("segments_1", new StoreFileMetaData("segments_1", 50, null, null, new BytesRef(new byte[]{1})));
+        metaDataMap.put("_0_1.del", new StoreFileMetaData("_0_1.del", 42, "foobarbaz", null, new BytesRef()));
+        Store.MetadataSnapshot first = new Store.MetadataSnapshot(metaDataMap, Collections.EMPTY_MAP, 0);
+
+        Store.MetadataSnapshot second = new Store.MetadataSnapshot(metaDataMap, Collections.EMPTY_MAP, 0);
+        Store.RecoveryDiff recoveryDiff = first.recoveryDiff(second);
+        assertEquals(recoveryDiff.toString(), recoveryDiff.different.size(), 2);
+    }
+
+
+    @Test
+    public void testRecoveryDiff() throws IOException, InterruptedException {
+        int numDocs = 2 + random().nextInt(100);
+        List<Document> docs = new ArrayList<>();
+        for (int i = 0; i < numDocs; i++) {
+            Document doc = new Document();
+            doc.add(new StringField("id", "" + i, random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+            doc.add(new TextField("body", TestUtil.randomRealisticUnicodeString(random()), random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+            doc.add(new SortedDocValuesField("dv", new BytesRef(TestUtil.randomRealisticUnicodeString(random()))));
+            docs.add(doc);
+        }
+        long seed = random().nextLong();
+        Store.MetadataSnapshot first;
+        {
+            Random random = new Random(seed);
+            IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random)).setCodec(TestUtil.getDefaultCodec());
+            iwc.setMergePolicy(NoMergePolicy.INSTANCE);
+            iwc.setUseCompoundFile(random.nextBoolean());
+            final ShardId shardId = new ShardId(new Index("index"), 1);
+            DirectoryService directoryService = new LuceneManagedDirectoryService(random);
+            Store store = new Store(shardId, Settings.EMPTY, directoryService, new DummyShardLock(shardId));
+            IndexWriter writer = new IndexWriter(store.directory(), iwc);
+            final boolean lotsOfSegments = rarely(random);
+            for (Document d : docs) {
+                writer.addDocument(d);
+                if (lotsOfSegments && random.nextBoolean()) {
+                    writer.commit();
+                } else if (rarely(random)) {
+                    writer.commit();
+                }
+            }
+            writer.commit();
+            writer.close();
+            first = store.getMetadata();
+            assertDeleteContent(store, directoryService);
+            store.close();
+        }
+        long time = new Date().getTime();
+        while (time == new Date().getTime()) {
+            Thread.sleep(10); // bump the time
+        }
+        Store.MetadataSnapshot second;
+        Store store;
+        {
+            Random random = new Random(seed);
+            IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random)).setCodec(TestUtil.getDefaultCodec());
+            iwc.setMergePolicy(NoMergePolicy.INSTANCE);
+            iwc.setUseCompoundFile(random.nextBoolean());
+            final ShardId shardId = new ShardId(new Index("index"), 1);
+            DirectoryService directoryService = new LuceneManagedDirectoryService(random);
+            store = new Store(shardId, Settings.EMPTY, directoryService, new DummyShardLock(shardId));
+            IndexWriter writer = new IndexWriter(store.directory(), iwc);
+            final boolean lotsOfSegments = rarely(random);
+            for (Document d : docs) {
+                writer.addDocument(d);
+                if (lotsOfSegments && random.nextBoolean()) {
+                    writer.commit();
+                } else if (rarely(random)) {
+                    writer.commit();
+                }
+            }
+            writer.commit();
+            writer.close();
+            second = store.getMetadata();
+        }
+        Store.RecoveryDiff diff = first.recoveryDiff(second);
+        assertThat(first.size(), equalTo(second.size()));
+        for (StoreFileMetaData md : first) {
+            assertThat(second.get(md.name()), notNullValue());
+            // si files are different - containing timestamps etc
+            assertThat(second.get(md.name()).isSame(md), equalTo(false));
+        }
+        assertThat(diff.different.size(), equalTo(first.size()));
+        assertThat(diff.identical.size(), equalTo(0)); // in lucene 5 nothing is identical - we use random ids in file headers
+        assertThat(diff.missing, empty());
+
+        // check the self diff
+        Store.RecoveryDiff selfDiff = first.recoveryDiff(first);
+        assertThat(selfDiff.identical.size(), equalTo(first.size()));
+        assertThat(selfDiff.different, empty());
+        assertThat(selfDiff.missing, empty());
+
+
+        // lets add some deletes
+        Random random = new Random(seed);
+        IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random)).setCodec(TestUtil.getDefaultCodec());
+        iwc.setMergePolicy(NoMergePolicy.INSTANCE);
+        iwc.setUseCompoundFile(random.nextBoolean());
+        iwc.setOpenMode(IndexWriterConfig.OpenMode.APPEND);
+        IndexWriter writer = new IndexWriter(store.directory(), iwc);
+        writer.deleteDocuments(new Term("id", Integer.toString(random().nextInt(numDocs))));
+        writer.commit();
+        writer.close();
+        Store.MetadataSnapshot metadata = store.getMetadata();
+        StoreFileMetaData delFile = null;
+        for (StoreFileMetaData md : metadata) {
+            if (md.name().endsWith(".liv")) {
+                delFile = md;
+                break;
+            }
+        }
+        Store.RecoveryDiff afterDeleteDiff = metadata.recoveryDiff(second);
+        if (delFile != null) {
+            assertThat(afterDeleteDiff.identical.size(), equalTo(metadata.size() - 2)); // segments_N + del file
+            assertThat(afterDeleteDiff.different.size(), equalTo(0));
+            assertThat(afterDeleteDiff.missing.size(), equalTo(2));
+        } else {
+            // an entire segment must be missing (single doc segment got dropped)
+            assertThat(afterDeleteDiff.identical.size(), greaterThan(0));
+            assertThat(afterDeleteDiff.different.size(), equalTo(0));
+            assertThat(afterDeleteDiff.missing.size(), equalTo(1)); // the commit file is different
+        }
+
+        // check the self diff
+        selfDiff = metadata.recoveryDiff(metadata);
+        assertThat(selfDiff.identical.size(), equalTo(metadata.size()));
+        assertThat(selfDiff.different, empty());
+        assertThat(selfDiff.missing, empty());
+
+        // add a new commit
+        iwc = new IndexWriterConfig(new MockAnalyzer(random)).setCodec(TestUtil.getDefaultCodec());
+        iwc.setMergePolicy(NoMergePolicy.INSTANCE);
+        iwc.setUseCompoundFile(true); // force CFS - easier to test here since we know it will add 3 files
+        iwc.setOpenMode(IndexWriterConfig.OpenMode.APPEND);
+        writer = new IndexWriter(store.directory(), iwc);
+        writer.addDocument(docs.get(0));
+        writer.close();
+
+        Store.MetadataSnapshot newCommitMetaData = store.getMetadata();
+        Store.RecoveryDiff newCommitDiff = newCommitMetaData.recoveryDiff(metadata);
+        if (delFile != null) {
+            assertThat(newCommitDiff.identical.size(), equalTo(newCommitMetaData.size() - 5)); // segments_N, del file, cfs, cfe, si for the new segment
+            assertThat(newCommitDiff.different.size(), equalTo(1)); // the del file must be different
+            assertThat(newCommitDiff.different.get(0).name(), endsWith(".liv"));
+            assertThat(newCommitDiff.missing.size(), equalTo(4)); // segments_N,cfs, cfe, si for the new segment
+        } else {
+            assertThat(newCommitDiff.identical.size(), equalTo(newCommitMetaData.size() - 4)); // segments_N, cfs, cfe, si for the new segment
+            assertThat(newCommitDiff.different.size(), equalTo(0));
+            assertThat(newCommitDiff.missing.size(), equalTo(4)); // an entire segment must be missing (single doc segment got dropped)  plus the commit is different
+        }
+
+        deleteContent(store.directory());
+        IOUtils.close(store);
+    }
+
+    @Test
+    public void testCleanupFromSnapshot() throws IOException {
+        final ShardId shardId = new ShardId(new Index("index"), 1);
+        DirectoryService directoryService = new LuceneManagedDirectoryService(random());
+        Store store = new Store(shardId, Settings.EMPTY, directoryService, new DummyShardLock(shardId));
+        // this time random codec....
+        IndexWriterConfig indexWriterConfig = newIndexWriterConfig(random(), new MockAnalyzer(random())).setCodec(TestUtil.getDefaultCodec());
+        // we keep all commits and that allows us clean based on multiple snapshots
+        indexWriterConfig.setIndexDeletionPolicy(NoDeletionPolicy.INSTANCE);
+        IndexWriter writer = new IndexWriter(store.directory(), indexWriterConfig);
+        int docs = 1 + random().nextInt(100);
+        int numCommits = 0;
+        for (int i = 0; i < docs; i++) {
+            if (i > 0 && randomIntBetween(0, 10) == 0) {
+                writer.commit();
+                numCommits++;
+            }
+            Document doc = new Document();
+            doc.add(new TextField("id", "" + i, random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+            doc.add(new TextField("body", TestUtil.randomRealisticUnicodeString(random()), random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+            doc.add(new SortedDocValuesField("dv", new BytesRef(TestUtil.randomRealisticUnicodeString(random()))));
+            writer.addDocument(doc);
+
+        }
+        if (numCommits < 1) {
+            writer.commit();
+            Document doc = new Document();
+            doc.add(new TextField("id", "" + docs++, random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+            doc.add(new TextField("body", TestUtil.randomRealisticUnicodeString(random()), random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+            doc.add(new SortedDocValuesField("dv", new BytesRef(TestUtil.randomRealisticUnicodeString(random()))));
+            writer.addDocument(doc);
+        }
+
+        Store.MetadataSnapshot firstMeta = store.getMetadata();
+
+        if (random().nextBoolean()) {
+            for (int i = 0; i < docs; i++) {
+                if (random().nextBoolean()) {
+                    Document doc = new Document();
+                    doc.add(new TextField("id", "" + i, random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+                    doc.add(new TextField("body", TestUtil.randomRealisticUnicodeString(random()), random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+                    writer.updateDocument(new Term("id", "" + i), doc);
+                }
+            }
+        }
+        writer.commit();
+        writer.close();
+
+        Store.MetadataSnapshot secondMeta = store.getMetadata();
+
+        Store.LegacyChecksums checksums = new Store.LegacyChecksums();
+        Map<String, StoreFileMetaData> legacyMeta = new HashMap<>();
+        for (String file : store.directory().listAll()) {
+            if (file.equals("write.lock") || file.equals(IndexFileNames.OLD_SEGMENTS_GEN) || file.startsWith("extra")) {
+                continue;
+            }
+            BytesRef hash = new BytesRef();
+            if (file.startsWith("segments")) {
+                hash = Store.MetadataSnapshot.hashFile(store.directory(), file);
+            }
+            StoreFileMetaData storeFileMetaData = new StoreFileMetaData(file, store.directory().fileLength(file), file + "checksum", null, hash);
+            legacyMeta.put(file, storeFileMetaData);
+            checksums.add(storeFileMetaData);
+        }
+        checksums.write(store); // write one checksum file here - we expect it to survive all the cleanups
+
+        if (randomBoolean()) {
+            store.cleanupAndVerify("test", firstMeta);
+            String[] strings = store.directory().listAll();
+            int numChecksums = 0;
+            int numNotFound = 0;
+            for (String file : strings) {
+                if (file.startsWith("extra")) {
+                    continue;
+                }
+                assertTrue(firstMeta.contains(file) || Store.isChecksum(file) || file.equals("write.lock"));
+                if (Store.isChecksum(file)) {
+                    numChecksums++;
+                } else if (secondMeta.contains(file) == false) {
+                    numNotFound++;
+                }
+
+            }
+            assertTrue("at least one file must not be in here since we have two commits?", numNotFound > 0);
+            assertEquals("we wrote one checksum but it's gone now? - checksums are supposed to be kept", numChecksums, 1);
+        } else {
+            store.cleanupAndVerify("test", secondMeta);
+            String[] strings = store.directory().listAll();
+            int numChecksums = 0;
+            int numNotFound = 0;
+            for (String file : strings) {
+                if (file.startsWith("extra")) {
+                    continue;
+                }
+                assertTrue(file, secondMeta.contains(file) || Store.isChecksum(file) || file.equals("write.lock"));
+                if (Store.isChecksum(file)) {
+                    numChecksums++;
+                } else if (firstMeta.contains(file) == false) {
+                    numNotFound++;
+                }
+
+            }
+            assertTrue("at least one file must not be in here since we have two commits?", numNotFound > 0);
+            assertEquals("we wrote one checksum but it's gone now? - checksums are supposed to be kept", numChecksums, 1);
+        }
+
+        deleteContent(store.directory());
+        IOUtils.close(store);
+    }
+
+    @Test
+    public void testCleanUpWithLegacyChecksums() throws IOException {
+        Map<String, StoreFileMetaData> metaDataMap = new HashMap<>();
+        metaDataMap.put("segments_1", new StoreFileMetaData("segments_1", 50, null, null, new BytesRef(new byte[]{1})));
+        metaDataMap.put("_0_1.del", new StoreFileMetaData("_0_1.del", 42, "foobarbaz", null, new BytesRef()));
+        Store.MetadataSnapshot snapshot = new Store.MetadataSnapshot(metaDataMap, Collections.EMPTY_MAP, 0);
+
+        final ShardId shardId = new ShardId(new Index("index"), 1);
+        DirectoryService directoryService = new LuceneManagedDirectoryService(random());
+        Store store = new Store(shardId, Settings.EMPTY, directoryService, new DummyShardLock(shardId));
+        for (String file : metaDataMap.keySet()) {
+            try (IndexOutput output = store.directory().createOutput(file, IOContext.DEFAULT)) {
+                BytesRef bytesRef = new BytesRef(TestUtil.randomRealisticUnicodeString(random(), 10, 1024));
+                output.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
+                CodecUtil.writeFooter(output);
+            }
+        }
+
+        store.verifyAfterCleanup(snapshot, snapshot);
+        deleteContent(store.directory());
+        IOUtils.close(store);
+    }
+
+    public void testOnCloseCallback() throws IOException {
+        final ShardId shardId = new ShardId(new Index(randomRealisticUnicodeOfCodepointLengthBetween(1, 10)), randomIntBetween(0, 100));
+        DirectoryService directoryService = new LuceneManagedDirectoryService(random());
+        final AtomicInteger count = new AtomicInteger(0);
+        final ShardLock lock = new DummyShardLock(shardId);
+
+        Store store = new Store(shardId, Settings.EMPTY, directoryService, lock, new Store.OnClose() {
+            @Override
+            public void handle(ShardLock theLock) {
+                assertEquals(shardId, theLock.getShardId());
+                assertEquals(lock, theLock);
+                count.incrementAndGet();
+            }
+        });
+        assertEquals(count.get(), 0);
+
+        final int iters = randomIntBetween(1, 10);
+        for (int i = 0; i < iters; i++) {
+            store.close();
+        }
+
+        assertEquals(count.get(), 1);
+    }
+
+    @Test
+    public void testStoreStats() throws IOException {
+        final ShardId shardId = new ShardId(new Index("index"), 1);
+        DirectoryService directoryService = new LuceneManagedDirectoryService(random());
+        Settings settings = Settings.builder().put(Store.INDEX_STORE_STATS_REFRESH_INTERVAL, TimeValue.timeValueMinutes(0)).build();
+        Store store = new Store(shardId, settings, directoryService, new DummyShardLock(shardId));
+        long initialStoreSize = 0;
+        for (String extraFiles : store.directory().listAll()) {
+            assertTrue("expected extraFS file but got: " + extraFiles, extraFiles.startsWith("extra"));
+            initialStoreSize += store.directory().fileLength(extraFiles);
+        }
+        StoreStats stats = store.stats();
+        assertEquals(stats.getSize().bytes(), initialStoreSize);
+
+        Directory dir = store.directory();
+        final long length;
+        try (IndexOutput output = dir.createOutput("foo.bar", IOContext.DEFAULT)) {
+            int iters = scaledRandomIntBetween(10, 100);
+            for (int i = 0; i < iters; i++) {
+                BytesRef bytesRef = new BytesRef(TestUtil.randomRealisticUnicodeString(random(), 10, 1024));
+                output.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
+            }
+            length = output.getFilePointer();
+        }
+
+        assertTrue(numNonExtraFiles(store) > 0);
+        stats = store.stats();
+        assertEquals(stats.getSizeInBytes(), length + initialStoreSize);
+
+        deleteContent(store.directory());
+        IOUtils.close(store);
+    }
+
+
+    public static void deleteContent(Directory directory) throws IOException {
+        final String[] files = directory.listAll();
+        final List<IOException> exceptions = new ArrayList<>();
+        for (String file : files) {
+            try {
+                directory.deleteFile(file);
+            } catch (NoSuchFileException | FileNotFoundException e) {
+                // ignore
+            } catch (IOException e) {
+                exceptions.add(e);
+            }
+        }
+        ExceptionsHelper.rethrowAndSuppress(exceptions);
+    }
+
+    public int numNonExtraFiles(Store store) throws IOException {
+        int numNonExtra = 0;
+        for (String file : store.directory().listAll()) {
+            if (file.startsWith("extra") == false) {
+                numNonExtra++;
+            }
+        }
+        return numNonExtra;
+    }
+
+    @Test
+    public void testMetadataSnapshotStreaming() throws Exception {
+
+        Store.MetadataSnapshot outMetadataSnapshot = createMetaDataSnapshot();
+        org.elasticsearch.Version targetNodeVersion = randomVersion(random());
+
+        ByteArrayOutputStream outBuffer = new ByteArrayOutputStream();
+        OutputStreamStreamOutput out = new OutputStreamStreamOutput(outBuffer);
+        out.setVersion(targetNodeVersion);
+        outMetadataSnapshot.writeTo(out);
+
+        ByteArrayInputStream inBuffer = new ByteArrayInputStream(outBuffer.toByteArray());
+        InputStreamStreamInput in = new InputStreamStreamInput(inBuffer);
+        in.setVersion(targetNodeVersion);
+        Store.MetadataSnapshot inMetadataSnapshot = new Store.MetadataSnapshot(in);
+        Map<String, StoreFileMetaData> origEntries = new HashMap<>();
+        origEntries.putAll(outMetadataSnapshot.asMap());
+        for (Map.Entry<String, StoreFileMetaData> entry : inMetadataSnapshot.asMap().entrySet()) {
+            assertThat(entry.getValue().name(), equalTo(origEntries.remove(entry.getKey()).name()));
+        }
+        assertThat(origEntries.size(), equalTo(0));
+        assertThat(inMetadataSnapshot.getCommitUserData(), equalTo(outMetadataSnapshot.getCommitUserData()));
+    }
+
+    protected Store.MetadataSnapshot createMetaDataSnapshot() {
+        StoreFileMetaData storeFileMetaData1 = new StoreFileMetaData("segments", 1);
+        StoreFileMetaData storeFileMetaData2 = new StoreFileMetaData("no_segments", 1);
+        Map<String, StoreFileMetaData> storeFileMetaDataMap = new HashMap<>();
+        storeFileMetaDataMap.put(storeFileMetaData1.name(), storeFileMetaData1);
+        storeFileMetaDataMap.put(storeFileMetaData2.name(), storeFileMetaData2);
+        Map<String, String> commitUserData = new HashMap<>();
+        commitUserData.put("userdata_1", "test");
+        commitUserData.put("userdata_2", "test");
+        return new Store.MetadataSnapshot(storeFileMetaDataMap, commitUserData, 0);
+    }
+
+    @Test
+    public void testUserDataRead() throws IOException {
+        final ShardId shardId = new ShardId(new Index("index"), 1);
+        DirectoryService directoryService = new LuceneManagedDirectoryService(random());
+        Store store = new Store(shardId, Settings.EMPTY, directoryService, new DummyShardLock(shardId));
+        IndexWriterConfig config = newIndexWriterConfig(random(), new MockAnalyzer(random())).setCodec(TestUtil.getDefaultCodec());
+        SnapshotDeletionPolicy deletionPolicy = new SnapshotDeletionPolicy(new KeepOnlyLastDeletionPolicy(shardId, EMPTY_SETTINGS));
+        config.setIndexDeletionPolicy(deletionPolicy);
+        IndexWriter writer = new IndexWriter(store.directory(), config);
+        Document doc = new Document();
+        doc.add(new TextField("id", "1", Field.Store.NO));
+        writer.addDocument(doc);
+        Map<String, String> commitData = new HashMap<>(2);
+        String syncId = "a sync id";
+        String translogId = "a translog id";
+        commitData.put(Engine.SYNC_COMMIT_ID, syncId);
+        commitData.put(Translog.TRANSLOG_GENERATION_KEY, translogId);
+        writer.setCommitData(commitData);
+        writer.commit();
+        writer.close();
+        Store.MetadataSnapshot metadata;
+        if (randomBoolean()) {
+            metadata = store.getMetadata();
+        } else {
+            metadata = store.getMetadata(deletionPolicy.snapshot());
+        }
+        assertFalse(metadata.asMap().isEmpty());
+        // do not check for correct files, we have enough tests for that above
+        assertThat(metadata.getCommitUserData().get(Engine.SYNC_COMMIT_ID), equalTo(syncId));
+        assertThat(metadata.getCommitUserData().get(Translog.TRANSLOG_GENERATION_KEY), equalTo(translogId));
+        TestUtil.checkIndex(store.directory());
+        assertDeleteContent(store, directoryService);
+        IOUtils.close(store);
+    }
+
+    @Test
+    public void testStreamStoreFilesMetaData() throws Exception {
+        Store.MetadataSnapshot metadataSnapshot = createMetaDataSnapshot();
+        TransportNodesListShardStoreMetaData.StoreFilesMetaData outStoreFileMetaData = new TransportNodesListShardStoreMetaData.StoreFilesMetaData(randomBoolean(), new ShardId("test", 0),metadataSnapshot);
+        ByteArrayOutputStream outBuffer = new ByteArrayOutputStream();
+        OutputStreamStreamOutput out = new OutputStreamStreamOutput(outBuffer);
+        org.elasticsearch.Version targetNodeVersion = randomVersion(random());
+        out.setVersion(targetNodeVersion);
+        outStoreFileMetaData.writeTo(out);
+        ByteArrayInputStream inBuffer = new ByteArrayInputStream(outBuffer.toByteArray());
+        InputStreamStreamInput in = new InputStreamStreamInput(inBuffer);
+        in.setVersion(targetNodeVersion);
+        TransportNodesListShardStoreMetaData.StoreFilesMetaData inStoreFileMetaData = TransportNodesListShardStoreMetaData.StoreFilesMetaData.readStoreFilesMetaData(in);
+        Iterator<StoreFileMetaData> outFiles = outStoreFileMetaData.iterator();
+        for (StoreFileMetaData inFile : inStoreFileMetaData) {
+            assertThat(inFile.name(), equalTo(outFiles.next().name()));
+        }
+        assertThat(outStoreFileMetaData.syncId(), equalTo(inStoreFileMetaData.syncId()));
+    }
+
+    public void testMarkCorruptedOnTruncatedSegmentsFile() throws IOException {
+        IndexWriterConfig iwc = newIndexWriterConfig();
+        final ShardId shardId = new ShardId(new Index("index"), 1);
+        DirectoryService directoryService = new LuceneManagedDirectoryService(random());
+        Store store = new Store(shardId, Settings.EMPTY, directoryService, new DummyShardLock(shardId));
+        IndexWriter writer = new IndexWriter(store.directory(), iwc);
+
+        int numDocs = 1 + random().nextInt(10);
+        List<Document> docs = new ArrayList<>();
+        for (int i = 0; i < numDocs; i++) {
+            Document doc = new Document();
+            doc.add(new StringField("id", "" + i, random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+            doc.add(new TextField("body", TestUtil.randomRealisticUnicodeString(random()), random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+            doc.add(new SortedDocValuesField("dv", new BytesRef(TestUtil.randomRealisticUnicodeString(random()))));
+            docs.add(doc);
+        }
+        for (Document d : docs) {
+            writer.addDocument(d);
+        }
+        writer.commit();
+        writer.close();
+        MockDirectoryWrapper leaf = DirectoryUtils.getLeaf(store.directory(), MockDirectoryWrapper.class);
+        if (leaf != null) {
+            leaf.setPreventDoubleWrite(false); // I do this on purpose
+        }
+        SegmentInfos segmentCommitInfos = store.readLastCommittedSegmentsInfo();
+        try (IndexOutput out = store.directory().createOutput(segmentCommitInfos.getSegmentsFileName(), IOContext.DEFAULT)) {
+            // empty file
+        }
+
+        try {
+            if (randomBoolean()) {
+                store.getMetadata();
+            } else {
+                store.readLastCommittedSegmentsInfo();
+            }
+            fail("corrupted segments_N file");
+        } catch (CorruptIndexException ex) {
+            // expected
+        }
+        assertTrue(store.isMarkedCorrupted());
+        Lucene.cleanLuceneIndex(store.directory()); // we have to remove the index since it's corrupted and might fail the MocKDirWrapper checkindex call
+        store.close();
+    }
+
+    public void testCanOpenIndex() throws IOException {
+        IndexWriterConfig iwc = newIndexWriterConfig();
+        Path tempDir = createTempDir();
+        final BaseDirectoryWrapper dir = newFSDirectory(tempDir);
+        assertFalse(Store.canOpenIndex(logger, tempDir));
+        IndexWriter writer = new IndexWriter(dir, iwc);
+        Document doc = new Document();
+        doc.add(new StringField("id", "1", random().nextBoolean() ? Field.Store.YES : Field.Store.NO));
+        writer.addDocument(doc);
+        writer.commit();
+        writer.close();
+        assertTrue(Store.canOpenIndex(logger, tempDir));
+
+        final ShardId shardId = new ShardId(new Index("index"), 1);
+        DirectoryService directoryService = new DirectoryService(shardId, Settings.EMPTY) {
+            @Override
+            public long throttleTimeInNanos() {
+                return 0;
+            }
+
+            @Override
+            public Directory newDirectory() throws IOException {
+                return dir;
+            }
+        };
+        Store store = new Store(shardId, Settings.EMPTY, directoryService, new DummyShardLock(shardId));
+        store.markStoreCorrupted(new CorruptIndexException("foo", "bar"));
+        assertFalse(Store.canOpenIndex(logger, tempDir));
+        store.close();
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/indices/IndicesModuleTests.java b/core/src/test/java/org/elasticsearch/indices/IndicesModuleTests.java
index fcba11a..7727d8c 100644
--- a/core/src/test/java/org/elasticsearch/indices/IndicesModuleTests.java
+++ b/core/src/test/java/org/elasticsearch/indices/IndicesModuleTests.java
@@ -23,7 +23,10 @@ import org.apache.lucene.analysis.hunspell.Dictionary;
 import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.ModuleTestCase;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.query.*;
+import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.query.QueryParser;
+import org.elasticsearch.index.query.QueryParsingException;
+import org.elasticsearch.index.query.TermQueryParser;
 
 import java.io.IOException;
 import java.io.InputStream;
@@ -36,19 +39,8 @@ public class IndicesModuleTests extends ModuleTestCase {
         public String[] names() {
             return new String[] {"fake-query-parser"};
         }
-
-        @Override
-        public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
-            return null;
-        }
-
-        @Override
-        public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-            return null;
-        }
-
         @Override
-        public QueryBuilder getBuilderPrototype() {
+        public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
             return null;
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/indices/IndicesServiceTest.java b/core/src/test/java/org/elasticsearch/indices/IndicesServiceTest.java
deleted file mode 100644
index 89f1290..0000000
--- a/core/src/test/java/org/elasticsearch/indices/IndicesServiceTest.java
+++ /dev/null
@@ -1,169 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.indices;
-
-import org.apache.lucene.store.LockObtainFailedException;
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.env.NodeEnvironment;
-import org.elasticsearch.gateway.GatewayMetaState;
-import org.elasticsearch.index.IndexService;
-import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.index.shard.ShardPath;
-import org.elasticsearch.test.ESSingleNodeTestCase;
-
-import java.io.IOException;
-import java.util.concurrent.TimeUnit;
-
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-
-public class IndicesServiceTest extends ESSingleNodeTestCase {
-
-    public IndicesService getIndicesService() {
-        return getInstanceFromNode(IndicesService.class);
-    }
-    public NodeEnvironment getNodeEnvironment() {
-        return getInstanceFromNode(NodeEnvironment.class);
-    }
-
-    @Override
-    protected boolean resetNodeAfterTest() {
-        return true;
-    }
-
-    public void testCanDeleteShardContent() {
-        IndicesService indicesService = getIndicesService();
-        IndexMetaData meta = IndexMetaData.builder("test").settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(
-                1).build();
-        assertFalse("no shard location", indicesService.canDeleteShardContent(new ShardId("test", 0), meta));
-        IndexService test = createIndex("test");
-        assertTrue(test.hasShard(0));
-        assertFalse("shard is allocated", indicesService.canDeleteShardContent(new ShardId("test", 0), meta));
-        test.removeShard(0, "boom");
-        assertTrue("shard is removed", indicesService.canDeleteShardContent(new ShardId("test", 0), meta));
-    }
-
-    public void testDeleteIndexStore() throws Exception {
-        IndicesService indicesService = getIndicesService();
-        IndexService test = createIndex("test");
-        ClusterService clusterService = getInstanceFromNode(ClusterService.class);
-        IndexMetaData firstMetaData = clusterService.state().metaData().index("test");
-        assertTrue(test.hasShard(0));
-
-        try {
-            indicesService.deleteIndexStore("boom", firstMetaData, clusterService.state());
-            fail();
-        } catch (IllegalStateException ex) {
-            // all good
-        }
-
-        GatewayMetaState gwMetaState = getInstanceFromNode(GatewayMetaState.class);
-        MetaData meta = gwMetaState.loadMetaState();
-        assertNotNull(meta);
-        assertNotNull(meta.index("test"));
-        assertAcked(client().admin().indices().prepareDelete("test"));
-
-        meta = gwMetaState.loadMetaState();
-        assertNotNull(meta);
-        assertNull(meta.index("test"));
-
-
-        test = createIndex("test");
-        client().prepareIndex("test", "type", "1").setSource("field", "value").setRefresh(true).get();
-        client().admin().indices().prepareFlush("test").get();
-        assertHitCount(client().prepareSearch("test").get(), 1);
-        IndexMetaData secondMetaData = clusterService.state().metaData().index("test");
-        assertAcked(client().admin().indices().prepareClose("test"));
-        ShardPath path = ShardPath.loadShardPath(logger, getNodeEnvironment(), new ShardId(test.index(), 0), test.getIndexSettings());
-        assertTrue(path.exists());
-
-        try {
-            indicesService.deleteIndexStore("boom", secondMetaData, clusterService.state());
-            fail();
-        } catch (IllegalStateException ex) {
-            // all good
-        }
-
-        assertTrue(path.exists());
-
-        // now delete the old one and make sure we resolve against the name
-        try {
-            indicesService.deleteIndexStore("boom", firstMetaData, clusterService.state());
-            fail();
-        } catch (IllegalStateException ex) {
-            // all good
-        }
-        assertAcked(client().admin().indices().prepareOpen("test"));
-        ensureGreen("test");
-    }
-
-    public void testPendingTasks() throws IOException {
-        IndicesService indicesService = getIndicesService();
-        IndexService test = createIndex("test");
-
-        assertTrue(test.hasShard(0));
-        ShardPath path = test.shard(0).shardPath();
-        assertTrue(test.shard(0).routingEntry().started());
-        ShardPath shardPath = ShardPath.loadShardPath(logger, getNodeEnvironment(), new ShardId(test.index(), 0), test.getIndexSettings());
-        assertEquals(shardPath, path);
-        try {
-            indicesService.processPendingDeletes(test.index(), test.getIndexSettings(), new TimeValue(0, TimeUnit.MILLISECONDS));
-            fail("can't get lock");
-        } catch (LockObtainFailedException ex) {
-
-        }
-        assertTrue(path.exists());
-
-        int numPending = 1;
-        if (randomBoolean()) {
-            indicesService.addPendingDelete(new ShardId(test.index(), 0), test.getIndexSettings());
-        } else {
-            if (randomBoolean()) {
-                numPending++;
-                indicesService.addPendingDelete(new ShardId(test.index(), 0), test.getIndexSettings());
-            }
-            indicesService.addPendingDelete(test.index(), test.getIndexSettings());
-        }
-        assertAcked(client().admin().indices().prepareClose("test"));
-        assertTrue(path.exists());
-
-        assertEquals(indicesService.numPendingDeletes(test.index()), numPending);
-
-        // shard lock released... we can now delete
-        indicesService.processPendingDeletes(test.index(), test.getIndexSettings(), new TimeValue(0, TimeUnit.MILLISECONDS));
-        assertEquals(indicesService.numPendingDeletes(test.index()), 0);
-        assertFalse(path.exists());
-
-        if (randomBoolean()) {
-            indicesService.addPendingDelete(new ShardId(test.index(), 0), test.getIndexSettings());
-            indicesService.addPendingDelete(new ShardId(test.index(), 1), test.getIndexSettings());
-            indicesService.addPendingDelete(new ShardId("bogus", 1), test.getIndexSettings());
-            assertEquals(indicesService.numPendingDeletes(test.index()), 2);
-            // shard lock released... we can now delete
-            indicesService.processPendingDeletes(test.index(),  test.getIndexSettings(), new TimeValue(0, TimeUnit.MILLISECONDS));
-            assertEquals(indicesService.numPendingDeletes(test.index()), 0);
-        }
-        assertAcked(client().admin().indices().prepareOpen("test"));
-
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/indices/IndicesServiceTests.java b/core/src/test/java/org/elasticsearch/indices/IndicesServiceTests.java
new file mode 100644
index 0000000..08c3338
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/indices/IndicesServiceTests.java
@@ -0,0 +1,169 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.indices;
+
+import org.apache.lucene.store.LockObtainFailedException;
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.cluster.metadata.MetaData;
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.env.NodeEnvironment;
+import org.elasticsearch.gateway.GatewayMetaState;
+import org.elasticsearch.index.IndexService;
+import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.index.shard.ShardPath;
+import org.elasticsearch.test.ESSingleNodeTestCase;
+
+import java.io.IOException;
+import java.util.concurrent.TimeUnit;
+
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
+
+public class IndicesServiceTests extends ESSingleNodeTestCase {
+
+    public IndicesService getIndicesService() {
+        return getInstanceFromNode(IndicesService.class);
+    }
+    public NodeEnvironment getNodeEnvironment() {
+        return getInstanceFromNode(NodeEnvironment.class);
+    }
+
+    @Override
+    protected boolean resetNodeAfterTest() {
+        return true;
+    }
+
+    public void testCanDeleteShardContent() {
+        IndicesService indicesService = getIndicesService();
+        IndexMetaData meta = IndexMetaData.builder("test").settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(
+                1).build();
+        assertFalse("no shard location", indicesService.canDeleteShardContent(new ShardId("test", 0), meta));
+        IndexService test = createIndex("test");
+        assertTrue(test.hasShard(0));
+        assertFalse("shard is allocated", indicesService.canDeleteShardContent(new ShardId("test", 0), meta));
+        test.removeShard(0, "boom");
+        assertTrue("shard is removed", indicesService.canDeleteShardContent(new ShardId("test", 0), meta));
+    }
+
+    public void testDeleteIndexStore() throws Exception {
+        IndicesService indicesService = getIndicesService();
+        IndexService test = createIndex("test");
+        ClusterService clusterService = getInstanceFromNode(ClusterService.class);
+        IndexMetaData firstMetaData = clusterService.state().metaData().index("test");
+        assertTrue(test.hasShard(0));
+
+        try {
+            indicesService.deleteIndexStore("boom", firstMetaData, clusterService.state());
+            fail();
+        } catch (IllegalStateException ex) {
+            // all good
+        }
+
+        GatewayMetaState gwMetaState = getInstanceFromNode(GatewayMetaState.class);
+        MetaData meta = gwMetaState.loadMetaState();
+        assertNotNull(meta);
+        assertNotNull(meta.index("test"));
+        assertAcked(client().admin().indices().prepareDelete("test"));
+
+        meta = gwMetaState.loadMetaState();
+        assertNotNull(meta);
+        assertNull(meta.index("test"));
+
+
+        test = createIndex("test");
+        client().prepareIndex("test", "type", "1").setSource("field", "value").setRefresh(true).get();
+        client().admin().indices().prepareFlush("test").get();
+        assertHitCount(client().prepareSearch("test").get(), 1);
+        IndexMetaData secondMetaData = clusterService.state().metaData().index("test");
+        assertAcked(client().admin().indices().prepareClose("test"));
+        ShardPath path = ShardPath.loadShardPath(logger, getNodeEnvironment(), new ShardId(test.index(), 0), test.getIndexSettings());
+        assertTrue(path.exists());
+
+        try {
+            indicesService.deleteIndexStore("boom", secondMetaData, clusterService.state());
+            fail();
+        } catch (IllegalStateException ex) {
+            // all good
+        }
+
+        assertTrue(path.exists());
+
+        // now delete the old one and make sure we resolve against the name
+        try {
+            indicesService.deleteIndexStore("boom", firstMetaData, clusterService.state());
+            fail();
+        } catch (IllegalStateException ex) {
+            // all good
+        }
+        assertAcked(client().admin().indices().prepareOpen("test"));
+        ensureGreen("test");
+    }
+
+    public void testPendingTasks() throws IOException {
+        IndicesService indicesService = getIndicesService();
+        IndexService test = createIndex("test");
+
+        assertTrue(test.hasShard(0));
+        ShardPath path = test.shard(0).shardPath();
+        assertTrue(test.shard(0).routingEntry().started());
+        ShardPath shardPath = ShardPath.loadShardPath(logger, getNodeEnvironment(), new ShardId(test.index(), 0), test.getIndexSettings());
+        assertEquals(shardPath, path);
+        try {
+            indicesService.processPendingDeletes(test.index(), test.getIndexSettings(), new TimeValue(0, TimeUnit.MILLISECONDS));
+            fail("can't get lock");
+        } catch (LockObtainFailedException ex) {
+
+        }
+        assertTrue(path.exists());
+
+        int numPending = 1;
+        if (randomBoolean()) {
+            indicesService.addPendingDelete(new ShardId(test.index(), 0), test.getIndexSettings());
+        } else {
+            if (randomBoolean()) {
+                numPending++;
+                indicesService.addPendingDelete(new ShardId(test.index(), 0), test.getIndexSettings());
+            }
+            indicesService.addPendingDelete(test.index(), test.getIndexSettings());
+        }
+        assertAcked(client().admin().indices().prepareClose("test"));
+        assertTrue(path.exists());
+
+        assertEquals(indicesService.numPendingDeletes(test.index()), numPending);
+
+        // shard lock released... we can now delete
+        indicesService.processPendingDeletes(test.index(), test.getIndexSettings(), new TimeValue(0, TimeUnit.MILLISECONDS));
+        assertEquals(indicesService.numPendingDeletes(test.index()), 0);
+        assertFalse(path.exists());
+
+        if (randomBoolean()) {
+            indicesService.addPendingDelete(new ShardId(test.index(), 0), test.getIndexSettings());
+            indicesService.addPendingDelete(new ShardId(test.index(), 1), test.getIndexSettings());
+            indicesService.addPendingDelete(new ShardId("bogus", 1), test.getIndexSettings());
+            assertEquals(indicesService.numPendingDeletes(test.index()), 2);
+            // shard lock released... we can now delete
+            indicesService.processPendingDeletes(test.index(),  test.getIndexSettings(), new TimeValue(0, TimeUnit.MILLISECONDS));
+            assertEquals(indicesService.numPendingDeletes(test.index()), 0);
+        }
+        assertAcked(client().admin().indices().prepareOpen("test"));
+
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/indices/flush/SyncedFlushSingleNodeTest.java b/core/src/test/java/org/elasticsearch/indices/flush/SyncedFlushSingleNodeTest.java
deleted file mode 100644
index 80cc143..0000000
--- a/core/src/test/java/org/elasticsearch/indices/flush/SyncedFlushSingleNodeTest.java
+++ /dev/null
@@ -1,225 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.indices.flush;
-
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
-import org.elasticsearch.cluster.routing.ShardRouting;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.index.IndexService;
-import org.elasticsearch.index.engine.Engine;
-import org.elasticsearch.index.shard.IndexShard;
-import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.index.shard.ShardNotFoundException;
-import org.elasticsearch.indices.IndicesService;
-import org.elasticsearch.test.ESSingleNodeTestCase;
-
-import java.util.List;
-import java.util.Map;
-
-/**
- */
-public class SyncedFlushSingleNodeTest extends ESSingleNodeTestCase {
-
-    public void testModificationPreventsFlushing() throws InterruptedException {
-        createIndex("test");
-        client().prepareIndex("test", "test", "1").setSource("{}").get();
-        IndexService test = getInstanceFromNode(IndicesService.class).indexService("test");
-        IndexShard shard = test.shard(0);
-
-        SyncedFlushService flushService = getInstanceFromNode(SyncedFlushService.class);
-        final ShardId shardId = shard.shardId();
-        final ClusterState state = getInstanceFromNode(ClusterService.class).state();
-        final IndexShardRoutingTable shardRoutingTable = flushService.getShardRoutingTable(shardId, state);
-        final List<ShardRouting> activeShards = shardRoutingTable.activeShards();
-        assertEquals("exactly one active shard", 1, activeShards.size());
-        Map<String, Engine.CommitId> commitIds = SyncedFlushUtil.sendPreSyncRequests(flushService, activeShards, state, shardId);
-        assertEquals("exactly one commit id", 1, commitIds.size());
-        client().prepareIndex("test", "test", "2").setSource("{}").get();
-        String syncId = Strings.base64UUID();
-        SyncedFlushUtil.LatchedListener<ShardsSyncedFlushResult> listener = new SyncedFlushUtil.LatchedListener<>();
-        flushService.sendSyncRequests(syncId, activeShards, state, commitIds, shardId, shardRoutingTable.size(), listener);
-        listener.latch.await();
-        assertNull(listener.error);
-        ShardsSyncedFlushResult syncedFlushResult = listener.result;
-        assertNotNull(syncedFlushResult);
-        assertEquals(0, syncedFlushResult.successfulShards());
-        assertEquals(1, syncedFlushResult.totalShards());
-        assertEquals(syncId, syncedFlushResult.syncId());
-        assertNotNull(syncedFlushResult.shardResponses().get(activeShards.get(0)));
-        assertFalse(syncedFlushResult.shardResponses().get(activeShards.get(0)).success());
-        assertEquals("pending operations", syncedFlushResult.shardResponses().get(activeShards.get(0)).failureReason());
-
-        SyncedFlushUtil.sendPreSyncRequests(flushService, activeShards, state, shardId); // pull another commit and make sure we can't sync-flush with the old one
-        listener = new SyncedFlushUtil.LatchedListener();
-        flushService.sendSyncRequests(syncId, activeShards, state, commitIds, shardId, shardRoutingTable.size(), listener);
-        listener.latch.await();
-        assertNull(listener.error);
-        syncedFlushResult = listener.result;
-        assertNotNull(syncedFlushResult);
-        assertEquals(0, syncedFlushResult.successfulShards());
-        assertEquals(1, syncedFlushResult.totalShards());
-        assertEquals(syncId, syncedFlushResult.syncId());
-        assertNotNull(syncedFlushResult.shardResponses().get(activeShards.get(0)));
-        assertFalse(syncedFlushResult.shardResponses().get(activeShards.get(0)).success());
-        assertEquals("commit has changed", syncedFlushResult.shardResponses().get(activeShards.get(0)).failureReason());
-    }
-
-    public void testSingleShardSuccess() throws InterruptedException {
-        createIndex("test");
-        client().prepareIndex("test", "test", "1").setSource("{}").get();
-        IndexService test = getInstanceFromNode(IndicesService.class).indexService("test");
-        IndexShard shard = test.shard(0);
-
-        SyncedFlushService flushService = getInstanceFromNode(SyncedFlushService.class);
-        final ShardId shardId = shard.shardId();
-        SyncedFlushUtil.LatchedListener<ShardsSyncedFlushResult> listener = new SyncedFlushUtil.LatchedListener();
-        flushService.attemptSyncedFlush(shardId, listener);
-        listener.latch.await();
-        assertNull(listener.error);
-        ShardsSyncedFlushResult syncedFlushResult = listener.result;
-        assertNotNull(syncedFlushResult);
-        assertEquals(1, syncedFlushResult.successfulShards());
-        assertEquals(1, syncedFlushResult.totalShards());
-        SyncedFlushService.SyncedFlushResponse response = syncedFlushResult.shardResponses().values().iterator().next();
-        assertTrue(response.success());
-    }
-
-    public void testSyncFailsIfOperationIsInFlight() throws InterruptedException {
-        createIndex("test");
-        client().prepareIndex("test", "test", "1").setSource("{}").get();
-        IndexService test = getInstanceFromNode(IndicesService.class).indexService("test");
-        IndexShard shard = test.shard(0);
-
-        SyncedFlushService flushService = getInstanceFromNode(SyncedFlushService.class);
-        final ShardId shardId = shard.shardId();
-        shard.incrementOperationCounter();
-        try {
-            SyncedFlushUtil.LatchedListener<ShardsSyncedFlushResult> listener = new SyncedFlushUtil.LatchedListener<>();
-            flushService.attemptSyncedFlush(shardId, listener);
-            listener.latch.await();
-            assertNull(listener.error);
-            ShardsSyncedFlushResult syncedFlushResult = listener.result;
-            assertNotNull(syncedFlushResult);
-            assertEquals(0, syncedFlushResult.successfulShards());
-            assertNotEquals(0, syncedFlushResult.totalShards());
-            assertEquals("[1] ongoing operations on primary", syncedFlushResult.failureReason());
-        } finally {
-            shard.decrementOperationCounter();
-        }
-    }
-
-    public void testSyncFailsOnIndexClosedOrMissing() throws InterruptedException {
-        createIndex("test");
-        IndexService test = getInstanceFromNode(IndicesService.class).indexService("test");
-        IndexShard shard = test.shard(0);
-
-        SyncedFlushService flushService = getInstanceFromNode(SyncedFlushService.class);
-        SyncedFlushUtil.LatchedListener listener = new SyncedFlushUtil.LatchedListener();
-        flushService.attemptSyncedFlush(new ShardId("test", 1), listener);
-        listener.latch.await();
-        assertNotNull(listener.error);
-        assertNull(listener.result);
-        assertEquals(ShardNotFoundException.class, listener.error.getClass());
-        assertEquals("no such shard", listener.error.getMessage());
-
-        final ShardId shardId = shard.shardId();
-
-        client().admin().indices().prepareClose("test").get();
-        listener = new SyncedFlushUtil.LatchedListener();
-        flushService.attemptSyncedFlush(shardId, listener);
-        listener.latch.await();
-        assertNotNull(listener.error);
-        assertNull(listener.result);
-        assertEquals("closed", listener.error.getMessage());
-
-        listener = new SyncedFlushUtil.LatchedListener();
-        flushService.attemptSyncedFlush(new ShardId("index not found", 0), listener);
-        listener.latch.await();
-        assertNotNull(listener.error);
-        assertNull(listener.result);
-        assertEquals("no such index", listener.error.getMessage());
-    }
-    
-    public void testFailAfterIntermediateCommit() throws InterruptedException {
-        createIndex("test");
-        client().prepareIndex("test", "test", "1").setSource("{}").get();
-        IndexService test = getInstanceFromNode(IndicesService.class).indexService("test");
-        IndexShard shard = test.shard(0);
-
-        SyncedFlushService flushService = getInstanceFromNode(SyncedFlushService.class);
-        final ShardId shardId = shard.shardId();
-        final ClusterState state = getInstanceFromNode(ClusterService.class).state();
-        final IndexShardRoutingTable shardRoutingTable = flushService.getShardRoutingTable(shardId, state);
-        final List<ShardRouting> activeShards = shardRoutingTable.activeShards();
-        assertEquals("exactly one active shard", 1, activeShards.size());
-        Map<String, Engine.CommitId> commitIds = SyncedFlushUtil.sendPreSyncRequests(flushService, activeShards, state, shardId);
-        assertEquals("exactly one commit id", 1, commitIds.size());
-        if (randomBoolean()) {
-            client().prepareIndex("test", "test", "2").setSource("{}").get();
-        }
-        client().admin().indices().prepareFlush("test").setForce(true).get();
-        String syncId = Strings.base64UUID();
-        final SyncedFlushUtil.LatchedListener<ShardsSyncedFlushResult> listener = new SyncedFlushUtil.LatchedListener();
-        flushService.sendSyncRequests(syncId, activeShards, state, commitIds, shardId, shardRoutingTable.size(), listener);
-        listener.latch.await();
-        assertNull(listener.error);
-        ShardsSyncedFlushResult syncedFlushResult = listener.result;
-        assertNotNull(syncedFlushResult);
-        assertEquals(0, syncedFlushResult.successfulShards());
-        assertEquals(1, syncedFlushResult.totalShards());
-        assertEquals(syncId, syncedFlushResult.syncId());
-        assertNotNull(syncedFlushResult.shardResponses().get(activeShards.get(0)));
-        assertFalse(syncedFlushResult.shardResponses().get(activeShards.get(0)).success());
-        assertEquals("commit has changed", syncedFlushResult.shardResponses().get(activeShards.get(0)).failureReason());
-    }
-
-    public void testFailWhenCommitIsMissing() throws InterruptedException {
-        createIndex("test");
-        client().prepareIndex("test", "test", "1").setSource("{}").get();
-        IndexService test = getInstanceFromNode(IndicesService.class).indexService("test");
-        IndexShard shard = test.shard(0);
-
-        SyncedFlushService flushService = getInstanceFromNode(SyncedFlushService.class);
-        final ShardId shardId = shard.shardId();
-        final ClusterState state = getInstanceFromNode(ClusterService.class).state();
-        final IndexShardRoutingTable shardRoutingTable = flushService.getShardRoutingTable(shardId, state);
-        final List<ShardRouting> activeShards = shardRoutingTable.activeShards();
-        assertEquals("exactly one active shard", 1, activeShards.size());
-        Map<String, Engine.CommitId> commitIds =  SyncedFlushUtil.sendPreSyncRequests(flushService, activeShards, state, shardId);
-        assertEquals("exactly one commit id", 1, commitIds.size());
-        commitIds.clear(); // wipe it...
-        String syncId = Strings.base64UUID();
-        SyncedFlushUtil.LatchedListener<ShardsSyncedFlushResult> listener = new SyncedFlushUtil.LatchedListener();
-        flushService.sendSyncRequests(syncId, activeShards, state, commitIds, shardId, shardRoutingTable.size(), listener);
-        listener.latch.await();
-        assertNull(listener.error);
-        ShardsSyncedFlushResult syncedFlushResult = listener.result;
-        assertNotNull(syncedFlushResult);
-        assertEquals(0, syncedFlushResult.successfulShards());
-        assertEquals(1, syncedFlushResult.totalShards());
-        assertEquals(syncId, syncedFlushResult.syncId());
-        assertNotNull(syncedFlushResult.shardResponses().get(activeShards.get(0)));
-        assertFalse(syncedFlushResult.shardResponses().get(activeShards.get(0)).success());
-        assertEquals("no commit id from pre-sync flush", syncedFlushResult.shardResponses().get(activeShards.get(0)).failureReason());
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/indices/flush/SyncedFlushSingleNodeTests.java b/core/src/test/java/org/elasticsearch/indices/flush/SyncedFlushSingleNodeTests.java
new file mode 100644
index 0000000..06c2566
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/indices/flush/SyncedFlushSingleNodeTests.java
@@ -0,0 +1,225 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.indices.flush;
+
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
+import org.elasticsearch.cluster.routing.ShardRouting;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.index.IndexService;
+import org.elasticsearch.index.engine.Engine;
+import org.elasticsearch.index.shard.IndexShard;
+import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.index.shard.ShardNotFoundException;
+import org.elasticsearch.indices.IndicesService;
+import org.elasticsearch.test.ESSingleNodeTestCase;
+
+import java.util.List;
+import java.util.Map;
+
+/**
+ */
+public class SyncedFlushSingleNodeTests extends ESSingleNodeTestCase {
+
+    public void testModificationPreventsFlushing() throws InterruptedException {
+        createIndex("test");
+        client().prepareIndex("test", "test", "1").setSource("{}").get();
+        IndexService test = getInstanceFromNode(IndicesService.class).indexService("test");
+        IndexShard shard = test.shard(0);
+
+        SyncedFlushService flushService = getInstanceFromNode(SyncedFlushService.class);
+        final ShardId shardId = shard.shardId();
+        final ClusterState state = getInstanceFromNode(ClusterService.class).state();
+        final IndexShardRoutingTable shardRoutingTable = flushService.getShardRoutingTable(shardId, state);
+        final List<ShardRouting> activeShards = shardRoutingTable.activeShards();
+        assertEquals("exactly one active shard", 1, activeShards.size());
+        Map<String, Engine.CommitId> commitIds = SyncedFlushUtil.sendPreSyncRequests(flushService, activeShards, state, shardId);
+        assertEquals("exactly one commit id", 1, commitIds.size());
+        client().prepareIndex("test", "test", "2").setSource("{}").get();
+        String syncId = Strings.base64UUID();
+        SyncedFlushUtil.LatchedListener<ShardsSyncedFlushResult> listener = new SyncedFlushUtil.LatchedListener<>();
+        flushService.sendSyncRequests(syncId, activeShards, state, commitIds, shardId, shardRoutingTable.size(), listener);
+        listener.latch.await();
+        assertNull(listener.error);
+        ShardsSyncedFlushResult syncedFlushResult = listener.result;
+        assertNotNull(syncedFlushResult);
+        assertEquals(0, syncedFlushResult.successfulShards());
+        assertEquals(1, syncedFlushResult.totalShards());
+        assertEquals(syncId, syncedFlushResult.syncId());
+        assertNotNull(syncedFlushResult.shardResponses().get(activeShards.get(0)));
+        assertFalse(syncedFlushResult.shardResponses().get(activeShards.get(0)).success());
+        assertEquals("pending operations", syncedFlushResult.shardResponses().get(activeShards.get(0)).failureReason());
+
+        SyncedFlushUtil.sendPreSyncRequests(flushService, activeShards, state, shardId); // pull another commit and make sure we can't sync-flush with the old one
+        listener = new SyncedFlushUtil.LatchedListener();
+        flushService.sendSyncRequests(syncId, activeShards, state, commitIds, shardId, shardRoutingTable.size(), listener);
+        listener.latch.await();
+        assertNull(listener.error);
+        syncedFlushResult = listener.result;
+        assertNotNull(syncedFlushResult);
+        assertEquals(0, syncedFlushResult.successfulShards());
+        assertEquals(1, syncedFlushResult.totalShards());
+        assertEquals(syncId, syncedFlushResult.syncId());
+        assertNotNull(syncedFlushResult.shardResponses().get(activeShards.get(0)));
+        assertFalse(syncedFlushResult.shardResponses().get(activeShards.get(0)).success());
+        assertEquals("commit has changed", syncedFlushResult.shardResponses().get(activeShards.get(0)).failureReason());
+    }
+
+    public void testSingleShardSuccess() throws InterruptedException {
+        createIndex("test");
+        client().prepareIndex("test", "test", "1").setSource("{}").get();
+        IndexService test = getInstanceFromNode(IndicesService.class).indexService("test");
+        IndexShard shard = test.shard(0);
+
+        SyncedFlushService flushService = getInstanceFromNode(SyncedFlushService.class);
+        final ShardId shardId = shard.shardId();
+        SyncedFlushUtil.LatchedListener<ShardsSyncedFlushResult> listener = new SyncedFlushUtil.LatchedListener();
+        flushService.attemptSyncedFlush(shardId, listener);
+        listener.latch.await();
+        assertNull(listener.error);
+        ShardsSyncedFlushResult syncedFlushResult = listener.result;
+        assertNotNull(syncedFlushResult);
+        assertEquals(1, syncedFlushResult.successfulShards());
+        assertEquals(1, syncedFlushResult.totalShards());
+        SyncedFlushService.SyncedFlushResponse response = syncedFlushResult.shardResponses().values().iterator().next();
+        assertTrue(response.success());
+    }
+
+    public void testSyncFailsIfOperationIsInFlight() throws InterruptedException {
+        createIndex("test");
+        client().prepareIndex("test", "test", "1").setSource("{}").get();
+        IndexService test = getInstanceFromNode(IndicesService.class).indexService("test");
+        IndexShard shard = test.shard(0);
+
+        SyncedFlushService flushService = getInstanceFromNode(SyncedFlushService.class);
+        final ShardId shardId = shard.shardId();
+        shard.incrementOperationCounter();
+        try {
+            SyncedFlushUtil.LatchedListener<ShardsSyncedFlushResult> listener = new SyncedFlushUtil.LatchedListener<>();
+            flushService.attemptSyncedFlush(shardId, listener);
+            listener.latch.await();
+            assertNull(listener.error);
+            ShardsSyncedFlushResult syncedFlushResult = listener.result;
+            assertNotNull(syncedFlushResult);
+            assertEquals(0, syncedFlushResult.successfulShards());
+            assertNotEquals(0, syncedFlushResult.totalShards());
+            assertEquals("[1] ongoing operations on primary", syncedFlushResult.failureReason());
+        } finally {
+            shard.decrementOperationCounter();
+        }
+    }
+
+    public void testSyncFailsOnIndexClosedOrMissing() throws InterruptedException {
+        createIndex("test");
+        IndexService test = getInstanceFromNode(IndicesService.class).indexService("test");
+        IndexShard shard = test.shard(0);
+
+        SyncedFlushService flushService = getInstanceFromNode(SyncedFlushService.class);
+        SyncedFlushUtil.LatchedListener listener = new SyncedFlushUtil.LatchedListener();
+        flushService.attemptSyncedFlush(new ShardId("test", 1), listener);
+        listener.latch.await();
+        assertNotNull(listener.error);
+        assertNull(listener.result);
+        assertEquals(ShardNotFoundException.class, listener.error.getClass());
+        assertEquals("no such shard", listener.error.getMessage());
+
+        final ShardId shardId = shard.shardId();
+
+        client().admin().indices().prepareClose("test").get();
+        listener = new SyncedFlushUtil.LatchedListener();
+        flushService.attemptSyncedFlush(shardId, listener);
+        listener.latch.await();
+        assertNotNull(listener.error);
+        assertNull(listener.result);
+        assertEquals("closed", listener.error.getMessage());
+
+        listener = new SyncedFlushUtil.LatchedListener();
+        flushService.attemptSyncedFlush(new ShardId("index not found", 0), listener);
+        listener.latch.await();
+        assertNotNull(listener.error);
+        assertNull(listener.result);
+        assertEquals("no such index", listener.error.getMessage());
+    }
+    
+    public void testFailAfterIntermediateCommit() throws InterruptedException {
+        createIndex("test");
+        client().prepareIndex("test", "test", "1").setSource("{}").get();
+        IndexService test = getInstanceFromNode(IndicesService.class).indexService("test");
+        IndexShard shard = test.shard(0);
+
+        SyncedFlushService flushService = getInstanceFromNode(SyncedFlushService.class);
+        final ShardId shardId = shard.shardId();
+        final ClusterState state = getInstanceFromNode(ClusterService.class).state();
+        final IndexShardRoutingTable shardRoutingTable = flushService.getShardRoutingTable(shardId, state);
+        final List<ShardRouting> activeShards = shardRoutingTable.activeShards();
+        assertEquals("exactly one active shard", 1, activeShards.size());
+        Map<String, Engine.CommitId> commitIds = SyncedFlushUtil.sendPreSyncRequests(flushService, activeShards, state, shardId);
+        assertEquals("exactly one commit id", 1, commitIds.size());
+        if (randomBoolean()) {
+            client().prepareIndex("test", "test", "2").setSource("{}").get();
+        }
+        client().admin().indices().prepareFlush("test").setForce(true).get();
+        String syncId = Strings.base64UUID();
+        final SyncedFlushUtil.LatchedListener<ShardsSyncedFlushResult> listener = new SyncedFlushUtil.LatchedListener();
+        flushService.sendSyncRequests(syncId, activeShards, state, commitIds, shardId, shardRoutingTable.size(), listener);
+        listener.latch.await();
+        assertNull(listener.error);
+        ShardsSyncedFlushResult syncedFlushResult = listener.result;
+        assertNotNull(syncedFlushResult);
+        assertEquals(0, syncedFlushResult.successfulShards());
+        assertEquals(1, syncedFlushResult.totalShards());
+        assertEquals(syncId, syncedFlushResult.syncId());
+        assertNotNull(syncedFlushResult.shardResponses().get(activeShards.get(0)));
+        assertFalse(syncedFlushResult.shardResponses().get(activeShards.get(0)).success());
+        assertEquals("commit has changed", syncedFlushResult.shardResponses().get(activeShards.get(0)).failureReason());
+    }
+
+    public void testFailWhenCommitIsMissing() throws InterruptedException {
+        createIndex("test");
+        client().prepareIndex("test", "test", "1").setSource("{}").get();
+        IndexService test = getInstanceFromNode(IndicesService.class).indexService("test");
+        IndexShard shard = test.shard(0);
+
+        SyncedFlushService flushService = getInstanceFromNode(SyncedFlushService.class);
+        final ShardId shardId = shard.shardId();
+        final ClusterState state = getInstanceFromNode(ClusterService.class).state();
+        final IndexShardRoutingTable shardRoutingTable = flushService.getShardRoutingTable(shardId, state);
+        final List<ShardRouting> activeShards = shardRoutingTable.activeShards();
+        assertEquals("exactly one active shard", 1, activeShards.size());
+        Map<String, Engine.CommitId> commitIds =  SyncedFlushUtil.sendPreSyncRequests(flushService, activeShards, state, shardId);
+        assertEquals("exactly one commit id", 1, commitIds.size());
+        commitIds.clear(); // wipe it...
+        String syncId = Strings.base64UUID();
+        SyncedFlushUtil.LatchedListener<ShardsSyncedFlushResult> listener = new SyncedFlushUtil.LatchedListener();
+        flushService.sendSyncRequests(syncId, activeShards, state, commitIds, shardId, shardRoutingTable.size(), listener);
+        listener.latch.await();
+        assertNull(listener.error);
+        ShardsSyncedFlushResult syncedFlushResult = listener.result;
+        assertNotNull(syncedFlushResult);
+        assertEquals(0, syncedFlushResult.successfulShards());
+        assertEquals(1, syncedFlushResult.totalShards());
+        assertEquals(syncId, syncedFlushResult.syncId());
+        assertNotNull(syncedFlushResult.shardResponses().get(activeShards.get(0)));
+        assertFalse(syncedFlushResult.shardResponses().get(activeShards.get(0)).success());
+        assertEquals("no commit id from pre-sync flush", syncedFlushResult.shardResponses().get(activeShards.get(0)).failureReason());
+    }
+
+
+}
diff --git a/core/src/test/java/org/elasticsearch/indices/recovery/RecoveryStateTest.java b/core/src/test/java/org/elasticsearch/indices/recovery/RecoveryStateTest.java
deleted file mode 100644
index 3f0fe79..0000000
--- a/core/src/test/java/org/elasticsearch/indices/recovery/RecoveryStateTest.java
+++ /dev/null
@@ -1,531 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.indices.recovery;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.Streamable;
-import org.elasticsearch.common.transport.DummyTransportAddress;
-import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.indices.recovery.RecoveryState.*;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.concurrent.atomic.AtomicBoolean;
-import java.util.concurrent.atomic.AtomicReference;
-
-import static org.elasticsearch.test.VersionUtils.randomVersion;
-import static org.hamcrest.Matchers.*;
-
-public class RecoveryStateTest extends ESTestCase {
-
-    abstract class Streamer<T extends Streamable> extends Thread {
-
-        private T lastRead;
-        final private AtomicBoolean shouldStop;
-        final private T source;
-        final AtomicReference<Throwable> error = new AtomicReference<>();
-        final Version streamVersion;
-
-        Streamer(AtomicBoolean shouldStop, T source) {
-            this(shouldStop, source, randomVersion(random()));
-        }
-
-        Streamer(AtomicBoolean shouldStop, T source, Version streamVersion) {
-            this.shouldStop = shouldStop;
-            this.source = source;
-            this.streamVersion = streamVersion;
-        }
-
-        public T lastRead() throws Throwable {
-            Throwable t = error.get();
-            if (t != null) {
-                throw t;
-            }
-            return lastRead;
-        }
-
-        public T serializeDeserialize() throws IOException {
-            BytesStreamOutput out = new BytesStreamOutput();
-            source.writeTo(out);
-            out.close();
-            StreamInput in = StreamInput.wrap(out.bytes());
-            T obj = deserialize(in);
-            lastRead = obj;
-            return obj;
-        }
-
-        protected T deserialize(StreamInput in) throws IOException {
-            T obj = createObj();
-            obj.readFrom(in);
-            return obj;
-        }
-
-        abstract T createObj();
-
-        @Override
-        public void run() {
-            try {
-                while (shouldStop.get() == false) {
-                    serializeDeserialize();
-                }
-                serializeDeserialize();
-            } catch (Throwable t) {
-                error.set(t);
-            }
-        }
-    }
-
-    public void testTimers() throws Throwable {
-        final Timer timer;
-        Streamer<Timer> streamer;
-        AtomicBoolean stop = new AtomicBoolean();
-        if (randomBoolean()) {
-            timer = new Timer();
-            streamer = new Streamer<Timer>(stop, timer) {
-                @Override
-                Timer createObj() {
-                    return new Timer();
-                }
-            };
-        } else if (randomBoolean()) {
-            timer = new Index();
-            streamer = new Streamer<Timer>(stop, timer) {
-                @Override
-                Timer createObj() {
-                    return new Index();
-                }
-            };
-        } else if (randomBoolean()) {
-            timer = new VerifyIndex();
-            streamer = new Streamer<Timer>(stop, timer) {
-                @Override
-                Timer createObj() {
-                    return new VerifyIndex();
-                }
-            };
-        } else {
-            timer = new Translog();
-            streamer = new Streamer<Timer>(stop, timer) {
-                @Override
-                Timer createObj() {
-                    return new Translog();
-                }
-            };
-        }
-
-        timer.start();
-        assertThat(timer.startTime(), greaterThan(0l));
-        assertThat(timer.stopTime(), equalTo(0l));
-        Timer lastRead = streamer.serializeDeserialize();
-        final long time = lastRead.time();
-        assertThat(time, lessThanOrEqualTo(timer.time()));
-        assertBusy(new Runnable() {
-            @Override
-            public void run() {
-                assertThat("timer timer should progress compared to captured one ", time, lessThan(timer.time()));
-            }
-        });
-        assertThat("captured time shouldn't change", lastRead.time(), equalTo(time));
-
-        if (randomBoolean()) {
-            timer.stop();
-            assertThat(timer.stopTime(), greaterThanOrEqualTo(timer.startTime()));
-            assertThat(timer.time(), greaterThan(0l));
-            lastRead = streamer.serializeDeserialize();
-            assertThat(lastRead.startTime(), equalTo(timer.startTime()));
-            assertThat(lastRead.time(), equalTo(timer.time()));
-            assertThat(lastRead.stopTime(), equalTo(timer.stopTime()));
-        }
-
-        timer.reset();
-        assertThat(timer.startTime(), equalTo(0l));
-        assertThat(timer.time(), equalTo(0l));
-        assertThat(timer.stopTime(), equalTo(0l));
-        lastRead = streamer.serializeDeserialize();
-        assertThat(lastRead.startTime(), equalTo(0l));
-        assertThat(lastRead.time(), equalTo(0l));
-        assertThat(lastRead.stopTime(), equalTo(0l));
-
-    }
-
-    public void testIndex() throws Throwable {
-        File[] files = new File[randomIntBetween(1, 20)];
-        ArrayList<File> filesToRecover = new ArrayList<>();
-        long totalFileBytes = 0;
-        long totalReusedBytes = 0;
-        int totalReused = 0;
-        for (int i = 0; i < files.length; i++) {
-            final int fileLength = randomIntBetween(1, 1000);
-            final boolean reused = randomBoolean();
-            totalFileBytes += fileLength;
-            files[i] = new RecoveryState.File("f_" + i, fileLength, reused);
-            if (reused) {
-                totalReused++;
-                totalReusedBytes += fileLength;
-            } else {
-                filesToRecover.add(files[i]);
-            }
-        }
-
-        Collections.shuffle(Arrays.asList(files));
-        final RecoveryState.Index index = new RecoveryState.Index();
-
-        if (randomBoolean()) {
-            // initialize with some data and then reset
-            index.start();
-            for (int i = randomIntBetween(0, 10); i > 0; i--) {
-                index.addFileDetail("t_" + i, randomIntBetween(1, 100), randomBoolean());
-                if (randomBoolean()) {
-                    index.addSourceThrottling(randomIntBetween(0, 20));
-                }
-                if (randomBoolean()) {
-                    index.addTargetThrottling(randomIntBetween(0, 20));
-                }
-            }
-            if (randomBoolean()) {
-                index.stop();
-            }
-            index.reset();
-        }
-
-
-        // before we start we must report 0
-        assertThat(index.recoveredFilesPercent(), equalTo((float) 0.0));
-        assertThat(index.recoveredBytesPercent(), equalTo((float) 0.0));
-        assertThat(index.sourceThrottling().nanos(), equalTo(Index.UNKNOWN));
-        assertThat(index.targetThrottling().nanos(), equalTo(Index.UNKNOWN));
-
-        index.start();
-        for (File file : files) {
-            index.addFileDetail(file.name(), file.length(), file.reused());
-        }
-
-        logger.info("testing initial information");
-        assertThat(index.totalBytes(), equalTo(totalFileBytes));
-        assertThat(index.reusedBytes(), equalTo(totalReusedBytes));
-        assertThat(index.totalRecoverBytes(), equalTo(totalFileBytes - totalReusedBytes));
-        assertThat(index.totalFileCount(), equalTo(files.length));
-        assertThat(index.reusedFileCount(), equalTo(totalReused));
-        assertThat(index.totalRecoverFiles(), equalTo(filesToRecover.size()));
-        assertThat(index.recoveredFileCount(), equalTo(0));
-        assertThat(index.recoveredBytes(), equalTo(0l));
-        assertThat(index.recoveredFilesPercent(), equalTo(filesToRecover.size() == 0 ? 100.0f : 0.0f));
-        assertThat(index.recoveredBytesPercent(), equalTo(filesToRecover.size() == 0 ? 100.0f : 0.0f));
-
-
-        long bytesToRecover = totalFileBytes - totalReusedBytes;
-        boolean completeRecovery = bytesToRecover == 0 || randomBoolean();
-        if (completeRecovery == false) {
-            bytesToRecover = randomIntBetween(1, (int) bytesToRecover);
-            logger.info("performing partial recovery ([{}] bytes of [{}])", bytesToRecover, totalFileBytes - totalReusedBytes);
-        }
-        AtomicBoolean streamShouldStop = new AtomicBoolean();
-
-        Streamer<Index> backgroundReader = new Streamer<RecoveryState.Index>(streamShouldStop, index) {
-            @Override
-            Index createObj() {
-                return new Index();
-            }
-        };
-
-        backgroundReader.start();
-
-        long recoveredBytes = 0;
-        long sourceThrottling = Index.UNKNOWN;
-        long targetThrottling = Index.UNKNOWN;
-        while (bytesToRecover > 0) {
-            File file = randomFrom(filesToRecover);
-            final long toRecover = Math.min(bytesToRecover, randomIntBetween(1, (int) (file.length() - file.recovered())));
-            final long throttledOnSource = rarely() ? randomIntBetween(10, 200) : 0;
-            index.addSourceThrottling(throttledOnSource);
-            if (sourceThrottling == Index.UNKNOWN) {
-                sourceThrottling = throttledOnSource;
-            } else {
-                sourceThrottling += throttledOnSource;
-            }
-            index.addRecoveredBytesToFile(file.name(), toRecover);
-            file.addRecoveredBytes(toRecover);
-            final long throttledOnTarget = rarely() ? randomIntBetween(10, 200) : 0;
-            if (targetThrottling == Index.UNKNOWN) {
-                targetThrottling = throttledOnTarget;
-            } else {
-                targetThrottling += throttledOnTarget;
-            }
-            index.addTargetThrottling(throttledOnTarget);
-            bytesToRecover -= toRecover;
-            recoveredBytes += toRecover;
-            if (file.reused() || file.fullyRecovered()) {
-                filesToRecover.remove(file);
-            }
-        }
-
-        if (completeRecovery) {
-            assertThat(filesToRecover.size(), equalTo(0));
-            index.stop();
-            assertThat(index.time(), greaterThanOrEqualTo(0l));
-        }
-
-        logger.info("testing serialized information");
-        streamShouldStop.set(true);
-        backgroundReader.join();
-        final Index lastRead = backgroundReader.lastRead();
-        assertThat(lastRead.fileDetails().toArray(), arrayContainingInAnyOrder(index.fileDetails().toArray()));
-        assertThat(lastRead.startTime(), equalTo(index.startTime()));
-        if (completeRecovery) {
-            assertThat(lastRead.time(), equalTo(index.time()));
-        } else {
-            assertThat(lastRead.time(), lessThanOrEqualTo(index.time()));
-        }
-        assertThat(lastRead.stopTime(), equalTo(index.stopTime()));
-        assertThat(lastRead.targetThrottling(), equalTo(index.targetThrottling()));
-        assertThat(lastRead.sourceThrottling(), equalTo(index.sourceThrottling()));
-
-        logger.info("testing post recovery");
-        assertThat(index.totalBytes(), equalTo(totalFileBytes));
-        assertThat(index.reusedBytes(), equalTo(totalReusedBytes));
-        assertThat(index.totalRecoverBytes(), equalTo(totalFileBytes - totalReusedBytes));
-        assertThat(index.totalFileCount(), equalTo(files.length));
-        assertThat(index.reusedFileCount(), equalTo(totalReused));
-        assertThat(index.totalRecoverFiles(), equalTo(files.length - totalReused));
-        assertThat(index.recoveredFileCount(), equalTo(index.totalRecoverFiles() - filesToRecover.size()));
-        assertThat(index.recoveredBytes(), equalTo(recoveredBytes));
-        assertThat(index.targetThrottling().nanos(), equalTo(targetThrottling));
-        assertThat(index.sourceThrottling().nanos(), equalTo(sourceThrottling));
-        if (index.totalRecoverFiles() == 0) {
-            assertThat((double) index.recoveredFilesPercent(), equalTo(100.0));
-            assertThat((double) index.recoveredBytesPercent(), equalTo(100.0));
-        } else {
-            assertThat((double) index.recoveredFilesPercent(), closeTo(100.0 * index.recoveredFileCount() / index.totalRecoverFiles(), 0.1));
-            assertThat((double) index.recoveredBytesPercent(), closeTo(100.0 * index.recoveredBytes() / index.totalRecoverBytes(), 0.1));
-        }
-    }
-
-    public void testStageSequenceEnforcement() {
-        final DiscoveryNode discoveryNode = new DiscoveryNode("1", DummyTransportAddress.INSTANCE, Version.CURRENT);
-        Stage[] stages = Stage.values();
-        int i = randomIntBetween(0, stages.length - 1);
-        int j;
-        do {
-            j = randomIntBetween(0, stages.length - 1);
-        } while (j == i);
-        Stage t = stages[i];
-        stages[i] = stages[j];
-        stages[j] = t;
-        try {
-            RecoveryState state = new RecoveryState(new ShardId("bla", 0), randomBoolean(), randomFrom(Type.values()), discoveryNode, discoveryNode);
-            for (Stage stage : stages) {
-                state.setStage(stage);
-            }
-            fail("succeeded in performing the illegal sequence [" + Strings.arrayToCommaDelimitedString(stages) + "]");
-        } catch (IllegalStateException e) {
-            // cool
-        }
-
-        // but reset should be always possible.
-        stages = Stage.values();
-        i = randomIntBetween(1, stages.length - 1);
-        ArrayList<Stage> list = new ArrayList<>(Arrays.asList(Arrays.copyOfRange(stages, 0, i)));
-        list.addAll(Arrays.asList(stages));
-        RecoveryState state = new RecoveryState(new ShardId("bla", 0), randomBoolean(), randomFrom(Type.values()), discoveryNode, discoveryNode);
-        for (Stage stage : list) {
-            state.setStage(stage);
-        }
-
-        assertThat(state.getStage(), equalTo(Stage.DONE));
-    }
-
-    public void testTranslog() throws Throwable {
-        final Translog translog = new Translog();
-        AtomicBoolean stop = new AtomicBoolean();
-        Streamer<Translog> streamer = new Streamer<Translog>(stop, translog) {
-            @Override
-            Translog createObj() {
-                return new Translog();
-            }
-        };
-
-        // we don't need to test the time aspect, it's done in the timer test
-        translog.start();
-        assertThat(translog.recoveredOperations(), equalTo(0));
-        assertThat(translog.totalOperations(), equalTo(Translog.UNKNOWN));
-        assertThat(translog.totalOperationsOnStart(), equalTo(Translog.UNKNOWN));
-        streamer.start();
-        // force one
-        streamer.serializeDeserialize();
-        int ops = 0;
-        int totalOps = 0;
-        int totalOpsOnStart = randomIntBetween(10, 200);
-        translog.totalOperationsOnStart(totalOpsOnStart);
-        for (int i = scaledRandomIntBetween(10, 200); i > 0; i--) {
-            final int iterationOps = randomIntBetween(1, 10);
-            totalOps += iterationOps;
-            translog.totalOperations(totalOps);
-            assertThat((double) translog.recoveredPercent(), closeTo(100.0 * ops / totalOps, 0.1));
-            for (int j = iterationOps; j > 0; j--) {
-                ops++;
-                translog.incrementRecoveredOperations();
-                if (randomBoolean()) {
-                    translog.decrementRecoveredOperations(1);
-                    translog.incrementRecoveredOperations();
-                }
-            }
-            assertThat(translog.recoveredOperations(), equalTo(ops));
-            assertThat(translog.totalOperations(), equalTo(totalOps));
-            assertThat(translog.recoveredPercent(), equalTo(100.f));
-            assertThat(streamer.lastRead().recoveredOperations(), greaterThanOrEqualTo(0));
-            assertThat(streamer.lastRead().recoveredOperations(), lessThanOrEqualTo(ops));
-            assertThat(streamer.lastRead().totalOperations(), lessThanOrEqualTo(totalOps));
-            assertThat(streamer.lastRead().totalOperationsOnStart(), lessThanOrEqualTo(totalOpsOnStart));
-            assertThat(streamer.lastRead().recoveredPercent(), either(greaterThanOrEqualTo(0.f)).or(equalTo(-1.f)));
-        }
-
-        boolean stopped = false;
-        if (randomBoolean()) {
-            translog.stop();
-            stopped = true;
-        }
-
-        if (randomBoolean()) {
-            translog.reset();
-            ops = 0;
-            totalOps = Translog.UNKNOWN;
-            totalOpsOnStart = Translog.UNKNOWN;
-            assertThat(translog.recoveredOperations(), equalTo(0));
-            assertThat(translog.totalOperationsOnStart(), equalTo(Translog.UNKNOWN));
-            assertThat(translog.totalOperations(), equalTo(Translog.UNKNOWN));
-        }
-
-        stop.set(true);
-        streamer.join();
-        final Translog lastRead = streamer.lastRead();
-        assertThat(lastRead.recoveredOperations(), equalTo(ops));
-        assertThat(lastRead.totalOperations(), equalTo(totalOps));
-        assertThat(lastRead.totalOperationsOnStart(), equalTo(totalOpsOnStart));
-        assertThat(lastRead.startTime(), equalTo(translog.startTime()));
-        assertThat(lastRead.stopTime(), equalTo(translog.stopTime()));
-
-        if (stopped) {
-            assertThat(lastRead.time(), equalTo(translog.time()));
-        } else {
-            assertThat(lastRead.time(), lessThanOrEqualTo(translog.time()));
-        }
-    }
-
-    public void testStart() throws IOException {
-        final VerifyIndex verifyIndex = new VerifyIndex();
-        AtomicBoolean stop = new AtomicBoolean();
-        Streamer<VerifyIndex> streamer = new Streamer<VerifyIndex>(stop, verifyIndex) {
-            @Override
-            VerifyIndex createObj() {
-                return new VerifyIndex();
-            }
-        };
-
-        // we don't need to test the time aspect, it's done in the timer test
-        verifyIndex.start();
-        assertThat(verifyIndex.checkIndexTime(), equalTo(0l));
-        // force one
-        VerifyIndex lastRead = streamer.serializeDeserialize();
-        assertThat(lastRead.checkIndexTime(), equalTo(0l));
-
-        long took = randomLong();
-        if (took < 0) {
-            took = -took;
-            took = Math.max(0l, took);
-
-        }
-        verifyIndex.checkIndexTime(took);
-        assertThat(verifyIndex.checkIndexTime(), equalTo(took));
-
-        boolean stopped = false;
-        if (randomBoolean()) {
-            verifyIndex.stop();
-            stopped = true;
-        }
-
-        if (randomBoolean()) {
-            verifyIndex.reset();
-            took = 0;
-            assertThat(verifyIndex.checkIndexTime(), equalTo(took));
-        }
-
-        lastRead = streamer.serializeDeserialize();
-        assertThat(lastRead.checkIndexTime(), equalTo(took));
-        assertThat(lastRead.startTime(), equalTo(verifyIndex.startTime()));
-        assertThat(lastRead.stopTime(), equalTo(verifyIndex.stopTime()));
-
-        if (stopped) {
-            assertThat(lastRead.time(), equalTo(verifyIndex.time()));
-        } else {
-            assertThat(lastRead.time(), lessThanOrEqualTo(verifyIndex.time()));
-        }
-    }
-
-    @Test
-    public void testConcurrentModificationIndexFileDetailsMap() throws InterruptedException {
-        final Index index = new Index();
-        final AtomicBoolean stop = new AtomicBoolean(false);
-        Streamer<Index> readWriteIndex = new Streamer<Index>(stop, index) {
-            @Override
-            Index createObj() {
-                return new Index();
-            }
-        };
-        Thread modifyThread = new Thread() {
-            public void run() {
-                for (int i = 0; i < 1000; i++) {
-                    index.addFileDetail(randomAsciiOfLength(10), 100, true);
-                }
-                stop.set(true);
-            }
-        };
-        readWriteIndex.start();
-        modifyThread.start();
-        modifyThread.join();
-        readWriteIndex.join();
-        assertThat(readWriteIndex.error.get(), equalTo(null));
-    }
-
-    @Test
-    public void testFileHashCodeAndEquals() {
-        File f = new File("foo", randomIntBetween(0, 100), randomBoolean());
-        File anotherFile = new File(f.name(), f.length(), f.reused());
-        assertEquals(f, anotherFile);
-        assertEquals(f.hashCode(), anotherFile.hashCode());
-        int iters = randomIntBetween(10, 100);
-        for (int i = 0; i < iters; i++) {
-            f = new File("foo", randomIntBetween(0, 100), randomBoolean());
-            anotherFile = new File(f.name(), randomIntBetween(0, 100), randomBoolean());
-            if (f.equals(anotherFile)) {
-                assertEquals(f.hashCode(), anotherFile.hashCode());
-            } else if (f.hashCode() != anotherFile.hashCode()) {
-               assertFalse(f.equals(anotherFile));
-            }
-        }
-
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/indices/recovery/RecoveryStateTests.java b/core/src/test/java/org/elasticsearch/indices/recovery/RecoveryStateTests.java
new file mode 100644
index 0000000..3a571a6
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/indices/recovery/RecoveryStateTests.java
@@ -0,0 +1,531 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.indices.recovery;
+
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.io.stream.BytesStreamOutput;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.Streamable;
+import org.elasticsearch.common.transport.DummyTransportAddress;
+import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.indices.recovery.RecoveryState.*;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.atomic.AtomicReference;
+
+import static org.elasticsearch.test.VersionUtils.randomVersion;
+import static org.hamcrest.Matchers.*;
+
+public class RecoveryStateTests extends ESTestCase {
+
+    abstract class Streamer<T extends Streamable> extends Thread {
+
+        private T lastRead;
+        final private AtomicBoolean shouldStop;
+        final private T source;
+        final AtomicReference<Throwable> error = new AtomicReference<>();
+        final Version streamVersion;
+
+        Streamer(AtomicBoolean shouldStop, T source) {
+            this(shouldStop, source, randomVersion(random()));
+        }
+
+        Streamer(AtomicBoolean shouldStop, T source, Version streamVersion) {
+            this.shouldStop = shouldStop;
+            this.source = source;
+            this.streamVersion = streamVersion;
+        }
+
+        public T lastRead() throws Throwable {
+            Throwable t = error.get();
+            if (t != null) {
+                throw t;
+            }
+            return lastRead;
+        }
+
+        public T serializeDeserialize() throws IOException {
+            BytesStreamOutput out = new BytesStreamOutput();
+            source.writeTo(out);
+            out.close();
+            StreamInput in = StreamInput.wrap(out.bytes());
+            T obj = deserialize(in);
+            lastRead = obj;
+            return obj;
+        }
+
+        protected T deserialize(StreamInput in) throws IOException {
+            T obj = createObj();
+            obj.readFrom(in);
+            return obj;
+        }
+
+        abstract T createObj();
+
+        @Override
+        public void run() {
+            try {
+                while (shouldStop.get() == false) {
+                    serializeDeserialize();
+                }
+                serializeDeserialize();
+            } catch (Throwable t) {
+                error.set(t);
+            }
+        }
+    }
+
+    public void testTimers() throws Throwable {
+        final Timer timer;
+        Streamer<Timer> streamer;
+        AtomicBoolean stop = new AtomicBoolean();
+        if (randomBoolean()) {
+            timer = new Timer();
+            streamer = new Streamer<Timer>(stop, timer) {
+                @Override
+                Timer createObj() {
+                    return new Timer();
+                }
+            };
+        } else if (randomBoolean()) {
+            timer = new Index();
+            streamer = new Streamer<Timer>(stop, timer) {
+                @Override
+                Timer createObj() {
+                    return new Index();
+                }
+            };
+        } else if (randomBoolean()) {
+            timer = new VerifyIndex();
+            streamer = new Streamer<Timer>(stop, timer) {
+                @Override
+                Timer createObj() {
+                    return new VerifyIndex();
+                }
+            };
+        } else {
+            timer = new Translog();
+            streamer = new Streamer<Timer>(stop, timer) {
+                @Override
+                Timer createObj() {
+                    return new Translog();
+                }
+            };
+        }
+
+        timer.start();
+        assertThat(timer.startTime(), greaterThan(0l));
+        assertThat(timer.stopTime(), equalTo(0l));
+        Timer lastRead = streamer.serializeDeserialize();
+        final long time = lastRead.time();
+        assertThat(time, lessThanOrEqualTo(timer.time()));
+        assertBusy(new Runnable() {
+            @Override
+            public void run() {
+                assertThat("timer timer should progress compared to captured one ", time, lessThan(timer.time()));
+            }
+        });
+        assertThat("captured time shouldn't change", lastRead.time(), equalTo(time));
+
+        if (randomBoolean()) {
+            timer.stop();
+            assertThat(timer.stopTime(), greaterThanOrEqualTo(timer.startTime()));
+            assertThat(timer.time(), greaterThan(0l));
+            lastRead = streamer.serializeDeserialize();
+            assertThat(lastRead.startTime(), equalTo(timer.startTime()));
+            assertThat(lastRead.time(), equalTo(timer.time()));
+            assertThat(lastRead.stopTime(), equalTo(timer.stopTime()));
+        }
+
+        timer.reset();
+        assertThat(timer.startTime(), equalTo(0l));
+        assertThat(timer.time(), equalTo(0l));
+        assertThat(timer.stopTime(), equalTo(0l));
+        lastRead = streamer.serializeDeserialize();
+        assertThat(lastRead.startTime(), equalTo(0l));
+        assertThat(lastRead.time(), equalTo(0l));
+        assertThat(lastRead.stopTime(), equalTo(0l));
+
+    }
+
+    public void testIndex() throws Throwable {
+        File[] files = new File[randomIntBetween(1, 20)];
+        ArrayList<File> filesToRecover = new ArrayList<>();
+        long totalFileBytes = 0;
+        long totalReusedBytes = 0;
+        int totalReused = 0;
+        for (int i = 0; i < files.length; i++) {
+            final int fileLength = randomIntBetween(1, 1000);
+            final boolean reused = randomBoolean();
+            totalFileBytes += fileLength;
+            files[i] = new RecoveryState.File("f_" + i, fileLength, reused);
+            if (reused) {
+                totalReused++;
+                totalReusedBytes += fileLength;
+            } else {
+                filesToRecover.add(files[i]);
+            }
+        }
+
+        Collections.shuffle(Arrays.asList(files));
+        final RecoveryState.Index index = new RecoveryState.Index();
+
+        if (randomBoolean()) {
+            // initialize with some data and then reset
+            index.start();
+            for (int i = randomIntBetween(0, 10); i > 0; i--) {
+                index.addFileDetail("t_" + i, randomIntBetween(1, 100), randomBoolean());
+                if (randomBoolean()) {
+                    index.addSourceThrottling(randomIntBetween(0, 20));
+                }
+                if (randomBoolean()) {
+                    index.addTargetThrottling(randomIntBetween(0, 20));
+                }
+            }
+            if (randomBoolean()) {
+                index.stop();
+            }
+            index.reset();
+        }
+
+
+        // before we start we must report 0
+        assertThat(index.recoveredFilesPercent(), equalTo((float) 0.0));
+        assertThat(index.recoveredBytesPercent(), equalTo((float) 0.0));
+        assertThat(index.sourceThrottling().nanos(), equalTo(Index.UNKNOWN));
+        assertThat(index.targetThrottling().nanos(), equalTo(Index.UNKNOWN));
+
+        index.start();
+        for (File file : files) {
+            index.addFileDetail(file.name(), file.length(), file.reused());
+        }
+
+        logger.info("testing initial information");
+        assertThat(index.totalBytes(), equalTo(totalFileBytes));
+        assertThat(index.reusedBytes(), equalTo(totalReusedBytes));
+        assertThat(index.totalRecoverBytes(), equalTo(totalFileBytes - totalReusedBytes));
+        assertThat(index.totalFileCount(), equalTo(files.length));
+        assertThat(index.reusedFileCount(), equalTo(totalReused));
+        assertThat(index.totalRecoverFiles(), equalTo(filesToRecover.size()));
+        assertThat(index.recoveredFileCount(), equalTo(0));
+        assertThat(index.recoveredBytes(), equalTo(0l));
+        assertThat(index.recoveredFilesPercent(), equalTo(filesToRecover.size() == 0 ? 100.0f : 0.0f));
+        assertThat(index.recoveredBytesPercent(), equalTo(filesToRecover.size() == 0 ? 100.0f : 0.0f));
+
+
+        long bytesToRecover = totalFileBytes - totalReusedBytes;
+        boolean completeRecovery = bytesToRecover == 0 || randomBoolean();
+        if (completeRecovery == false) {
+            bytesToRecover = randomIntBetween(1, (int) bytesToRecover);
+            logger.info("performing partial recovery ([{}] bytes of [{}])", bytesToRecover, totalFileBytes - totalReusedBytes);
+        }
+        AtomicBoolean streamShouldStop = new AtomicBoolean();
+
+        Streamer<Index> backgroundReader = new Streamer<RecoveryState.Index>(streamShouldStop, index) {
+            @Override
+            Index createObj() {
+                return new Index();
+            }
+        };
+
+        backgroundReader.start();
+
+        long recoveredBytes = 0;
+        long sourceThrottling = Index.UNKNOWN;
+        long targetThrottling = Index.UNKNOWN;
+        while (bytesToRecover > 0) {
+            File file = randomFrom(filesToRecover);
+            final long toRecover = Math.min(bytesToRecover, randomIntBetween(1, (int) (file.length() - file.recovered())));
+            final long throttledOnSource = rarely() ? randomIntBetween(10, 200) : 0;
+            index.addSourceThrottling(throttledOnSource);
+            if (sourceThrottling == Index.UNKNOWN) {
+                sourceThrottling = throttledOnSource;
+            } else {
+                sourceThrottling += throttledOnSource;
+            }
+            index.addRecoveredBytesToFile(file.name(), toRecover);
+            file.addRecoveredBytes(toRecover);
+            final long throttledOnTarget = rarely() ? randomIntBetween(10, 200) : 0;
+            if (targetThrottling == Index.UNKNOWN) {
+                targetThrottling = throttledOnTarget;
+            } else {
+                targetThrottling += throttledOnTarget;
+            }
+            index.addTargetThrottling(throttledOnTarget);
+            bytesToRecover -= toRecover;
+            recoveredBytes += toRecover;
+            if (file.reused() || file.fullyRecovered()) {
+                filesToRecover.remove(file);
+            }
+        }
+
+        if (completeRecovery) {
+            assertThat(filesToRecover.size(), equalTo(0));
+            index.stop();
+            assertThat(index.time(), greaterThanOrEqualTo(0l));
+        }
+
+        logger.info("testing serialized information");
+        streamShouldStop.set(true);
+        backgroundReader.join();
+        final Index lastRead = backgroundReader.lastRead();
+        assertThat(lastRead.fileDetails().toArray(), arrayContainingInAnyOrder(index.fileDetails().toArray()));
+        assertThat(lastRead.startTime(), equalTo(index.startTime()));
+        if (completeRecovery) {
+            assertThat(lastRead.time(), equalTo(index.time()));
+        } else {
+            assertThat(lastRead.time(), lessThanOrEqualTo(index.time()));
+        }
+        assertThat(lastRead.stopTime(), equalTo(index.stopTime()));
+        assertThat(lastRead.targetThrottling(), equalTo(index.targetThrottling()));
+        assertThat(lastRead.sourceThrottling(), equalTo(index.sourceThrottling()));
+
+        logger.info("testing post recovery");
+        assertThat(index.totalBytes(), equalTo(totalFileBytes));
+        assertThat(index.reusedBytes(), equalTo(totalReusedBytes));
+        assertThat(index.totalRecoverBytes(), equalTo(totalFileBytes - totalReusedBytes));
+        assertThat(index.totalFileCount(), equalTo(files.length));
+        assertThat(index.reusedFileCount(), equalTo(totalReused));
+        assertThat(index.totalRecoverFiles(), equalTo(files.length - totalReused));
+        assertThat(index.recoveredFileCount(), equalTo(index.totalRecoverFiles() - filesToRecover.size()));
+        assertThat(index.recoveredBytes(), equalTo(recoveredBytes));
+        assertThat(index.targetThrottling().nanos(), equalTo(targetThrottling));
+        assertThat(index.sourceThrottling().nanos(), equalTo(sourceThrottling));
+        if (index.totalRecoverFiles() == 0) {
+            assertThat((double) index.recoveredFilesPercent(), equalTo(100.0));
+            assertThat((double) index.recoveredBytesPercent(), equalTo(100.0));
+        } else {
+            assertThat((double) index.recoveredFilesPercent(), closeTo(100.0 * index.recoveredFileCount() / index.totalRecoverFiles(), 0.1));
+            assertThat((double) index.recoveredBytesPercent(), closeTo(100.0 * index.recoveredBytes() / index.totalRecoverBytes(), 0.1));
+        }
+    }
+
+    public void testStageSequenceEnforcement() {
+        final DiscoveryNode discoveryNode = new DiscoveryNode("1", DummyTransportAddress.INSTANCE, Version.CURRENT);
+        Stage[] stages = Stage.values();
+        int i = randomIntBetween(0, stages.length - 1);
+        int j;
+        do {
+            j = randomIntBetween(0, stages.length - 1);
+        } while (j == i);
+        Stage t = stages[i];
+        stages[i] = stages[j];
+        stages[j] = t;
+        try {
+            RecoveryState state = new RecoveryState(new ShardId("bla", 0), randomBoolean(), randomFrom(Type.values()), discoveryNode, discoveryNode);
+            for (Stage stage : stages) {
+                state.setStage(stage);
+            }
+            fail("succeeded in performing the illegal sequence [" + Strings.arrayToCommaDelimitedString(stages) + "]");
+        } catch (IllegalStateException e) {
+            // cool
+        }
+
+        // but reset should be always possible.
+        stages = Stage.values();
+        i = randomIntBetween(1, stages.length - 1);
+        ArrayList<Stage> list = new ArrayList<>(Arrays.asList(Arrays.copyOfRange(stages, 0, i)));
+        list.addAll(Arrays.asList(stages));
+        RecoveryState state = new RecoveryState(new ShardId("bla", 0), randomBoolean(), randomFrom(Type.values()), discoveryNode, discoveryNode);
+        for (Stage stage : list) {
+            state.setStage(stage);
+        }
+
+        assertThat(state.getStage(), equalTo(Stage.DONE));
+    }
+
+    public void testTranslog() throws Throwable {
+        final Translog translog = new Translog();
+        AtomicBoolean stop = new AtomicBoolean();
+        Streamer<Translog> streamer = new Streamer<Translog>(stop, translog) {
+            @Override
+            Translog createObj() {
+                return new Translog();
+            }
+        };
+
+        // we don't need to test the time aspect, it's done in the timer test
+        translog.start();
+        assertThat(translog.recoveredOperations(), equalTo(0));
+        assertThat(translog.totalOperations(), equalTo(Translog.UNKNOWN));
+        assertThat(translog.totalOperationsOnStart(), equalTo(Translog.UNKNOWN));
+        streamer.start();
+        // force one
+        streamer.serializeDeserialize();
+        int ops = 0;
+        int totalOps = 0;
+        int totalOpsOnStart = randomIntBetween(10, 200);
+        translog.totalOperationsOnStart(totalOpsOnStart);
+        for (int i = scaledRandomIntBetween(10, 200); i > 0; i--) {
+            final int iterationOps = randomIntBetween(1, 10);
+            totalOps += iterationOps;
+            translog.totalOperations(totalOps);
+            assertThat((double) translog.recoveredPercent(), closeTo(100.0 * ops / totalOps, 0.1));
+            for (int j = iterationOps; j > 0; j--) {
+                ops++;
+                translog.incrementRecoveredOperations();
+                if (randomBoolean()) {
+                    translog.decrementRecoveredOperations(1);
+                    translog.incrementRecoveredOperations();
+                }
+            }
+            assertThat(translog.recoveredOperations(), equalTo(ops));
+            assertThat(translog.totalOperations(), equalTo(totalOps));
+            assertThat(translog.recoveredPercent(), equalTo(100.f));
+            assertThat(streamer.lastRead().recoveredOperations(), greaterThanOrEqualTo(0));
+            assertThat(streamer.lastRead().recoveredOperations(), lessThanOrEqualTo(ops));
+            assertThat(streamer.lastRead().totalOperations(), lessThanOrEqualTo(totalOps));
+            assertThat(streamer.lastRead().totalOperationsOnStart(), lessThanOrEqualTo(totalOpsOnStart));
+            assertThat(streamer.lastRead().recoveredPercent(), either(greaterThanOrEqualTo(0.f)).or(equalTo(-1.f)));
+        }
+
+        boolean stopped = false;
+        if (randomBoolean()) {
+            translog.stop();
+            stopped = true;
+        }
+
+        if (randomBoolean()) {
+            translog.reset();
+            ops = 0;
+            totalOps = Translog.UNKNOWN;
+            totalOpsOnStart = Translog.UNKNOWN;
+            assertThat(translog.recoveredOperations(), equalTo(0));
+            assertThat(translog.totalOperationsOnStart(), equalTo(Translog.UNKNOWN));
+            assertThat(translog.totalOperations(), equalTo(Translog.UNKNOWN));
+        }
+
+        stop.set(true);
+        streamer.join();
+        final Translog lastRead = streamer.lastRead();
+        assertThat(lastRead.recoveredOperations(), equalTo(ops));
+        assertThat(lastRead.totalOperations(), equalTo(totalOps));
+        assertThat(lastRead.totalOperationsOnStart(), equalTo(totalOpsOnStart));
+        assertThat(lastRead.startTime(), equalTo(translog.startTime()));
+        assertThat(lastRead.stopTime(), equalTo(translog.stopTime()));
+
+        if (stopped) {
+            assertThat(lastRead.time(), equalTo(translog.time()));
+        } else {
+            assertThat(lastRead.time(), lessThanOrEqualTo(translog.time()));
+        }
+    }
+
+    public void testStart() throws IOException {
+        final VerifyIndex verifyIndex = new VerifyIndex();
+        AtomicBoolean stop = new AtomicBoolean();
+        Streamer<VerifyIndex> streamer = new Streamer<VerifyIndex>(stop, verifyIndex) {
+            @Override
+            VerifyIndex createObj() {
+                return new VerifyIndex();
+            }
+        };
+
+        // we don't need to test the time aspect, it's done in the timer test
+        verifyIndex.start();
+        assertThat(verifyIndex.checkIndexTime(), equalTo(0l));
+        // force one
+        VerifyIndex lastRead = streamer.serializeDeserialize();
+        assertThat(lastRead.checkIndexTime(), equalTo(0l));
+
+        long took = randomLong();
+        if (took < 0) {
+            took = -took;
+            took = Math.max(0l, took);
+
+        }
+        verifyIndex.checkIndexTime(took);
+        assertThat(verifyIndex.checkIndexTime(), equalTo(took));
+
+        boolean stopped = false;
+        if (randomBoolean()) {
+            verifyIndex.stop();
+            stopped = true;
+        }
+
+        if (randomBoolean()) {
+            verifyIndex.reset();
+            took = 0;
+            assertThat(verifyIndex.checkIndexTime(), equalTo(took));
+        }
+
+        lastRead = streamer.serializeDeserialize();
+        assertThat(lastRead.checkIndexTime(), equalTo(took));
+        assertThat(lastRead.startTime(), equalTo(verifyIndex.startTime()));
+        assertThat(lastRead.stopTime(), equalTo(verifyIndex.stopTime()));
+
+        if (stopped) {
+            assertThat(lastRead.time(), equalTo(verifyIndex.time()));
+        } else {
+            assertThat(lastRead.time(), lessThanOrEqualTo(verifyIndex.time()));
+        }
+    }
+
+    @Test
+    public void testConcurrentModificationIndexFileDetailsMap() throws InterruptedException {
+        final Index index = new Index();
+        final AtomicBoolean stop = new AtomicBoolean(false);
+        Streamer<Index> readWriteIndex = new Streamer<Index>(stop, index) {
+            @Override
+            Index createObj() {
+                return new Index();
+            }
+        };
+        Thread modifyThread = new Thread() {
+            public void run() {
+                for (int i = 0; i < 1000; i++) {
+                    index.addFileDetail(randomAsciiOfLength(10), 100, true);
+                }
+                stop.set(true);
+            }
+        };
+        readWriteIndex.start();
+        modifyThread.start();
+        modifyThread.join();
+        readWriteIndex.join();
+        assertThat(readWriteIndex.error.get(), equalTo(null));
+    }
+
+    @Test
+    public void testFileHashCodeAndEquals() {
+        File f = new File("foo", randomIntBetween(0, 100), randomBoolean());
+        File anotherFile = new File(f.name(), f.length(), f.reused());
+        assertEquals(f, anotherFile);
+        assertEquals(f.hashCode(), anotherFile.hashCode());
+        int iters = randomIntBetween(10, 100);
+        for (int i = 0; i < iters; i++) {
+            f = new File("foo", randomIntBetween(0, 100), randomBoolean());
+            anotherFile = new File(f.name(), randomIntBetween(0, 100), randomBoolean());
+            if (f.equals(anotherFile)) {
+                assertEquals(f.hashCode(), anotherFile.hashCode());
+            } else if (f.hashCode() != anotherFile.hashCode()) {
+               assertFalse(f.equals(anotherFile));
+            }
+        }
+
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/indices/recovery/StartRecoveryRequestTest.java b/core/src/test/java/org/elasticsearch/indices/recovery/StartRecoveryRequestTest.java
deleted file mode 100644
index 52c46c7..0000000
--- a/core/src/test/java/org/elasticsearch/indices/recovery/StartRecoveryRequestTest.java
+++ /dev/null
@@ -1,76 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.indices.recovery;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.common.io.stream.InputStreamStreamInput;
-import org.elasticsearch.common.io.stream.OutputStreamStreamOutput;
-import org.elasticsearch.common.transport.LocalTransportAddress;
-import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.index.store.Store;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.io.ByteArrayInputStream;
-import java.io.ByteArrayOutputStream;
-
-import static org.elasticsearch.test.VersionUtils.randomVersion;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.nullValue;
-
-/**
- */
-public class StartRecoveryRequestTest extends ESTestCase {
-
-    @Test
-    public void testSerialization() throws Exception {
-        Version targetNodeVersion = randomVersion(random());
-        StartRecoveryRequest outRequest = new StartRecoveryRequest(
-                new ShardId("test", 0),
-                new DiscoveryNode("a", new LocalTransportAddress("1"), targetNodeVersion),
-                new DiscoveryNode("b", new LocalTransportAddress("1"), targetNodeVersion),
-                true,
-                Store.MetadataSnapshot.EMPTY,
-                RecoveryState.Type.RELOCATION,
-                1l
-
-        );
-        ByteArrayOutputStream outBuffer = new ByteArrayOutputStream();
-        OutputStreamStreamOutput out = new OutputStreamStreamOutput(outBuffer);
-        out.setVersion(targetNodeVersion);
-        outRequest.writeTo(out);
-
-        ByteArrayInputStream inBuffer = new ByteArrayInputStream(outBuffer.toByteArray());
-        InputStreamStreamInput in = new InputStreamStreamInput(inBuffer);
-        in.setVersion(targetNodeVersion);
-        StartRecoveryRequest inRequest = new StartRecoveryRequest();
-        inRequest.readFrom(in);
-
-        assertThat(outRequest.shardId(), equalTo(inRequest.shardId()));
-        assertThat(outRequest.sourceNode(), equalTo(inRequest.sourceNode()));
-        assertThat(outRequest.targetNode(), equalTo(inRequest.targetNode()));
-        assertThat(outRequest.markAsRelocated(), equalTo(inRequest.markAsRelocated()));
-        assertThat(outRequest.metadataSnapshot().asMap(), equalTo(inRequest.metadataSnapshot().asMap()));
-        assertThat(outRequest.recoveryId(), equalTo(inRequest.recoveryId()));
-        assertThat(outRequest.recoveryType(), equalTo(inRequest.recoveryType()));
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/indices/recovery/StartRecoveryRequestTests.java b/core/src/test/java/org/elasticsearch/indices/recovery/StartRecoveryRequestTests.java
new file mode 100644
index 0000000..4df8257
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/indices/recovery/StartRecoveryRequestTests.java
@@ -0,0 +1,76 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.indices.recovery;
+
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.common.io.stream.InputStreamStreamInput;
+import org.elasticsearch.common.io.stream.OutputStreamStreamOutput;
+import org.elasticsearch.common.transport.LocalTransportAddress;
+import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.index.store.Store;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+
+import static org.elasticsearch.test.VersionUtils.randomVersion;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.nullValue;
+
+/**
+ */
+public class StartRecoveryRequestTests extends ESTestCase {
+
+    @Test
+    public void testSerialization() throws Exception {
+        Version targetNodeVersion = randomVersion(random());
+        StartRecoveryRequest outRequest = new StartRecoveryRequest(
+                new ShardId("test", 0),
+                new DiscoveryNode("a", new LocalTransportAddress("1"), targetNodeVersion),
+                new DiscoveryNode("b", new LocalTransportAddress("1"), targetNodeVersion),
+                true,
+                Store.MetadataSnapshot.EMPTY,
+                RecoveryState.Type.RELOCATION,
+                1l
+
+        );
+        ByteArrayOutputStream outBuffer = new ByteArrayOutputStream();
+        OutputStreamStreamOutput out = new OutputStreamStreamOutput(outBuffer);
+        out.setVersion(targetNodeVersion);
+        outRequest.writeTo(out);
+
+        ByteArrayInputStream inBuffer = new ByteArrayInputStream(outBuffer.toByteArray());
+        InputStreamStreamInput in = new InputStreamStreamInput(inBuffer);
+        in.setVersion(targetNodeVersion);
+        StartRecoveryRequest inRequest = new StartRecoveryRequest();
+        inRequest.readFrom(in);
+
+        assertThat(outRequest.shardId(), equalTo(inRequest.shardId()));
+        assertThat(outRequest.sourceNode(), equalTo(inRequest.sourceNode()));
+        assertThat(outRequest.targetNode(), equalTo(inRequest.targetNode()));
+        assertThat(outRequest.markAsRelocated(), equalTo(inRequest.markAsRelocated()));
+        assertThat(outRequest.metadataSnapshot().asMap(), equalTo(inRequest.metadataSnapshot().asMap()));
+        assertThat(outRequest.recoveryId(), equalTo(inRequest.recoveryId()));
+        assertThat(outRequest.recoveryType(), equalTo(inRequest.recoveryType()));
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/percolator/MultiPercolatorIT.java b/core/src/test/java/org/elasticsearch/percolator/MultiPercolatorIT.java
index cb64437..77a4b63 100644
--- a/core/src/test/java/org/elasticsearch/percolator/MultiPercolatorIT.java
+++ b/core/src/test/java/org/elasticsearch/percolator/MultiPercolatorIT.java
@@ -26,7 +26,6 @@ import org.elasticsearch.client.Requests;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.MatchQueryBuilder;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
@@ -361,7 +360,7 @@ public class MultiPercolatorIT extends ESIntegTestCase {
         ensureGreen("nestedindex");
 
         client().prepareIndex("nestedindex", PercolatorService.TYPE_NAME, "Q").setSource(jsonBuilder().startObject()
-                .field("query", QueryBuilders.nestedQuery("employee", QueryBuilders.matchQuery("employee.name", "virginia potts").operator(Operator.AND)).scoreMode("avg")).endObject()).get();
+                .field("query", QueryBuilders.nestedQuery("employee", QueryBuilders.matchQuery("employee.name", "virginia potts").operator(MatchQueryBuilder.Operator.AND)).scoreMode("avg")).endObject()).get();
 
         refresh();
 
diff --git a/core/src/test/java/org/elasticsearch/percolator/PercolatorBackwardsCompatibilityIT.java b/core/src/test/java/org/elasticsearch/percolator/PercolatorBackwardsCompatibilityIT.java
index f250e92..ecee193 100644
--- a/core/src/test/java/org/elasticsearch/percolator/PercolatorBackwardsCompatibilityIT.java
+++ b/core/src/test/java/org/elasticsearch/percolator/PercolatorBackwardsCompatibilityIT.java
@@ -23,8 +23,8 @@ import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.percolate.PercolateResponse;
 import org.elasticsearch.action.percolate.PercolateSourceBuilder;
 import org.elasticsearch.index.percolator.PercolatorException;
+import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.test.ESIntegTestCase;
-import org.elasticsearch.index.query.QueryShardException;
 import org.junit.Test;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
@@ -67,7 +67,7 @@ public class PercolatorBackwardsCompatibilityIT extends ESIntegTestCase {
             fail();
         } catch (PercolatorException e) {
             e.printStackTrace();
-            assertThat(e.getRootCause(), instanceOf(QueryShardException.class));
+            assertThat(e.getRootCause(), instanceOf(QueryParsingException.class));
         }
     }
 
diff --git a/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java b/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java
index 004771a..fb37a0d 100644
--- a/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java
+++ b/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java
@@ -42,9 +42,8 @@ import org.elasticsearch.index.engine.DocumentMissingException;
 import org.elasticsearch.index.engine.VersionConflictEngineException;
 import org.elasticsearch.index.percolator.PercolatorException;
 import org.elasticsearch.index.query.MatchQueryBuilder;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.query.functionscore.factor.FactorBuilder;
 import org.elasticsearch.index.query.support.QueryInnerHitBuilder;
 import org.elasticsearch.rest.RestStatus;
@@ -1765,7 +1764,7 @@ public class PercolatorIT extends ESIntegTestCase {
                     .get();
             fail();
         } catch (PercolatorException e) {
-            assertThat(e.getRootCause(), instanceOf(QueryShardException.class));
+            assertThat(e.getRootCause(), instanceOf(QueryParsingException.class));
         }
 
         try {
@@ -1774,7 +1773,7 @@ public class PercolatorIT extends ESIntegTestCase {
                     .get();
             fail();
         } catch (PercolatorException e) {
-            assertThat(e.getRootCause(), instanceOf(QueryShardException.class));
+            assertThat(e.getRootCause(), instanceOf(QueryParsingException.class));
         }
     }
 
@@ -1813,7 +1812,7 @@ public class PercolatorIT extends ESIntegTestCase {
         ensureGreen("nestedindex");
 
         client().prepareIndex("nestedindex", PercolatorService.TYPE_NAME, "Q").setSource(jsonBuilder().startObject()
-                .field("query", QueryBuilders.nestedQuery("employee", QueryBuilders.matchQuery("employee.name", "virginia potts").operator(Operator.AND)).scoreMode("avg")).endObject()).get();
+                .field("query", QueryBuilders.nestedQuery("employee", QueryBuilders.matchQuery("employee.name", "virginia potts").operator(MatchQueryBuilder.Operator.AND)).scoreMode("avg")).endObject()).get();
 
         refresh();
 
@@ -2017,7 +2016,7 @@ public class PercolatorIT extends ESIntegTestCase {
                     .execute().actionGet();
             fail("Expected a parse error, because inner_hits isn't supported in the percolate api");
         } catch (Exception e) {
-            assertThat(e.getCause(), instanceOf(QueryShardException.class));
+            assertThat(e.getCause(), instanceOf(QueryParsingException.class));
             assertThat(e.getCause().getMessage(), containsString("inner_hits unsupported"));
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/recovery/RecoverySettingsTest.java b/core/src/test/java/org/elasticsearch/recovery/RecoverySettingsTest.java
deleted file mode 100644
index 3d67886..0000000
--- a/core/src/test/java/org/elasticsearch/recovery/RecoverySettingsTest.java
+++ /dev/null
@@ -1,141 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.recovery;
-
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.ByteSizeUnit;
-import org.elasticsearch.indices.recovery.RecoverySettings;
-import org.elasticsearch.test.ESSingleNodeTestCase;
-import org.junit.Test;
-
-import java.util.concurrent.TimeUnit;
-
-public class RecoverySettingsTest extends ESSingleNodeTestCase {
-
-    @Override
-    protected boolean resetNodeAfterTest() {
-        return true;
-    }
-
-    @Test
-    public void testAllSettingsAreDynamicallyUpdatable() {
-        innerTestSettings(RecoverySettings.INDICES_RECOVERY_FILE_CHUNK_SIZE, randomIntBetween(1, 200), ByteSizeUnit.BYTES, new Validator() {
-            @Override
-            public void validate(RecoverySettings recoverySettings, int expectedValue) {
-                assertEquals(expectedValue, recoverySettings.fileChunkSize().bytesAsInt());
-            }
-        });
-        innerTestSettings(RecoverySettings.INDICES_RECOVERY_TRANSLOG_OPS, randomIntBetween(1, 200), new Validator() {
-            @Override
-            public void validate(RecoverySettings recoverySettings, int expectedValue) {
-                assertEquals(expectedValue, recoverySettings.translogOps());
-            }
-        });
-        innerTestSettings(RecoverySettings.INDICES_RECOVERY_TRANSLOG_SIZE, randomIntBetween(1, 200), ByteSizeUnit.BYTES, new Validator() {
-            @Override
-            public void validate(RecoverySettings recoverySettings, int expectedValue) {
-                assertEquals(expectedValue, recoverySettings.translogSize().bytesAsInt());
-            }
-        });
-        innerTestSettings(RecoverySettings.INDICES_RECOVERY_CONCURRENT_STREAMS, randomIntBetween(1, 200), new Validator() {
-            @Override
-            public void validate(RecoverySettings recoverySettings, int expectedValue) {
-                assertEquals(expectedValue, recoverySettings.concurrentStreamPool().getMaximumPoolSize());
-            }
-        });
-        innerTestSettings(RecoverySettings.INDICES_RECOVERY_CONCURRENT_SMALL_FILE_STREAMS, randomIntBetween(1, 200), new Validator() {
-            @Override
-            public void validate(RecoverySettings recoverySettings, int expectedValue) {
-                assertEquals(expectedValue, recoverySettings.concurrentSmallFileStreamPool().getMaximumPoolSize());
-            }
-        });
-        innerTestSettings(RecoverySettings.INDICES_RECOVERY_MAX_BYTES_PER_SEC, 0, new Validator() {
-            @Override
-            public void validate(RecoverySettings recoverySettings, int expectedValue) {
-                assertEquals(null, recoverySettings.rateLimiter());
-            }
-        });
-        innerTestSettings(RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_STATE_SYNC, randomIntBetween(1, 200), TimeUnit.MILLISECONDS, new Validator() {
-            @Override
-            public void validate(RecoverySettings recoverySettings, int expectedValue) {
-                assertEquals(expectedValue, recoverySettings.retryDelayStateSync().millis());
-            }
-        });
-        innerTestSettings(RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_NETWORK, randomIntBetween(1, 200), TimeUnit.MILLISECONDS, new Validator() {
-            @Override
-            public void validate(RecoverySettings recoverySettings, int expectedValue) {
-                assertEquals(expectedValue, recoverySettings.retryDelayNetwork().millis());
-            }
-        });
-        innerTestSettings(RecoverySettings.INDICES_RECOVERY_ACTIVITY_TIMEOUT, randomIntBetween(1, 200), TimeUnit.MILLISECONDS, new Validator() {
-            @Override
-            public void validate(RecoverySettings recoverySettings, int expectedValue) {
-                assertEquals(expectedValue, recoverySettings.activityTimeout().millis());
-            }
-        });
-        innerTestSettings(RecoverySettings.INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT, randomIntBetween(1, 200), TimeUnit.MILLISECONDS, new Validator() {
-            @Override
-            public void validate(RecoverySettings recoverySettings, int expectedValue) {
-                assertEquals(expectedValue, recoverySettings.internalActionTimeout().millis());
-            }
-        });
-        innerTestSettings(RecoverySettings.INDICES_RECOVERY_INTERNAL_LONG_ACTION_TIMEOUT, randomIntBetween(1, 200), TimeUnit.MILLISECONDS, new Validator() {
-            @Override
-            public void validate(RecoverySettings recoverySettings, int expectedValue) {
-                assertEquals(expectedValue, recoverySettings.internalActionLongTimeout().millis());
-            }
-        });
-
-        innerTestSettings(RecoverySettings.INDICES_RECOVERY_COMPRESS, false, new Validator() {
-            @Override
-            public void validate(RecoverySettings recoverySettings, boolean expectedValue) {
-                assertEquals(expectedValue, recoverySettings.compress());
-            }
-        });
-    }
-
-    private static class Validator {
-        public void validate(RecoverySettings recoverySettings, int expectedValue) {
-        }
-
-        public void validate(RecoverySettings recoverySettings, boolean expectedValue) {
-        }
-    }
-
-    private void innerTestSettings(String key, int newValue, Validator validator) {
-        client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.builder().put(key, newValue)).get();
-        validator.validate(getInstanceFromNode(RecoverySettings.class), newValue);
-    }
-
-    private void innerTestSettings(String key, int newValue, TimeUnit timeUnit, Validator validator) {
-        client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.builder().put(key, newValue, timeUnit)).get();
-        validator.validate(getInstanceFromNode(RecoverySettings.class), newValue);
-    }
-
-    private void innerTestSettings(String key, int newValue, ByteSizeUnit byteSizeUnit, Validator validator) {
-        client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.builder().put(key, newValue, byteSizeUnit)).get();
-        validator.validate(getInstanceFromNode(RecoverySettings.class), newValue);
-    }
-
-    private void innerTestSettings(String key, boolean newValue, Validator validator) {
-        client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.builder().put(key, newValue)).get();
-        validator.validate(getInstanceFromNode(RecoverySettings.class), newValue);
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/recovery/RecoverySettingsTests.java b/core/src/test/java/org/elasticsearch/recovery/RecoverySettingsTests.java
new file mode 100644
index 0000000..a722e3a
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/recovery/RecoverySettingsTests.java
@@ -0,0 +1,141 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.recovery;
+
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.ByteSizeUnit;
+import org.elasticsearch.indices.recovery.RecoverySettings;
+import org.elasticsearch.test.ESSingleNodeTestCase;
+import org.junit.Test;
+
+import java.util.concurrent.TimeUnit;
+
+public class RecoverySettingsTests extends ESSingleNodeTestCase {
+
+    @Override
+    protected boolean resetNodeAfterTest() {
+        return true;
+    }
+
+    @Test
+    public void testAllSettingsAreDynamicallyUpdatable() {
+        innerTestSettings(RecoverySettings.INDICES_RECOVERY_FILE_CHUNK_SIZE, randomIntBetween(1, 200), ByteSizeUnit.BYTES, new Validator() {
+            @Override
+            public void validate(RecoverySettings recoverySettings, int expectedValue) {
+                assertEquals(expectedValue, recoverySettings.fileChunkSize().bytesAsInt());
+            }
+        });
+        innerTestSettings(RecoverySettings.INDICES_RECOVERY_TRANSLOG_OPS, randomIntBetween(1, 200), new Validator() {
+            @Override
+            public void validate(RecoverySettings recoverySettings, int expectedValue) {
+                assertEquals(expectedValue, recoverySettings.translogOps());
+            }
+        });
+        innerTestSettings(RecoverySettings.INDICES_RECOVERY_TRANSLOG_SIZE, randomIntBetween(1, 200), ByteSizeUnit.BYTES, new Validator() {
+            @Override
+            public void validate(RecoverySettings recoverySettings, int expectedValue) {
+                assertEquals(expectedValue, recoverySettings.translogSize().bytesAsInt());
+            }
+        });
+        innerTestSettings(RecoverySettings.INDICES_RECOVERY_CONCURRENT_STREAMS, randomIntBetween(1, 200), new Validator() {
+            @Override
+            public void validate(RecoverySettings recoverySettings, int expectedValue) {
+                assertEquals(expectedValue, recoverySettings.concurrentStreamPool().getMaximumPoolSize());
+            }
+        });
+        innerTestSettings(RecoverySettings.INDICES_RECOVERY_CONCURRENT_SMALL_FILE_STREAMS, randomIntBetween(1, 200), new Validator() {
+            @Override
+            public void validate(RecoverySettings recoverySettings, int expectedValue) {
+                assertEquals(expectedValue, recoverySettings.concurrentSmallFileStreamPool().getMaximumPoolSize());
+            }
+        });
+        innerTestSettings(RecoverySettings.INDICES_RECOVERY_MAX_BYTES_PER_SEC, 0, new Validator() {
+            @Override
+            public void validate(RecoverySettings recoverySettings, int expectedValue) {
+                assertEquals(null, recoverySettings.rateLimiter());
+            }
+        });
+        innerTestSettings(RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_STATE_SYNC, randomIntBetween(1, 200), TimeUnit.MILLISECONDS, new Validator() {
+            @Override
+            public void validate(RecoverySettings recoverySettings, int expectedValue) {
+                assertEquals(expectedValue, recoverySettings.retryDelayStateSync().millis());
+            }
+        });
+        innerTestSettings(RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_NETWORK, randomIntBetween(1, 200), TimeUnit.MILLISECONDS, new Validator() {
+            @Override
+            public void validate(RecoverySettings recoverySettings, int expectedValue) {
+                assertEquals(expectedValue, recoverySettings.retryDelayNetwork().millis());
+            }
+        });
+        innerTestSettings(RecoverySettings.INDICES_RECOVERY_ACTIVITY_TIMEOUT, randomIntBetween(1, 200), TimeUnit.MILLISECONDS, new Validator() {
+            @Override
+            public void validate(RecoverySettings recoverySettings, int expectedValue) {
+                assertEquals(expectedValue, recoverySettings.activityTimeout().millis());
+            }
+        });
+        innerTestSettings(RecoverySettings.INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT, randomIntBetween(1, 200), TimeUnit.MILLISECONDS, new Validator() {
+            @Override
+            public void validate(RecoverySettings recoverySettings, int expectedValue) {
+                assertEquals(expectedValue, recoverySettings.internalActionTimeout().millis());
+            }
+        });
+        innerTestSettings(RecoverySettings.INDICES_RECOVERY_INTERNAL_LONG_ACTION_TIMEOUT, randomIntBetween(1, 200), TimeUnit.MILLISECONDS, new Validator() {
+            @Override
+            public void validate(RecoverySettings recoverySettings, int expectedValue) {
+                assertEquals(expectedValue, recoverySettings.internalActionLongTimeout().millis());
+            }
+        });
+
+        innerTestSettings(RecoverySettings.INDICES_RECOVERY_COMPRESS, false, new Validator() {
+            @Override
+            public void validate(RecoverySettings recoverySettings, boolean expectedValue) {
+                assertEquals(expectedValue, recoverySettings.compress());
+            }
+        });
+    }
+
+    private static class Validator {
+        public void validate(RecoverySettings recoverySettings, int expectedValue) {
+        }
+
+        public void validate(RecoverySettings recoverySettings, boolean expectedValue) {
+        }
+    }
+
+    private void innerTestSettings(String key, int newValue, Validator validator) {
+        client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.builder().put(key, newValue)).get();
+        validator.validate(getInstanceFromNode(RecoverySettings.class), newValue);
+    }
+
+    private void innerTestSettings(String key, int newValue, TimeUnit timeUnit, Validator validator) {
+        client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.builder().put(key, newValue, timeUnit)).get();
+        validator.validate(getInstanceFromNode(RecoverySettings.class), newValue);
+    }
+
+    private void innerTestSettings(String key, int newValue, ByteSizeUnit byteSizeUnit, Validator validator) {
+        client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.builder().put(key, newValue, byteSizeUnit)).get();
+        validator.validate(getInstanceFromNode(RecoverySettings.class), newValue);
+    }
+
+    private void innerTestSettings(String key, boolean newValue, Validator validator) {
+        client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.builder().put(key, newValue)).get();
+        validator.validate(getInstanceFromNode(RecoverySettings.class), newValue);
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/rest/action/support/RestTableTest.java b/core/src/test/java/org/elasticsearch/rest/action/support/RestTableTest.java
deleted file mode 100644
index 3df6c25..0000000
--- a/core/src/test/java/org/elasticsearch/rest/action/support/RestTableTest.java
+++ /dev/null
@@ -1,82 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.rest.action.support;
-
-import org.elasticsearch.common.Table;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.rest.FakeRestRequest;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.util.ArrayList;
-import java.util.List;
-
-import static org.elasticsearch.rest.action.support.RestTable.buildDisplayHeaders;
-import static org.hamcrest.Matchers.*;
-
-public class RestTableTest extends ESTestCase {
-
-    private Table table = new Table();
-    private FakeRestRequest restRequest = new FakeRestRequest();
-
-    @Before
-    public void setup() {
-        table.startHeaders();
-        table.addCell("bulk.foo", "alias:f;desc:foo");
-        table.addCell("bulk.bar", "alias:b;desc:bar");
-        // should be matched as well due to the aliases
-        table.addCell("aliasedBulk", "alias:bulkWhatever;desc:bar");
-        table.addCell("aliasedSecondBulk", "alias:foobar,bulkolicious,bulkotastic;desc:bar");
-        // no match
-        table.addCell("unmatched", "alias:un.matched;desc:bar");
-        // invalid alias
-        table.addCell("invalidAliasesBulk", "alias:,,,;desc:bar");
-        table.endHeaders();
-    }
-
-    @Test
-    public void testThatDisplayHeadersSupportWildcards() throws Exception {
-        restRequest.params().put("h", "bulk*");
-        List<RestTable.DisplayHeader> headers = buildDisplayHeaders(table, restRequest);
-
-        List<String> headerNames = getHeaderNames(headers);
-        assertThat(headerNames, contains("bulk.foo", "bulk.bar", "aliasedBulk", "aliasedSecondBulk"));
-        assertThat(headerNames, not(hasItem("unmatched")));
-    }
-
-    @Test
-    public void testThatDisplayHeadersAreNotAddedTwice() throws Exception {
-        restRequest.params().put("h", "nonexistent,bulk*,bul*");
-        List<RestTable.DisplayHeader> headers = buildDisplayHeaders(table, restRequest);
-
-        List<String> headerNames = getHeaderNames(headers);
-        assertThat(headerNames, contains("bulk.foo", "bulk.bar", "aliasedBulk", "aliasedSecondBulk"));
-        assertThat(headerNames, not(hasItem("unmatched")));
-    }
-
-    private List<String> getHeaderNames(List<RestTable.DisplayHeader> headers) {
-        List<String> headerNames = new ArrayList<>();
-        for (RestTable.DisplayHeader header : headers) {
-            headerNames.add(header.name);
-        }
-
-        return headerNames;
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/rest/action/support/RestTableTests.java b/core/src/test/java/org/elasticsearch/rest/action/support/RestTableTests.java
new file mode 100644
index 0000000..237c62d
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/rest/action/support/RestTableTests.java
@@ -0,0 +1,82 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.rest.action.support;
+
+import org.elasticsearch.common.Table;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.rest.FakeRestRequest;
+import org.junit.Before;
+import org.junit.Test;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import static org.elasticsearch.rest.action.support.RestTable.buildDisplayHeaders;
+import static org.hamcrest.Matchers.*;
+
+public class RestTableTests extends ESTestCase {
+
+    private Table table = new Table();
+    private FakeRestRequest restRequest = new FakeRestRequest();
+
+    @Before
+    public void setup() {
+        table.startHeaders();
+        table.addCell("bulk.foo", "alias:f;desc:foo");
+        table.addCell("bulk.bar", "alias:b;desc:bar");
+        // should be matched as well due to the aliases
+        table.addCell("aliasedBulk", "alias:bulkWhatever;desc:bar");
+        table.addCell("aliasedSecondBulk", "alias:foobar,bulkolicious,bulkotastic;desc:bar");
+        // no match
+        table.addCell("unmatched", "alias:un.matched;desc:bar");
+        // invalid alias
+        table.addCell("invalidAliasesBulk", "alias:,,,;desc:bar");
+        table.endHeaders();
+    }
+
+    @Test
+    public void testThatDisplayHeadersSupportWildcards() throws Exception {
+        restRequest.params().put("h", "bulk*");
+        List<RestTable.DisplayHeader> headers = buildDisplayHeaders(table, restRequest);
+
+        List<String> headerNames = getHeaderNames(headers);
+        assertThat(headerNames, contains("bulk.foo", "bulk.bar", "aliasedBulk", "aliasedSecondBulk"));
+        assertThat(headerNames, not(hasItem("unmatched")));
+    }
+
+    @Test
+    public void testThatDisplayHeadersAreNotAddedTwice() throws Exception {
+        restRequest.params().put("h", "nonexistent,bulk*,bul*");
+        List<RestTable.DisplayHeader> headers = buildDisplayHeaders(table, restRequest);
+
+        List<String> headerNames = getHeaderNames(headers);
+        assertThat(headerNames, contains("bulk.foo", "bulk.bar", "aliasedBulk", "aliasedSecondBulk"));
+        assertThat(headerNames, not(hasItem("unmatched")));
+    }
+
+    private List<String> getHeaderNames(List<RestTable.DisplayHeader> headers) {
+        List<String> headerNames = new ArrayList<>();
+        for (RestTable.DisplayHeader header : headers) {
+            headerNames.add(header.name);
+        }
+
+        return headerNames;
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/script/ScriptParameterParserTest.java b/core/src/test/java/org/elasticsearch/script/ScriptParameterParserTest.java
deleted file mode 100644
index 70c6617..0000000
--- a/core/src/test/java/org/elasticsearch/script/ScriptParameterParserTest.java
+++ /dev/null
@@ -1,1264 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.script;
-
-
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.bytes.BytesArray;
-import org.elasticsearch.common.xcontent.ToXContent.MapParams;
-import org.elasticsearch.common.xcontent.XContentHelper;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.script.Script.ScriptParseException;
-import org.elasticsearch.script.ScriptParameterParser.ScriptParameterValue;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.*;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.nullValue;
-
-public class ScriptParameterParserTest extends ESTestCase {
-
-    @Test
-    public void testTokenDefaultInline() throws IOException {
-        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"script\" : \"scriptValue\" }"));
-        Token token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        ScriptParameterParser paramParser = new ScriptParameterParser();
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
-        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INLINE);
-        assertThat(paramParser.lang(), nullValue());
-        paramParser = new ScriptParameterParser(null);
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
-        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INLINE);
-        assertThat(paramParser.lang(), nullValue());
-        paramParser = new ScriptParameterParser(new HashSet<String>());
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
-        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INLINE);
-        assertThat(paramParser.lang(), nullValue());
-    }
-
-    @Test
-    public void testTokenDefaultFile() throws IOException {
-        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"script_file\" : \"scriptValue\" }"));
-        Token token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        ScriptParameterParser paramParser = new ScriptParameterParser();
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
-        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.FILE);
-        assertThat(paramParser.lang(), nullValue());
-
-        parser = XContentHelper.createParser(new BytesArray("{ \"scriptFile\" : \"scriptValue\" }"));
-        token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        paramParser = new ScriptParameterParser();
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
-        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.FILE);
-        assertThat(paramParser.lang(), nullValue());
-    }
-
-    @Test
-    public void testTokenDefaultIndexed() throws IOException {
-        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"script_id\" : \"scriptValue\" }"));
-        Token token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        ScriptParameterParser paramParser = new ScriptParameterParser();
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
-        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INDEXED);
-        assertThat(paramParser.lang(), nullValue());
-
-        parser = XContentHelper.createParser(new BytesArray("{ \"scriptId\" : \"scriptValue\" }"));
-        token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        paramParser = new ScriptParameterParser();
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
-        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INDEXED);
-        assertThat(paramParser.lang(), nullValue());
-    }
-
-    @Test
-    public void testTokenDefaultNotFound() throws IOException {
-        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo\" : \"bar\" }"));
-        Token token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        ScriptParameterParser paramParser = new ScriptParameterParser();
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(false));
-        assertThat(paramParser.getDefaultScriptParameterValue(), nullValue());
-        assertThat(paramParser.getScriptParameterValue("script"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-    }
-
-    @Test
-    public void testTokenSingleParameter() throws IOException {
-        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo\" : \"scriptValue\" }"));
-        Token token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
-        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.INLINE);
-        assertThat(paramParser.lang(), nullValue());
-    }
-
-    @Test
-    public void testTokenSingleParameterFile() throws IOException {
-        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo_file\" : \"scriptValue\" }"));
-        Token token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
-        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.FILE);
-        assertThat(paramParser.lang(), nullValue());
-    }
-
-    @Test
-    public void testTokenSingleParameterIndexed() throws IOException {
-        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo_id\" : \"scriptValue\" }"));
-        Token token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
-        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.INDEXED);
-        assertThat(paramParser.lang(), nullValue());
-    }
-
-    @Test(expected = ScriptParseException.class)
-    public void testTokenSingleParameterDelcaredTwiceInlineFile() throws IOException {
-        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo\" : \"scriptValue\", \"foo_file\" : \"scriptValue\" }"));
-        Token token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
-        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.INLINE);
-        assertThat(paramParser.lang(), nullValue());
-        token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT);
-    }
-
-    @Test(expected = ScriptParseException.class)
-    public void testTokenSingleParameterDelcaredTwiceInlineIndexed() throws IOException {
-        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo\" : \"scriptValue\", \"foo_id\" : \"scriptValue\" }"));
-        Token token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
-        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.INLINE);
-        assertThat(paramParser.lang(), nullValue());
-        token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT);
-    }
-
-    @Test(expected = ScriptParseException.class)
-    public void testTokenSingleParameterDelcaredTwiceFileInline() throws IOException {
-        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo_file\" : \"scriptValue\", \"foo\" : \"scriptValue\" }"));
-        Token token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
-        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.FILE);
-        assertThat(paramParser.lang(), nullValue());
-        token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT);
-    }
-
-    @Test(expected = ScriptParseException.class)
-    public void testTokenSingleParameterDelcaredTwiceFileIndexed() throws IOException {
-        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo_file\" : \"scriptValue\", \"foo_id\" : \"scriptValue\" }"));
-        Token token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
-        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.FILE);
-        assertThat(paramParser.lang(), nullValue());
-        token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT);
-    }
-
-    @Test(expected = ScriptParseException.class)
-    public void testTokenSingleParameterDelcaredTwiceIndexedInline() throws IOException {
-        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo_id\" : \"scriptValue\", \"foo\" : \"scriptValue\" }"));
-        Token token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
-        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.INDEXED);
-        assertThat(paramParser.lang(), nullValue());
-        token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT);
-    }
-
-    @Test(expected = ScriptParseException.class)
-    public void testTokenSingleParameterDelcaredTwiceIndexedFile() throws IOException {
-        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo_id\" : \"scriptValue\", \"foo_file\" : \"scriptValue\" }"));
-        Token token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
-        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.INDEXED);
-        assertThat(paramParser.lang(), nullValue());
-        token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT);
-    }
-
-    @Test
-    public void testTokenMultipleParameters() throws IOException {
-        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo\" : \"fooScriptValue\", \"bar_file\" : \"barScriptValue\", \"baz_id\" : \"bazScriptValue\" }"));
-        Set<String> parameters = new HashSet<>();
-        parameters.add("foo");
-        parameters.add("bar");
-        parameters.add("baz");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        Token token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
-        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
-        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
-        assertParameterValue(paramParser, "bar", "barScriptValue", ScriptType.FILE);
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
-        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
-        assertParameterValue(paramParser, "bar", "barScriptValue", ScriptType.FILE);
-        assertParameterValue(paramParser, "baz", "bazScriptValue", ScriptType.INDEXED);
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-    }
-
-    @Test
-    public void testTokenMultipleParametersWithLang() throws IOException {
-        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo\" : \"fooScriptValue\", \"bar_file\" : \"barScriptValue\", \"lang\" : \"myLang\", \"baz_id\" : \"bazScriptValue\" }"));
-        Set<String> parameters = new HashSet<>();
-        parameters.add("foo");
-        parameters.add("bar");
-        parameters.add("baz");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        Token token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
-        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
-        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
-        assertParameterValue(paramParser, "bar", "barScriptValue", ScriptType.FILE);
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
-        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
-        assertParameterValue(paramParser, "bar", "barScriptValue", ScriptType.FILE);
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), equalTo("myLang"));
-        token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
-        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
-        assertParameterValue(paramParser, "bar", "barScriptValue", ScriptType.FILE);
-        assertParameterValue(paramParser, "baz", "bazScriptValue", ScriptType.INDEXED);
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), equalTo("myLang"));
-    }
-
-    @Test
-    public void testTokenMultipleParametersNotFound() throws IOException {
-        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"other\" : \"scriptValue\" }"));
-        Set<String> parameters = new HashSet<>();
-        parameters.add("foo");
-        parameters.add("bar");
-        parameters.add("baz");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        Token token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(false));
-        assertThat(paramParser.getScriptParameterValue("other"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-    }
-
-    @Test
-    public void testTokenMultipleParametersSomeNotFound() throws IOException {
-        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo\" : \"fooScriptValue\", \"other_file\" : \"barScriptValue\", \"baz_id\" : \"bazScriptValue\" }"));
-        Set<String> parameters = new HashSet<>();
-        parameters.add("foo");
-        parameters.add("bar");
-        parameters.add("baz");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("other"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("other_file"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        Token token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
-        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("other"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("other_file"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(false));
-        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("other"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("other_file"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        token = parser.nextToken();
-        while (token != Token.VALUE_STRING) {
-            token = parser.nextToken();
-        }
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
-        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertParameterValue(paramParser, "baz", "bazScriptValue", ScriptType.INDEXED);
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("other"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("other_file"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-    }
-
-    @Test
-    public void testTokenMultipleParametersWrongType() throws IOException {
-        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo\" : \"fooScriptValue\", \"bar_file\" : \"barScriptValue\", \"baz_id\" : \"bazScriptValue\" }"));
-        Set<String> parameters = new HashSet<>();
-        parameters.add("foo");
-        parameters.add("bar");
-        parameters.add("baz");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(false));
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-    }
-
-    @Test(expected=IllegalArgumentException.class)
-    public void testReservedParameters() {
-        Set<String> parameterNames = Collections.singleton("lang");
-        new ScriptParameterParser(parameterNames );
-    }
-
-    @Test
-    public void testConfigDefaultInline() throws IOException {
-        Map<String, Object> config = new HashMap<>();
-        config.put("script", "scriptValue");
-        ScriptParameterParser paramParser = new ScriptParameterParser();
-        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
-        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INLINE);
-        assertThat(paramParser.lang(), nullValue());
-        assertThat(config.isEmpty(), equalTo(true));
-        config = new HashMap<>();
-        config.put("script", "scriptValue");
-        paramParser = new ScriptParameterParser(null);
-        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
-        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INLINE);
-        assertThat(paramParser.lang(), nullValue());
-        assertThat(config.isEmpty(), equalTo(true));
-        config = new HashMap<>();
-        config.put("script", "scriptValue");
-        paramParser = new ScriptParameterParser(new HashSet<String>());
-        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
-        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INLINE);
-        assertThat(paramParser.lang(), nullValue());
-        assertThat(config.isEmpty(), equalTo(true));
-    }
-
-    @Test
-    public void testConfigDefaultFile() throws IOException {
-        Map<String, Object> config = new HashMap<>();
-        config.put("script_file", "scriptValue");
-        ScriptParameterParser paramParser = new ScriptParameterParser();
-        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
-        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.FILE);
-        assertThat(paramParser.lang(), nullValue());
-        assertThat(config.isEmpty(), equalTo(true));
-
-        config = new HashMap<>();
-        config.put("scriptFile", "scriptValue");
-        paramParser = new ScriptParameterParser();
-        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
-        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.FILE);
-        assertThat(paramParser.lang(), nullValue());
-        assertThat(config.isEmpty(), equalTo(true));
-    }
-
-    @Test
-    public void testConfigDefaultIndexed() throws IOException {
-        Map<String, Object> config = new HashMap<>();
-        config.put("script_id", "scriptValue");
-        ScriptParameterParser paramParser = new ScriptParameterParser();
-        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
-        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INDEXED);
-        assertThat(paramParser.lang(), nullValue());
-        assertThat(config.isEmpty(), equalTo(true));
-
-        config = new HashMap<>();
-        config.put("scriptId", "scriptValue");
-        paramParser = new ScriptParameterParser();
-        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
-        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INDEXED);
-        assertThat(paramParser.lang(), nullValue());
-        assertThat(config.isEmpty(), equalTo(true));
-    }
-
-    @Test
-    public void testConfigDefaultIndexedNoRemove() throws IOException {
-        Map<String, Object> config = new HashMap<>();
-        config.put("script_id", "scriptValue");
-        ScriptParameterParser paramParser = new ScriptParameterParser();
-        paramParser.parseConfig(config, false, ParseFieldMatcher.STRICT);
-        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INDEXED);
-        assertThat(paramParser.lang(), nullValue());
-        assertThat(config.size(), equalTo(1));
-        assertThat((String) config.get("script_id"), equalTo("scriptValue"));
-
-        config = new HashMap<>();
-        config.put("scriptId", "scriptValue");
-        paramParser = new ScriptParameterParser();
-        paramParser.parseConfig(config, false, ParseFieldMatcher.STRICT);
-        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INDEXED);
-        assertThat(paramParser.lang(), nullValue());
-        assertThat(config.size(), equalTo(1));
-        assertThat((String) config.get("scriptId"), equalTo("scriptValue"));
-    }
-
-    @Test
-    public void testConfigDefaultNotFound() throws IOException {
-        Map<String, Object> config = new HashMap<>();
-        config.put("foo", "bar");
-        ScriptParameterParser paramParser = new ScriptParameterParser();
-        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
-        assertThat(paramParser.getDefaultScriptParameterValue(), nullValue());
-        assertThat(paramParser.getScriptParameterValue("script"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        assertThat(config.size(), equalTo(1));
-        assertThat((String) config.get("foo"), equalTo("bar"));
-    }
-
-    @Test
-    public void testConfigSingleParameter() throws IOException {
-        Map<String, Object> config = new HashMap<>();
-        config.put("foo", "scriptValue");
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
-        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.INLINE);
-        assertThat(paramParser.lang(), nullValue());
-        assertThat(config.isEmpty(), equalTo(true));
-    }
-
-    @Test
-    public void testConfigSingleParameterFile() throws IOException {
-        Map<String, Object> config = new HashMap<>();
-        config.put("foo_file", "scriptValue");
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
-        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.FILE);
-        assertThat(paramParser.lang(), nullValue());
-        assertThat(config.isEmpty(), equalTo(true));
-    }
-
-    @Test
-    public void testConfigSingleParameterIndexed() throws IOException {
-        Map<String, Object> config = new HashMap<>();
-        config.put("foo_id", "scriptValue");
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
-        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.INDEXED);
-        assertThat(paramParser.lang(), nullValue());
-        assertThat(config.isEmpty(), equalTo(true));
-    }
-
-    @Test(expected = ScriptParseException.class)
-    public void testConfigSingleParameterDelcaredTwiceInlineFile() throws IOException {
-        Map<String, Object> config = new LinkedHashMap<>();
-        config.put("foo", "scriptValue");
-        config.put("foo_file", "scriptValue");
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
-    }
-
-    @Test(expected = ScriptParseException.class)
-    public void testConfigSingleParameterDelcaredTwiceInlineIndexed() throws IOException {
-        Map<String, Object> config = new LinkedHashMap<>();
-        config.put("foo", "scriptValue");
-        config.put("foo_id", "scriptValue");
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
-    }
-
-    @Test(expected = ScriptParseException.class)
-    public void testConfigSingleParameterDelcaredTwiceFileInline() throws IOException {
-        Map<String, Object> config = new LinkedHashMap<>();
-        config.put("foo_file", "scriptValue");
-        config.put("foo", "scriptValue");
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
-    }
-
-    @Test(expected = ScriptParseException.class)
-    public void testConfigSingleParameterDelcaredTwiceFileIndexed() throws IOException {
-        Map<String, Object> config = new LinkedHashMap<>();
-        config.put("foo_file", "scriptValue");
-        config.put("foo_id", "scriptValue");
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
-    }
-
-    @Test(expected = ScriptParseException.class)
-    public void testConfigSingleParameterDelcaredTwiceIndexedInline() throws IOException {
-        Map<String, Object> config = new LinkedHashMap<>();
-        config.put("foo_id", "scriptValue");
-        config.put("foo", "scriptValue");
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
-    }
-
-    @Test(expected = ScriptParseException.class)
-    public void testConfigSingleParameterDelcaredTwiceIndexedFile() throws IOException {
-        Map<String, Object> config = new LinkedHashMap<>();
-        config.put("foo_id", "scriptValue");
-        config.put("foo_file", "scriptValue");
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
-    }
-
-    @Test
-    public void testConfigMultipleParameters() throws IOException {
-        Map<String, Object> config = new HashMap<>();
-        config.put("foo", "fooScriptValue");
-        config.put("bar_file", "barScriptValue");
-        config.put("baz_id", "bazScriptValue");
-        Set<String> parameters = new HashSet<>();
-        parameters.add("foo");
-        parameters.add("bar");
-        parameters.add("baz");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
-        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
-        assertParameterValue(paramParser, "bar", "barScriptValue", ScriptType.FILE);
-        assertParameterValue(paramParser, "baz", "bazScriptValue", ScriptType.INDEXED);
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        assertThat(config.isEmpty(), equalTo(true));
-    }
-
-    @Test
-    public void testConfigMultipleParametersWithLang() throws IOException {
-        Map<String, Object> config = new HashMap<>();
-        config.put("foo", "fooScriptValue");
-        config.put("bar_file", "barScriptValue");
-        config.put("lang", "myLang");
-        config.put("baz_id", "bazScriptValue");
-        Set<String> parameters = new HashSet<>();
-        parameters.add("foo");
-        parameters.add("bar");
-        parameters.add("baz");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
-        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
-        assertParameterValue(paramParser, "bar", "barScriptValue", ScriptType.FILE);
-        assertParameterValue(paramParser, "baz", "bazScriptValue", ScriptType.INDEXED);
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), equalTo("myLang"));
-        assertThat(config.isEmpty(), equalTo(true));
-    }
-
-    @Test
-    public void testConfigMultipleParametersWithLangNoRemove() throws IOException {
-        Map<String, Object> config = new HashMap<>();
-        config.put("foo", "fooScriptValue");
-        config.put("bar_file", "barScriptValue");
-        config.put("lang", "myLang");
-        config.put("baz_id", "bazScriptValue");
-        Set<String> parameters = new HashSet<>();
-        parameters.add("foo");
-        parameters.add("bar");
-        parameters.add("baz");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        paramParser.parseConfig(config, false, ParseFieldMatcher.STRICT);
-        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
-        assertParameterValue(paramParser, "bar", "barScriptValue", ScriptType.FILE);
-        assertParameterValue(paramParser, "baz", "bazScriptValue", ScriptType.INDEXED);
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), equalTo("myLang"));
-        assertThat(config.size(), equalTo(4));
-        assertThat((String) config.get("foo"), equalTo("fooScriptValue"));
-        assertThat((String) config.get("bar_file"), equalTo("barScriptValue"));
-        assertThat((String) config.get("baz_id"), equalTo("bazScriptValue"));
-        assertThat((String) config.get("lang"), equalTo("myLang"));
-    }
-
-    @Test
-    public void testConfigMultipleParametersNotFound() throws IOException {
-        Map<String, Object> config = new HashMap<>();
-        config.put("other", "scriptValue");
-        Set<String> parameters = new HashSet<>();
-        parameters.add("foo");
-        parameters.add("bar");
-        parameters.add("baz");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
-        assertThat(paramParser.getScriptParameterValue("other"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        assertThat(config.size(), equalTo(1));
-        assertThat((String) config.get("other"), equalTo("scriptValue"));
-    }
-
-    @Test
-    public void testConfigMultipleParametersSomeNotFound() throws IOException {
-        Map<String, Object> config = new HashMap<>();
-        config.put("foo", "fooScriptValue");
-        config.put("other_file", "barScriptValue");
-        config.put("baz_id", "bazScriptValue");
-        Set<String> parameters = new HashSet<>();
-        parameters.add("foo");
-        parameters.add("bar");
-        parameters.add("baz");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("other"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("other_file"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
-        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertParameterValue(paramParser, "baz", "bazScriptValue", ScriptType.INDEXED);
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("other"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("other_file"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        assertThat(config.size(), equalTo(1));
-        assertThat((String) config.get("other_file"), equalTo("barScriptValue"));
-    }
-
-    @Test(expected = ScriptParseException.class)
-    public void testConfigMultipleParametersInlineWrongType() throws IOException {
-        Map<String, Object> config = new HashMap<>();
-        config.put("foo", 1l);
-        config.put("bar_file", "barScriptValue");
-        config.put("baz_id", "bazScriptValue");
-        config.put("lang", "myLang");
-        Set<String> parameters = new HashSet<>();
-        parameters.add("foo");
-        parameters.add("bar");
-        parameters.add("baz");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
-    }
-
-    @Test(expected = ScriptParseException.class)
-    public void testConfigMultipleParametersFileWrongType() throws IOException {
-        Map<String, Object> config = new HashMap<>();
-        config.put("foo", "fooScriptValue");
-        config.put("bar_file", 1l);
-        config.put("baz_id", "bazScriptValue");
-        config.put("lang", "myLang");
-        Set<String> parameters = new HashSet<>();
-        parameters.add("foo");
-        parameters.add("bar");
-        parameters.add("baz");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
-    }
-
-    @Test(expected = ScriptParseException.class)
-    public void testConfigMultipleParametersIndexedWrongType() throws IOException {
-        Map<String, Object> config = new HashMap<>();
-        config.put("foo", "fooScriptValue");
-        config.put("bar_file", "barScriptValue");
-        config.put("baz_id", 1l);
-        config.put("lang", "myLang");
-        Set<String> parameters = new HashSet<>();
-        parameters.add("foo");
-        parameters.add("bar");
-        parameters.add("baz");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
-    }
-
-    @Test(expected = ScriptParseException.class)
-    public void testConfigMultipleParametersLangWrongType() throws IOException {
-        Map<String, Object> config = new HashMap<>();
-        config.put("foo", "fooScriptValue");
-        config.put("bar_file", "barScriptValue");
-        config.put("baz_id", "bazScriptValue");
-        config.put("lang", 1l);
-        Set<String> parameters = new HashSet<>();
-        parameters.add("foo");
-        parameters.add("bar");
-        parameters.add("baz");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
-    }
-
-    @Test
-    public void testParamsDefaultInline() throws IOException {
-        Map<String, String> config = new HashMap<>();
-        config.put("script", "scriptValue");
-        MapParams params = new MapParams(config);
-        ScriptParameterParser paramParser = new ScriptParameterParser();
-        paramParser.parseParams(params);
-        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INLINE);
-        assertThat(paramParser.lang(), nullValue());
-        
-        paramParser = new ScriptParameterParser(null);
-        paramParser.parseParams(params);
-        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INLINE);
-        assertThat(paramParser.lang(), nullValue());
-
-        paramParser = new ScriptParameterParser(new HashSet<String>());
-        paramParser.parseParams(params);
-        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INLINE);
-        assertThat(paramParser.lang(), nullValue());
-    }
-
-    @Test
-    public void testParamsDefaultFile() throws IOException {
-        Map<String, String> config = new HashMap<>();
-        config.put("script_file", "scriptValue");
-        MapParams params = new MapParams(config);
-        ScriptParameterParser paramParser = new ScriptParameterParser();
-        paramParser.parseParams(params);
-        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.FILE);
-        assertThat(paramParser.lang(), nullValue());
-    }
-
-    @Test
-    public void testParamsDefaultIndexed() throws IOException {
-        Map<String, String> config = new HashMap<>();
-        config.put("script_id", "scriptValue");
-        MapParams params = new MapParams(config);
-        ScriptParameterParser paramParser = new ScriptParameterParser();
-        paramParser.parseParams(params);
-        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INDEXED);
-        assertThat(paramParser.lang(), nullValue());
-    }
-
-    @Test
-    public void testParamsDefaultNotFound() throws IOException {
-        Map<String, String> config = new HashMap<>();
-        config.put("foo", "bar");
-        MapParams params = new MapParams(config);
-        ScriptParameterParser paramParser = new ScriptParameterParser();
-        paramParser.parseParams(params);
-        assertThat(paramParser.getDefaultScriptParameterValue(), nullValue());
-        assertThat(paramParser.getScriptParameterValue("script"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-    }
-
-    @Test
-    public void testParamsSingleParameter() throws IOException {
-        Map<String, String> config = new HashMap<>();
-        config.put("foo", "scriptValue");
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        MapParams params = new MapParams(config);
-        paramParser.parseParams(params);
-        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.INLINE);
-        assertThat(paramParser.lang(), nullValue());
-    }
-
-    @Test
-    public void testParamsSingleParameterFile() throws IOException {
-        Map<String, String> config = new HashMap<>();
-        config.put("foo_file", "scriptValue");
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        MapParams params = new MapParams(config);
-        paramParser.parseParams(params);
-        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.FILE);
-        assertThat(paramParser.lang(), nullValue());
-    }
-
-    @Test
-    public void testParamsSingleParameterIndexed() throws IOException {
-        Map<String, String> config = new HashMap<>();
-        config.put("foo_id", "scriptValue");
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        MapParams params = new MapParams(config);
-        paramParser.parseParams(params);
-        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.INDEXED);
-        assertThat(paramParser.lang(), nullValue());
-    }
-
-    @Test(expected = ScriptParseException.class)
-    public void testParamsSingleParameterDelcaredTwiceInlineFile() throws IOException {
-        Map<String, String> config = new LinkedHashMap<>();
-        config.put("foo", "scriptValue");
-        config.put("foo_file", "scriptValue");
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        MapParams params = new MapParams(config);
-        paramParser.parseParams(params);
-    }
-
-    @Test(expected = ScriptParseException.class)
-    public void testParamsSingleParameterDelcaredTwiceInlineIndexed() throws IOException {
-        Map<String, String> config = new LinkedHashMap<>();
-        config.put("foo", "scriptValue");
-        config.put("foo_id", "scriptValue");
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        MapParams params = new MapParams(config);
-        paramParser.parseParams(params);
-    }
-
-    @Test(expected = ScriptParseException.class)
-    public void testParamsSingleParameterDelcaredTwiceFileInline() throws IOException {
-        Map<String, String> config = new LinkedHashMap<>();
-        config.put("foo_file", "scriptValue");
-        config.put("foo", "scriptValue");
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        MapParams params = new MapParams(config);
-        paramParser.parseParams(params);
-    }
-
-    @Test(expected = ScriptParseException.class)
-    public void testParamsSingleParameterDelcaredTwiceFileIndexed() throws IOException {
-        Map<String, String> config = new LinkedHashMap<>();
-        config.put("foo_file", "scriptValue");
-        config.put("foo_id", "scriptValue");
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        MapParams params = new MapParams(config);
-        paramParser.parseParams(params);
-    }
-
-    @Test(expected = ScriptParseException.class)
-    public void testParamsSingleParameterDelcaredTwiceIndexedInline() throws IOException {
-        Map<String, String> config = new LinkedHashMap<>();
-        config.put("foo_id", "scriptValue");
-        config.put("foo", "scriptValue");
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        MapParams params = new MapParams(config);
-        paramParser.parseParams(params);
-    }
-
-    @Test(expected = ScriptParseException.class)
-    public void testParamsSingleParameterDelcaredTwiceIndexedFile() throws IOException {
-        Map<String, String> config = new LinkedHashMap<>();
-        config.put("foo_id", "scriptValue");
-        config.put("foo_file", "scriptValue");
-        Set<String> parameters = Collections.singleton("foo");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        MapParams params = new MapParams(config);
-        paramParser.parseParams(params);
-    }
-
-    @Test
-    public void testParamsMultipleParameters() throws IOException {
-        Map<String, String> config = new HashMap<>();
-        config.put("foo", "fooScriptValue");
-        config.put("bar_file", "barScriptValue");
-        config.put("baz_id", "bazScriptValue");
-        Set<String> parameters = new HashSet<>();
-        parameters.add("foo");
-        parameters.add("bar");
-        parameters.add("baz");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        MapParams params = new MapParams(config);
-        paramParser.parseParams(params);
-        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
-        assertParameterValue(paramParser, "bar", "barScriptValue", ScriptType.FILE);
-        assertParameterValue(paramParser, "baz", "bazScriptValue", ScriptType.INDEXED);
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-    }
-
-    @Test
-    public void testParamsMultipleParametersWithLang() throws IOException {
-        Map<String, String> config = new HashMap<>();
-        config.put("foo", "fooScriptValue");
-        config.put("bar_file", "barScriptValue");
-        config.put("lang", "myLang");
-        config.put("baz_id", "bazScriptValue");
-        Set<String> parameters = new HashSet<>();
-        parameters.add("foo");
-        parameters.add("bar");
-        parameters.add("baz");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        MapParams params = new MapParams(config);
-        paramParser.parseParams(params);
-        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
-        assertParameterValue(paramParser, "bar", "barScriptValue", ScriptType.FILE);
-        assertParameterValue(paramParser, "baz", "bazScriptValue", ScriptType.INDEXED);
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), equalTo("myLang"));
-    }
-
-    @Test
-    public void testParamsMultipleParametersWithLangNoRemove() throws IOException {
-        Map<String, String> config = new HashMap<>();
-        config.put("foo", "fooScriptValue");
-        config.put("bar_file", "barScriptValue");
-        config.put("lang", "myLang");
-        config.put("baz_id", "bazScriptValue");
-        Set<String> parameters = new HashSet<>();
-        parameters.add("foo");
-        parameters.add("bar");
-        parameters.add("baz");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        MapParams params = new MapParams(config);
-        paramParser.parseParams(params);
-        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
-        assertParameterValue(paramParser, "bar", "barScriptValue", ScriptType.FILE);
-        assertParameterValue(paramParser, "baz", "bazScriptValue", ScriptType.INDEXED);
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), equalTo("myLang"));
-    }
-
-    @Test
-    public void testParamsMultipleParametersNotFound() throws IOException {
-        Map<String, String> config = new HashMap<>();
-        config.put("other", "scriptValue");
-        Set<String> parameters = new HashSet<>();
-        parameters.add("foo");
-        parameters.add("bar");
-        parameters.add("baz");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        MapParams params = new MapParams(config);
-        paramParser.parseParams(params);
-        assertThat(paramParser.getScriptParameterValue("other"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-    }
-
-    @Test
-    public void testParamsMultipleParametersSomeNotFound() throws IOException {
-        Map<String, String> config = new HashMap<>();
-        config.put("foo", "fooScriptValue");
-        config.put("other_file", "barScriptValue");
-        config.put("baz_id", "bazScriptValue");
-        Set<String> parameters = new HashSet<>();
-        parameters.add("foo");
-        parameters.add("bar");
-        parameters.add("baz");
-        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
-        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("other"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("other_file"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-        MapParams params = new MapParams(config);
-        paramParser.parseParams(params);
-        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
-        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
-        assertParameterValue(paramParser, "baz", "bazScriptValue", ScriptType.INDEXED);
-        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("other"), nullValue());
-        assertThat(paramParser.getScriptParameterValue("other_file"), nullValue());
-        assertThat(paramParser.lang(), nullValue());
-    }
-
-    private void assertDefaultParameterValue(ScriptParameterParser paramParser, String expectedScript, ScriptType expectedScriptType) throws IOException {
-        ScriptParameterValue defaultValue = paramParser.getDefaultScriptParameterValue();
-        ScriptParameterValue defaultValueByName = paramParser.getScriptParameterValue("script");
-        assertThat(defaultValue.scriptType(), equalTo(expectedScriptType));
-        assertThat(defaultValue.script(), equalTo(expectedScript));
-        assertThat(defaultValueByName.scriptType(), equalTo(expectedScriptType));
-        assertThat(defaultValueByName.script(), equalTo(expectedScript));
-    }
-
-    private void assertParameterValue(ScriptParameterParser paramParser, String parameterName, String expectedScript, ScriptType expectedScriptType) throws IOException {
-        ScriptParameterValue value = paramParser.getScriptParameterValue(parameterName);
-        assertThat(value.scriptType(), equalTo(expectedScriptType));
-        assertThat(value.script(), equalTo(expectedScript));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/script/ScriptParameterParserTests.java b/core/src/test/java/org/elasticsearch/script/ScriptParameterParserTests.java
new file mode 100644
index 0000000..85dc650
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/script/ScriptParameterParserTests.java
@@ -0,0 +1,1264 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script;
+
+
+import org.elasticsearch.common.ParseFieldMatcher;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.xcontent.ToXContent.MapParams;
+import org.elasticsearch.common.xcontent.XContentHelper;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.common.xcontent.XContentParser.Token;
+import org.elasticsearch.script.Script.ScriptParseException;
+import org.elasticsearch.script.ScriptParameterParser.ScriptParameterValue;
+import org.elasticsearch.script.ScriptService.ScriptType;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.*;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.nullValue;
+
+public class ScriptParameterParserTests extends ESTestCase {
+
+    @Test
+    public void testTokenDefaultInline() throws IOException {
+        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"script\" : \"scriptValue\" }"));
+        Token token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        ScriptParameterParser paramParser = new ScriptParameterParser();
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
+        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INLINE);
+        assertThat(paramParser.lang(), nullValue());
+        paramParser = new ScriptParameterParser(null);
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
+        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INLINE);
+        assertThat(paramParser.lang(), nullValue());
+        paramParser = new ScriptParameterParser(new HashSet<String>());
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
+        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INLINE);
+        assertThat(paramParser.lang(), nullValue());
+    }
+
+    @Test
+    public void testTokenDefaultFile() throws IOException {
+        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"script_file\" : \"scriptValue\" }"));
+        Token token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        ScriptParameterParser paramParser = new ScriptParameterParser();
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
+        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.FILE);
+        assertThat(paramParser.lang(), nullValue());
+
+        parser = XContentHelper.createParser(new BytesArray("{ \"scriptFile\" : \"scriptValue\" }"));
+        token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        paramParser = new ScriptParameterParser();
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
+        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.FILE);
+        assertThat(paramParser.lang(), nullValue());
+    }
+
+    @Test
+    public void testTokenDefaultIndexed() throws IOException {
+        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"script_id\" : \"scriptValue\" }"));
+        Token token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        ScriptParameterParser paramParser = new ScriptParameterParser();
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
+        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INDEXED);
+        assertThat(paramParser.lang(), nullValue());
+
+        parser = XContentHelper.createParser(new BytesArray("{ \"scriptId\" : \"scriptValue\" }"));
+        token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        paramParser = new ScriptParameterParser();
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
+        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INDEXED);
+        assertThat(paramParser.lang(), nullValue());
+    }
+
+    @Test
+    public void testTokenDefaultNotFound() throws IOException {
+        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo\" : \"bar\" }"));
+        Token token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        ScriptParameterParser paramParser = new ScriptParameterParser();
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(false));
+        assertThat(paramParser.getDefaultScriptParameterValue(), nullValue());
+        assertThat(paramParser.getScriptParameterValue("script"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+    }
+
+    @Test
+    public void testTokenSingleParameter() throws IOException {
+        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo\" : \"scriptValue\" }"));
+        Token token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
+        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.INLINE);
+        assertThat(paramParser.lang(), nullValue());
+    }
+
+    @Test
+    public void testTokenSingleParameterFile() throws IOException {
+        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo_file\" : \"scriptValue\" }"));
+        Token token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
+        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.FILE);
+        assertThat(paramParser.lang(), nullValue());
+    }
+
+    @Test
+    public void testTokenSingleParameterIndexed() throws IOException {
+        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo_id\" : \"scriptValue\" }"));
+        Token token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
+        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.INDEXED);
+        assertThat(paramParser.lang(), nullValue());
+    }
+
+    @Test(expected = ScriptParseException.class)
+    public void testTokenSingleParameterDelcaredTwiceInlineFile() throws IOException {
+        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo\" : \"scriptValue\", \"foo_file\" : \"scriptValue\" }"));
+        Token token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
+        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.INLINE);
+        assertThat(paramParser.lang(), nullValue());
+        token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT);
+    }
+
+    @Test(expected = ScriptParseException.class)
+    public void testTokenSingleParameterDelcaredTwiceInlineIndexed() throws IOException {
+        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo\" : \"scriptValue\", \"foo_id\" : \"scriptValue\" }"));
+        Token token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
+        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.INLINE);
+        assertThat(paramParser.lang(), nullValue());
+        token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT);
+    }
+
+    @Test(expected = ScriptParseException.class)
+    public void testTokenSingleParameterDelcaredTwiceFileInline() throws IOException {
+        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo_file\" : \"scriptValue\", \"foo\" : \"scriptValue\" }"));
+        Token token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
+        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.FILE);
+        assertThat(paramParser.lang(), nullValue());
+        token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT);
+    }
+
+    @Test(expected = ScriptParseException.class)
+    public void testTokenSingleParameterDelcaredTwiceFileIndexed() throws IOException {
+        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo_file\" : \"scriptValue\", \"foo_id\" : \"scriptValue\" }"));
+        Token token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
+        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.FILE);
+        assertThat(paramParser.lang(), nullValue());
+        token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT);
+    }
+
+    @Test(expected = ScriptParseException.class)
+    public void testTokenSingleParameterDelcaredTwiceIndexedInline() throws IOException {
+        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo_id\" : \"scriptValue\", \"foo\" : \"scriptValue\" }"));
+        Token token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
+        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.INDEXED);
+        assertThat(paramParser.lang(), nullValue());
+        token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT);
+    }
+
+    @Test(expected = ScriptParseException.class)
+    public void testTokenSingleParameterDelcaredTwiceIndexedFile() throws IOException {
+        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo_id\" : \"scriptValue\", \"foo_file\" : \"scriptValue\" }"));
+        Token token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
+        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.INDEXED);
+        assertThat(paramParser.lang(), nullValue());
+        token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT);
+    }
+
+    @Test
+    public void testTokenMultipleParameters() throws IOException {
+        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo\" : \"fooScriptValue\", \"bar_file\" : \"barScriptValue\", \"baz_id\" : \"bazScriptValue\" }"));
+        Set<String> parameters = new HashSet<>();
+        parameters.add("foo");
+        parameters.add("bar");
+        parameters.add("baz");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        Token token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
+        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
+        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
+        assertParameterValue(paramParser, "bar", "barScriptValue", ScriptType.FILE);
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
+        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
+        assertParameterValue(paramParser, "bar", "barScriptValue", ScriptType.FILE);
+        assertParameterValue(paramParser, "baz", "bazScriptValue", ScriptType.INDEXED);
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+    }
+
+    @Test
+    public void testTokenMultipleParametersWithLang() throws IOException {
+        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo\" : \"fooScriptValue\", \"bar_file\" : \"barScriptValue\", \"lang\" : \"myLang\", \"baz_id\" : \"bazScriptValue\" }"));
+        Set<String> parameters = new HashSet<>();
+        parameters.add("foo");
+        parameters.add("bar");
+        parameters.add("baz");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        Token token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
+        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
+        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
+        assertParameterValue(paramParser, "bar", "barScriptValue", ScriptType.FILE);
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
+        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
+        assertParameterValue(paramParser, "bar", "barScriptValue", ScriptType.FILE);
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), equalTo("myLang"));
+        token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
+        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
+        assertParameterValue(paramParser, "bar", "barScriptValue", ScriptType.FILE);
+        assertParameterValue(paramParser, "baz", "bazScriptValue", ScriptType.INDEXED);
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), equalTo("myLang"));
+    }
+
+    @Test
+    public void testTokenMultipleParametersNotFound() throws IOException {
+        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"other\" : \"scriptValue\" }"));
+        Set<String> parameters = new HashSet<>();
+        parameters.add("foo");
+        parameters.add("bar");
+        parameters.add("baz");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        Token token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(false));
+        assertThat(paramParser.getScriptParameterValue("other"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+    }
+
+    @Test
+    public void testTokenMultipleParametersSomeNotFound() throws IOException {
+        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo\" : \"fooScriptValue\", \"other_file\" : \"barScriptValue\", \"baz_id\" : \"bazScriptValue\" }"));
+        Set<String> parameters = new HashSet<>();
+        parameters.add("foo");
+        parameters.add("bar");
+        parameters.add("baz");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("other"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("other_file"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        Token token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
+        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("other"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("other_file"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(false));
+        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("other"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("other_file"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        token = parser.nextToken();
+        while (token != Token.VALUE_STRING) {
+            token = parser.nextToken();
+        }
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(true));
+        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertParameterValue(paramParser, "baz", "bazScriptValue", ScriptType.INDEXED);
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("other"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("other_file"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+    }
+
+    @Test
+    public void testTokenMultipleParametersWrongType() throws IOException {
+        XContentParser parser = XContentHelper.createParser(new BytesArray("{ \"foo\" : \"fooScriptValue\", \"bar_file\" : \"barScriptValue\", \"baz_id\" : \"bazScriptValue\" }"));
+        Set<String> parameters = new HashSet<>();
+        parameters.add("foo");
+        parameters.add("bar");
+        parameters.add("baz");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        assertThat(paramParser.token(parser.currentName(), parser.currentToken(), parser, ParseFieldMatcher.STRICT), equalTo(false));
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+    }
+
+    @Test(expected=IllegalArgumentException.class)
+    public void testReservedParameters() {
+        Set<String> parameterNames = Collections.singleton("lang");
+        new ScriptParameterParser(parameterNames );
+    }
+
+    @Test
+    public void testConfigDefaultInline() throws IOException {
+        Map<String, Object> config = new HashMap<>();
+        config.put("script", "scriptValue");
+        ScriptParameterParser paramParser = new ScriptParameterParser();
+        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
+        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INLINE);
+        assertThat(paramParser.lang(), nullValue());
+        assertThat(config.isEmpty(), equalTo(true));
+        config = new HashMap<>();
+        config.put("script", "scriptValue");
+        paramParser = new ScriptParameterParser(null);
+        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
+        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INLINE);
+        assertThat(paramParser.lang(), nullValue());
+        assertThat(config.isEmpty(), equalTo(true));
+        config = new HashMap<>();
+        config.put("script", "scriptValue");
+        paramParser = new ScriptParameterParser(new HashSet<String>());
+        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
+        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INLINE);
+        assertThat(paramParser.lang(), nullValue());
+        assertThat(config.isEmpty(), equalTo(true));
+    }
+
+    @Test
+    public void testConfigDefaultFile() throws IOException {
+        Map<String, Object> config = new HashMap<>();
+        config.put("script_file", "scriptValue");
+        ScriptParameterParser paramParser = new ScriptParameterParser();
+        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
+        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.FILE);
+        assertThat(paramParser.lang(), nullValue());
+        assertThat(config.isEmpty(), equalTo(true));
+
+        config = new HashMap<>();
+        config.put("scriptFile", "scriptValue");
+        paramParser = new ScriptParameterParser();
+        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
+        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.FILE);
+        assertThat(paramParser.lang(), nullValue());
+        assertThat(config.isEmpty(), equalTo(true));
+    }
+
+    @Test
+    public void testConfigDefaultIndexed() throws IOException {
+        Map<String, Object> config = new HashMap<>();
+        config.put("script_id", "scriptValue");
+        ScriptParameterParser paramParser = new ScriptParameterParser();
+        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
+        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INDEXED);
+        assertThat(paramParser.lang(), nullValue());
+        assertThat(config.isEmpty(), equalTo(true));
+
+        config = new HashMap<>();
+        config.put("scriptId", "scriptValue");
+        paramParser = new ScriptParameterParser();
+        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
+        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INDEXED);
+        assertThat(paramParser.lang(), nullValue());
+        assertThat(config.isEmpty(), equalTo(true));
+    }
+
+    @Test
+    public void testConfigDefaultIndexedNoRemove() throws IOException {
+        Map<String, Object> config = new HashMap<>();
+        config.put("script_id", "scriptValue");
+        ScriptParameterParser paramParser = new ScriptParameterParser();
+        paramParser.parseConfig(config, false, ParseFieldMatcher.STRICT);
+        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INDEXED);
+        assertThat(paramParser.lang(), nullValue());
+        assertThat(config.size(), equalTo(1));
+        assertThat((String) config.get("script_id"), equalTo("scriptValue"));
+
+        config = new HashMap<>();
+        config.put("scriptId", "scriptValue");
+        paramParser = new ScriptParameterParser();
+        paramParser.parseConfig(config, false, ParseFieldMatcher.STRICT);
+        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INDEXED);
+        assertThat(paramParser.lang(), nullValue());
+        assertThat(config.size(), equalTo(1));
+        assertThat((String) config.get("scriptId"), equalTo("scriptValue"));
+    }
+
+    @Test
+    public void testConfigDefaultNotFound() throws IOException {
+        Map<String, Object> config = new HashMap<>();
+        config.put("foo", "bar");
+        ScriptParameterParser paramParser = new ScriptParameterParser();
+        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
+        assertThat(paramParser.getDefaultScriptParameterValue(), nullValue());
+        assertThat(paramParser.getScriptParameterValue("script"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        assertThat(config.size(), equalTo(1));
+        assertThat((String) config.get("foo"), equalTo("bar"));
+    }
+
+    @Test
+    public void testConfigSingleParameter() throws IOException {
+        Map<String, Object> config = new HashMap<>();
+        config.put("foo", "scriptValue");
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
+        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.INLINE);
+        assertThat(paramParser.lang(), nullValue());
+        assertThat(config.isEmpty(), equalTo(true));
+    }
+
+    @Test
+    public void testConfigSingleParameterFile() throws IOException {
+        Map<String, Object> config = new HashMap<>();
+        config.put("foo_file", "scriptValue");
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
+        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.FILE);
+        assertThat(paramParser.lang(), nullValue());
+        assertThat(config.isEmpty(), equalTo(true));
+    }
+
+    @Test
+    public void testConfigSingleParameterIndexed() throws IOException {
+        Map<String, Object> config = new HashMap<>();
+        config.put("foo_id", "scriptValue");
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
+        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.INDEXED);
+        assertThat(paramParser.lang(), nullValue());
+        assertThat(config.isEmpty(), equalTo(true));
+    }
+
+    @Test(expected = ScriptParseException.class)
+    public void testConfigSingleParameterDelcaredTwiceInlineFile() throws IOException {
+        Map<String, Object> config = new LinkedHashMap<>();
+        config.put("foo", "scriptValue");
+        config.put("foo_file", "scriptValue");
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
+    }
+
+    @Test(expected = ScriptParseException.class)
+    public void testConfigSingleParameterDelcaredTwiceInlineIndexed() throws IOException {
+        Map<String, Object> config = new LinkedHashMap<>();
+        config.put("foo", "scriptValue");
+        config.put("foo_id", "scriptValue");
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
+    }
+
+    @Test(expected = ScriptParseException.class)
+    public void testConfigSingleParameterDelcaredTwiceFileInline() throws IOException {
+        Map<String, Object> config = new LinkedHashMap<>();
+        config.put("foo_file", "scriptValue");
+        config.put("foo", "scriptValue");
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
+    }
+
+    @Test(expected = ScriptParseException.class)
+    public void testConfigSingleParameterDelcaredTwiceFileIndexed() throws IOException {
+        Map<String, Object> config = new LinkedHashMap<>();
+        config.put("foo_file", "scriptValue");
+        config.put("foo_id", "scriptValue");
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
+    }
+
+    @Test(expected = ScriptParseException.class)
+    public void testConfigSingleParameterDelcaredTwiceIndexedInline() throws IOException {
+        Map<String, Object> config = new LinkedHashMap<>();
+        config.put("foo_id", "scriptValue");
+        config.put("foo", "scriptValue");
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
+    }
+
+    @Test(expected = ScriptParseException.class)
+    public void testConfigSingleParameterDelcaredTwiceIndexedFile() throws IOException {
+        Map<String, Object> config = new LinkedHashMap<>();
+        config.put("foo_id", "scriptValue");
+        config.put("foo_file", "scriptValue");
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
+    }
+
+    @Test
+    public void testConfigMultipleParameters() throws IOException {
+        Map<String, Object> config = new HashMap<>();
+        config.put("foo", "fooScriptValue");
+        config.put("bar_file", "barScriptValue");
+        config.put("baz_id", "bazScriptValue");
+        Set<String> parameters = new HashSet<>();
+        parameters.add("foo");
+        parameters.add("bar");
+        parameters.add("baz");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
+        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
+        assertParameterValue(paramParser, "bar", "barScriptValue", ScriptType.FILE);
+        assertParameterValue(paramParser, "baz", "bazScriptValue", ScriptType.INDEXED);
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        assertThat(config.isEmpty(), equalTo(true));
+    }
+
+    @Test
+    public void testConfigMultipleParametersWithLang() throws IOException {
+        Map<String, Object> config = new HashMap<>();
+        config.put("foo", "fooScriptValue");
+        config.put("bar_file", "barScriptValue");
+        config.put("lang", "myLang");
+        config.put("baz_id", "bazScriptValue");
+        Set<String> parameters = new HashSet<>();
+        parameters.add("foo");
+        parameters.add("bar");
+        parameters.add("baz");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
+        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
+        assertParameterValue(paramParser, "bar", "barScriptValue", ScriptType.FILE);
+        assertParameterValue(paramParser, "baz", "bazScriptValue", ScriptType.INDEXED);
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), equalTo("myLang"));
+        assertThat(config.isEmpty(), equalTo(true));
+    }
+
+    @Test
+    public void testConfigMultipleParametersWithLangNoRemove() throws IOException {
+        Map<String, Object> config = new HashMap<>();
+        config.put("foo", "fooScriptValue");
+        config.put("bar_file", "barScriptValue");
+        config.put("lang", "myLang");
+        config.put("baz_id", "bazScriptValue");
+        Set<String> parameters = new HashSet<>();
+        parameters.add("foo");
+        parameters.add("bar");
+        parameters.add("baz");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        paramParser.parseConfig(config, false, ParseFieldMatcher.STRICT);
+        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
+        assertParameterValue(paramParser, "bar", "barScriptValue", ScriptType.FILE);
+        assertParameterValue(paramParser, "baz", "bazScriptValue", ScriptType.INDEXED);
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), equalTo("myLang"));
+        assertThat(config.size(), equalTo(4));
+        assertThat((String) config.get("foo"), equalTo("fooScriptValue"));
+        assertThat((String) config.get("bar_file"), equalTo("barScriptValue"));
+        assertThat((String) config.get("baz_id"), equalTo("bazScriptValue"));
+        assertThat((String) config.get("lang"), equalTo("myLang"));
+    }
+
+    @Test
+    public void testConfigMultipleParametersNotFound() throws IOException {
+        Map<String, Object> config = new HashMap<>();
+        config.put("other", "scriptValue");
+        Set<String> parameters = new HashSet<>();
+        parameters.add("foo");
+        parameters.add("bar");
+        parameters.add("baz");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
+        assertThat(paramParser.getScriptParameterValue("other"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        assertThat(config.size(), equalTo(1));
+        assertThat((String) config.get("other"), equalTo("scriptValue"));
+    }
+
+    @Test
+    public void testConfigMultipleParametersSomeNotFound() throws IOException {
+        Map<String, Object> config = new HashMap<>();
+        config.put("foo", "fooScriptValue");
+        config.put("other_file", "barScriptValue");
+        config.put("baz_id", "bazScriptValue");
+        Set<String> parameters = new HashSet<>();
+        parameters.add("foo");
+        parameters.add("bar");
+        parameters.add("baz");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("other"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("other_file"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
+        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertParameterValue(paramParser, "baz", "bazScriptValue", ScriptType.INDEXED);
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("other"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("other_file"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        assertThat(config.size(), equalTo(1));
+        assertThat((String) config.get("other_file"), equalTo("barScriptValue"));
+    }
+
+    @Test(expected = ScriptParseException.class)
+    public void testConfigMultipleParametersInlineWrongType() throws IOException {
+        Map<String, Object> config = new HashMap<>();
+        config.put("foo", 1l);
+        config.put("bar_file", "barScriptValue");
+        config.put("baz_id", "bazScriptValue");
+        config.put("lang", "myLang");
+        Set<String> parameters = new HashSet<>();
+        parameters.add("foo");
+        parameters.add("bar");
+        parameters.add("baz");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
+    }
+
+    @Test(expected = ScriptParseException.class)
+    public void testConfigMultipleParametersFileWrongType() throws IOException {
+        Map<String, Object> config = new HashMap<>();
+        config.put("foo", "fooScriptValue");
+        config.put("bar_file", 1l);
+        config.put("baz_id", "bazScriptValue");
+        config.put("lang", "myLang");
+        Set<String> parameters = new HashSet<>();
+        parameters.add("foo");
+        parameters.add("bar");
+        parameters.add("baz");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
+    }
+
+    @Test(expected = ScriptParseException.class)
+    public void testConfigMultipleParametersIndexedWrongType() throws IOException {
+        Map<String, Object> config = new HashMap<>();
+        config.put("foo", "fooScriptValue");
+        config.put("bar_file", "barScriptValue");
+        config.put("baz_id", 1l);
+        config.put("lang", "myLang");
+        Set<String> parameters = new HashSet<>();
+        parameters.add("foo");
+        parameters.add("bar");
+        parameters.add("baz");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
+    }
+
+    @Test(expected = ScriptParseException.class)
+    public void testConfigMultipleParametersLangWrongType() throws IOException {
+        Map<String, Object> config = new HashMap<>();
+        config.put("foo", "fooScriptValue");
+        config.put("bar_file", "barScriptValue");
+        config.put("baz_id", "bazScriptValue");
+        config.put("lang", 1l);
+        Set<String> parameters = new HashSet<>();
+        parameters.add("foo");
+        parameters.add("bar");
+        parameters.add("baz");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        paramParser.parseConfig(config, true, ParseFieldMatcher.STRICT);
+    }
+
+    @Test
+    public void testParamsDefaultInline() throws IOException {
+        Map<String, String> config = new HashMap<>();
+        config.put("script", "scriptValue");
+        MapParams params = new MapParams(config);
+        ScriptParameterParser paramParser = new ScriptParameterParser();
+        paramParser.parseParams(params);
+        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INLINE);
+        assertThat(paramParser.lang(), nullValue());
+        
+        paramParser = new ScriptParameterParser(null);
+        paramParser.parseParams(params);
+        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INLINE);
+        assertThat(paramParser.lang(), nullValue());
+
+        paramParser = new ScriptParameterParser(new HashSet<String>());
+        paramParser.parseParams(params);
+        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INLINE);
+        assertThat(paramParser.lang(), nullValue());
+    }
+
+    @Test
+    public void testParamsDefaultFile() throws IOException {
+        Map<String, String> config = new HashMap<>();
+        config.put("script_file", "scriptValue");
+        MapParams params = new MapParams(config);
+        ScriptParameterParser paramParser = new ScriptParameterParser();
+        paramParser.parseParams(params);
+        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.FILE);
+        assertThat(paramParser.lang(), nullValue());
+    }
+
+    @Test
+    public void testParamsDefaultIndexed() throws IOException {
+        Map<String, String> config = new HashMap<>();
+        config.put("script_id", "scriptValue");
+        MapParams params = new MapParams(config);
+        ScriptParameterParser paramParser = new ScriptParameterParser();
+        paramParser.parseParams(params);
+        assertDefaultParameterValue(paramParser, "scriptValue", ScriptType.INDEXED);
+        assertThat(paramParser.lang(), nullValue());
+    }
+
+    @Test
+    public void testParamsDefaultNotFound() throws IOException {
+        Map<String, String> config = new HashMap<>();
+        config.put("foo", "bar");
+        MapParams params = new MapParams(config);
+        ScriptParameterParser paramParser = new ScriptParameterParser();
+        paramParser.parseParams(params);
+        assertThat(paramParser.getDefaultScriptParameterValue(), nullValue());
+        assertThat(paramParser.getScriptParameterValue("script"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+    }
+
+    @Test
+    public void testParamsSingleParameter() throws IOException {
+        Map<String, String> config = new HashMap<>();
+        config.put("foo", "scriptValue");
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        MapParams params = new MapParams(config);
+        paramParser.parseParams(params);
+        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.INLINE);
+        assertThat(paramParser.lang(), nullValue());
+    }
+
+    @Test
+    public void testParamsSingleParameterFile() throws IOException {
+        Map<String, String> config = new HashMap<>();
+        config.put("foo_file", "scriptValue");
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        MapParams params = new MapParams(config);
+        paramParser.parseParams(params);
+        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.FILE);
+        assertThat(paramParser.lang(), nullValue());
+    }
+
+    @Test
+    public void testParamsSingleParameterIndexed() throws IOException {
+        Map<String, String> config = new HashMap<>();
+        config.put("foo_id", "scriptValue");
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        MapParams params = new MapParams(config);
+        paramParser.parseParams(params);
+        assertParameterValue(paramParser, "foo", "scriptValue", ScriptType.INDEXED);
+        assertThat(paramParser.lang(), nullValue());
+    }
+
+    @Test(expected = ScriptParseException.class)
+    public void testParamsSingleParameterDelcaredTwiceInlineFile() throws IOException {
+        Map<String, String> config = new LinkedHashMap<>();
+        config.put("foo", "scriptValue");
+        config.put("foo_file", "scriptValue");
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        MapParams params = new MapParams(config);
+        paramParser.parseParams(params);
+    }
+
+    @Test(expected = ScriptParseException.class)
+    public void testParamsSingleParameterDelcaredTwiceInlineIndexed() throws IOException {
+        Map<String, String> config = new LinkedHashMap<>();
+        config.put("foo", "scriptValue");
+        config.put("foo_id", "scriptValue");
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        MapParams params = new MapParams(config);
+        paramParser.parseParams(params);
+    }
+
+    @Test(expected = ScriptParseException.class)
+    public void testParamsSingleParameterDelcaredTwiceFileInline() throws IOException {
+        Map<String, String> config = new LinkedHashMap<>();
+        config.put("foo_file", "scriptValue");
+        config.put("foo", "scriptValue");
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        MapParams params = new MapParams(config);
+        paramParser.parseParams(params);
+    }
+
+    @Test(expected = ScriptParseException.class)
+    public void testParamsSingleParameterDelcaredTwiceFileIndexed() throws IOException {
+        Map<String, String> config = new LinkedHashMap<>();
+        config.put("foo_file", "scriptValue");
+        config.put("foo_id", "scriptValue");
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        MapParams params = new MapParams(config);
+        paramParser.parseParams(params);
+    }
+
+    @Test(expected = ScriptParseException.class)
+    public void testParamsSingleParameterDelcaredTwiceIndexedInline() throws IOException {
+        Map<String, String> config = new LinkedHashMap<>();
+        config.put("foo_id", "scriptValue");
+        config.put("foo", "scriptValue");
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        MapParams params = new MapParams(config);
+        paramParser.parseParams(params);
+    }
+
+    @Test(expected = ScriptParseException.class)
+    public void testParamsSingleParameterDelcaredTwiceIndexedFile() throws IOException {
+        Map<String, String> config = new LinkedHashMap<>();
+        config.put("foo_id", "scriptValue");
+        config.put("foo_file", "scriptValue");
+        Set<String> parameters = Collections.singleton("foo");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        MapParams params = new MapParams(config);
+        paramParser.parseParams(params);
+    }
+
+    @Test
+    public void testParamsMultipleParameters() throws IOException {
+        Map<String, String> config = new HashMap<>();
+        config.put("foo", "fooScriptValue");
+        config.put("bar_file", "barScriptValue");
+        config.put("baz_id", "bazScriptValue");
+        Set<String> parameters = new HashSet<>();
+        parameters.add("foo");
+        parameters.add("bar");
+        parameters.add("baz");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        MapParams params = new MapParams(config);
+        paramParser.parseParams(params);
+        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
+        assertParameterValue(paramParser, "bar", "barScriptValue", ScriptType.FILE);
+        assertParameterValue(paramParser, "baz", "bazScriptValue", ScriptType.INDEXED);
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+    }
+
+    @Test
+    public void testParamsMultipleParametersWithLang() throws IOException {
+        Map<String, String> config = new HashMap<>();
+        config.put("foo", "fooScriptValue");
+        config.put("bar_file", "barScriptValue");
+        config.put("lang", "myLang");
+        config.put("baz_id", "bazScriptValue");
+        Set<String> parameters = new HashSet<>();
+        parameters.add("foo");
+        parameters.add("bar");
+        parameters.add("baz");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        MapParams params = new MapParams(config);
+        paramParser.parseParams(params);
+        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
+        assertParameterValue(paramParser, "bar", "barScriptValue", ScriptType.FILE);
+        assertParameterValue(paramParser, "baz", "bazScriptValue", ScriptType.INDEXED);
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), equalTo("myLang"));
+    }
+
+    @Test
+    public void testParamsMultipleParametersWithLangNoRemove() throws IOException {
+        Map<String, String> config = new HashMap<>();
+        config.put("foo", "fooScriptValue");
+        config.put("bar_file", "barScriptValue");
+        config.put("lang", "myLang");
+        config.put("baz_id", "bazScriptValue");
+        Set<String> parameters = new HashSet<>();
+        parameters.add("foo");
+        parameters.add("bar");
+        parameters.add("baz");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        MapParams params = new MapParams(config);
+        paramParser.parseParams(params);
+        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
+        assertParameterValue(paramParser, "bar", "barScriptValue", ScriptType.FILE);
+        assertParameterValue(paramParser, "baz", "bazScriptValue", ScriptType.INDEXED);
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), equalTo("myLang"));
+    }
+
+    @Test
+    public void testParamsMultipleParametersNotFound() throws IOException {
+        Map<String, String> config = new HashMap<>();
+        config.put("other", "scriptValue");
+        Set<String> parameters = new HashSet<>();
+        parameters.add("foo");
+        parameters.add("bar");
+        parameters.add("baz");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        MapParams params = new MapParams(config);
+        paramParser.parseParams(params);
+        assertThat(paramParser.getScriptParameterValue("other"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+    }
+
+    @Test
+    public void testParamsMultipleParametersSomeNotFound() throws IOException {
+        Map<String, String> config = new HashMap<>();
+        config.put("foo", "fooScriptValue");
+        config.put("other_file", "barScriptValue");
+        config.put("baz_id", "bazScriptValue");
+        Set<String> parameters = new HashSet<>();
+        parameters.add("foo");
+        parameters.add("bar");
+        parameters.add("baz");
+        ScriptParameterParser paramParser = new ScriptParameterParser(parameters);
+        assertThat(paramParser.getScriptParameterValue("foo"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("other"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("other_file"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+        MapParams params = new MapParams(config);
+        paramParser.parseParams(params);
+        assertParameterValue(paramParser, "foo", "fooScriptValue", ScriptType.INLINE);
+        assertThat(paramParser.getScriptParameterValue("bar"), nullValue());
+        assertParameterValue(paramParser, "baz", "bazScriptValue", ScriptType.INDEXED);
+        assertThat(paramParser.getScriptParameterValue("bar_file"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("baz_id"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("other"), nullValue());
+        assertThat(paramParser.getScriptParameterValue("other_file"), nullValue());
+        assertThat(paramParser.lang(), nullValue());
+    }
+
+    private void assertDefaultParameterValue(ScriptParameterParser paramParser, String expectedScript, ScriptType expectedScriptType) throws IOException {
+        ScriptParameterValue defaultValue = paramParser.getDefaultScriptParameterValue();
+        ScriptParameterValue defaultValueByName = paramParser.getScriptParameterValue("script");
+        assertThat(defaultValue.scriptType(), equalTo(expectedScriptType));
+        assertThat(defaultValue.script(), equalTo(expectedScript));
+        assertThat(defaultValueByName.scriptType(), equalTo(expectedScriptType));
+        assertThat(defaultValueByName.script(), equalTo(expectedScript));
+    }
+
+    private void assertParameterValue(ScriptParameterParser paramParser, String parameterName, String expectedScript, ScriptType expectedScriptType) throws IOException {
+        ScriptParameterValue value = paramParser.getScriptParameterValue(parameterName);
+        assertThat(value.scriptType(), equalTo(expectedScriptType));
+        assertThat(value.script(), equalTo(expectedScript));
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTest.java b/core/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTest.java
deleted file mode 100644
index c2060b9..0000000
--- a/core/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTest.java
+++ /dev/null
@@ -1,173 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.script.mustache;
-
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.script.CompiledScript;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.io.StringWriter;
-import java.nio.charset.Charset;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-
-/**
- * Mustache based templating test
- */
-public class MustacheScriptEngineTest extends ESTestCase {
-    private MustacheScriptEngineService qe;
-    private JsonEscapingMustacheFactory escaper;
-
-    @Before
-    public void setup() {
-        qe = new MustacheScriptEngineService(Settings.Builder.EMPTY_SETTINGS);
-        escaper = new JsonEscapingMustacheFactory();
-    }
-
-    @Test
-    public void testSimpleParameterReplace() {
-        {
-            String template = "GET _search {\"query\": " + "{\"boosting\": {" + "\"positive\": {\"match\": {\"body\": \"gift\"}},"
-                    + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}" + "}}, \"negative_boost\": {{boost_val}} } }}";
-            Map<String, Object> vars = new HashMap<>();
-            vars.put("boost_val", "0.3");
-            BytesReference o = (BytesReference) qe.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "", "mustache", qe.compile(template)), vars);
-            assertEquals("GET _search {\"query\": {\"boosting\": {\"positive\": {\"match\": {\"body\": \"gift\"}},"
-                    + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}}}, \"negative_boost\": 0.3 } }}",
-                    new String(o.toBytes(), Charset.forName("UTF-8")));
-        }
-        {
-            String template = "GET _search {\"query\": " + "{\"boosting\": {" + "\"positive\": {\"match\": {\"body\": \"gift\"}},"
-                    + "\"negative\": {\"term\": {\"body\": {\"value\": \"{{body_val}}\"}" + "}}, \"negative_boost\": {{boost_val}} } }}";
-            Map<String, Object> vars = new HashMap<>();
-            vars.put("boost_val", "0.3");
-            vars.put("body_val", "\"quick brown\"");
-            BytesReference o = (BytesReference) qe.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "", "mustache", qe.compile(template)), vars);
-            assertEquals("GET _search {\"query\": {\"boosting\": {\"positive\": {\"match\": {\"body\": \"gift\"}},"
-                    + "\"negative\": {\"term\": {\"body\": {\"value\": \"\\\"quick brown\\\"\"}}}, \"negative_boost\": 0.3 } }}",
-                    new String(o.toBytes(), Charset.forName("UTF-8")));
-        }
-    }
-
-    @Test
-    public void testEscapeJson() throws IOException {
-        {
-            StringWriter writer = new StringWriter();
-            escaper.encode("hello \n world", writer);
-            assertThat(writer.toString(), equalTo("hello \\n world"));
-        }
-        {
-            StringWriter writer = new StringWriter();
-            escaper.encode("\n", writer);
-            assertThat(writer.toString(), equalTo("\\n"));
-        }
-
-        Character[] specialChars = new Character[]{
-                '\"', 
-                '\\', 
-                '\u0000', 
-                '\u0001',
-                '\u0002',
-                '\u0003',
-                '\u0004',
-                '\u0005',
-                '\u0006',
-                '\u0007',
-                '\u0008',
-                '\u0009',
-                '\u000B',
-                '\u000C',
-                '\u000E',
-                '\u000F',
-                '\u001F'};
-        String[] escapedChars = new String[]{
-                "\\\"", 
-                "\\\\", 
-                "\\u0000", 
-                "\\u0001",
-                "\\u0002",
-                "\\u0003",
-                "\\u0004",
-                "\\u0005",
-                "\\u0006",
-                "\\u0007",
-                "\\u0008",
-                "\\u0009",
-                "\\u000B",
-                "\\u000C",
-                "\\u000E",
-                "\\u000F",
-                "\\u001F"};
-        int iters = scaledRandomIntBetween(100, 1000);
-        for (int i = 0; i < iters; i++) {
-            int rounds = scaledRandomIntBetween(1, 20);
-            StringWriter expect = new StringWriter();
-            StringWriter writer = new StringWriter();
-            for (int j = 0; j < rounds; j++) {
-                String s = getChars();
-                writer.write(s);
-                expect.write(s);
-
-                int charIndex = randomInt(7);
-                writer.append(specialChars[charIndex]);
-                expect.append(escapedChars[charIndex]);
-            }
-            StringWriter target = new StringWriter();
-            escaper.encode(writer.toString(), target);
-            assertThat(expect.toString(), equalTo(target.toString()));
-        }
-    }
-
-    private String getChars() {
-        String string = randomRealisticUnicodeOfCodepointLengthBetween(0, 10);
-        for (int i = 0; i < string.length(); i++) {
-            if (isEscapeChar(string.charAt(i))) {
-                return string.substring(0, i);
-            }
-        }
-        return string;
-    }
-    
-    /**
-     * From https://www.ietf.org/rfc/rfc4627.txt:
-     * 
-     * All Unicode characters may be placed within the
-     * quotation marks except for the characters that must be escaped:
-     * quotation mark, reverse solidus, and the control characters (U+0000
-     * through U+001F). 
-     * */
-    private static boolean isEscapeChar(char c) {
-        switch (c) {
-        case '"':
-        case '\\':
-            return true;
-        }
-        
-        if (c < '\u002F')
-            return true;
-        return false;
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java b/core/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java
new file mode 100644
index 0000000..b44d180
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java
@@ -0,0 +1,173 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.script.mustache;
+
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.script.CompiledScript;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Before;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.io.StringWriter;
+import java.nio.charset.Charset;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.Matchers.equalTo;
+
+/**
+ * Mustache based templating test
+ */
+public class MustacheScriptEngineTests extends ESTestCase {
+    private MustacheScriptEngineService qe;
+    private JsonEscapingMustacheFactory escaper;
+
+    @Before
+    public void setup() {
+        qe = new MustacheScriptEngineService(Settings.Builder.EMPTY_SETTINGS);
+        escaper = new JsonEscapingMustacheFactory();
+    }
+
+    @Test
+    public void testSimpleParameterReplace() {
+        {
+            String template = "GET _search {\"query\": " + "{\"boosting\": {" + "\"positive\": {\"match\": {\"body\": \"gift\"}},"
+                    + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}" + "}}, \"negative_boost\": {{boost_val}} } }}";
+            Map<String, Object> vars = new HashMap<>();
+            vars.put("boost_val", "0.3");
+            BytesReference o = (BytesReference) qe.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "", "mustache", qe.compile(template)), vars);
+            assertEquals("GET _search {\"query\": {\"boosting\": {\"positive\": {\"match\": {\"body\": \"gift\"}},"
+                    + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}}}, \"negative_boost\": 0.3 } }}",
+                    new String(o.toBytes(), Charset.forName("UTF-8")));
+        }
+        {
+            String template = "GET _search {\"query\": " + "{\"boosting\": {" + "\"positive\": {\"match\": {\"body\": \"gift\"}},"
+                    + "\"negative\": {\"term\": {\"body\": {\"value\": \"{{body_val}}\"}" + "}}, \"negative_boost\": {{boost_val}} } }}";
+            Map<String, Object> vars = new HashMap<>();
+            vars.put("boost_val", "0.3");
+            vars.put("body_val", "\"quick brown\"");
+            BytesReference o = (BytesReference) qe.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "", "mustache", qe.compile(template)), vars);
+            assertEquals("GET _search {\"query\": {\"boosting\": {\"positive\": {\"match\": {\"body\": \"gift\"}},"
+                    + "\"negative\": {\"term\": {\"body\": {\"value\": \"\\\"quick brown\\\"\"}}}, \"negative_boost\": 0.3 } }}",
+                    new String(o.toBytes(), Charset.forName("UTF-8")));
+        }
+    }
+
+    @Test
+    public void testEscapeJson() throws IOException {
+        {
+            StringWriter writer = new StringWriter();
+            escaper.encode("hello \n world", writer);
+            assertThat(writer.toString(), equalTo("hello \\n world"));
+        }
+        {
+            StringWriter writer = new StringWriter();
+            escaper.encode("\n", writer);
+            assertThat(writer.toString(), equalTo("\\n"));
+        }
+
+        Character[] specialChars = new Character[]{
+                '\"', 
+                '\\', 
+                '\u0000', 
+                '\u0001',
+                '\u0002',
+                '\u0003',
+                '\u0004',
+                '\u0005',
+                '\u0006',
+                '\u0007',
+                '\u0008',
+                '\u0009',
+                '\u000B',
+                '\u000C',
+                '\u000E',
+                '\u000F',
+                '\u001F'};
+        String[] escapedChars = new String[]{
+                "\\\"", 
+                "\\\\", 
+                "\\u0000", 
+                "\\u0001",
+                "\\u0002",
+                "\\u0003",
+                "\\u0004",
+                "\\u0005",
+                "\\u0006",
+                "\\u0007",
+                "\\u0008",
+                "\\u0009",
+                "\\u000B",
+                "\\u000C",
+                "\\u000E",
+                "\\u000F",
+                "\\u001F"};
+        int iters = scaledRandomIntBetween(100, 1000);
+        for (int i = 0; i < iters; i++) {
+            int rounds = scaledRandomIntBetween(1, 20);
+            StringWriter expect = new StringWriter();
+            StringWriter writer = new StringWriter();
+            for (int j = 0; j < rounds; j++) {
+                String s = getChars();
+                writer.write(s);
+                expect.write(s);
+
+                int charIndex = randomInt(7);
+                writer.append(specialChars[charIndex]);
+                expect.append(escapedChars[charIndex]);
+            }
+            StringWriter target = new StringWriter();
+            escaper.encode(writer.toString(), target);
+            assertThat(expect.toString(), equalTo(target.toString()));
+        }
+    }
+
+    private String getChars() {
+        String string = randomRealisticUnicodeOfCodepointLengthBetween(0, 10);
+        for (int i = 0; i < string.length(); i++) {
+            if (isEscapeChar(string.charAt(i))) {
+                return string.substring(0, i);
+            }
+        }
+        return string;
+    }
+    
+    /**
+     * From https://www.ietf.org/rfc/rfc4627.txt:
+     * 
+     * All Unicode characters may be placed within the
+     * quotation marks except for the characters that must be escaped:
+     * quotation mark, reverse solidus, and the control characters (U+0000
+     * through U+001F). 
+     * */
+    private static boolean isEscapeChar(char c) {
+        switch (c) {
+        case '"':
+        case '\\':
+            return true;
+        }
+        
+        if (c < '\u002F')
+            return true;
+        return false;
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/script/mustache/MustacheTest.java b/core/src/test/java/org/elasticsearch/script/mustache/MustacheTest.java
deleted file mode 100644
index 7dea084..0000000
--- a/core/src/test/java/org/elasticsearch/script/mustache/MustacheTest.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.script.mustache;
-
-import com.github.mustachejava.DefaultMustacheFactory;
-import com.github.mustachejava.Mustache;
-import com.github.mustachejava.MustacheFactory;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.io.StringReader;
-import java.io.StringWriter;
-import java.util.HashMap;
-
-/**
- * Figure out how Mustache works for the simplest use case. Leaving in here for now for reference.
- * */
-public class MustacheTest extends ESTestCase {
-
-    @Test
-    public void test() {
-        HashMap<String, Object> scopes = new HashMap<>();
-        scopes.put("boost_val", "0.2");
-
-        String template = "GET _search {\"query\": " + "{\"boosting\": {"
-                + "\"positive\": {\"match\": {\"body\": \"gift\"}},"
-                + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}"
-                + "}}, \"negative_boost\": {{boost_val}} } }}";
-        MustacheFactory f = new DefaultMustacheFactory();
-        Mustache mustache = f.compile(new StringReader(template), "example");
-        StringWriter writer = new StringWriter();
-        mustache.execute(writer, scopes);
-        writer.flush();
-        assertEquals(
-                "Mustache templating broken",
-                "GET _search {\"query\": {\"boosting\": {\"positive\": {\"match\": {\"body\": \"gift\"}},"
-                        + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}}}, \"negative_boost\": 0.2 } }}",
-                writer.toString());
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/script/mustache/MustacheTests.java b/core/src/test/java/org/elasticsearch/script/mustache/MustacheTests.java
new file mode 100644
index 0000000..9bda581
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/script/mustache/MustacheTests.java
@@ -0,0 +1,56 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.script.mustache;
+
+import com.github.mustachejava.DefaultMustacheFactory;
+import com.github.mustachejava.Mustache;
+import com.github.mustachejava.MustacheFactory;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.io.StringReader;
+import java.io.StringWriter;
+import java.util.HashMap;
+
+/**
+ * Figure out how Mustache works for the simplest use case. Leaving in here for now for reference.
+ * */
+public class MustacheTests extends ESTestCase {
+
+    @Test
+    public void test() {
+        HashMap<String, Object> scopes = new HashMap<>();
+        scopes.put("boost_val", "0.2");
+
+        String template = "GET _search {\"query\": " + "{\"boosting\": {"
+                + "\"positive\": {\"match\": {\"body\": \"gift\"}},"
+                + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}"
+                + "}}, \"negative_boost\": {{boost_val}} } }}";
+        MustacheFactory f = new DefaultMustacheFactory();
+        Mustache mustache = f.compile(new StringReader(template), "example");
+        StringWriter writer = new StringWriter();
+        mustache.execute(writer, scopes);
+        writer.flush();
+        assertEquals(
+                "Mustache templating broken",
+                "GET _search {\"query\": {\"boosting\": {\"positive\": {\"match\": {\"body\": \"gift\"}},"
+                        + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}}}, \"negative_boost\": 0.2 } }}",
+                writer.toString());
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java
index 26cb3a9..27bfab0 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java
@@ -29,7 +29,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptModule;
@@ -237,7 +237,7 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
 
             @Override
             public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
-                    throws IOException, QueryShardException {
+                    throws IOException, QueryParsingException {
                 parser.nextToken();
                 return new SimpleHeuristic();
             }
@@ -621,4 +621,4 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         }
         indexRandom(true, indexRequestBuilderList);
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTest.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTest.java
deleted file mode 100644
index 15da498..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTest.java
+++ /dev/null
@@ -1,148 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.nested;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.NoMergePolicy;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.BooleanClause.Occur;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.store.Directory;
-import org.elasticsearch.action.admin.indices.mapping.put.PutMappingRequest;
-import org.elasticsearch.common.compress.CompressedXContent;
-import org.elasticsearch.common.lucene.search.Queries;
-import org.elasticsearch.index.IndexService;
-import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
-import org.elasticsearch.index.mapper.internal.UidFieldMapper;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorFactories;
-import org.elasticsearch.search.aggregations.BucketCollector;
-import org.elasticsearch.search.aggregations.SearchContextAggregations;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.internal.SearchContext;
-import org.elasticsearch.test.ESSingleNodeTestCase;
-import org.junit.Test;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-
-import static org.hamcrest.Matchers.equalTo;
-
-/**
- */
-public class NestedAggregatorTest extends ESSingleNodeTestCase {
-
-    @Test
-    public void testResetRootDocId() throws Exception {
-        Directory directory = newDirectory();
-        IndexWriterConfig iwc = new IndexWriterConfig(null);
-        iwc.setMergePolicy(NoMergePolicy.INSTANCE);
-        RandomIndexWriter indexWriter = new RandomIndexWriter(random(), directory, iwc);
-
-        List<Document> documents = new ArrayList<>();
-
-        // 1 segment with, 1 root document, with 3 nested sub docs
-        Document document = new Document();
-        document.add(new Field(UidFieldMapper.NAME, "type#1", UidFieldMapper.Defaults.NESTED_FIELD_TYPE));
-        document.add(new Field(TypeFieldMapper.NAME, "__nested_field", TypeFieldMapper.Defaults.FIELD_TYPE));
-        documents.add(document);
-        document = new Document();
-        document.add(new Field(UidFieldMapper.NAME, "type#1", UidFieldMapper.Defaults.NESTED_FIELD_TYPE));
-        document.add(new Field(TypeFieldMapper.NAME, "__nested_field", TypeFieldMapper.Defaults.FIELD_TYPE));
-        documents.add(document);
-        document = new Document();
-        document.add(new Field(UidFieldMapper.NAME, "type#1", UidFieldMapper.Defaults.NESTED_FIELD_TYPE));
-        document.add(new Field(TypeFieldMapper.NAME, "__nested_field", TypeFieldMapper.Defaults.FIELD_TYPE));
-        documents.add(document);
-        document = new Document();
-        document.add(new Field(UidFieldMapper.NAME, "type#1", UidFieldMapper.Defaults.FIELD_TYPE));
-        document.add(new Field(TypeFieldMapper.NAME, "test", TypeFieldMapper.Defaults.FIELD_TYPE));
-        documents.add(document);
-        indexWriter.addDocuments(documents);
-        indexWriter.commit();
-
-        documents.clear();
-        // 1 segment with:
-        // 1 document, with 1 nested subdoc
-        document = new Document();
-        document.add(new Field(UidFieldMapper.NAME, "type#2", UidFieldMapper.Defaults.NESTED_FIELD_TYPE));
-        document.add(new Field(TypeFieldMapper.NAME, "__nested_field", TypeFieldMapper.Defaults.FIELD_TYPE));
-        documents.add(document);
-        document = new Document();
-        document.add(new Field(UidFieldMapper.NAME, "type#2", UidFieldMapper.Defaults.FIELD_TYPE));
-        document.add(new Field(TypeFieldMapper.NAME, "test", TypeFieldMapper.Defaults.FIELD_TYPE));
-        documents.add(document);
-        indexWriter.addDocuments(documents);
-        documents.clear();
-        // and 1 document, with 1 nested subdoc
-        document = new Document();
-        document.add(new Field(UidFieldMapper.NAME, "type#3", UidFieldMapper.Defaults.NESTED_FIELD_TYPE));
-        document.add(new Field(TypeFieldMapper.NAME, "__nested_field", TypeFieldMapper.Defaults.FIELD_TYPE));
-        documents.add(document);
-        document = new Document();
-        document.add(new Field(UidFieldMapper.NAME, "type#3", UidFieldMapper.Defaults.FIELD_TYPE));
-        document.add(new Field(TypeFieldMapper.NAME, "test", TypeFieldMapper.Defaults.FIELD_TYPE));
-        documents.add(document);
-        indexWriter.addDocuments(documents);
-
-        indexWriter.commit();
-        indexWriter.close();
-
-        DirectoryReader directoryReader = DirectoryReader.open(directory);
-        IndexSearcher searcher = new IndexSearcher(directoryReader);
-
-        IndexService indexService = createIndex("test");
-        indexService.mapperService().merge("test", new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef("test", "nested_field", "type=nested").string()), true, false);
-        SearchContext searchContext = createSearchContext(indexService);
-        AggregationContext context = new AggregationContext(searchContext);
-
-        AggregatorFactories.Builder builder = AggregatorFactories.builder();
-        builder.addAggregator(new NestedAggregator.Factory("test", "nested_field"));
-        AggregatorFactories factories = builder.build();
-        searchContext.aggregations(new SearchContextAggregations(factories));
-        Aggregator[] aggs = factories.createTopLevelAggregators(context);
-        BucketCollector collector = BucketCollector.wrap(Arrays.asList(aggs));
-        collector.preCollection();
-        // A regular search always exclude nested docs, so we use NonNestedDocsFilter.INSTANCE here (otherwise MatchAllDocsQuery would be sufficient)
-        // We exclude root doc with uid type#2, this will trigger the bug if we don't reset the root doc when we process a new segment, because
-        // root doc type#3 and root doc type#1 have the same segment docid
-        BooleanQuery bq = new BooleanQuery();
-        bq.add(Queries.newNonNestedFilter(), Occur.MUST);
-        bq.add(new TermQuery(new Term(UidFieldMapper.NAME, "type#2")), Occur.MUST_NOT);
-        searcher.search(new ConstantScoreQuery(bq), collector);
-        collector.postCollection();
-
-        Nested nested = (Nested) aggs[0].buildAggregation(0);
-        // The bug manifests if 6 docs are returned, because currentRootDoc isn't reset the previous child docs from the first segment are emitted as hits.
-        assertThat(nested.getDocCount(), equalTo(4l));
-
-        directoryReader.close();
-        directory.close();
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java
new file mode 100644
index 0000000..ef6112d
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java
@@ -0,0 +1,148 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.bucket.nested;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.NoMergePolicy;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.store.Directory;
+import org.elasticsearch.action.admin.indices.mapping.put.PutMappingRequest;
+import org.elasticsearch.common.compress.CompressedXContent;
+import org.elasticsearch.common.lucene.search.Queries;
+import org.elasticsearch.index.IndexService;
+import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
+import org.elasticsearch.index.mapper.internal.UidFieldMapper;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactories;
+import org.elasticsearch.search.aggregations.BucketCollector;
+import org.elasticsearch.search.aggregations.SearchContextAggregations;
+import org.elasticsearch.search.aggregations.support.AggregationContext;
+import org.elasticsearch.search.internal.SearchContext;
+import org.elasticsearch.test.ESSingleNodeTestCase;
+import org.junit.Test;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+
+import static org.hamcrest.Matchers.equalTo;
+
+/**
+ */
+public class NestedAggregatorTests extends ESSingleNodeTestCase {
+
+    @Test
+    public void testResetRootDocId() throws Exception {
+        Directory directory = newDirectory();
+        IndexWriterConfig iwc = new IndexWriterConfig(null);
+        iwc.setMergePolicy(NoMergePolicy.INSTANCE);
+        RandomIndexWriter indexWriter = new RandomIndexWriter(random(), directory, iwc);
+
+        List<Document> documents = new ArrayList<>();
+
+        // 1 segment with, 1 root document, with 3 nested sub docs
+        Document document = new Document();
+        document.add(new Field(UidFieldMapper.NAME, "type#1", UidFieldMapper.Defaults.NESTED_FIELD_TYPE));
+        document.add(new Field(TypeFieldMapper.NAME, "__nested_field", TypeFieldMapper.Defaults.FIELD_TYPE));
+        documents.add(document);
+        document = new Document();
+        document.add(new Field(UidFieldMapper.NAME, "type#1", UidFieldMapper.Defaults.NESTED_FIELD_TYPE));
+        document.add(new Field(TypeFieldMapper.NAME, "__nested_field", TypeFieldMapper.Defaults.FIELD_TYPE));
+        documents.add(document);
+        document = new Document();
+        document.add(new Field(UidFieldMapper.NAME, "type#1", UidFieldMapper.Defaults.NESTED_FIELD_TYPE));
+        document.add(new Field(TypeFieldMapper.NAME, "__nested_field", TypeFieldMapper.Defaults.FIELD_TYPE));
+        documents.add(document);
+        document = new Document();
+        document.add(new Field(UidFieldMapper.NAME, "type#1", UidFieldMapper.Defaults.FIELD_TYPE));
+        document.add(new Field(TypeFieldMapper.NAME, "test", TypeFieldMapper.Defaults.FIELD_TYPE));
+        documents.add(document);
+        indexWriter.addDocuments(documents);
+        indexWriter.commit();
+
+        documents.clear();
+        // 1 segment with:
+        // 1 document, with 1 nested subdoc
+        document = new Document();
+        document.add(new Field(UidFieldMapper.NAME, "type#2", UidFieldMapper.Defaults.NESTED_FIELD_TYPE));
+        document.add(new Field(TypeFieldMapper.NAME, "__nested_field", TypeFieldMapper.Defaults.FIELD_TYPE));
+        documents.add(document);
+        document = new Document();
+        document.add(new Field(UidFieldMapper.NAME, "type#2", UidFieldMapper.Defaults.FIELD_TYPE));
+        document.add(new Field(TypeFieldMapper.NAME, "test", TypeFieldMapper.Defaults.FIELD_TYPE));
+        documents.add(document);
+        indexWriter.addDocuments(documents);
+        documents.clear();
+        // and 1 document, with 1 nested subdoc
+        document = new Document();
+        document.add(new Field(UidFieldMapper.NAME, "type#3", UidFieldMapper.Defaults.NESTED_FIELD_TYPE));
+        document.add(new Field(TypeFieldMapper.NAME, "__nested_field", TypeFieldMapper.Defaults.FIELD_TYPE));
+        documents.add(document);
+        document = new Document();
+        document.add(new Field(UidFieldMapper.NAME, "type#3", UidFieldMapper.Defaults.FIELD_TYPE));
+        document.add(new Field(TypeFieldMapper.NAME, "test", TypeFieldMapper.Defaults.FIELD_TYPE));
+        documents.add(document);
+        indexWriter.addDocuments(documents);
+
+        indexWriter.commit();
+        indexWriter.close();
+
+        DirectoryReader directoryReader = DirectoryReader.open(directory);
+        IndexSearcher searcher = new IndexSearcher(directoryReader);
+
+        IndexService indexService = createIndex("test");
+        indexService.mapperService().merge("test", new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef("test", "nested_field", "type=nested").string()), true, false);
+        SearchContext searchContext = createSearchContext(indexService);
+        AggregationContext context = new AggregationContext(searchContext);
+
+        AggregatorFactories.Builder builder = AggregatorFactories.builder();
+        builder.addAggregator(new NestedAggregator.Factory("test", "nested_field"));
+        AggregatorFactories factories = builder.build();
+        searchContext.aggregations(new SearchContextAggregations(factories));
+        Aggregator[] aggs = factories.createTopLevelAggregators(context);
+        BucketCollector collector = BucketCollector.wrap(Arrays.asList(aggs));
+        collector.preCollection();
+        // A regular search always exclude nested docs, so we use NonNestedDocsFilter.INSTANCE here (otherwise MatchAllDocsQuery would be sufficient)
+        // We exclude root doc with uid type#2, this will trigger the bug if we don't reset the root doc when we process a new segment, because
+        // root doc type#3 and root doc type#1 have the same segment docid
+        BooleanQuery bq = new BooleanQuery();
+        bq.add(Queries.newNonNestedFilter(), Occur.MUST);
+        bq.add(new TermQuery(new Term(UidFieldMapper.NAME, "type#2")), Occur.MUST_NOT);
+        searcher.search(new ConstantScoreQuery(bq), collector);
+        collector.postCollection();
+
+        Nested nested = (Nested) aggs[0].buildAggregation(0);
+        // The bug manifests if 6 docs are returned, because currentRootDoc isn't reset the previous child docs from the first segment are emitted as hits.
+        assertThat(nested.getDocCount(), equalTo(4l));
+
+        directoryReader.close();
+        directory.close();
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTest.java b/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTest.java
deleted file mode 100644
index 27d81f4..0000000
--- a/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTest.java
+++ /dev/null
@@ -1,90 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.search.builder;
-
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.json.JsonXContent;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.*;
-
-public class SearchSourceBuilderTest extends ESTestCase {
-
-    SearchSourceBuilder builder = new SearchSourceBuilder();
-
-    @Test // issue #6632
-    public void testThatSearchSourceBuilderIncludesExcludesAreAppliedCorrectly() throws Exception {
-        builder.fetchSource("foo", null);
-        assertIncludes(builder, "foo");
-        assertExcludes(builder);
-
-        builder.fetchSource(null, "foo");
-        assertIncludes(builder);
-        assertExcludes(builder, "foo");
-
-        builder.fetchSource(null, new String[]{"foo"});
-        assertIncludes(builder);
-        assertExcludes(builder, "foo");
-
-        builder.fetchSource(new String[]{"foo"}, null);
-        assertIncludes(builder, "foo");
-        assertExcludes(builder);
-
-        builder.fetchSource("foo", "bar");
-        assertIncludes(builder, "foo");
-        assertExcludes(builder, "bar");
-
-        builder.fetchSource(new String[]{"foo"}, new String[]{"bar", "baz"});
-        assertIncludes(builder, "foo");
-        assertExcludes(builder, "bar", "baz");
-    }
-
-    private void assertIncludes(SearchSourceBuilder builder, String... elems) throws IOException {
-        assertFieldValues(builder, "includes", elems);
-    }
-
-    private void assertExcludes(SearchSourceBuilder builder, String... elems) throws IOException {
-        assertFieldValues(builder, "excludes", elems);
-    }
-
-    private void assertFieldValues(SearchSourceBuilder builder, String fieldName, String... elems) throws IOException {
-        Map<String, Object> map = getSourceMap(builder);
-
-        assertThat(map, hasKey(fieldName));
-        assertThat(map.get(fieldName), is(instanceOf(List.class)));
-        List<String> castedList = (List<String>) map.get(fieldName);
-        assertThat(castedList, hasSize(elems.length));
-        assertThat(castedList, hasItems(elems));
-    }
-
-    private Map<String, Object> getSourceMap(SearchSourceBuilder builder) throws IOException {
-        Map<String, Object> data;
-        try (XContentParser parser = JsonXContent.jsonXContent.createParser(builder.toString())) {
-            data = parser.map();
-        }
-        assertThat(data, hasKey("_source"));
-        return (Map<String, Object>) data.get("_source");
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java b/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java
new file mode 100644
index 0000000..80a683c
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java
@@ -0,0 +1,90 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.builder;
+
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.common.xcontent.json.JsonXContent;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+
+import static org.hamcrest.Matchers.*;
+
+public class SearchSourceBuilderTests extends ESTestCase {
+
+    SearchSourceBuilder builder = new SearchSourceBuilder();
+
+    @Test // issue #6632
+    public void testThatSearchSourceBuilderIncludesExcludesAreAppliedCorrectly() throws Exception {
+        builder.fetchSource("foo", null);
+        assertIncludes(builder, "foo");
+        assertExcludes(builder);
+
+        builder.fetchSource(null, "foo");
+        assertIncludes(builder);
+        assertExcludes(builder, "foo");
+
+        builder.fetchSource(null, new String[]{"foo"});
+        assertIncludes(builder);
+        assertExcludes(builder, "foo");
+
+        builder.fetchSource(new String[]{"foo"}, null);
+        assertIncludes(builder, "foo");
+        assertExcludes(builder);
+
+        builder.fetchSource("foo", "bar");
+        assertIncludes(builder, "foo");
+        assertExcludes(builder, "bar");
+
+        builder.fetchSource(new String[]{"foo"}, new String[]{"bar", "baz"});
+        assertIncludes(builder, "foo");
+        assertExcludes(builder, "bar", "baz");
+    }
+
+    private void assertIncludes(SearchSourceBuilder builder, String... elems) throws IOException {
+        assertFieldValues(builder, "includes", elems);
+    }
+
+    private void assertExcludes(SearchSourceBuilder builder, String... elems) throws IOException {
+        assertFieldValues(builder, "excludes", elems);
+    }
+
+    private void assertFieldValues(SearchSourceBuilder builder, String fieldName, String... elems) throws IOException {
+        Map<String, Object> map = getSourceMap(builder);
+
+        assertThat(map, hasKey(fieldName));
+        assertThat(map.get(fieldName), is(instanceOf(List.class)));
+        List<String> castedList = (List<String>) map.get(fieldName);
+        assertThat(castedList, hasSize(elems.length));
+        assertThat(castedList, hasItems(elems));
+    }
+
+    private Map<String, Object> getSourceMap(SearchSourceBuilder builder) throws IOException {
+        Map<String, Object> data;
+        try (XContentParser parser = JsonXContent.jsonXContent.createParser(builder.toString())) {
+            data = parser.map();
+        }
+        assertThat(data, hasKey("_source"));
+        return (Map<String, Object>) data.get("_source");
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/search/fetch/innerhits/NestedChildrenFilterTest.java b/core/src/test/java/org/elasticsearch/search/fetch/innerhits/NestedChildrenFilterTest.java
deleted file mode 100644
index 977ce01..0000000
--- a/core/src/test/java/org/elasticsearch/search/fetch/innerhits/NestedChildrenFilterTest.java
+++ /dev/null
@@ -1,102 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.fetch.innerhits;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.IntField;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.Filter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.QueryWrapperFilter;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TotalHitCountCollector;
-import org.apache.lucene.search.join.BitDocIdSetCachingWrapperFilter;
-import org.apache.lucene.search.join.BitDocIdSetFilter;
-import org.apache.lucene.store.Directory;
-import org.elasticsearch.search.fetch.FetchSubPhase;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsContext.NestedInnerHits.NestedChildrenQuery;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.util.ArrayList;
-import java.util.List;
-
-import static org.hamcrest.Matchers.equalTo;
-
-/**
- */
-public class NestedChildrenFilterTest extends ESTestCase {
-
-    @Test
-    public void testNestedChildrenFilter() throws Exception {
-        int numParentDocs = scaledRandomIntBetween(0, 32);
-        int maxChildDocsPerParent = scaledRandomIntBetween(8, 16);
-
-        Directory dir = newDirectory();
-        RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-        for (int i = 0; i < numParentDocs; i++) {
-            int numChildDocs = scaledRandomIntBetween(0, maxChildDocsPerParent);
-            List<Document> docs = new ArrayList<>(numChildDocs + 1);
-            for (int j = 0; j < numChildDocs; j++) {
-                Document childDoc = new Document();
-                childDoc.add(new StringField("type", "child", Field.Store.NO));
-                docs.add(childDoc);
-            }
-
-            Document parenDoc = new Document();
-            parenDoc.add(new StringField("type", "parent", Field.Store.NO));
-            parenDoc.add(new IntField("num_child_docs", numChildDocs, Field.Store.YES));
-            docs.add(parenDoc);
-            writer.addDocuments(docs);
-        }
-
-        IndexReader reader = writer.getReader();
-        writer.close();
-
-        IndexSearcher searcher = new IndexSearcher(reader);
-        FetchSubPhase.HitContext hitContext = new FetchSubPhase.HitContext();
-        BitDocIdSetFilter parentFilter = new BitDocIdSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term("type", "parent"))));
-        Filter childFilter = new QueryWrapperFilter(new TermQuery(new Term("type", "child")));
-        int checkedParents = 0;
-        for (LeafReaderContext leaf : reader.leaves()) {
-            DocIdSetIterator parents = parentFilter.getDocIdSet(leaf).iterator();
-            for (int parentDoc = parents.nextDoc(); parentDoc != DocIdSetIterator.NO_MORE_DOCS ; parentDoc = parents.nextDoc()) {
-                int expectedChildDocs = leaf.reader().document(parentDoc).getField("num_child_docs").numericValue().intValue();
-                hitContext.reset(null, leaf, parentDoc, searcher);
-                NestedChildrenQuery nestedChildrenFilter = new NestedChildrenQuery(parentFilter, childFilter, hitContext);
-                TotalHitCountCollector totalHitCountCollector = new TotalHitCountCollector();
-                searcher.search(new ConstantScoreQuery(nestedChildrenFilter), totalHitCountCollector);
-                assertThat(totalHitCountCollector.getTotalHits(), equalTo(expectedChildDocs));
-                checkedParents++;
-            }
-        }
-        assertThat(checkedParents, equalTo(numParentDocs));
-        reader.close();
-        dir.close();
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/fetch/innerhits/NestedChildrenFilterTests.java b/core/src/test/java/org/elasticsearch/search/fetch/innerhits/NestedChildrenFilterTests.java
new file mode 100644
index 0000000..ee0b52e
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/search/fetch/innerhits/NestedChildrenFilterTests.java
@@ -0,0 +1,102 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.fetch.innerhits;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.IntField;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.search.Filter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.QueryWrapperFilter;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TotalHitCountCollector;
+import org.apache.lucene.search.join.BitDocIdSetCachingWrapperFilter;
+import org.apache.lucene.search.join.BitDocIdSetFilter;
+import org.apache.lucene.store.Directory;
+import org.elasticsearch.search.fetch.FetchSubPhase;
+import org.elasticsearch.search.fetch.innerhits.InnerHitsContext.NestedInnerHits.NestedChildrenQuery;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import static org.hamcrest.Matchers.equalTo;
+
+/**
+ */
+public class NestedChildrenFilterTests extends ESTestCase {
+
+    @Test
+    public void testNestedChildrenFilter() throws Exception {
+        int numParentDocs = scaledRandomIntBetween(0, 32);
+        int maxChildDocsPerParent = scaledRandomIntBetween(8, 16);
+
+        Directory dir = newDirectory();
+        RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+        for (int i = 0; i < numParentDocs; i++) {
+            int numChildDocs = scaledRandomIntBetween(0, maxChildDocsPerParent);
+            List<Document> docs = new ArrayList<>(numChildDocs + 1);
+            for (int j = 0; j < numChildDocs; j++) {
+                Document childDoc = new Document();
+                childDoc.add(new StringField("type", "child", Field.Store.NO));
+                docs.add(childDoc);
+            }
+
+            Document parenDoc = new Document();
+            parenDoc.add(new StringField("type", "parent", Field.Store.NO));
+            parenDoc.add(new IntField("num_child_docs", numChildDocs, Field.Store.YES));
+            docs.add(parenDoc);
+            writer.addDocuments(docs);
+        }
+
+        IndexReader reader = writer.getReader();
+        writer.close();
+
+        IndexSearcher searcher = new IndexSearcher(reader);
+        FetchSubPhase.HitContext hitContext = new FetchSubPhase.HitContext();
+        BitDocIdSetFilter parentFilter = new BitDocIdSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term("type", "parent"))));
+        Filter childFilter = new QueryWrapperFilter(new TermQuery(new Term("type", "child")));
+        int checkedParents = 0;
+        for (LeafReaderContext leaf : reader.leaves()) {
+            DocIdSetIterator parents = parentFilter.getDocIdSet(leaf).iterator();
+            for (int parentDoc = parents.nextDoc(); parentDoc != DocIdSetIterator.NO_MORE_DOCS ; parentDoc = parents.nextDoc()) {
+                int expectedChildDocs = leaf.reader().document(parentDoc).getField("num_child_docs").numericValue().intValue();
+                hitContext.reset(null, leaf, parentDoc, searcher);
+                NestedChildrenQuery nestedChildrenFilter = new NestedChildrenQuery(parentFilter, childFilter, hitContext);
+                TotalHitCountCollector totalHitCountCollector = new TotalHitCountCollector();
+                searcher.search(new ConstantScoreQuery(nestedChildrenFilter), totalHitCountCollector);
+                assertThat(totalHitCountCollector.getTotalHits(), equalTo(expectedChildDocs));
+                checkedParents++;
+            }
+        }
+        assertThat(checkedParents, equalTo(numParentDocs));
+        reader.close();
+        dir.close();
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java b/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
index 61c4dc7..0c569b1 100644
--- a/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
@@ -27,9 +27,10 @@ import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.settings.Settings.Builder;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.query.*;
+import org.elasticsearch.index.query.BoostableQueryBuilder;
 import org.elasticsearch.index.query.IdsQueryBuilder;
 import org.elasticsearch.index.query.MatchQueryBuilder;
+import org.elasticsearch.index.query.MatchQueryBuilder.Operator;
 import org.elasticsearch.index.query.MatchQueryBuilder.Type;
 import org.elasticsearch.index.query.MultiMatchQueryBuilder;
 import org.elasticsearch.index.query.QueryBuilder;
@@ -70,7 +71,12 @@ import static org.elasticsearch.index.query.QueryBuilders.typeQuery;
 import static org.elasticsearch.index.query.QueryBuilders.wildcardQuery;
 import static org.elasticsearch.search.builder.SearchSourceBuilder.highlight;
 import static org.elasticsearch.search.builder.SearchSourceBuilder.searchSource;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFailures;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHighlight;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNotHighlighted;
 import static org.elasticsearch.test.hamcrest.RegexMatcher.matches;
 import static org.hamcrest.Matchers.anyOf;
 import static org.hamcrest.Matchers.containsString;
@@ -1391,7 +1397,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
-                .query(boostingQuery(termQuery("field2", "brown"), termQuery("field2", "foobar")).negativeBoost(0.5f))
+                .query(boostingQuery().positive(termQuery("field2", "brown")).negative(termQuery("field2", "foobar")).negativeBoost(0.5f))
                 .highlight(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
 
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
@@ -1410,7 +1416,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
-                .query(boostingQuery(termQuery("field2", "brown"), termQuery("field2", "foobar")).negativeBoost(0.5f))
+                .query(boostingQuery().positive(termQuery("field2", "brown")).negative(termQuery("field2", "foobar")).negativeBoost(0.5f))
                 .highlight(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
 
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
@@ -2324,7 +2330,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
-                .query(boostingQuery(termQuery("field2", "brown"), termQuery("field2", "foobar")).negativeBoost(0.5f))
+                .query(boostingQuery().positive(termQuery("field2", "brown")).negative(termQuery("field2", "foobar")).negativeBoost(0.5f))
                 .highlight(highlight().field("field2").preTags("<x>").postTags("</x>"));
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -2613,7 +2619,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 queryStringQuery("\"highlight words together\"").field("field1^100").autoGeneratePhraseQueries(true));
     }
 
-    private <P extends AbstractQueryBuilder<P>> void
+    private <P extends QueryBuilder & BoostableQueryBuilder<?>> void
             phraseBoostTestCaseForClauses(String highlighterType, float boost, QueryBuilder terms, P phrase) {
         Matcher<String> highlightedMatcher = Matchers.either(containsString("<em>highlight words together</em>")).or(
                 containsString("<em>highlight</em> <em>words</em> <em>together</em>"));
@@ -2627,10 +2633,10 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         assertHighlight(response, 0, "field1", 0, 1, highlightedMatcher);
         phrase.boost(1);
         // Try with a boosting query
-        response = search.setQuery(boostingQuery(phrase, terms).boost(boost).negativeBoost(1)).get();
+        response = search.setQuery(boostingQuery().positive(phrase).negative(terms).boost(boost).negativeBoost(1)).get();
         assertHighlight(response, 0, "field1", 0, 1, highlightedMatcher);
         // Try with a boosting query using a negative boost
-        response = search.setQuery(boostingQuery(phrase, terms).boost(1).negativeBoost(1/boost)).get();
+        response = search.setQuery(boostingQuery().positive(phrase).negative(terms).boost(1).negativeBoost(1/boost)).get();
         assertHighlight(response, 0, "field1", 0, 1, highlightedMatcher);
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/search/query/MultiMatchQueryIT.java b/core/src/test/java/org/elasticsearch/search/query/MultiMatchQueryIT.java
index 793d365..b9099d0 100644
--- a/core/src/test/java/org/elasticsearch/search/query/MultiMatchQueryIT.java
+++ b/core/src/test/java/org/elasticsearch/search/query/MultiMatchQueryIT.java
@@ -28,7 +28,6 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.MatchQueryBuilder;
 import org.elasticsearch.index.query.MultiMatchQueryBuilder;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.search.SearchHits;
@@ -155,7 +154,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
         MatchQueryBuilder.Type type = randomBoolean() ? null : MatchQueryBuilder.Type.BOOLEAN;
         SearchResponse searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR))).get();
+                        .operator(MatchQueryBuilder.Operator.OR))).get();
         Set<String> topNIds = Sets.newHashSet("theone", "theother");
         for (int i = 0; i < searchResponse.getHits().hits().length; i++) {
             topNIds.remove(searchResponse.getHits().getAt(i).getId());
@@ -167,25 +166,25 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR).useDisMax(false).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).useDisMax(false).type(type))).get();
         assertFirstHit(searchResponse, anyOf(hasId("theone"), hasId("theother")));
         assertThat(searchResponse.getHits().hits()[0].getScore(), greaterThan(searchResponse.getHits().hits()[1].getScore()));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).type(type))).get();
         assertFirstHit(searchResponse, hasId("theother"));
 
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.AND).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.AND).type(type))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.AND).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.AND).type(type))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
     }
@@ -194,18 +193,18 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
     public void testPhraseType() {
         SearchResponse searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("Man the Ultimate", "full_name_phrase", "first_name_phrase", "last_name_phrase", "category_phrase")
-                        .operator(Operator.OR).type(MatchQueryBuilder.Type.PHRASE))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).type(MatchQueryBuilder.Type.PHRASE))).get();
         assertFirstHit(searchResponse, hasId("ultimate2"));
         assertHitCount(searchResponse, 1l);
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("Captain", "full_name_phrase", "first_name_phrase", "last_name_phrase", "category_phrase")
-                        .operator(Operator.OR).type(MatchQueryBuilder.Type.PHRASE))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).type(MatchQueryBuilder.Type.PHRASE))).get();
         assertThat(searchResponse.getHits().getTotalHits(), greaterThan(1l));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("the Ul", "full_name_phrase", "first_name_phrase", "last_name_phrase", "category_phrase")
-                        .operator(Operator.OR).type(MatchQueryBuilder.Type.PHRASE_PREFIX))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).type(MatchQueryBuilder.Type.PHRASE_PREFIX))).get();
         assertSearchHits(searchResponse, "ultimate2", "ultimate1");
         assertHitCount(searchResponse, 2l);
     }
@@ -264,7 +263,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
         Float cutoffFrequency = randomBoolean() ? Math.min(1, numDocs * 1.f / between(10, 20)) : 1.f / between(10, 20);
         SearchResponse searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR).cutoffFrequency(cutoffFrequency))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).cutoffFrequency(cutoffFrequency))).get();
         Set<String> topNIds = Sets.newHashSet("theone", "theother");
         for (int i = 0; i < searchResponse.getHits().hits().length; i++) {
             topNIds.remove(searchResponse.getHits().getAt(i).getId());
@@ -277,39 +276,39 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
         cutoffFrequency = randomBoolean() ? Math.min(1, numDocs * 1.f / between(10, 20)) : 1.f / between(10, 20);
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR).useDisMax(false).cutoffFrequency(cutoffFrequency).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).useDisMax(false).cutoffFrequency(cutoffFrequency).type(type))).get();
         assertFirstHit(searchResponse, anyOf(hasId("theone"), hasId("theother")));
         assertThat(searchResponse.getHits().hits()[0].getScore(), greaterThan(searchResponse.getHits().hits()[1].getScore()));
         long size = searchResponse.getHits().getTotalHits();
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR).useDisMax(false).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).useDisMax(false).type(type))).get();
         assertFirstHit(searchResponse, anyOf(hasId("theone"), hasId("theother")));
         assertThat("common terms expected to be a way smaller result set", size, lessThan(searchResponse.getHits().getTotalHits()));
 
         cutoffFrequency = randomBoolean() ? Math.min(1, numDocs * 1.f / between(10, 20)) : 1.f / between(10, 20);
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR).cutoffFrequency(cutoffFrequency).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).cutoffFrequency(cutoffFrequency).type(type))).get();
         assertFirstHit(searchResponse, hasId("theother"));
 
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.AND).cutoffFrequency(cutoffFrequency).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.AND).cutoffFrequency(cutoffFrequency).type(type))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.AND).cutoffFrequency(cutoffFrequency).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.AND).cutoffFrequency(cutoffFrequency).type(type))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero", "first_name", "last_name", "category")
-                        .operator(Operator.AND).cutoffFrequency(cutoffFrequency)
+                        .operator(MatchQueryBuilder.Operator.AND).cutoffFrequency(cutoffFrequency)
                         .analyzer("category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS))).get();
         assertHitCount(searchResponse, 1l);
@@ -331,7 +330,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
                 SearchResponse left = client().prepareSearch("test").setSize(numDocs)
                         .addSort(SortBuilders.scoreSort()).addSort(SortBuilders.fieldSort("_uid"))
                         .setQuery(randomizeType(multiMatchQueryBuilder
-                                .operator(Operator.OR).type(type))).get();
+                                .operator(MatchQueryBuilder.Operator.OR).type(type))).get();
 
                 SearchResponse right = client().prepareSearch("test").setSize(numDocs)
                         .addSort(SortBuilders.scoreSort()).addSort(SortBuilders.fieldSort("_uid"))
@@ -347,7 +346,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
             {
                 MatchQueryBuilder.Type type = randomBoolean() ? null : MatchQueryBuilder.Type.BOOLEAN;
                 String minShouldMatch = randomBoolean() ? null : "" + between(0, 1);
-                Operator op = randomBoolean() ? Operator.AND : Operator.OR;
+                MatchQueryBuilder.Operator op = randomBoolean() ? MatchQueryBuilder.Operator.AND : MatchQueryBuilder.Operator.OR;
                 MultiMatchQueryBuilder multiMatchQueryBuilder = randomBoolean() ? multiMatchQuery("captain america", "full_name", "first_name", "last_name", "category") :
                         multiMatchQuery("captain america", "*_name", randomBoolean() ? "category" : "categ*");
                 SearchResponse left = client().prepareSearch("test").setSize(numDocs)
@@ -368,7 +367,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
 
             {
                 String minShouldMatch = randomBoolean() ? null : "" + between(0, 1);
-                Operator op = randomBoolean() ? Operator.AND : Operator.OR;
+                MatchQueryBuilder.Operator op = randomBoolean() ? MatchQueryBuilder.Operator.AND : MatchQueryBuilder.Operator.OR;
                 SearchResponse left = client().prepareSearch("test").setSize(numDocs)
                         .addSort(SortBuilders.scoreSort()).addSort(SortBuilders.fieldSort("_uid"))
                         .setQuery(randomizeType(multiMatchQuery("capta", "full_name", "first_name", "last_name", "category")
@@ -386,7 +385,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
             }
             {
                 String minShouldMatch = randomBoolean() ? null : "" + between(0, 1);
-                Operator op = randomBoolean() ? Operator.AND : Operator.OR;
+                MatchQueryBuilder.Operator op = randomBoolean() ? MatchQueryBuilder.Operator.AND : MatchQueryBuilder.Operator.OR;
                 SearchResponse left;
                 if (randomBoolean()) {
                     left = client().prepareSearch("test").setSize(numDocs)
@@ -417,13 +416,13 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america", "full_name", "first_name", "last_name")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
-                        .operator(Operator.OR))).get();
+                        .operator(MatchQueryBuilder.Operator.OR))).get();
         assertFirstHit(searchResponse, hasId("theone"));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero captain america", "full_name", "first_name", "last_name", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
-                        .operator(Operator.OR))).get();
+                        .operator(MatchQueryBuilder.Operator.OR))).get();
         assertFirstHit(searchResponse, hasId("theone"));
         assertSecondHit(searchResponse, hasId("theother"));
         assertThat(searchResponse.getHits().hits()[0].getScore(), greaterThan(searchResponse.getHits().hits()[1].getScore()));
@@ -431,13 +430,13 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero", "full_name", "first_name", "last_name", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
-                        .operator(Operator.OR))).get();
+                        .operator(MatchQueryBuilder.Operator.OR))).get();
         assertFirstHit(searchResponse, hasId("theother"));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america", "full_name", "first_name", "last_name", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
-                        .operator(Operator.AND))).get();
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
 
@@ -445,7 +444,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
                 .setQuery(randomizeType(multiMatchQuery("captain america 15", "full_name", "first_name", "last_name", "category", "skill")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
                         .analyzer("category")
-                        .operator(Operator.AND))).get();
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
 
@@ -466,7 +465,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
                         .cutoffFrequency(0.1f)
                         .analyzer("category")
-                        .operator(Operator.OR))).get();
+                        .operator(MatchQueryBuilder.Operator.OR))).get();
         assertFirstHit(searchResponse, anyOf(hasId("theother"), hasId("theone")));
         long numResults = searchResponse.getHits().totalHits();
 
@@ -474,7 +473,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
                 .setQuery(randomizeType(multiMatchQuery("captain america marvel hero", "first_name", "last_name", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
                         .analyzer("category")
-                        .operator(Operator.OR))).get();
+                        .operator(MatchQueryBuilder.Operator.OR))).get();
         assertThat(numResults, lessThan(searchResponse.getHits().getTotalHits()));
         assertFirstHit(searchResponse, hasId("theone"));
 
@@ -484,28 +483,28 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
                 .setQuery(randomizeType(multiMatchQuery("captain america marvel hero", "first_name", "last_name", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
                         .analyzer("category")
-                        .operator(Operator.AND))).get();
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
         // counter example
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america marvel hero", "first_name", "last_name", "category")
                         .type(randomBoolean() ? MultiMatchQueryBuilder.Type.CROSS_FIELDS : null)
-                        .operator(Operator.AND))).get();
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertHitCount(searchResponse, 0l);
 
         // counter example
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america marvel hero", "first_name", "last_name", "category")
                         .type(randomBoolean() ? MultiMatchQueryBuilder.Type.CROSS_FIELDS : null)
-                        .operator(Operator.AND))).get();
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertHitCount(searchResponse, 0l);
 
         // test if boosts work
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("the ultimate", "full_name", "first_name", "last_name^2", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
-                        .operator(Operator.AND))).get();
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertFirstHit(searchResponse, hasId("ultimate1"));   // has ultimate in the last_name and that is boosted
         assertSecondHit(searchResponse, hasId("ultimate2"));
         assertThat(searchResponse.getHits().hits()[0].getScore(), greaterThan(searchResponse.getHits().hits()[1].getScore()));
@@ -515,7 +514,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("the ultimate", "full_name", "first_name", "last_name", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
-                        .operator(Operator.AND))).get();
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertFirstHit(searchResponse, hasId("ultimate2"));
         assertSecondHit(searchResponse, hasId("ultimate1"));
         assertThat(searchResponse.getHits().hits()[0].getScore(), greaterThan(searchResponse.getHits().hits()[1].getScore()));
diff --git a/core/src/test/java/org/elasticsearch/search/query/QueryPhaseTests.java b/core/src/test/java/org/elasticsearch/search/query/QueryPhaseTests.java
new file mode 100644
index 0000000..99d6007
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/search/query/QueryPhaseTests.java
@@ -0,0 +1,167 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.query;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field.Store;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.index.MultiReader;
+import org.apache.lucene.index.NoMergePolicy;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.MatchNoDocsQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.Weight;
+import org.apache.lucene.store.Directory;
+import org.elasticsearch.index.query.ParsedQuery;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.TestSearchContext;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+public class QueryPhaseTests extends ESTestCase {
+
+    private void countTestCase(Query query, IndexReader reader, boolean shouldCollect) throws Exception {
+        TestSearchContext context = new TestSearchContext();
+        context.parsedQuery(new ParsedQuery(query));
+        context.setSize(0);
+
+        IndexSearcher searcher = new IndexSearcher(reader);
+        final AtomicBoolean collected = new AtomicBoolean();
+        IndexSearcher contextSearcher = new IndexSearcher(reader) {
+            protected void search(List<LeafReaderContext> leaves, Weight weight, Collector collector) throws IOException {
+                collected.set(true);
+                super.search(leaves, weight, collector);
+            }
+        };
+
+        final boolean rescore = QueryPhase.execute(context, contextSearcher);
+        assertFalse(rescore);
+        assertEquals(searcher.count(query), context.queryResult().topDocs().totalHits);
+        assertEquals(shouldCollect, collected.get());
+    }
+
+    private void countTestCase(boolean withDeletions) throws Exception {
+        Directory dir = newDirectory();
+        IndexWriterConfig iwc = newIndexWriterConfig().setMergePolicy(NoMergePolicy.INSTANCE);
+        RandomIndexWriter w = new RandomIndexWriter(getRandom(), dir, iwc);
+        final int numDocs = scaledRandomIntBetween(100, 200);
+        for (int i = 0; i < numDocs; ++i) {
+            Document doc = new Document();
+            if (randomBoolean()) {
+                doc.add(new StringField("foo", "bar", Store.NO));
+            }
+            if (randomBoolean()) {
+                doc.add(new StringField("foo", "baz", Store.NO));
+            }
+            if (withDeletions && (rarely() || i == 0)) {
+                doc.add(new StringField("delete", "yes", Store.NO));
+            }
+            w.addDocument(doc);
+        }
+        if (withDeletions) {
+            w.deleteDocuments(new Term("delete", "yes"));
+        }
+        final IndexReader reader = w.getReader();
+        Query matchAll = new MatchAllDocsQuery();
+        Query matchAllCsq = new ConstantScoreQuery(matchAll);
+        Query tq = new TermQuery(new Term("foo", "bar"));
+        Query tCsq = new ConstantScoreQuery(tq);
+        BooleanQuery bq = new BooleanQuery();
+        bq.add(matchAll, Occur.SHOULD);
+        bq.add(tq, Occur.MUST);
+
+        countTestCase(matchAll, reader, false);
+        countTestCase(matchAllCsq, reader, false);
+        countTestCase(tq, reader, withDeletions);
+        countTestCase(tCsq, reader, withDeletions);
+        countTestCase(bq, reader, true);
+        reader.close();
+        w.close();
+        dir.close();
+    }
+
+    public void testCountWithoutDeletions() throws Exception {
+        countTestCase(false);
+    }
+
+    public void testCountWithDeletions() throws Exception {
+        countTestCase(true);
+    }
+
+    public void testPostFilterDisablesCountOptimization() throws Exception {
+        TestSearchContext context = new TestSearchContext();
+        context.parsedQuery(new ParsedQuery(new MatchAllDocsQuery()));
+        context.setSize(0);
+
+        final AtomicBoolean collected = new AtomicBoolean();
+        IndexSearcher contextSearcher = new IndexSearcher(new MultiReader()) {
+            protected void search(List<LeafReaderContext> leaves, Weight weight, Collector collector) throws IOException {
+                collected.set(true);
+                super.search(leaves, weight, collector);
+            }
+        };
+
+        QueryPhase.execute(context, contextSearcher);
+        assertEquals(0, context.queryResult().topDocs().totalHits);
+        assertFalse(collected.get());
+
+        context.parsedPostFilter(new ParsedQuery(new MatchNoDocsQuery()));
+        QueryPhase.execute(context, contextSearcher);
+        assertEquals(0, context.queryResult().topDocs().totalHits);
+        assertTrue(collected.get());
+    }
+
+    public void testMinScoreDisablesCountOptimization() throws Exception {
+        TestSearchContext context = new TestSearchContext();
+        context.parsedQuery(new ParsedQuery(new MatchAllDocsQuery()));
+        context.setSize(0);
+
+        final AtomicBoolean collected = new AtomicBoolean();
+        IndexSearcher contextSearcher = new IndexSearcher(new MultiReader()) {
+            protected void search(List<LeafReaderContext> leaves, Weight weight, Collector collector) throws IOException {
+                collected.set(true);
+                super.search(leaves, weight, collector);
+            }
+        };
+
+        QueryPhase.execute(context, contextSearcher);
+        assertEquals(0, context.queryResult().topDocs().totalHits);
+        assertFalse(collected.get());
+
+        context.minimumScore(1);
+        QueryPhase.execute(context, contextSearcher);
+        assertEquals(0, context.queryResult().topDocs().totalHits);
+        assertTrue(collected.get());
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java b/core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
index 8bc89c7..7fa543a 100644
--- a/core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
+++ b/core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.search.query;
 
 import org.apache.lucene.util.English;
+import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
 import org.elasticsearch.action.index.IndexRequestBuilder;
@@ -31,8 +32,15 @@ import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.mapper.MapperParsingException;
-import org.elasticsearch.index.query.*;
+import org.elasticsearch.index.query.BoolQueryBuilder;
+import org.elasticsearch.index.query.CommonTermsQueryBuilder.Operator;
+import org.elasticsearch.index.query.MatchQueryBuilder;
 import org.elasticsearch.index.query.MatchQueryBuilder.Type;
+import org.elasticsearch.index.query.MultiMatchQueryBuilder;
+import org.elasticsearch.index.query.QueryBuilders;
+import org.elasticsearch.index.query.QueryStringQueryBuilder;
+import org.elasticsearch.index.query.TermQueryBuilder;
+import org.elasticsearch.index.query.WrapperQueryBuilder;
 import org.elasticsearch.rest.RestStatus;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.search.SearchHit;
@@ -56,8 +64,24 @@ import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.*;
 import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.scriptFunction;
 import static org.elasticsearch.test.VersionUtils.randomVersion;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
-import static org.hamcrest.Matchers.*;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFailures;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFirstHit;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHit;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHits;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSecondHit;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThirdHit;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasId;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasScore;
+import static org.hamcrest.Matchers.allOf;
+import static org.hamcrest.Matchers.closeTo;
+import static org.hamcrest.Matchers.containsString;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.greaterThan;
+import static org.hamcrest.Matchers.is;
 
 public class SearchQueryIT extends ESIntegTestCase {
 
@@ -119,7 +143,7 @@ public class SearchQueryIT extends ESIntegTestCase {
         client().prepareIndex("test", "type1", "1").setSource("field1", "value1").get();
         client().prepareIndex("test", "type1", "2").setSource("field1", "value2").get();
         client().prepareIndex("test", "type1", "3").setSource("field1", "value3").get();
-
+        ensureGreen();
         waitForRelocation();
         optimize();
         refresh();
@@ -327,18 +351,18 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertThirdHit(searchResponse, hasId("2"));
 
         // try the same with match query
-        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the quick brown").cutoffFrequency(3).operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the quick brown").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2l);
         assertFirstHit(searchResponse, hasId("1"));
         assertSecondHit(searchResponse, hasId("2"));
 
-        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the quick brown").cutoffFrequency(3).operator(Operator.OR)).get();
+        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the quick brown").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.OR)).get();
         assertHitCount(searchResponse, 3l);
         assertFirstHit(searchResponse, hasId("1"));
         assertSecondHit(searchResponse, hasId("2"));
         assertThirdHit(searchResponse, hasId("3"));
 
-        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the quick brown").cutoffFrequency(3).operator(Operator.AND).analyzer("stop")).get();
+        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the quick brown").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.AND).analyzer("stop")).get();
         assertHitCount(searchResponse, 3l);
         // stop drops "the" since its a stopword
         assertFirstHit(searchResponse, hasId("1"));
@@ -346,7 +370,7 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertThirdHit(searchResponse, hasId("2"));
 
         // try the same with multi match query
-        searchResponse = client().prepareSearch().setQuery(multiMatchQuery("the quick brown", "field1", "field2").cutoffFrequency(3).operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch().setQuery(multiMatchQuery("the quick brown", "field1", "field2").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 3l);
         assertFirstHit(searchResponse, hasId("3")); // better score due to different query stats
         assertSecondHit(searchResponse, hasId("1"));
@@ -419,18 +443,18 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertThirdHit(searchResponse, hasId("2"));
 
         // try the same with match query
-        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the fast brown").cutoffFrequency(3).operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the fast brown").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2l);
         assertFirstHit(searchResponse, hasId("1"));
         assertSecondHit(searchResponse, hasId("2"));
 
-        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the fast brown").cutoffFrequency(3).operator(Operator.OR)).get();
+        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the fast brown").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.OR)).get();
         assertHitCount(searchResponse, 3l);
         assertFirstHit(searchResponse, hasId("1"));
         assertSecondHit(searchResponse, hasId("2"));
         assertThirdHit(searchResponse, hasId("3"));
 
-        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the fast brown").cutoffFrequency(3).operator(Operator.AND).analyzer("stop")).get();
+        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the fast brown").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.AND).analyzer("stop")).get();
         assertHitCount(searchResponse, 3l);
         // stop drops "the" since its a stopword
         assertFirstHit(searchResponse, hasId("1"));
@@ -443,7 +467,7 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertSecondHit(searchResponse, hasId("2"));
 
         // try the same with multi match query
-        searchResponse = client().prepareSearch().setQuery(multiMatchQuery("the fast brown", "field1", "field2").cutoffFrequency(3).operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch().setQuery(multiMatchQuery("the fast brown", "field1", "field2").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 3l);
         assertFirstHit(searchResponse, hasId("3")); // better score due to different query stats
         assertSecondHit(searchResponse, hasId("1"));
@@ -914,7 +938,7 @@ public class SearchQueryIT extends ESIntegTestCase {
 
         client().admin().indices().prepareRefresh("test").get();
         builder = multiMatchQuery("value1", "field1", "field2")
-                .operator(Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together.
+                .operator(MatchQueryBuilder.Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together.
         searchResponse = client().prepareSearch()
                 .setQuery(builder)
                 .get();
@@ -923,14 +947,14 @@ public class SearchQueryIT extends ESIntegTestCase {
 
         refresh();
         builder = multiMatchQuery("value1", "field1", "field3^1.5")
-                .operator(Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together.
+                .operator(MatchQueryBuilder.Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together.
         searchResponse = client().prepareSearch().setQuery(builder).get();
         assertHitCount(searchResponse, 2l);
         assertSearchHits(searchResponse, "3", "1");
 
         client().admin().indices().prepareRefresh("test").get();
         builder = multiMatchQuery("value1").field("field1").field("field3", 1.5f)
-                .operator(Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together.
+                .operator(MatchQueryBuilder.Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together.
         searchResponse = client().prepareSearch().setQuery(builder).get();
         assertHitCount(searchResponse, 2l);
         assertSearchHits(searchResponse, "3", "1");
@@ -1573,9 +1597,10 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertHitCount(searchResponse, 1l);
 
         searchResponse = client().prepareSearch("test").setQuery(
-                spanNearQuery(3)
+                spanNearQuery()
                         .clause(spanTermQuery("description", "foo"))
-                        .clause(spanTermQuery("description", "other"))).get();
+                        .clause(spanTermQuery("description", "other"))
+                        .slop(3)).get();
         assertHitCount(searchResponse, 3l);
     }
 
@@ -1620,22 +1645,33 @@ public class SearchQueryIT extends ESIntegTestCase {
         refresh();
 
         SearchResponse searchResponse = client().prepareSearch("test")
-                .setQuery(spanNotQuery(spanNearQuery(1)
+                .setQuery(spanNotQuery().include(spanNearQuery()
                         .clause(QueryBuilders.spanTermQuery("description", "quick"))
-                        .clause(QueryBuilders.spanTermQuery("description", "fox")), spanTermQuery("description", "brown"))).get();
+                        .clause(QueryBuilders.spanTermQuery("description", "fox")).slop(1)).exclude(spanTermQuery("description", "brown"))).get();
         assertHitCount(searchResponse, 1l);
 
         searchResponse = client().prepareSearch("test")
-                .setQuery(spanNotQuery(spanNearQuery(1)
+                .setQuery(spanNotQuery().include(spanNearQuery()
                         .clause(QueryBuilders.spanTermQuery("description", "quick"))
-                        .clause(QueryBuilders.spanTermQuery("description", "fox")), spanTermQuery("description", "sleeping")).dist(5)).get();
+                        .clause(QueryBuilders.spanTermQuery("description", "fox")).slop(1)).exclude(spanTermQuery("description", "sleeping")).dist(5)).get();
         assertHitCount(searchResponse, 1l);
 
         searchResponse = client().prepareSearch("test")
-                .setQuery(spanNotQuery(spanNearQuery(1)
+                .setQuery(spanNotQuery().include(spanNearQuery()
                         .clause(QueryBuilders.spanTermQuery("description", "quick"))
-                        .clause(QueryBuilders.spanTermQuery("description", "fox")), spanTermQuery("description", "jumped")).pre(1).post(1)).get();
+                        .clause(QueryBuilders.spanTermQuery("description", "fox")).slop(1)).exclude(spanTermQuery("description", "jumped")).pre(1).post(1)).get();
         assertHitCount(searchResponse, 1l);
+
+        try {
+            client().prepareSearch("test")
+                    .setQuery(spanNotQuery().include(spanNearQuery()
+                            .clause(QueryBuilders.spanTermQuery("description", "quick"))
+                            .clause(QueryBuilders.spanTermQuery("description", "fox")).slop(1)).exclude(spanTermQuery("description", "jumped")).dist(2).pre(2)
+                    ).get();
+            fail("ElasticsearchIllegalArgumentException should have been caught");
+        } catch (ElasticsearchException e) {
+            assertThat("ElasticsearchIllegalArgumentException should have been caught", e.getDetailedMessage(), containsString("spanNot can either use [dist] or [pre] & [post] (or none)"));
+        }
     }
 
     @Test
@@ -1731,18 +1767,18 @@ public class SearchQueryIT extends ESIntegTestCase {
 
         client().prepareIndex("test", "test", "1").setSource("text", "quick brown fox").get();
         refresh();
-        SearchResponse searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick").operator(Operator.AND)).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
-        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick brown").operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick brown").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
-        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "fast").operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "fast").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
 
         client().prepareIndex("test", "test", "2").setSource("text", "fast brown fox").get();
         refresh();
-        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick").operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2);
-        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick brown").operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick brown").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2);
     }
 
@@ -1762,12 +1798,12 @@ public class SearchQueryIT extends ESIntegTestCase {
 
         client().prepareIndex("test", "test", "1").setSource("text", "the fox runs across the street").get();
         refresh();
-        SearchResponse searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "fox runs").operator(Operator.AND)).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "fox runs").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
 
         client().prepareIndex("test", "test", "2").setSource("text", "run fox run").get();
         refresh();
-        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "fox runs").operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "fox runs").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2);
     }
 
@@ -1788,19 +1824,19 @@ public class SearchQueryIT extends ESIntegTestCase {
         client().prepareIndex("test", "test", "1").setSource("text", "quick brown fox").get();
         refresh();
 
-        SearchResponse searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick").defaultField("text").defaultOperator(Operator.AND)).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick").defaultField("text").defaultOperator(QueryStringQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
-        searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick brown").defaultField("text").defaultOperator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick brown").defaultField("text").defaultOperator(QueryStringQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
-        searchResponse = client().prepareSearch().setQuery(queryStringQuery("fast").defaultField("text").defaultOperator(Operator.AND)).get();
+        searchResponse = client().prepareSearch().setQuery(queryStringQuery("fast").defaultField("text").defaultOperator(QueryStringQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
 
         client().prepareIndex("test", "test", "2").setSource("text", "fast brown fox").get();
         refresh();
 
-        searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick").defaultField("text").defaultOperator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick").defaultField("text").defaultOperator(QueryStringQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2);
-        searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick brown").defaultField("text").defaultOperator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick brown").defaultField("text").defaultOperator(QueryStringQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2);
     }
 
@@ -1826,7 +1862,7 @@ public class SearchQueryIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("test")
                 .setQuery(
-                        queryStringQuery("foo.baz").useDisMax(false).defaultOperator(Operator.AND)
+                        queryStringQuery("foo.baz").useDisMax(false).defaultOperator(QueryStringQueryBuilder.Operator.AND)
                                 .field("field1").field("field2")).get();
         assertHitCount(response, 1l);
     }
@@ -1892,7 +1928,7 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertSearchHits(searchResponse, "1", "2", "3");
         searchResponse = client().prepareSearch("index1", "index2", "index3")
                 .setQuery(indicesQuery(matchQuery("text", "value1"), "index1")
-                        .noMatchQuery(QueryBuilders.matchAllQuery())).get();
+                        .noMatchQuery("all")).get();
         assertHitCount(searchResponse, 3l);
         assertSearchHits(searchResponse, "1", "2", "3");
 
@@ -1903,7 +1939,6 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertFirstHit(searchResponse, hasId("1"));
     }
 
-    @AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch/issues/12822")
     @Test // https://github.com/elasticsearch/elasticsearch/issues/2416
     public void testIndicesQuerySkipParsing() throws Exception {
         createIndex("simple");
diff --git a/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java b/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java
index a8c5ccb..e41c451 100644
--- a/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java
+++ b/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java
@@ -23,7 +23,7 @@ import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.BoolQueryBuilder;
-import org.elasticsearch.index.query.Operator;
+import org.elasticsearch.index.query.SimpleQueryStringBuilder;
 import org.elasticsearch.index.query.SimpleQueryStringFlag;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
@@ -33,7 +33,10 @@ import java.util.Locale;
 import java.util.concurrent.ExecutionException;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.*;
+import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
+import static org.elasticsearch.index.query.QueryBuilders.queryStringQuery;
+import static org.elasticsearch.index.query.QueryBuilders.simpleQueryStringQuery;
+import static org.elasticsearch.index.query.QueryBuilders.termQuery;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
 import static org.hamcrest.Matchers.equalTo;
 
@@ -67,7 +70,7 @@ public class SimpleQueryStringIT extends ESIntegTestCase {
         assertFirstHit(searchResponse, hasId("3"));
 
         searchResponse = client().prepareSearch().setQuery(
-                simpleQueryStringQuery("foo bar").defaultOperator(Operator.AND)).get();
+                simpleQueryStringQuery("foo bar").defaultOperator(SimpleQueryStringBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("3"));
 
@@ -248,21 +251,21 @@ public class SimpleQueryStringIT extends ESIntegTestCase {
 
         searchResponse = client().prepareSearch().setQuery(
                 simpleQueryStringQuery("foo | bar")
-                        .defaultOperator(Operator.AND)
+                        .defaultOperator(SimpleQueryStringBuilder.Operator.AND)
                         .flags(SimpleQueryStringFlag.OR)).get();
         assertHitCount(searchResponse, 3l);
         assertSearchHits(searchResponse, "1", "2", "3");
 
         searchResponse = client().prepareSearch().setQuery(
                 simpleQueryStringQuery("foo | bar")
-                        .defaultOperator(Operator.AND)
+                        .defaultOperator(SimpleQueryStringBuilder.Operator.AND)
                         .flags(SimpleQueryStringFlag.NONE)).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("3"));
 
         searchResponse = client().prepareSearch().setQuery(
                 simpleQueryStringQuery("baz | egg*")
-                        .defaultOperator(Operator.AND)
+                        .defaultOperator(SimpleQueryStringBuilder.Operator.AND)
                         .flags(SimpleQueryStringFlag.NONE)).get();
         assertHitCount(searchResponse, 0l);
 
@@ -279,7 +282,7 @@ public class SimpleQueryStringIT extends ESIntegTestCase {
 
         searchResponse = client().prepareSearch().setQuery(
                 simpleQueryStringQuery("baz | egg*")
-                        .defaultOperator(Operator.AND)
+                        .defaultOperator(SimpleQueryStringBuilder.Operator.AND)
                         .flags(SimpleQueryStringFlag.WHITESPACE, SimpleQueryStringFlag.PREFIX)).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("4"));
diff --git a/core/src/test/java/org/elasticsearch/search/rescore/QueryRescorerIT.java b/core/src/test/java/org/elasticsearch/search/rescore/QueryRescorerIT.java
index 5b559da..6aa31ca 100644
--- a/core/src/test/java/org/elasticsearch/search/rescore/QueryRescorerIT.java
+++ b/core/src/test/java/org/elasticsearch/search/rescore/QueryRescorerIT.java
@@ -33,7 +33,6 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.MatchQueryBuilder;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders;
 import org.elasticsearch.script.Script;
@@ -117,7 +116,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         ensureYellow();
         refresh();
         SearchResponse searchResponse = client().prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(MatchQueryBuilder.Operator.OR))
                 .setRescorer(RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "quick brown").slop(2).boost(4.0f)).setRescoreQueryWeight(2))
                 .setRescoreWindow(5).execute().actionGet();
 
@@ -127,7 +126,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         assertThat(searchResponse.getHits().getHits()[2].getId(), equalTo("2"));
 
         searchResponse = client().prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(MatchQueryBuilder.Operator.OR))
                 .setRescorer(RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "the quick brown").slop(3)))
                 .setRescoreWindow(5).execute().actionGet();
 
@@ -137,7 +136,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         assertThirdHit(searchResponse, hasId("3"));
 
         searchResponse = client().prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(MatchQueryBuilder.Operator.OR))
                 .setRescorer(RescoreBuilder.queryRescorer((QueryBuilders.matchPhraseQuery("field1", "the quick brown"))))
                 .setRescoreWindow(5).execute().actionGet();
 
@@ -180,7 +179,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         client().admin().indices().prepareRefresh("test").execute().actionGet();
         SearchResponse searchResponse = client()
                 .prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "lexington avenue massachusetts").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "lexington avenue massachusetts").operator(MatchQueryBuilder.Operator.OR))
                 .setFrom(0)
                 .setSize(5)
                 .setRescorer(
@@ -195,7 +194,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
 
         searchResponse = client()
                 .prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "lexington avenue massachusetts").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "lexington avenue massachusetts").operator(MatchQueryBuilder.Operator.OR))
                 .setFrom(0)
                 .setSize(5)
                 .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
@@ -212,7 +211,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         // Make sure non-zero from works:
         searchResponse = client()
                 .prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "lexington avenue massachusetts").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "lexington avenue massachusetts").operator(MatchQueryBuilder.Operator.OR))
                 .setFrom(2)
                 .setSize(5)
                 .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
@@ -321,7 +320,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
 
         SearchResponse searchResponse = client()
                 .prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "massachusetts").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "massachusetts").operator(MatchQueryBuilder.Operator.OR))
                 .setFrom(0)
             .setSize(5).execute().actionGet();
         assertThat(searchResponse.getHits().hits().length, equalTo(4));
@@ -334,7 +333,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         // Now, penalizing rescore (nothing matches the rescore query):
         searchResponse = client()
                 .prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "massachusetts").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "massachusetts").operator(MatchQueryBuilder.Operator.OR))
                 .setFrom(0)
                 .setSize(5)
                 .setRescorer(
@@ -426,7 +425,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                     .prepareSearch()
                     .setSearchType(SearchType.QUERY_THEN_FETCH)
                     .setPreference("test") // ensure we hit the same shards for tie-breaking
-                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(Operator.OR))
+                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(MatchQueryBuilder.Operator.OR))
                     .setFrom(0)
                     .setSize(resultSize)
                     .setRescorer(
@@ -441,7 +440,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
             SearchResponse plain = client().prepareSearch()
                     .setSearchType(SearchType.QUERY_THEN_FETCH)
                     .setPreference("test") // ensure we hit the same shards for tie-breaking
-                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(Operator.OR)).setFrom(0).setSize(resultSize)
+                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(MatchQueryBuilder.Operator.OR)).setFrom(0).setSize(resultSize)
                     .execute().actionGet();
             
             // check equivalence
@@ -451,7 +450,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                     .prepareSearch()
                     .setSearchType(SearchType.QUERY_THEN_FETCH)
                     .setPreference("test") // ensure we hit the same shards for tie-breaking
-                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(Operator.OR))
+                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(MatchQueryBuilder.Operator.OR))
                     .setFrom(0)
                     .setSize(resultSize)
                     .setRescorer(
@@ -469,7 +468,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                     .prepareSearch()
                     .setSearchType(SearchType.QUERY_THEN_FETCH)
                     .setPreference("test") // ensure we hit the same shards for tie-breaking
-                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(Operator.OR))
+                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(MatchQueryBuilder.Operator.OR))
                     .setFrom(0)
                     .setSize(resultSize)
                     .setRescorer(
@@ -504,7 +503,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
             SearchResponse searchResponse = client()
                     .prepareSearch()
                     .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
-                    .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
+                    .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(MatchQueryBuilder.Operator.OR))
                     .setRescorer(
                             RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "the quick brown").slop(2).boost(4.0f))
                                     .setQueryWeight(0.5f).setRescoreQueryWeight(0.4f)).setRescoreWindow(5).setExplain(true).execute()
@@ -542,7 +541,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
             SearchResponse searchResponse = client()
                     .prepareSearch()
                     .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
-                    .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
+                    .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(MatchQueryBuilder.Operator.OR))
                     .setRescorer(innerRescoreQuery).setRescoreWindow(5).setExplain(true).execute()
                     .actionGet();
             assertHitCount(searchResponse, 3);
@@ -565,7 +564,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                 searchResponse = client()
                         .prepareSearch()
                         .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
-                        .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
+                        .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(MatchQueryBuilder.Operator.OR))
                         .addRescorer(innerRescoreQuery).setRescoreWindow(5)
                         .addRescorer(outerRescoreQuery).setRescoreWindow(10)
                         .setExplain(true).get();
diff --git a/core/src/test/java/org/elasticsearch/search/suggest/CompletionTokenStreamTest.java b/core/src/test/java/org/elasticsearch/search/suggest/CompletionTokenStreamTest.java
deleted file mode 100644
index fde5037..0000000
--- a/core/src/test/java/org/elasticsearch/search/suggest/CompletionTokenStreamTest.java
+++ /dev/null
@@ -1,195 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.search.suggest;
-
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.TokenFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.core.SimpleAnalyzer;
-import org.apache.lucene.analysis.synonym.SynonymFilter;
-import org.apache.lucene.analysis.synonym.SynonymMap;
-import org.apache.lucene.analysis.synonym.SynonymMap.Builder;
-import org.apache.lucene.analysis.tokenattributes.*;
-import org.apache.lucene.search.suggest.analyzing.XAnalyzingSuggester;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CharsRef;
-import org.apache.lucene.util.IntsRef;
-import org.elasticsearch.search.suggest.completion.CompletionTokenStream;
-import org.elasticsearch.search.suggest.completion.CompletionTokenStream.ByteTermAttribute;
-import org.elasticsearch.test.ESTokenStreamTestCase;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.io.StringReader;
-import java.util.Set;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public class CompletionTokenStreamTest extends ESTokenStreamTestCase {
-
-    final XAnalyzingSuggester suggester = new XAnalyzingSuggester(new SimpleAnalyzer());
-
-    @Test
-    public void testSuggestTokenFilter() throws Exception {
-        Tokenizer tokenStream = new MockTokenizer(MockTokenizer.WHITESPACE, true);
-        tokenStream.setReader(new StringReader("mykeyword"));
-        BytesRef payload = new BytesRef("Surface keyword|friggin payload|10");
-        TokenStream suggestTokenStream = new ByteTermAttrToCharTermAttrFilter(new CompletionTokenStream(tokenStream, payload, new CompletionTokenStream.ToFiniteStrings() {
-            @Override
-            public Set<IntsRef> toFiniteStrings(TokenStream stream) throws IOException {
-                return suggester.toFiniteStrings(stream);
-            }
-        }));
-        assertTokenStreamContents(suggestTokenStream, new String[] {"mykeyword"}, null, null, new String[] {"Surface keyword|friggin payload|10"}, new int[] { 1 }, null, null);
-    }
-
-    @Test
-    public void testSuggestTokenFilterWithSynonym() throws Exception {
-        Builder builder = new SynonymMap.Builder(true);
-        builder.add(new CharsRef("mykeyword"), new CharsRef("mysynonym"), true);
-
-        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);
-        tokenizer.setReader(new StringReader("mykeyword"));
-        SynonymFilter filter = new SynonymFilter(tokenizer, builder.build(), true);
-
-        BytesRef payload = new BytesRef("Surface keyword|friggin payload|10");
-        TokenStream suggestTokenStream = new ByteTermAttrToCharTermAttrFilter(new CompletionTokenStream(filter, payload, new CompletionTokenStream.ToFiniteStrings() {
-            @Override
-            public Set<IntsRef> toFiniteStrings(TokenStream stream) throws IOException {
-                return suggester.toFiniteStrings(stream);
-            }
-        }));
-        assertTokenStreamContents(suggestTokenStream, new String[] {"mysynonym", "mykeyword"}, null, null, new String[] {"Surface keyword|friggin payload|10", "Surface keyword|friggin payload|10"}, new int[] { 2, 0 }, null, null);
-    }
-
-    @Test
-    public void testValidNumberOfExpansions() throws IOException {
-        Builder builder = new SynonymMap.Builder(true);
-        for (int i = 0; i < 256; i++) {
-            builder.add(new CharsRef("" + (i+1)), new CharsRef("" + (1000 + (i+1))), true);
-        }
-        StringBuilder valueBuilder = new StringBuilder();
-        for (int i = 0 ; i < 8 ; i++) {
-            valueBuilder.append(i+1);
-            valueBuilder.append(" ");
-        }
-        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);
-        tokenizer.setReader(new StringReader(valueBuilder.toString()));
-        SynonymFilter filter = new SynonymFilter(tokenizer, builder.build(), true);
-       
-        TokenStream suggestTokenStream = new CompletionTokenStream(filter, new BytesRef("Surface keyword|friggin payload|10"), new CompletionTokenStream.ToFiniteStrings() {
-            @Override
-            public Set<IntsRef> toFiniteStrings(TokenStream stream) throws IOException {
-                Set<IntsRef> finiteStrings = suggester.toFiniteStrings(stream);
-                return finiteStrings;
-            }
-        });
-        
-        suggestTokenStream.reset();
-        ByteTermAttribute attr = suggestTokenStream.addAttribute(ByteTermAttribute.class);
-        PositionIncrementAttribute posAttr = suggestTokenStream.addAttribute(PositionIncrementAttribute.class);
-        int maxPos = 0;
-        int count = 0;
-        while(suggestTokenStream.incrementToken()) {
-            count++;
-            assertNotNull(attr.getBytesRef());
-            assertTrue(attr.getBytesRef().length > 0);
-            maxPos += posAttr.getPositionIncrement();
-        }
-        suggestTokenStream.close();
-        assertEquals(count, 256);
-        assertEquals(count, maxPos);
-
-    }
-    
-    @Test(expected = IllegalArgumentException.class)
-    public void testInValidNumberOfExpansions() throws IOException {
-        Builder builder = new SynonymMap.Builder(true);
-        for (int i = 0; i < 256; i++) {
-            builder.add(new CharsRef("" + (i+1)), new CharsRef("" + (1000 + (i+1))), true);
-        }
-        StringBuilder valueBuilder = new StringBuilder();
-        for (int i = 0 ; i < 9 ; i++) { // 9 -> expands to 512
-            valueBuilder.append(i+1);
-            valueBuilder.append(" ");
-        }
-        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);
-        tokenizer.setReader(new StringReader(valueBuilder.toString()));
-        SynonymFilter filter = new SynonymFilter(tokenizer, builder.build(), true);
-       
-        TokenStream suggestTokenStream = new CompletionTokenStream(filter, new BytesRef("Surface keyword|friggin payload|10"), new CompletionTokenStream.ToFiniteStrings() {
-            @Override
-            public Set<IntsRef> toFiniteStrings(TokenStream stream) throws IOException {
-                Set<IntsRef> finiteStrings = suggester.toFiniteStrings(stream);
-                return finiteStrings;
-            }
-        });
-        
-        suggestTokenStream.reset();
-        suggestTokenStream.incrementToken();
-        suggestTokenStream.close();
-
-    }
-
-    @Test
-    public void testSuggestTokenFilterProperlyDelegateInputStream() throws Exception {
-        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);
-        tokenizer.setReader(new StringReader("mykeyword"));
-        BytesRef payload = new BytesRef("Surface keyword|friggin payload|10");
-        TokenStream suggestTokenStream = new ByteTermAttrToCharTermAttrFilter(new CompletionTokenStream(tokenizer, payload, new CompletionTokenStream.ToFiniteStrings() {
-            @Override
-            public Set<IntsRef> toFiniteStrings(TokenStream stream) throws IOException {
-                return suggester.toFiniteStrings(stream);
-            }
-        }));
-        TermToBytesRefAttribute termAtt = suggestTokenStream.getAttribute(TermToBytesRefAttribute.class);
-        assertNotNull(termAtt.getBytesRef());
-        suggestTokenStream.reset();
-
-        while (suggestTokenStream.incrementToken()) {
-            assertThat(termAtt.getBytesRef().utf8ToString(), equalTo("mykeyword"));
-        }
-        suggestTokenStream.end();
-        suggestTokenStream.close();
-    }
-
-
-    public final static class ByteTermAttrToCharTermAttrFilter extends TokenFilter {
-        private ByteTermAttribute byteAttr = addAttribute(ByteTermAttribute.class);
-        private PayloadAttribute payload = addAttribute(PayloadAttribute.class);
-        private TypeAttribute type = addAttribute(TypeAttribute.class);
-        private CharTermAttribute charTermAttribute = addAttribute(CharTermAttribute.class);
-        protected ByteTermAttrToCharTermAttrFilter(TokenStream input) {
-            super(input);
-        }
-
-        @Override
-        public boolean incrementToken() throws IOException {
-            if (input.incrementToken()) {
-                BytesRef bytesRef = byteAttr.getBytesRef();
-                // we move them over so we can assert them more easily in the tests
-                type.setType(payload.getPayload().utf8ToString());
-                return true;
-            }
-            return false;
-        }
-
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/suggest/CompletionTokenStreamTests.java b/core/src/test/java/org/elasticsearch/search/suggest/CompletionTokenStreamTests.java
new file mode 100644
index 0000000..d7280c8
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/search/suggest/CompletionTokenStreamTests.java
@@ -0,0 +1,195 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.suggest;
+
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.core.SimpleAnalyzer;
+import org.apache.lucene.analysis.synonym.SynonymFilter;
+import org.apache.lucene.analysis.synonym.SynonymMap;
+import org.apache.lucene.analysis.synonym.SynonymMap.Builder;
+import org.apache.lucene.analysis.tokenattributes.*;
+import org.apache.lucene.search.suggest.analyzing.XAnalyzingSuggester;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CharsRef;
+import org.apache.lucene.util.IntsRef;
+import org.elasticsearch.search.suggest.completion.CompletionTokenStream;
+import org.elasticsearch.search.suggest.completion.CompletionTokenStream.ByteTermAttribute;
+import org.elasticsearch.test.ESTokenStreamTestCase;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.io.StringReader;
+import java.util.Set;
+
+import static org.hamcrest.Matchers.equalTo;
+
+public class CompletionTokenStreamTests extends ESTokenStreamTestCase {
+
+    final XAnalyzingSuggester suggester = new XAnalyzingSuggester(new SimpleAnalyzer());
+
+    @Test
+    public void testSuggestTokenFilter() throws Exception {
+        Tokenizer tokenStream = new MockTokenizer(MockTokenizer.WHITESPACE, true);
+        tokenStream.setReader(new StringReader("mykeyword"));
+        BytesRef payload = new BytesRef("Surface keyword|friggin payload|10");
+        TokenStream suggestTokenStream = new ByteTermAttrToCharTermAttrFilter(new CompletionTokenStream(tokenStream, payload, new CompletionTokenStream.ToFiniteStrings() {
+            @Override
+            public Set<IntsRef> toFiniteStrings(TokenStream stream) throws IOException {
+                return suggester.toFiniteStrings(stream);
+            }
+        }));
+        assertTokenStreamContents(suggestTokenStream, new String[] {"mykeyword"}, null, null, new String[] {"Surface keyword|friggin payload|10"}, new int[] { 1 }, null, null);
+    }
+
+    @Test
+    public void testSuggestTokenFilterWithSynonym() throws Exception {
+        Builder builder = new SynonymMap.Builder(true);
+        builder.add(new CharsRef("mykeyword"), new CharsRef("mysynonym"), true);
+
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);
+        tokenizer.setReader(new StringReader("mykeyword"));
+        SynonymFilter filter = new SynonymFilter(tokenizer, builder.build(), true);
+
+        BytesRef payload = new BytesRef("Surface keyword|friggin payload|10");
+        TokenStream suggestTokenStream = new ByteTermAttrToCharTermAttrFilter(new CompletionTokenStream(filter, payload, new CompletionTokenStream.ToFiniteStrings() {
+            @Override
+            public Set<IntsRef> toFiniteStrings(TokenStream stream) throws IOException {
+                return suggester.toFiniteStrings(stream);
+            }
+        }));
+        assertTokenStreamContents(suggestTokenStream, new String[] {"mysynonym", "mykeyword"}, null, null, new String[] {"Surface keyword|friggin payload|10", "Surface keyword|friggin payload|10"}, new int[] { 2, 0 }, null, null);
+    }
+
+    @Test
+    public void testValidNumberOfExpansions() throws IOException {
+        Builder builder = new SynonymMap.Builder(true);
+        for (int i = 0; i < 256; i++) {
+            builder.add(new CharsRef("" + (i+1)), new CharsRef("" + (1000 + (i+1))), true);
+        }
+        StringBuilder valueBuilder = new StringBuilder();
+        for (int i = 0 ; i < 8 ; i++) {
+            valueBuilder.append(i+1);
+            valueBuilder.append(" ");
+        }
+        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);
+        tokenizer.setReader(new StringReader(valueBuilder.toString()));
+        SynonymFilter filter = new SynonymFilter(tokenizer, builder.build(), true);
+       
+        TokenStream suggestTokenStream = new CompletionTokenStream(filter, new BytesRef("Surface keyword|friggin payload|10"), new CompletionTokenStream.ToFiniteStrings() {
+            @Override
+            public Set<IntsRef> toFiniteStrings(TokenStream stream) throws IOException {
+                Set<IntsRef> finiteStrings = suggester.toFiniteStrings(stream);
+                return finiteStrings;
+            }
+        });
+        
+        suggestTokenStream.reset();
+        ByteTermAttribute attr = suggestTokenStream.addAttribute(ByteTermAttribute.class);
+        PositionIncrementAttribute posAttr = suggestTokenStream.addAttribute(PositionIncrementAttribute.class);
+        int maxPos = 0;
+        int count = 0;
+        while(suggestTokenStream.incrementToken()) {
+            count++;
+            assertNotNull(attr.getBytesRef());
+            assertTrue(attr.getBytesRef().length > 0);
+            maxPos += posAttr.getPositionIncrement();
+        }
+        suggestTokenStream.close();
+        assertEquals(count, 256);
+        assertEquals(count, maxPos);
+
+    }
+    
+    @Test(expected = IllegalArgumentException.class)
+    public void testInValidNumberOfExpansions() throws IOException {
+        Builder builder = new SynonymMap.Builder(true);
+        for (int i = 0; i < 256; i++) {
+            builder.add(new CharsRef("" + (i+1)), new CharsRef("" + (1000 + (i+1))), true);
+        }
+        StringBuilder valueBuilder = new StringBuilder();
+        for (int i = 0 ; i < 9 ; i++) { // 9 -> expands to 512
+            valueBuilder.append(i+1);
+            valueBuilder.append(" ");
+        }
+        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);
+        tokenizer.setReader(new StringReader(valueBuilder.toString()));
+        SynonymFilter filter = new SynonymFilter(tokenizer, builder.build(), true);
+       
+        TokenStream suggestTokenStream = new CompletionTokenStream(filter, new BytesRef("Surface keyword|friggin payload|10"), new CompletionTokenStream.ToFiniteStrings() {
+            @Override
+            public Set<IntsRef> toFiniteStrings(TokenStream stream) throws IOException {
+                Set<IntsRef> finiteStrings = suggester.toFiniteStrings(stream);
+                return finiteStrings;
+            }
+        });
+        
+        suggestTokenStream.reset();
+        suggestTokenStream.incrementToken();
+        suggestTokenStream.close();
+
+    }
+
+    @Test
+    public void testSuggestTokenFilterProperlyDelegateInputStream() throws Exception {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, true);
+        tokenizer.setReader(new StringReader("mykeyword"));
+        BytesRef payload = new BytesRef("Surface keyword|friggin payload|10");
+        TokenStream suggestTokenStream = new ByteTermAttrToCharTermAttrFilter(new CompletionTokenStream(tokenizer, payload, new CompletionTokenStream.ToFiniteStrings() {
+            @Override
+            public Set<IntsRef> toFiniteStrings(TokenStream stream) throws IOException {
+                return suggester.toFiniteStrings(stream);
+            }
+        }));
+        TermToBytesRefAttribute termAtt = suggestTokenStream.getAttribute(TermToBytesRefAttribute.class);
+        assertNotNull(termAtt.getBytesRef());
+        suggestTokenStream.reset();
+
+        while (suggestTokenStream.incrementToken()) {
+            assertThat(termAtt.getBytesRef().utf8ToString(), equalTo("mykeyword"));
+        }
+        suggestTokenStream.end();
+        suggestTokenStream.close();
+    }
+
+
+    public final static class ByteTermAttrToCharTermAttrFilter extends TokenFilter {
+        private ByteTermAttribute byteAttr = addAttribute(ByteTermAttribute.class);
+        private PayloadAttribute payload = addAttribute(PayloadAttribute.class);
+        private TypeAttribute type = addAttribute(TypeAttribute.class);
+        private CharTermAttribute charTermAttribute = addAttribute(CharTermAttribute.class);
+        protected ByteTermAttrToCharTermAttrFilter(TokenStream input) {
+            super(input);
+        }
+
+        @Override
+        public boolean incrementToken() throws IOException {
+            if (input.incrementToken()) {
+                BytesRef bytesRef = byteAttr.getBytesRef();
+                // we move them over so we can assert them more easily in the tests
+                type.setType(payload.getPayload().utf8ToString());
+                return true;
+            }
+            return false;
+        }
+
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/search/suggest/completion/AnalyzingCompletionLookupProviderV1.java b/core/src/test/java/org/elasticsearch/search/suggest/completion/AnalyzingCompletionLookupProviderV1.java
index eb78b65..398310d 100644
--- a/core/src/test/java/org/elasticsearch/search/suggest/completion/AnalyzingCompletionLookupProviderV1.java
+++ b/core/src/test/java/org/elasticsearch/search/suggest/completion/AnalyzingCompletionLookupProviderV1.java
@@ -65,7 +65,7 @@ import static org.apache.lucene.search.suggest.analyzing.XAnalyzingSuggester.HOL
 /**
  * This is an older implementation of the AnalyzingCompletionLookupProvider class
  * We use this to test for backwards compatibility in our tests, namely
- * CompletionPostingsFormatTest
+ * CompletionPostingsFormatTests
  * This ensures upgrades between versions work smoothly
  */
 public class AnalyzingCompletionLookupProviderV1 extends CompletionLookupProvider {
diff --git a/core/src/test/java/org/elasticsearch/search/suggest/completion/CompletionPostingsFormatTest.java b/core/src/test/java/org/elasticsearch/search/suggest/completion/CompletionPostingsFormatTest.java
deleted file mode 100644
index 35a222a..0000000
--- a/core/src/test/java/org/elasticsearch/search/suggest/completion/CompletionPostingsFormatTest.java
+++ /dev/null
@@ -1,544 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.suggest.completion;
-
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.lucene53.Lucene53Codec;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.IndexableField;
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.index.PostingsEnum;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.search.suggest.InputIterator;
-import org.apache.lucene.search.suggest.Lookup;
-import org.apache.lucene.search.suggest.Lookup.LookupResult;
-import org.apache.lucene.search.suggest.analyzing.AnalyzingSuggester;
-import org.apache.lucene.search.suggest.analyzing.XAnalyzingSuggester;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.LineFileDocs;
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.analysis.NamedAnalyzer;
-import org.elasticsearch.index.mapper.FieldMapper;
-import org.elasticsearch.index.mapper.MappedFieldType.Names;
-import org.elasticsearch.index.mapper.core.CompletionFieldMapper;
-import org.elasticsearch.search.suggest.SuggestUtils;
-import org.elasticsearch.search.suggest.completion.Completion090PostingsFormat.LookupFactory;
-import org.elasticsearch.search.suggest.context.ContextMapping;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.lang.reflect.Field;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Set;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.is;
-
-public class CompletionPostingsFormatTest extends ESTestCase {
-
-    Settings indexSettings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT.id).build();
-    static final CompletionFieldMapper.CompletionFieldType FIELD_TYPE = CompletionFieldMapper.Defaults.FIELD_TYPE.clone();
-    static final NamedAnalyzer analyzer = new NamedAnalyzer("foo", new StandardAnalyzer());
-    static {
-        FIELD_TYPE.setNames(new Names("foo"));
-        FIELD_TYPE.setIndexAnalyzer(analyzer);
-        FIELD_TYPE.setSearchAnalyzer(analyzer);
-        FIELD_TYPE.freeze();
-    }
-
-    @Test
-    public void testCompletionPostingsFormat() throws IOException {
-        AnalyzingCompletionLookupProviderV1 providerV1 = new AnalyzingCompletionLookupProviderV1(true, false, true, true);
-        AnalyzingCompletionLookupProvider currentProvider = new AnalyzingCompletionLookupProvider(true, false, true, true);
-        List<Completion090PostingsFormat.CompletionLookupProvider> providers = Arrays.asList(providerV1, currentProvider);
-
-        Completion090PostingsFormat.CompletionLookupProvider randomProvider = providers.get(getRandom().nextInt(providers.size()));
-        RAMDirectory dir = new RAMDirectory();
-        writeData(dir, randomProvider);
-
-        IndexInput input = dir.openInput("foo.txt", IOContext.DEFAULT);
-        LookupFactory load = currentProvider.load(input);
-        CompletionFieldMapper.CompletionFieldType fieldType = FIELD_TYPE.clone();
-        fieldType.setProvider(currentProvider);
-        Lookup lookup = load.getLookup(fieldType, new CompletionSuggestionContext(null));
-        List<LookupResult> result = lookup.lookup("ge", false, 10);
-        assertThat(result.get(0).key.toString(), equalTo("Generator - Foo Fighters"));
-        assertThat(result.get(0).payload.utf8ToString(), equalTo("id:10"));
-        dir.close();
-    }
-
-    @Test
-    public void testProviderBackwardCompatibilityForVersion1() throws IOException {
-        AnalyzingCompletionLookupProviderV1 providerV1 = new AnalyzingCompletionLookupProviderV1(true, false, true, true);
-        AnalyzingCompletionLookupProvider currentProvider = new AnalyzingCompletionLookupProvider(true, false, true, true);
-
-        RAMDirectory dir = new RAMDirectory();
-        writeData(dir, providerV1);
-
-        IndexInput input = dir.openInput("foo.txt", IOContext.DEFAULT);
-        LookupFactory load = currentProvider.load(input);
-        CompletionFieldMapper.CompletionFieldType fieldType = FIELD_TYPE.clone();
-        fieldType.setProvider(currentProvider);
-        AnalyzingCompletionLookupProvider.AnalyzingSuggestHolder analyzingSuggestHolder = load.getAnalyzingSuggestHolder(fieldType);
-        assertThat(analyzingSuggestHolder.sepLabel, is(AnalyzingCompletionLookupProviderV1.SEP_LABEL));
-        assertThat(analyzingSuggestHolder.payloadSep, is(AnalyzingCompletionLookupProviderV1.PAYLOAD_SEP));
-        assertThat(analyzingSuggestHolder.endByte, is(AnalyzingCompletionLookupProviderV1.END_BYTE));
-        dir.close();
-    }
-
-    @Test
-    public void testProviderVersion2() throws IOException {
-        AnalyzingCompletionLookupProvider currentProvider = new AnalyzingCompletionLookupProvider(true, false, true, true);
-
-        RAMDirectory dir = new RAMDirectory();
-        writeData(dir, currentProvider);
-
-        IndexInput input = dir.openInput("foo.txt", IOContext.DEFAULT);
-        LookupFactory load = currentProvider.load(input);
-        CompletionFieldMapper.CompletionFieldType fieldType = FIELD_TYPE.clone();
-        fieldType.setProvider(currentProvider);
-        AnalyzingCompletionLookupProvider.AnalyzingSuggestHolder analyzingSuggestHolder = load.getAnalyzingSuggestHolder(fieldType);
-        assertThat(analyzingSuggestHolder.sepLabel, is(XAnalyzingSuggester.SEP_LABEL));
-        assertThat(analyzingSuggestHolder.payloadSep, is(XAnalyzingSuggester.PAYLOAD_SEP));
-        assertThat(analyzingSuggestHolder.endByte, is(XAnalyzingSuggester.END_BYTE));
-        dir.close();
-    }
-
-    @Test
-    public void testDuellCompletions() throws IOException, NoSuchFieldException, SecurityException, IllegalArgumentException,
-            IllegalAccessException {
-        final boolean preserveSeparators = getRandom().nextBoolean();
-        final boolean preservePositionIncrements = getRandom().nextBoolean();
-        final boolean usePayloads = getRandom().nextBoolean();
-        final int options = preserveSeparators ? AnalyzingSuggester.PRESERVE_SEP : 0;
-
-        XAnalyzingSuggester reference = new XAnalyzingSuggester(new StandardAnalyzer(), null, new StandardAnalyzer(), 
-                options, 256, -1, preservePositionIncrements, null, false, 1, XAnalyzingSuggester.SEP_LABEL, XAnalyzingSuggester.PAYLOAD_SEP, XAnalyzingSuggester.END_BYTE, XAnalyzingSuggester.HOLE_CHARACTER);
-        LineFileDocs docs = new LineFileDocs(getRandom());
-        int num = scaledRandomIntBetween(150, 300);
-        final String[] titles = new String[num];
-        final long[] weights = new long[num];
-        for (int i = 0; i < titles.length; i++) {
-            Document nextDoc = docs.nextDoc();
-            IndexableField field = nextDoc.getField("title");
-            titles[i] = field.stringValue();
-            weights[i] = between(0, 100);
-           
-        }
-        docs.close();
-        final InputIterator primaryIter = new InputIterator() {
-            int index = 0;
-            long currentWeight = -1;
-
-            @Override
-            public BytesRef next() throws IOException {
-                if (index < titles.length) {
-                    currentWeight = weights[index];
-                    return new BytesRef(titles[index++]);
-                }
-                return null;
-            }
-
-            @Override
-            public long weight() {
-                return currentWeight;
-            }
-
-            @Override
-            public BytesRef payload() {
-                return null;
-            }
-
-            @Override
-            public boolean hasPayloads() {
-                return false;
-            }
-
-            @Override
-            public Set<BytesRef> contexts() {
-                return null;
-            }
-
-            @Override
-            public boolean hasContexts() {
-                return false;
-            }
-
-        };
-        InputIterator iter;
-        if (usePayloads) {
-            iter = new InputIterator() {
-                @Override
-                public long weight() {
-                    return primaryIter.weight();
-                }
-
-                @Override
-                public BytesRef next() throws IOException {
-                    return primaryIter.next();
-                }
-
-                @Override
-                public BytesRef payload() {
-                    return new BytesRef(Long.toString(weight()));
-                }
-
-                @Override
-                public boolean hasPayloads() {
-                    return true;
-                }
-                
-                @Override
-                public Set<BytesRef> contexts() {
-                    return null;
-                }
-
-                @Override
-                public boolean hasContexts() {
-                    return false;
-                }
-            };
-        } else {
-            iter = primaryIter;
-        }
-        reference.build(iter);
-
-        AnalyzingCompletionLookupProvider currentProvider = new AnalyzingCompletionLookupProvider(preserveSeparators, false, preservePositionIncrements, usePayloads);
-        CompletionFieldMapper.CompletionFieldType fieldType = FIELD_TYPE.clone();
-        fieldType.setProvider(currentProvider);
-        final CompletionFieldMapper mapper = new CompletionFieldMapper("foo", fieldType, Integer.MAX_VALUE, indexSettings, FieldMapper.MultiFields.empty(), null);
-        Lookup buildAnalyzingLookup = buildAnalyzingLookup(mapper, titles, titles, weights);
-        Field field = buildAnalyzingLookup.getClass().getDeclaredField("maxAnalyzedPathsForOneInput");
-        field.setAccessible(true);
-        Field refField = reference.getClass().getDeclaredField("maxAnalyzedPathsForOneInput");
-        refField.setAccessible(true);
-        assertThat(refField.get(reference), equalTo(field.get(buildAnalyzingLookup)));
-
-        for (int i = 0; i < titles.length; i++) {
-            int res = between(1, 10);
-            final StringBuilder builder = new StringBuilder();
-            SuggestUtils.analyze(analyzer.tokenStream("foo", titles[i]), new SuggestUtils.TokenConsumer() {
-                @Override
-                public void nextToken() throws IOException {
-                    if (builder.length() == 0) {
-                        builder.append(this.charTermAttr.toString());
-                    }
-                }
-            });
-            String firstTerm = builder.toString();
-            String prefix = firstTerm.isEmpty() ? "" : firstTerm.substring(0, between(1, firstTerm.length()));
-            List<LookupResult> refLookup = reference.lookup(prefix, false, res);
-            List<LookupResult> lookup = buildAnalyzingLookup.lookup(prefix, false, res);
-            assertThat(refLookup.toString(),lookup.size(), equalTo(refLookup.size()));
-            for (int j = 0; j < refLookup.size(); j++) {
-                assertThat(lookup.get(j).key, equalTo(refLookup.get(j).key));
-                assertThat("prefix: " + prefix + " " + j + " -- missmatch cost: " + lookup.get(j).key + " - " + lookup.get(j).value + " | " + refLookup.get(j).key + " - " + refLookup.get(j).value ,
-                        lookup.get(j).value, equalTo(refLookup.get(j).value));
-                assertThat(lookup.get(j).payload, equalTo(refLookup.get(j).payload));
-                if (usePayloads) {
-                    assertThat(lookup.get(j).payload.utf8ToString(),  equalTo(Long.toString(lookup.get(j).value)));    
-                }
-            }
-        }
-    }
-
-    public Lookup buildAnalyzingLookup(final CompletionFieldMapper mapper, String[] terms, String[] surfaces, long[] weights)
-            throws IOException {
-        RAMDirectory dir = new RAMDirectory();
-        Codec codec = new Lucene53Codec() {
-            public PostingsFormat getPostingsFormatForField(String field) {
-                final PostingsFormat in = super.getPostingsFormatForField(field);
-                return mapper.fieldType().postingsFormat(in);
-            }
-        };
-        IndexWriterConfig indexWriterConfig = new IndexWriterConfig(mapper.fieldType().indexAnalyzer());
-
-        indexWriterConfig.setCodec(codec);
-        IndexWriter writer = new IndexWriter(dir, indexWriterConfig);
-        for (int i = 0; i < weights.length; i++) {
-            Document doc = new Document();
-            BytesRef payload = mapper.buildPayload(new BytesRef(surfaces[i]), weights[i], new BytesRef(Long.toString(weights[i])));
-            doc.add(mapper.getCompletionField(ContextMapping.EMPTY_CONTEXT, terms[i], payload));
-            if (randomBoolean()) {
-                writer.commit();
-            }
-            writer.addDocument(doc);
-        }
-        writer.commit();
-        writer.forceMerge(1, true);
-        writer.commit();
-        DirectoryReader reader = DirectoryReader.open(writer, true);
-        assertThat(reader.leaves().size(), equalTo(1));
-        assertThat(reader.leaves().get(0).reader().numDocs(), equalTo(weights.length));
-        LeafReaderContext atomicReaderContext = reader.leaves().get(0);
-        Terms luceneTerms = atomicReaderContext.reader().terms(mapper.fieldType().names().fullName());
-        Lookup lookup = ((Completion090PostingsFormat.CompletionTerms) luceneTerms).getLookup(mapper.fieldType(), new CompletionSuggestionContext(null));
-        reader.close();
-        writer.close();
-        dir.close();
-        return lookup;
-    }
-    @Test
-    public void testNoDocs() throws IOException {
-        AnalyzingCompletionLookupProvider provider = new AnalyzingCompletionLookupProvider(true, false, true, true);
-        RAMDirectory dir = new RAMDirectory();
-        IndexOutput output = dir.createOutput("foo.txt", IOContext.DEFAULT);
-        FieldsConsumer consumer = provider.consumer(output);
-        consumer.write(new Fields() {
-            @Override
-            public Iterator<String> iterator() {
-                return Arrays.asList("foo").iterator();
-            }
-
-            @Override
-            public Terms terms(String field) throws IOException {
-                return null;
-            }
-
-            @Override
-            public int size() {
-                return 1;
-            }
-        });
-        consumer.close();
-        output.close();
-
-        IndexInput input = dir.openInput("foo.txt", IOContext.DEFAULT);
-        LookupFactory load = provider.load(input);
-        CompletionFieldMapper.CompletionFieldType fieldType = FIELD_TYPE.clone();
-        fieldType.setProvider(provider);
-        assertNull(load.getLookup(fieldType, new CompletionSuggestionContext(null)));
-        dir.close();
-    }
-
-    // TODO ADD more unittests
-    private void writeData(Directory dir, Completion090PostingsFormat.CompletionLookupProvider provider) throws IOException {
-        IndexOutput output = dir.createOutput("foo.txt", IOContext.DEFAULT);
-        FieldsConsumer consumer = provider.consumer(output);
-        final List<TermPosAndPayload> terms = new ArrayList<>();
-        terms.add(new TermPosAndPayload("foofightersgenerator", 256 - 2, provider.buildPayload(new BytesRef("Generator - Foo Fighters"), 9, new BytesRef("id:10"))));
-        terms.add(new TermPosAndPayload("generator", 256 - 1, provider.buildPayload(new BytesRef("Generator - Foo Fighters"), 9, new BytesRef("id:10"))));
-        Fields fields = new Fields() {
-            @Override
-            public Iterator<String> iterator() {
-                return Arrays.asList("foo").iterator();
-            }
-
-            @Override
-            public Terms terms(String field) throws IOException {
-                if (field.equals("foo")) {
-                    return new Terms() {
-                        @Override
-                        public TermsEnum iterator() throws IOException {
-                            final Iterator<TermPosAndPayload> iterator = terms.iterator();
-                            return new TermsEnum() {
-                                private TermPosAndPayload current = null;
-                                @Override
-                                public SeekStatus seekCeil(BytesRef text) throws IOException {
-                                    throw new UnsupportedOperationException();
-                                }
-
-                                @Override
-                                public void seekExact(long ord) throws IOException {
-                                    throw new UnsupportedOperationException();
-                                }
-
-                                @Override
-                                public BytesRef term() throws IOException {
-                                    return current == null ? null : current.term;
-                                }
-
-                                @Override
-                                public long ord() throws IOException {
-                                    throw new UnsupportedOperationException();
-                                }
-
-                                @Override
-                                public int docFreq() throws IOException {
-                                    return current == null ? 0 : 1;
-                                }
-
-                                @Override
-                                public long totalTermFreq() throws IOException {
-                                    throw new UnsupportedOperationException();
-                                }
-
-                                @Override
-                                public PostingsEnum postings(PostingsEnum reuse, int flags) throws IOException {
-                                    final TermPosAndPayload data = current;
-                                    return new PostingsEnum() {
-                                        boolean done = false;
-                                        @Override
-                                        public int nextPosition() throws IOException {
-                                            return data.pos;
-                                        }
-
-                                        @Override
-                                        public int startOffset() throws IOException {
-                                            return 0;
-                                        }
-
-                                        @Override
-                                        public int endOffset() throws IOException {
-                                            return 0;
-                                        }
-
-                                        @Override
-                                        public BytesRef getPayload() throws IOException {
-                                            return data.payload;
-                                        }
-
-                                        @Override
-                                        public int freq() throws IOException {
-                                            return 1;
-                                        }
-
-                                        @Override
-                                        public int docID() {
-                                            if (done) {
-                                                return NO_MORE_DOCS;
-                                            }
-                                            return 0;
-                                        }
-
-                                        @Override
-                                        public int nextDoc() throws IOException {
-                                            if (done) {
-                                                return NO_MORE_DOCS;
-                                            }
-                                            done = true;
-                                            return 0;
-                                        }
-
-                                        @Override
-                                        public int advance(int target) throws IOException {
-                                            if (done) {
-                                                return NO_MORE_DOCS;
-                                            }
-                                            done = true;
-                                            return 0;
-                                        }
-
-                                        @Override
-                                        public long cost() {
-                                            return 0;
-                                        }
-                                    };
-                                }
-
-                                @Override
-                                public BytesRef next() throws IOException {
-                                    if (iterator.hasNext()) {
-                                        current = iterator.next();
-                                        return current.term;
-                                    }
-                                    current = null;
-                                    return null;
-                                }
-                            };
-                        }
-
-                        @Override
-                        public long size() throws IOException {
-                            throw new UnsupportedOperationException();
-                        }
-
-                        @Override
-                        public long getSumTotalTermFreq() throws IOException {
-                            throw new UnsupportedOperationException();
-                        }
-
-                        @Override
-                        public long getSumDocFreq() throws IOException {
-                            throw new UnsupportedOperationException();
-                        }
-
-                        @Override
-                        public int getDocCount() throws IOException {
-                            throw new UnsupportedOperationException();
-                        }
-
-                        @Override
-                        public boolean hasFreqs() {
-                            throw new UnsupportedOperationException();
-                        }
-
-                        @Override
-                        public boolean hasOffsets() {
-                            throw new UnsupportedOperationException();
-                        }
-
-                        @Override
-                        public boolean hasPositions() {
-                            throw new UnsupportedOperationException();
-                        }
-
-                        @Override
-                        public boolean hasPayloads() {
-                            throw new UnsupportedOperationException();
-                        }
-                    };
-                }
-                return null;
-            }
-
-            @Override
-            public int size() {
-                return 0;
-            }
-        };
-        consumer.write(fields);
-        consumer.close();
-        output.close();
-
-    }
-
-    private static class TermPosAndPayload {
-        final BytesRef term;
-        final int pos;
-        final BytesRef payload;
-
-
-        private TermPosAndPayload(String term, int pos, BytesRef payload) {
-            this.term = new BytesRef(term);
-            this.pos = pos;
-            this.payload = payload;
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/suggest/completion/CompletionPostingsFormatTests.java b/core/src/test/java/org/elasticsearch/search/suggest/completion/CompletionPostingsFormatTests.java
new file mode 100644
index 0000000..672f8bb
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/search/suggest/completion/CompletionPostingsFormatTests.java
@@ -0,0 +1,544 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.suggest.completion;
+
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.lucene53.Lucene53Codec;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.IndexableField;
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.index.PostingsEnum;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.search.suggest.InputIterator;
+import org.apache.lucene.search.suggest.Lookup;
+import org.apache.lucene.search.suggest.Lookup.LookupResult;
+import org.apache.lucene.search.suggest.analyzing.AnalyzingSuggester;
+import org.apache.lucene.search.suggest.analyzing.XAnalyzingSuggester;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LineFileDocs;
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.analysis.NamedAnalyzer;
+import org.elasticsearch.index.mapper.FieldMapper;
+import org.elasticsearch.index.mapper.MappedFieldType.Names;
+import org.elasticsearch.index.mapper.core.CompletionFieldMapper;
+import org.elasticsearch.search.suggest.SuggestUtils;
+import org.elasticsearch.search.suggest.completion.Completion090PostingsFormat.LookupFactory;
+import org.elasticsearch.search.suggest.context.ContextMapping;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.lang.reflect.Field;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Set;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.is;
+
+public class CompletionPostingsFormatTests extends ESTestCase {
+
+    Settings indexSettings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT.id).build();
+    static final CompletionFieldMapper.CompletionFieldType FIELD_TYPE = CompletionFieldMapper.Defaults.FIELD_TYPE.clone();
+    static final NamedAnalyzer analyzer = new NamedAnalyzer("foo", new StandardAnalyzer());
+    static {
+        FIELD_TYPE.setNames(new Names("foo"));
+        FIELD_TYPE.setIndexAnalyzer(analyzer);
+        FIELD_TYPE.setSearchAnalyzer(analyzer);
+        FIELD_TYPE.freeze();
+    }
+
+    @Test
+    public void testCompletionPostingsFormat() throws IOException {
+        AnalyzingCompletionLookupProviderV1 providerV1 = new AnalyzingCompletionLookupProviderV1(true, false, true, true);
+        AnalyzingCompletionLookupProvider currentProvider = new AnalyzingCompletionLookupProvider(true, false, true, true);
+        List<Completion090PostingsFormat.CompletionLookupProvider> providers = Arrays.asList(providerV1, currentProvider);
+
+        Completion090PostingsFormat.CompletionLookupProvider randomProvider = providers.get(getRandom().nextInt(providers.size()));
+        RAMDirectory dir = new RAMDirectory();
+        writeData(dir, randomProvider);
+
+        IndexInput input = dir.openInput("foo.txt", IOContext.DEFAULT);
+        LookupFactory load = currentProvider.load(input);
+        CompletionFieldMapper.CompletionFieldType fieldType = FIELD_TYPE.clone();
+        fieldType.setProvider(currentProvider);
+        Lookup lookup = load.getLookup(fieldType, new CompletionSuggestionContext(null));
+        List<LookupResult> result = lookup.lookup("ge", false, 10);
+        assertThat(result.get(0).key.toString(), equalTo("Generator - Foo Fighters"));
+        assertThat(result.get(0).payload.utf8ToString(), equalTo("id:10"));
+        dir.close();
+    }
+
+    @Test
+    public void testProviderBackwardCompatibilityForVersion1() throws IOException {
+        AnalyzingCompletionLookupProviderV1 providerV1 = new AnalyzingCompletionLookupProviderV1(true, false, true, true);
+        AnalyzingCompletionLookupProvider currentProvider = new AnalyzingCompletionLookupProvider(true, false, true, true);
+
+        RAMDirectory dir = new RAMDirectory();
+        writeData(dir, providerV1);
+
+        IndexInput input = dir.openInput("foo.txt", IOContext.DEFAULT);
+        LookupFactory load = currentProvider.load(input);
+        CompletionFieldMapper.CompletionFieldType fieldType = FIELD_TYPE.clone();
+        fieldType.setProvider(currentProvider);
+        AnalyzingCompletionLookupProvider.AnalyzingSuggestHolder analyzingSuggestHolder = load.getAnalyzingSuggestHolder(fieldType);
+        assertThat(analyzingSuggestHolder.sepLabel, is(AnalyzingCompletionLookupProviderV1.SEP_LABEL));
+        assertThat(analyzingSuggestHolder.payloadSep, is(AnalyzingCompletionLookupProviderV1.PAYLOAD_SEP));
+        assertThat(analyzingSuggestHolder.endByte, is(AnalyzingCompletionLookupProviderV1.END_BYTE));
+        dir.close();
+    }
+
+    @Test
+    public void testProviderVersion2() throws IOException {
+        AnalyzingCompletionLookupProvider currentProvider = new AnalyzingCompletionLookupProvider(true, false, true, true);
+
+        RAMDirectory dir = new RAMDirectory();
+        writeData(dir, currentProvider);
+
+        IndexInput input = dir.openInput("foo.txt", IOContext.DEFAULT);
+        LookupFactory load = currentProvider.load(input);
+        CompletionFieldMapper.CompletionFieldType fieldType = FIELD_TYPE.clone();
+        fieldType.setProvider(currentProvider);
+        AnalyzingCompletionLookupProvider.AnalyzingSuggestHolder analyzingSuggestHolder = load.getAnalyzingSuggestHolder(fieldType);
+        assertThat(analyzingSuggestHolder.sepLabel, is(XAnalyzingSuggester.SEP_LABEL));
+        assertThat(analyzingSuggestHolder.payloadSep, is(XAnalyzingSuggester.PAYLOAD_SEP));
+        assertThat(analyzingSuggestHolder.endByte, is(XAnalyzingSuggester.END_BYTE));
+        dir.close();
+    }
+
+    @Test
+    public void testDuellCompletions() throws IOException, NoSuchFieldException, SecurityException, IllegalArgumentException,
+            IllegalAccessException {
+        final boolean preserveSeparators = getRandom().nextBoolean();
+        final boolean preservePositionIncrements = getRandom().nextBoolean();
+        final boolean usePayloads = getRandom().nextBoolean();
+        final int options = preserveSeparators ? AnalyzingSuggester.PRESERVE_SEP : 0;
+
+        XAnalyzingSuggester reference = new XAnalyzingSuggester(new StandardAnalyzer(), null, new StandardAnalyzer(), 
+                options, 256, -1, preservePositionIncrements, null, false, 1, XAnalyzingSuggester.SEP_LABEL, XAnalyzingSuggester.PAYLOAD_SEP, XAnalyzingSuggester.END_BYTE, XAnalyzingSuggester.HOLE_CHARACTER);
+        LineFileDocs docs = new LineFileDocs(getRandom());
+        int num = scaledRandomIntBetween(150, 300);
+        final String[] titles = new String[num];
+        final long[] weights = new long[num];
+        for (int i = 0; i < titles.length; i++) {
+            Document nextDoc = docs.nextDoc();
+            IndexableField field = nextDoc.getField("title");
+            titles[i] = field.stringValue();
+            weights[i] = between(0, 100);
+           
+        }
+        docs.close();
+        final InputIterator primaryIter = new InputIterator() {
+            int index = 0;
+            long currentWeight = -1;
+
+            @Override
+            public BytesRef next() throws IOException {
+                if (index < titles.length) {
+                    currentWeight = weights[index];
+                    return new BytesRef(titles[index++]);
+                }
+                return null;
+            }
+
+            @Override
+            public long weight() {
+                return currentWeight;
+            }
+
+            @Override
+            public BytesRef payload() {
+                return null;
+            }
+
+            @Override
+            public boolean hasPayloads() {
+                return false;
+            }
+
+            @Override
+            public Set<BytesRef> contexts() {
+                return null;
+            }
+
+            @Override
+            public boolean hasContexts() {
+                return false;
+            }
+
+        };
+        InputIterator iter;
+        if (usePayloads) {
+            iter = new InputIterator() {
+                @Override
+                public long weight() {
+                    return primaryIter.weight();
+                }
+
+                @Override
+                public BytesRef next() throws IOException {
+                    return primaryIter.next();
+                }
+
+                @Override
+                public BytesRef payload() {
+                    return new BytesRef(Long.toString(weight()));
+                }
+
+                @Override
+                public boolean hasPayloads() {
+                    return true;
+                }
+                
+                @Override
+                public Set<BytesRef> contexts() {
+                    return null;
+                }
+
+                @Override
+                public boolean hasContexts() {
+                    return false;
+                }
+            };
+        } else {
+            iter = primaryIter;
+        }
+        reference.build(iter);
+
+        AnalyzingCompletionLookupProvider currentProvider = new AnalyzingCompletionLookupProvider(preserveSeparators, false, preservePositionIncrements, usePayloads);
+        CompletionFieldMapper.CompletionFieldType fieldType = FIELD_TYPE.clone();
+        fieldType.setProvider(currentProvider);
+        final CompletionFieldMapper mapper = new CompletionFieldMapper("foo", fieldType, Integer.MAX_VALUE, indexSettings, FieldMapper.MultiFields.empty(), null);
+        Lookup buildAnalyzingLookup = buildAnalyzingLookup(mapper, titles, titles, weights);
+        Field field = buildAnalyzingLookup.getClass().getDeclaredField("maxAnalyzedPathsForOneInput");
+        field.setAccessible(true);
+        Field refField = reference.getClass().getDeclaredField("maxAnalyzedPathsForOneInput");
+        refField.setAccessible(true);
+        assertThat(refField.get(reference), equalTo(field.get(buildAnalyzingLookup)));
+
+        for (int i = 0; i < titles.length; i++) {
+            int res = between(1, 10);
+            final StringBuilder builder = new StringBuilder();
+            SuggestUtils.analyze(analyzer.tokenStream("foo", titles[i]), new SuggestUtils.TokenConsumer() {
+                @Override
+                public void nextToken() throws IOException {
+                    if (builder.length() == 0) {
+                        builder.append(this.charTermAttr.toString());
+                    }
+                }
+            });
+            String firstTerm = builder.toString();
+            String prefix = firstTerm.isEmpty() ? "" : firstTerm.substring(0, between(1, firstTerm.length()));
+            List<LookupResult> refLookup = reference.lookup(prefix, false, res);
+            List<LookupResult> lookup = buildAnalyzingLookup.lookup(prefix, false, res);
+            assertThat(refLookup.toString(),lookup.size(), equalTo(refLookup.size()));
+            for (int j = 0; j < refLookup.size(); j++) {
+                assertThat(lookup.get(j).key, equalTo(refLookup.get(j).key));
+                assertThat("prefix: " + prefix + " " + j + " -- missmatch cost: " + lookup.get(j).key + " - " + lookup.get(j).value + " | " + refLookup.get(j).key + " - " + refLookup.get(j).value ,
+                        lookup.get(j).value, equalTo(refLookup.get(j).value));
+                assertThat(lookup.get(j).payload, equalTo(refLookup.get(j).payload));
+                if (usePayloads) {
+                    assertThat(lookup.get(j).payload.utf8ToString(),  equalTo(Long.toString(lookup.get(j).value)));    
+                }
+            }
+        }
+    }
+
+    public Lookup buildAnalyzingLookup(final CompletionFieldMapper mapper, String[] terms, String[] surfaces, long[] weights)
+            throws IOException {
+        RAMDirectory dir = new RAMDirectory();
+        Codec codec = new Lucene53Codec() {
+            public PostingsFormat getPostingsFormatForField(String field) {
+                final PostingsFormat in = super.getPostingsFormatForField(field);
+                return mapper.fieldType().postingsFormat(in);
+            }
+        };
+        IndexWriterConfig indexWriterConfig = new IndexWriterConfig(mapper.fieldType().indexAnalyzer());
+
+        indexWriterConfig.setCodec(codec);
+        IndexWriter writer = new IndexWriter(dir, indexWriterConfig);
+        for (int i = 0; i < weights.length; i++) {
+            Document doc = new Document();
+            BytesRef payload = mapper.buildPayload(new BytesRef(surfaces[i]), weights[i], new BytesRef(Long.toString(weights[i])));
+            doc.add(mapper.getCompletionField(ContextMapping.EMPTY_CONTEXT, terms[i], payload));
+            if (randomBoolean()) {
+                writer.commit();
+            }
+            writer.addDocument(doc);
+        }
+        writer.commit();
+        writer.forceMerge(1, true);
+        writer.commit();
+        DirectoryReader reader = DirectoryReader.open(writer, true);
+        assertThat(reader.leaves().size(), equalTo(1));
+        assertThat(reader.leaves().get(0).reader().numDocs(), equalTo(weights.length));
+        LeafReaderContext atomicReaderContext = reader.leaves().get(0);
+        Terms luceneTerms = atomicReaderContext.reader().terms(mapper.fieldType().names().fullName());
+        Lookup lookup = ((Completion090PostingsFormat.CompletionTerms) luceneTerms).getLookup(mapper.fieldType(), new CompletionSuggestionContext(null));
+        reader.close();
+        writer.close();
+        dir.close();
+        return lookup;
+    }
+    @Test
+    public void testNoDocs() throws IOException {
+        AnalyzingCompletionLookupProvider provider = new AnalyzingCompletionLookupProvider(true, false, true, true);
+        RAMDirectory dir = new RAMDirectory();
+        IndexOutput output = dir.createOutput("foo.txt", IOContext.DEFAULT);
+        FieldsConsumer consumer = provider.consumer(output);
+        consumer.write(new Fields() {
+            @Override
+            public Iterator<String> iterator() {
+                return Arrays.asList("foo").iterator();
+            }
+
+            @Override
+            public Terms terms(String field) throws IOException {
+                return null;
+            }
+
+            @Override
+            public int size() {
+                return 1;
+            }
+        });
+        consumer.close();
+        output.close();
+
+        IndexInput input = dir.openInput("foo.txt", IOContext.DEFAULT);
+        LookupFactory load = provider.load(input);
+        CompletionFieldMapper.CompletionFieldType fieldType = FIELD_TYPE.clone();
+        fieldType.setProvider(provider);
+        assertNull(load.getLookup(fieldType, new CompletionSuggestionContext(null)));
+        dir.close();
+    }
+
+    // TODO ADD more unittests
+    private void writeData(Directory dir, Completion090PostingsFormat.CompletionLookupProvider provider) throws IOException {
+        IndexOutput output = dir.createOutput("foo.txt", IOContext.DEFAULT);
+        FieldsConsumer consumer = provider.consumer(output);
+        final List<TermPosAndPayload> terms = new ArrayList<>();
+        terms.add(new TermPosAndPayload("foofightersgenerator", 256 - 2, provider.buildPayload(new BytesRef("Generator - Foo Fighters"), 9, new BytesRef("id:10"))));
+        terms.add(new TermPosAndPayload("generator", 256 - 1, provider.buildPayload(new BytesRef("Generator - Foo Fighters"), 9, new BytesRef("id:10"))));
+        Fields fields = new Fields() {
+            @Override
+            public Iterator<String> iterator() {
+                return Arrays.asList("foo").iterator();
+            }
+
+            @Override
+            public Terms terms(String field) throws IOException {
+                if (field.equals("foo")) {
+                    return new Terms() {
+                        @Override
+                        public TermsEnum iterator() throws IOException {
+                            final Iterator<TermPosAndPayload> iterator = terms.iterator();
+                            return new TermsEnum() {
+                                private TermPosAndPayload current = null;
+                                @Override
+                                public SeekStatus seekCeil(BytesRef text) throws IOException {
+                                    throw new UnsupportedOperationException();
+                                }
+
+                                @Override
+                                public void seekExact(long ord) throws IOException {
+                                    throw new UnsupportedOperationException();
+                                }
+
+                                @Override
+                                public BytesRef term() throws IOException {
+                                    return current == null ? null : current.term;
+                                }
+
+                                @Override
+                                public long ord() throws IOException {
+                                    throw new UnsupportedOperationException();
+                                }
+
+                                @Override
+                                public int docFreq() throws IOException {
+                                    return current == null ? 0 : 1;
+                                }
+
+                                @Override
+                                public long totalTermFreq() throws IOException {
+                                    throw new UnsupportedOperationException();
+                                }
+
+                                @Override
+                                public PostingsEnum postings(PostingsEnum reuse, int flags) throws IOException {
+                                    final TermPosAndPayload data = current;
+                                    return new PostingsEnum() {
+                                        boolean done = false;
+                                        @Override
+                                        public int nextPosition() throws IOException {
+                                            return data.pos;
+                                        }
+
+                                        @Override
+                                        public int startOffset() throws IOException {
+                                            return 0;
+                                        }
+
+                                        @Override
+                                        public int endOffset() throws IOException {
+                                            return 0;
+                                        }
+
+                                        @Override
+                                        public BytesRef getPayload() throws IOException {
+                                            return data.payload;
+                                        }
+
+                                        @Override
+                                        public int freq() throws IOException {
+                                            return 1;
+                                        }
+
+                                        @Override
+                                        public int docID() {
+                                            if (done) {
+                                                return NO_MORE_DOCS;
+                                            }
+                                            return 0;
+                                        }
+
+                                        @Override
+                                        public int nextDoc() throws IOException {
+                                            if (done) {
+                                                return NO_MORE_DOCS;
+                                            }
+                                            done = true;
+                                            return 0;
+                                        }
+
+                                        @Override
+                                        public int advance(int target) throws IOException {
+                                            if (done) {
+                                                return NO_MORE_DOCS;
+                                            }
+                                            done = true;
+                                            return 0;
+                                        }
+
+                                        @Override
+                                        public long cost() {
+                                            return 0;
+                                        }
+                                    };
+                                }
+
+                                @Override
+                                public BytesRef next() throws IOException {
+                                    if (iterator.hasNext()) {
+                                        current = iterator.next();
+                                        return current.term;
+                                    }
+                                    current = null;
+                                    return null;
+                                }
+                            };
+                        }
+
+                        @Override
+                        public long size() throws IOException {
+                            throw new UnsupportedOperationException();
+                        }
+
+                        @Override
+                        public long getSumTotalTermFreq() throws IOException {
+                            throw new UnsupportedOperationException();
+                        }
+
+                        @Override
+                        public long getSumDocFreq() throws IOException {
+                            throw new UnsupportedOperationException();
+                        }
+
+                        @Override
+                        public int getDocCount() throws IOException {
+                            throw new UnsupportedOperationException();
+                        }
+
+                        @Override
+                        public boolean hasFreqs() {
+                            throw new UnsupportedOperationException();
+                        }
+
+                        @Override
+                        public boolean hasOffsets() {
+                            throw new UnsupportedOperationException();
+                        }
+
+                        @Override
+                        public boolean hasPositions() {
+                            throw new UnsupportedOperationException();
+                        }
+
+                        @Override
+                        public boolean hasPayloads() {
+                            throw new UnsupportedOperationException();
+                        }
+                    };
+                }
+                return null;
+            }
+
+            @Override
+            public int size() {
+                return 0;
+            }
+        };
+        consumer.write(fields);
+        consumer.close();
+        output.close();
+
+    }
+
+    private static class TermPosAndPayload {
+        final BytesRef term;
+        final int pos;
+        final BytesRef payload;
+
+
+        private TermPosAndPayload(String term, int pos, BytesRef payload) {
+            this.term = new BytesRef(term);
+            this.pos = pos;
+            this.payload = payload;
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/search/suggest/context/GeoLocationContextMappingTest.java b/core/src/test/java/org/elasticsearch/search/suggest/context/GeoLocationContextMappingTest.java
deleted file mode 100644
index f1c1761..0000000
--- a/core/src/test/java/org/elasticsearch/search/suggest/context/GeoLocationContextMappingTest.java
+++ /dev/null
@@ -1,199 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.search.suggest.context;
-
-import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.common.geo.GeoHashUtils;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentHelper;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.suggest.context.ContextMapping.ContextConfig;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashMap;
-
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-
-/**
- *
- */
-public class GeoLocationContextMappingTest extends ESTestCase {
-
-    @Test
-    public void testThatParsingGeoPointsWorksWithCoercion() throws Exception {
-        XContentBuilder builder = jsonBuilder().startObject().field("lat", "52").field("lon", "4").endObject();
-        XContentParser parser = XContentHelper.createParser(builder.bytes());
-        parser.nextToken();
-
-        HashMap<String, Object> config = new HashMap<>();
-        config.put("precision", 12);
-        GeolocationContextMapping mapping = GeolocationContextMapping.load("foo", config);
-        mapping.parseQuery("foo", parser);
-    }
-    
-
-    @Test
-    public void testUseWithDefaultGeoHash() throws Exception {
-        XContentBuilder builder = jsonBuilder().startObject().field("lat", 52d).field("lon", 4d).endObject();
-        XContentParser parser = XContentHelper.createParser(builder.bytes());
-        parser.nextToken();
-
-        String geohash = GeoHashUtils.encode(randomIntBetween(-90, +90), randomIntBetween(-180, +180));
-        HashMap<String, Object> config = new HashMap<>();
-        config.put("precision", 12);
-        config.put("default", geohash);
-        GeolocationContextMapping mapping = GeolocationContextMapping.load("foo", config);
-        mapping.parseQuery("foo", parser);
-    }    
-    
-    @Test
-    public void testUseWithDefaultLatLon() throws Exception {
-        XContentBuilder builder = jsonBuilder().startObject().field("lat", 52d).field("lon", 4d).endObject();
-        XContentParser parser = XContentHelper.createParser(builder.bytes());
-        parser.nextToken();
-
-        HashMap<String, Object> config = new HashMap<>();
-        config.put("precision", 12);
-        HashMap<String, Double> pointAsMap = new HashMap<>();
-        pointAsMap.put("lat", 51d);
-        pointAsMap.put("lon", 0d);
-        config.put("default", pointAsMap);
-        GeolocationContextMapping mapping = GeolocationContextMapping.load("foo", config);
-        mapping.parseQuery("foo", parser);
-    } 
-    
-    @Test
-    public void testUseWithDefaultBadLatLon() throws Exception {
-        XContentBuilder builder = jsonBuilder().startObject().field("lat", 52d).field("lon", 4d).endObject();
-        XContentParser parser = XContentHelper.createParser(builder.bytes());
-        parser.nextToken();
-
-        HashMap<String, Object> config = new HashMap<>();
-        config.put("precision", 12);
-        HashMap<String, Double> pointAsMap = new HashMap<>();
-        pointAsMap.put("latitude", 51d); // invalid field names
-        pointAsMap.put("longitude", 0d); // invalid field names
-        config.put("default", pointAsMap);
-        ElasticsearchParseException expected = null;
-        try {
-            GeolocationContextMapping mapping = GeolocationContextMapping.load("foo", config);
-            mapping.parseQuery("foo", parser);
-
-        } catch (ElasticsearchParseException e) {
-            expected = e;
-        }
-        assertNotNull(expected);
-    }  
-    
-    @Test
-    public void testUseWithMultiplePrecisions() throws Exception {
-        XContentBuilder builder = jsonBuilder().startObject().field("lat", 52d).field("lon", 4d).endObject();
-        XContentParser parser = XContentHelper.createParser(builder.bytes());
-        parser.nextToken();
-
-        HashMap<String, Object> config = new HashMap<>();
-        int numElements = randomIntBetween(1, 12);
-        ArrayList<Integer> precisions = new ArrayList<>();
-        for (int i = 0; i < numElements; i++) {
-            precisions.add(randomIntBetween(1, 12));
-        }
-        config.put("precision", precisions);
-        GeolocationContextMapping mapping = GeolocationContextMapping.load("foo", config);
-        mapping.parseQuery("foo", parser);
-    }
-    
-    @Test
-    public void testHashcode() throws Exception {
-        HashMap<String, Object> config = new HashMap<>();
-        if (randomBoolean()) {
-            config.put("precision", Arrays.asList(1, 2, 3, 4));
-        } else {
-            config.put("precision", randomIntBetween(1, 12));
-        }
-        if (randomBoolean()) {
-            HashMap<String, Double> pointAsMap = new HashMap<>();
-            pointAsMap.put("lat", 51d);
-            pointAsMap.put("lon", 0d);
-            config.put("default", pointAsMap);
-        }
-        HashMap<String, Object> config2 = new HashMap<>(config);
-        GeolocationContextMapping mapping = GeolocationContextMapping.load("foo", config);
-        GeolocationContextMapping mapping2 = GeolocationContextMapping.load("foo", config2);
-
-        assertEquals(mapping, mapping2);
-        assertEquals(mapping.hashCode(), mapping2.hashCode());
-    }
-
-    @Test
-    public void testUseWithBadGeoContext() throws Exception {
-        double lon = 4d;
-        String badLat = "W";
-        XContentBuilder builder = jsonBuilder().startObject().startArray("location").value(4d).value(badLat).endArray().endObject();
-        XContentParser parser = XContentHelper.createParser(builder.bytes());
-        parser.nextToken(); // start of object
-        parser.nextToken(); // "location" field name
-        parser.nextToken(); // array
-
-        HashMap<String, Object> config = new HashMap<>();
-        config.put("precision", randomIntBetween(1, 12));
-        GeolocationContextMapping mapping = GeolocationContextMapping.load("foo", config);
-        ElasticsearchParseException expected = null;
-        try {
-            ContextConfig geoconfig = mapping.parseContext(null, parser);
-        } catch (ElasticsearchParseException e) {
-            expected = e;
-        }
-        assertNotNull(expected);
-    }
-
-    @Test
-    public void testUseWithLonLatGeoContext() throws Exception {
-        double lon = 4d;
-        double lat = 52d;
-        XContentBuilder builder = jsonBuilder().startObject().startArray("location").value(lon).value(lat).endArray().endObject();
-        XContentParser parser = XContentHelper.createParser(builder.bytes());
-        parser.nextToken(); // start of object
-        parser.nextToken(); // "location" field name
-        parser.nextToken(); // array
-
-        HashMap<String, Object> config = new HashMap<>();
-        config.put("precision", randomIntBetween(1, 12));
-        GeolocationContextMapping mapping = GeolocationContextMapping.load("foo", config);
-        mapping.parseContext(null, parser);
-    }
-
-    public void testUseWithMultiGeoHashGeoContext() throws Exception {
-        String geohash1 = GeoHashUtils.encode(randomIntBetween(-90, +90), randomIntBetween(-180, +180));
-        String geohash2 = GeoHashUtils.encode(randomIntBetween(-90, +90), randomIntBetween(-180, +180));
-        XContentBuilder builder = jsonBuilder().startObject().startArray("location").value(geohash1).value(geohash2).endArray().endObject();
-        XContentParser parser = XContentHelper.createParser(builder.bytes());
-        parser.nextToken(); // start of object
-        parser.nextToken(); // "location" field name
-        parser.nextToken(); // array
-
-        HashMap<String, Object> config = new HashMap<>();
-        config.put("precision", randomIntBetween(1, 12));
-        GeolocationContextMapping mapping = GeolocationContextMapping.load("foo", config);
-        ContextConfig parsedContext = mapping.parseContext(null, parser);
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/suggest/context/GeoLocationContextMappingTests.java b/core/src/test/java/org/elasticsearch/search/suggest/context/GeoLocationContextMappingTests.java
new file mode 100644
index 0000000..60d2fd6
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/search/suggest/context/GeoLocationContextMappingTests.java
@@ -0,0 +1,199 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.suggest.context;
+
+import org.elasticsearch.ElasticsearchParseException;
+import org.elasticsearch.common.geo.GeoHashUtils;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentHelper;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.suggest.context.ContextMapping.ContextConfig;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+
+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
+
+/**
+ *
+ */
+public class GeoLocationContextMappingTests extends ESTestCase {
+
+    @Test
+    public void testThatParsingGeoPointsWorksWithCoercion() throws Exception {
+        XContentBuilder builder = jsonBuilder().startObject().field("lat", "52").field("lon", "4").endObject();
+        XContentParser parser = XContentHelper.createParser(builder.bytes());
+        parser.nextToken();
+
+        HashMap<String, Object> config = new HashMap<>();
+        config.put("precision", 12);
+        GeolocationContextMapping mapping = GeolocationContextMapping.load("foo", config);
+        mapping.parseQuery("foo", parser);
+    }
+    
+
+    @Test
+    public void testUseWithDefaultGeoHash() throws Exception {
+        XContentBuilder builder = jsonBuilder().startObject().field("lat", 52d).field("lon", 4d).endObject();
+        XContentParser parser = XContentHelper.createParser(builder.bytes());
+        parser.nextToken();
+
+        String geohash = GeoHashUtils.encode(randomIntBetween(-90, +90), randomIntBetween(-180, +180));
+        HashMap<String, Object> config = new HashMap<>();
+        config.put("precision", 12);
+        config.put("default", geohash);
+        GeolocationContextMapping mapping = GeolocationContextMapping.load("foo", config);
+        mapping.parseQuery("foo", parser);
+    }    
+    
+    @Test
+    public void testUseWithDefaultLatLon() throws Exception {
+        XContentBuilder builder = jsonBuilder().startObject().field("lat", 52d).field("lon", 4d).endObject();
+        XContentParser parser = XContentHelper.createParser(builder.bytes());
+        parser.nextToken();
+
+        HashMap<String, Object> config = new HashMap<>();
+        config.put("precision", 12);
+        HashMap<String, Double> pointAsMap = new HashMap<>();
+        pointAsMap.put("lat", 51d);
+        pointAsMap.put("lon", 0d);
+        config.put("default", pointAsMap);
+        GeolocationContextMapping mapping = GeolocationContextMapping.load("foo", config);
+        mapping.parseQuery("foo", parser);
+    } 
+    
+    @Test
+    public void testUseWithDefaultBadLatLon() throws Exception {
+        XContentBuilder builder = jsonBuilder().startObject().field("lat", 52d).field("lon", 4d).endObject();
+        XContentParser parser = XContentHelper.createParser(builder.bytes());
+        parser.nextToken();
+
+        HashMap<String, Object> config = new HashMap<>();
+        config.put("precision", 12);
+        HashMap<String, Double> pointAsMap = new HashMap<>();
+        pointAsMap.put("latitude", 51d); // invalid field names
+        pointAsMap.put("longitude", 0d); // invalid field names
+        config.put("default", pointAsMap);
+        ElasticsearchParseException expected = null;
+        try {
+            GeolocationContextMapping mapping = GeolocationContextMapping.load("foo", config);
+            mapping.parseQuery("foo", parser);
+
+        } catch (ElasticsearchParseException e) {
+            expected = e;
+        }
+        assertNotNull(expected);
+    }  
+    
+    @Test
+    public void testUseWithMultiplePrecisions() throws Exception {
+        XContentBuilder builder = jsonBuilder().startObject().field("lat", 52d).field("lon", 4d).endObject();
+        XContentParser parser = XContentHelper.createParser(builder.bytes());
+        parser.nextToken();
+
+        HashMap<String, Object> config = new HashMap<>();
+        int numElements = randomIntBetween(1, 12);
+        ArrayList<Integer> precisions = new ArrayList<>();
+        for (int i = 0; i < numElements; i++) {
+            precisions.add(randomIntBetween(1, 12));
+        }
+        config.put("precision", precisions);
+        GeolocationContextMapping mapping = GeolocationContextMapping.load("foo", config);
+        mapping.parseQuery("foo", parser);
+    }
+    
+    @Test
+    public void testHashcode() throws Exception {
+        HashMap<String, Object> config = new HashMap<>();
+        if (randomBoolean()) {
+            config.put("precision", Arrays.asList(1, 2, 3, 4));
+        } else {
+            config.put("precision", randomIntBetween(1, 12));
+        }
+        if (randomBoolean()) {
+            HashMap<String, Double> pointAsMap = new HashMap<>();
+            pointAsMap.put("lat", 51d);
+            pointAsMap.put("lon", 0d);
+            config.put("default", pointAsMap);
+        }
+        HashMap<String, Object> config2 = new HashMap<>(config);
+        GeolocationContextMapping mapping = GeolocationContextMapping.load("foo", config);
+        GeolocationContextMapping mapping2 = GeolocationContextMapping.load("foo", config2);
+
+        assertEquals(mapping, mapping2);
+        assertEquals(mapping.hashCode(), mapping2.hashCode());
+    }
+
+    @Test
+    public void testUseWithBadGeoContext() throws Exception {
+        double lon = 4d;
+        String badLat = "W";
+        XContentBuilder builder = jsonBuilder().startObject().startArray("location").value(4d).value(badLat).endArray().endObject();
+        XContentParser parser = XContentHelper.createParser(builder.bytes());
+        parser.nextToken(); // start of object
+        parser.nextToken(); // "location" field name
+        parser.nextToken(); // array
+
+        HashMap<String, Object> config = new HashMap<>();
+        config.put("precision", randomIntBetween(1, 12));
+        GeolocationContextMapping mapping = GeolocationContextMapping.load("foo", config);
+        ElasticsearchParseException expected = null;
+        try {
+            ContextConfig geoconfig = mapping.parseContext(null, parser);
+        } catch (ElasticsearchParseException e) {
+            expected = e;
+        }
+        assertNotNull(expected);
+    }
+
+    @Test
+    public void testUseWithLonLatGeoContext() throws Exception {
+        double lon = 4d;
+        double lat = 52d;
+        XContentBuilder builder = jsonBuilder().startObject().startArray("location").value(lon).value(lat).endArray().endObject();
+        XContentParser parser = XContentHelper.createParser(builder.bytes());
+        parser.nextToken(); // start of object
+        parser.nextToken(); // "location" field name
+        parser.nextToken(); // array
+
+        HashMap<String, Object> config = new HashMap<>();
+        config.put("precision", randomIntBetween(1, 12));
+        GeolocationContextMapping mapping = GeolocationContextMapping.load("foo", config);
+        mapping.parseContext(null, parser);
+    }
+
+    public void testUseWithMultiGeoHashGeoContext() throws Exception {
+        String geohash1 = GeoHashUtils.encode(randomIntBetween(-90, +90), randomIntBetween(-180, +180));
+        String geohash2 = GeoHashUtils.encode(randomIntBetween(-90, +90), randomIntBetween(-180, +180));
+        XContentBuilder builder = jsonBuilder().startObject().startArray("location").value(geohash1).value(geohash2).endArray().endObject();
+        XContentParser parser = XContentHelper.createParser(builder.bytes());
+        parser.nextToken(); // start of object
+        parser.nextToken(); // "location" field name
+        parser.nextToken(); // array
+
+        HashMap<String, Object> config = new HashMap<>();
+        config.put("precision", randomIntBetween(1, 12));
+        GeolocationContextMapping mapping = GeolocationContextMapping.load("foo", config);
+        ContextConfig parsedContext = mapping.parseContext(null, parser);
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java b/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
index b327c06..aea102d 100644
--- a/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
+++ b/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
@@ -669,6 +669,7 @@ public class SharedClusterSnapshotRestoreIT extends AbstractSnapshotIntegTestCas
         assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
         CountResponse countResponse = client.prepareCount("test-idx").get();
         assertThat(countResponse.getCount(), equalTo(100L));
+        logger.info("--> total number of simulated failures during restore: [{}]", getFailureCount("test-repo"));
     }
 
 
diff --git a/core/src/test/java/org/elasticsearch/snapshots/mockstore/MockRepository.java b/core/src/test/java/org/elasticsearch/snapshots/mockstore/MockRepository.java
index e430827..be10f6b 100644
--- a/core/src/test/java/org/elasticsearch/snapshots/mockstore/MockRepository.java
+++ b/core/src/test/java/org/elasticsearch/snapshots/mockstore/MockRepository.java
@@ -108,6 +108,8 @@ public class MockRepository extends FsRepository {
 
     private final double randomDataFileIOExceptionRate;
 
+    private final long maximumNumberOfFailures;
+
     private final long waitAfterUnblock;
 
     private final MockBlobStore mockBlobStore;
@@ -127,6 +129,7 @@ public class MockRepository extends FsRepository {
         super(name, overrideSettings(repositorySettings, clusterService), indexShardRepository, environment);
         randomControlIOExceptionRate = repositorySettings.settings().getAsDouble("random_control_io_exception_rate", 0.0);
         randomDataFileIOExceptionRate = repositorySettings.settings().getAsDouble("random_data_file_io_exception_rate", 0.0);
+        maximumNumberOfFailures = repositorySettings.settings().getAsLong("max_failure_number", 100L);
         blockOnControlFiles = repositorySettings.settings().getAsBoolean("block_on_control", false);
         blockOnDataFiles = repositorySettings.settings().getAsBoolean("block_on_data", false);
         blockOnInitialization = repositorySettings.settings().getAsBoolean("block_on_init", false);
@@ -160,8 +163,8 @@ public class MockRepository extends FsRepository {
         return settingsBuilder().put(settings).put("location", location.toAbsolutePath()).build();
     }
 
-    private void addFailure() {
-        failureCounter.incrementAndGet();
+    private long incrementAndGetFailureCount() {
+        return failureCounter.incrementAndGet();
     }
 
     @Override
@@ -269,9 +272,8 @@ public class MockRepository extends FsRepository {
 
             private void maybeIOExceptionOrBlock(String blobName) throws IOException {
                 if (blobName.startsWith("__")) {
-                    if (shouldFail(blobName, randomDataFileIOExceptionRate)) {
+                    if (shouldFail(blobName, randomDataFileIOExceptionRate) && (incrementAndGetFailureCount() < maximumNumberOfFailures)) {
                         logger.info("throwing random IOException for file [{}] at path [{}]", blobName, path());
-                        addFailure();
                         throw new IOException("Random IOException");
                     } else if (blockOnDataFiles) {
                         logger.info("blocking I/O operation for file [{}] at path [{}]", blobName, path());
@@ -286,9 +288,8 @@ public class MockRepository extends FsRepository {
                         }
                     }
                 } else {
-                    if (shouldFail(blobName, randomControlIOExceptionRate)) {
+                    if (shouldFail(blobName, randomControlIOExceptionRate) && (incrementAndGetFailureCount() < maximumNumberOfFailures)) {
                         logger.info("throwing random IOException for file [{}] at path [{}]", blobName, path());
-                        addFailure();
                         throw new IOException("Random IOException");
                     } else if (blockOnControlFiles) {
                         logger.info("blocking I/O operation for file [{}] at path [{}]", blobName, path());
diff --git a/core/src/test/java/org/elasticsearch/stresstest/client/ClientFailover.java b/core/src/test/java/org/elasticsearch/stresstest/client/ClientFailover.java
deleted file mode 100644
index 9466851..0000000
--- a/core/src/test/java/org/elasticsearch/stresstest/client/ClientFailover.java
+++ /dev/null
@@ -1,90 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.stresstest.client;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.client.transport.TransportClient;
-import org.elasticsearch.common.transport.InetSocketTransportAddress;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
-
-import java.net.InetAddress;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.atomic.AtomicBoolean;
-import java.util.concurrent.atomic.AtomicLong;
-
-/**
- */
-public class ClientFailover {
-
-    public static void main(String[] args) throws Exception {
-        Node[] nodes = new Node[3];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = NodeBuilder.nodeBuilder().node();
-        }
-        
-        // TODO: what is this? a public static void main test?!?!
-
-        final TransportClient client = TransportClient.builder().build()
-                .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("localhost"), 9300))
-                .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("localhost"), 9301))
-                .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("localhost"), 9302));
-
-        final AtomicBoolean done = new AtomicBoolean();
-        final AtomicLong indexed = new AtomicLong();
-        final CountDownLatch latch = new CountDownLatch(1);
-        Thread indexer = new Thread(new Runnable() {
-            @Override
-            public void run() {
-                while (!done.get()) {
-                    try {
-                        client.prepareIndex("test", "type").setSource("field", "value").execute().actionGet();
-                        indexed.incrementAndGet();
-                    } catch (Exception e) {
-                        e.printStackTrace();
-                    }
-                }
-                latch.countDown();
-            }
-        });
-        indexer.start();
-
-        for (int i = 0; i < 100; i++) {
-            int index = i % nodes.length;
-            nodes[index].close();
-
-            ClusterHealthResponse health = client.admin().cluster().prepareHealth().setWaitForGreenStatus().execute().actionGet();
-            if (health.isTimedOut()) {
-                System.err.println("timed out on health");
-            }
-
-            nodes[index] = NodeBuilder.nodeBuilder().node();
-
-            health = client.admin().cluster().prepareHealth().setWaitForGreenStatus().execute().actionGet();
-            if (health.isTimedOut()) {
-                System.err.println("timed out on health");
-            }
-        }
-
-        latch.await();
-
-        // TODO add verification to the number of indexed docs
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/stresstest/fullrestart/FullRestartStressTest.java b/core/src/test/java/org/elasticsearch/stresstest/fullrestart/FullRestartStressTest.java
deleted file mode 100644
index 59fca1b..0000000
--- a/core/src/test/java/org/elasticsearch/stresstest/fullrestart/FullRestartStressTest.java
+++ /dev/null
@@ -1,222 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.stresstest.fullrestart;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.count.CountResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.Loggers;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
-
-import java.util.Random;
-import java.util.concurrent.ThreadLocalRandom;
-import java.util.concurrent.atomic.AtomicLong;
-
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-
-/**
- *
- */
-public class FullRestartStressTest {
-
-    private final ESLogger logger = Loggers.getLogger(getClass());
-
-    private int numberOfNodes = 4;
-
-    private int numberOfIndices = 5;
-    private int textTokens = 150;
-    private int numberOfFields = 10;
-    private int bulkSize = 1000;
-    private int numberOfDocsPerRound = 50000;
-
-    private Settings settings = Settings.Builder.EMPTY_SETTINGS;
-
-    private TimeValue period = TimeValue.timeValueMinutes(20);
-
-    private AtomicLong indexCounter = new AtomicLong();
-
-    public FullRestartStressTest numberOfNodes(int numberOfNodes) {
-        this.numberOfNodes = numberOfNodes;
-        return this;
-    }
-
-    public FullRestartStressTest numberOfIndices(int numberOfIndices) {
-        this.numberOfIndices = numberOfIndices;
-        return this;
-    }
-
-    public FullRestartStressTest textTokens(int textTokens) {
-        this.textTokens = textTokens;
-        return this;
-    }
-
-    public FullRestartStressTest numberOfFields(int numberOfFields) {
-        this.numberOfFields = numberOfFields;
-        return this;
-    }
-
-    public FullRestartStressTest bulkSize(int bulkSize) {
-        this.bulkSize = bulkSize;
-        return this;
-    }
-
-    public FullRestartStressTest numberOfDocsPerRound(int numberOfDocsPerRound) {
-        this.numberOfDocsPerRound = numberOfDocsPerRound;
-        return this;
-    }
-
-    public FullRestartStressTest settings(Settings settings) {
-        this.settings = settings;
-        return this;
-    }
-
-    public FullRestartStressTest period(TimeValue period) {
-        this.period = period;
-        return this;
-    }
-
-    public void run() throws Exception {
-        long numberOfRounds = 0;
-        Random random = new Random(0);
-        long testStart = System.currentTimeMillis();
-        while (true) {
-            Node[] nodes = new Node[numberOfNodes];
-            for (int i = 0; i < nodes.length; i++) {
-                nodes[i] = NodeBuilder.nodeBuilder().settings(settings).node();
-            }
-            Node client = NodeBuilder.nodeBuilder().settings(settings).client(true).node();
-
-            // verify that the indices are there
-            for (int i = 0; i < numberOfIndices; i++) {
-                try {
-                    client.client().admin().indices().prepareCreate("test" + i).execute().actionGet();
-                } catch (Exception e) {
-                    // might already exists, fine
-                }
-            }
-
-            logger.info("*** Waiting for GREEN status");
-            try {
-                ClusterHealthResponse clusterHealth = client.client().admin().cluster().prepareHealth().setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-                if (clusterHealth.isTimedOut()) {
-                    logger.warn("timed out waiting for green status....");
-                }
-            } catch (Exception e) {
-                logger.warn("failed to execute cluster health....");
-            }
-
-            CountResponse count = client.client().prepareCount().setQuery(matchAllQuery()).execute().actionGet();
-            logger.info("*** index_count [{}], expected_count [{}]", count.getCount(), indexCounter.get());
-            // verify count
-            for (int i = 0; i < (nodes.length * 5); i++) {
-                count = client.client().prepareCount().setQuery(matchAllQuery()).execute().actionGet();
-                logger.debug("index_count [{}], expected_count [{}]", count.getCount(), indexCounter.get());
-                if (count.getCount() != indexCounter.get()) {
-                    logger.warn("!!! count does not match, index_count [{}], expected_count [{}]", count.getCount(), indexCounter.get());
-                    throw new Exception("failed test, count does not match...");
-                }
-            }
-
-            // verify search
-            for (int i = 0; i < (nodes.length * 5); i++) {
-                // do a search with norms field, so we don't rely on match all filtering cache
-                SearchResponse search = client.client().prepareSearch().setQuery(matchAllQuery()).execute().actionGet();
-                logger.debug("index_count [{}], expected_count [{}]", search.getHits().totalHits(), indexCounter.get());
-                if (count.getCount() != indexCounter.get()) {
-                    logger.warn("!!! search does not match, index_count [{}], expected_count [{}]", search.getHits().totalHits(), indexCounter.get());
-                    throw new Exception("failed test, count does not match...");
-                }
-            }
-
-            logger.info("*** ROUND {}", ++numberOfRounds);
-            // bulk index data
-            int numberOfBulks = numberOfDocsPerRound / bulkSize;
-            for (int b = 0; b < numberOfBulks; b++) {
-                BulkRequestBuilder bulk = client.client().prepareBulk();
-                for (int k = 0; k < bulkSize; k++) {
-                    StringBuilder sb = new StringBuilder();
-                    XContentBuilder json = XContentFactory.jsonBuilder().startObject()
-                            .field("field", "value" + ThreadLocalRandom.current().nextInt());
-
-                    int fields = ThreadLocalRandom.current().nextInt() % numberOfFields;
-                    for (int i = 0; i < fields; i++) {
-                        json.field("num_" + i, ThreadLocalRandom.current().nextDouble());
-                        int tokens = ThreadLocalRandom.current().nextInt() % textTokens;
-                        sb.setLength(0);
-                        for (int j = 0; j < tokens; j++) {
-                            sb.append(Strings.randomBase64UUID(random)).append(' ');
-                        }
-                        json.field("text_" + i, sb.toString());
-                    }
-
-                    json.endObject();
-
-                    bulk.add(Requests.indexRequest("test" + (Math.abs(ThreadLocalRandom.current().nextInt()) % numberOfIndices)).type("type1").source(json));
-                    indexCounter.incrementAndGet();
-                }
-                bulk.execute().actionGet();
-            }
-
-            client.close();
-            for (Node node : nodes) {
-                node.close();
-            }
-
-            if ((System.currentTimeMillis() - testStart) > period.millis()) {
-                logger.info("test finished, full_restart_rounds [{}]", numberOfRounds);
-                break;
-            }
-
-        }
-    }
-
-    public static void main(String[] args) throws Exception {
-        System.setProperty("es.logger.prefix", "");
-
-        int numberOfNodes = 2;
-        Settings settings = Settings.settingsBuilder()
-                .put("index.shard.check_on_startup", true)
-                .put("gateway.recover_after_nodes", numberOfNodes)
-                .put("index.number_of_shards", 1)
-                .put("path.data", "data/data1,data/data2")
-                .build();
-
-        FullRestartStressTest test = new FullRestartStressTest()
-                .settings(settings)
-                .period(TimeValue.timeValueMinutes(20))
-                .numberOfNodes(numberOfNodes)
-                .numberOfIndices(1)
-                .textTokens(150)
-                .numberOfFields(10)
-                .bulkSize(1000)
-                .numberOfDocsPerRound(10000);
-
-        test.run();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/stresstest/gcbehavior/FilterCacheGcStress.java b/core/src/test/java/org/elasticsearch/stresstest/gcbehavior/FilterCacheGcStress.java
deleted file mode 100644
index 315dab8..0000000
--- a/core/src/test/java/org/elasticsearch/stresstest/gcbehavior/FilterCacheGcStress.java
+++ /dev/null
@@ -1,70 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.stresstest.gcbehavior;
-
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
-
-import java.util.concurrent.atomic.AtomicBoolean;
-
-import static org.elasticsearch.index.query.QueryBuilders.rangeQuery;
-import static org.elasticsearch.index.query.QueryBuilders.filteredQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-
-public class FilterCacheGcStress {
-
-    public static void main(String[] args) {
-
-        Settings settings = Settings.EMPTY;
-
-        Node node = NodeBuilder.nodeBuilder().settings(settings).node();
-        final Client client = node.client();
-
-        client.admin().indices().prepareCreate("test").execute().actionGet();
-        client.admin().cluster().prepareHealth().setWaitForYellowStatus().execute().actionGet();
-
-        final AtomicBoolean stop = new AtomicBoolean();
-
-        Thread indexingThread = new Thread() {
-            @Override
-            public void run() {
-                while (!stop.get()) {
-                    client.prepareIndex("test", "type1").setSource("field", System.currentTimeMillis()).execute().actionGet();
-                }
-            }
-        };
-        indexingThread.start();
-
-        Thread searchThread = new Thread() {
-            @Override
-            public void run() {
-                while (!stop.get()) {
-                    client.prepareSearch()
-                            .setQuery(filteredQuery(matchAllQuery(), rangeQuery("field").from(System.currentTimeMillis() - 1000000)))
-                            .execute().actionGet();
-                }
-            }
-        };
-
-        searchThread.start();
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/stresstest/get/GetStressTest.java b/core/src/test/java/org/elasticsearch/stresstest/get/GetStressTest.java
deleted file mode 100644
index 33d4b6f..0000000
--- a/core/src/test/java/org/elasticsearch/stresstest/get/GetStressTest.java
+++ /dev/null
@@ -1,96 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.stresstest.get;
-
-import org.elasticsearch.action.get.GetResponse;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
-
-import java.util.concurrent.ThreadLocalRandom;
-import java.util.concurrent.atomic.AtomicBoolean;
-import java.util.concurrent.atomic.AtomicLong;
-
-public class GetStressTest {
-
-    public static void main(String[] args) throws Exception {
-        Settings settings = Settings.settingsBuilder()
-                .put("index.number_of_shards", 2)
-                .put("index.number_of_replicas", 1)
-                .build();
-
-        final int NUMBER_OF_NODES = 2;
-        final int NUMBER_OF_THREADS = 50;
-        final TimeValue TEST_TIME = TimeValue.parseTimeValue("10m", null, "TEST_TIME");
-
-        Node[] nodes = new Node[NUMBER_OF_NODES];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = NodeBuilder.nodeBuilder().settings(settings).node();
-        }
-
-        final Node client = NodeBuilder.nodeBuilder()
-                .settings(settings)
-                .client(true)
-                .node();
-
-        client.client().admin().indices().prepareCreate("test").execute().actionGet();
-
-        final AtomicBoolean done = new AtomicBoolean();
-        final AtomicLong idGenerator = new AtomicLong();
-        final AtomicLong counter = new AtomicLong();
-
-        Thread[] threads = new Thread[NUMBER_OF_THREADS];
-        for (int i = 0; i < threads.length; i++) {
-            threads[i] = new Thread(new Runnable() {
-                @Override
-                public void run() {
-                    ThreadLocalRandom random = ThreadLocalRandom.current();
-                    while (!done.get()) {
-                        String id = String.valueOf(idGenerator.incrementAndGet());
-                        client.client().prepareIndex("test", "type1", id)
-                                .setSource("field", random.nextInt(100))
-                                .execute().actionGet();
-
-                        GetResponse getResponse = client.client().prepareGet("test", "type1", id)
-                                //.setFields(Strings.EMPTY_ARRAY)
-                                .execute().actionGet();
-                        if (!getResponse.isExists()) {
-                            System.err.println("Failed to find " + id);
-                        }
-
-                        long count = counter.incrementAndGet();
-                        if ((count % 10000) == 0) {
-                            System.out.println("Executed " + count);
-                        }
-                    }
-                }
-            });
-        }
-        for (Thread thread : threads) {
-            thread.start();
-        }
-
-        Thread.sleep(TEST_TIME.millis());
-
-        System.out.println("test done.");
-        done.set(true);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/stresstest/get/MGetStress1.java b/core/src/test/java/org/elasticsearch/stresstest/get/MGetStress1.java
deleted file mode 100644
index 3118c22..0000000
--- a/core/src/test/java/org/elasticsearch/stresstest/get/MGetStress1.java
+++ /dev/null
@@ -1,106 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.stresstest.get;
-
-import com.google.common.collect.Sets;
-import org.elasticsearch.action.get.MultiGetItemResponse;
-import org.elasticsearch.action.get.MultiGetResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
-
-import java.util.Set;
-import java.util.concurrent.ThreadLocalRandom;
-import java.util.concurrent.atomic.AtomicBoolean;
-
-/**
- */
-public class MGetStress1 {
-
-    public static void main(String[] args) throws Exception {
-        final int NUMBER_OF_NODES = 2;
-        final int NUMBER_OF_DOCS = 50000;
-        final int MGET_BATCH = 1000;
-
-        Node[] nodes = new Node[NUMBER_OF_NODES];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = NodeBuilder.nodeBuilder().node();
-        }
-
-        System.out.println("---> START Indexing initial data [" + NUMBER_OF_DOCS + "]");
-        final Client client = nodes[0].client();
-        for (int i = 0; i < NUMBER_OF_DOCS; i++) {
-            client.prepareIndex("test", "type", Integer.toString(i)).setSource("field", "value").execute().actionGet();
-        }
-        System.out.println("---> DONE Indexing initial data [" + NUMBER_OF_DOCS + "]");
-
-        final AtomicBoolean done = new AtomicBoolean();
-        // start indexer
-        Thread indexer = new Thread(new Runnable() {
-            @Override
-            public void run() {
-                while (!done.get()) {
-                    client.prepareIndex("test", "type", Integer.toString(ThreadLocalRandom.current().nextInt(NUMBER_OF_DOCS)))
-                            .setSource("field", "value").execute().actionGet();
-                }
-            }
-        });
-        indexer.start();
-        System.out.println("---> Starting indexer");
-
-        // start the mget one
-        Thread mget = new Thread(new Runnable() {
-            @Override
-            public void run() {
-                while (!done.get()) {
-                    Set<String> ids = Sets.newHashSet();
-                    for (int i = 0; i < MGET_BATCH; i++) {
-                        ids.add(Integer.toString(ThreadLocalRandom.current().nextInt(NUMBER_OF_DOCS)));
-                    }
-                    //System.out.println("---> mget for [" + ids.size() + "]");
-                    MultiGetResponse response = client.prepareMultiGet().add("test", "type", ids).execute().actionGet();
-                    int expected = ids.size();
-                    int count = 0;
-                    for (MultiGetItemResponse item : response) {
-                        count++;
-                        if (item.isFailed()) {
-                            System.err.println("item failed... " + item.getFailure());
-                        } else {
-                            boolean removed = ids.remove(item.getId());
-                            if (!removed) {
-                                System.err.println("got id twice " + item.getId());
-                            }
-                        }
-                    }
-                    if (expected != count) {
-                        System.err.println("Expected [" + expected + "], got back [" + count + "]");
-                    }
-                }
-            }
-        });
-        mget.start();
-        System.out.println("---> Starting mget");
-
-        Thread.sleep(TimeValue.timeValueMinutes(10).millis());
-
-        done.set(true);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/stresstest/indexing/BulkIndexingStressTest.java b/core/src/test/java/org/elasticsearch/stresstest/indexing/BulkIndexingStressTest.java
deleted file mode 100644
index 640a523..0000000
--- a/core/src/test/java/org/elasticsearch/stresstest/indexing/BulkIndexingStressTest.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.stresstest.indexing;
-
-import org.elasticsearch.action.bulk.BulkItemResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
-
-import java.util.concurrent.ThreadLocalRandom;
-
-/**
- */
-public class BulkIndexingStressTest {
-
-    public static void main(String[] args) {
-        final int NUMBER_OF_NODES = 4;
-        final int NUMBER_OF_INDICES = 600;
-        final int BATCH = 300;
-
-        final Settings nodeSettings = Settings.settingsBuilder().put("index.number_of_shards", 2).build();
-
-//            ESLogger logger = Loggers.getLogger("org.elasticsearch");
-//            logger.setLevel("DEBUG");
-        Node[] nodes = new Node[NUMBER_OF_NODES];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = NodeBuilder.nodeBuilder().settings(nodeSettings).node();
-        }
-
-        Client client = nodes.length == 1 ? nodes[0].client() : nodes[1].client();
-
-        while (true) {
-            BulkRequestBuilder bulkRequest = client.prepareBulk();
-            for (int i = 0; i < BATCH; i++) {
-                bulkRequest.add(Requests.indexRequest("test" + ThreadLocalRandom.current().nextInt(NUMBER_OF_INDICES)).type("type").source("field", "value"));
-            }
-            BulkResponse bulkResponse = bulkRequest.execute().actionGet();
-            if (bulkResponse.hasFailures()) {
-                for (BulkItemResponse item : bulkResponse) {
-                    if (item.isFailed()) {
-                        System.out.println("failed response:" + item.getFailureMessage());
-                    }
-                }
-
-                throw new RuntimeException("Failed responses");
-            }
-            ;
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/stresstest/indexing/ConcurrentIndexingVersioningStressTest.java b/core/src/test/java/org/elasticsearch/stresstest/indexing/ConcurrentIndexingVersioningStressTest.java
deleted file mode 100644
index 3fca814..0000000
--- a/core/src/test/java/org/elasticsearch/stresstest/indexing/ConcurrentIndexingVersioningStressTest.java
+++ /dev/null
@@ -1,118 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.stresstest.indexing;
-
-import org.elasticsearch.action.get.GetResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.node.Node;
-
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.ThreadLocalRandom;
-
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- * Checks that index operation does not create duplicate documents.
- */
-public class ConcurrentIndexingVersioningStressTest {
-
-    public static void main(String[] args) throws Exception {
-
-        Settings settings = Settings.EMPTY;
-
-        Node node1 = nodeBuilder().settings(settings).node();
-        Node node2 = nodeBuilder().settings(settings).node();
-        final Node client = nodeBuilder().settings(settings).client(true).node();
-
-        final int NUMBER_OF_DOCS = 10000;
-        final int NUMBER_OF_THREADS = 10;
-        final long NUMBER_OF_ITERATIONS = SizeValue.parseSizeValue("10k").singles();
-        final long DELETE_EVERY = 10;
-
-        final CountDownLatch latch = new CountDownLatch(NUMBER_OF_THREADS);
-        Thread[] threads = new Thread[NUMBER_OF_THREADS];
-        for (int i = 0; i < threads.length; i++) {
-            threads[i] = new Thread() {
-                @Override
-                public void run() {
-                    try {
-                        for (long i = 0; i < NUMBER_OF_ITERATIONS; i++) {
-                            if ((i % DELETE_EVERY) == 0) {
-                                client.client().prepareDelete("test", "type1", Integer.toString(ThreadLocalRandom.current().nextInt(NUMBER_OF_DOCS))).execute().actionGet();
-                            } else {
-                                client.client().prepareIndex("test", "type1", Integer.toString(ThreadLocalRandom.current().nextInt(NUMBER_OF_DOCS))).setSource("field1", "value1").execute().actionGet();
-                            }
-                        }
-                    } finally {
-                        latch.countDown();
-                    }
-                }
-            };
-        }
-
-        for (Thread thread : threads) {
-            thread.start();
-        }
-
-        latch.await();
-        System.out.println("done indexing, verifying docs");
-        client.client().admin().indices().prepareRefresh().execute().actionGet();
-        for (int i = 0; i < NUMBER_OF_DOCS; i++) {
-            String id = Integer.toString(i);
-            for (int j = 0; j < 5; j++) {
-                SearchResponse response = client.client().prepareSearch().setQuery(QueryBuilders.termQuery("_id", id)).execute().actionGet();
-                if (response.getHits().totalHits() > 1) {
-                    System.err.println("[" + i + "] FAIL, HITS [" + response.getHits().totalHits() + "]");
-                }
-            }
-            GetResponse getResponse = client.client().prepareGet("test", "type1", id).execute().actionGet();
-            if (getResponse.isExists()) {
-                long version = getResponse.getVersion();
-                for (int j = 0; j < 5; j++) {
-                    getResponse = client.client().prepareGet("test", "type1", id).execute().actionGet();
-                    if (!getResponse.isExists()) {
-                        System.err.println("[" + i + "] FAIL, EXISTED, and NOT_EXISTED");
-                        break;
-                    }
-                    if (version != getResponse.getVersion()) {
-                        System.err.println("[" + i + "] FAIL, DIFFERENT VERSIONS: [" + version + "], [" + getResponse.getVersion() + "]");
-                        break;
-                    }
-                }
-            } else {
-                for (int j = 0; j < 5; j++) {
-                    getResponse = client.client().prepareGet("test", "type1", id).execute().actionGet();
-                    if (getResponse.isExists()) {
-                        System.err.println("[" + i + "] FAIL, EXISTED, and NOT_EXISTED");
-                        break;
-                    }
-                }
-            }
-        }
-        System.out.println("done.");
-
-        client.close();
-        node1.close();
-        node2.close();
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/stresstest/leaks/GenericStatsLeak.java b/core/src/test/java/org/elasticsearch/stresstest/leaks/GenericStatsLeak.java
deleted file mode 100644
index fc0d5bc..0000000
--- a/core/src/test/java/org/elasticsearch/stresstest/leaks/GenericStatsLeak.java
+++ /dev/null
@@ -1,47 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.stresstest.leaks;
-
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.monitor.jvm.JvmService;
-import org.elasticsearch.monitor.os.OsService;
-import org.elasticsearch.monitor.process.ProcessService;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
-
-public class GenericStatsLeak {
-
-    public static void main(String[] args) {
-        Node node = NodeBuilder.nodeBuilder().settings(Settings.settingsBuilder()
-                .put("monitor.os.refresh_interval", 0)
-                .put("monitor.process.refresh_interval", 0)
-        ).node();
-
-        JvmService jvmService = node.injector().getInstance(JvmService.class);
-        OsService osService = node.injector().getInstance(OsService.class);
-        ProcessService processService = node.injector().getInstance(ProcessService.class);
-
-        while (true) {
-            jvmService.stats();
-            osService.stats();
-            processService.stats();
-        }
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/stresstest/leaks/JvmStatsLeak.java b/core/src/test/java/org/elasticsearch/stresstest/leaks/JvmStatsLeak.java
deleted file mode 100644
index e558b47..0000000
--- a/core/src/test/java/org/elasticsearch/stresstest/leaks/JvmStatsLeak.java
+++ /dev/null
@@ -1,35 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.stresstest.leaks;
-
-import org.elasticsearch.monitor.jvm.JvmStats;
-
-/**
- * This test mainly comes to check the native memory leak with getLastGCInfo (which is now
- * disabled by default).
- */
-public class JvmStatsLeak {
-
-    public static void main(String[] args) {
-        while (true) {
-            JvmStats.jvmStats();
-        }
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/stresstest/manyindices/ManyIndicesRemoteStressTest.java b/core/src/test/java/org/elasticsearch/stresstest/manyindices/ManyIndicesRemoteStressTest.java
deleted file mode 100644
index 1917fd6..0000000
--- a/core/src/test/java/org/elasticsearch/stresstest/manyindices/ManyIndicesRemoteStressTest.java
+++ /dev/null
@@ -1,77 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.stresstest.manyindices;
-
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.transport.TransportClient;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.Loggers;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.transport.InetSocketTransportAddress;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
-
-import java.net.InetAddress;
-import java.util.Date;
-
-/**
- *
- */
-public class ManyIndicesRemoteStressTest {
-
-    private static final ESLogger logger = Loggers.getLogger(ManyIndicesRemoteStressTest.class);
-
-    public static void main(String[] args) throws Exception {
-        System.setProperty("es.logger.prefix", "");
-
-        int numberOfShards = 1;
-        int numberOfReplicas = 1;
-        int numberOfIndices = 1000;
-        int numberOfDocs = 1;
-
-        Client client;
-        Node node = null;
-        // TODO: what is this? a public static void main test?!?!?!
-        if (true) {
-            client = TransportClient.builder().settings(Settings.EMPTY).build().addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("localhost"), 9300));
-        } else {
-            node = NodeBuilder.nodeBuilder().client(true).node();
-            client = node.client();
-        }
-
-        for (int i = 0; i < numberOfIndices; i++) {
-            logger.info("START index [{}] ...", i);
-            client.admin().indices().prepareCreate("index_" + i)
-                    .setSettings(Settings.settingsBuilder().put("index.number_of_shards", numberOfShards).put("index.number_of_replicas", numberOfReplicas))
-                    .execute().actionGet();
-
-            for (int j = 0; j < numberOfDocs; j++) {
-                client.prepareIndex("index_" + i, "type").setSource("field1", "test", "field2", 2, "field3", new Date()).execute().actionGet();
-            }
-            logger.info("DONE  index [{}]", i);
-        }
-
-        logger.info("closing node...");
-        if (node != null) {
-            node.close();
-        }
-        logger.info("node closed");
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/stresstest/manyindices/ManyIndicesStressTest.java b/core/src/test/java/org/elasticsearch/stresstest/manyindices/ManyIndicesStressTest.java
deleted file mode 100644
index 01476f1..0000000
--- a/core/src/test/java/org/elasticsearch/stresstest/manyindices/ManyIndicesStressTest.java
+++ /dev/null
@@ -1,98 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.stresstest.manyindices;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.Loggers;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
-
-import java.util.Date;
-
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-
-/**
- *
- */
-public class ManyIndicesStressTest {
-
-    private static final ESLogger logger = Loggers.getLogger(ManyIndicesStressTest.class);
-
-    public static void main(String[] args) throws Exception {
-        System.setProperty("es.logger.prefix", "");
-
-        int numberOfIndices = 100;
-        int numberOfDocs = 100;
-
-        Settings settings = Settings.settingsBuilder()
-                .put("index.shard.check_on_startup", false)
-                .put("index.number_of_shards", 1)
-                .build();
-        Node node = NodeBuilder.nodeBuilder().settings(settings).node();
-
-        for (int i = 0; i < numberOfIndices; i++) {
-            logger.info("START index [{}] ...", i);
-            node.client().admin().indices().prepareCreate("index_" + i).execute().actionGet();
-
-            for (int j = 0; j < numberOfDocs; j++) {
-                node.client().prepareIndex("index_" + i, "type").setSource("field1", "test", "field2", 2, "field3", new Date()).execute().actionGet();
-            }
-            logger.info("DONE  index [{}] ...", i);
-        }
-
-        logger.info("closing node...");
-        node.close();
-        logger.info("node closed");
-
-        logger.info("starting node...");
-        node = NodeBuilder.nodeBuilder().settings(settings).node();
-
-        ClusterHealthResponse health = node.client().admin().cluster().prepareHealth().setTimeout("5m").setWaitForYellowStatus().execute().actionGet();
-        logger.info("health: " + health.getStatus());
-        logger.info("active shards: " + health.getActiveShards());
-        logger.info("active primary shards: " + health.getActivePrimaryShards());
-        if (health.isTimedOut()) {
-            logger.error("Timed out on health...");
-        }
-
-        ClusterState clusterState = node.client().admin().cluster().prepareState().execute().actionGet().getState();
-        for (int i = 0; i < numberOfIndices; i++) {
-            if (clusterState.blocks().indices().containsKey("index_" + i)) {
-                logger.error("index [{}] has blocks: {}", i, clusterState.blocks().indices().get("index_" + i));
-            }
-        }
-
-        for (int i = 0; i < numberOfIndices; i++) {
-            long count = node.client().prepareCount("index_" + i).setQuery(matchAllQuery()).execute().actionGet().getCount();
-            if (count == numberOfDocs) {
-                logger.info("VERIFIED [{}], count [{}]", i, count);
-            } else {
-                logger.error("FAILED [{}], expected [{}], got [{}]", i, numberOfDocs, count);
-            }
-        }
-
-        logger.info("closing node...");
-        node.close();
-        logger.info("node closed");
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/stresstest/manyindices/ManyNodesManyIndicesRecoveryStressTest.java b/core/src/test/java/org/elasticsearch/stresstest/manyindices/ManyNodesManyIndicesRecoveryStressTest.java
deleted file mode 100644
index ccd25a1..0000000
--- a/core/src/test/java/org/elasticsearch/stresstest/manyindices/ManyNodesManyIndicesRecoveryStressTest.java
+++ /dev/null
@@ -1,126 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.stresstest.manyindices;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.count.CountResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
-
-import java.util.ArrayList;
-import java.util.List;
-
-public class ManyNodesManyIndicesRecoveryStressTest {
-
-    public static void main(String[] args) throws Exception {
-        final int NUM_NODES = 40;
-        final int NUM_INDICES = 100;
-        final int NUM_DOCS = 2;
-        final int FLUSH_AFTER = 1;
-
-        final Settings nodeSettings = Settings.settingsBuilder()
-                .put("transport.netty.connections_per_node.low", 0)
-                .put("transport.netty.connections_per_node.med", 0)
-                .put("transport.netty.connections_per_node.high", 1)
-                .build();
-
-        final Settings indexSettings = Settings.settingsBuilder()
-                .put("index.number_of_shards", 1)
-                .build();
-
-        List<Node> nodes = new ArrayList<>();
-        for (int i = 0; i < NUM_NODES; i++) {
-            nodes.add(NodeBuilder.nodeBuilder().settings(Settings.settingsBuilder().put(nodeSettings).put("name", "node" + i)).node());
-        }
-        Client client = nodes.get(0).client();
-
-        for (int index = 0; index < NUM_INDICES; index++) {
-            String indexName = "index_" + index;
-            System.out.println("--> Processing index [" + indexName + "]...");
-            client.admin().indices().prepareCreate(indexName).setSettings(indexSettings).execute().actionGet();
-
-            boolean flushed = false;
-            for (int doc = 0; doc < NUM_DOCS; doc++) {
-                if (!flushed && doc > FLUSH_AFTER) {
-                    flushed = true;
-                    client.admin().indices().prepareFlush(indexName).execute().actionGet();
-                }
-                client.prepareIndex(indexName, "type1", Integer.toString(doc)).setSource("field", "value" + doc).execute().actionGet();
-            }
-            System.out.println("--> DONE index [" + indexName + "]");
-        }
-
-        System.out.println("--> Initiating shutdown");
-        for (Node node : nodes) {
-            node.close();
-        }
-
-        System.out.println("--> Waiting for all nodes to be closed...");
-        while (true) {
-            boolean allAreClosed = true;
-            for (Node node : nodes) {
-                if (!node.isClosed()) {
-                    allAreClosed = false;
-                    break;
-                }
-            }
-            if (allAreClosed) {
-                break;
-            }
-            Thread.sleep(100);
-        }
-        System.out.println("Waiting a bit for node lock to really be released?");
-        Thread.sleep(5000);
-        System.out.println("--> All nodes are closed, starting back...");
-
-        nodes = new ArrayList<>();
-        for (int i = 0; i < NUM_NODES; i++) {
-            nodes.add(NodeBuilder.nodeBuilder().settings(Settings.settingsBuilder().put(nodeSettings).put("name", "node" + i)).node());
-        }
-        client = nodes.get(0).client();
-
-        System.out.println("--> Waiting for green status");
-        while (true) {
-            ClusterHealthResponse clusterHealth = client.admin().cluster().prepareHealth().setWaitForGreenStatus().execute().actionGet();
-            if (clusterHealth.isTimedOut()) {
-                System.err.println("--> cluster health timed out..., active shards [" + clusterHealth.getActiveShards() + "]");
-            } else {
-                break;
-            }
-        }
-
-        System.out.println("Verifying counts...");
-        for (int index = 0; index < NUM_INDICES; index++) {
-            String indexName = "index_" + index;
-            CountResponse count = client.prepareCount(indexName).setQuery(QueryBuilders.matchAllQuery()).execute().actionGet();
-            if (count.getCount() != NUM_DOCS) {
-                System.err.println("Wrong count value, expected [" + NUM_DOCS + "], got [" + count.getCount() + "] for index [" + indexName + "]");
-            }
-        }
-
-        System.out.println("Test end");
-        for (Node node : nodes) {
-            node.close();
-        }
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/stresstest/refresh/RefreshStressTest1.java b/core/src/test/java/org/elasticsearch/stresstest/refresh/RefreshStressTest1.java
deleted file mode 100644
index eec3852..0000000
--- a/core/src/test/java/org/elasticsearch/stresstest/refresh/RefreshStressTest1.java
+++ /dev/null
@@ -1,96 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.stresstest.refresh;
-
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
-
-import java.io.IOException;
-import java.util.UUID;
-
-/**
- */
-public class RefreshStressTest1 {
-
-    public static void main(String[] args) throws InterruptedException, IOException {
-        int numberOfShards = 5;
-        Node node = NodeBuilder.nodeBuilder().local(true).loadConfigSettings(false).clusterName("testCluster").settings(
-                Settings.settingsBuilder()
-                        .put("node.name", "node1")
-                        .put("index.number_of_shards", numberOfShards)
-                                //.put("path.data", new File("target/data").getAbsolutePath())
-                        .build()).node();
-        Node node2 = NodeBuilder.nodeBuilder().local(true).loadConfigSettings(false).clusterName("testCluster").settings(
-                Settings.settingsBuilder()
-                        .put("node.name", "node2")
-                        .put("index.number_of_shards", numberOfShards)
-                                //.put("path.data", new File("target/data").getAbsolutePath())
-                        .build()).node();
-        Client client = node.client();
-
-        for (int loop = 1; loop < 1000; loop++) {
-            String indexName = "testindex" + loop;
-            String typeName = "testType" + loop;
-            String id = UUID.randomUUID().toString();
-            String mapping = "{ \"" + typeName + "\" :  {\"dynamic_templates\" : [{\"no_analyze_strings\" : {\"match_mapping_type\" : \"string\",\"match\" : \"*\",\"mapping\" : {\"type\" : \"string\",\"index\" : \"not_analyzed\"}}}]}}";
-            client.admin().indices().prepareCreate(indexName).execute().actionGet();
-            client.admin().indices().preparePutMapping(indexName).setType(typeName).setSource(mapping).execute().actionGet();
-//      sleep after put mapping
-//      Thread.sleep(100);
-
-            System.out.println("indexing " + loop);
-            String name = "name" + id;
-            client.prepareIndex(indexName, typeName, id).setSource("{ \"id\": \"" + id + "\", \"name\": \"" + name + "\" }").execute().actionGet();
-
-            client.admin().indices().prepareRefresh(indexName).execute().actionGet();
-//      sleep after refresh
-//      Thread.sleep(100);
-
-            System.out.println("searching " + loop);
-            SearchResponse result = client.prepareSearch(indexName).setPostFilter(QueryBuilders.termQuery("name", name)).execute().actionGet();
-            if (result.getHits().hits().length != 1) {
-                for (int i = 1; i <= 100; i++) {
-                    System.out.println("retry " + loop + ", " + i + ", previous total hits: " + result.getHits().getTotalHits());
-                    client.admin().indices().prepareRefresh(indexName).execute().actionGet();
-                    Thread.sleep(100);
-                    result = client.prepareSearch(indexName).setPostFilter(QueryBuilders.termQuery("name", name)).execute().actionGet();
-                    if (result.getHits().hits().length == 1) {
-                        client.admin().indices().prepareRefresh(indexName).execute().actionGet();
-                        result = client.prepareSearch(indexName).setPostFilter(QueryBuilders.termQuery("name", name)).execute().actionGet();
-                        throw new RuntimeException("Record found after " + (i * 100) + " ms, second go: " + result.getHits().hits().length);
-                    } else if (i == 100) {
-                        if (client.prepareGet(indexName, typeName, id).execute().actionGet().isExists())
-                            throw new RuntimeException("Record wasn't found after 10s but can be get by id");
-                        else throw new RuntimeException("Record wasn't found after 10s and can't be get by id");
-                    }
-                }
-            }
-
-            //client.admin().indices().prepareDelete(indexName).execute().actionGet();
-        }
-        client.close();
-        node2.close();
-        node.close();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/stresstest/rollingrestart/QuickRollingRestartStressTest.java b/core/src/test/java/org/elasticsearch/stresstest/rollingrestart/QuickRollingRestartStressTest.java
deleted file mode 100644
index a2fd5d8..0000000
--- a/core/src/test/java/org/elasticsearch/stresstest/rollingrestart/QuickRollingRestartStressTest.java
+++ /dev/null
@@ -1,124 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.stresstest.rollingrestart;
-
-import com.carrotsearch.randomizedtesting.generators.RandomStrings;
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
-
-import java.util.Date;
-import java.util.Random;
-import java.util.concurrent.ThreadLocalRandom;
-
-/**
- */
-public class QuickRollingRestartStressTest {
-
-    public static void main(String[] args) throws Exception {
-        System.setProperty("es.logger.prefix", "");
-
-        Random random = new Random();
-
-        Settings settings = Settings.settingsBuilder().build();
-
-        Node[] nodes = new Node[5];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = NodeBuilder.nodeBuilder().settings(settings).node();
-        }
-
-        Node client = NodeBuilder.nodeBuilder().client(true).node();
-
-        long COUNT;
-        if (client.client().admin().indices().prepareExists("test").execute().actionGet().isExists()) {
-            ClusterHealthResponse clusterHealthResponse = client.client().admin().cluster().prepareHealth().setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                throw new ElasticsearchException("failed to wait for green state on startup...");
-            }
-            COUNT = client.client().prepareCount().execute().actionGet().getCount();
-            System.out.println("--> existing index, count [" + COUNT + "]");
-        } else {
-            COUNT = SizeValue.parseSizeValue("100k").singles();
-            System.out.println("--> indexing data...");
-            for (long i = 0; i < COUNT; i++) {
-                client.client().prepareIndex("test", "type", Long.toString(i))
-                        .setSource("date", new Date(), "data", RandomStrings.randomAsciiOfLength(random, 10000))
-                        .execute().actionGet();
-            }
-            System.out.println("--> done indexing data [" + COUNT + "]");
-            client.client().admin().indices().prepareRefresh().execute().actionGet();
-            for (int i = 0; i < 10; i++) {
-                long count = client.client().prepareCount().execute().actionGet().getCount();
-                if (COUNT != count) {
-                    System.err.println("--> the indexed docs do not match the count..., got [" + count + "], expected [" + COUNT + "]");
-                }
-            }
-        }
-
-        final int ROLLING_RESTARTS = 100;
-        System.out.println("--> starting rolling restarts [" + ROLLING_RESTARTS + "]");
-        for (int rollingRestart = 0; rollingRestart < ROLLING_RESTARTS; rollingRestart++) {
-            System.out.println("--> doing rolling restart [" + rollingRestart + "]...");
-            int nodeId = ThreadLocalRandom.current().nextInt();
-            for (int i = 0; i < nodes.length; i++) {
-                int nodeIdx = Math.abs(nodeId++) % nodes.length;
-                nodes[nodeIdx].close();
-                nodes[nodeIdx] = NodeBuilder.nodeBuilder().settings(settings).node();
-            }
-            System.out.println("--> done rolling restart [" + rollingRestart + "]");
-
-            System.out.println("--> waiting for green state now...");
-            ClusterHealthResponse clusterHealthResponse = client.client().admin().cluster().prepareHealth().setWaitForGreenStatus().setWaitForRelocatingShards(0).setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> timed out waiting for green state...");
-                ClusterState state = client.client().admin().cluster().prepareState().execute().actionGet().getState();
-                System.out.println(state.nodes().prettyPrint());
-                System.out.println(state.routingTable().prettyPrint());
-                System.out.println(state.getRoutingNodes().prettyPrint());
-                throw new ElasticsearchException("timed out waiting for green state");
-            } else {
-                System.out.println("--> got green status");
-            }
-
-            System.out.println("--> checking data [" + rollingRestart + "]....");
-            boolean failed = false;
-            for (int i = 0; i < 10; i++) {
-                long count = client.client().prepareCount().execute().actionGet().getCount();
-                if (COUNT != count) {
-                    failed = true;
-                    System.err.println("--> ERROR the indexed docs do not match the count..., got [" + count + "], expected [" + COUNT + "]");
-                }
-            }
-            if (!failed) {
-                System.out.println("--> count verified");
-            }
-        }
-
-        System.out.println("--> shutting down...");
-        client.close();
-        for (Node node : nodes) {
-            node.close();
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/stresstest/rollingrestart/RollingRestartStressTest.java b/core/src/test/java/org/elasticsearch/stresstest/rollingrestart/RollingRestartStressTest.java
deleted file mode 100644
index 76e27a5..0000000
--- a/core/src/test/java/org/elasticsearch/stresstest/rollingrestart/RollingRestartStressTest.java
+++ /dev/null
@@ -1,354 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.stresstest.rollingrestart;
-
-import org.apache.lucene.util.IOUtils;
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.count.CountResponse;
-import org.elasticsearch.action.get.GetResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.action.search.SearchType;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.Loggers;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.env.NodeEnvironment;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
-import org.elasticsearch.search.SearchHit;
-
-import java.nio.file.Path;
-import java.util.Arrays;
-import java.util.Random;
-import java.util.concurrent.ThreadLocalRandom;
-import java.util.concurrent.atomic.AtomicLong;
-
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-
-/**
- *
- */
-public class RollingRestartStressTest {
-
-    private final ESLogger logger = Loggers.getLogger(getClass());
-
-    private int numberOfShards = 5;
-    private int numberOfReplicas = 1;
-    private int numberOfNodes = 4;
-
-    private int textTokens = 150;
-    private int numberOfFields = 10;
-    private long initialNumberOfDocs = 100000;
-
-    private int indexers = 0;
-
-    private TimeValue indexerThrottle = TimeValue.timeValueMillis(100);
-
-    private Settings settings = Settings.Builder.EMPTY_SETTINGS;
-
-    private TimeValue period = TimeValue.timeValueMinutes(20);
-
-    private boolean clearNodeData = true;
-
-    private Node client;
-
-    private AtomicLong indexCounter = new AtomicLong();
-    private AtomicLong idCounter = new AtomicLong();
-
-
-    public RollingRestartStressTest numberOfNodes(int numberOfNodes) {
-        this.numberOfNodes = numberOfNodes;
-        return this;
-    }
-
-    public RollingRestartStressTest numberOfShards(int numberOfShards) {
-        this.numberOfShards = numberOfShards;
-        return this;
-    }
-
-    public RollingRestartStressTest numberOfReplicas(int numberOfReplicas) {
-        this.numberOfReplicas = numberOfReplicas;
-        return this;
-    }
-
-    public RollingRestartStressTest initialNumberOfDocs(long initialNumberOfDocs) {
-        this.initialNumberOfDocs = initialNumberOfDocs;
-        return this;
-    }
-
-    public RollingRestartStressTest textTokens(int textTokens) {
-        this.textTokens = textTokens;
-        return this;
-    }
-
-    public RollingRestartStressTest numberOfFields(int numberOfFields) {
-        this.numberOfFields = numberOfFields;
-        return this;
-    }
-
-    public RollingRestartStressTest indexers(int indexers) {
-        this.indexers = indexers;
-        return this;
-    }
-
-    public RollingRestartStressTest indexerThrottle(TimeValue indexerThrottle) {
-        this.indexerThrottle = indexerThrottle;
-        return this;
-    }
-
-    public RollingRestartStressTest period(TimeValue period) {
-        this.period = period;
-        return this;
-    }
-
-    public RollingRestartStressTest cleanNodeData(boolean clearNodeData) {
-        this.clearNodeData = clearNodeData;
-        return this;
-    }
-
-    public RollingRestartStressTest settings(Settings settings) {
-        this.settings = settings;
-        return this;
-    }
-
-    public void run() throws Exception {
-        Random random = new Random(0);
-
-        Node[] nodes = new Node[numberOfNodes];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = NodeBuilder.nodeBuilder().settings(settings).node();
-        }
-        client = NodeBuilder.nodeBuilder().settings(settings).client(true).node();
-
-        client.client().admin().indices().prepareCreate("test").setSettings(settingsBuilder()
-                .put("index.number_of_shards", numberOfShards)
-                .put("index.number_of_replicas", numberOfReplicas)
-        ).execute().actionGet();
-
-        logger.info("********** [START] INDEXING INITIAL DOCS");
-        for (long i = 0; i < initialNumberOfDocs; i++) {
-            indexDoc(random);
-        }
-        logger.info("********** [DONE ] INDEXING INITIAL DOCS");
-
-        Indexer[] indexerThreads = new Indexer[indexers];
-        for (int i = 0; i < indexerThreads.length; i++) {
-            indexerThreads[i] = new Indexer();
-        }
-        for (int i = 0; i < indexerThreads.length; i++) {
-            indexerThreads[i].start();
-        }
-
-        long testStart = System.currentTimeMillis();
-
-        // start doing the rolling restart
-        int nodeIndex = 0;
-        while (true) {
-            Path[] nodeData = nodes[nodeIndex].injector().getInstance(NodeEnvironment.class).nodeDataPaths();
-            nodes[nodeIndex].close();
-            if (clearNodeData) {
-                try {
-                    IOUtils.rm(nodeData);
-                } catch (Exception ex) {
-                     logger.debug("Failed to delete node data directories", ex);
-
-                }
-            }
-
-            try {
-                ClusterHealthResponse clusterHealth = client.client().admin().cluster().prepareHealth()
-                        .setWaitForGreenStatus()
-                        .setWaitForNodes(Integer.toString(numberOfNodes + 0 /* client node*/))
-                        .setWaitForRelocatingShards(0)
-                        .setTimeout("10m").execute().actionGet();
-                if (clusterHealth.isTimedOut()) {
-                    logger.warn("timed out waiting for green status....");
-                }
-            } catch (Exception e) {
-                logger.warn("failed to execute cluster health....");
-            }
-
-            nodes[nodeIndex] = NodeBuilder.nodeBuilder().settings(settings).node();
-
-            Thread.sleep(1000);
-
-            try {
-                ClusterHealthResponse clusterHealth = client.client().admin().cluster().prepareHealth()
-                        .setWaitForGreenStatus()
-                        .setWaitForNodes(Integer.toString(numberOfNodes + 1 /* client node*/))
-                        .setWaitForRelocatingShards(0)
-                        .setTimeout("10m").execute().actionGet();
-                if (clusterHealth.isTimedOut()) {
-                    logger.warn("timed out waiting for green status....");
-                }
-            } catch (Exception e) {
-                logger.warn("failed to execute cluster health....");
-            }
-
-            if (++nodeIndex == nodes.length) {
-                nodeIndex = 0;
-            }
-
-            if ((System.currentTimeMillis() - testStart) > period.millis()) {
-                logger.info("test finished");
-                break;
-            }
-        }
-
-        for (int i = 0; i < indexerThreads.length; i++) {
-            indexerThreads[i].close = true;
-        }
-
-        Thread.sleep(indexerThrottle.millis() + 10000);
-
-        for (int i = 0; i < indexerThreads.length; i++) {
-            if (!indexerThreads[i].closed) {
-                logger.warn("thread not closed!");
-            }
-        }
-
-        client.client().admin().indices().prepareRefresh().execute().actionGet();
-
-        // check the count
-        for (int i = 0; i < (nodes.length * 5); i++) {
-            CountResponse count = client.client().prepareCount().setQuery(matchAllQuery()).execute().actionGet();
-            logger.info("indexed [{}], count [{}], [{}]", count.getCount(), indexCounter.get(), count.getCount() == indexCounter.get() ? "OK" : "FAIL");
-            if (count.getCount() != indexCounter.get()) {
-                logger.warn("count does not match!");
-            }
-        }
-
-        // scan all the docs, verify all have the same version based on the number of replicas
-        SearchResponse searchResponse = client.client().prepareSearch()
-                .setSearchType(SearchType.SCAN)
-                .setQuery(matchAllQuery())
-                .setSize(50)
-                .setScroll(TimeValue.timeValueMinutes(2))
-                .execute().actionGet();
-        logger.info("Verifying versions for {} hits...", searchResponse.getHits().totalHits());
-
-        while (true) {
-            searchResponse = client.client().prepareSearchScroll(searchResponse.getScrollId()).setScroll(TimeValue.timeValueMinutes(2)).execute().actionGet();
-            if (searchResponse.getFailedShards() > 0) {
-                logger.warn("Search Failures " + Arrays.toString(searchResponse.getShardFailures()));
-            }
-            for (SearchHit hit : searchResponse.getHits()) {
-                long version = -1;
-                for (int i = 0; i < (numberOfReplicas + 1); i++) {
-                    GetResponse getResponse = client.client().prepareGet(hit.index(), hit.type(), hit.id()).execute().actionGet();
-                    if (version == -1) {
-                        version = getResponse.getVersion();
-                    } else {
-                        if (version != getResponse.getVersion()) {
-                            logger.warn("Doc {} has different version numbers {} and {}", hit.id(), version, getResponse.getVersion());
-                        }
-                    }
-                }
-            }
-            if (searchResponse.getHits().hits().length == 0) {
-                break;
-            }
-        }
-        logger.info("Done verifying versions");
-
-        client.close();
-        for (Node node : nodes) {
-            node.close();
-        }
-    }
-
-    private class Indexer extends Thread {
-
-        volatile boolean close = false;
-
-        volatile boolean closed = false;
-
-        @Override
-        public void run() {
-            Random random = new Random(0);
-            while (true) {
-                if (close) {
-                    closed = true;
-                    return;
-                }
-                try {
-                    indexDoc(random);
-                    Thread.sleep(indexerThrottle.millis());
-                } catch (Exception e) {
-                    logger.warn("failed to index / sleep", e);
-                }
-            }
-        }
-    }
-
-    private void indexDoc(Random random) throws Exception {
-        StringBuilder sb = new StringBuilder();
-        XContentBuilder json = XContentFactory.jsonBuilder().startObject()
-                .field("field", "value" + ThreadLocalRandom.current().nextInt());
-
-        int fields = Math.abs(ThreadLocalRandom.current().nextInt()) % numberOfFields;
-        for (int i = 0; i < fields; i++) {
-            json.field("num_" + i, ThreadLocalRandom.current().nextDouble());
-            int tokens = ThreadLocalRandom.current().nextInt() % textTokens;
-            sb.setLength(0);
-            for (int j = 0; j < tokens; j++) {
-                sb.append(Strings.randomBase64UUID(random)).append(' ');
-            }
-            json.field("text_" + i, sb.toString());
-        }
-
-        json.endObject();
-
-        String id = Long.toString(idCounter.incrementAndGet());
-        client.client().prepareIndex("test", "type1", id)
-                .setCreate(true)
-                .setSource(json)
-                .execute().actionGet();
-        indexCounter.incrementAndGet();
-    }
-
-    public static void main(String[] args) throws Exception {
-        System.setProperty("es.logger.prefix", "");
-
-        Settings settings = settingsBuilder()
-                .put("index.shard.check_on_startup", true)
-                .put("path.data", "data/data1,data/data2")
-                .build();
-
-        RollingRestartStressTest test = new RollingRestartStressTest()
-                .settings(settings)
-                .numberOfNodes(4)
-                .numberOfShards(5)
-                .numberOfReplicas(1)
-                .initialNumberOfDocs(1000)
-                .textTokens(150)
-                .numberOfFields(10)
-                .cleanNodeData(false)
-                .indexers(5)
-                .indexerThrottle(TimeValue.timeValueMillis(50))
-                .period(TimeValue.timeValueMinutes(3));
-
-        test.run();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/stresstest/search1/ConcurrentSearchSerializationBenchmark.java b/core/src/test/java/org/elasticsearch/stresstest/search1/ConcurrentSearchSerializationBenchmark.java
deleted file mode 100644
index a0f883b..0000000
--- a/core/src/test/java/org/elasticsearch/stresstest/search1/ConcurrentSearchSerializationBenchmark.java
+++ /dev/null
@@ -1,110 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.stresstest.search1;
-
-import com.carrotsearch.randomizedtesting.generators.RandomStrings;
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.index.IndexResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
-import org.elasticsearch.search.SearchHit;
-import org.junit.Ignore;
-
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.ThreadLocalRandom;
-
-/**
- * Tests that data don't get corrupted while reading it over the streams.
- * <p/>
- * See: https://github.com/elasticsearch/elasticsearch/issues/1686.
- */
-public class ConcurrentSearchSerializationBenchmark {
-
-    public static void main(String[] args) throws Exception {
-
-        Node node1 = NodeBuilder.nodeBuilder().node();
-        Node node2 = NodeBuilder.nodeBuilder().node();
-        Node node3 = NodeBuilder.nodeBuilder().node();
-
-        final Client client = node1.client();
-
-        System.out.println("Indexing...");
-        final String data = RandomStrings.randomAsciiOfLength(ThreadLocalRandom.current(), 100);
-        final CountDownLatch latch1 = new CountDownLatch(100);
-        for (int i = 0; i < 100; i++) {
-            client.prepareIndex("test", "type", Integer.toString(i))
-                    .setSource("field", data)
-                    .execute(new ActionListener<IndexResponse>() {
-                        @Override
-                        public void onResponse(IndexResponse indexResponse) {
-                            latch1.countDown();
-                        }
-
-                        @Override
-                        public void onFailure(Throwable e) {
-                            latch1.countDown();
-                        }
-                    });
-        }
-        latch1.await();
-        System.out.println("Indexed");
-
-        System.out.println("searching...");
-        Thread[] threads = new Thread[10];
-        final CountDownLatch latch = new CountDownLatch(threads.length);
-        for (int i = 0; i < threads.length; i++) {
-            threads[i] = new Thread(new Runnable() {
-                @Override
-                public void run() {
-                    for (int i = 0; i < 1000; i++) {
-                        SearchResponse searchResponse = client.prepareSearch("test")
-                                .setQuery(QueryBuilders.matchAllQuery())
-                                .setSize(i % 100)
-                                .execute().actionGet();
-                        for (SearchHit hit : searchResponse.getHits()) {
-                            try {
-                                if (!hit.sourceAsMap().get("field").equals(data)) {
-                                    System.err.println("Field not equal!");
-                                }
-                            } catch (Exception e) {
-                                e.printStackTrace();
-                            }
-                        }
-                    }
-                    latch.countDown();
-                }
-            });
-        }
-        for (Thread thread : threads) {
-            thread.start();
-        }
-
-        latch.await();
-
-        System.out.println("done searching");
-        client.close();
-        node1.close();
-        node2.close();
-        node3.close();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/stresstest/search1/ParentChildStressTest.java b/core/src/test/java/org/elasticsearch/stresstest/search1/ParentChildStressTest.java
deleted file mode 100644
index 23943f9..0000000
--- a/core/src/test/java/org/elasticsearch/stresstest/search1/ParentChildStressTest.java
+++ /dev/null
@@ -1,237 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.stresstest.search1;
-
-import org.elasticsearch.action.admin.indices.create.CreateIndexRequest;
-import org.elasticsearch.action.index.IndexRequestBuilder;
-import org.elasticsearch.action.search.SearchRequest;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.action.search.ShardSearchFailure;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
-import org.elasticsearch.search.SearchHit;
-import org.elasticsearch.transport.RemoteTransportException;
-
-import java.io.IOException;
-import java.util.*;
-
-
-public class ParentChildStressTest {
-
-    private Node elasticNode;
-    private Client client;
-
-    private static final String PARENT_TYPE_NAME = "content";
-    private static final String CHILD_TYPE_NAME = "contentFiles";
-    private static final String INDEX_NAME = "acme";
-
-    /**
-     * Constructor.  Initialize elastic and create the index/mapping
-     */
-    public ParentChildStressTest() {
-        NodeBuilder nodeBuilder = NodeBuilder.nodeBuilder();
-        Settings settings = nodeBuilder.settings()
-                .build();
-        this.elasticNode = nodeBuilder.settings(settings).client(true).node();
-        this.client = this.elasticNode.client();
-
-        String mapping =
-                "{\"contentFiles\": {" +
-                        "\"_parent\": {" +
-                        "\"type\" : \"content\"" +
-                        "}}}";
-
-        try {
-            client.admin().indices().create(new CreateIndexRequest(INDEX_NAME).mapping(CHILD_TYPE_NAME, mapping)).actionGet();
-        } catch (RemoteTransportException e) {
-            // usually means the index is already created.
-        }
-    }
-
-    public void shutdown() throws IOException {
-        client.close();
-        elasticNode.close();
-    }
-
-    /**
-     * Deletes the item from both the parent and child type locations.
-     */
-    public void deleteById(String id) {
-        client.prepareDelete(INDEX_NAME, PARENT_TYPE_NAME, id).execute().actionGet();
-        client.prepareDelete(INDEX_NAME, CHILD_TYPE_NAME, id).execute().actionGet();
-    }
-
-    /**
-     * Index a parent doc
-     */
-    public void indexParent(String id, Map<String, Object> objectMap) throws IOException {
-        XContentBuilder builder = XContentFactory.jsonBuilder();
-
-        // index content
-        client.prepareIndex(INDEX_NAME, PARENT_TYPE_NAME, id).setSource(builder.map(objectMap)).execute().actionGet();
-    }
-
-    /**
-     * Index the file as a child doc
-     */
-    public void indexChild(String id, Map<String, Object> objectMap) throws IOException {
-        XContentBuilder builder = XContentFactory.jsonBuilder();
-
-        IndexRequestBuilder indexRequestbuilder = client.prepareIndex(INDEX_NAME, CHILD_TYPE_NAME, id);
-        indexRequestbuilder = indexRequestbuilder.setParent(id);
-        indexRequestbuilder = indexRequestbuilder.setSource(builder.map(objectMap));
-        indexRequestbuilder.execute().actionGet();
-    }
-
-    /**
-     * Execute a search based on a JSON String in QueryDSL format.
-     * <p/>
-     * Throws a RuntimeException if there are any shard failures to
-     * elevate the visibility of the problem.
-     */
-    public List<String> executeSearch(String source) {
-        SearchRequest request = Requests.searchRequest(INDEX_NAME).source(source);
-
-        List<ShardSearchFailure> failures;
-        SearchResponse response;
-
-        response = client.search(request).actionGet();
-        failures = Arrays.asList(response.getShardFailures());
-
-        // throw an exception so that we see the shard failures
-        if (failures.size() != 0) {
-            String failuresStr = failures.toString();
-            if (!failuresStr.contains("reason [No active shards]")) {
-                throw new RuntimeException(failures.toString());
-            }
-        }
-
-        ArrayList<String> results = new ArrayList<>();
-        if (response != null) {
-            for (SearchHit hit : response.getHits()) {
-                String sourceStr = hit.sourceAsString();
-                results.add(sourceStr);
-            }
-        }
-        return results;
-    }
-
-    /**
-     * Create a document as a parent and index it.
-     * Load a file and index it as a child.
-     */
-    public String indexDoc() throws IOException {
-        String id = UUID.randomUUID().toString();
-
-        Map<String, Object> objectMap = new HashMap<>();
-        objectMap.put("title", "this is a document");
-
-        Map<String, Object> objectMap2 = new HashMap<>();
-        objectMap2.put("description", "child test");
-
-        this.indexParent(id, objectMap);
-        this.indexChild(id, objectMap2);
-        return id;
-    }
-
-    /**
-     * Perform the has_child query for the doc.
-     * <p/>
-     * Since it might take time to get indexed, it
-     * loops until it finds the doc.
-     */
-    public void searchDocByChild() throws InterruptedException {
-        String dslString =
-                "{\"query\":{" +
-                        "\"has_child\":{" +
-                        "\"query\":{" +
-                        "\"field\":{" +
-                        "\"description\":\"child test\"}}," +
-                        "\"type\":\"contentFiles\"}}}";
-
-        int numTries = 0;
-        List<String> items = new ArrayList<>();
-
-        while (items.size() != 1 && numTries < 20) {
-            items = executeSearch(dslString);
-
-            numTries++;
-            if (items.size() != 1) {
-                Thread.sleep(250);
-            }
-        }
-        if (items.size() != 1) {
-            System.out.println("Exceeded number of retries");
-            System.exit(1);
-        }
-    }
-
-    /**
-     * Program to loop on:
-     * create parent/child doc
-     * search for the doc
-     * delete the doc
-     * repeat the above until shard failure.
-     * <p/>
-     * Eventually fails with:
-     * <p/>
-     * [shard [[74wz0lrXRSmSOsJOqgPvlw][acme][1]], reason [RemoteTransportException
-     * [[Kismet][inet[/10.10.30.52:9300]][search/phase/query]]; nested:
-     * QueryPhaseExecutionException[[acme][1]:
-     * query[ConstantScore(child_filter[contentFiles
-     * /content](filtered(file:mission
-     * file:statement)->FilterCacheFilterWrapper(
-     * _type:contentFiles)))],from[0],size[10]: Query Failed [Failed to execute
-     * child query [filtered(file:mission
-     * file:statement)->FilterCacheFilterWrapper(_type:contentFiles)]]]; nested:
-     * ]]
-     *
-     * @param args
-     */
-    public static void main(String[] args) throws IOException {
-        ParentChildStressTest elasticTest = new ParentChildStressTest();
-        try {
-            // loop a bunch of times - usually fails before the count is done.
-            int NUM_LOOPS = 1000;
-            System.out.println();
-            System.out.println("Looping [" + NUM_LOOPS + "] times:");
-            System.out.println();
-            for (int i = 0; i < NUM_LOOPS; i++) {
-                String id = elasticTest.indexDoc();
-
-                elasticTest.searchDocByChild();
-
-                elasticTest.deleteById(id);
-
-                System.out.println("    Success: " + i);
-            }
-            elasticTest.shutdown();
-        } catch (Exception e) {
-            e.printStackTrace();
-        } finally {
-            elasticTest.shutdown();
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/stresstest/search1/Search1StressBenchmark.java b/core/src/test/java/org/elasticsearch/stresstest/search1/Search1StressBenchmark.java
deleted file mode 100644
index 001c234..0000000
--- a/core/src/test/java/org/elasticsearch/stresstest/search1/Search1StressBenchmark.java
+++ /dev/null
@@ -1,374 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.stresstest.search1;
-
-import org.elasticsearch.action.search.SearchRequestBuilder;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.action.search.SearchType;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.Loggers;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
-import org.elasticsearch.search.SearchHit;
-import org.elasticsearch.search.sort.SortOrder;
-import org.junit.Ignore;
-
-import java.util.Arrays;
-import java.util.concurrent.ThreadLocalRandom;
-import java.util.concurrent.atomic.AtomicLong;
-
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-
-public class Search1StressBenchmark {
-
-    private final ESLogger logger = Loggers.getLogger(getClass());
-
-
-    private int numberOfNodes = 4;
-
-    private int indexers = 0;
-    private SizeValue preIndexDocs = new SizeValue(0);
-    private TimeValue indexerThrottle = TimeValue.timeValueMillis(100);
-    private int searchers = 0;
-    private TimeValue searcherThrottle = TimeValue.timeValueMillis(20);
-    private int numberOfIndices = 10;
-    private int numberOfTypes = 4;
-    private int numberOfValues = 20;
-    private int numberOfHits = 300;
-    private TimeValue flusherThrottle = TimeValue.timeValueMillis(1000);
-
-    private Settings settings = Settings.Builder.EMPTY_SETTINGS;
-
-    private TimeValue period = TimeValue.timeValueMinutes(20);
-
-    private AtomicLong indexCounter = new AtomicLong();
-    private AtomicLong searchCounter = new AtomicLong();
-
-
-    private Node client;
-
-    public Search1StressBenchmark setNumberOfNodes(int numberOfNodes) {
-        this.numberOfNodes = numberOfNodes;
-        return this;
-    }
-
-    public Search1StressBenchmark setPreIndexDocs(SizeValue preIndexDocs) {
-        this.preIndexDocs = preIndexDocs;
-        return this;
-    }
-
-    public Search1StressBenchmark setIndexers(int indexers) {
-        this.indexers = indexers;
-        return this;
-    }
-
-    public Search1StressBenchmark setIndexerThrottle(TimeValue indexerThrottle) {
-        this.indexerThrottle = indexerThrottle;
-        return this;
-    }
-
-    public Search1StressBenchmark setSearchers(int searchers) {
-        this.searchers = searchers;
-        return this;
-    }
-
-    public Search1StressBenchmark setSearcherThrottle(TimeValue searcherThrottle) {
-        this.searcherThrottle = searcherThrottle;
-        return this;
-    }
-
-    public Search1StressBenchmark setNumberOfIndices(int numberOfIndices) {
-        this.numberOfIndices = numberOfIndices;
-        return this;
-    }
-
-    public Search1StressBenchmark setNumberOfTypes(int numberOfTypes) {
-        this.numberOfTypes = numberOfTypes;
-        return this;
-    }
-
-    public Search1StressBenchmark setNumberOfValues(int numberOfValues) {
-        this.numberOfValues = numberOfValues;
-        return this;
-    }
-
-    public Search1StressBenchmark setNumberOfHits(int numberOfHits) {
-        this.numberOfHits = numberOfHits;
-        return this;
-    }
-
-    public Search1StressBenchmark setFlusherThrottle(TimeValue flusherThrottle) {
-        this.flusherThrottle = flusherThrottle;
-        return this;
-    }
-
-    public Search1StressBenchmark setSettings(Settings settings) {
-        this.settings = settings;
-        return this;
-    }
-
-    public Search1StressBenchmark setPeriod(TimeValue period) {
-        this.period = period;
-        return this;
-    }
-
-    private String nextIndex() {
-        return "test" + Math.abs(ThreadLocalRandom.current().nextInt()) % numberOfIndices;
-    }
-
-    private String nextType() {
-        return "type" + Math.abs(ThreadLocalRandom.current().nextInt()) % numberOfTypes;
-    }
-
-    private int nextNumValue() {
-        return Math.abs(ThreadLocalRandom.current().nextInt()) % numberOfValues;
-    }
-
-    private String nextFieldValue() {
-        return "value" + Math.abs(ThreadLocalRandom.current().nextInt()) % numberOfValues;
-    }
-
-    private class Searcher extends Thread {
-
-        volatile boolean close = false;
-
-        volatile boolean closed = false;
-
-        @Override
-        public void run() {
-            while (true) {
-                if (close) {
-                    closed = true;
-                    return;
-                }
-                try {
-                    String indexName = nextIndex();
-                    SearchRequestBuilder builder = client.client().prepareSearch(indexName);
-                    if (ThreadLocalRandom.current().nextBoolean()) {
-                        builder.addSort("num", SortOrder.DESC);
-                    } else if (ThreadLocalRandom.current().nextBoolean()) {
-                        // add a _score based sorting, won't do any sorting, just to test...
-                        builder.addSort("_score", SortOrder.DESC);
-                    }
-                    if (ThreadLocalRandom.current().nextBoolean()) {
-                        builder.setSearchType(SearchType.DFS_QUERY_THEN_FETCH);
-                    }
-                    int size = Math.abs(ThreadLocalRandom.current().nextInt()) % numberOfHits;
-                    builder.setSize(size);
-                    if (ThreadLocalRandom.current().nextBoolean()) {
-                        // update from
-                        builder.setFrom(size / 2);
-                    }
-                    String value = nextFieldValue();
-                    builder.setQuery(termQuery("field", value));
-                    searchCounter.incrementAndGet();
-                    SearchResponse searchResponse = builder.execute().actionGet();
-                    if (searchResponse.getFailedShards() > 0) {
-                        logger.warn("failed search " + Arrays.toString(searchResponse.getShardFailures()));
-                    }
-                    // verify that all come from the requested index
-                    for (SearchHit hit : searchResponse.getHits()) {
-                        if (!hit.shard().index().equals(indexName)) {
-                            logger.warn("got wrong index, asked for [{}], got [{}]", indexName, hit.shard().index());
-                        }
-                    }
-                    // verify that all has the relevant value
-                    for (SearchHit hit : searchResponse.getHits()) {
-                        if (!value.equals(hit.sourceAsMap().get("field"))) {
-                            logger.warn("got wrong field, asked for [{}], got [{}]", value, hit.sourceAsMap().get("field"));
-                        }
-                    }
-                    Thread.sleep(searcherThrottle.millis());
-                } catch (Exception e) {
-                    logger.warn("failed to search", e);
-                }
-            }
-        }
-    }
-
-    private class Indexer extends Thread {
-
-        volatile boolean close = false;
-
-        volatile boolean closed = false;
-
-        @Override
-        public void run() {
-            while (true) {
-                if (close) {
-                    closed = true;
-                    return;
-                }
-                try {
-                    indexDoc();
-                    Thread.sleep(indexerThrottle.millis());
-                } catch (Exception e) {
-                    logger.warn("failed to index / sleep", e);
-                }
-            }
-        }
-    }
-
-    private class Flusher extends Thread {
-        volatile boolean close = false;
-
-        volatile boolean closed = false;
-
-        @Override
-        public void run() {
-            while (true) {
-                if (close) {
-                    closed = true;
-                    return;
-                }
-                try {
-                    client.client().admin().indices().prepareFlush().execute().actionGet();
-                    Thread.sleep(indexerThrottle.millis());
-                } catch (Exception e) {
-                    logger.warn("failed to flush / sleep", e);
-                }
-            }
-        }
-    }
-
-    private void indexDoc() throws Exception {
-        XContentBuilder json = XContentFactory.jsonBuilder().startObject()
-                .field("num", nextNumValue())
-                .field("field", nextFieldValue());
-
-        json.endObject();
-
-        client.client().prepareIndex(nextIndex(), nextType())
-                .setSource(json)
-                .execute().actionGet();
-        indexCounter.incrementAndGet();
-    }
-
-    public void run() throws Exception {
-        Node[] nodes = new Node[numberOfNodes];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = NodeBuilder.nodeBuilder().settings(settings).node();
-        }
-        client = NodeBuilder.nodeBuilder().settings(settings).client(true).node();
-
-        for (int i = 0; i < numberOfIndices; i++) {
-            client.client().admin().indices().prepareCreate("test" + i).execute().actionGet();
-        }
-
-        logger.info("Pre indexing docs [{}]...", preIndexDocs);
-        for (long i = 0; i < preIndexDocs.singles(); i++) {
-            indexDoc();
-        }
-        logger.info("Done pre indexing docs [{}]", preIndexDocs);
-
-        Indexer[] indexerThreads = new Indexer[indexers];
-        for (int i = 0; i < indexerThreads.length; i++) {
-            indexerThreads[i] = new Indexer();
-        }
-        for (Indexer indexerThread : indexerThreads) {
-            indexerThread.start();
-        }
-
-        Thread.sleep(10000);
-
-        Searcher[] searcherThreads = new Searcher[searchers];
-        for (int i = 0; i < searcherThreads.length; i++) {
-            searcherThreads[i] = new Searcher();
-        }
-        for (Searcher searcherThread : searcherThreads) {
-            searcherThread.start();
-        }
-
-        Flusher flusher = null;
-        if (flusherThrottle.millis() > 0) {
-            flusher = new Flusher();
-            flusher.start();
-        }
-
-        long testStart = System.currentTimeMillis();
-
-        while (true) {
-            Thread.sleep(5000);
-            if ((System.currentTimeMillis() - testStart) > period.millis()) {
-                break;
-            }
-        }
-
-        System.out.println("DONE, closing .....");
-
-        if (flusher != null) {
-            flusher.close = true;
-        }
-
-        for (Searcher searcherThread : searcherThreads) {
-            searcherThread.close = true;
-        }
-
-        for (Indexer indexerThread : indexerThreads) {
-            indexerThread.close = true;
-        }
-
-        Thread.sleep(indexerThrottle.millis() + 10000);
-
-        if (flusher != null && !flusher.closed) {
-            logger.warn("flusher not closed!");
-        }
-        for (Searcher searcherThread : searcherThreads) {
-            if (!searcherThread.closed) {
-                logger.warn("search thread not closed!");
-            }
-        }
-        for (Indexer indexerThread : indexerThreads) {
-            if (!indexerThread.closed) {
-                logger.warn("index thread not closed!");
-            }
-        }
-
-        client.close();
-        for (Node node : nodes) {
-            node.close();
-        }
-
-        System.out.println("********** DONE, indexed [" + indexCounter.get() + "], searched [" + searchCounter.get() + "]");
-    }
-
-    public static void main(String[] args) throws Exception {
-        Search1StressBenchmark test = new Search1StressBenchmark()
-                .setPeriod(TimeValue.timeValueMinutes(10))
-                .setNumberOfNodes(2)
-                .setPreIndexDocs(SizeValue.parseSizeValue("100"))
-                .setIndexers(2)
-                .setIndexerThrottle(TimeValue.timeValueMillis(100))
-                .setSearchers(10)
-                .setSearcherThrottle(TimeValue.timeValueMillis(10))
-                .setFlusherThrottle(TimeValue.timeValueMillis(1000))
-                .setNumberOfIndices(10)
-                .setNumberOfTypes(5)
-                .setNumberOfValues(50)
-                .setNumberOfHits(300);
-
-        test.run();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java b/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java
index 35dd8ca..2dc270c 100644
--- a/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java
+++ b/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java
@@ -191,7 +191,7 @@ import static org.hamcrest.Matchers.*;
  * should be used, here is an example:
  * <pre>
  *
- * @ClusterScope(scope=Scope.TEST) public class SomeIntegrationTest extends ESIntegTestCase {
+ * @ClusterScope(scope=Scope.TEST) public class SomeIT extends ESIntegTestCase {
  * @Test public void testMethod() {}
  * }
  * </pre>
@@ -204,7 +204,7 @@ import static org.hamcrest.Matchers.*;
  * <p/>
  *  <pre>
  * @ClusterScope(scope=Scope.SUITE, numDataNodes=3)
- * public class SomeIntegrationTest extends ESIntegTestCase {
+ * public class SomeIT extends ESIntegTestCase {
  * @Test public void testMethod() {}
  * }
  * </pre>
@@ -344,7 +344,6 @@ public abstract class ESIntegTestCase extends ESTestCase {
             cluster().beforeTest(getRandom(), getPerTestTransportClientRatio());
             cluster().wipe();
             randomIndexTemplate();
-            printTestMessage("before");
         } catch (OutOfMemoryError e) {
             if (e.getMessage().contains("unable to create new native thread")) {
                 ESTestCase.printStackDump(logger);
@@ -354,7 +353,7 @@ public abstract class ESIntegTestCase extends ESTestCase {
     }
 
     private void printTestMessage(String message) {
-        if (isSuiteScopedTest(getClass())) {
+        if (isSuiteScopedTest(getClass()) && (getTestName().equals("<unknown>"))) {
             logger.info("[{}]: {} suite", getTestClass().getSimpleName(), message);
         } else {
             logger.info("[{}#{}]: {} test", getTestClass().getSimpleName(), getTestName(), message);
@@ -593,7 +592,6 @@ public abstract class ESIntegTestCase extends ESTestCase {
         boolean success = false;
         try {
             final Scope currentClusterScope = getCurrentClusterScope();
-            printTestMessage("cleaning up after");
             clearDisruptionScheme();
             try {
                 if (cluster() != null) {
@@ -618,7 +616,6 @@ public abstract class ESIntegTestCase extends ESTestCase {
                     clearClusters(); // it is ok to leave persistent / transient cluster state behind if scope is TEST
                 }
             }
-            printTestMessage("cleaned up after");
             success = true;
         } finally {
             if (!success) {
@@ -1953,20 +1950,26 @@ public abstract class ESIntegTestCase extends ESTestCase {
 
     @Before
     public final void before() throws Exception {
+
         if (runTestScopeLifecycle()) {
+            printTestMessage("setup");
             beforeInternal();
         }
+        printTestMessage("starting");
     }
 
 
     @After
     public final void after() throws Exception {
+        printTestMessage("finished");
         // Deleting indices is going to clear search contexts implicitely so we
         // need to check that there are no more in-flight search contexts before
         // we remove indices
         super.ensureAllSearchContextsReleased();
         if (runTestScopeLifecycle()) {
+            printTestMessage("cleaning up after");
             afterInternal(false);
+            printTestMessage("cleaned up after");
         }
     }
 
@@ -1974,6 +1977,7 @@ public abstract class ESIntegTestCase extends ESTestCase {
     public static void afterClass() throws Exception {
         if (!runTestScopeLifecycle()) {
             try {
+                INSTANCE.printTestMessage("cleaning up after");
                 INSTANCE.afterInternal(true);
             } finally {
                 INSTANCE = null;
@@ -1999,6 +2003,7 @@ public abstract class ESIntegTestCase extends ESTestCase {
             INSTANCE = (ESIntegTestCase) targetClass.newInstance();
             boolean success = false;
             try {
+                INSTANCE.printTestMessage("setup");
                 INSTANCE.beforeInternal();
                 INSTANCE.setupSuiteScopeCluster();
                 success = true;
diff --git a/core/src/test/java/org/elasticsearch/test/ESTestCase.java b/core/src/test/java/org/elasticsearch/test/ESTestCase.java
index 3057982..3624b0a 100644
--- a/core/src/test/java/org/elasticsearch/test/ESTestCase.java
+++ b/core/src/test/java/org/elasticsearch/test/ESTestCase.java
@@ -378,27 +378,17 @@ public abstract class ESTestCase extends LuceneTestCase {
         return RandomizedTest.randomRealisticUnicodeOfCodepointLength(codePoints);
     }
 
-    public static String[] generateRandomStringArray(int maxArraySize, int maxStringSize, boolean allowNull, boolean allowEmpty) {
+    public static String[] generateRandomStringArray(int maxArraySize, int maxStringSize, boolean allowNull) {
         if (allowNull && random().nextBoolean()) {
             return null;
         }
-        int arraySize = randomIntBetween(allowEmpty ? 0 : 1, maxArraySize);
-        String[] array = new String[arraySize];
-        for (int i = 0; i < arraySize; i++) {
+        String[] array = new String[random().nextInt(maxArraySize)]; // allow empty arrays
+        for (int i = 0; i < array.length; i++) {
             array[i] = RandomStrings.randomAsciiOfLength(random(), maxStringSize);
         }
         return array;
     }
 
-    public static String[] generateRandomStringArray(int maxArraySize, int maxStringSize, boolean allowNull) {
-        return generateRandomStringArray(maxArraySize, maxStringSize, allowNull, true);
-    }
-
-    public static String randomTimeValue() {
-        final String[] values = new String[]{"d", "H", "ms", "s", "S", "w"};
-        return randomIntBetween(0, 1000) + randomFrom(values);
-    }
-
     /**
      * Runs the code block for 10 seconds waiting for no assertion to trip.
      */
diff --git a/core/src/test/java/org/elasticsearch/test/TestSearchContext.java b/core/src/test/java/org/elasticsearch/test/TestSearchContext.java
index 48725f4..56766b7 100644
--- a/core/src/test/java/org/elasticsearch/test/TestSearchContext.java
+++ b/core/src/test/java/org/elasticsearch/test/TestSearchContext.java
@@ -83,6 +83,12 @@ public class TestSearchContext extends SearchContext {
     final ThreadPool threadPool;
     final Map<Class<?>, Collector> queryCollectors = new HashMap<>();
     final IndexShard indexShard;
+    final Counter timeEstimateCounter = Counter.newCounter();
+    final QuerySearchResult queryResult = new QuerySearchResult();
+    ParsedQuery originalQuery;
+    ParsedQuery postFilter;
+    Query query;
+    Float minScore;
 
     ContextIndexSearcher searcher;
     int size;
@@ -363,12 +369,13 @@ public class TestSearchContext extends SearchContext {
 
     @Override
     public SearchContext minimumScore(float minimumScore) {
-        return null;
+        this.minScore = minimumScore;
+        return this;
     }
 
     @Override
     public Float minimumScore() {
-        return null;
+        return minScore;
     }
 
     @Override
@@ -393,12 +400,13 @@ public class TestSearchContext extends SearchContext {
 
     @Override
     public SearchContext parsedPostFilter(ParsedQuery postFilter) {
-        return null;
+        this.postFilter = postFilter;
+        return this;
     }
 
     @Override
     public ParsedQuery parsedPostFilter() {
-        return null;
+        return postFilter;
     }
 
     @Override
@@ -408,17 +416,19 @@ public class TestSearchContext extends SearchContext {
 
     @Override
     public SearchContext parsedQuery(ParsedQuery query) {
-        return null;
+        this.originalQuery = query;
+        this.query = query.query();
+        return this;
     }
 
     @Override
     public ParsedQuery parsedQuery() {
-        return null;
+        return originalQuery;
     }
 
     @Override
     public Query query() {
-        return null;
+        return query;
     }
 
     @Override
@@ -537,7 +547,7 @@ public class TestSearchContext extends SearchContext {
 
     @Override
     public QuerySearchResult queryResult() {
-        return null;
+        return queryResult;
     }
 
     @Override
@@ -580,7 +590,7 @@ public class TestSearchContext extends SearchContext {
 
     @Override
     public Counter timeEstimateCounter() {
-        throw new UnsupportedOperationException();
+        return timeEstimateCounter;
     }
 
     @Override
diff --git a/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java b/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
index c253a75..d9b9b49 100644
--- a/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
+++ b/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
@@ -80,7 +80,7 @@ public class AssertingLocalTransport extends LocalTransport {
         ElasticsearchAssertions.assertVersionSerializable(VersionUtils.randomVersionBetween(random, minVersion, maxVersion), response);
         super.handleParsedResponse(response, handler);
     }
-
+    
     @Override
     public void sendRequest(final DiscoveryNode node, final long requestId, final String action, final TransportRequest request, TransportRequestOptions options) throws IOException, TransportException {
         ElasticsearchAssertions.assertVersionSerializable(VersionUtils.randomVersionBetween(random, minVersion, maxVersion), request);
diff --git a/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java b/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java
new file mode 100644
index 0000000..4499d92
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java
@@ -0,0 +1,1221 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.transport;
+
+import com.google.common.collect.ImmutableMap;
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.transport.TransportAddress;
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.junit.annotations.TestLogging;
+import org.elasticsearch.test.transport.MockTransportService;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.Semaphore;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicReference;
+
+import static org.elasticsearch.transport.TransportRequestOptions.options;
+import static org.hamcrest.Matchers.*;
+
+/**
+ *
+ */
+public abstract class AbstractSimpleTransportTestCase extends ESTestCase {
+
+    protected ThreadPool threadPool;
+
+    protected static final Version version0 = Version.fromId(/*0*/99);
+    protected DiscoveryNode nodeA;
+    protected MockTransportService serviceA;
+
+    protected static final Version version1 = Version.fromId(199);
+    protected DiscoveryNode nodeB;
+    protected MockTransportService serviceB;
+
+    protected abstract MockTransportService build(Settings settings, Version version, NamedWriteableRegistry namedWriteableRegistry);
+
+    @Override
+    @Before
+    public void setUp() throws Exception {
+        super.setUp();
+        threadPool = new ThreadPool(getClass().getName());
+        serviceA = build(
+                Settings.builder().put("name", "TS_A", TransportService.SETTING_TRACE_LOG_INCLUDE, "", TransportService.SETTING_TRACE_LOG_EXCLUDE, "NOTHING").build(),
+                version0, new NamedWriteableRegistry()
+        );
+        nodeA = new DiscoveryNode("TS_A", "TS_A", serviceA.boundAddress().publishAddress(), ImmutableMap.<String, String>of(), version0);
+        serviceB = build(
+                Settings.builder().put("name", "TS_B", TransportService.SETTING_TRACE_LOG_INCLUDE, "", TransportService.SETTING_TRACE_LOG_EXCLUDE, "NOTHING").build(),
+                version1, new NamedWriteableRegistry()
+        );
+        nodeB = new DiscoveryNode("TS_B", "TS_B", serviceB.boundAddress().publishAddress(), ImmutableMap.<String, String>of(), version1);
+
+        // wait till all nodes are properly connected and the event has been sent, so tests in this class
+        // will not get this callback called on the connections done in this setup
+        final boolean useLocalNode = randomBoolean();
+        final CountDownLatch latch = new CountDownLatch(useLocalNode ? 2 : 4);
+        TransportConnectionListener waitForConnection = new TransportConnectionListener() {
+            @Override
+            public void onNodeConnected(DiscoveryNode node) {
+                latch.countDown();
+            }
+
+            @Override
+            public void onNodeDisconnected(DiscoveryNode node) {
+                fail("disconnect should not be called " + node);
+            }
+        };
+        serviceA.addConnectionListener(waitForConnection);
+        serviceB.addConnectionListener(waitForConnection);
+
+        if (useLocalNode) {
+            logger.info("--> using local node optimization");
+            serviceA.setLocalNode(nodeA);
+            serviceB.setLocalNode(nodeB);
+        } else {
+            logger.info("--> actively connecting to local node");
+            serviceA.connectToNode(nodeA);
+            serviceB.connectToNode(nodeB);
+        }
+
+        serviceA.connectToNode(nodeB);
+        serviceB.connectToNode(nodeA);
+
+        assertThat("failed to wait for all nodes to connect", latch.await(5, TimeUnit.SECONDS), equalTo(true));
+        serviceA.removeConnectionListener(waitForConnection);
+        serviceB.removeConnectionListener(waitForConnection);
+    }
+
+    @Override
+    @After
+    public void tearDown() throws Exception {
+        super.tearDown();
+        serviceA.close();
+        serviceB.close();
+        terminate(threadPool);
+    }
+
+    @Test
+    public void testHelloWorld() {
+        serviceA.registerRequestHandler("sayHello", StringMessageRequest.class, ThreadPool.Names.GENERIC, new TransportRequestHandler<StringMessageRequest>() {
+            @Override
+            public void messageReceived(StringMessageRequest request, TransportChannel channel) {
+                assertThat("moshe", equalTo(request.message));
+                try {
+                    channel.sendResponse(new StringMessageResponse("hello " + request.message));
+                } catch (IOException e) {
+                    e.printStackTrace();
+                    assertThat(e.getMessage(), false, equalTo(true));
+                }
+            }
+        });
+
+        TransportFuture<StringMessageResponse> res = serviceB.submitRequest(nodeA, "sayHello",
+                new StringMessageRequest("moshe"), new BaseTransportResponseHandler<StringMessageResponse>() {
+                    @Override
+                    public StringMessageResponse newInstance() {
+                        return new StringMessageResponse();
+                    }
+
+                    @Override
+                    public String executor() {
+                        return ThreadPool.Names.GENERIC;
+                    }
+
+                    @Override
+                    public void handleResponse(StringMessageResponse response) {
+                        assertThat("hello moshe", equalTo(response.message));
+                    }
+
+                    @Override
+                    public void handleException(TransportException exp) {
+                        exp.printStackTrace();
+                        assertThat("got exception instead of a response: " + exp.getMessage(), false, equalTo(true));
+                    }
+                });
+
+        try {
+            StringMessageResponse message = res.get();
+            assertThat("hello moshe", equalTo(message.message));
+        } catch (Exception e) {
+            assertThat(e.getMessage(), false, equalTo(true));
+        }
+
+        res = serviceB.submitRequest(nodeA, "sayHello",
+                new StringMessageRequest("moshe"), TransportRequestOptions.options().withCompress(true), new BaseTransportResponseHandler<StringMessageResponse>() {
+                    @Override
+                    public StringMessageResponse newInstance() {
+                        return new StringMessageResponse();
+                    }
+
+                    @Override
+                    public String executor() {
+                        return ThreadPool.Names.GENERIC;
+                    }
+
+                    @Override
+                    public void handleResponse(StringMessageResponse response) {
+                        assertThat("hello moshe", equalTo(response.message));
+                    }
+
+                    @Override
+                    public void handleException(TransportException exp) {
+                        exp.printStackTrace();
+                        assertThat("got exception instead of a response: " + exp.getMessage(), false, equalTo(true));
+                    }
+                });
+
+        try {
+            StringMessageResponse message = res.get();
+            assertThat("hello moshe", equalTo(message.message));
+        } catch (Exception e) {
+            assertThat(e.getMessage(), false, equalTo(true));
+        }
+
+        serviceA.removeHandler("sayHello");
+    }
+
+    @Test
+    public void testLocalNodeConnection() throws InterruptedException {
+        assertTrue("serviceA is not connected to nodeA", serviceA.nodeConnected(nodeA));
+        if (((TransportService) serviceA).getLocalNode() != null) {
+            // this should be a noop
+            serviceA.disconnectFromNode(nodeA);
+        }
+        final AtomicReference<Exception> exception = new AtomicReference<>();
+        serviceA.registerRequestHandler("localNode", StringMessageRequest.class, ThreadPool.Names.GENERIC, new TransportRequestHandler<StringMessageRequest>() {
+            @Override
+            public void messageReceived(StringMessageRequest request, TransportChannel channel) {
+                try {
+                    channel.sendResponse(new StringMessageResponse(request.message));
+                } catch (IOException e) {
+                    exception.set(e);
+                }
+            }
+        });
+        final AtomicReference<String> responseString = new AtomicReference<>();
+        final CountDownLatch responseLatch = new CountDownLatch(1);
+        serviceA.sendRequest(nodeA, "localNode", new StringMessageRequest("test"), new TransportResponseHandler<StringMessageResponse>() {
+            @Override
+            public StringMessageResponse newInstance() {
+                return new StringMessageResponse();
+            }
+
+            @Override
+            public void handleResponse(StringMessageResponse response) {
+                responseString.set(response.message);
+                responseLatch.countDown();
+            }
+
+            @Override
+            public void handleException(TransportException exp) {
+                exception.set(exp);
+                responseLatch.countDown();
+            }
+
+            @Override
+            public String executor() {
+                return ThreadPool.Names.GENERIC;
+            }
+        });
+        responseLatch.await();
+        assertNull(exception.get());
+        assertThat(responseString.get(), equalTo("test"));
+    }
+
+    @Test
+    public void testVoidMessageCompressed() {
+        serviceA.registerRequestHandler("sayHello", TransportRequest.Empty.class, ThreadPool.Names.GENERIC, new TransportRequestHandler<TransportRequest.Empty>() {
+            @Override
+            public void messageReceived(TransportRequest.Empty request, TransportChannel channel) {
+                try {
+                    channel.sendResponse(TransportResponse.Empty.INSTANCE, TransportResponseOptions.options().withCompress(true));
+                } catch (IOException e) {
+                    e.printStackTrace();
+                    assertThat(e.getMessage(), false, equalTo(true));
+                }
+            }
+        });
+
+        TransportFuture<TransportResponse.Empty> res = serviceB.submitRequest(nodeA, "sayHello",
+                TransportRequest.Empty.INSTANCE, TransportRequestOptions.options().withCompress(true), new BaseTransportResponseHandler<TransportResponse.Empty>() {
+                    @Override
+                    public TransportResponse.Empty newInstance() {
+                        return TransportResponse.Empty.INSTANCE;
+                    }
+
+                    @Override
+                    public String executor() {
+                        return ThreadPool.Names.GENERIC;
+                    }
+
+                    @Override
+                    public void handleResponse(TransportResponse.Empty response) {
+                    }
+
+                    @Override
+                    public void handleException(TransportException exp) {
+                        exp.printStackTrace();
+                        assertThat("got exception instead of a response: " + exp.getMessage(), false, equalTo(true));
+                    }
+                });
+
+        try {
+            TransportResponse.Empty message = res.get();
+            assertThat(message, notNullValue());
+        } catch (Exception e) {
+            assertThat(e.getMessage(), false, equalTo(true));
+        }
+
+        serviceA.removeHandler("sayHello");
+    }
+
+    @Test
+    public void testHelloWorldCompressed() {
+        serviceA.registerRequestHandler("sayHello", StringMessageRequest.class, ThreadPool.Names.GENERIC, new TransportRequestHandler<StringMessageRequest>() {
+            @Override
+            public void messageReceived(StringMessageRequest request, TransportChannel channel) {
+                assertThat("moshe", equalTo(request.message));
+                try {
+                    channel.sendResponse(new StringMessageResponse("hello " + request.message), TransportResponseOptions.options().withCompress(true));
+                } catch (IOException e) {
+                    e.printStackTrace();
+                    assertThat(e.getMessage(), false, equalTo(true));
+                }
+            }
+        });
+
+        TransportFuture<StringMessageResponse> res = serviceB.submitRequest(nodeA, "sayHello",
+                new StringMessageRequest("moshe"), TransportRequestOptions.options().withCompress(true), new BaseTransportResponseHandler<StringMessageResponse>() {
+                    @Override
+                    public StringMessageResponse newInstance() {
+                        return new StringMessageResponse();
+                    }
+
+                    @Override
+                    public String executor() {
+                        return ThreadPool.Names.GENERIC;
+                    }
+
+                    @Override
+                    public void handleResponse(StringMessageResponse response) {
+                        assertThat("hello moshe", equalTo(response.message));
+                    }
+
+                    @Override
+                    public void handleException(TransportException exp) {
+                        exp.printStackTrace();
+                        assertThat("got exception instead of a response: " + exp.getMessage(), false, equalTo(true));
+                    }
+                });
+
+        try {
+            StringMessageResponse message = res.get();
+            assertThat("hello moshe", equalTo(message.message));
+        } catch (Exception e) {
+            assertThat(e.getMessage(), false, equalTo(true));
+        }
+
+        serviceA.removeHandler("sayHello");
+    }
+
+    @Test
+    public void testErrorMessage() {
+        serviceA.registerRequestHandler("sayHelloException", StringMessageRequest.class, ThreadPool.Names.GENERIC, new TransportRequestHandler<StringMessageRequest>() {
+            @Override
+            public void messageReceived(StringMessageRequest request, TransportChannel channel) throws Exception {
+                assertThat("moshe", equalTo(request.message));
+                throw new RuntimeException("bad message !!!");
+            }
+        });
+
+        TransportFuture<StringMessageResponse> res = serviceB.submitRequest(nodeA, "sayHelloException",
+                new StringMessageRequest("moshe"), new BaseTransportResponseHandler<StringMessageResponse>() {
+                    @Override
+                    public StringMessageResponse newInstance() {
+                        return new StringMessageResponse();
+                    }
+
+                    @Override
+                    public String executor() {
+                        return ThreadPool.Names.GENERIC;
+                    }
+
+                    @Override
+                    public void handleResponse(StringMessageResponse response) {
+                        fail("got response instead of exception");
+                    }
+
+                    @Override
+                    public void handleException(TransportException exp) {
+                        assertThat("bad message !!!", equalTo(exp.getCause().getMessage()));
+                    }
+                });
+
+        try {
+            res.txGet();
+            fail("exception should be thrown");
+        } catch (Exception e) {
+            assertThat(e.getCause().getMessage(), equalTo("bad message !!!"));
+        }
+
+        serviceA.removeHandler("sayHelloException");
+    }
+
+    @Test
+    public void testDisconnectListener() throws Exception {
+        final CountDownLatch latch = new CountDownLatch(1);
+        TransportConnectionListener disconnectListener = new TransportConnectionListener() {
+            @Override
+            public void onNodeConnected(DiscoveryNode node) {
+                fail("node connected should not be called, all connection have been done previously, node: " + node);
+            }
+
+            @Override
+            public void onNodeDisconnected(DiscoveryNode node) {
+                latch.countDown();
+            }
+        };
+        serviceA.addConnectionListener(disconnectListener);
+        serviceB.close();
+        assertThat(latch.await(5, TimeUnit.SECONDS), equalTo(true));
+    }
+
+    @Test
+    public void testNotifyOnShutdown() throws Exception {
+        final CountDownLatch latch2 = new CountDownLatch(1);
+
+        serviceA.registerRequestHandler("foobar", StringMessageRequest.class, ThreadPool.Names.GENERIC, new TransportRequestHandler<StringMessageRequest>() {
+            @Override
+            public void messageReceived(StringMessageRequest request, TransportChannel channel) {
+                try {
+                    latch2.await();
+                    logger.info("Stop ServiceB now");
+                    serviceB.stop();
+                } catch (Exception e) {
+                    fail(e.getMessage());
+                }
+            }
+        });
+        TransportFuture<TransportResponse.Empty> foobar = serviceB.submitRequest(nodeA, "foobar",
+                new StringMessageRequest(""), options(), EmptyTransportResponseHandler.INSTANCE_SAME);
+        latch2.countDown();
+        try {
+            foobar.txGet();
+            fail("TransportException expected");
+        } catch (TransportException ex) {
+
+        }
+        serviceA.removeHandler("sayHelloTimeoutDelayedResponse");
+    }
+
+    @Test
+    public void testTimeoutSendExceptionWithNeverSendingBackResponse() throws Exception {
+        serviceA.registerRequestHandler("sayHelloTimeoutNoResponse", StringMessageRequest.class, ThreadPool.Names.GENERIC, new TransportRequestHandler<StringMessageRequest>() {
+            @Override
+            public void messageReceived(StringMessageRequest request, TransportChannel channel) {
+                assertThat("moshe", equalTo(request.message));
+                // don't send back a response
+//                try {
+//                    channel.sendResponse(new StringMessage("hello " + request.message));
+//                } catch (IOException e) {
+//                    e.printStackTrace();
+//                    assertThat(e.getMessage(), false, equalTo(true));
+//                }
+            }
+        });
+
+        TransportFuture<StringMessageResponse> res = serviceB.submitRequest(nodeA, "sayHelloTimeoutNoResponse",
+                new StringMessageRequest("moshe"), options().withTimeout(100), new BaseTransportResponseHandler<StringMessageResponse>() {
+                    @Override
+                    public StringMessageResponse newInstance() {
+                        return new StringMessageResponse();
+                    }
+
+                    @Override
+                    public String executor() {
+                        return ThreadPool.Names.GENERIC;
+                    }
+
+                    @Override
+                    public void handleResponse(StringMessageResponse response) {
+                        fail("got response instead of exception");
+                    }
+
+                    @Override
+                    public void handleException(TransportException exp) {
+                        assertThat(exp, instanceOf(ReceiveTimeoutTransportException.class));
+                    }
+                });
+
+        try {
+            StringMessageResponse message = res.txGet();
+            fail("exception should be thrown");
+        } catch (Exception e) {
+            assertThat(e, instanceOf(ReceiveTimeoutTransportException.class));
+        }
+
+        serviceA.removeHandler("sayHelloTimeoutNoResponse");
+    }
+
+    @Test
+    public void testTimeoutSendExceptionWithDelayedResponse() throws Exception {
+        serviceA.registerRequestHandler("sayHelloTimeoutDelayedResponse", StringMessageRequest.class, ThreadPool.Names.GENERIC, new TransportRequestHandler<StringMessageRequest>() {
+            @Override
+            public void messageReceived(StringMessageRequest request, TransportChannel channel) {
+                TimeValue sleep = TimeValue.parseTimeValue(request.message, null, "sleep");
+                try {
+                    Thread.sleep(sleep.millis());
+                } catch (InterruptedException e) {
+                    // ignore
+                }
+                try {
+                    channel.sendResponse(new StringMessageResponse("hello " + request.message));
+                } catch (IOException e) {
+                    e.printStackTrace();
+                    assertThat(e.getMessage(), false, equalTo(true));
+                }
+            }
+        });
+        final CountDownLatch latch = new CountDownLatch(1);
+        TransportFuture<StringMessageResponse> res = serviceB.submitRequest(nodeA, "sayHelloTimeoutDelayedResponse",
+                new StringMessageRequest("300ms"), options().withTimeout(100), new BaseTransportResponseHandler<StringMessageResponse>() {
+                    @Override
+                    public StringMessageResponse newInstance() {
+                        return new StringMessageResponse();
+                    }
+
+                    @Override
+                    public String executor() {
+                        return ThreadPool.Names.GENERIC;
+                    }
+
+                    @Override
+                    public void handleResponse(StringMessageResponse response) {
+                        latch.countDown();
+                        fail("got response instead of exception");
+                    }
+
+                    @Override
+                    public void handleException(TransportException exp) {
+                        latch.countDown();
+                        assertThat(exp, instanceOf(ReceiveTimeoutTransportException.class));
+                    }
+                });
+
+        try {
+            StringMessageResponse message = res.txGet();
+            fail("exception should be thrown");
+        } catch (Exception e) {
+            assertThat(e, instanceOf(ReceiveTimeoutTransportException.class));
+        }
+        latch.await();
+
+        for (int i = 0; i < 10; i++) {
+            final int counter = i;
+            // now, try and send another request, this times, with a short timeout
+            res = serviceB.submitRequest(nodeA, "sayHelloTimeoutDelayedResponse",
+                    new StringMessageRequest(counter + "ms"), options().withTimeout(3000), new BaseTransportResponseHandler<StringMessageResponse>() {
+                        @Override
+                        public StringMessageResponse newInstance() {
+                            return new StringMessageResponse();
+                        }
+
+                        @Override
+                        public String executor() {
+                            return ThreadPool.Names.GENERIC;
+                        }
+
+                        @Override
+                        public void handleResponse(StringMessageResponse response) {
+                            assertThat("hello " + counter + "ms", equalTo(response.message));
+                        }
+
+                        @Override
+                        public void handleException(TransportException exp) {
+                            exp.printStackTrace();
+                            fail("got exception instead of a response for " + counter + ": " + exp.getDetailedMessage());
+                        }
+                    });
+
+            StringMessageResponse message = res.txGet();
+            assertThat(message.message, equalTo("hello " + counter + "ms"));
+        }
+
+        serviceA.removeHandler("sayHelloTimeoutDelayedResponse");
+    }
+
+
+    @Test
+    @TestLogging(value = "test. transport.tracer:TRACE")
+    public void testTracerLog() throws InterruptedException {
+        TransportRequestHandler handler = new TransportRequestHandler<StringMessageRequest>() {
+            @Override
+            public void messageReceived(StringMessageRequest request, TransportChannel channel) throws Exception {
+                channel.sendResponse(new StringMessageResponse(""));
+            }
+        };
+
+        TransportRequestHandler handlerWithError = new TransportRequestHandler<StringMessageRequest>() {
+            @Override
+            public void messageReceived(StringMessageRequest request, TransportChannel channel) throws Exception {
+                if (request.timeout() > 0) {
+                    Thread.sleep(request.timeout);
+                }
+                channel.sendResponse(new RuntimeException(""));
+
+            }
+        };
+
+        final Semaphore requestCompleted = new Semaphore(0);
+        TransportResponseHandler noopResponseHandler = new BaseTransportResponseHandler<StringMessageResponse>() {
+
+            @Override
+            public StringMessageResponse newInstance() {
+                return new StringMessageResponse();
+            }
+
+            @Override
+            public void handleResponse(StringMessageResponse response) {
+                requestCompleted.release();
+            }
+
+            @Override
+            public void handleException(TransportException exp) {
+                requestCompleted.release();
+            }
+
+            @Override
+            public String executor() {
+                return ThreadPool.Names.SAME;
+            }
+        };
+
+        serviceA.registerRequestHandler("test", StringMessageRequest.class, ThreadPool.Names.SAME, handler);
+        serviceA.registerRequestHandler("testError", StringMessageRequest.class, ThreadPool.Names.SAME, handlerWithError);
+        serviceB.registerRequestHandler("test", StringMessageRequest.class, ThreadPool.Names.SAME, handler);
+        serviceB.registerRequestHandler("testError", StringMessageRequest.class, ThreadPool.Names.SAME, handlerWithError);
+
+        final Tracer tracer = new Tracer();
+        serviceA.addTracer(tracer);
+        serviceB.addTracer(tracer);
+
+        tracer.reset(4);
+        boolean timeout = randomBoolean();
+        TransportRequestOptions options = timeout ? new TransportRequestOptions().withTimeout(1) : TransportRequestOptions.EMPTY;
+        serviceA.sendRequest(nodeB, "test", new StringMessageRequest("", 10), options, noopResponseHandler);
+        requestCompleted.acquire();
+        tracer.expectedEvents.get().await();
+        assertThat("didn't see request sent", tracer.sawRequestSent, equalTo(true));
+        assertThat("didn't see request received", tracer.sawRequestReceived, equalTo(true));
+        assertThat("didn't see response sent", tracer.sawResponseSent, equalTo(true));
+        assertThat("didn't see response received", tracer.sawResponseReceived, equalTo(true));
+        assertThat("saw error sent", tracer.sawErrorSent, equalTo(false));
+
+        tracer.reset(4);
+        serviceA.sendRequest(nodeB, "testError", new StringMessageRequest(""), noopResponseHandler);
+        requestCompleted.acquire();
+        tracer.expectedEvents.get().await();
+        assertThat("didn't see request sent", tracer.sawRequestSent, equalTo(true));
+        assertThat("didn't see request received", tracer.sawRequestReceived, equalTo(true));
+        assertThat("saw response sent", tracer.sawResponseSent, equalTo(false));
+        assertThat("didn't see response received", tracer.sawResponseReceived, equalTo(true));
+        assertThat("didn't see error sent", tracer.sawErrorSent, equalTo(true));
+
+        String includeSettings;
+        String excludeSettings;
+        if (randomBoolean()) {
+            // sometimes leave include empty (default)
+            includeSettings = randomBoolean() ? "*" : "";
+            excludeSettings = "*Error";
+        } else {
+            includeSettings = "test";
+            excludeSettings = "DOESN'T_MATCH";
+        }
+
+        serviceA.applySettings(Settings.builder()
+                .put(TransportService.SETTING_TRACE_LOG_INCLUDE, includeSettings, TransportService.SETTING_TRACE_LOG_EXCLUDE, excludeSettings)
+                .build());
+
+        tracer.reset(4);
+        serviceA.sendRequest(nodeB, "test", new StringMessageRequest(""), noopResponseHandler);
+        requestCompleted.acquire();
+        tracer.expectedEvents.get().await();
+        assertThat("didn't see request sent", tracer.sawRequestSent, equalTo(true));
+        assertThat("didn't see request received", tracer.sawRequestReceived, equalTo(true));
+        assertThat("didn't see response sent", tracer.sawResponseSent, equalTo(true));
+        assertThat("didn't see response received", tracer.sawResponseReceived, equalTo(true));
+        assertThat("saw error sent", tracer.sawErrorSent, equalTo(false));
+
+        tracer.reset(2);
+        serviceA.sendRequest(nodeB, "testError", new StringMessageRequest(""), noopResponseHandler);
+        requestCompleted.acquire();
+        tracer.expectedEvents.get().await();
+        assertThat("saw request sent", tracer.sawRequestSent, equalTo(false));
+        assertThat("didn't see request received", tracer.sawRequestReceived, equalTo(true));
+        assertThat("saw response sent", tracer.sawResponseSent, equalTo(false));
+        assertThat("saw response received", tracer.sawResponseReceived, equalTo(false));
+        assertThat("didn't see error sent", tracer.sawErrorSent, equalTo(true));
+    }
+
+    private static class Tracer extends MockTransportService.Tracer {
+        public volatile boolean sawRequestSent;
+        public volatile boolean sawRequestReceived;
+        public volatile boolean sawResponseSent;
+        public volatile boolean sawErrorSent;
+        public volatile boolean sawResponseReceived;
+
+        public AtomicReference<CountDownLatch> expectedEvents = new AtomicReference<>();
+
+
+        @Override
+        public void receivedRequest(long requestId, String action) {
+            super.receivedRequest(requestId, action);
+            sawRequestReceived = true;
+            expectedEvents.get().countDown();
+        }
+
+        @Override
+        public void requestSent(DiscoveryNode node, long requestId, String action, TransportRequestOptions options) {
+            super.requestSent(node, requestId, action, options);
+            sawRequestSent = true;
+            expectedEvents.get().countDown();
+        }
+
+        @Override
+        public void responseSent(long requestId, String action) {
+            super.responseSent(requestId, action);
+            sawResponseSent = true;
+            expectedEvents.get().countDown();
+        }
+
+        @Override
+        public void responseSent(long requestId, String action, Throwable t) {
+            super.responseSent(requestId, action, t);
+            sawErrorSent = true;
+            expectedEvents.get().countDown();
+        }
+
+        @Override
+        public void receivedResponse(long requestId, DiscoveryNode sourceNode, String action) {
+            super.receivedResponse(requestId, sourceNode, action);
+            sawResponseReceived = true;
+            expectedEvents.get().countDown();
+        }
+
+        public void reset(int expectedCount) {
+            sawRequestSent = false;
+            sawRequestReceived = false;
+            sawResponseSent = false;
+            sawErrorSent = false;
+            sawResponseReceived = false;
+            expectedEvents.set(new CountDownLatch(expectedCount));
+        }
+    }
+
+
+    static class StringMessageRequest extends TransportRequest {
+
+        private String message;
+        private long timeout;
+
+        StringMessageRequest(String message, long timeout) {
+            this.message = message;
+            this.timeout = timeout;
+        }
+
+        StringMessageRequest() {
+        }
+
+        public StringMessageRequest(String message) {
+            this(message, -1);
+        }
+
+        public long timeout() {
+            return timeout;
+        }
+
+        @Override
+        public void readFrom(StreamInput in) throws IOException {
+            super.readFrom(in);
+            message = in.readString();
+            timeout = in.readLong();
+        }
+
+        @Override
+        public void writeTo(StreamOutput out) throws IOException {
+            super.writeTo(out);
+            out.writeString(message);
+            out.writeLong(timeout);
+        }
+    }
+
+    static class StringMessageResponse extends TransportResponse {
+
+        private String message;
+
+        StringMessageResponse(String message) {
+            this.message = message;
+        }
+
+        StringMessageResponse() {
+        }
+
+        @Override
+        public void readFrom(StreamInput in) throws IOException {
+            super.readFrom(in);
+            message = in.readString();
+        }
+
+        @Override
+        public void writeTo(StreamOutput out) throws IOException {
+            super.writeTo(out);
+            out.writeString(message);
+        }
+    }
+
+
+    static class Version0Request extends TransportRequest {
+
+        int value1;
+
+
+        @Override
+        public void readFrom(StreamInput in) throws IOException {
+            super.readFrom(in);
+            value1 = in.readInt();
+        }
+
+        @Override
+        public void writeTo(StreamOutput out) throws IOException {
+            super.writeTo(out);
+            out.writeInt(value1);
+        }
+    }
+
+    static class Version1Request extends Version0Request {
+
+        int value2;
+
+        @Override
+        public void readFrom(StreamInput in) throws IOException {
+            super.readFrom(in);
+            if (in.getVersion().onOrAfter(version1)) {
+                value2 = in.readInt();
+            }
+        }
+
+        @Override
+        public void writeTo(StreamOutput out) throws IOException {
+            super.writeTo(out);
+            if (out.getVersion().onOrAfter(version1)) {
+                out.writeInt(value2);
+            }
+        }
+    }
+
+    static class Version0Response extends TransportResponse {
+
+        int value1;
+
+        @Override
+        public void readFrom(StreamInput in) throws IOException {
+            super.readFrom(in);
+            value1 = in.readInt();
+        }
+
+        @Override
+        public void writeTo(StreamOutput out) throws IOException {
+            super.writeTo(out);
+            out.writeInt(value1);
+        }
+    }
+
+    static class Version1Response extends Version0Response {
+
+        int value2;
+
+        @Override
+        public void readFrom(StreamInput in) throws IOException {
+            super.readFrom(in);
+            if (in.getVersion().onOrAfter(version1)) {
+                value2 = in.readInt();
+            }
+        }
+
+        @Override
+        public void writeTo(StreamOutput out) throws IOException {
+            super.writeTo(out);
+            if (out.getVersion().onOrAfter(version1)) {
+                out.writeInt(value2);
+            }
+        }
+    }
+
+    @Test
+    public void testVersion_from0to1() throws Exception {
+        serviceB.registerRequestHandler("/version", Version1Request.class, ThreadPool.Names.SAME, new TransportRequestHandler<Version1Request>() {
+            @Override
+            public void messageReceived(Version1Request request, TransportChannel channel) throws Exception {
+                assertThat(request.value1, equalTo(1));
+                assertThat(request.value2, equalTo(0)); // not set, coming from service A
+                Version1Response response = new Version1Response();
+                response.value1 = 1;
+                response.value2 = 2;
+                channel.sendResponse(response);
+            }
+        });
+
+        Version0Request version0Request = new Version0Request();
+        version0Request.value1 = 1;
+        Version0Response version0Response = serviceA.submitRequest(nodeB, "/version", version0Request, new BaseTransportResponseHandler<Version0Response>() {
+            @Override
+            public Version0Response newInstance() {
+                return new Version0Response();
+            }
+
+            @Override
+            public void handleResponse(Version0Response response) {
+                assertThat(response.value1, equalTo(1));
+            }
+
+            @Override
+            public void handleException(TransportException exp) {
+                exp.printStackTrace();
+                fail();
+            }
+
+            @Override
+            public String executor() {
+                return ThreadPool.Names.SAME;
+            }
+        }).txGet();
+
+        assertThat(version0Response.value1, equalTo(1));
+    }
+
+    @Test
+    public void testVersion_from1to0() throws Exception {
+        serviceA.registerRequestHandler("/version", Version0Request.class, ThreadPool.Names.SAME, new TransportRequestHandler<Version0Request>() {
+            @Override
+            public void messageReceived(Version0Request request, TransportChannel channel) throws Exception {
+                assertThat(request.value1, equalTo(1));
+                Version0Response response = new Version0Response();
+                response.value1 = 1;
+                channel.sendResponse(response);
+            }
+        });
+
+        Version1Request version1Request = new Version1Request();
+        version1Request.value1 = 1;
+        version1Request.value2 = 2;
+        Version1Response version1Response = serviceB.submitRequest(nodeA, "/version", version1Request, new BaseTransportResponseHandler<Version1Response>() {
+            @Override
+            public Version1Response newInstance() {
+                return new Version1Response();
+            }
+
+            @Override
+            public void handleResponse(Version1Response response) {
+                assertThat(response.value1, equalTo(1));
+                assertThat(response.value2, equalTo(0)); // initial values, cause its serialized from version 0
+            }
+
+            @Override
+            public void handleException(TransportException exp) {
+                exp.printStackTrace();
+                fail();
+            }
+
+            @Override
+            public String executor() {
+                return ThreadPool.Names.SAME;
+            }
+        }).txGet();
+
+        assertThat(version1Response.value1, equalTo(1));
+        assertThat(version1Response.value2, equalTo(0));
+    }
+
+    @Test
+    public void testVersion_from1to1() throws Exception {
+        serviceB.registerRequestHandler("/version", Version1Request.class, ThreadPool.Names.SAME, new TransportRequestHandler<Version1Request>() {
+            @Override
+            public void messageReceived(Version1Request request, TransportChannel channel) throws Exception {
+                assertThat(request.value1, equalTo(1));
+                assertThat(request.value2, equalTo(2));
+                Version1Response response = new Version1Response();
+                response.value1 = 1;
+                response.value2 = 2;
+                channel.sendResponse(response);
+            }
+        });
+
+        Version1Request version1Request = new Version1Request();
+        version1Request.value1 = 1;
+        version1Request.value2 = 2;
+        Version1Response version1Response = serviceB.submitRequest(nodeB, "/version", version1Request, new BaseTransportResponseHandler<Version1Response>() {
+            @Override
+            public Version1Response newInstance() {
+                return new Version1Response();
+            }
+
+            @Override
+            public void handleResponse(Version1Response response) {
+                assertThat(response.value1, equalTo(1));
+                assertThat(response.value2, equalTo(2));
+            }
+
+            @Override
+            public void handleException(TransportException exp) {
+                exp.printStackTrace();
+                fail();
+            }
+
+            @Override
+            public String executor() {
+                return ThreadPool.Names.SAME;
+            }
+        }).txGet();
+
+        assertThat(version1Response.value1, equalTo(1));
+        assertThat(version1Response.value2, equalTo(2));
+    }
+
+    @Test
+    public void testVersion_from0to0() throws Exception {
+        serviceA.registerRequestHandler("/version", Version0Request.class, ThreadPool.Names.SAME, new TransportRequestHandler<Version0Request>() {
+            @Override
+            public void messageReceived(Version0Request request, TransportChannel channel) throws Exception {
+                assertThat(request.value1, equalTo(1));
+                Version0Response response = new Version0Response();
+                response.value1 = 1;
+                channel.sendResponse(response);
+            }
+        });
+
+        Version0Request version0Request = new Version0Request();
+        version0Request.value1 = 1;
+        Version0Response version0Response = serviceA.submitRequest(nodeA, "/version", version0Request, new BaseTransportResponseHandler<Version0Response>() {
+            @Override
+            public Version0Response newInstance() {
+                return new Version0Response();
+            }
+
+            @Override
+            public void handleResponse(Version0Response response) {
+                assertThat(response.value1, equalTo(1));
+            }
+
+            @Override
+            public void handleException(TransportException exp) {
+                exp.printStackTrace();
+                fail();
+            }
+
+            @Override
+            public String executor() {
+                return ThreadPool.Names.SAME;
+            }
+        }).txGet();
+
+        assertThat(version0Response.value1, equalTo(1));
+    }
+
+    @Test
+    public void testMockFailToSendNoConnectRule() {
+        serviceA.registerRequestHandler("sayHello", StringMessageRequest.class, ThreadPool.Names.GENERIC, new TransportRequestHandler<StringMessageRequest>() {
+            @Override
+            public void messageReceived(StringMessageRequest request, TransportChannel channel) throws Exception {
+                assertThat("moshe", equalTo(request.message));
+                throw new RuntimeException("bad message !!!");
+            }
+        });
+
+        serviceB.addFailToSendNoConnectRule(nodeA);
+
+        TransportFuture<StringMessageResponse> res = serviceB.submitRequest(nodeA, "sayHello",
+                new StringMessageRequest("moshe"), new BaseTransportResponseHandler<StringMessageResponse>() {
+                    @Override
+                    public StringMessageResponse newInstance() {
+                        return new StringMessageResponse();
+                    }
+
+                    @Override
+                    public String executor() {
+                        return ThreadPool.Names.GENERIC;
+                    }
+
+                    @Override
+                    public void handleResponse(StringMessageResponse response) {
+                        fail("got response instead of exception");
+                    }
+
+                    @Override
+                    public void handleException(TransportException exp) {
+                        assertThat(exp.getCause().getMessage(), endsWith("DISCONNECT: simulated"));
+                    }
+                });
+
+        try {
+            res.txGet();
+            fail("exception should be thrown");
+        } catch (Exception e) {
+            assertThat(e.getCause().getMessage(), endsWith("DISCONNECT: simulated"));
+        }
+
+        try {
+            serviceB.connectToNode(nodeA);
+            fail("exception should be thrown");
+        } catch (ConnectTransportException e) {
+            // all is well
+        }
+
+        try {
+            serviceB.connectToNodeLight(nodeA);
+            fail("exception should be thrown");
+        } catch (ConnectTransportException e) {
+            // all is well
+        }
+
+        serviceA.removeHandler("sayHello");
+    }
+
+    @Test
+    public void testMockUnresponsiveRule() {
+        serviceA.registerRequestHandler("sayHello", StringMessageRequest.class, ThreadPool.Names.GENERIC, new TransportRequestHandler<StringMessageRequest>() {
+            @Override
+            public void messageReceived(StringMessageRequest request, TransportChannel channel) throws Exception {
+                assertThat("moshe", equalTo(request.message));
+                throw new RuntimeException("bad message !!!");
+            }
+        });
+
+        serviceB.addUnresponsiveRule(nodeA);
+
+        TransportFuture<StringMessageResponse> res = serviceB.submitRequest(nodeA, "sayHello",
+                new StringMessageRequest("moshe"), TransportRequestOptions.options().withTimeout(100), new BaseTransportResponseHandler<StringMessageResponse>() {
+                    @Override
+                    public StringMessageResponse newInstance() {
+                        return new StringMessageResponse();
+                    }
+
+                    @Override
+                    public String executor() {
+                        return ThreadPool.Names.GENERIC;
+                    }
+
+                    @Override
+                    public void handleResponse(StringMessageResponse response) {
+                        fail("got response instead of exception");
+                    }
+
+                    @Override
+                    public void handleException(TransportException exp) {
+                        assertThat(exp, instanceOf(ReceiveTimeoutTransportException.class));
+                    }
+                });
+
+        try {
+            res.txGet();
+            fail("exception should be thrown");
+        } catch (Exception e) {
+            assertThat(e, instanceOf(ReceiveTimeoutTransportException.class));
+        }
+
+        try {
+            serviceB.connectToNode(nodeA);
+            fail("exception should be thrown");
+        } catch (ConnectTransportException e) {
+            // all is well
+        }
+
+        try {
+            serviceB.connectToNodeLight(nodeA);
+            fail("exception should be thrown");
+        } catch (ConnectTransportException e) {
+            // all is well
+        }
+
+        serviceA.removeHandler("sayHello");
+    }
+
+
+    @Test
+    public void testHostOnMessages() throws InterruptedException {
+        final CountDownLatch latch = new CountDownLatch(2);
+        final AtomicReference<TransportAddress> addressA = new AtomicReference<>();
+        final AtomicReference<TransportAddress> addressB = new AtomicReference<>();
+        serviceB.registerRequestHandler("action1", TestRequest.class, ThreadPool.Names.SAME, new TransportRequestHandler<TestRequest>() {
+            @Override
+            public void messageReceived(TestRequest request, TransportChannel channel) throws Exception {
+                addressA.set(request.remoteAddress());
+                channel.sendResponse(new TestResponse());
+                latch.countDown();
+            }
+        });
+        serviceA.sendRequest(nodeB, "action1", new TestRequest(), new TransportResponseHandler<TestResponse>() {
+            @Override
+            public TestResponse newInstance() {
+                return new TestResponse();
+            }
+
+            @Override
+            public void handleResponse(TestResponse response) {
+                addressB.set(response.remoteAddress());
+                latch.countDown();
+            }
+
+            @Override
+            public void handleException(TransportException exp) {
+                latch.countDown();
+            }
+
+            @Override
+            public String executor() {
+                return ThreadPool.Names.SAME;
+            }
+        });
+
+        if (!latch.await(10, TimeUnit.SECONDS)) {
+            fail("message round trip did not complete within a sensible time frame");
+        }
+
+        assertTrue(nodeA.address().sameHost(addressA.get()));
+        assertTrue(nodeB.address().sameHost(addressB.get()));
+    }
+
+    private static class TestRequest extends TransportRequest {
+    }
+
+    private static class TestResponse extends TransportResponse {
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTests.java b/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTests.java
deleted file mode 100644
index 9041bca..0000000
--- a/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTests.java
+++ /dev/null
@@ -1,1221 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.transport;
-
-import com.google.common.collect.ImmutableMap;
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.transport.TransportAddress;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.junit.annotations.TestLogging;
-import org.elasticsearch.test.transport.MockTransportService;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.Semaphore;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.atomic.AtomicReference;
-
-import static org.elasticsearch.transport.TransportRequestOptions.options;
-import static org.hamcrest.Matchers.*;
-
-/**
- *
- */
-public abstract class AbstractSimpleTransportTests extends ESTestCase {
-
-    protected ThreadPool threadPool;
-
-    protected static final Version version0 = Version.fromId(/*0*/99);
-    protected DiscoveryNode nodeA;
-    protected MockTransportService serviceA;
-
-    protected static final Version version1 = Version.fromId(199);
-    protected DiscoveryNode nodeB;
-    protected MockTransportService serviceB;
-
-    protected abstract MockTransportService build(Settings settings, Version version, NamedWriteableRegistry namedWriteableRegistry);
-
-    @Override
-    @Before
-    public void setUp() throws Exception {
-        super.setUp();
-        threadPool = new ThreadPool(getClass().getName());
-        serviceA = build(
-                Settings.builder().put("name", "TS_A", TransportService.SETTING_TRACE_LOG_INCLUDE, "", TransportService.SETTING_TRACE_LOG_EXCLUDE, "NOTHING").build(),
-                version0, new NamedWriteableRegistry()
-        );
-        nodeA = new DiscoveryNode("TS_A", "TS_A", serviceA.boundAddress().publishAddress(), ImmutableMap.<String, String>of(), version0);
-        serviceB = build(
-                Settings.builder().put("name", "TS_B", TransportService.SETTING_TRACE_LOG_INCLUDE, "", TransportService.SETTING_TRACE_LOG_EXCLUDE, "NOTHING").build(),
-                version1, new NamedWriteableRegistry()
-        );
-        nodeB = new DiscoveryNode("TS_B", "TS_B", serviceB.boundAddress().publishAddress(), ImmutableMap.<String, String>of(), version1);
-
-        // wait till all nodes are properly connected and the event has been sent, so tests in this class
-        // will not get this callback called on the connections done in this setup
-        final boolean useLocalNode = randomBoolean();
-        final CountDownLatch latch = new CountDownLatch(useLocalNode ? 2 : 4);
-        TransportConnectionListener waitForConnection = new TransportConnectionListener() {
-            @Override
-            public void onNodeConnected(DiscoveryNode node) {
-                latch.countDown();
-            }
-
-            @Override
-            public void onNodeDisconnected(DiscoveryNode node) {
-                fail("disconnect should not be called " + node);
-            }
-        };
-        serviceA.addConnectionListener(waitForConnection);
-        serviceB.addConnectionListener(waitForConnection);
-
-        if (useLocalNode) {
-            logger.info("--> using local node optimization");
-            serviceA.setLocalNode(nodeA);
-            serviceB.setLocalNode(nodeB);
-        } else {
-            logger.info("--> actively connecting to local node");
-            serviceA.connectToNode(nodeA);
-            serviceB.connectToNode(nodeB);
-        }
-
-        serviceA.connectToNode(nodeB);
-        serviceB.connectToNode(nodeA);
-
-        assertThat("failed to wait for all nodes to connect", latch.await(5, TimeUnit.SECONDS), equalTo(true));
-        serviceA.removeConnectionListener(waitForConnection);
-        serviceB.removeConnectionListener(waitForConnection);
-    }
-
-    @Override
-    @After
-    public void tearDown() throws Exception {
-        super.tearDown();
-        serviceA.close();
-        serviceB.close();
-        terminate(threadPool);
-    }
-
-    @Test
-    public void testHelloWorld() {
-        serviceA.registerRequestHandler("sayHello", StringMessageRequest.class, ThreadPool.Names.GENERIC, new TransportRequestHandler<StringMessageRequest>() {
-            @Override
-            public void messageReceived(StringMessageRequest request, TransportChannel channel) {
-                assertThat("moshe", equalTo(request.message));
-                try {
-                    channel.sendResponse(new StringMessageResponse("hello " + request.message));
-                } catch (IOException e) {
-                    e.printStackTrace();
-                    assertThat(e.getMessage(), false, equalTo(true));
-                }
-            }
-        });
-
-        TransportFuture<StringMessageResponse> res = serviceB.submitRequest(nodeA, "sayHello",
-                new StringMessageRequest("moshe"), new BaseTransportResponseHandler<StringMessageResponse>() {
-                    @Override
-                    public StringMessageResponse newInstance() {
-                        return new StringMessageResponse();
-                    }
-
-                    @Override
-                    public String executor() {
-                        return ThreadPool.Names.GENERIC;
-                    }
-
-                    @Override
-                    public void handleResponse(StringMessageResponse response) {
-                        assertThat("hello moshe", equalTo(response.message));
-                    }
-
-                    @Override
-                    public void handleException(TransportException exp) {
-                        exp.printStackTrace();
-                        assertThat("got exception instead of a response: " + exp.getMessage(), false, equalTo(true));
-                    }
-                });
-
-        try {
-            StringMessageResponse message = res.get();
-            assertThat("hello moshe", equalTo(message.message));
-        } catch (Exception e) {
-            assertThat(e.getMessage(), false, equalTo(true));
-        }
-
-        res = serviceB.submitRequest(nodeA, "sayHello",
-                new StringMessageRequest("moshe"), TransportRequestOptions.options().withCompress(true), new BaseTransportResponseHandler<StringMessageResponse>() {
-                    @Override
-                    public StringMessageResponse newInstance() {
-                        return new StringMessageResponse();
-                    }
-
-                    @Override
-                    public String executor() {
-                        return ThreadPool.Names.GENERIC;
-                    }
-
-                    @Override
-                    public void handleResponse(StringMessageResponse response) {
-                        assertThat("hello moshe", equalTo(response.message));
-                    }
-
-                    @Override
-                    public void handleException(TransportException exp) {
-                        exp.printStackTrace();
-                        assertThat("got exception instead of a response: " + exp.getMessage(), false, equalTo(true));
-                    }
-                });
-
-        try {
-            StringMessageResponse message = res.get();
-            assertThat("hello moshe", equalTo(message.message));
-        } catch (Exception e) {
-            assertThat(e.getMessage(), false, equalTo(true));
-        }
-
-        serviceA.removeHandler("sayHello");
-    }
-
-    @Test
-    public void testLocalNodeConnection() throws InterruptedException {
-        assertTrue("serviceA is not connected to nodeA", serviceA.nodeConnected(nodeA));
-        if (((TransportService) serviceA).getLocalNode() != null) {
-            // this should be a noop
-            serviceA.disconnectFromNode(nodeA);
-        }
-        final AtomicReference<Exception> exception = new AtomicReference<>();
-        serviceA.registerRequestHandler("localNode", StringMessageRequest.class, ThreadPool.Names.GENERIC, new TransportRequestHandler<StringMessageRequest>() {
-            @Override
-            public void messageReceived(StringMessageRequest request, TransportChannel channel) {
-                try {
-                    channel.sendResponse(new StringMessageResponse(request.message));
-                } catch (IOException e) {
-                    exception.set(e);
-                }
-            }
-        });
-        final AtomicReference<String> responseString = new AtomicReference<>();
-        final CountDownLatch responseLatch = new CountDownLatch(1);
-        serviceA.sendRequest(nodeA, "localNode", new StringMessageRequest("test"), new TransportResponseHandler<StringMessageResponse>() {
-            @Override
-            public StringMessageResponse newInstance() {
-                return new StringMessageResponse();
-            }
-
-            @Override
-            public void handleResponse(StringMessageResponse response) {
-                responseString.set(response.message);
-                responseLatch.countDown();
-            }
-
-            @Override
-            public void handleException(TransportException exp) {
-                exception.set(exp);
-                responseLatch.countDown();
-            }
-
-            @Override
-            public String executor() {
-                return ThreadPool.Names.GENERIC;
-            }
-        });
-        responseLatch.await();
-        assertNull(exception.get());
-        assertThat(responseString.get(), equalTo("test"));
-    }
-
-    @Test
-    public void testVoidMessageCompressed() {
-        serviceA.registerRequestHandler("sayHello", TransportRequest.Empty.class, ThreadPool.Names.GENERIC, new TransportRequestHandler<TransportRequest.Empty>() {
-            @Override
-            public void messageReceived(TransportRequest.Empty request, TransportChannel channel) {
-                try {
-                    channel.sendResponse(TransportResponse.Empty.INSTANCE, TransportResponseOptions.options().withCompress(true));
-                } catch (IOException e) {
-                    e.printStackTrace();
-                    assertThat(e.getMessage(), false, equalTo(true));
-                }
-            }
-        });
-
-        TransportFuture<TransportResponse.Empty> res = serviceB.submitRequest(nodeA, "sayHello",
-                TransportRequest.Empty.INSTANCE, TransportRequestOptions.options().withCompress(true), new BaseTransportResponseHandler<TransportResponse.Empty>() {
-                    @Override
-                    public TransportResponse.Empty newInstance() {
-                        return TransportResponse.Empty.INSTANCE;
-                    }
-
-                    @Override
-                    public String executor() {
-                        return ThreadPool.Names.GENERIC;
-                    }
-
-                    @Override
-                    public void handleResponse(TransportResponse.Empty response) {
-                    }
-
-                    @Override
-                    public void handleException(TransportException exp) {
-                        exp.printStackTrace();
-                        assertThat("got exception instead of a response: " + exp.getMessage(), false, equalTo(true));
-                    }
-                });
-
-        try {
-            TransportResponse.Empty message = res.get();
-            assertThat(message, notNullValue());
-        } catch (Exception e) {
-            assertThat(e.getMessage(), false, equalTo(true));
-        }
-
-        serviceA.removeHandler("sayHello");
-    }
-
-    @Test
-    public void testHelloWorldCompressed() {
-        serviceA.registerRequestHandler("sayHello", StringMessageRequest.class, ThreadPool.Names.GENERIC, new TransportRequestHandler<StringMessageRequest>() {
-            @Override
-            public void messageReceived(StringMessageRequest request, TransportChannel channel) {
-                assertThat("moshe", equalTo(request.message));
-                try {
-                    channel.sendResponse(new StringMessageResponse("hello " + request.message), TransportResponseOptions.options().withCompress(true));
-                } catch (IOException e) {
-                    e.printStackTrace();
-                    assertThat(e.getMessage(), false, equalTo(true));
-                }
-            }
-        });
-
-        TransportFuture<StringMessageResponse> res = serviceB.submitRequest(nodeA, "sayHello",
-                new StringMessageRequest("moshe"), TransportRequestOptions.options().withCompress(true), new BaseTransportResponseHandler<StringMessageResponse>() {
-                    @Override
-                    public StringMessageResponse newInstance() {
-                        return new StringMessageResponse();
-                    }
-
-                    @Override
-                    public String executor() {
-                        return ThreadPool.Names.GENERIC;
-                    }
-
-                    @Override
-                    public void handleResponse(StringMessageResponse response) {
-                        assertThat("hello moshe", equalTo(response.message));
-                    }
-
-                    @Override
-                    public void handleException(TransportException exp) {
-                        exp.printStackTrace();
-                        assertThat("got exception instead of a response: " + exp.getMessage(), false, equalTo(true));
-                    }
-                });
-
-        try {
-            StringMessageResponse message = res.get();
-            assertThat("hello moshe", equalTo(message.message));
-        } catch (Exception e) {
-            assertThat(e.getMessage(), false, equalTo(true));
-        }
-
-        serviceA.removeHandler("sayHello");
-    }
-
-    @Test
-    public void testErrorMessage() {
-        serviceA.registerRequestHandler("sayHelloException", StringMessageRequest.class, ThreadPool.Names.GENERIC, new TransportRequestHandler<StringMessageRequest>() {
-            @Override
-            public void messageReceived(StringMessageRequest request, TransportChannel channel) throws Exception {
-                assertThat("moshe", equalTo(request.message));
-                throw new RuntimeException("bad message !!!");
-            }
-        });
-
-        TransportFuture<StringMessageResponse> res = serviceB.submitRequest(nodeA, "sayHelloException",
-                new StringMessageRequest("moshe"), new BaseTransportResponseHandler<StringMessageResponse>() {
-                    @Override
-                    public StringMessageResponse newInstance() {
-                        return new StringMessageResponse();
-                    }
-
-                    @Override
-                    public String executor() {
-                        return ThreadPool.Names.GENERIC;
-                    }
-
-                    @Override
-                    public void handleResponse(StringMessageResponse response) {
-                        fail("got response instead of exception");
-                    }
-
-                    @Override
-                    public void handleException(TransportException exp) {
-                        assertThat("bad message !!!", equalTo(exp.getCause().getMessage()));
-                    }
-                });
-
-        try {
-            res.txGet();
-            fail("exception should be thrown");
-        } catch (Exception e) {
-            assertThat(e.getCause().getMessage(), equalTo("bad message !!!"));
-        }
-
-        serviceA.removeHandler("sayHelloException");
-    }
-
-    @Test
-    public void testDisconnectListener() throws Exception {
-        final CountDownLatch latch = new CountDownLatch(1);
-        TransportConnectionListener disconnectListener = new TransportConnectionListener() {
-            @Override
-            public void onNodeConnected(DiscoveryNode node) {
-                fail("node connected should not be called, all connection have been done previously, node: " + node);
-            }
-
-            @Override
-            public void onNodeDisconnected(DiscoveryNode node) {
-                latch.countDown();
-            }
-        };
-        serviceA.addConnectionListener(disconnectListener);
-        serviceB.close();
-        assertThat(latch.await(5, TimeUnit.SECONDS), equalTo(true));
-    }
-
-    @Test
-    public void testNotifyOnShutdown() throws Exception {
-        final CountDownLatch latch2 = new CountDownLatch(1);
-
-        serviceA.registerRequestHandler("foobar", StringMessageRequest.class, ThreadPool.Names.GENERIC, new TransportRequestHandler<StringMessageRequest>() {
-            @Override
-            public void messageReceived(StringMessageRequest request, TransportChannel channel) {
-                try {
-                    latch2.await();
-                    logger.info("Stop ServiceB now");
-                    serviceB.stop();
-                } catch (Exception e) {
-                    fail(e.getMessage());
-                }
-            }
-        });
-        TransportFuture<TransportResponse.Empty> foobar = serviceB.submitRequest(nodeA, "foobar",
-                new StringMessageRequest(""), options(), EmptyTransportResponseHandler.INSTANCE_SAME);
-        latch2.countDown();
-        try {
-            foobar.txGet();
-            fail("TransportException expected");
-        } catch (TransportException ex) {
-
-        }
-        serviceA.removeHandler("sayHelloTimeoutDelayedResponse");
-    }
-
-    @Test
-    public void testTimeoutSendExceptionWithNeverSendingBackResponse() throws Exception {
-        serviceA.registerRequestHandler("sayHelloTimeoutNoResponse", StringMessageRequest.class, ThreadPool.Names.GENERIC, new TransportRequestHandler<StringMessageRequest>() {
-            @Override
-            public void messageReceived(StringMessageRequest request, TransportChannel channel) {
-                assertThat("moshe", equalTo(request.message));
-                // don't send back a response
-//                try {
-//                    channel.sendResponse(new StringMessage("hello " + request.message));
-//                } catch (IOException e) {
-//                    e.printStackTrace();
-//                    assertThat(e.getMessage(), false, equalTo(true));
-//                }
-            }
-        });
-
-        TransportFuture<StringMessageResponse> res = serviceB.submitRequest(nodeA, "sayHelloTimeoutNoResponse",
-                new StringMessageRequest("moshe"), options().withTimeout(100), new BaseTransportResponseHandler<StringMessageResponse>() {
-                    @Override
-                    public StringMessageResponse newInstance() {
-                        return new StringMessageResponse();
-                    }
-
-                    @Override
-                    public String executor() {
-                        return ThreadPool.Names.GENERIC;
-                    }
-
-                    @Override
-                    public void handleResponse(StringMessageResponse response) {
-                        fail("got response instead of exception");
-                    }
-
-                    @Override
-                    public void handleException(TransportException exp) {
-                        assertThat(exp, instanceOf(ReceiveTimeoutTransportException.class));
-                    }
-                });
-
-        try {
-            StringMessageResponse message = res.txGet();
-            fail("exception should be thrown");
-        } catch (Exception e) {
-            assertThat(e, instanceOf(ReceiveTimeoutTransportException.class));
-        }
-
-        serviceA.removeHandler("sayHelloTimeoutNoResponse");
-    }
-
-    @Test
-    public void testTimeoutSendExceptionWithDelayedResponse() throws Exception {
-        serviceA.registerRequestHandler("sayHelloTimeoutDelayedResponse", StringMessageRequest.class, ThreadPool.Names.GENERIC, new TransportRequestHandler<StringMessageRequest>() {
-            @Override
-            public void messageReceived(StringMessageRequest request, TransportChannel channel) {
-                TimeValue sleep = TimeValue.parseTimeValue(request.message, null, "sleep");
-                try {
-                    Thread.sleep(sleep.millis());
-                } catch (InterruptedException e) {
-                    // ignore
-                }
-                try {
-                    channel.sendResponse(new StringMessageResponse("hello " + request.message));
-                } catch (IOException e) {
-                    e.printStackTrace();
-                    assertThat(e.getMessage(), false, equalTo(true));
-                }
-            }
-        });
-        final CountDownLatch latch = new CountDownLatch(1);
-        TransportFuture<StringMessageResponse> res = serviceB.submitRequest(nodeA, "sayHelloTimeoutDelayedResponse",
-                new StringMessageRequest("300ms"), options().withTimeout(100), new BaseTransportResponseHandler<StringMessageResponse>() {
-                    @Override
-                    public StringMessageResponse newInstance() {
-                        return new StringMessageResponse();
-                    }
-
-                    @Override
-                    public String executor() {
-                        return ThreadPool.Names.GENERIC;
-                    }
-
-                    @Override
-                    public void handleResponse(StringMessageResponse response) {
-                        latch.countDown();
-                        fail("got response instead of exception");
-                    }
-
-                    @Override
-                    public void handleException(TransportException exp) {
-                        latch.countDown();
-                        assertThat(exp, instanceOf(ReceiveTimeoutTransportException.class));
-                    }
-                });
-
-        try {
-            StringMessageResponse message = res.txGet();
-            fail("exception should be thrown");
-        } catch (Exception e) {
-            assertThat(e, instanceOf(ReceiveTimeoutTransportException.class));
-        }
-        latch.await();
-
-        for (int i = 0; i < 10; i++) {
-            final int counter = i;
-            // now, try and send another request, this times, with a short timeout
-            res = serviceB.submitRequest(nodeA, "sayHelloTimeoutDelayedResponse",
-                    new StringMessageRequest(counter + "ms"), options().withTimeout(3000), new BaseTransportResponseHandler<StringMessageResponse>() {
-                        @Override
-                        public StringMessageResponse newInstance() {
-                            return new StringMessageResponse();
-                        }
-
-                        @Override
-                        public String executor() {
-                            return ThreadPool.Names.GENERIC;
-                        }
-
-                        @Override
-                        public void handleResponse(StringMessageResponse response) {
-                            assertThat("hello " + counter + "ms", equalTo(response.message));
-                        }
-
-                        @Override
-                        public void handleException(TransportException exp) {
-                            exp.printStackTrace();
-                            fail("got exception instead of a response for " + counter + ": " + exp.getDetailedMessage());
-                        }
-                    });
-
-            StringMessageResponse message = res.txGet();
-            assertThat(message.message, equalTo("hello " + counter + "ms"));
-        }
-
-        serviceA.removeHandler("sayHelloTimeoutDelayedResponse");
-    }
-
-
-    @Test
-    @TestLogging(value = "test. transport.tracer:TRACE")
-    public void testTracerLog() throws InterruptedException {
-        TransportRequestHandler handler = new TransportRequestHandler<StringMessageRequest>() {
-            @Override
-            public void messageReceived(StringMessageRequest request, TransportChannel channel) throws Exception {
-                channel.sendResponse(new StringMessageResponse(""));
-            }
-        };
-
-        TransportRequestHandler handlerWithError = new TransportRequestHandler<StringMessageRequest>() {
-            @Override
-            public void messageReceived(StringMessageRequest request, TransportChannel channel) throws Exception {
-                if (request.timeout() > 0) {
-                    Thread.sleep(request.timeout);
-                }
-                channel.sendResponse(new RuntimeException(""));
-
-            }
-        };
-
-        final Semaphore requestCompleted = new Semaphore(0);
-        TransportResponseHandler noopResponseHandler = new BaseTransportResponseHandler<StringMessageResponse>() {
-
-            @Override
-            public StringMessageResponse newInstance() {
-                return new StringMessageResponse();
-            }
-
-            @Override
-            public void handleResponse(StringMessageResponse response) {
-                requestCompleted.release();
-            }
-
-            @Override
-            public void handleException(TransportException exp) {
-                requestCompleted.release();
-            }
-
-            @Override
-            public String executor() {
-                return ThreadPool.Names.SAME;
-            }
-        };
-
-        serviceA.registerRequestHandler("test", StringMessageRequest.class, ThreadPool.Names.SAME, handler);
-        serviceA.registerRequestHandler("testError", StringMessageRequest.class, ThreadPool.Names.SAME, handlerWithError);
-        serviceB.registerRequestHandler("test", StringMessageRequest.class, ThreadPool.Names.SAME, handler);
-        serviceB.registerRequestHandler("testError", StringMessageRequest.class, ThreadPool.Names.SAME, handlerWithError);
-
-        final Tracer tracer = new Tracer();
-        serviceA.addTracer(tracer);
-        serviceB.addTracer(tracer);
-
-        tracer.reset(4);
-        boolean timeout = randomBoolean();
-        TransportRequestOptions options = timeout ? new TransportRequestOptions().withTimeout(1) : TransportRequestOptions.EMPTY;
-        serviceA.sendRequest(nodeB, "test", new StringMessageRequest("", 10), options, noopResponseHandler);
-        requestCompleted.acquire();
-        tracer.expectedEvents.get().await();
-        assertThat("didn't see request sent", tracer.sawRequestSent, equalTo(true));
-        assertThat("didn't see request received", tracer.sawRequestReceived, equalTo(true));
-        assertThat("didn't see response sent", tracer.sawResponseSent, equalTo(true));
-        assertThat("didn't see response received", tracer.sawResponseReceived, equalTo(true));
-        assertThat("saw error sent", tracer.sawErrorSent, equalTo(false));
-
-        tracer.reset(4);
-        serviceA.sendRequest(nodeB, "testError", new StringMessageRequest(""), noopResponseHandler);
-        requestCompleted.acquire();
-        tracer.expectedEvents.get().await();
-        assertThat("didn't see request sent", tracer.sawRequestSent, equalTo(true));
-        assertThat("didn't see request received", tracer.sawRequestReceived, equalTo(true));
-        assertThat("saw response sent", tracer.sawResponseSent, equalTo(false));
-        assertThat("didn't see response received", tracer.sawResponseReceived, equalTo(true));
-        assertThat("didn't see error sent", tracer.sawErrorSent, equalTo(true));
-
-        String includeSettings;
-        String excludeSettings;
-        if (randomBoolean()) {
-            // sometimes leave include empty (default)
-            includeSettings = randomBoolean() ? "*" : "";
-            excludeSettings = "*Error";
-        } else {
-            includeSettings = "test";
-            excludeSettings = "DOESN'T_MATCH";
-        }
-
-        serviceA.applySettings(Settings.builder()
-                .put(TransportService.SETTING_TRACE_LOG_INCLUDE, includeSettings, TransportService.SETTING_TRACE_LOG_EXCLUDE, excludeSettings)
-                .build());
-
-        tracer.reset(4);
-        serviceA.sendRequest(nodeB, "test", new StringMessageRequest(""), noopResponseHandler);
-        requestCompleted.acquire();
-        tracer.expectedEvents.get().await();
-        assertThat("didn't see request sent", tracer.sawRequestSent, equalTo(true));
-        assertThat("didn't see request received", tracer.sawRequestReceived, equalTo(true));
-        assertThat("didn't see response sent", tracer.sawResponseSent, equalTo(true));
-        assertThat("didn't see response received", tracer.sawResponseReceived, equalTo(true));
-        assertThat("saw error sent", tracer.sawErrorSent, equalTo(false));
-
-        tracer.reset(2);
-        serviceA.sendRequest(nodeB, "testError", new StringMessageRequest(""), noopResponseHandler);
-        requestCompleted.acquire();
-        tracer.expectedEvents.get().await();
-        assertThat("saw request sent", tracer.sawRequestSent, equalTo(false));
-        assertThat("didn't see request received", tracer.sawRequestReceived, equalTo(true));
-        assertThat("saw response sent", tracer.sawResponseSent, equalTo(false));
-        assertThat("saw response received", tracer.sawResponseReceived, equalTo(false));
-        assertThat("didn't see error sent", tracer.sawErrorSent, equalTo(true));
-    }
-
-    private static class Tracer extends MockTransportService.Tracer {
-        public volatile boolean sawRequestSent;
-        public volatile boolean sawRequestReceived;
-        public volatile boolean sawResponseSent;
-        public volatile boolean sawErrorSent;
-        public volatile boolean sawResponseReceived;
-
-        public AtomicReference<CountDownLatch> expectedEvents = new AtomicReference<>();
-
-
-        @Override
-        public void receivedRequest(long requestId, String action) {
-            super.receivedRequest(requestId, action);
-            sawRequestReceived = true;
-            expectedEvents.get().countDown();
-        }
-
-        @Override
-        public void requestSent(DiscoveryNode node, long requestId, String action, TransportRequestOptions options) {
-            super.requestSent(node, requestId, action, options);
-            sawRequestSent = true;
-            expectedEvents.get().countDown();
-        }
-
-        @Override
-        public void responseSent(long requestId, String action) {
-            super.responseSent(requestId, action);
-            sawResponseSent = true;
-            expectedEvents.get().countDown();
-        }
-
-        @Override
-        public void responseSent(long requestId, String action, Throwable t) {
-            super.responseSent(requestId, action, t);
-            sawErrorSent = true;
-            expectedEvents.get().countDown();
-        }
-
-        @Override
-        public void receivedResponse(long requestId, DiscoveryNode sourceNode, String action) {
-            super.receivedResponse(requestId, sourceNode, action);
-            sawResponseReceived = true;
-            expectedEvents.get().countDown();
-        }
-
-        public void reset(int expectedCount) {
-            sawRequestSent = false;
-            sawRequestReceived = false;
-            sawResponseSent = false;
-            sawErrorSent = false;
-            sawResponseReceived = false;
-            expectedEvents.set(new CountDownLatch(expectedCount));
-        }
-    }
-
-
-    static class StringMessageRequest extends TransportRequest {
-
-        private String message;
-        private long timeout;
-
-        StringMessageRequest(String message, long timeout) {
-            this.message = message;
-            this.timeout = timeout;
-        }
-
-        StringMessageRequest() {
-        }
-
-        public StringMessageRequest(String message) {
-            this(message, -1);
-        }
-
-        public long timeout() {
-            return timeout;
-        }
-
-        @Override
-        public void readFrom(StreamInput in) throws IOException {
-            super.readFrom(in);
-            message = in.readString();
-            timeout = in.readLong();
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            super.writeTo(out);
-            out.writeString(message);
-            out.writeLong(timeout);
-        }
-    }
-
-    static class StringMessageResponse extends TransportResponse {
-
-        private String message;
-
-        StringMessageResponse(String message) {
-            this.message = message;
-        }
-
-        StringMessageResponse() {
-        }
-
-        @Override
-        public void readFrom(StreamInput in) throws IOException {
-            super.readFrom(in);
-            message = in.readString();
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            super.writeTo(out);
-            out.writeString(message);
-        }
-    }
-
-
-    static class Version0Request extends TransportRequest {
-
-        int value1;
-
-
-        @Override
-        public void readFrom(StreamInput in) throws IOException {
-            super.readFrom(in);
-            value1 = in.readInt();
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            super.writeTo(out);
-            out.writeInt(value1);
-        }
-    }
-
-    static class Version1Request extends Version0Request {
-
-        int value2;
-
-        @Override
-        public void readFrom(StreamInput in) throws IOException {
-            super.readFrom(in);
-            if (in.getVersion().onOrAfter(version1)) {
-                value2 = in.readInt();
-            }
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            super.writeTo(out);
-            if (out.getVersion().onOrAfter(version1)) {
-                out.writeInt(value2);
-            }
-        }
-    }
-
-    static class Version0Response extends TransportResponse {
-
-        int value1;
-
-        @Override
-        public void readFrom(StreamInput in) throws IOException {
-            super.readFrom(in);
-            value1 = in.readInt();
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            super.writeTo(out);
-            out.writeInt(value1);
-        }
-    }
-
-    static class Version1Response extends Version0Response {
-
-        int value2;
-
-        @Override
-        public void readFrom(StreamInput in) throws IOException {
-            super.readFrom(in);
-            if (in.getVersion().onOrAfter(version1)) {
-                value2 = in.readInt();
-            }
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            super.writeTo(out);
-            if (out.getVersion().onOrAfter(version1)) {
-                out.writeInt(value2);
-            }
-        }
-    }
-
-    @Test
-    public void testVersion_from0to1() throws Exception {
-        serviceB.registerRequestHandler("/version", Version1Request.class, ThreadPool.Names.SAME, new TransportRequestHandler<Version1Request>() {
-            @Override
-            public void messageReceived(Version1Request request, TransportChannel channel) throws Exception {
-                assertThat(request.value1, equalTo(1));
-                assertThat(request.value2, equalTo(0)); // not set, coming from service A
-                Version1Response response = new Version1Response();
-                response.value1 = 1;
-                response.value2 = 2;
-                channel.sendResponse(response);
-            }
-        });
-
-        Version0Request version0Request = new Version0Request();
-        version0Request.value1 = 1;
-        Version0Response version0Response = serviceA.submitRequest(nodeB, "/version", version0Request, new BaseTransportResponseHandler<Version0Response>() {
-            @Override
-            public Version0Response newInstance() {
-                return new Version0Response();
-            }
-
-            @Override
-            public void handleResponse(Version0Response response) {
-                assertThat(response.value1, equalTo(1));
-            }
-
-            @Override
-            public void handleException(TransportException exp) {
-                exp.printStackTrace();
-                fail();
-            }
-
-            @Override
-            public String executor() {
-                return ThreadPool.Names.SAME;
-            }
-        }).txGet();
-
-        assertThat(version0Response.value1, equalTo(1));
-    }
-
-    @Test
-    public void testVersion_from1to0() throws Exception {
-        serviceA.registerRequestHandler("/version", Version0Request.class, ThreadPool.Names.SAME, new TransportRequestHandler<Version0Request>() {
-            @Override
-            public void messageReceived(Version0Request request, TransportChannel channel) throws Exception {
-                assertThat(request.value1, equalTo(1));
-                Version0Response response = new Version0Response();
-                response.value1 = 1;
-                channel.sendResponse(response);
-            }
-        });
-
-        Version1Request version1Request = new Version1Request();
-        version1Request.value1 = 1;
-        version1Request.value2 = 2;
-        Version1Response version1Response = serviceB.submitRequest(nodeA, "/version", version1Request, new BaseTransportResponseHandler<Version1Response>() {
-            @Override
-            public Version1Response newInstance() {
-                return new Version1Response();
-            }
-
-            @Override
-            public void handleResponse(Version1Response response) {
-                assertThat(response.value1, equalTo(1));
-                assertThat(response.value2, equalTo(0)); // initial values, cause its serialized from version 0
-            }
-
-            @Override
-            public void handleException(TransportException exp) {
-                exp.printStackTrace();
-                fail();
-            }
-
-            @Override
-            public String executor() {
-                return ThreadPool.Names.SAME;
-            }
-        }).txGet();
-
-        assertThat(version1Response.value1, equalTo(1));
-        assertThat(version1Response.value2, equalTo(0));
-    }
-
-    @Test
-    public void testVersion_from1to1() throws Exception {
-        serviceB.registerRequestHandler("/version", Version1Request.class, ThreadPool.Names.SAME, new TransportRequestHandler<Version1Request>() {
-            @Override
-            public void messageReceived(Version1Request request, TransportChannel channel) throws Exception {
-                assertThat(request.value1, equalTo(1));
-                assertThat(request.value2, equalTo(2));
-                Version1Response response = new Version1Response();
-                response.value1 = 1;
-                response.value2 = 2;
-                channel.sendResponse(response);
-            }
-        });
-
-        Version1Request version1Request = new Version1Request();
-        version1Request.value1 = 1;
-        version1Request.value2 = 2;
-        Version1Response version1Response = serviceB.submitRequest(nodeB, "/version", version1Request, new BaseTransportResponseHandler<Version1Response>() {
-            @Override
-            public Version1Response newInstance() {
-                return new Version1Response();
-            }
-
-            @Override
-            public void handleResponse(Version1Response response) {
-                assertThat(response.value1, equalTo(1));
-                assertThat(response.value2, equalTo(2));
-            }
-
-            @Override
-            public void handleException(TransportException exp) {
-                exp.printStackTrace();
-                fail();
-            }
-
-            @Override
-            public String executor() {
-                return ThreadPool.Names.SAME;
-            }
-        }).txGet();
-
-        assertThat(version1Response.value1, equalTo(1));
-        assertThat(version1Response.value2, equalTo(2));
-    }
-
-    @Test
-    public void testVersion_from0to0() throws Exception {
-        serviceA.registerRequestHandler("/version", Version0Request.class, ThreadPool.Names.SAME, new TransportRequestHandler<Version0Request>() {
-            @Override
-            public void messageReceived(Version0Request request, TransportChannel channel) throws Exception {
-                assertThat(request.value1, equalTo(1));
-                Version0Response response = new Version0Response();
-                response.value1 = 1;
-                channel.sendResponse(response);
-            }
-        });
-
-        Version0Request version0Request = new Version0Request();
-        version0Request.value1 = 1;
-        Version0Response version0Response = serviceA.submitRequest(nodeA, "/version", version0Request, new BaseTransportResponseHandler<Version0Response>() {
-            @Override
-            public Version0Response newInstance() {
-                return new Version0Response();
-            }
-
-            @Override
-            public void handleResponse(Version0Response response) {
-                assertThat(response.value1, equalTo(1));
-            }
-
-            @Override
-            public void handleException(TransportException exp) {
-                exp.printStackTrace();
-                fail();
-            }
-
-            @Override
-            public String executor() {
-                return ThreadPool.Names.SAME;
-            }
-        }).txGet();
-
-        assertThat(version0Response.value1, equalTo(1));
-    }
-
-    @Test
-    public void testMockFailToSendNoConnectRule() {
-        serviceA.registerRequestHandler("sayHello", StringMessageRequest.class, ThreadPool.Names.GENERIC, new TransportRequestHandler<StringMessageRequest>() {
-            @Override
-            public void messageReceived(StringMessageRequest request, TransportChannel channel) throws Exception {
-                assertThat("moshe", equalTo(request.message));
-                throw new RuntimeException("bad message !!!");
-            }
-        });
-
-        serviceB.addFailToSendNoConnectRule(nodeA);
-
-        TransportFuture<StringMessageResponse> res = serviceB.submitRequest(nodeA, "sayHello",
-                new StringMessageRequest("moshe"), new BaseTransportResponseHandler<StringMessageResponse>() {
-                    @Override
-                    public StringMessageResponse newInstance() {
-                        return new StringMessageResponse();
-                    }
-
-                    @Override
-                    public String executor() {
-                        return ThreadPool.Names.GENERIC;
-                    }
-
-                    @Override
-                    public void handleResponse(StringMessageResponse response) {
-                        fail("got response instead of exception");
-                    }
-
-                    @Override
-                    public void handleException(TransportException exp) {
-                        assertThat(exp.getCause().getMessage(), endsWith("DISCONNECT: simulated"));
-                    }
-                });
-
-        try {
-            res.txGet();
-            fail("exception should be thrown");
-        } catch (Exception e) {
-            assertThat(e.getCause().getMessage(), endsWith("DISCONNECT: simulated"));
-        }
-
-        try {
-            serviceB.connectToNode(nodeA);
-            fail("exception should be thrown");
-        } catch (ConnectTransportException e) {
-            // all is well
-        }
-
-        try {
-            serviceB.connectToNodeLight(nodeA);
-            fail("exception should be thrown");
-        } catch (ConnectTransportException e) {
-            // all is well
-        }
-
-        serviceA.removeHandler("sayHello");
-    }
-
-    @Test
-    public void testMockUnresponsiveRule() {
-        serviceA.registerRequestHandler("sayHello", StringMessageRequest.class, ThreadPool.Names.GENERIC, new TransportRequestHandler<StringMessageRequest>() {
-            @Override
-            public void messageReceived(StringMessageRequest request, TransportChannel channel) throws Exception {
-                assertThat("moshe", equalTo(request.message));
-                throw new RuntimeException("bad message !!!");
-            }
-        });
-
-        serviceB.addUnresponsiveRule(nodeA);
-
-        TransportFuture<StringMessageResponse> res = serviceB.submitRequest(nodeA, "sayHello",
-                new StringMessageRequest("moshe"), TransportRequestOptions.options().withTimeout(100), new BaseTransportResponseHandler<StringMessageResponse>() {
-                    @Override
-                    public StringMessageResponse newInstance() {
-                        return new StringMessageResponse();
-                    }
-
-                    @Override
-                    public String executor() {
-                        return ThreadPool.Names.GENERIC;
-                    }
-
-                    @Override
-                    public void handleResponse(StringMessageResponse response) {
-                        fail("got response instead of exception");
-                    }
-
-                    @Override
-                    public void handleException(TransportException exp) {
-                        assertThat(exp, instanceOf(ReceiveTimeoutTransportException.class));
-                    }
-                });
-
-        try {
-            res.txGet();
-            fail("exception should be thrown");
-        } catch (Exception e) {
-            assertThat(e, instanceOf(ReceiveTimeoutTransportException.class));
-        }
-
-        try {
-            serviceB.connectToNode(nodeA);
-            fail("exception should be thrown");
-        } catch (ConnectTransportException e) {
-            // all is well
-        }
-
-        try {
-            serviceB.connectToNodeLight(nodeA);
-            fail("exception should be thrown");
-        } catch (ConnectTransportException e) {
-            // all is well
-        }
-
-        serviceA.removeHandler("sayHello");
-    }
-
-
-    @Test
-    public void testHostOnMessages() throws InterruptedException {
-        final CountDownLatch latch = new CountDownLatch(2);
-        final AtomicReference<TransportAddress> addressA = new AtomicReference<>();
-        final AtomicReference<TransportAddress> addressB = new AtomicReference<>();
-        serviceB.registerRequestHandler("action1", TestRequest.class, ThreadPool.Names.SAME, new TransportRequestHandler<TestRequest>() {
-            @Override
-            public void messageReceived(TestRequest request, TransportChannel channel) throws Exception {
-                addressA.set(request.remoteAddress());
-                channel.sendResponse(new TestResponse());
-                latch.countDown();
-            }
-        });
-        serviceA.sendRequest(nodeB, "action1", new TestRequest(), new TransportResponseHandler<TestResponse>() {
-            @Override
-            public TestResponse newInstance() {
-                return new TestResponse();
-            }
-
-            @Override
-            public void handleResponse(TestResponse response) {
-                addressB.set(response.remoteAddress());
-                latch.countDown();
-            }
-
-            @Override
-            public void handleException(TransportException exp) {
-                latch.countDown();
-            }
-
-            @Override
-            public String executor() {
-                return ThreadPool.Names.SAME;
-            }
-        });
-
-        if (!latch.await(10, TimeUnit.SECONDS)) {
-            fail("message round trip did not complete within a sensible time frame");
-        }
-
-        assertTrue(nodeA.address().sameHost(addressA.get()));
-        assertTrue(nodeB.address().sameHost(addressB.get()));
-    }
-
-    private static class TestRequest extends TransportRequest {
-    }
-
-    private static class TestResponse extends TransportResponse {
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java b/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java
index 03ac788..8aff7ea 100644
--- a/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java
+++ b/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java
@@ -53,7 +53,7 @@ import org.elasticsearch.index.query.BoolQueryBuilder;
 import org.elasticsearch.index.query.GeoShapeQueryBuilder;
 import org.elasticsearch.index.query.MoreLikeThisQueryBuilder;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.query.TermsQueryBuilder;
+import org.elasticsearch.index.query.TermsLookupQueryBuilder;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.rest.RestController;
 import org.elasticsearch.script.Script;
@@ -160,7 +160,7 @@ public class ContextAndHeaderTransportIT extends ESIntegTestCase {
                 .setSource(jsonBuilder().startObject().field("username", "foo").endObject()).get();
         transportClient().admin().indices().prepareRefresh(queryIndex, lookupIndex).get();
 
-        TermsQueryBuilder termsLookupFilterBuilder = QueryBuilders.termsLookupQuery("username").lookupIndex(lookupIndex).lookupType("type").lookupId("1").lookupPath("followers");
+        TermsLookupQueryBuilder termsLookupFilterBuilder = QueryBuilders.termsLookupQuery("username").lookupIndex(lookupIndex).lookupType("type").lookupId("1").lookupPath("followers");
         BoolQueryBuilder queryBuilder = QueryBuilders.boolQuery().must(QueryBuilders.matchAllQuery()).must(termsLookupFilterBuilder);
 
         SearchResponse searchResponse = transportClient()
diff --git a/core/src/test/java/org/elasticsearch/transport/local/SimpleLocalTransportTests.java b/core/src/test/java/org/elasticsearch/transport/local/SimpleLocalTransportTests.java
index e87b078..1d1ad8d 100644
--- a/core/src/test/java/org/elasticsearch/transport/local/SimpleLocalTransportTests.java
+++ b/core/src/test/java/org/elasticsearch/transport/local/SimpleLocalTransportTests.java
@@ -23,9 +23,9 @@ import org.elasticsearch.Version;
 import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.test.transport.MockTransportService;
-import org.elasticsearch.transport.AbstractSimpleTransportTests;
+import org.elasticsearch.transport.AbstractSimpleTransportTestCase;
 
-public class SimpleLocalTransportTests extends AbstractSimpleTransportTests {
+public class SimpleLocalTransportTests extends AbstractSimpleTransportTestCase {
 
     @Override
     protected MockTransportService build(Settings settings, Version version, NamedWriteableRegistry namedWriteableRegistry) {
diff --git a/core/src/test/java/org/elasticsearch/transport/netty/SimpleNettyTransportTests.java b/core/src/test/java/org/elasticsearch/transport/netty/SimpleNettyTransportTests.java
index 5b85571..923ed63 100644
--- a/core/src/test/java/org/elasticsearch/transport/netty/SimpleNettyTransportTests.java
+++ b/core/src/test/java/org/elasticsearch/transport/netty/SimpleNettyTransportTests.java
@@ -27,14 +27,14 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.InetSocketTransportAddress;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.test.transport.MockTransportService;
-import org.elasticsearch.transport.AbstractSimpleTransportTests;
+import org.elasticsearch.transport.AbstractSimpleTransportTestCase;
 import org.elasticsearch.transport.ConnectTransportException;
 import org.junit.Test;
 
 import java.net.InetAddress;
 import java.net.UnknownHostException;
 
-public class SimpleNettyTransportTests extends AbstractSimpleTransportTests {
+public class SimpleNettyTransportTests extends AbstractSimpleTransportTestCase {
 
     @Override
     protected MockTransportService build(Settings settings, Version version, NamedWriteableRegistry namedWriteableRegistry) {
diff --git a/core/src/test/java/org/elasticsearch/validate/SimpleValidateQueryIT.java b/core/src/test/java/org/elasticsearch/validate/SimpleValidateQueryIT.java
index 832f5d3..b55fd55 100644
--- a/core/src/test/java/org/elasticsearch/validate/SimpleValidateQueryIT.java
+++ b/core/src/test/java/org/elasticsearch/validate/SimpleValidateQueryIT.java
@@ -236,7 +236,7 @@ public class SimpleValidateQueryIT extends ESIntegTestCase {
                 containsString("+field:pidgin (field:huge field:brown)"), true);
         assertExplanation(QueryBuilders.commonTermsQuery("field", "the brown").analyzer("stop"),
                 containsString("field:brown"), true);
-
+        
         // match queries with cutoff frequency
         assertExplanation(QueryBuilders.matchQuery("field", "huge brown pidgin").cutoffFrequency(1),
                 containsString("+field:pidgin (field:huge field:brown)"), true);
@@ -276,7 +276,11 @@ public class SimpleValidateQueryIT extends ESIntegTestCase {
         assertThat(client().admin().indices().prepareValidateQuery("test").setSource(new BytesArray("{\"query\": {\"term\" : { \"user\" : \"kimchy\" }}, \"foo\": \"bar\"}")).get().isValid(), equalTo(false));
     }
 
-    private static void assertExplanation(QueryBuilder queryBuilder, Matcher<String> matcher, boolean withRewrite) {
+    private void assertExplanation(QueryBuilder queryBuilder, Matcher<String> matcher) {
+        assertExplanation(queryBuilder, matcher, false);
+    }
+
+    private void assertExplanation(QueryBuilder queryBuilder, Matcher<String> matcher, boolean withRewrite) {
         ValidateQueryResponse response = client().admin().indices().prepareValidateQuery("test")
                 .setTypes("type1")
                 .setQuery(queryBuilder)
diff --git a/core/src/test/java/org/elasticsearch/watcher/FileWatcherTest.java b/core/src/test/java/org/elasticsearch/watcher/FileWatcherTest.java
deleted file mode 100644
index 537fd87..0000000
--- a/core/src/test/java/org/elasticsearch/watcher/FileWatcherTest.java
+++ /dev/null
@@ -1,402 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.watcher;
-
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.LuceneTestCase;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.io.BufferedWriter;
-import java.io.IOException;
-import java.nio.charset.Charset;
-import java.nio.file.Files;
-import java.nio.file.Path;
-import java.nio.file.StandardOpenOption;
-import java.util.ArrayList;
-import java.util.List;
-
-import static org.hamcrest.Matchers.*;
-
-@LuceneTestCase.SuppressFileSystems("ExtrasFS")
-public class FileWatcherTest extends ESTestCase {
-
-    private class RecordingChangeListener extends FileChangesListener {
-
-        private Path rootDir;
-
-        private RecordingChangeListener(Path rootDir) {
-            this.rootDir = rootDir;
-        }
-
-        private String getRelativeFileName(Path file) {
-            return rootDir.toUri().relativize(file.toUri()).getPath();
-        }
-
-        private List<String> notifications = new ArrayList<>();
-
-        @Override
-        public void onFileInit(Path file) {
-            notifications.add("onFileInit: " + getRelativeFileName(file));
-        }
-
-        @Override
-        public void onDirectoryInit(Path file) {
-            notifications.add("onDirectoryInit: " + getRelativeFileName(file));
-        }
-
-        @Override
-        public void onFileCreated(Path file) {
-            notifications.add("onFileCreated: " + getRelativeFileName(file));
-        }
-
-        @Override
-        public void onFileDeleted(Path file) {
-            notifications.add("onFileDeleted: " + getRelativeFileName(file));
-        }
-
-        @Override
-        public void onFileChanged(Path file) {
-            notifications.add("onFileChanged: " + getRelativeFileName(file));
-        }
-
-        @Override
-        public void onDirectoryCreated(Path file) {
-            notifications.add("onDirectoryCreated: " + getRelativeFileName(file));
-        }
-
-        @Override
-        public void onDirectoryDeleted(Path file) {
-            notifications.add("onDirectoryDeleted: " + getRelativeFileName(file));
-        }
-
-        public List<String> notifications() {
-            return notifications;
-        }
-    }
-
-    @Test
-    public void testSimpleFileOperations() throws IOException {
-        Path tempDir = createTempDir();
-        RecordingChangeListener changes = new RecordingChangeListener(tempDir);
-        Path testFile = tempDir.resolve("test.txt");
-        touch(testFile);
-        FileWatcher fileWatcher = new FileWatcher(testFile);
-        fileWatcher.addListener(changes);
-        fileWatcher.init();
-        assertThat(changes.notifications(), contains(equalTo("onFileInit: test.txt")));
-
-        changes.notifications().clear();
-        fileWatcher.checkAndNotify();
-        assertThat(changes.notifications(), hasSize(0));
-
-        append("Test", testFile, Charset.defaultCharset());
-        fileWatcher.checkAndNotify();
-        assertThat(changes.notifications(), contains(equalTo("onFileChanged: test.txt")));
-
-        changes.notifications().clear();
-        fileWatcher.checkAndNotify();
-        assertThat(changes.notifications(), hasSize(0));
-
-        Files.delete(testFile);
-        fileWatcher.checkAndNotify();
-        assertThat(changes.notifications(), contains(equalTo("onFileDeleted: test.txt")));
-
-    }
-
-    @Test
-    public void testSimpleDirectoryOperations() throws IOException {
-        Path tempDir = createTempDir();
-        RecordingChangeListener changes = new RecordingChangeListener(tempDir);
-        Path testDir = tempDir.resolve("test-dir");
-        Files.createDirectories(testDir);
-        touch(testDir.resolve("test.txt"));
-        touch(testDir.resolve("test0.txt"));
-
-        FileWatcher fileWatcher = new FileWatcher(testDir);
-        fileWatcher.addListener(changes);
-        fileWatcher.init();
-        assertThat(changes.notifications(), contains(
-                equalTo("onDirectoryInit: test-dir/"),
-                equalTo("onFileInit: test-dir/test.txt"),
-                equalTo("onFileInit: test-dir/test0.txt")
-        ));
-
-        changes.notifications().clear();
-        fileWatcher.checkAndNotify();
-        assertThat(changes.notifications(), hasSize(0));
-
-        for (int i = 0; i < 4; i++) {
-            touch(testDir.resolve("test" + i + ".txt"));
-        }
-        // Make sure that first file is modified
-        append("Test", testDir.resolve("test0.txt"), Charset.defaultCharset());
-
-        fileWatcher.checkAndNotify();
-        assertThat(changes.notifications(), contains(
-                equalTo("onFileChanged: test-dir/test0.txt"),
-                equalTo("onFileCreated: test-dir/test1.txt"),
-                equalTo("onFileCreated: test-dir/test2.txt"),
-                equalTo("onFileCreated: test-dir/test3.txt")
-        ));
-
-        changes.notifications().clear();
-        fileWatcher.checkAndNotify();
-        assertThat(changes.notifications(), hasSize(0));
-
-        Files.delete(testDir.resolve("test1.txt"));
-        Files.delete(testDir.resolve("test2.txt"));
-
-        fileWatcher.checkAndNotify();
-        assertThat(changes.notifications(), contains(
-                equalTo("onFileDeleted: test-dir/test1.txt"),
-                equalTo("onFileDeleted: test-dir/test2.txt")
-        ));
-
-        changes.notifications().clear();
-        fileWatcher.checkAndNotify();
-        assertThat(changes.notifications(), hasSize(0));
-
-        Files.delete(testDir.resolve("test0.txt"));
-        touch(testDir.resolve("test2.txt"));
-        touch(testDir.resolve("test4.txt"));
-        fileWatcher.checkAndNotify();
-
-        assertThat(changes.notifications(), contains(
-                equalTo("onFileDeleted: test-dir/test0.txt"),
-                equalTo("onFileCreated: test-dir/test2.txt"),
-                equalTo("onFileCreated: test-dir/test4.txt")
-        ));
-
-
-        changes.notifications().clear();
-
-        Files.delete(testDir.resolve("test3.txt"));
-        Files.delete(testDir.resolve("test4.txt"));
-        fileWatcher.checkAndNotify();
-        assertThat(changes.notifications(), contains(
-                equalTo("onFileDeleted: test-dir/test3.txt"),
-                equalTo("onFileDeleted: test-dir/test4.txt")
-        ));
-
-
-        changes.notifications().clear();
-        if (Files.exists(testDir)) {
-            IOUtils.rm(testDir);
-        }
-        fileWatcher.checkAndNotify();
-
-        assertThat(changes.notifications(), contains(
-                equalTo("onFileDeleted: test-dir/test.txt"),
-                equalTo("onFileDeleted: test-dir/test2.txt"),
-                equalTo("onDirectoryDeleted: test-dir")
-        ));
-
-    }
-
-    @Test
-    public void testNestedDirectoryOperations() throws IOException {
-        Path tempDir = createTempDir();
-        RecordingChangeListener changes = new RecordingChangeListener(tempDir);
-        Path testDir = tempDir.resolve("test-dir");
-        Files.createDirectories(testDir);
-        touch(testDir.resolve("test.txt"));
-        Files.createDirectories(testDir.resolve("sub-dir"));
-        touch(testDir.resolve("sub-dir/test0.txt"));
-
-        FileWatcher fileWatcher = new FileWatcher(testDir);
-        fileWatcher.addListener(changes);
-        fileWatcher.init();
-        assertThat(changes.notifications(), contains(
-                equalTo("onDirectoryInit: test-dir/"),
-                equalTo("onDirectoryInit: test-dir/sub-dir/"),
-                equalTo("onFileInit: test-dir/sub-dir/test0.txt"),
-                equalTo("onFileInit: test-dir/test.txt")
-        ));
-
-        changes.notifications().clear();
-        fileWatcher.checkAndNotify();
-        assertThat(changes.notifications(), hasSize(0));
-
-        // Create new file in subdirectory
-        touch(testDir.resolve("sub-dir/test1.txt"));
-        fileWatcher.checkAndNotify();
-        assertThat(changes.notifications(), contains(
-                equalTo("onFileCreated: test-dir/sub-dir/test1.txt")
-        ));
-
-        changes.notifications().clear();
-        fileWatcher.checkAndNotify();
-        assertThat(changes.notifications(), hasSize(0));
-
-        // Create new subdirectory in subdirectory
-        Files.createDirectories(testDir.resolve("first-level"));
-        touch(testDir.resolve("first-level/file1.txt"));
-        Files.createDirectories(testDir.resolve("first-level/second-level"));
-        touch(testDir.resolve("first-level/second-level/file2.txt"));
-        fileWatcher.checkAndNotify();
-        assertThat(changes.notifications(), contains(
-                equalTo("onDirectoryCreated: test-dir/first-level/"),
-                equalTo("onFileCreated: test-dir/first-level/file1.txt"),
-                equalTo("onDirectoryCreated: test-dir/first-level/second-level/"),
-                equalTo("onFileCreated: test-dir/first-level/second-level/file2.txt")
-        ));
-
-        changes.notifications().clear();
-        fileWatcher.checkAndNotify();
-        assertThat(changes.notifications(), hasSize(0));
-
-        // Delete a directory, check notifications for
-        Path path = testDir.resolve("first-level");
-        if (Files.exists(path)) {
-            IOUtils.rm(path);
-        }
-        fileWatcher.checkAndNotify();
-        assertThat(changes.notifications(), contains(
-                equalTo("onFileDeleted: test-dir/first-level/file1.txt"),
-                equalTo("onFileDeleted: test-dir/first-level/second-level/file2.txt"),
-                equalTo("onDirectoryDeleted: test-dir/first-level/second-level"),
-                equalTo("onDirectoryDeleted: test-dir/first-level")
-        ));
-    }
-
-    @Test
-    public void testFileReplacingDirectory() throws IOException {
-        Path tempDir = createTempDir();
-        RecordingChangeListener changes = new RecordingChangeListener(tempDir);
-        Path testDir = tempDir.resolve("test-dir");
-        Files.createDirectories(testDir);
-        Path subDir = testDir.resolve("sub-dir");
-        Files.createDirectories(subDir);
-        touch(subDir.resolve("test0.txt"));
-        touch(subDir.resolve("test1.txt"));
-
-        FileWatcher fileWatcher = new FileWatcher(testDir);
-        fileWatcher.addListener(changes);
-        fileWatcher.init();
-        assertThat(changes.notifications(), contains(
-                equalTo("onDirectoryInit: test-dir/"),
-                equalTo("onDirectoryInit: test-dir/sub-dir/"),
-                equalTo("onFileInit: test-dir/sub-dir/test0.txt"),
-                equalTo("onFileInit: test-dir/sub-dir/test1.txt")
-        ));
-
-        changes.notifications().clear();
-
-        if (Files.exists(subDir)) {
-            IOUtils.rm(subDir);
-        }
-        touch(subDir);
-        fileWatcher.checkAndNotify();
-        assertThat(changes.notifications(), contains(
-                equalTo("onFileDeleted: test-dir/sub-dir/test0.txt"),
-                equalTo("onFileDeleted: test-dir/sub-dir/test1.txt"),
-                equalTo("onDirectoryDeleted: test-dir/sub-dir"),
-                equalTo("onFileCreated: test-dir/sub-dir")
-        ));
-
-        changes.notifications().clear();
-
-        Files.delete(subDir);
-        Files.createDirectories(subDir);
-
-        fileWatcher.checkAndNotify();
-        assertThat(changes.notifications(), contains(
-                equalTo("onFileDeleted: test-dir/sub-dir/"),
-                equalTo("onDirectoryCreated: test-dir/sub-dir/")
-        ));
-    }
-
-    @Test
-    public void testEmptyDirectory() throws IOException {
-        Path tempDir = createTempDir();
-        RecordingChangeListener changes = new RecordingChangeListener(tempDir);
-        Path testDir = tempDir.resolve("test-dir");
-        Files.createDirectories(testDir);
-        touch(testDir.resolve("test0.txt"));
-        touch(testDir.resolve("test1.txt"));
-
-        FileWatcher fileWatcher = new FileWatcher(testDir);
-        fileWatcher.addListener(changes);
-        fileWatcher.init();
-        changes.notifications().clear();
-
-        Files.delete(testDir.resolve("test0.txt"));
-        Files.delete(testDir.resolve("test1.txt"));
-        fileWatcher.checkAndNotify();
-        assertThat(changes.notifications(), contains(
-                equalTo("onFileDeleted: test-dir/test0.txt"),
-                equalTo("onFileDeleted: test-dir/test1.txt")
-        ));
-    }
-
-    @Test
-    public void testNoDirectoryOnInit() throws IOException {
-        Path tempDir = createTempDir();
-        RecordingChangeListener changes = new RecordingChangeListener(tempDir);
-        Path testDir = tempDir.resolve("test-dir");
-
-        FileWatcher fileWatcher = new FileWatcher(testDir);
-        fileWatcher.addListener(changes);
-        fileWatcher.init();
-        assertThat(changes.notifications(), hasSize(0));
-        changes.notifications().clear();
-
-        Files.createDirectories(testDir);
-        touch(testDir.resolve("test0.txt"));
-        touch(testDir.resolve("test1.txt"));
-
-        fileWatcher.checkAndNotify();
-        assertThat(changes.notifications(), contains(
-                equalTo("onDirectoryCreated: test-dir/"),
-                equalTo("onFileCreated: test-dir/test0.txt"),
-                equalTo("onFileCreated: test-dir/test1.txt")
-        ));
-    }
-
-    @Test
-    public void testNoFileOnInit() throws IOException {
-        Path tempDir = createTempDir();
-        RecordingChangeListener changes = new RecordingChangeListener(tempDir);
-        Path testFile = tempDir.resolve("testfile.txt");
-
-        FileWatcher fileWatcher = new FileWatcher(testFile);
-        fileWatcher.addListener(changes);
-        fileWatcher.init();
-        assertThat(changes.notifications(), hasSize(0));
-        changes.notifications().clear();
-
-        touch(testFile);
-
-        fileWatcher.checkAndNotify();
-        assertThat(changes.notifications(), contains(
-                equalTo("onFileCreated: testfile.txt")
-        ));
-    }
-    
-    static void touch(Path path) throws IOException {
-        Files.newOutputStream(path).close();
-    }
-    
-    static void append(String string, Path path, Charset cs) throws IOException {
-        try (BufferedWriter writer = Files.newBufferedWriter(path, cs, StandardOpenOption.APPEND)) {
-            writer.append(string);
-        }
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/watcher/FileWatcherTests.java b/core/src/test/java/org/elasticsearch/watcher/FileWatcherTests.java
new file mode 100644
index 0000000..14f7eca
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/watcher/FileWatcherTests.java
@@ -0,0 +1,402 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.watcher;
+
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.LuceneTestCase;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.io.BufferedWriter;
+import java.io.IOException;
+import java.nio.charset.Charset;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.StandardOpenOption;
+import java.util.ArrayList;
+import java.util.List;
+
+import static org.hamcrest.Matchers.*;
+
+@LuceneTestCase.SuppressFileSystems("ExtrasFS")
+public class FileWatcherTests extends ESTestCase {
+
+    private class RecordingChangeListener extends FileChangesListener {
+
+        private Path rootDir;
+
+        private RecordingChangeListener(Path rootDir) {
+            this.rootDir = rootDir;
+        }
+
+        private String getRelativeFileName(Path file) {
+            return rootDir.toUri().relativize(file.toUri()).getPath();
+        }
+
+        private List<String> notifications = new ArrayList<>();
+
+        @Override
+        public void onFileInit(Path file) {
+            notifications.add("onFileInit: " + getRelativeFileName(file));
+        }
+
+        @Override
+        public void onDirectoryInit(Path file) {
+            notifications.add("onDirectoryInit: " + getRelativeFileName(file));
+        }
+
+        @Override
+        public void onFileCreated(Path file) {
+            notifications.add("onFileCreated: " + getRelativeFileName(file));
+        }
+
+        @Override
+        public void onFileDeleted(Path file) {
+            notifications.add("onFileDeleted: " + getRelativeFileName(file));
+        }
+
+        @Override
+        public void onFileChanged(Path file) {
+            notifications.add("onFileChanged: " + getRelativeFileName(file));
+        }
+
+        @Override
+        public void onDirectoryCreated(Path file) {
+            notifications.add("onDirectoryCreated: " + getRelativeFileName(file));
+        }
+
+        @Override
+        public void onDirectoryDeleted(Path file) {
+            notifications.add("onDirectoryDeleted: " + getRelativeFileName(file));
+        }
+
+        public List<String> notifications() {
+            return notifications;
+        }
+    }
+
+    @Test
+    public void testSimpleFileOperations() throws IOException {
+        Path tempDir = createTempDir();
+        RecordingChangeListener changes = new RecordingChangeListener(tempDir);
+        Path testFile = tempDir.resolve("test.txt");
+        touch(testFile);
+        FileWatcher fileWatcher = new FileWatcher(testFile);
+        fileWatcher.addListener(changes);
+        fileWatcher.init();
+        assertThat(changes.notifications(), contains(equalTo("onFileInit: test.txt")));
+
+        changes.notifications().clear();
+        fileWatcher.checkAndNotify();
+        assertThat(changes.notifications(), hasSize(0));
+
+        append("Test", testFile, Charset.defaultCharset());
+        fileWatcher.checkAndNotify();
+        assertThat(changes.notifications(), contains(equalTo("onFileChanged: test.txt")));
+
+        changes.notifications().clear();
+        fileWatcher.checkAndNotify();
+        assertThat(changes.notifications(), hasSize(0));
+
+        Files.delete(testFile);
+        fileWatcher.checkAndNotify();
+        assertThat(changes.notifications(), contains(equalTo("onFileDeleted: test.txt")));
+
+    }
+
+    @Test
+    public void testSimpleDirectoryOperations() throws IOException {
+        Path tempDir = createTempDir();
+        RecordingChangeListener changes = new RecordingChangeListener(tempDir);
+        Path testDir = tempDir.resolve("test-dir");
+        Files.createDirectories(testDir);
+        touch(testDir.resolve("test.txt"));
+        touch(testDir.resolve("test0.txt"));
+
+        FileWatcher fileWatcher = new FileWatcher(testDir);
+        fileWatcher.addListener(changes);
+        fileWatcher.init();
+        assertThat(changes.notifications(), contains(
+                equalTo("onDirectoryInit: test-dir/"),
+                equalTo("onFileInit: test-dir/test.txt"),
+                equalTo("onFileInit: test-dir/test0.txt")
+        ));
+
+        changes.notifications().clear();
+        fileWatcher.checkAndNotify();
+        assertThat(changes.notifications(), hasSize(0));
+
+        for (int i = 0; i < 4; i++) {
+            touch(testDir.resolve("test" + i + ".txt"));
+        }
+        // Make sure that first file is modified
+        append("Test", testDir.resolve("test0.txt"), Charset.defaultCharset());
+
+        fileWatcher.checkAndNotify();
+        assertThat(changes.notifications(), contains(
+                equalTo("onFileChanged: test-dir/test0.txt"),
+                equalTo("onFileCreated: test-dir/test1.txt"),
+                equalTo("onFileCreated: test-dir/test2.txt"),
+                equalTo("onFileCreated: test-dir/test3.txt")
+        ));
+
+        changes.notifications().clear();
+        fileWatcher.checkAndNotify();
+        assertThat(changes.notifications(), hasSize(0));
+
+        Files.delete(testDir.resolve("test1.txt"));
+        Files.delete(testDir.resolve("test2.txt"));
+
+        fileWatcher.checkAndNotify();
+        assertThat(changes.notifications(), contains(
+                equalTo("onFileDeleted: test-dir/test1.txt"),
+                equalTo("onFileDeleted: test-dir/test2.txt")
+        ));
+
+        changes.notifications().clear();
+        fileWatcher.checkAndNotify();
+        assertThat(changes.notifications(), hasSize(0));
+
+        Files.delete(testDir.resolve("test0.txt"));
+        touch(testDir.resolve("test2.txt"));
+        touch(testDir.resolve("test4.txt"));
+        fileWatcher.checkAndNotify();
+
+        assertThat(changes.notifications(), contains(
+                equalTo("onFileDeleted: test-dir/test0.txt"),
+                equalTo("onFileCreated: test-dir/test2.txt"),
+                equalTo("onFileCreated: test-dir/test4.txt")
+        ));
+
+
+        changes.notifications().clear();
+
+        Files.delete(testDir.resolve("test3.txt"));
+        Files.delete(testDir.resolve("test4.txt"));
+        fileWatcher.checkAndNotify();
+        assertThat(changes.notifications(), contains(
+                equalTo("onFileDeleted: test-dir/test3.txt"),
+                equalTo("onFileDeleted: test-dir/test4.txt")
+        ));
+
+
+        changes.notifications().clear();
+        if (Files.exists(testDir)) {
+            IOUtils.rm(testDir);
+        }
+        fileWatcher.checkAndNotify();
+
+        assertThat(changes.notifications(), contains(
+                equalTo("onFileDeleted: test-dir/test.txt"),
+                equalTo("onFileDeleted: test-dir/test2.txt"),
+                equalTo("onDirectoryDeleted: test-dir")
+        ));
+
+    }
+
+    @Test
+    public void testNestedDirectoryOperations() throws IOException {
+        Path tempDir = createTempDir();
+        RecordingChangeListener changes = new RecordingChangeListener(tempDir);
+        Path testDir = tempDir.resolve("test-dir");
+        Files.createDirectories(testDir);
+        touch(testDir.resolve("test.txt"));
+        Files.createDirectories(testDir.resolve("sub-dir"));
+        touch(testDir.resolve("sub-dir/test0.txt"));
+
+        FileWatcher fileWatcher = new FileWatcher(testDir);
+        fileWatcher.addListener(changes);
+        fileWatcher.init();
+        assertThat(changes.notifications(), contains(
+                equalTo("onDirectoryInit: test-dir/"),
+                equalTo("onDirectoryInit: test-dir/sub-dir/"),
+                equalTo("onFileInit: test-dir/sub-dir/test0.txt"),
+                equalTo("onFileInit: test-dir/test.txt")
+        ));
+
+        changes.notifications().clear();
+        fileWatcher.checkAndNotify();
+        assertThat(changes.notifications(), hasSize(0));
+
+        // Create new file in subdirectory
+        touch(testDir.resolve("sub-dir/test1.txt"));
+        fileWatcher.checkAndNotify();
+        assertThat(changes.notifications(), contains(
+                equalTo("onFileCreated: test-dir/sub-dir/test1.txt")
+        ));
+
+        changes.notifications().clear();
+        fileWatcher.checkAndNotify();
+        assertThat(changes.notifications(), hasSize(0));
+
+        // Create new subdirectory in subdirectory
+        Files.createDirectories(testDir.resolve("first-level"));
+        touch(testDir.resolve("first-level/file1.txt"));
+        Files.createDirectories(testDir.resolve("first-level/second-level"));
+        touch(testDir.resolve("first-level/second-level/file2.txt"));
+        fileWatcher.checkAndNotify();
+        assertThat(changes.notifications(), contains(
+                equalTo("onDirectoryCreated: test-dir/first-level/"),
+                equalTo("onFileCreated: test-dir/first-level/file1.txt"),
+                equalTo("onDirectoryCreated: test-dir/first-level/second-level/"),
+                equalTo("onFileCreated: test-dir/first-level/second-level/file2.txt")
+        ));
+
+        changes.notifications().clear();
+        fileWatcher.checkAndNotify();
+        assertThat(changes.notifications(), hasSize(0));
+
+        // Delete a directory, check notifications for
+        Path path = testDir.resolve("first-level");
+        if (Files.exists(path)) {
+            IOUtils.rm(path);
+        }
+        fileWatcher.checkAndNotify();
+        assertThat(changes.notifications(), contains(
+                equalTo("onFileDeleted: test-dir/first-level/file1.txt"),
+                equalTo("onFileDeleted: test-dir/first-level/second-level/file2.txt"),
+                equalTo("onDirectoryDeleted: test-dir/first-level/second-level"),
+                equalTo("onDirectoryDeleted: test-dir/first-level")
+        ));
+    }
+
+    @Test
+    public void testFileReplacingDirectory() throws IOException {
+        Path tempDir = createTempDir();
+        RecordingChangeListener changes = new RecordingChangeListener(tempDir);
+        Path testDir = tempDir.resolve("test-dir");
+        Files.createDirectories(testDir);
+        Path subDir = testDir.resolve("sub-dir");
+        Files.createDirectories(subDir);
+        touch(subDir.resolve("test0.txt"));
+        touch(subDir.resolve("test1.txt"));
+
+        FileWatcher fileWatcher = new FileWatcher(testDir);
+        fileWatcher.addListener(changes);
+        fileWatcher.init();
+        assertThat(changes.notifications(), contains(
+                equalTo("onDirectoryInit: test-dir/"),
+                equalTo("onDirectoryInit: test-dir/sub-dir/"),
+                equalTo("onFileInit: test-dir/sub-dir/test0.txt"),
+                equalTo("onFileInit: test-dir/sub-dir/test1.txt")
+        ));
+
+        changes.notifications().clear();
+
+        if (Files.exists(subDir)) {
+            IOUtils.rm(subDir);
+        }
+        touch(subDir);
+        fileWatcher.checkAndNotify();
+        assertThat(changes.notifications(), contains(
+                equalTo("onFileDeleted: test-dir/sub-dir/test0.txt"),
+                equalTo("onFileDeleted: test-dir/sub-dir/test1.txt"),
+                equalTo("onDirectoryDeleted: test-dir/sub-dir"),
+                equalTo("onFileCreated: test-dir/sub-dir")
+        ));
+
+        changes.notifications().clear();
+
+        Files.delete(subDir);
+        Files.createDirectories(subDir);
+
+        fileWatcher.checkAndNotify();
+        assertThat(changes.notifications(), contains(
+                equalTo("onFileDeleted: test-dir/sub-dir/"),
+                equalTo("onDirectoryCreated: test-dir/sub-dir/")
+        ));
+    }
+
+    @Test
+    public void testEmptyDirectory() throws IOException {
+        Path tempDir = createTempDir();
+        RecordingChangeListener changes = new RecordingChangeListener(tempDir);
+        Path testDir = tempDir.resolve("test-dir");
+        Files.createDirectories(testDir);
+        touch(testDir.resolve("test0.txt"));
+        touch(testDir.resolve("test1.txt"));
+
+        FileWatcher fileWatcher = new FileWatcher(testDir);
+        fileWatcher.addListener(changes);
+        fileWatcher.init();
+        changes.notifications().clear();
+
+        Files.delete(testDir.resolve("test0.txt"));
+        Files.delete(testDir.resolve("test1.txt"));
+        fileWatcher.checkAndNotify();
+        assertThat(changes.notifications(), contains(
+                equalTo("onFileDeleted: test-dir/test0.txt"),
+                equalTo("onFileDeleted: test-dir/test1.txt")
+        ));
+    }
+
+    @Test
+    public void testNoDirectoryOnInit() throws IOException {
+        Path tempDir = createTempDir();
+        RecordingChangeListener changes = new RecordingChangeListener(tempDir);
+        Path testDir = tempDir.resolve("test-dir");
+
+        FileWatcher fileWatcher = new FileWatcher(testDir);
+        fileWatcher.addListener(changes);
+        fileWatcher.init();
+        assertThat(changes.notifications(), hasSize(0));
+        changes.notifications().clear();
+
+        Files.createDirectories(testDir);
+        touch(testDir.resolve("test0.txt"));
+        touch(testDir.resolve("test1.txt"));
+
+        fileWatcher.checkAndNotify();
+        assertThat(changes.notifications(), contains(
+                equalTo("onDirectoryCreated: test-dir/"),
+                equalTo("onFileCreated: test-dir/test0.txt"),
+                equalTo("onFileCreated: test-dir/test1.txt")
+        ));
+    }
+
+    @Test
+    public void testNoFileOnInit() throws IOException {
+        Path tempDir = createTempDir();
+        RecordingChangeListener changes = new RecordingChangeListener(tempDir);
+        Path testFile = tempDir.resolve("testfile.txt");
+
+        FileWatcher fileWatcher = new FileWatcher(testFile);
+        fileWatcher.addListener(changes);
+        fileWatcher.init();
+        assertThat(changes.notifications(), hasSize(0));
+        changes.notifications().clear();
+
+        touch(testFile);
+
+        fileWatcher.checkAndNotify();
+        assertThat(changes.notifications(), contains(
+                equalTo("onFileCreated: testfile.txt")
+        ));
+    }
+    
+    static void touch(Path path) throws IOException {
+        Files.newOutputStream(path).close();
+    }
+    
+    static void append(String string, Path path, Charset cs) throws IOException {
+        try (BufferedWriter writer = Files.newBufferedWriter(path, cs, StandardOpenOption.APPEND)) {
+            writer.append(string);
+        }
+    }
+}
\ No newline at end of file
diff --git a/dev-tools/pom.xml b/dev-tools/pom.xml
index 760f135..f02d6a8 100644
--- a/dev-tools/pom.xml
+++ b/dev-tools/pom.xml
@@ -2,7 +2,7 @@
   <modelVersion>4.0.0</modelVersion>
   <groupId>org.elasticsearch</groupId>
   <artifactId>dev-tools</artifactId>
-  <version>2.1.0-SNAPSHOT</version>
+  <version>3.0.0-SNAPSHOT</version>
   <name>Build Tools and Resources</name>
   <description>Tools to assist in building and developing in the Elasticsearch project</description>
   <parent>
diff --git a/dev-tools/src/main/resources/ant/integration-tests.xml b/dev-tools/src/main/resources/ant/integration-tests.xml
index 9c8df5d..23df373 100644
--- a/dev-tools/src/main/resources/ant/integration-tests.xml
+++ b/dev-tools/src/main/resources/ant/integration-tests.xml
@@ -297,6 +297,11 @@
     <startup-elasticsearch/>
   </target>
 
+  <!-- unzip core release artifact then start ES -->
+  <target name="start-external-cluster" depends="setup-workspace">
+    <startup-elasticsearch/>
+  </target>
+
   <target name="stop-external-cluster" if="integ.pidfile.exists">
     <stop-node/>
   </target>
diff --git a/distribution/deb/pom.xml b/distribution/deb/pom.xml
index aae6f6f..7237674 100644
--- a/distribution/deb/pom.xml
+++ b/distribution/deb/pom.xml
@@ -6,7 +6,7 @@
     <parent>
         <groupId>org.elasticsearch.distribution</groupId>
         <artifactId>distributions</artifactId>
-        <version>2.1.0-SNAPSHOT</version>
+        <version>3.0.0-SNAPSHOT</version>
     </parent>
 
     <groupId>org.elasticsearch.distribution.deb</groupId>
diff --git a/distribution/pom.xml b/distribution/pom.xml
index 94cd8ad..44c59b8 100644
--- a/distribution/pom.xml
+++ b/distribution/pom.xml
@@ -6,7 +6,7 @@
     <parent>
         <groupId>org.elasticsearch</groupId>
         <artifactId>parent</artifactId>
-        <version>2.1.0-SNAPSHOT</version>
+        <version>3.0.0-SNAPSHOT</version>
     </parent>
 
     <groupId>org.elasticsearch.distribution</groupId>
diff --git a/distribution/rpm/pom.xml b/distribution/rpm/pom.xml
index f0e22b6..37f7203 100644
--- a/distribution/rpm/pom.xml
+++ b/distribution/rpm/pom.xml
@@ -6,7 +6,7 @@
     <parent>
         <groupId>org.elasticsearch.distribution</groupId>
         <artifactId>distributions</artifactId>
-        <version>2.1.0-SNAPSHOT</version>
+        <version>3.0.0-SNAPSHOT</version>
     </parent>
 
     <groupId>org.elasticsearch.distribution.rpm</groupId>
diff --git a/distribution/src/main/resources/bin/elasticsearch.in.sh b/distribution/src/main/resources/bin/elasticsearch.in.sh
index 527819d..2661544 100644
--- a/distribution/src/main/resources/bin/elasticsearch.in.sh
+++ b/distribution/src/main/resources/bin/elasticsearch.in.sh
@@ -62,7 +62,7 @@ if [ -n "$ES_GC_LOG_FILE" ]; then
   JAVA_OPTS="$JAVA_OPTS -XX:+PrintClassHistogram"
   JAVA_OPTS="$JAVA_OPTS -XX:+PrintTenuringDistribution"
   JAVA_OPTS="$JAVA_OPTS -XX:+PrintGCApplicationStoppedTime"
-  JAVA_OPTS="$JAVA_OPTS -Xloggc:\"$ES_GC_LOG_FILE\""
+  JAVA_OPTS="$JAVA_OPTS -Xloggc:$ES_GC_LOG_FILE"
 
   # Ensure that the directory for the log file exists: the JVM will not create it.
   mkdir -p "`dirname \"$ES_GC_LOG_FILE\"`"
diff --git a/distribution/tar/pom.xml b/distribution/tar/pom.xml
index 744d7c9..f1ce827 100644
--- a/distribution/tar/pom.xml
+++ b/distribution/tar/pom.xml
@@ -6,7 +6,7 @@
     <parent>
         <groupId>org.elasticsearch.distribution</groupId>
         <artifactId>distributions</artifactId>
-        <version>2.1.0-SNAPSHOT</version>
+        <version>3.0.0-SNAPSHOT</version>
     </parent>
 
     <groupId>org.elasticsearch.distribution.tar</groupId>
diff --git a/distribution/zip/pom.xml b/distribution/zip/pom.xml
index 6bb38fb..a24c449 100644
--- a/distribution/zip/pom.xml
+++ b/distribution/zip/pom.xml
@@ -6,7 +6,7 @@
     <parent>
         <groupId>org.elasticsearch.distribution</groupId>
         <artifactId>distributions</artifactId>
-        <version>2.1.0-SNAPSHOT</version>
+        <version>3.0.0-SNAPSHOT</version>
     </parent>
 
     <groupId>org.elasticsearch.distribution.zip</groupId>
diff --git a/docs/plugins/cloud-aws.asciidoc b/docs/plugins/cloud-aws.asciidoc
deleted file mode 100644
index 1cab147..0000000
--- a/docs/plugins/cloud-aws.asciidoc
+++ /dev/null
@@ -1,471 +0,0 @@
-[[cloud-aws]]
-=== AWS Cloud Plugin
-
-The Amazon Web Service (AWS) Cloud plugin uses the
-https://github.com/aws/aws-sdk-java[AWS API] for unicast discovery, and adds
-support for using S3 as a repository for
-{ref}/modules-snapshots.html[Snapshot/Restore].
-
-[[cloud-aws-install]]
-[float]
-==== Installation
-
-This plugin can be installed using the plugin manager:
-
-[source,sh]
-----------------------------------------------------------------
-sudo bin/plugin install cloud-aws
-----------------------------------------------------------------
-
-The plugin must be installed on every node in the cluster, and each node must
-be restarted after installation.
-
-[[cloud-aws-remove]]
-[float]
-==== Removal
-
-The plugin can be removed with the following command:
-
-[source,sh]
-----------------------------------------------------------------
-sudo bin/plugin remove cloud-aws
-----------------------------------------------------------------
-
-The node must be stopped before removing the plugin.
-
-[[cloud-aws-usage]]
-==== Getting started with AWS
-
-The plugin will default to using
-http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html[IAM Role]
-credentials for authentication. These can be overridden by, in increasing
-order of precedence, system properties `aws.accessKeyId` and `aws.secretKey`,
-environment variables `AWS_ACCESS_KEY_ID` and `AWS_SECRET_KEY`, or the
-elasticsearch config using `cloud.aws.access_key` and `cloud.aws.secret_key`:
-
-[source,yaml]
-----
-cloud:
-    aws:
-        access_key: AKVAIQBF2RECL7FJWGJQ
-        secret_key: vExyMThREXeRMm/b/LRzEB8jWwvzQeXgjqMX+6br
-----
-
-[[cloud-aws-usage-security]]
-===== Transport security
-
-By default this plugin uses HTTPS for all API calls to AWS endpoints. If you wish to configure HTTP you can set
-`cloud.aws.protocol` in the elasticsearch config. You can optionally override this setting per individual service
-via: `cloud.aws.ec2.protocol` or `cloud.aws.s3.protocol`.
-
-[source,yaml]
-----
-cloud:
-    aws:
-        protocol: https
-        s3:
-            protocol: http
-        ec2:
-            protocol: https
-----
-
-In addition, a proxy can be configured with the `proxy_host` and `proxy_port` settings (note that protocol can be
-`http` or `https`):
-
-[source,yaml]
-----
-cloud:
-    aws:
-        protocol: https
-        proxy_host: proxy1.company.com
-        proxy_port: 8083
-----
-
-You can also set different proxies for `ec2` and `s3`:
-
-[source,yaml]
-----
-cloud:
-    aws:
-        s3:
-            proxy_host: proxy1.company.com
-            proxy_port: 8083
-        ec2:
-            proxy_host: proxy2.company.com
-            proxy_port: 8083
-----
-
-[[cloud-aws-usage-region]]
-===== Region
-
-The `cloud.aws.region` can be set to a region and will automatically use the relevant settings for both `ec2` and `s3`.
-The available values are:
-
-* `us-east` (`us-east-1`)
-* `us-west` (`us-west-1`)
-* `us-west-1`
-* `us-west-2`
-* `ap-southeast` (`ap-southeast-1`)
-* `ap-southeast-1`
-* `ap-southeast-2`
-* `ap-northeast` (`ap-northeast-1`)
-* `eu-west` (`eu-west-1`)
-* `eu-central` (`eu-central-1`)
-* `sa-east` (`sa-east-1`)
-* `cn-north` (`cn-north-1`)
-
-[[cloud-aws-usage-signer]]
-===== EC2/S3 Signer API
-
-If you are using a compatible EC2 or S3 service, they might be using an older API to sign the requests.
-You can set your compatible signer API using `cloud.aws.signer` (or `cloud.aws.ec2.signer` and `cloud.aws.s3.signer`)
-with the right signer to use. Defaults to `AWS4SignerType`.
-
-[[cloud-aws-discovery]]
-==== EC2 Discovery
-
-ec2 discovery allows to use the ec2 APIs to perform automatic discovery (similar to multicast in non hostile multicast
-environments). Here is a simple sample configuration:
-
-[source,yaml]
-----
-discovery:
-    type: ec2
-----
-
-The ec2 discovery is using the same credentials as the rest of the AWS services provided by this plugin (`repositories`).
-See <<cloud-aws-usage>> for details.
-
-The following are a list of settings (prefixed with `discovery.ec2`) that can further control the discovery:
-
-`groups`::
-
-    Either a comma separated list or array based list of (security) groups.
-    Only instances with the provided security groups will be used in the
-    cluster discovery. (NOTE: You could provide either group NAME or group
-    ID.)
-
-`host_type`::
-
-    The type of host type to use to communicate with other instances. Can be
-    one of `private_ip`, `public_ip`, `private_dns`, `public_dns`. Defaults to
-    `private_ip`.
-
-`availability_zones`::
-
-    Either a comma separated list or array based list of availability zones.
-    Only instances within the provided availability zones will be used in the
-    cluster discovery.
-
-`any_group`::
-
-    If set to `false`, will require all security groups to be present for the
-    instance to be used for the discovery. Defaults to `true`.
-
-`ping_timeout`::
-
-    How long to wait for existing EC2 nodes to reply during discovery.
-    Defaults to `3s`. If no unit like `ms`, `s` or `m` is specified,
-    milliseconds are used.
-
-[[cloud-aws-discovery-permissions]]
-===== Recommended EC2 Permissions
-
-EC2 discovery requires making a call to the EC2 service. You'll want to setup
-an IAM policy to allow this. You can create a custom policy via the IAM
-Management Console. It should look similar to this.
-
-[source,js]
-----
-{
-  "Statement": [
-    {
-      "Action": [
-        "ec2:DescribeInstances"
-      ],
-      "Effect": "Allow",
-      "Resource": [
-        "*"
-      ]
-    }
-  ],
-  "Version": "2012-10-17"
-}
-----
-
-[[cloud-aws-discovery-filtering]]
-===== Filtering by Tags
-
-The ec2 discovery can also filter machines to include in the cluster based on tags (and not just groups). The settings
-to use include the `discovery.ec2.tag.` prefix. For example, setting `discovery.ec2.tag.stage` to `dev` will only
-filter instances with a tag key set to `stage`, and a value of `dev`. Several tags set will require all of those tags
-to be set for the instance to be included.
-
-One practical use for tag filtering is when an ec2 cluster contains many nodes that are not running elasticsearch. In
-this case (particularly with high `ping_timeout` values) there is a risk that a new node's discovery phase will end
-before it has found the cluster (which will result in it declaring itself master of a new cluster with the same name
-- highly undesirable). Tagging elasticsearch ec2 nodes and then filtering by that tag will resolve this issue.
-
-[[cloud-aws-discovery-attributes]]
-===== Automatic Node Attributes
-
-Though not dependent on actually using `ec2` as discovery (but still requires the cloud aws plugin installed), the
-plugin can automatically add node attributes relating to ec2 (for example, availability zone, that can be used with
-the awareness allocation feature). In order to enable it, set `cloud.node.auto_attributes` to `true` in the settings.
-
-[[cloud-aws-discovery-endpoint]]
-===== Using other EC2 endpoint
-
-If you are using any EC2 api compatible service, you can set the endpoint you want to use by setting
-`cloud.aws.ec2.endpoint` to your URL provider.
-
-[[cloud-aws-repository]]
-==== S3 Repository
-
-The S3 repository is using S3 to store snapshots. The S3 repository can be created using the following command:
-
-[source,json]
-----
-PUT _snapshot/my_s3_repository
-{
-  "type": "s3",
-  "settings": {
-    "bucket": "my_bucket_name",
-    "region": "us-west"
-  }
-}
-----
-// AUTOSENSE
-
-The following settings are supported:
-
-`bucket`::
-
-    The name of the bucket to be used for snapshots. (Mandatory)
-
-`region`::
-
-    The region where bucket is located. Defaults to US Standard
-
-`endpoint`::
-
-    The endpoint to the S3 API. Defaults to AWS's default S3 endpoint. Note
-    that setting a region overrides the endpoint setting.
-
-`protocol`::
-
-    The protocol to use (`http` or `https`). Defaults to value of
-    `cloud.aws.protocol` or `cloud.aws.s3.protocol`.
-
-`base_path`::
-
-    Specifies the path within bucket to repository data. Defaults to
-    value of `repositories.s3.base_path` or to root directory if not set.
-
-`access_key`::
-
-    The access key to use for authentication. Defaults to value of
-    `cloud.aws.access_key`.
-
-`secret_key`::
-
-    The secret key to use for authentication. Defaults to value of
-    `cloud.aws.secret_key`.
-
-`chunk_size`::
-
-    Big files can be broken down into chunks during snapshotting if needed.
-    The chunk size can be specified in bytes or by using size value notation,
-    i.e. `1g`, `10m`, `5k`. Defaults to `100m`.
-
-`compress`::
-
-    When set to `true` metadata files are stored in compressed format. This
-    setting doesn't affect index files that are already compressed by default.
-    Defaults to `false`.
-
-`server_side_encryption`::
-
-    When set to `true` files are encrypted on server side using AES256
-    algorithm. Defaults to `false`.
-
-`buffer_size`::
-
-    Minimum threshold below which the chunk is uploaded using a single
-    request. Beyond this threshold, the S3 repository will use the
-    http://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html[AWS Multipart Upload API]
-    to split the chunk into several parts, each of `buffer_size` length, and
-    to upload each part in its own request. Note that positioning a buffer
-    size lower than `5mb` is not allowed since it will prevents the use of the
-    Multipart API and may result in upload errors. Defaults to `5mb`.
-
-`max_retries`::
-
-    Number of retries in case of S3 errors. Defaults to `3`.
-
-`read_only`::
-
-    Makes repository read-only. coming[2.1.0]  Defaults to `false`.
-
-The S3 repositories use the same credentials as the rest of the AWS services
-provided by this plugin (`discovery`). See <<cloud-aws-usage>> for details.
-
-Multiple S3 repositories can be created. If the buckets require different
-credentials, then define them as part of the repository settings.
-
-[[cloud-aws-repository-permissions]]
-===== Recommended S3 Permissions
-
-In order to restrict the Elasticsearch snapshot process to the minimum required resources, we recommend using Amazon
-IAM in conjunction with pre-existing S3 buckets. Here is an example policy which will allow the snapshot access to an
- S3 bucket named "snaps.example.com". This may be configured through the AWS IAM console, by creating a Custom Policy,
- and using a Policy Document similar to this (changing snaps.example.com to your bucket name).
-
-[source,js]
-----
-{
-  "Statement": [
-    {
-      "Action": [
-        "s3:ListBucket",
-        "s3:GetBucketLocation",
-        "s3:ListBucketMultipartUploads",
-        "s3:ListBucketVersions"
-      ],
-      "Effect": "Allow",
-      "Resource": [
-        "arn:aws:s3:::snaps.example.com"
-      ]
-    },
-    {
-      "Action": [
-        "s3:GetObject",
-        "s3:PutObject",
-        "s3:DeleteObject",
-        "s3:AbortMultipartUpload",
-        "s3:ListMultipartUploadParts"
-      ],
-      "Effect": "Allow",
-      "Resource": [
-        "arn:aws:s3:::snaps.example.com/*"
-      ]
-    }
-  ],
-  "Version": "2012-10-17"
-}
-----
-
-You may further restrict the permissions by specifying a prefix within the bucket, in this example, named "foo".
-
-[source,js]
-----
-{
-  "Statement": [
-    {
-      "Action": [
-        "s3:ListBucket",
-        "s3:GetBucketLocation",
-        "s3:ListBucketMultipartUploads",
-        "s3:ListBucketVersions"
-      ],
-      "Condition": {
-        "StringLike": {
-          "s3:prefix": [
-            "foo/*"
-          ]
-        }
-      },
-      "Effect": "Allow",
-      "Resource": [
-        "arn:aws:s3:::snaps.example.com"
-      ]
-    },
-    {
-      "Action": [
-        "s3:GetObject",
-        "s3:PutObject",
-        "s3:DeleteObject",
-        "s3:AbortMultipartUpload",
-        "s3:ListMultipartUploadParts"
-      ],
-      "Effect": "Allow",
-      "Resource": [
-        "arn:aws:s3:::snaps.example.com/foo/*"
-      ]
-    }
-  ],
-  "Version": "2012-10-17"
-}
-----
-
-The bucket needs to exist to register a repository for snapshots. If you did not create the bucket then the repository
-registration will fail. If you want elasticsearch to create the bucket instead, you can add the permission to create a
-specific bucket like this:
-
-[source,js]
-----
-{
-   "Action": [
-      "s3:CreateBucket"
-   ],
-   "Effect": "Allow",
-   "Resource": [
-      "arn:aws:s3:::snaps.example.com"
-   ]
-}
-----
-
-[[cloud-aws-repository-endpoint]]
-===== Using other S3 endpoint
-
-If you are using any S3 api compatible service, you can set a global endpoint by setting `cloud.aws.s3.endpoint`
-to your URL provider. Note that this setting will be used for all S3 repositories.
-
-Different `endpoint`, `region` and `protocol` settings can be set on a per-repository basis
-See <<cloud-aws-repository>> for details.
-
-[[cloud-aws-testing]]
-==== Testing AWS
-
-Integrations tests in this plugin require working AWS configuration and therefore disabled by default. Three buckets
-and two iam users have to be created. The first iam user needs access to two buckets in different regions and the final
-bucket is exclusive for the other iam user. To enable tests prepare a config file elasticsearch.yml with the following
-content:
-
-[source,yaml]
-----
-cloud:
-    aws:
-        access_key: AKVAIQBF2RECL7FJWGJQ
-        secret_key: vExyMThREXeRMm/b/LRzEB8jWwvzQeXgjqMX+6br
-
-repositories:
-    s3:
-        bucket: "bucket_name"
-        region: "us-west-2"
-        private-bucket:
-            bucket: <bucket not accessible by default key>
-            access_key: <access key>
-            secret_key: <secret key>
-        remote-bucket:
-            bucket: <bucket in other region>
-            region: <region>
-	external-bucket:
-	    bucket: <bucket>
-	    access_key: <access key>
-	    secret_key: <secret key>
-	    endpoint: <endpoint>
-	    protocol: <protocol>
-
-----
-
-Replace all occurrences of `access_key`, `secret_key`, `endpoint`, `protocol`, `bucket` and `region` with your settings.
-Please, note that the test will delete all snapshot/restore related files in the specified buckets.
-
-To run test:
-
-[source,sh]
-----
-mvn -Dtests.aws=true -Dtests.config=/path/to/config/file/elasticsearch.yml clean test
-----
-
diff --git a/docs/plugins/discovery-ec2.asciidoc b/docs/plugins/discovery-ec2.asciidoc
new file mode 100644
index 0000000..6e5965c
--- /dev/null
+++ b/docs/plugins/discovery-ec2.asciidoc
@@ -0,0 +1,215 @@
+[[discovery-ec2]]
+=== EC2 Discovery Plugin
+
+The EC2 discovery plugin uses the https://github.com/aws/aws-sdk-java[AWS API] for unicast discovery.
+
+[[discovery-ec2-install]]
+[float]
+==== Installation
+
+This plugin can be installed using the plugin manager:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin install discovery-ec2
+----------------------------------------------------------------
+
+The plugin must be installed on every node in the cluster, and each node must
+be restarted after installation.
+
+[[discovery-ec2-remove]]
+[float]
+==== Removal
+
+The plugin can be removed with the following command:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin remove discovery-ec2
+----------------------------------------------------------------
+
+The node must be stopped before removing the plugin.
+
+[[discovery-ec2-usage]]
+==== Getting started with AWS
+
+The plugin will default to using
+http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html[IAM Role]
+credentials for authentication. These can be overridden by, in increasing
+order of precedence, system properties `aws.accessKeyId` and `aws.secretKey`,
+environment variables `AWS_ACCESS_KEY_ID` and `AWS_SECRET_KEY`, or the
+elasticsearch config using `cloud.aws.access_key` and `cloud.aws.secret_key`:
+
+[source,yaml]
+----
+cloud:
+    aws:
+        access_key: AKVAIQBF2RECL7FJWGJQ
+        secret_key: vExyMThREXeRMm/b/LRzEB8jWwvzQeXgjqMX+6br
+----
+
+[[discovery-ec2-usage-security]]
+===== Transport security
+
+By default this plugin uses HTTPS for all API calls to AWS endpoints. If you wish to configure HTTP you can set
+`cloud.aws.protocol` in the elasticsearch config. You can optionally override this setting per individual service
+via: `cloud.aws.ec2.protocol` or `cloud.aws.s3.protocol`.
+
+[source,yaml]
+----
+cloud:
+    aws:
+        protocol: https
+        ec2:
+            protocol: https
+----
+
+In addition, a proxy can be configured with the `proxy_host` and `proxy_port` settings (note that protocol can be
+`http` or `https`):
+
+[source,yaml]
+----
+cloud:
+    aws:
+        protocol: https
+        proxy_host: proxy1.company.com
+        proxy_port: 8083
+----
+
+You can also set different proxies for `ec2` and `s3`:
+
+[source,yaml]
+----
+cloud:
+    aws:
+        s3:
+            proxy_host: proxy1.company.com
+            proxy_port: 8083
+        ec2:
+            proxy_host: proxy2.company.com
+            proxy_port: 8083
+----
+
+[[discovery-ec2-usage-region]]
+===== Region
+
+The `cloud.aws.region` can be set to a region and will automatically use the relevant settings for both `ec2` and `s3`.
+The available values are:
+
+* `us-east` (`us-east-1`)
+* `us-west` (`us-west-1`)
+* `us-west-1`
+* `us-west-2`
+* `ap-southeast` (`ap-southeast-1`)
+* `ap-southeast-1`
+* `ap-southeast-2`
+* `ap-northeast` (`ap-northeast-1`)
+* `eu-west` (`eu-west-1`)
+* `eu-central` (`eu-central-1`)
+* `sa-east` (`sa-east-1`)
+* `cn-north` (`cn-north-1`)
+
+[[discovery-ec2-usage-signer]]
+===== EC2/S3 Signer API
+
+If you are using a compatible EC2 or S3 service, they might be using an older API to sign the requests.
+You can set your compatible signer API using `cloud.aws.signer` (or `cloud.aws.ec2.signer` and `cloud.aws.s3.signer`)
+with the right signer to use. Defaults to `AWS4SignerType`.
+
+[[discovery-ec2-discovery]]
+==== EC2 Discovery
+
+ec2 discovery allows to use the ec2 APIs to perform automatic discovery (similar to multicast in non hostile multicast
+environments). Here is a simple sample configuration:
+
+[source,yaml]
+----
+discovery:
+    type: ec2
+----
+
+The ec2 discovery is using the same credentials as the rest of the AWS services provided by this plugin (`repositories`).
+See <<discovery-ec2-usage>> for details.
+
+The following are a list of settings (prefixed with `discovery.ec2`) that can further control the discovery:
+
+`groups`::
+
+    Either a comma separated list or array based list of (security) groups.
+    Only instances with the provided security groups will be used in the
+    cluster discovery. (NOTE: You could provide either group NAME or group
+    ID.)
+
+`host_type`::
+
+    The type of host type to use to communicate with other instances. Can be
+    one of `private_ip`, `public_ip`, `private_dns`, `public_dns`. Defaults to
+    `private_ip`.
+
+`availability_zones`::
+
+    Either a comma separated list or array based list of availability zones.
+    Only instances within the provided availability zones will be used in the
+    cluster discovery.
+
+`any_group`::
+
+    If set to `false`, will require all security groups to be present for the
+    instance to be used for the discovery. Defaults to `true`.
+
+`ping_timeout`::
+
+    How long to wait for existing EC2 nodes to reply during discovery.
+    Defaults to `3s`. If no unit like `ms`, `s` or `m` is specified,
+    milliseconds are used.
+
+[[discovery-ec2-permissions]]
+===== Recommended EC2 Permissions
+
+EC2 discovery requires making a call to the EC2 service. You'll want to setup
+an IAM policy to allow this. You can create a custom policy via the IAM
+Management Console. It should look similar to this.
+
+[source,js]
+----
+{
+  "Statement": [
+    {
+      "Action": [
+        "ec2:DescribeInstances"
+      ],
+      "Effect": "Allow",
+      "Resource": [
+        "*"
+      ]
+    }
+  ],
+  "Version": "2012-10-17"
+}
+----
+
+[[discovery-ec2-filtering]]
+===== Filtering by Tags
+
+The ec2 discovery can also filter machines to include in the cluster based on tags (and not just groups). The settings
+to use include the `discovery.ec2.tag.` prefix. For example, setting `discovery.ec2.tag.stage` to `dev` will only
+filter instances with a tag key set to `stage`, and a value of `dev`. Several tags set will require all of those tags
+to be set for the instance to be included.
+
+One practical use for tag filtering is when an ec2 cluster contains many nodes that are not running elasticsearch. In
+this case (particularly with high `ping_timeout` values) there is a risk that a new node's discovery phase will end
+before it has found the cluster (which will result in it declaring itself master of a new cluster with the same name
+- highly undesirable). Tagging elasticsearch ec2 nodes and then filtering by that tag will resolve this issue.
+
+[[discovery-ec2-attributes]]
+===== Automatic Node Attributes
+
+Though not dependent on actually using `ec2` as discovery (but still requires the cloud aws plugin installed), the
+plugin can automatically add node attributes relating to ec2 (for example, availability zone, that can be used with
+the awareness allocation feature). In order to enable it, set `cloud.node.auto_attributes` to `true` in the settings.
+
+[[discovery-ec2-endpoint]]
+===== Using other EC2 endpoint
+
+If you are using any EC2 api compatible service, you can set the endpoint you want to use by setting
+`cloud.aws.ec2.endpoint` to your URL provider.
diff --git a/docs/plugins/discovery.asciidoc b/docs/plugins/discovery.asciidoc
index 3b80ecc..0e39d29 100644
--- a/docs/plugins/discovery.asciidoc
+++ b/docs/plugins/discovery.asciidoc
@@ -9,12 +9,9 @@ can be used instead of {ref}/modules-discovery-zen.html[Zen Discovery].
 
 The core discovery plugins are:
 
-<<cloud-aws,AWS Cloud>>::
+<<discovery-ec2,EC2 discovery>>::
 
-The Amazon Web Service (AWS) Cloud plugin uses the
-https://github.com/aws/aws-sdk-java[AWS API] for unicast discovery, and adds
-support for using S3 as a repository for
-{ref}/modules-snapshots.html[Snapshot/Restore].
+The EC2 discovery plugin uses the https://github.com/aws/aws-sdk-java[AWS API] for unicast discovery.
 
 <<cloud-azure,Azure Cloud>>::
 
@@ -39,7 +36,7 @@ A number of discovery plugins have been contributed by our community:
 * https://github.com/shikhar/eskka[eskka Discovery Plugin] (by Shikhar Bhushan)
 * https://github.com/grmblfrz/elasticsearch-zookeeper[ZooKeeper Discovery Plugin] (by Sonian Inc.)
 
-include::cloud-aws.asciidoc[]
+include::discovery-ec2.asciidoc[]
 
 include::cloud-azure.asciidoc[]
 
@@ -47,5 +44,3 @@ include::cloud-gce.asciidoc[]
 
 include::discovery-multicast.asciidoc[]
 
-
-
diff --git a/docs/plugins/repository-s3.asciidoc b/docs/plugins/repository-s3.asciidoc
new file mode 100644
index 0000000..75e7083
--- /dev/null
+++ b/docs/plugins/repository-s3.asciidoc
@@ -0,0 +1,371 @@
+[[repository-s3]]
+=== S3 Repository Plugin
+
+The S3 repository plugin adds support for using S3 as a repository for
+{ref}/modules-snapshots.html[Snapshot/Restore].
+
+[[repository-s3-install]]
+[float]
+==== Installation
+
+This plugin can be installed using the plugin manager:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin install repository-s3
+----------------------------------------------------------------
+
+The plugin must be installed on every node in the cluster, and each node must
+be restarted after installation.
+
+[[repository-s3-remove]]
+[float]
+==== Removal
+
+The plugin can be removed with the following command:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin remove repository-s3
+----------------------------------------------------------------
+
+The node must be stopped before removing the plugin.
+
+[[repository-s3-usage]]
+==== Getting started with AWS
+
+The plugin will default to using
+http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html[IAM Role]
+credentials for authentication. These can be overridden by, in increasing
+order of precedence, system properties `aws.accessKeyId` and `aws.secretKey`,
+environment variables `AWS_ACCESS_KEY_ID` and `AWS_SECRET_KEY`, or the
+elasticsearch config using `cloud.aws.access_key` and `cloud.aws.secret_key`:
+
+[source,yaml]
+----
+cloud:
+    aws:
+        access_key: AKVAIQBF2RECL7FJWGJQ
+        secret_key: vExyMThREXeRMm/b/LRzEB8jWwvzQeXgjqMX+6br
+----
+
+[[repository-s3-usage-security]]
+===== Transport security
+
+By default this plugin uses HTTPS for all API calls to AWS endpoints. If you wish to configure HTTP you can set
+`cloud.aws.protocol` in the elasticsearch config. You can optionally override this setting per individual service
+via: `cloud.aws.ec2.protocol` or `cloud.aws.s3.protocol`.
+
+[source,yaml]
+----
+cloud:
+    aws:
+        protocol: https
+        s3:
+            protocol: http
+        ec2:
+            protocol: https
+----
+
+In addition, a proxy can be configured with the `proxy_host` and `proxy_port` settings (note that protocol can be
+`http` or `https`):
+
+[source,yaml]
+----
+cloud:
+    aws:
+        protocol: https
+        proxy_host: proxy1.company.com
+        proxy_port: 8083
+----
+
+You can also set different proxies for `ec2` and `s3`:
+
+[source,yaml]
+----
+cloud:
+    aws:
+        s3:
+            proxy_host: proxy1.company.com
+            proxy_port: 8083
+        ec2:
+            proxy_host: proxy2.company.com
+            proxy_port: 8083
+----
+
+[[repository-s3-usage-region]]
+===== Region
+
+The `cloud.aws.region` can be set to a region and will automatically use the relevant settings for both `ec2` and `s3`.
+The available values are:
+
+* `us-east` (`us-east-1`)
+* `us-west` (`us-west-1`)
+* `us-west-1`
+* `us-west-2`
+* `ap-southeast` (`ap-southeast-1`)
+* `ap-southeast-1`
+* `ap-southeast-2`
+* `ap-northeast` (`ap-northeast-1`)
+* `eu-west` (`eu-west-1`)
+* `eu-central` (`eu-central-1`)
+* `sa-east` (`sa-east-1`)
+* `cn-north` (`cn-north-1`)
+
+[[repository-s3-usage-signer]]
+===== EC2/S3 Signer API
+
+If you are using a compatible EC2 or S3 service, they might be using an older API to sign the requests.
+You can set your compatible signer API using `cloud.aws.signer` (or `cloud.aws.ec2.signer` and `cloud.aws.s3.signer`)
+with the right signer to use. Defaults to `AWS4SignerType`.
+
+[[repository-s3-repository]]
+==== S3 Repository
+
+The S3 repository is using S3 to store snapshots. The S3 repository can be created using the following command:
+
+[source,json]
+----
+PUT _snapshot/my_s3_repository
+{
+  "type": "s3",
+  "settings": {
+    "bucket": "my_bucket_name",
+    "region": "us-west"
+  }
+}
+----
+// AUTOSENSE
+
+The following settings are supported:
+
+`bucket`::
+
+    The name of the bucket to be used for snapshots. (Mandatory)
+
+`region`::
+
+    The region where bucket is located. Defaults to US Standard
+
+`endpoint`::
+
+    The endpoint to the S3 API. Defaults to AWS's default S3 endpoint. Note
+    that setting a region overrides the endpoint setting.
+
+`protocol`::
+
+    The protocol to use (`http` or `https`). Defaults to value of
+    `cloud.aws.protocol` or `cloud.aws.s3.protocol`.
+
+`base_path`::
+
+    Specifies the path within bucket to repository data. Defaults to
+    value of `repositories.s3.base_path` or to root directory if not set.
+
+`access_key`::
+
+    The access key to use for authentication. Defaults to value of
+    `cloud.aws.access_key`.
+
+`secret_key`::
+
+    The secret key to use for authentication. Defaults to value of
+    `cloud.aws.secret_key`.
+
+`chunk_size`::
+
+    Big files can be broken down into chunks during snapshotting if needed.
+    The chunk size can be specified in bytes or by using size value notation,
+    i.e. `1g`, `10m`, `5k`. Defaults to `100m`.
+
+`compress`::
+
+    When set to `true` metadata files are stored in compressed format. This
+    setting doesn't affect index files that are already compressed by default.
+    Defaults to `false`.
+
+`server_side_encryption`::
+
+    When set to `true` files are encrypted on server side using AES256
+    algorithm. Defaults to `false`.
+
+`buffer_size`::
+
+    Minimum threshold below which the chunk is uploaded using a single
+    request. Beyond this threshold, the S3 repository will use the
+    http://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html[AWS Multipart Upload API]
+    to split the chunk into several parts, each of `buffer_size` length, and
+    to upload each part in its own request. Note that positioning a buffer
+    size lower than `5mb` is not allowed since it will prevents the use of the
+    Multipart API and may result in upload errors. Defaults to `5mb`.
+
+`max_retries`::
+
+    Number of retries in case of S3 errors. Defaults to `3`.
+
+`read_only`::
+
+    Makes repository read-only. coming[2.1.0]  Defaults to `false`.
+
+The S3 repositories use the same credentials as the rest of the AWS services
+provided by this plugin (`discovery`). See <<repository-s3-usage>> for details.
+
+Multiple S3 repositories can be created. If the buckets require different
+credentials, then define them as part of the repository settings.
+
+[[repository-s3-permissions]]
+===== Recommended S3 Permissions
+
+In order to restrict the Elasticsearch snapshot process to the minimum required resources, we recommend using Amazon
+IAM in conjunction with pre-existing S3 buckets. Here is an example policy which will allow the snapshot access to an
+ S3 bucket named "snaps.example.com". This may be configured through the AWS IAM console, by creating a Custom Policy,
+ and using a Policy Document similar to this (changing snaps.example.com to your bucket name).
+
+[source,js]
+----
+{
+  "Statement": [
+    {
+      "Action": [
+        "s3:ListBucket",
+        "s3:GetBucketLocation",
+        "s3:ListBucketMultipartUploads",
+        "s3:ListBucketVersions"
+      ],
+      "Effect": "Allow",
+      "Resource": [
+        "arn:aws:s3:::snaps.example.com"
+      ]
+    },
+    {
+      "Action": [
+        "s3:GetObject",
+        "s3:PutObject",
+        "s3:DeleteObject",
+        "s3:AbortMultipartUpload",
+        "s3:ListMultipartUploadParts"
+      ],
+      "Effect": "Allow",
+      "Resource": [
+        "arn:aws:s3:::snaps.example.com/*"
+      ]
+    }
+  ],
+  "Version": "2012-10-17"
+}
+----
+
+You may further restrict the permissions by specifying a prefix within the bucket, in this example, named "foo".
+
+[source,js]
+----
+{
+  "Statement": [
+    {
+      "Action": [
+        "s3:ListBucket",
+        "s3:GetBucketLocation",
+        "s3:ListBucketMultipartUploads",
+        "s3:ListBucketVersions"
+      ],
+      "Condition": {
+        "StringLike": {
+          "s3:prefix": [
+            "foo/*"
+          ]
+        }
+      },
+      "Effect": "Allow",
+      "Resource": [
+        "arn:aws:s3:::snaps.example.com"
+      ]
+    },
+    {
+      "Action": [
+        "s3:GetObject",
+        "s3:PutObject",
+        "s3:DeleteObject",
+        "s3:AbortMultipartUpload",
+        "s3:ListMultipartUploadParts"
+      ],
+      "Effect": "Allow",
+      "Resource": [
+        "arn:aws:s3:::snaps.example.com/foo/*"
+      ]
+    }
+  ],
+  "Version": "2012-10-17"
+}
+----
+
+The bucket needs to exist to register a repository for snapshots. If you did not create the bucket then the repository
+registration will fail. If you want elasticsearch to create the bucket instead, you can add the permission to create a
+specific bucket like this:
+
+[source,js]
+----
+{
+   "Action": [
+      "s3:CreateBucket"
+   ],
+   "Effect": "Allow",
+   "Resource": [
+      "arn:aws:s3:::snaps.example.com"
+   ]
+}
+----
+
+[[repository-s3-endpoint]]
+===== Using other S3 endpoint
+
+If you are using any S3 api compatible service, you can set a global endpoint by setting `cloud.aws.s3.endpoint`
+to your URL provider. Note that this setting will be used for all S3 repositories.
+
+Different `endpoint`, `region` and `protocol` settings can be set on a per-repository basis
+See <<repository-s3-repository>> for details.
+
+[[repository-s3-testing]]
+==== Testing AWS
+
+Integrations tests in this plugin require working AWS configuration and therefore disabled by default. Three buckets
+and two iam users have to be created. The first iam user needs access to two buckets in different regions and the final
+bucket is exclusive for the other iam user. To enable tests prepare a config file elasticsearch.yml with the following
+content:
+
+[source,yaml]
+----
+cloud:
+    aws:
+        access_key: AKVAIQBF2RECL7FJWGJQ
+        secret_key: vExyMThREXeRMm/b/LRzEB8jWwvzQeXgjqMX+6br
+
+repositories:
+    s3:
+        bucket: "bucket_name"
+        region: "us-west-2"
+        private-bucket:
+            bucket: <bucket not accessible by default key>
+            access_key: <access key>
+            secret_key: <secret key>
+        remote-bucket:
+            bucket: <bucket in other region>
+            region: <region>
+	external-bucket:
+	    bucket: <bucket>
+	    access_key: <access key>
+	    secret_key: <secret key>
+	    endpoint: <endpoint>
+	    protocol: <protocol>
+
+----
+
+Replace all occurrences of `access_key`, `secret_key`, `endpoint`, `protocol`, `bucket` and `region` with your settings.
+Please, note that the test will delete all snapshot/restore related files in the specified buckets.
+
+To run test:
+
+[source,sh]
+----
+mvn -Dtests.aws=true -Dtests.config=/path/to/config/file/elasticsearch.yml clean test
+----
+
diff --git a/docs/plugins/repository.asciidoc b/docs/plugins/repository.asciidoc
index daab621..cddde5f 100644
--- a/docs/plugins/repository.asciidoc
+++ b/docs/plugins/repository.asciidoc
@@ -10,10 +10,9 @@ by distributed file systems:
 
 The core repository plugins are:
 
-<<cloud-aws,AWS Cloud>>::
+<<repository-s3,S3 Repository>>::
 
-The Amazon Web Service (AWS) Cloud plugin adds support for using S3 as a
-repository.
+The S3 repository plugin adds support for using S3 as a repository.
 
 <<cloud-azure,Azure Cloud>>::
 
@@ -35,3 +34,7 @@ The following plugin has been contributed by our community:
 This community plugin appears to have been abandoned:
 
 * https://github.com/kzwang/elasticsearch-repository-gridfs[GridFS] Repository (by Kevin Wang)
+
+
+include::repository-s3.asciidoc[]
+
diff --git a/docs/reference/aggregations/pipeline/bucket-script-aggregation.asciidoc b/docs/reference/aggregations/pipeline/bucket-script-aggregation.asciidoc
index 72addad..81372c1 100644
--- a/docs/reference/aggregations/pipeline/bucket-script-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/bucket-script-aggregation.asciidoc
@@ -20,7 +20,7 @@ A `bucket_script` aggregation looks like this in isolation:
             "my_var1": "the_sum", <1>
             "my_var2": "the_value_count"
         },
-        script: "my_var1 / my_var2"
+        "script": "my_var1 / my_var2"
     }
 }
 --------------------------------------------------
diff --git a/docs/reference/aggregations/pipeline/bucket-selector-aggregation.asciidoc b/docs/reference/aggregations/pipeline/bucket-selector-aggregation.asciidoc
index 2b838ba..cef1e67 100644
--- a/docs/reference/aggregations/pipeline/bucket-selector-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/bucket-selector-aggregation.asciidoc
@@ -25,7 +25,7 @@ A `bucket_selector` aggregation looks like this in isolation:
             "my_var1": "the_sum", <1>
             "my_var2": "the_value_count"
         },
-        script: "my_var1 > my_var2"
+        "script": "my_var1 > my_var2"
     }
 }
 --------------------------------------------------
diff --git a/docs/reference/migration/migrate_query_refactoring.asciidoc b/docs/reference/migration/migrate_query_refactoring.asciidoc
deleted file mode 100644
index 61b86ad..0000000
--- a/docs/reference/migration/migrate_query_refactoring.asciidoc
+++ /dev/null
@@ -1,74 +0,0 @@
-[[breaking-changes query-refactoring]]
-== Breaking changes on the query-refactoring branch
-
-This section discusses changes that are breaking to the current rest or java-api
-on the query-refactoring feature branch.
-
-=== Plugins
-
-Plugins implementing custom queries need to implement the `fromXContent(QueryParseContext)` method in their
-`QueryParser` subclass rather than `parse`. This method will take care of parsing the query from `XContent` format
-into an intermediate query representation that can be streamed between the nodes in binary format, effectively the
-query object used in the java api. Also, the query parser needs to implement the `getBuilderPrototype` method that
-returns a prototype of the streamable query, which allows to deserialize an incoming query by calling
-`readFrom(StreamInput)` against it, which will create a new object, see usages of `Writeable`. The `QueryParser`
-also needs to declare the generic type of the query that it supports and it's able to parse.
-The query object can then transform itself into a lucene query through the new `toQuery(QueryShardContext)` method,
-which returns a lucene query to be executed on the data node. The query implementation also needs to implement the
-`validate` method that allows to validate the content of the query, no matter whether it came in through the java api
-directly or through the REST layer.
-
-=== Java-API
-
-==== BoostingQueryBuilder
-
-Removed setters for mandatory positive/negative query. Both arguments now have
-to be supplied at construction time already and have to be non-null.
-
-==== SpanContainingQueryBuilder
-
-Removed setters for mandatory big/little inner span queries. Both arguments now have
-to be supplied at construction time already and have to be non-null. Updated
-static factory methods in QueryBuilders accordingly.
-
-==== SpanNearQueryBuilder
-
-Removed setter for mandatory slop parameter, needs to be set in constructor now.
-Updated the static factory methods in QueryBuilders accordingly.
-
-==== SpanNotQueryBuilder
-
-Removed setter for mandatory include/exclude span query clause, needs to be set in constructor now.
-Updated the static factory methods in QueryBuilders and tests accordingly.
-
-==== SpanWithinQueryBuilder
-
-Removed setters for mandatory big/little inner span queries. Both arguments now have
-to be supplied at construction time already and have to be non-null. Updated
-static factory methods in QueryBuilders accordingly.
-
-==== QueryFilterBuilder
-
-Removed the setter `queryName(String queryName)` since this field is not supported
-in this type of query. Use `FQueryFilterBuilder.queryName(String queryName)` instead 
-when in need to wrap a named query as a filter.
-
-==== WrapperQueryBuilder
-
-Removed `wrapperQueryBuilder(byte[] source, int offset, int length)`. Instead simply
-use  `wrapperQueryBuilder(byte[] source)`. Updated the static factory methods in
-QueryBuilders accordingly.
-
-==== Operator
-
-Removed the enums called `Operator` from `MatchQueryBuilder`, `QueryStringQueryBuilder`,
-`SimpleQueryStringBuilder`, and `CommonTermsQueryBuilder` in favour of using the enum
-defined in `org.elasticsearch.index.query.Operator` in an effort to consolidate the
-codebase and avoid duplication.
-
-==== queryName and boost support
-
-Support for `queryName` and `boost` has been streamlined to all of the queries. That is
-a breaking change till queries get sent over the network as serialized json rather
-than in `Streamable` format. In fact whenever additional fields are added to the json
-representation of the query, older nodes might throw error when they find unknown fields.
diff --git a/docs/reference/modules/discovery/ec2.asciidoc b/docs/reference/modules/discovery/ec2.asciidoc
index 9d0fa3f..ba15f6b 100644
--- a/docs/reference/modules/discovery/ec2.asciidoc
+++ b/docs/reference/modules/discovery/ec2.asciidoc
@@ -1,6 +1,4 @@
 [[modules-discovery-ec2]]
 === EC2 Discovery
 
-EC2 discovery allows to use the EC2 APIs to perform automatic discovery (similar to multicast).
-Please check the https://github.com/elasticsearch/elasticsearch-cloud-aws[plugin website]
-to find the full documentation.
+EC2 discovery is available as a plugin. See {plugins}/discovery-ec2.html[discovery-ec2] for more information.
diff --git a/docs/reference/modules/network.asciidoc b/docs/reference/modules/network.asciidoc
index e855d82..dc6aca7 100644
--- a/docs/reference/modules/network.asciidoc
+++ b/docs/reference/modules/network.asciidoc
@@ -51,7 +51,7 @@ provided network interface. For example `_en0:ipv4_`.
 provided network interface. For example `_en0:ipv6_`.
 |=======================================================================
 
-When the `cloud-aws` plugin is installed, the following are also allowed
+When the `discovery-ec2` plugin is installed, the following are also allowed
 as valid network host settings:
 
 [cols="<,<",options="header",]
diff --git a/docs/reference/modules/snapshots.asciidoc b/docs/reference/modules/snapshots.asciidoc
index 90f2a02..449ca4b 100644
--- a/docs/reference/modules/snapshots.asciidoc
+++ b/docs/reference/modules/snapshots.asciidoc
@@ -150,7 +150,7 @@ shared file system repository.
 
 Other repository backends are available in these official plugins:
 
-* https://github.com/elasticsearch/elasticsearch-cloud-aws#s3-repository[AWS Cloud Plugin] for S3 repositories
+* {plugins}/repository-s3.html[repository-s3] for S3 repository support
 * https://github.com/elasticsearch/elasticsearch-hadoop/tree/master/repository-hdfs[HDFS Plugin] for Hadoop environments
 * https://github.com/elasticsearch/elasticsearch-cloud-azure#azure-repository[Azure Cloud Plugin] for Azure storage repositories
 
diff --git a/docs/reference/search/request-body.asciidoc b/docs/reference/search/request-body.asciidoc
index 1469073..857047c 100644
--- a/docs/reference/search/request-body.asciidoc
+++ b/docs/reference/search/request-body.asciidoc
@@ -59,7 +59,9 @@ And here is a sample response:
 
 `size`::
 
-    The number of hits to return. Defaults to `10`.
+    The number of hits to return. Defaults to `10`. If you do not care about
+    getting some hits back but only about the number of matches and/or
+    aggregations, setting the value to `0` will help performance.
 
 `search_type`::
 
diff --git a/docs/reference/search/request/sort.asciidoc b/docs/reference/search/request/sort.asciidoc
index 1cc07e2..63fca20 100644
--- a/docs/reference/search/request/sort.asciidoc
+++ b/docs/reference/search/request/sort.asciidoc
@@ -337,8 +337,6 @@ Allow to sort based on custom scripts, here is an example:
 }
 --------------------------------------------------
 
-Note, it is recommended, for single custom based script based sorting,
-to use `function_score` query instead as sorting based on score is faster.
 
 ==== Track Scores
 
diff --git a/plugins/analysis-icu/pom.xml b/plugins/analysis-icu/pom.xml
index b13f69d..a9fced7 100644
--- a/plugins/analysis-icu/pom.xml
+++ b/plugins/analysis-icu/pom.xml
@@ -7,7 +7,7 @@
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>plugins</artifactId>
-        <version>2.1.0-SNAPSHOT</version>
+        <version>3.0.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>analysis-icu</artifactId>
diff --git a/plugins/analysis-kuromoji/pom.xml b/plugins/analysis-kuromoji/pom.xml
index 71d290f..9b28307 100644
--- a/plugins/analysis-kuromoji/pom.xml
+++ b/plugins/analysis-kuromoji/pom.xml
@@ -7,7 +7,7 @@
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>plugins</artifactId>
-        <version>2.1.0-SNAPSHOT</version>
+        <version>3.0.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>analysis-kuromoji</artifactId>
diff --git a/plugins/analysis-phonetic/pom.xml b/plugins/analysis-phonetic/pom.xml
index a5b726e..a2b47f1 100644
--- a/plugins/analysis-phonetic/pom.xml
+++ b/plugins/analysis-phonetic/pom.xml
@@ -7,7 +7,7 @@
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>plugins</artifactId>
-        <version>2.1.0-SNAPSHOT</version>
+        <version>3.0.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>analysis-phonetic</artifactId>
diff --git a/plugins/analysis-smartcn/pom.xml b/plugins/analysis-smartcn/pom.xml
index 294dbaa..64e7b79 100644
--- a/plugins/analysis-smartcn/pom.xml
+++ b/plugins/analysis-smartcn/pom.xml
@@ -7,7 +7,7 @@
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>plugins</artifactId>
-        <version>2.1.0-SNAPSHOT</version>
+        <version>3.0.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>analysis-smartcn</artifactId>
diff --git a/plugins/analysis-stempel/pom.xml b/plugins/analysis-stempel/pom.xml
index 139e3db..4b9b7c3 100644
--- a/plugins/analysis-stempel/pom.xml
+++ b/plugins/analysis-stempel/pom.xml
@@ -7,7 +7,7 @@
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>plugins</artifactId>
-        <version>2.1.0-SNAPSHOT</version>
+        <version>3.0.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>analysis-stempel</artifactId>
diff --git a/plugins/cloud-aws/LICENSE.txt b/plugins/cloud-aws/LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/cloud-aws/LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/cloud-aws/NOTICE.txt b/plugins/cloud-aws/NOTICE.txt
deleted file mode 100644
index 4880904..0000000
--- a/plugins/cloud-aws/NOTICE.txt
+++ /dev/null
@@ -1,8 +0,0 @@
-Elasticsearch
-Copyright 2009-2015 Elasticsearch
-
-This product includes software developed by The Apache Software
-Foundation (http://www.apache.org/).
-
-The LICENSE and NOTICE files for all dependencies may be found in the licenses/
-directory.
diff --git a/plugins/cloud-aws/licenses/aws-java-sdk-LICENSE.txt b/plugins/cloud-aws/licenses/aws-java-sdk-LICENSE.txt
deleted file mode 100644
index 98d1f93..0000000
--- a/plugins/cloud-aws/licenses/aws-java-sdk-LICENSE.txt
+++ /dev/null
@@ -1,63 +0,0 @@
-Apache License
-Version 2.0, January 2004
-
-TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-1. Definitions.
-
-"License" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.
-
-"Licensor" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.
-
-"Legal Entity" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, "control" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.
-
-"You" (or "Your") shall mean an individual or Legal Entity exercising permissions granted by this License.
-
-"Source" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.
-
-"Object" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.
-
-"Work" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).
-
-"Derivative Works" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.
-
-"Contribution" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, "submitted" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as "Not a Contribution."
-
-"Contributor" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.
-
-2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.
-
-3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.
-
-4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:
-
-   1.   You must give any other recipients of the Work or Derivative Works a copy of this License; and
-   2.   You must cause any modified files to carry prominent notices stating that You changed the files; and
-   3.   You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and
-   4.   If the Work includes a "NOTICE" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License.
-
-You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.
-
-5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.
-
-6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.
-
-7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.
-
-8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.
-
-9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.
-
-END OF TERMS AND CONDITIONS
-
-Note: Other license terms may apply to certain, identified software files contained within or distributed with the accompanying software if such terms are included in the directory containing the accompanying software. Such other license terms will then apply in lieu of the terms of the software license above.
-
-JSON processing code subject to the JSON License from JSON.org:
-
-Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
-
-The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
-
-The Software shall be used for Good, not Evil.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
diff --git a/plugins/cloud-aws/licenses/aws-java-sdk-NOTICE.txt b/plugins/cloud-aws/licenses/aws-java-sdk-NOTICE.txt
deleted file mode 100644
index 1c648c8..0000000
--- a/plugins/cloud-aws/licenses/aws-java-sdk-NOTICE.txt
+++ /dev/null
@@ -1,15 +0,0 @@
-AWS SDK for Java
-Copyright 2010-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.
-
-This product includes software developed by
-Amazon Technologies, Inc (http://www.amazon.com/).
-
-**********************
-THIRD PARTY COMPONENTS
-**********************
-This software includes third party software subject to the following copyrights:
-- XML parsing and utility functions from JetS3t - Copyright 2006-2009 James Murty.
-- JSON parsing and utility functions from JSON.org - Copyright 2002 JSON.org.
-- PKCS#1 PEM encoded private key parsing and utility functions from oauth.googlecode.com - Copyright 1998-2010 AOL Inc.
-
-The licenses for these third party components are included in LICENSE.txt
diff --git a/plugins/cloud-aws/licenses/aws-java-sdk-core-1.10.12.jar.sha1 b/plugins/cloud-aws/licenses/aws-java-sdk-core-1.10.12.jar.sha1
deleted file mode 100644
index 659b6cc..0000000
--- a/plugins/cloud-aws/licenses/aws-java-sdk-core-1.10.12.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-7ff51040bbcc9085dcb9a24a2c2a3cc7ac995988
diff --git a/plugins/cloud-aws/licenses/aws-java-sdk-ec2-1.10.12.jar.sha1 b/plugins/cloud-aws/licenses/aws-java-sdk-ec2-1.10.12.jar.sha1
deleted file mode 100644
index 60bae7e..0000000
--- a/plugins/cloud-aws/licenses/aws-java-sdk-ec2-1.10.12.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-b0712cc659e72b9da0f5b03872d2476ab4a695f7
diff --git a/plugins/cloud-aws/licenses/aws-java-sdk-kms-1.10.12.jar.sha1 b/plugins/cloud-aws/licenses/aws-java-sdk-kms-1.10.12.jar.sha1
deleted file mode 100644
index 1948b0d..0000000
--- a/plugins/cloud-aws/licenses/aws-java-sdk-kms-1.10.12.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-31afbe46b65e9933316c7e8dfb8b88dc4b37b6ba
diff --git a/plugins/cloud-aws/licenses/aws-java-sdk-s3-1.10.12.jar.sha1 b/plugins/cloud-aws/licenses/aws-java-sdk-s3-1.10.12.jar.sha1
deleted file mode 100644
index 9814735..0000000
--- a/plugins/cloud-aws/licenses/aws-java-sdk-s3-1.10.12.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-c9e2593fdf398c5f8906a704db037d17b2de4b2a
diff --git a/plugins/cloud-aws/licenses/commons-codec-1.6.jar.sha1 b/plugins/cloud-aws/licenses/commons-codec-1.6.jar.sha1
deleted file mode 100644
index bf78aff..0000000
--- a/plugins/cloud-aws/licenses/commons-codec-1.6.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-b7f0fc8f61ecadeb3695f0b9464755eee44374d4
diff --git a/plugins/cloud-aws/licenses/commons-codec-LICENSE.txt b/plugins/cloud-aws/licenses/commons-codec-LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/cloud-aws/licenses/commons-codec-LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/cloud-aws/licenses/commons-codec-NOTICE.txt b/plugins/cloud-aws/licenses/commons-codec-NOTICE.txt
deleted file mode 100644
index 5691644..0000000
--- a/plugins/cloud-aws/licenses/commons-codec-NOTICE.txt
+++ /dev/null
@@ -1,17 +0,0 @@
-Apache Commons Codec
-Copyright 2002-2015 The Apache Software Foundation
-
-This product includes software developed at
-The Apache Software Foundation (http://www.apache.org/).
-
-src/test/org/apache/commons/codec/language/DoubleMetaphoneTest.java
-contains test data from http://aspell.net/test/orig/batch0.tab.
-Copyright (C) 2002 Kevin Atkinson (kevina@gnu.org)
-
-===============================================================================
-
-The content of package org.apache.commons.codec.language.bm has been translated
-from the original php source code available at http://stevemorse.org/phoneticinfo.htm
-with permission from the original authors.
-Original source copyright:
-Copyright (c) 2008 Alexander Beider & Stephen P. Morse.
diff --git a/plugins/cloud-aws/licenses/commons-logging-1.1.3.jar.sha1 b/plugins/cloud-aws/licenses/commons-logging-1.1.3.jar.sha1
deleted file mode 100644
index c8756c4..0000000
--- a/plugins/cloud-aws/licenses/commons-logging-1.1.3.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-f6f66e966c70a83ffbdb6f17a0919eaf7c8aca7f
diff --git a/plugins/cloud-aws/licenses/commons-logging-LICENSE.txt b/plugins/cloud-aws/licenses/commons-logging-LICENSE.txt
deleted file mode 100644
index 57bc88a..0000000
--- a/plugins/cloud-aws/licenses/commons-logging-LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-
diff --git a/plugins/cloud-aws/licenses/commons-logging-NOTICE.txt b/plugins/cloud-aws/licenses/commons-logging-NOTICE.txt
deleted file mode 100644
index 72eb32a..0000000
--- a/plugins/cloud-aws/licenses/commons-logging-NOTICE.txt
+++ /dev/null
@@ -1,5 +0,0 @@
-Apache Commons CLI
-Copyright 2001-2009 The Apache Software Foundation
-
-This product includes software developed by
-The Apache Software Foundation (http://www.apache.org/).
diff --git a/plugins/cloud-aws/licenses/httpclient-4.3.6.jar.sha1 b/plugins/cloud-aws/licenses/httpclient-4.3.6.jar.sha1
deleted file mode 100644
index 3d35ee9..0000000
--- a/plugins/cloud-aws/licenses/httpclient-4.3.6.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-4c47155e3e6c9a41a28db36680b828ced53b8af4
diff --git a/plugins/cloud-aws/licenses/httpclient-LICENSE.txt b/plugins/cloud-aws/licenses/httpclient-LICENSE.txt
deleted file mode 100644
index 32f01ed..0000000
--- a/plugins/cloud-aws/licenses/httpclient-LICENSE.txt
+++ /dev/null
@@ -1,558 +0,0 @@
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-=========================================================================
-
-This project includes Public Suffix List copied from
-<https://publicsuffix.org/list/effective_tld_names.dat>
-licensed under the terms of the Mozilla Public License, v. 2.0
-
-Full license text: <http://mozilla.org/MPL/2.0/>
-
-Mozilla Public License Version 2.0
-==================================
-
-1. Definitions
---------------
-
-1.1. "Contributor"
-    means each individual or legal entity that creates, contributes to
-    the creation of, or owns Covered Software.
-
-1.2. "Contributor Version"
-    means the combination of the Contributions of others (if any) used
-    by a Contributor and that particular Contributor's Contribution.
-
-1.3. "Contribution"
-    means Covered Software of a particular Contributor.
-
-1.4. "Covered Software"
-    means Source Code Form to which the initial Contributor has attached
-    the notice in Exhibit A, the Executable Form of such Source Code
-    Form, and Modifications of such Source Code Form, in each case
-    including portions thereof.
-
-1.5. "Incompatible With Secondary Licenses"
-    means
-
-    (a) that the initial Contributor has attached the notice described
-        in Exhibit B to the Covered Software; or
-
-    (b) that the Covered Software was made available under the terms of
-        version 1.1 or earlier of the License, but not also under the
-        terms of a Secondary License.
-
-1.6. "Executable Form"
-    means any form of the work other than Source Code Form.
-
-1.7. "Larger Work"
-    means a work that combines Covered Software with other material, in
-    a separate file or files, that is not Covered Software.
-
-1.8. "License"
-    means this document.
-
-1.9. "Licensable"
-    means having the right to grant, to the maximum extent possible,
-    whether at the time of the initial grant or subsequently, any and
-    all of the rights conveyed by this License.
-
-1.10. "Modifications"
-    means any of the following:
-
-    (a) any file in Source Code Form that results from an addition to,
-        deletion from, or modification of the contents of Covered
-        Software; or
-
-    (b) any new file in Source Code Form that contains any Covered
-        Software.
-
-1.11. "Patent Claims" of a Contributor
-    means any patent claim(s), including without limitation, method,
-    process, and apparatus claims, in any patent Licensable by such
-    Contributor that would be infringed, but for the grant of the
-    License, by the making, using, selling, offering for sale, having
-    made, import, or transfer of either its Contributions or its
-    Contributor Version.
-
-1.12. "Secondary License"
-    means either the GNU General Public License, Version 2.0, the GNU
-    Lesser General Public License, Version 2.1, the GNU Affero General
-    Public License, Version 3.0, or any later versions of those
-    licenses.
-
-1.13. "Source Code Form"
-    means the form of the work preferred for making modifications.
-
-1.14. "You" (or "Your")
-    means an individual or a legal entity exercising rights under this
-    License. For legal entities, "You" includes any entity that
-    controls, is controlled by, or is under common control with You. For
-    purposes of this definition, "control" means (a) the power, direct
-    or indirect, to cause the direction or management of such entity,
-    whether by contract or otherwise, or (b) ownership of more than
-    fifty percent (50%) of the outstanding shares or beneficial
-    ownership of such entity.
-
-2. License Grants and Conditions
---------------------------------
-
-2.1. Grants
-
-Each Contributor hereby grants You a world-wide, royalty-free,
-non-exclusive license:
-
-(a) under intellectual property rights (other than patent or trademark)
-    Licensable by such Contributor to use, reproduce, make available,
-    modify, display, perform, distribute, and otherwise exploit its
-    Contributions, either on an unmodified basis, with Modifications, or
-    as part of a Larger Work; and
-
-(b) under Patent Claims of such Contributor to make, use, sell, offer
-    for sale, have made, import, and otherwise transfer either its
-    Contributions or its Contributor Version.
-
-2.2. Effective Date
-
-The licenses granted in Section 2.1 with respect to any Contribution
-become effective for each Contribution on the date the Contributor first
-distributes such Contribution.
-
-2.3. Limitations on Grant Scope
-
-The licenses granted in this Section 2 are the only rights granted under
-this License. No additional rights or licenses will be implied from the
-distribution or licensing of Covered Software under this License.
-Notwithstanding Section 2.1(b) above, no patent license is granted by a
-Contributor:
-
-(a) for any code that a Contributor has removed from Covered Software;
-    or
-
-(b) for infringements caused by: (i) Your and any other third party's
-    modifications of Covered Software, or (ii) the combination of its
-    Contributions with other software (except as part of its Contributor
-    Version); or
-
-(c) under Patent Claims infringed by Covered Software in the absence of
-    its Contributions.
-
-This License does not grant any rights in the trademarks, service marks,
-or logos of any Contributor (except as may be necessary to comply with
-the notice requirements in Section 3.4).
-
-2.4. Subsequent Licenses
-
-No Contributor makes additional grants as a result of Your choice to
-distribute the Covered Software under a subsequent version of this
-License (see Section 10.2) or under the terms of a Secondary License (if
-permitted under the terms of Section 3.3).
-
-2.5. Representation
-
-Each Contributor represents that the Contributor believes its
-Contributions are its original creation(s) or it has sufficient rights
-to grant the rights to its Contributions conveyed by this License.
-
-2.6. Fair Use
-
-This License is not intended to limit any rights You have under
-applicable copyright doctrines of fair use, fair dealing, or other
-equivalents.
-
-2.7. Conditions
-
-Sections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted
-in Section 2.1.
-
-3. Responsibilities
--------------------
-
-3.1. Distribution of Source Form
-
-All distribution of Covered Software in Source Code Form, including any
-Modifications that You create or to which You contribute, must be under
-the terms of this License. You must inform recipients that the Source
-Code Form of the Covered Software is governed by the terms of this
-License, and how they can obtain a copy of this License. You may not
-attempt to alter or restrict the recipients' rights in the Source Code
-Form.
-
-3.2. Distribution of Executable Form
-
-If You distribute Covered Software in Executable Form then:
-
-(a) such Covered Software must also be made available in Source Code
-    Form, as described in Section 3.1, and You must inform recipients of
-    the Executable Form how they can obtain a copy of such Source Code
-    Form by reasonable means in a timely manner, at a charge no more
-    than the cost of distribution to the recipient; and
-
-(b) You may distribute such Executable Form under the terms of this
-    License, or sublicense it under different terms, provided that the
-    license for the Executable Form does not attempt to limit or alter
-    the recipients' rights in the Source Code Form under this License.
-
-3.3. Distribution of a Larger Work
-
-You may create and distribute a Larger Work under terms of Your choice,
-provided that You also comply with the requirements of this License for
-the Covered Software. If the Larger Work is a combination of Covered
-Software with a work governed by one or more Secondary Licenses, and the
-Covered Software is not Incompatible With Secondary Licenses, this
-License permits You to additionally distribute such Covered Software
-under the terms of such Secondary License(s), so that the recipient of
-the Larger Work may, at their option, further distribute the Covered
-Software under the terms of either this License or such Secondary
-License(s).
-
-3.4. Notices
-
-You may not remove or alter the substance of any license notices
-(including copyright notices, patent notices, disclaimers of warranty,
-or limitations of liability) contained within the Source Code Form of
-the Covered Software, except that You may alter any license notices to
-the extent required to remedy known factual inaccuracies.
-
-3.5. Application of Additional Terms
-
-You may choose to offer, and to charge a fee for, warranty, support,
-indemnity or liability obligations to one or more recipients of Covered
-Software. However, You may do so only on Your own behalf, and not on
-behalf of any Contributor. You must make it absolutely clear that any
-such warranty, support, indemnity, or liability obligation is offered by
-You alone, and You hereby agree to indemnify every Contributor for any
-liability incurred by such Contributor as a result of warranty, support,
-indemnity or liability terms You offer. You may include additional
-disclaimers of warranty and limitations of liability specific to any
-jurisdiction.
-
-4. Inability to Comply Due to Statute or Regulation
----------------------------------------------------
-
-If it is impossible for You to comply with any of the terms of this
-License with respect to some or all of the Covered Software due to
-statute, judicial order, or regulation then You must: (a) comply with
-the terms of this License to the maximum extent possible; and (b)
-describe the limitations and the code they affect. Such description must
-be placed in a text file included with all distributions of the Covered
-Software under this License. Except to the extent prohibited by statute
-or regulation, such description must be sufficiently detailed for a
-recipient of ordinary skill to be able to understand it.
-
-5. Termination
---------------
-
-5.1. The rights granted under this License will terminate automatically
-if You fail to comply with any of its terms. However, if You become
-compliant, then the rights granted under this License from a particular
-Contributor are reinstated (a) provisionally, unless and until such
-Contributor explicitly and finally terminates Your grants, and (b) on an
-ongoing basis, if such Contributor fails to notify You of the
-non-compliance by some reasonable means prior to 60 days after You have
-come back into compliance. Moreover, Your grants from a particular
-Contributor are reinstated on an ongoing basis if such Contributor
-notifies You of the non-compliance by some reasonable means, this is the
-first time You have received notice of non-compliance with this License
-from such Contributor, and You become compliant prior to 30 days after
-Your receipt of the notice.
-
-5.2. If You initiate litigation against any entity by asserting a patent
-infringement claim (excluding declaratory judgment actions,
-counter-claims, and cross-claims) alleging that a Contributor Version
-directly or indirectly infringes any patent, then the rights granted to
-You by any and all Contributors for the Covered Software under Section
-2.1 of this License shall terminate.
-
-5.3. In the event of termination under Sections 5.1 or 5.2 above, all
-end user license agreements (excluding distributors and resellers) which
-have been validly granted by You or Your distributors under this License
-prior to termination shall survive termination.
-
-************************************************************************
-*                                                                      *
-*  6. Disclaimer of Warranty                                           *
-*  -------------------------                                           *
-*                                                                      *
-*  Covered Software is provided under this License on an "as is"       *
-*  basis, without warranty of any kind, either expressed, implied, or  *
-*  statutory, including, without limitation, warranties that the       *
-*  Covered Software is free of defects, merchantable, fit for a        *
-*  particular purpose or non-infringing. The entire risk as to the     *
-*  quality and performance of the Covered Software is with You.        *
-*  Should any Covered Software prove defective in any respect, You     *
-*  (not any Contributor) assume the cost of any necessary servicing,   *
-*  repair, or correction. This disclaimer of warranty constitutes an   *
-*  essential part of this License. No use of any Covered Software is   *
-*  authorized under this License except under this disclaimer.         *
-*                                                                      *
-************************************************************************
-
-************************************************************************
-*                                                                      *
-*  7. Limitation of Liability                                          *
-*  --------------------------                                          *
-*                                                                      *
-*  Under no circumstances and under no legal theory, whether tort      *
-*  (including negligence), contract, or otherwise, shall any           *
-*  Contributor, or anyone who distributes Covered Software as          *
-*  permitted above, be liable to You for any direct, indirect,         *
-*  special, incidental, or consequential damages of any character      *
-*  including, without limitation, damages for lost profits, loss of    *
-*  goodwill, work stoppage, computer failure or malfunction, or any    *
-*  and all other commercial damages or losses, even if such party      *
-*  shall have been informed of the possibility of such damages. This   *
-*  limitation of liability shall not apply to liability for death or   *
-*  personal injury resulting from such party's negligence to the       *
-*  extent applicable law prohibits such limitation. Some               *
-*  jurisdictions do not allow the exclusion or limitation of           *
-*  incidental or consequential damages, so this exclusion and          *
-*  limitation may not apply to You.                                    *
-*                                                                      *
-************************************************************************
-
-8. Litigation
--------------
-
-Any litigation relating to this License may be brought only in the
-courts of a jurisdiction where the defendant maintains its principal
-place of business and such litigation shall be governed by laws of that
-jurisdiction, without reference to its conflict-of-law provisions.
-Nothing in this Section shall prevent a party's ability to bring
-cross-claims or counter-claims.
-
-9. Miscellaneous
-----------------
-
-This License represents the complete agreement concerning the subject
-matter hereof. If any provision of this License is held to be
-unenforceable, such provision shall be reformed only to the extent
-necessary to make it enforceable. Any law or regulation which provides
-that the language of a contract shall be construed against the drafter
-shall not be used to construe this License against a Contributor.
-
-10. Versions of the License
----------------------------
-
-10.1. New Versions
-
-Mozilla Foundation is the license steward. Except as provided in Section
-10.3, no one other than the license steward has the right to modify or
-publish new versions of this License. Each version will be given a
-distinguishing version number.
-
-10.2. Effect of New Versions
-
-You may distribute the Covered Software under the terms of the version
-of the License under which You originally received the Covered Software,
-or under the terms of any subsequent version published by the license
-steward.
-
-10.3. Modified Versions
-
-If you create software not governed by this License, and you want to
-create a new license for such software, you may create and use a
-modified version of this License if you rename the license and remove
-any references to the name of the license steward (except to note that
-such modified license differs from this License).
-
-10.4. Distributing Source Code Form that is Incompatible With Secondary
-Licenses
-
-If You choose to distribute Source Code Form that is Incompatible With
-Secondary Licenses under the terms of this version of the License, the
-notice described in Exhibit B of this License must be attached.
-
-Exhibit A - Source Code Form License Notice
--------------------------------------------
-
-  This Source Code Form is subject to the terms of the Mozilla Public
-  License, v. 2.0. If a copy of the MPL was not distributed with this
-  file, You can obtain one at http://mozilla.org/MPL/2.0/.
-
-If it is not possible or desirable to put the notice in a particular
-file, then You may include the notice in a location (such as a LICENSE
-file in a relevant directory) where a recipient would be likely to look
-for such a notice.
-
-You may add additional accurate notices of copyright ownership.
-
-Exhibit B - "Incompatible With Secondary Licenses" Notice
----------------------------------------------------------
-
-  This Source Code Form is "Incompatible With Secondary Licenses", as
-  defined by the Mozilla Public License, v. 2.0.
diff --git a/plugins/cloud-aws/licenses/httpclient-NOTICE.txt b/plugins/cloud-aws/licenses/httpclient-NOTICE.txt
deleted file mode 100644
index 4f60581..0000000
--- a/plugins/cloud-aws/licenses/httpclient-NOTICE.txt
+++ /dev/null
@@ -1,5 +0,0 @@
-Apache HttpComponents Client
-Copyright 1999-2015 The Apache Software Foundation
-
-This product includes software developed at
-The Apache Software Foundation (http://www.apache.org/).
diff --git a/plugins/cloud-aws/licenses/httpcore-4.3.3.jar.sha1 b/plugins/cloud-aws/licenses/httpcore-4.3.3.jar.sha1
deleted file mode 100644
index 5d9c0e2..0000000
--- a/plugins/cloud-aws/licenses/httpcore-4.3.3.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-f91b7a4aadc5cf486df6e4634748d7dd7a73f06d
diff --git a/plugins/cloud-aws/licenses/httpcore-LICENSE.txt b/plugins/cloud-aws/licenses/httpcore-LICENSE.txt
deleted file mode 100644
index 72819a9..0000000
--- a/plugins/cloud-aws/licenses/httpcore-LICENSE.txt
+++ /dev/null
@@ -1,241 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-=========================================================================
-
-This project contains annotations in the package org.apache.http.annotation
-which are derived from JCIP-ANNOTATIONS
-Copyright (c) 2005 Brian Goetz and Tim Peierls.
-See http://www.jcip.net and the Creative Commons Attribution License
-(http://creativecommons.org/licenses/by/2.5)
-Full text: http://creativecommons.org/licenses/by/2.5/legalcode
-
-License
-
-THE WORK (AS DEFINED BELOW) IS PROVIDED UNDER THE TERMS OF THIS CREATIVE COMMONS PUBLIC LICENSE ("CCPL" OR "LICENSE"). THE WORK IS PROTECTED BY COPYRIGHT AND/OR OTHER APPLICABLE LAW. ANY USE OF THE WORK OTHER THAN AS AUTHORIZED UNDER THIS LICENSE OR COPYRIGHT LAW IS PROHIBITED.
-
-BY EXERCISING ANY RIGHTS TO THE WORK PROVIDED HERE, YOU ACCEPT AND AGREE TO BE BOUND BY THE TERMS OF THIS LICENSE. THE LICENSOR GRANTS YOU THE RIGHTS CONTAINED HERE IN CONSIDERATION OF YOUR ACCEPTANCE OF SUCH TERMS AND CONDITIONS.
-
-1. Definitions
-
-    "Collective Work" means a work, such as a periodical issue, anthology or encyclopedia, in which the Work in its entirety in unmodified form, along with a number of other contributions, constituting separate and independent works in themselves, are assembled into a collective whole. A work that constitutes a Collective Work will not be considered a Derivative Work (as defined below) for the purposes of this License.
-    "Derivative Work" means a work based upon the Work or upon the Work and other pre-existing works, such as a translation, musical arrangement, dramatization, fictionalization, motion picture version, sound recording, art reproduction, abridgment, condensation, or any other form in which the Work may be recast, transformed, or adapted, except that a work that constitutes a Collective Work will not be considered a Derivative Work for the purpose of this License. For the avoidance of doubt, where the Work is a musical composition or sound recording, the synchronization of the Work in timed-relation with a moving image ("synching") will be considered a Derivative Work for the purpose of this License.
-    "Licensor" means the individual or entity that offers the Work under the terms of this License.
-    "Original Author" means the individual or entity who created the Work.
-    "Work" means the copyrightable work of authorship offered under the terms of this License.
-    "You" means an individual or entity exercising rights under this License who has not previously violated the terms of this License with respect to the Work, or who has received express permission from the Licensor to exercise rights under this License despite a previous violation.
-
-2. Fair Use Rights. Nothing in this license is intended to reduce, limit, or restrict any rights arising from fair use, first sale or other limitations on the exclusive rights of the copyright owner under copyright law or other applicable laws.
-
-3. License Grant. Subject to the terms and conditions of this License, Licensor hereby grants You a worldwide, royalty-free, non-exclusive, perpetual (for the duration of the applicable copyright) license to exercise the rights in the Work as stated below:
-
-    to reproduce the Work, to incorporate the Work into one or more Collective Works, and to reproduce the Work as incorporated in the Collective Works;
-    to create and reproduce Derivative Works;
-    to distribute copies or phonorecords of, display publicly, perform publicly, and perform publicly by means of a digital audio transmission the Work including as incorporated in Collective Works;
-    to distribute copies or phonorecords of, display publicly, perform publicly, and perform publicly by means of a digital audio transmission Derivative Works.
-
-    For the avoidance of doubt, where the work is a musical composition:
-        Performance Royalties Under Blanket Licenses. Licensor waives the exclusive right to collect, whether individually or via a performance rights society (e.g. ASCAP, BMI, SESAC), royalties for the public performance or public digital performance (e.g. webcast) of the Work.
-        Mechanical Rights and Statutory Royalties. Licensor waives the exclusive right to collect, whether individually or via a music rights agency or designated agent (e.g. Harry Fox Agency), royalties for any phonorecord You create from the Work ("cover version") and distribute, subject to the compulsory license created by 17 USC Section 115 of the US Copyright Act (or the equivalent in other jurisdictions).
-    Webcasting Rights and Statutory Royalties. For the avoidance of doubt, where the Work is a sound recording, Licensor waives the exclusive right to collect, whether individually or via a performance-rights society (e.g. SoundExchange), royalties for the public digital performance (e.g. webcast) of the Work, subject to the compulsory license created by 17 USC Section 114 of the US Copyright Act (or the equivalent in other jurisdictions).
-
-The above rights may be exercised in all media and formats whether now known or hereafter devised. The above rights include the right to make such modifications as are technically necessary to exercise the rights in other media and formats. All rights not expressly granted by Licensor are hereby reserved.
-
-4. Restrictions.The license granted in Section 3 above is expressly made subject to and limited by the following restrictions:
-
-    You may distribute, publicly display, publicly perform, or publicly digitally perform the Work only under the terms of this License, and You must include a copy of, or the Uniform Resource Identifier for, this License with every copy or phonorecord of the Work You distribute, publicly display, publicly perform, or publicly digitally perform. You may not offer or impose any terms on the Work that alter or restrict the terms of this License or the recipients' exercise of the rights granted hereunder. You may not sublicense the Work. You must keep intact all notices that refer to this License and to the disclaimer of warranties. You may not distribute, publicly display, publicly perform, or publicly digitally perform the Work with any technological measures that control access or use of the Work in a manner inconsistent with the terms of this License Agreement. The above applies to the Work as incorporated in a Collective Work, but this does not require the Collective Work apart from the Work itself to be made subject to the terms of this License. If You create a Collective Work, upon notice from any Licensor You must, to the extent practicable, remove from the Collective Work any credit as required by clause 4(b), as requested. If You create a Derivative Work, upon notice from any Licensor You must, to the extent practicable, remove from the Derivative Work any credit as required by clause 4(b), as requested.
-    If you distribute, publicly display, publicly perform, or publicly digitally perform the Work or any Derivative Works or Collective Works, You must keep intact all copyright notices for the Work and provide, reasonable to the medium or means You are utilizing: (i) the name of the Original Author (or pseudonym, if applicable) if supplied, and/or (ii) if the Original Author and/or Licensor designate another party or parties (e.g. a sponsor institute, publishing entity, journal) for attribution in Licensor's copyright notice, terms of service or by other reasonable means, the name of such party or parties; the title of the Work if supplied; to the extent reasonably practicable, the Uniform Resource Identifier, if any, that Licensor specifies to be associated with the Work, unless such URI does not refer to the copyright notice or licensing information for the Work; and in the case of a Derivative Work, a credit identifying the use of the Work in the Derivative Work (e.g., "French translation of the Work by Original Author," or "Screenplay based on original Work by Original Author"). Such credit may be implemented in any reasonable manner; provided, however, that in the case of a Derivative Work or Collective Work, at a minimum such credit will appear where any other comparable authorship credit appears and in a manner at least as prominent as such other comparable authorship credit.
-
-5. Representations, Warranties and Disclaimer
-
-UNLESS OTHERWISE MUTUALLY AGREED TO BY THE PARTIES IN WRITING, LICENSOR OFFERS THE WORK AS-IS AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND CONCERNING THE WORK, EXPRESS, IMPLIED, STATUTORY OR OTHERWISE, INCLUDING, WITHOUT LIMITATION, WARRANTIES OF TITLE, MERCHANTIBILITY, FITNESS FOR A PARTICULAR PURPOSE, NONINFRINGEMENT, OR THE ABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OF ABSENCE OF ERRORS, WHETHER OR NOT DISCOVERABLE. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF IMPLIED WARRANTIES, SO SUCH EXCLUSION MAY NOT APPLY TO YOU.
-
-6. Limitation on Liability. EXCEPT TO THE EXTENT REQUIRED BY APPLICABLE LAW, IN NO EVENT WILL LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY FOR ANY SPECIAL, INCIDENTAL, CONSEQUENTIAL, PUNITIVE OR EXEMPLARY DAMAGES ARISING OUT OF THIS LICENSE OR THE USE OF THE WORK, EVEN IF LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
-
-7. Termination
-
-    This License and the rights granted hereunder will terminate automatically upon any breach by You of the terms of this License. Individuals or entities who have received Derivative Works or Collective Works from You under this License, however, will not have their licenses terminated provided such individuals or entities remain in full compliance with those licenses. Sections 1, 2, 5, 6, 7, and 8 will survive any termination of this License.
-    Subject to the above terms and conditions, the license granted here is perpetual (for the duration of the applicable copyright in the Work). Notwithstanding the above, Licensor reserves the right to release the Work under different license terms or to stop distributing the Work at any time; provided, however that any such election will not serve to withdraw this License (or any other license that has been, or is required to be, granted under the terms of this License), and this License will continue in full force and effect unless terminated as stated above.
-
-8. Miscellaneous
-
-    Each time You distribute or publicly digitally perform the Work or a Collective Work, the Licensor offers to the recipient a license to the Work on the same terms and conditions as the license granted to You under this License.
-    Each time You distribute or publicly digitally perform a Derivative Work, Licensor offers to the recipient a license to the original Work on the same terms and conditions as the license granted to You under this License.
-    If any provision of this License is invalid or unenforceable under applicable law, it shall not affect the validity or enforceability of the remainder of the terms of this License, and without further action by the parties to this agreement, such provision shall be reformed to the minimum extent necessary to make such provision valid and enforceable.
-    No term or provision of this License shall be deemed waived and no breach consented to unless such waiver or consent shall be in writing and signed by the party to be charged with such waiver or consent.
-    This License constitutes the entire agreement between the parties with respect to the Work licensed here. There are no understandings, agreements or representations with respect to the Work not specified here. Licensor shall not be bound by any additional provisions that may appear in any communication from You. This License may not be modified without the mutual written agreement of the Licensor and You.
diff --git a/plugins/cloud-aws/licenses/httpcore-NOTICE.txt b/plugins/cloud-aws/licenses/httpcore-NOTICE.txt
deleted file mode 100644
index c0be50a..0000000
--- a/plugins/cloud-aws/licenses/httpcore-NOTICE.txt
+++ /dev/null
@@ -1,8 +0,0 @@
-Apache HttpComponents Core
-Copyright 2005-2014 The Apache Software Foundation
-
-This product includes software developed at
-The Apache Software Foundation (http://www.apache.org/).
-
-This project contains annotations derived from JCIP-ANNOTATIONS
-Copyright (c) 2005 Brian Goetz and Tim Peierls. See http://www.jcip.net
diff --git a/plugins/cloud-aws/licenses/jackson-LICENSE b/plugins/cloud-aws/licenses/jackson-LICENSE
deleted file mode 100644
index f5f45d2..0000000
--- a/plugins/cloud-aws/licenses/jackson-LICENSE
+++ /dev/null
@@ -1,8 +0,0 @@
-This copy of Jackson JSON processor streaming parser/generator is licensed under the
-Apache (Software) License, version 2.0 ("the License").
-See the License for details about distribution rights, and the
-specific rights regarding derivate works.
-
-You may obtain a copy of the License at:
-
-http://www.apache.org/licenses/LICENSE-2.0
diff --git a/plugins/cloud-aws/licenses/jackson-NOTICE b/plugins/cloud-aws/licenses/jackson-NOTICE
deleted file mode 100644
index 4c976b7..0000000
--- a/plugins/cloud-aws/licenses/jackson-NOTICE
+++ /dev/null
@@ -1,20 +0,0 @@
-# Jackson JSON processor
-
-Jackson is a high-performance, Free/Open Source JSON processing library.
-It was originally written by Tatu Saloranta (tatu.saloranta@iki.fi), and has
-been in development since 2007.
-It is currently developed by a community of developers, as well as supported
-commercially by FasterXML.com.
-
-## Licensing
-
-Jackson core and extension components may licensed under different licenses.
-To find the details that apply to this artifact see the accompanying LICENSE file.
-For more information, including possible other licensing options, contact
-FasterXML.com (http://fasterxml.com).
-
-## Credits
-
-A list of contributors may be found from CREDITS file, which is included
-in some artifacts (usually source distributions); but is always available
-from the source code management (SCM) system project uses.
diff --git a/plugins/cloud-aws/licenses/jackson-annotations-2.5.0.jar.sha1 b/plugins/cloud-aws/licenses/jackson-annotations-2.5.0.jar.sha1
deleted file mode 100644
index 862ac6f..0000000
--- a/plugins/cloud-aws/licenses/jackson-annotations-2.5.0.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-a2a55a3375bc1cef830ca426d68d2ea22961190e
diff --git a/plugins/cloud-aws/licenses/jackson-databind-2.5.3.jar.sha1 b/plugins/cloud-aws/licenses/jackson-databind-2.5.3.jar.sha1
deleted file mode 100644
index cdc6695..0000000
--- a/plugins/cloud-aws/licenses/jackson-databind-2.5.3.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-c37875ff66127d93e5f672708cb2dcc14c8232ab
diff --git a/plugins/cloud-aws/pom.xml b/plugins/cloud-aws/pom.xml
deleted file mode 100644
index 7d0ea71..0000000
--- a/plugins/cloud-aws/pom.xml
+++ /dev/null
@@ -1,55 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<project xmlns="http://maven.apache.org/POM/4.0.0"
-         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
-    <modelVersion>4.0.0</modelVersion>
-
-    <parent>
-        <groupId>org.elasticsearch.plugin</groupId>
-        <artifactId>plugins</artifactId>
-        <version>2.1.0-SNAPSHOT</version>
-    </parent>
-
-    <artifactId>cloud-aws</artifactId>
-    <name>Plugin: Cloud: AWS</name>
-    <description>The Amazon Web Service (AWS) Cloud plugin allows to use AWS API for the unicast discovery mechanism and add S3 repositories.</description>
-
-    <properties>
-        <elasticsearch.plugin.classname>org.elasticsearch.plugin.cloud.aws.CloudAwsPlugin</elasticsearch.plugin.classname>
-        <amazonaws.version>1.10.12</amazonaws.version>
-        <tests.jvms>1</tests.jvms>
-        <tests.rest.suite>cloud_aws</tests.rest.suite>
-        <tests.rest.load_packaged>false</tests.rest.load_packaged>
-    </properties>
-
-    <dependencies>
-        <!-- AWS SDK -->
-        <dependency>
-            <groupId>com.amazonaws</groupId>
-            <artifactId>aws-java-sdk-ec2</artifactId>
-            <version>${amazonaws.version}</version>
-        </dependency>
-        <dependency>
-            <groupId>com.amazonaws</groupId>
-            <artifactId>aws-java-sdk-s3</artifactId>
-            <version>${amazonaws.version}</version>
-        </dependency>
-        <!-- We need to force here the compile scope as it was defined as test scope in plugins/pom.xml -->
-        <!-- TODO: remove this dependency when we will have a REST Test module -->
-        <dependency>
-            <groupId>org.apache.httpcomponents</groupId>
-            <artifactId>httpclient</artifactId>
-            <scope>compile</scope>
-        </dependency>
-    </dependencies>
-
-    <build>
-        <plugins>
-            <plugin>
-                <groupId>org.apache.maven.plugins</groupId>
-                <artifactId>maven-assembly-plugin</artifactId>
-            </plugin>
-        </plugins>
-    </build>
-
-</project>
diff --git a/plugins/cloud-aws/rest-api-spec/test/cloud_aws/10_basic.yaml b/plugins/cloud-aws/rest-api-spec/test/cloud_aws/10_basic.yaml
deleted file mode 100644
index a2c8f88..0000000
--- a/plugins/cloud-aws/rest-api-spec/test/cloud_aws/10_basic.yaml
+++ /dev/null
@@ -1,14 +0,0 @@
-# Integration tests for Cloud AWS components
-#
-"Cloud AWS loaded":
-    - do:
-        cluster.state: {}
-
-    # Get master node id
-    - set: { master_node: master }
-
-    - do:
-        nodes.info: {}
-
-    - match:  { nodes.$master.plugins.0.name: cloud-aws  }
-    - match:  { nodes.$master.plugins.0.jvm: true  }
diff --git a/plugins/cloud-aws/rest-api-spec/test/cloud_aws/20_repository.yaml b/plugins/cloud-aws/rest-api-spec/test/cloud_aws/20_repository.yaml
deleted file mode 100644
index df26e51..0000000
--- a/plugins/cloud-aws/rest-api-spec/test/cloud_aws/20_repository.yaml
+++ /dev/null
@@ -1,23 +0,0 @@
-# Integration tests for Cloud AWS components
-#
-"S3 repository can be registereed":
-    - do:
-        snapshot.create_repository:
-          repository: test_repo_s3_1
-          verify: false
-          body:
-            type: s3
-            settings:
-              bucket: "my_bucket_name"
-              access_key: "AKVAIQBF2RECL7FJWGJQ"
-              secret_key: "vExyMThREXeRMm/b/LRzEB8jWwvzQeXgjqMX+6br"
-
-    # Get repositry
-    - do:
-        snapshot.get_repository:
-          repository: test_repo_s3_1
-
-    - is_true: test_repo_s3_1
-    - is_true: test_repo_s3_1.settings.bucket
-    - is_false: test_repo_s3_1.settings.access_key
-    - is_false: test_repo_s3_1.settings.secret_key
diff --git a/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java b/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java
deleted file mode 100644
index f8ecc3c..0000000
--- a/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java
+++ /dev/null
@@ -1,179 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.aws;
-
-import java.util.Locale;
-
-import com.amazonaws.ClientConfiguration;
-import com.amazonaws.Protocol;
-import com.amazonaws.auth.*;
-import com.amazonaws.internal.StaticCredentialsProvider;
-import com.amazonaws.services.ec2.AmazonEC2;
-import com.amazonaws.services.ec2.AmazonEC2Client;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.cloud.aws.network.Ec2NameResolver;
-import org.elasticsearch.cloud.aws.node.Ec2CustomNodeAttributes;
-import org.elasticsearch.cluster.node.DiscoveryNodeService;
-import org.elasticsearch.common.component.AbstractLifecycleComponent;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.network.NetworkService;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsFilter;
-
-/**
- *
- */
-public class AwsEc2Service extends AbstractLifecycleComponent<AwsEc2Service> {
-
-    public static final String EC2_METADATA_URL = "http://169.254.169.254/latest/meta-data/";
-
-    private AmazonEC2Client client;
-
-    @Inject
-    public AwsEc2Service(Settings settings, SettingsFilter settingsFilter, NetworkService networkService, DiscoveryNodeService discoveryNodeService) {
-        super(settings);
-        settingsFilter.addFilter("cloud.aws.access_key");
-        settingsFilter.addFilter("cloud.aws.secret_key");
-        // Filter repository-specific settings
-        settingsFilter.addFilter("access_key");
-        settingsFilter.addFilter("secret_key");
-        // add specific ec2 name resolver
-        networkService.addCustomNameResolver(new Ec2NameResolver(settings));
-        discoveryNodeService.addCustomAttributeProvider(new Ec2CustomNodeAttributes(settings));
-    }
-
-    public synchronized AmazonEC2 client() {
-        if (client != null) {
-            return client;
-        }
-
-        ClientConfiguration clientConfiguration = new ClientConfiguration();
-        // the response metadata cache is only there for diagnostics purposes,
-        // but can force objects from every response to the old generation.
-        clientConfiguration.setResponseMetadataCacheSize(0);
-        String protocol = settings.get("cloud.aws.protocol", "https").toLowerCase(Locale.ROOT);
-        protocol = settings.get("cloud.aws.ec2.protocol", protocol).toLowerCase(Locale.ROOT);
-        if ("http".equals(protocol)) {
-            clientConfiguration.setProtocol(Protocol.HTTP);
-        } else if ("https".equals(protocol)) {
-            clientConfiguration.setProtocol(Protocol.HTTPS);
-        } else {
-            throw new IllegalArgumentException("No protocol supported [" + protocol + "], can either be [http] or [https]");
-        }
-        String account = settings.get("cloud.aws.access_key");
-        String key = settings.get("cloud.aws.secret_key");
-
-        String proxyHost = settings.get("cloud.aws.proxy_host");
-        proxyHost = settings.get("cloud.aws.ec2.proxy_host", proxyHost);
-        if (proxyHost != null) {
-            String portString = settings.get("cloud.aws.proxy_port", "80");
-            portString = settings.get("cloud.aws.ec2.proxy_port", portString);
-            Integer proxyPort;
-            try {
-                proxyPort = Integer.parseInt(portString, 10);
-            } catch (NumberFormatException ex) {
-                throw new IllegalArgumentException("The configured proxy port value [" + portString + "] is invalid", ex);
-            }
-            clientConfiguration.withProxyHost(proxyHost).setProxyPort(proxyPort);
-        }
-
-        // #155: we might have 3rd party users using older EC2 API version
-        String awsSigner = settings.get("cloud.aws.ec2.signer", settings.get("cloud.aws.signer"));
-        if (awsSigner != null) {
-            logger.debug("using AWS API signer [{}]", awsSigner);
-            try {
-                AwsSigner.configureSigner(awsSigner, clientConfiguration);
-            } catch (IllegalArgumentException e) {
-                logger.warn("wrong signer set for [cloud.aws.ec2.signer] or [cloud.aws.signer]: [{}]", awsSigner);
-            }
-        }
-
-        AWSCredentialsProvider credentials;
-
-        if (account == null && key == null) {
-            credentials = new AWSCredentialsProviderChain(
-                    new EnvironmentVariableCredentialsProvider(),
-                    new SystemPropertiesCredentialsProvider(),
-                    new InstanceProfileCredentialsProvider()
-            );
-        } else {
-            credentials = new AWSCredentialsProviderChain(
-                    new StaticCredentialsProvider(new BasicAWSCredentials(account, key))
-            );
-        }
-
-        this.client = new AmazonEC2Client(credentials, clientConfiguration);
-
-        if (settings.get("cloud.aws.ec2.endpoint") != null) {
-            String endpoint = settings.get("cloud.aws.ec2.endpoint");
-            logger.debug("using explicit ec2 endpoint [{}]", endpoint);
-            client.setEndpoint(endpoint);
-        } else if (settings.get("cloud.aws.region") != null) {
-            String region = settings.get("cloud.aws.region").toLowerCase(Locale.ROOT);
-            String endpoint;
-            if (region.equals("us-east-1") || region.equals("us-east")) {
-                endpoint = "ec2.us-east-1.amazonaws.com";
-            } else if (region.equals("us-west") || region.equals("us-west-1")) {
-                endpoint = "ec2.us-west-1.amazonaws.com";
-            } else if (region.equals("us-west-2")) {
-                endpoint = "ec2.us-west-2.amazonaws.com";
-            } else if (region.equals("ap-southeast") || region.equals("ap-southeast-1")) {
-                endpoint = "ec2.ap-southeast-1.amazonaws.com";
-            } else if (region.equals("ap-southeast-2")) {
-                endpoint = "ec2.ap-southeast-2.amazonaws.com";
-            } else if (region.equals("ap-northeast") || region.equals("ap-northeast-1")) {
-                endpoint = "ec2.ap-northeast-1.amazonaws.com";
-            } else if (region.equals("eu-west") || region.equals("eu-west-1")) {
-                endpoint = "ec2.eu-west-1.amazonaws.com";
-            } else if (region.equals("eu-central") || region.equals("eu-central-1")) {
-                endpoint = "ec2.eu-central-1.amazonaws.com";
-            } else if (region.equals("sa-east") || region.equals("sa-east-1")) {
-                endpoint = "ec2.sa-east-1.amazonaws.com";
-            } else if (region.equals("cn-north") || region.equals("cn-north-1")) {
-                endpoint = "ec2.cn-north-1.amazonaws.com.cn";
-            } else {
-                throw new IllegalArgumentException("No automatic endpoint could be derived from region [" + region + "]");
-            }
-            if (endpoint != null) {
-                logger.debug("using ec2 region [{}], with endpoint [{}]", region, endpoint);
-                client.setEndpoint(endpoint);
-            }
-        }
-
-        return this.client;
-
-    }
-
-    @Override
-    protected void doStart() throws ElasticsearchException {
-    }
-
-    @Override
-    protected void doStop() throws ElasticsearchException {
-    }
-
-    @Override
-    protected void doClose() throws ElasticsearchException {
-        if (client != null) {
-            client.shutdown();
-        }
-    }
-}
diff --git a/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/AwsModule.java b/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/AwsModule.java
deleted file mode 100644
index e848d33..0000000
--- a/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/AwsModule.java
+++ /dev/null
@@ -1,40 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.aws;
-
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.settings.Settings;
-
-public class AwsModule extends AbstractModule {
-
-
-    // pkg private so it is settable by tests
-    static Class<? extends AwsS3Service> s3ServiceImpl = InternalAwsS3Service.class;
-
-    public static Class<? extends AwsS3Service> getS3ServiceImpl() {
-        return s3ServiceImpl;
-    }
-
-    @Override
-    protected void configure() {
-        bind(AwsS3Service.class).to(s3ServiceImpl).asEagerSingleton();
-        bind(AwsEc2Service.class).asEagerSingleton();
-    }
-}
\ No newline at end of file
diff --git a/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/AwsS3Service.java b/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/AwsS3Service.java
deleted file mode 100644
index e5db2ed..0000000
--- a/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/AwsS3Service.java
+++ /dev/null
@@ -1,34 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.aws;
-
-import com.amazonaws.services.s3.AmazonS3;
-import org.elasticsearch.common.component.LifecycleComponent;
-
-/**
- *
- */
-public interface AwsS3Service extends LifecycleComponent<AwsS3Service> {
-    AmazonS3 client();
-
-    AmazonS3 client(String endpoint, String protocol, String region, String account, String key);
-
-    AmazonS3 client(String endpoint, String protocol, String region, String account, String key, Integer maxRetries);
-}
diff --git a/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/AwsSigner.java b/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/AwsSigner.java
deleted file mode 100644
index 476bb74..0000000
--- a/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/AwsSigner.java
+++ /dev/null
@@ -1,53 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.aws;
-
-import com.amazonaws.ClientConfiguration;
-import com.amazonaws.auth.SignerFactory;
-
-public class AwsSigner {
-
-    private AwsSigner() {
-
-    }
-
-    /**
-     * Add a AWS API Signer.
-     * @param signer Signer to use
-     * @param configuration AWS Client configuration
-     * @throws IllegalArgumentException if signer does not exist
-     */
-    public static void configureSigner(String signer, ClientConfiguration configuration)
-        throws IllegalArgumentException {
-
-        if (signer == null) {
-            throw new IllegalArgumentException("[null] signer set");
-        }
-
-        try {
-            // We check this signer actually exists in AWS SDK
-            // It throws a IllegalArgumentException if not found
-            SignerFactory.getSignerByTypeAndService(signer, null);
-            configuration.setSignerOverride(signer);
-        } catch (IllegalArgumentException e) {
-            throw new IllegalArgumentException("wrong signer set [" + signer + "]");
-        }
-    }
-}
diff --git a/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java b/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java
deleted file mode 100644
index b7e029b..0000000
--- a/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java
+++ /dev/null
@@ -1,218 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.aws;
-
-import com.amazonaws.ClientConfiguration;
-import com.amazonaws.Protocol;
-import com.amazonaws.auth.*;
-import com.amazonaws.http.IdleConnectionReaper;
-import com.amazonaws.internal.StaticCredentialsProvider;
-import com.amazonaws.services.s3.AmazonS3;
-import com.amazonaws.services.s3.AmazonS3Client;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.collect.Tuple;
-import org.elasticsearch.common.component.AbstractLifecycleComponent;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-
-import java.util.HashMap;
-import java.util.Locale;
-import java.util.Map;
-
-/**
- *
- */
-public class InternalAwsS3Service extends AbstractLifecycleComponent<AwsS3Service> implements AwsS3Service {
-
-    /**
-     * (acceskey, endpoint) -> client
-     */
-    private Map<Tuple<String, String>, AmazonS3Client> clients = new HashMap<Tuple<String,String>, AmazonS3Client>();
-
-    @Inject
-    public InternalAwsS3Service(Settings settings) {
-        super(settings);
-    }
-
-    @Override
-    public synchronized AmazonS3 client() {
-        String endpoint = getDefaultEndpoint();
-        String account = settings.get("cloud.aws.access_key");
-        String key = settings.get("cloud.aws.secret_key");
-
-        return getClient(endpoint, null, account, key, null);
-    }
-
-    @Override
-    public AmazonS3 client(String endpoint, String protocol, String region, String account, String key) {
-        return client(endpoint, protocol, region, account, key, null);
-    }
-
-    @Override
-    public synchronized AmazonS3 client(String endpoint, String protocol, String region, String account, String key, Integer maxRetries) {
-        if (region != null && endpoint == null) {
-            endpoint = getEndpoint(region);
-            logger.debug("using s3 region [{}], with endpoint [{}]", region, endpoint);
-        } else if (endpoint == null) {
-            endpoint = getDefaultEndpoint();
-        }
-        if (account == null || key == null) {
-            account = settings.get("cloud.aws.access_key");
-            key = settings.get("cloud.aws.secret_key");
-        }
-
-        return getClient(endpoint, protocol, account, key, maxRetries);
-    }
-
-
-    private synchronized AmazonS3 getClient(String endpoint, String protocol, String account, String key, Integer maxRetries) {
-        Tuple<String, String> clientDescriptor = new Tuple<String, String>(endpoint, account);
-        AmazonS3Client client = clients.get(clientDescriptor);
-        if (client != null) {
-            return client;
-        }
-
-        ClientConfiguration clientConfiguration = new ClientConfiguration();
-        // the response metadata cache is only there for diagnostics purposes,
-        // but can force objects from every response to the old generation.
-        clientConfiguration.setResponseMetadataCacheSize(0);
-        if (protocol == null) {
-            protocol = settings.get("cloud.aws.protocol", "https").toLowerCase(Locale.ROOT);
-            protocol = settings.get("cloud.aws.s3.protocol", protocol).toLowerCase(Locale.ROOT);
-        }
-
-        if ("http".equals(protocol)) {
-            clientConfiguration.setProtocol(Protocol.HTTP);
-        } else if ("https".equals(protocol)) {
-            clientConfiguration.setProtocol(Protocol.HTTPS);
-        } else {
-            throw new IllegalArgumentException("No protocol supported [" + protocol + "], can either be [http] or [https]");
-        }
-
-        String proxyHost = settings.get("cloud.aws.proxy_host");
-        proxyHost = settings.get("cloud.aws.s3.proxy_host", proxyHost);
-        if (proxyHost != null) {
-            String portString = settings.get("cloud.aws.proxy_port", "80");
-            portString = settings.get("cloud.aws.s3.proxy_port", portString);
-            Integer proxyPort;
-            try {
-                proxyPort = Integer.parseInt(portString, 10);
-            } catch (NumberFormatException ex) {
-                throw new IllegalArgumentException("The configured proxy port value [" + portString + "] is invalid", ex);
-            }
-            clientConfiguration.withProxyHost(proxyHost).setProxyPort(proxyPort);
-        }
-
-        if (maxRetries != null) {
-            // If not explicitly set, default to 3 with exponential backoff policy
-            clientConfiguration.setMaxErrorRetry(maxRetries);
-        }
-
-        // #155: we might have 3rd party users using older S3 API version
-        String awsSigner = settings.get("cloud.aws.s3.signer", settings.get("cloud.aws.signer"));
-        if (awsSigner != null) {
-            logger.debug("using AWS API signer [{}]", awsSigner);
-            try {
-                AwsSigner.configureSigner(awsSigner, clientConfiguration);
-            } catch (IllegalArgumentException e) {
-                logger.warn("wrong signer set for [cloud.aws.s3.signer] or [cloud.aws.signer]: [{}]", awsSigner);
-            }
-        }
-
-        AWSCredentialsProvider credentials;
-
-        if (account == null && key == null) {
-            credentials = new AWSCredentialsProviderChain(
-                    new EnvironmentVariableCredentialsProvider(),
-                    new SystemPropertiesCredentialsProvider(),
-                    new InstanceProfileCredentialsProvider()
-            );
-        } else {
-            credentials = new AWSCredentialsProviderChain(
-                    new StaticCredentialsProvider(new BasicAWSCredentials(account, key))
-            );
-        }
-        client = new AmazonS3Client(credentials, clientConfiguration);
-
-        if (endpoint != null) {
-            client.setEndpoint(endpoint);
-        }
-        clients.put(clientDescriptor, client);
-        return client;
-    }
-
-    private String getDefaultEndpoint() {
-        String endpoint = null;
-        if (settings.get("cloud.aws.s3.endpoint") != null) {
-            endpoint = settings.get("cloud.aws.s3.endpoint");
-            logger.debug("using explicit s3 endpoint [{}]", endpoint);
-        } else if (settings.get("cloud.aws.region") != null) {
-            String region = settings.get("cloud.aws.region").toLowerCase(Locale.ROOT);
-            endpoint = getEndpoint(region);
-            logger.debug("using s3 region [{}], with endpoint [{}]", region, endpoint);
-        }
-        return endpoint;
-    }
-
-    private static String getEndpoint(String region) {
-        if ("us-east".equals(region) || "us-east-1".equals(region)) {
-            return "s3.amazonaws.com";
-        } else if ("us-west".equals(region) || "us-west-1".equals(region)) {
-            return "s3-us-west-1.amazonaws.com";
-        } else if ("us-west-2".equals(region)) {
-            return "s3-us-west-2.amazonaws.com";
-        } else if ("ap-southeast".equals(region) || "ap-southeast-1".equals(region)) {
-            return "s3-ap-southeast-1.amazonaws.com";
-        } else if ("ap-southeast-2".equals(region)) {
-            return "s3-ap-southeast-2.amazonaws.com";
-        } else if ("ap-northeast".equals(region) || "ap-northeast-1".equals(region)) {
-            return "s3-ap-northeast-1.amazonaws.com";
-        } else if ("eu-west".equals(region) || "eu-west-1".equals(region)) {
-            return "s3-eu-west-1.amazonaws.com";
-        } else if ("eu-central".equals(region) || "eu-central-1".equals(region)) {
-            return "s3.eu-central-1.amazonaws.com";
-        } else if ("sa-east".equals(region) || "sa-east-1".equals(region)) {
-            return "s3-sa-east-1.amazonaws.com";
-        } else if ("cn-north".equals(region) || "cn-north-1".equals(region)) {
-            return "s3.cn-north-1.amazonaws.com.cn";
-        } else {
-            throw new IllegalArgumentException("No automatic endpoint could be derived from region [" + region + "]");
-        }
-    }
-
-    @Override
-    protected void doStart() throws ElasticsearchException {
-    }
-
-    @Override
-    protected void doStop() throws ElasticsearchException {
-    }
-
-    @Override
-    protected void doClose() throws ElasticsearchException {
-        for (AmazonS3Client client : clients.values()) {
-            client.shutdown();
-        }
-
-        // Ensure that IdleConnectionReaper is shutdown
-        IdleConnectionReaper.shutdown();
-    }
-}
diff --git a/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/blobstore/DefaultS3OutputStream.java b/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/blobstore/DefaultS3OutputStream.java
deleted file mode 100644
index 67e6889..0000000
--- a/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/blobstore/DefaultS3OutputStream.java
+++ /dev/null
@@ -1,258 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.aws.blobstore;
-
-import com.amazonaws.AmazonClientException;
-import com.amazonaws.services.s3.model.*;
-import com.amazonaws.util.Base64;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.Loggers;
-import org.elasticsearch.common.unit.ByteSizeUnit;
-import org.elasticsearch.common.unit.ByteSizeValue;
-
-import java.io.ByteArrayInputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.security.DigestInputStream;
-import java.security.MessageDigest;
-import java.security.NoSuchAlgorithmException;
-import java.util.ArrayList;
-import java.util.List;
-
-/**
- * DefaultS3OutputStream uploads data to the AWS S3 service using 2 modes: single and multi part.
- * <p/>
- * When the length of the chunk is lower than buffer_size, the chunk is uploaded with a single request.
- * Otherwise multiple requests are made, each of buffer_size (except the last one which can be lower than buffer_size).
- * <p/>
- * Quick facts about S3:
- * <p/>
- * Maximum object size:                 5 TB
- * Maximum number of parts per upload:  10,000
- * Part numbers:                        1 to 10,000 (inclusive)
- * Part size:                           5 MB to 5 GB, last part can be < 5 MB
- * <p/>
- * See http://docs.aws.amazon.com/AmazonS3/latest/dev/qfacts.html
- * See http://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html
- */
-public class DefaultS3OutputStream extends S3OutputStream {
-
-    private static final ByteSizeValue MULTIPART_MAX_SIZE = new ByteSizeValue(5, ByteSizeUnit.GB);
-    private static final ESLogger logger = Loggers.getLogger("cloud.aws");
-    /**
-     * Multipart Upload API data
-     */
-    private String multipartId;
-    private int multipartChunks;
-    private List<PartETag> multiparts;
-
-    public DefaultS3OutputStream(S3BlobStore blobStore, String bucketName, String blobName, int bufferSizeInBytes, int numberOfRetries, boolean serverSideEncryption) {
-        super(blobStore, bucketName, blobName, bufferSizeInBytes, numberOfRetries, serverSideEncryption);
-    }
-
-    @Override
-    public void flush(byte[] bytes, int off, int len, boolean closing) throws IOException {
-        if (len > MULTIPART_MAX_SIZE.getBytes()) {
-            throw new IOException("Unable to upload files larger than " + MULTIPART_MAX_SIZE + " to Amazon S3");
-        }
-
-        if (!closing) {
-            if (len < getBufferSize()) {
-                upload(bytes, off, len);
-            } else {
-                if (getFlushCount() == 0) {
-                    initializeMultipart();
-                }
-                uploadMultipart(bytes, off, len, false);
-            }
-        } else {
-            if (multipartId != null) {
-                uploadMultipart(bytes, off, len, true);
-                completeMultipart();
-            } else {
-                upload(bytes, off, len);
-            }
-        }
-    }
-
-    /**
-     * Upload data using a single request.
-     *
-     * @param bytes
-     * @param off
-     * @param len
-     * @throws IOException
-     */
-    private void upload(byte[] bytes, int off, int len) throws IOException {
-        try (ByteArrayInputStream is = new ByteArrayInputStream(bytes, off, len)) {
-            int retry = 0;
-            while (retry <= getNumberOfRetries()) {
-                try {
-                    doUpload(getBlobStore(), getBucketName(), getBlobName(), is, len, isServerSideEncryption());
-                    break;
-                } catch (AmazonClientException e) {
-                    if (getBlobStore().shouldRetry(e) && retry < getNumberOfRetries()) {
-                        is.reset();
-                        retry++;
-                    } else {
-                        throw new IOException("Unable to upload object " + getBlobName(), e);
-                    }
-                }
-            }
-        }
-    }
-
-    protected void doUpload(S3BlobStore blobStore, String bucketName, String blobName, InputStream is, int length,
-            boolean serverSideEncryption) throws AmazonS3Exception {
-        ObjectMetadata md = new ObjectMetadata();
-        if (serverSideEncryption) {
-            md.setSSEAlgorithm(ObjectMetadata.AES_256_SERVER_SIDE_ENCRYPTION);
-        }
-        md.setContentLength(length);
-
-        InputStream inputStream = is;
-
-        // We try to compute a MD5 while reading it
-        MessageDigest messageDigest;
-        try {
-            messageDigest = MessageDigest.getInstance("MD5");
-            inputStream = new DigestInputStream(is, messageDigest);
-        } catch (NoSuchAlgorithmException impossible) {
-            // Every implementation of the Java platform is required to support MD5 (see MessageDigest)
-            throw new RuntimeException(impossible);
-        }
-        PutObjectResult putObjectResult = blobStore.client().putObject(bucketName, blobName, inputStream, md);
-
-        String localMd5 = Base64.encodeAsString(messageDigest.digest());
-        String remoteMd5 = putObjectResult.getContentMd5();
-        if (!localMd5.equals(remoteMd5)) {
-            logger.debug("MD5 local [{}], remote [{}] are not equal...", localMd5, remoteMd5);
-            throw new AmazonS3Exception("MD5 local [" + localMd5 +
-                    "], remote [" + remoteMd5 +
-                    "] are not equal...");
-        }
-    }
-
-    private void initializeMultipart() {
-        int retry = 0;
-        while ((retry <= getNumberOfRetries()) && (multipartId == null)) {
-            try {
-                multipartId = doInitialize(getBlobStore(), getBucketName(), getBlobName(), isServerSideEncryption());
-                if (multipartId != null) {
-                    multipartChunks = 1;
-                    multiparts = new ArrayList<>();
-                }
-            } catch (AmazonClientException e) {
-                if (getBlobStore().shouldRetry(e) && retry < getNumberOfRetries()) {
-                    retry++;
-                } else {
-                    throw e;
-                }
-            }
-        }
-    }
-
-    protected String doInitialize(S3BlobStore blobStore, String bucketName, String blobName, boolean serverSideEncryption) {
-        InitiateMultipartUploadRequest request = new InitiateMultipartUploadRequest(bucketName, blobName);
-        if (serverSideEncryption) {
-            ObjectMetadata md = new ObjectMetadata();
-            md.setSSEAlgorithm(ObjectMetadata.AES_256_SERVER_SIDE_ENCRYPTION);
-            request.setObjectMetadata(md);
-        }
-        return blobStore.client().initiateMultipartUpload(request).getUploadId();
-    }
-
-    private void uploadMultipart(byte[] bytes, int off, int len, boolean lastPart) throws IOException {
-        try (ByteArrayInputStream is = new ByteArrayInputStream(bytes, off, len)) {
-            int retry = 0;
-            while (retry <= getNumberOfRetries()) {
-                try {
-                    PartETag partETag = doUploadMultipart(getBlobStore(), getBucketName(), getBlobName(), multipartId, is, len, lastPart);
-                    multiparts.add(partETag);
-                    multipartChunks++;
-                    return;
-                } catch (AmazonClientException e) {
-                    if (getBlobStore().shouldRetry(e) && retry < getNumberOfRetries()) {
-                        is.reset();
-                        retry++;
-                    } else {
-                        abortMultipart();
-                        throw e;
-                    }
-                }
-            }
-        }
-    }
-
-    protected PartETag doUploadMultipart(S3BlobStore blobStore, String bucketName, String blobName, String uploadId, InputStream is,
-            int length, boolean lastPart) throws AmazonS3Exception {
-        UploadPartRequest request = new UploadPartRequest()
-        .withBucketName(bucketName)
-        .withKey(blobName)
-        .withUploadId(uploadId)
-        .withPartNumber(multipartChunks)
-        .withInputStream(is)
-        .withPartSize(length)
-        .withLastPart(lastPart);
-
-        UploadPartResult response = blobStore.client().uploadPart(request);
-        return response.getPartETag();
-
-    }
-
-    private void completeMultipart() {
-        int retry = 0;
-        while (retry <= getNumberOfRetries()) {
-            try {
-                doCompleteMultipart(getBlobStore(), getBucketName(), getBlobName(), multipartId, multiparts);
-                multipartId = null;
-                return;
-            } catch (AmazonClientException e) {
-                if (getBlobStore().shouldRetry(e) && retry < getNumberOfRetries()) {
-                    retry++;
-                } else {
-                    abortMultipart();
-                    throw e;
-                }
-            }
-        }
-    }
-
-    protected void doCompleteMultipart(S3BlobStore blobStore, String bucketName, String blobName, String uploadId, List<PartETag> parts)
-            throws AmazonS3Exception {
-        CompleteMultipartUploadRequest request = new CompleteMultipartUploadRequest(bucketName, blobName, uploadId, parts);
-        blobStore.client().completeMultipartUpload(request);
-    }
-
-    private void abortMultipart() {
-        if (multipartId != null) {
-            try {
-                doAbortMultipart(getBlobStore(), getBucketName(), getBlobName(), multipartId);
-            } finally {
-                multipartId = null;
-            }
-        }
-    }
-
-    protected void doAbortMultipart(S3BlobStore blobStore, String bucketName, String blobName, String uploadId)
-            throws AmazonS3Exception {
-        blobStore.client().abortMultipartUpload(new AbortMultipartUploadRequest(bucketName, blobName, uploadId));
-    }
-}
diff --git a/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobContainer.java b/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobContainer.java
deleted file mode 100644
index 1c0b23e..0000000
--- a/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobContainer.java
+++ /dev/null
@@ -1,162 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.aws.blobstore;
-
-import com.amazonaws.AmazonClientException;
-import com.amazonaws.services.s3.model.*;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.blobstore.BlobMetaData;
-import org.elasticsearch.common.blobstore.BlobPath;
-import org.elasticsearch.common.blobstore.BlobStoreException;
-import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;
-import org.elasticsearch.common.blobstore.support.PlainBlobMetaData;
-import org.elasticsearch.common.collect.MapBuilder;
-
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-import java.util.Map;
-
-/**
- *
- */
-public class S3BlobContainer extends AbstractBlobContainer {
-
-    protected final S3BlobStore blobStore;
-
-    protected final String keyPath;
-
-    public S3BlobContainer(BlobPath path, S3BlobStore blobStore) {
-        super(path);
-        this.blobStore = blobStore;
-        String keyPath = path.buildAsString("/");
-        if (!keyPath.isEmpty()) {
-            keyPath = keyPath + "/";
-        }
-        this.keyPath = keyPath;
-    }
-
-    @Override
-    public boolean blobExists(String blobName) {
-        try {
-            blobStore.client().getObjectMetadata(blobStore.bucket(), buildKey(blobName));
-            return true;
-        } catch (AmazonS3Exception e) {
-            return false;
-        } catch (Throwable e) {
-            throw new BlobStoreException("failed to check if blob exists", e);
-        }
-    }
-
-    @Override
-    public void deleteBlob(String blobName) throws IOException {
-        try {
-            blobStore.client().deleteObject(blobStore.bucket(), buildKey(blobName));
-        } catch (AmazonClientException e) {
-            throw new IOException("Exception when deleting blob [" + blobName + "]", e);
-        }
-    }
-
-    @Override
-    public InputStream openInput(String blobName) throws IOException {
-        int retry = 0;
-        while (retry <= blobStore.numberOfRetries()) {
-            try {
-                S3Object s3Object = blobStore.client().getObject(blobStore.bucket(), buildKey(blobName));
-                return s3Object.getObjectContent();
-            } catch (AmazonClientException e) {
-                if (blobStore.shouldRetry(e) && (retry < blobStore.numberOfRetries())) {
-                    retry++;
-                } else {
-                    if (e instanceof AmazonS3Exception) {
-                        if (404 == ((AmazonS3Exception) e).getStatusCode()) {
-                            throw new FileNotFoundException("Blob object [" + blobName + "] not found: " + e.getMessage());
-                        }
-                    }
-                    throw e;
-                }
-            }
-        }
-        throw new BlobStoreException("retries exhausted while attempting to access blob object [name:" + blobName + ", bucket:" + blobStore.bucket() +"]");
-    }
-
-    @Override
-    public OutputStream createOutput(final String blobName) throws IOException {
-        // UploadS3OutputStream does buffering & retry logic internally
-        return new DefaultS3OutputStream(blobStore, blobStore.bucket(), buildKey(blobName), blobStore.bufferSizeInBytes(), blobStore.numberOfRetries(), blobStore.serverSideEncryption());
-    }
-
-    @Override
-    public Map<String, BlobMetaData> listBlobsByPrefix(@Nullable String blobNamePrefix) throws IOException {
-        MapBuilder<String, BlobMetaData> blobsBuilder = MapBuilder.newMapBuilder();
-        ObjectListing prevListing = null;
-        while (true) {
-            ObjectListing list;
-            if (prevListing != null) {
-                list = blobStore.client().listNextBatchOfObjects(prevListing);
-            } else {
-                if (blobNamePrefix != null) {
-                    list = blobStore.client().listObjects(blobStore.bucket(), buildKey(blobNamePrefix));
-                } else {
-                    list = blobStore.client().listObjects(blobStore.bucket(), keyPath);
-                }
-            }
-            for (S3ObjectSummary summary : list.getObjectSummaries()) {
-                String name = summary.getKey().substring(keyPath.length());
-                blobsBuilder.put(name, new PlainBlobMetaData(name, summary.getSize()));
-            }
-            if (list.isTruncated()) {
-                prevListing = list;
-            } else {
-                break;
-            }
-        }
-        return blobsBuilder.immutableMap();
-    }
-
-    @Override
-    public void move(String sourceBlobName, String targetBlobName) throws IOException {
-        try {
-            CopyObjectRequest request = new CopyObjectRequest(blobStore.bucket(), buildKey(sourceBlobName),
-                    blobStore.bucket(), buildKey(targetBlobName));
-
-            if (blobStore.serverSideEncryption()) {
-                ObjectMetadata objectMetadata = new ObjectMetadata();
-                objectMetadata.setSSEAlgorithm(ObjectMetadata.AES_256_SERVER_SIDE_ENCRYPTION);
-                request.setNewObjectMetadata(objectMetadata);
-            }
-            blobStore.client().copyObject(request);
-            blobStore.client().deleteObject(blobStore.bucket(), buildKey(sourceBlobName));
-        } catch (AmazonS3Exception e){
-            throw new IOException(e);
-        }
-    }
-
-    @Override
-    public Map<String, BlobMetaData> listBlobs() throws IOException {
-        return listBlobsByPrefix(null);
-    }
-
-    protected String buildKey(String blobName) {
-        return keyPath + blobName;
-    }
-
-}
diff --git a/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobStore.java b/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobStore.java
deleted file mode 100644
index 91f06b6..0000000
--- a/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobStore.java
+++ /dev/null
@@ -1,185 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.aws.blobstore;
-
-import com.amazonaws.AmazonClientException;
-import com.amazonaws.services.s3.AmazonS3;
-import com.amazonaws.services.s3.model.AmazonS3Exception;
-import com.amazonaws.services.s3.model.DeleteObjectsRequest;
-import com.amazonaws.services.s3.model.DeleteObjectsRequest.KeyVersion;
-import com.amazonaws.services.s3.model.ObjectListing;
-import com.amazonaws.services.s3.model.S3ObjectSummary;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.blobstore.BlobContainer;
-import org.elasticsearch.common.blobstore.BlobPath;
-import org.elasticsearch.common.blobstore.BlobStore;
-import org.elasticsearch.common.blobstore.BlobStoreException;
-import org.elasticsearch.common.component.AbstractComponent;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.ByteSizeUnit;
-import org.elasticsearch.common.unit.ByteSizeValue;
-
-import java.util.ArrayList;
-
-/**
- *
- */
-public class S3BlobStore extends AbstractComponent implements BlobStore {
-
-    public static final ByteSizeValue MIN_BUFFER_SIZE = new ByteSizeValue(5, ByteSizeUnit.MB);
-
-    private final AmazonS3 client;
-
-    private final String bucket;
-
-    private final String region;
-
-    private final ByteSizeValue bufferSize;
-
-    private final boolean serverSideEncryption;
-
-    private final int numberOfRetries;
-
-    public S3BlobStore(Settings settings, AmazonS3 client, String bucket, @Nullable String region, boolean serverSideEncryption,
-                       ByteSizeValue bufferSize, int maxRetries) {
-        super(settings);
-        this.client = client;
-        this.bucket = bucket;
-        this.region = region;
-        this.serverSideEncryption = serverSideEncryption;
-
-        this.bufferSize = (bufferSize != null) ? bufferSize : MIN_BUFFER_SIZE;
-        if (this.bufferSize.getBytes() < MIN_BUFFER_SIZE.getBytes()) {
-            throw new BlobStoreException("Detected a buffer_size for the S3 storage lower than [" + MIN_BUFFER_SIZE + "]");
-        }
-
-        this.numberOfRetries = maxRetries;
-
-        // Note: the method client.doesBucketExist() may return 'true' is the bucket exists
-        // but we don't have access to it (ie, 403 Forbidden response code)
-        // Also, if invalid security credentials are used to execute this method, the
-        // client is not able to distinguish between bucket permission errors and
-        // invalid credential errors, and this method could return an incorrect result.
-        int retry = 0;
-        while (retry <= maxRetries) {
-            try {
-                if (!client.doesBucketExist(bucket)) {
-                    if (region != null) {
-                        client.createBucket(bucket, region);
-                    } else {
-                        client.createBucket(bucket);
-                    }
-                }
-                break;
-            } catch (AmazonClientException e) {
-                if (shouldRetry(e) && retry < maxRetries) {
-                    retry++;
-                } else {
-                    logger.debug("S3 client create bucket failed");
-                    throw e;
-                }
-            }
-        }
-    }
-
-    @Override
-    public String toString() {
-        return (region == null ? "" : region + "/") + bucket;
-    }
-
-    public AmazonS3 client() {
-        return client;
-    }
-
-    public String bucket() {
-        return bucket;
-    }
-
-    public boolean serverSideEncryption() { return serverSideEncryption; }
-
-    public int bufferSizeInBytes() {
-        return bufferSize.bytesAsInt();
-    }
-
-    public int numberOfRetries() {
-        return numberOfRetries;
-    }
-
-    @Override
-    public BlobContainer blobContainer(BlobPath path) {
-        return new S3BlobContainer(path, this);
-    }
-
-    @Override
-    public void delete(BlobPath path) {
-        ObjectListing prevListing = null;
-        //From http://docs.amazonwebservices.com/AmazonS3/latest/dev/DeletingMultipleObjectsUsingJava.html
-        //we can do at most 1K objects per delete
-        //We don't know the bucket name until first object listing
-        DeleteObjectsRequest multiObjectDeleteRequest = null;
-        ArrayList<KeyVersion> keys = new ArrayList<KeyVersion>();
-        while (true) {
-            ObjectListing list;
-            if (prevListing != null) {
-                list = client.listNextBatchOfObjects(prevListing);
-            } else {
-                String keyPath = path.buildAsString("/");
-                if (!keyPath.isEmpty()) {
-                    keyPath = keyPath + "/";
-                }
-                list = client.listObjects(bucket, keyPath);
-                multiObjectDeleteRequest = new DeleteObjectsRequest(list.getBucketName());
-            }
-            for (S3ObjectSummary summary : list.getObjectSummaries()) {
-                keys.add(new KeyVersion(summary.getKey()));
-                //Every 500 objects batch the delete request
-                if (keys.size() > 500) {
-                    multiObjectDeleteRequest.setKeys(keys);
-                    client.deleteObjects(multiObjectDeleteRequest);
-                    multiObjectDeleteRequest = new DeleteObjectsRequest(list.getBucketName());
-                    keys.clear();
-                }
-            }
-            if (list.isTruncated()) {
-                prevListing = list;
-            } else {
-                break;
-            }
-        }
-        if (!keys.isEmpty()) {
-            multiObjectDeleteRequest.setKeys(keys);
-            client.deleteObjects(multiObjectDeleteRequest);
-        }
-    }
-
-    protected boolean shouldRetry(AmazonClientException e) {
-        if (e instanceof AmazonS3Exception) {
-            AmazonS3Exception s3e = (AmazonS3Exception)e;
-            if (s3e.getStatusCode() == 400 && "RequestTimeout".equals(s3e.getErrorCode())) {
-                return true;
-            }
-        }
-        return e.isRetryable();
-    }
-
-    @Override
-    public void close() {
-    }
-}
diff --git a/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3OutputStream.java b/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3OutputStream.java
deleted file mode 100644
index a1b66ad..0000000
--- a/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3OutputStream.java
+++ /dev/null
@@ -1,125 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.aws.blobstore;
-
-import org.elasticsearch.common.unit.ByteSizeUnit;
-import org.elasticsearch.common.unit.ByteSizeValue;
-
-import java.io.IOException;
-import java.io.OutputStream;
-
-/**
- * S3OutputStream buffers data before flushing it to an underlying S3OutputStream.
- */
-public abstract class S3OutputStream extends OutputStream {
-
-    /**
-     * Limit of upload allowed by AWS S3.
-     */
-    protected static final ByteSizeValue MULTIPART_MAX_SIZE = new ByteSizeValue(5, ByteSizeUnit.GB);
-    protected static final ByteSizeValue MULTIPART_MIN_SIZE = new ByteSizeValue(5, ByteSizeUnit.MB);
-
-    private S3BlobStore blobStore;
-    private String bucketName;
-    private String blobName;
-    private int numberOfRetries;
-    private boolean serverSideEncryption;
-
-    private byte[] buffer;
-    private int count;
-    private long length;
-
-    private int flushCount = 0;
-
-    public S3OutputStream(S3BlobStore blobStore, String bucketName, String blobName, int bufferSizeInBytes, int numberOfRetries, boolean serverSideEncryption) {
-        this.blobStore = blobStore;
-        this.bucketName = bucketName;
-        this.blobName = blobName;
-        this.numberOfRetries = numberOfRetries;
-        this.serverSideEncryption = serverSideEncryption;
-
-        if (bufferSizeInBytes < MULTIPART_MIN_SIZE.getBytes()) {
-            throw new IllegalArgumentException("Buffer size can't be smaller than " + MULTIPART_MIN_SIZE);
-        }
-        if (bufferSizeInBytes > MULTIPART_MAX_SIZE.getBytes()) {
-            throw new IllegalArgumentException("Buffer size can't be larger than " + MULTIPART_MAX_SIZE);
-        }
-
-        this.buffer = new byte[bufferSizeInBytes];
-    }
-
-    public abstract void flush(byte[] bytes, int off, int len, boolean closing) throws IOException;
-
-    private void flushBuffer(boolean closing) throws IOException {
-        flush(buffer, 0, count, closing);
-        flushCount++;
-        count = 0;
-    }
-
-    @Override
-    public void write(int b) throws IOException {
-        if (count >= buffer.length) {
-            flushBuffer(false);
-        }
-
-        buffer[count++] = (byte) b;
-        length++;
-    }
-
-    @Override
-    public void close() throws IOException {
-        if (count > 0) {
-            flushBuffer(true);
-            count = 0;
-        }
-    }
-
-    public S3BlobStore getBlobStore() {
-        return blobStore;
-    }
-
-    public String getBucketName() {
-        return bucketName;
-    }
-
-    public String getBlobName() {
-        return blobName;
-    }
-
-    public int getBufferSize() {
-        return buffer.length;
-    }
-
-    public int getNumberOfRetries() {
-        return numberOfRetries;
-    }
-
-    public boolean isServerSideEncryption() {
-        return serverSideEncryption;
-    }
-
-    public long getLength() {
-        return length;
-    }
-
-    public int getFlushCount() {
-        return flushCount;
-    }
-}
diff --git a/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/network/Ec2NameResolver.java b/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/network/Ec2NameResolver.java
deleted file mode 100755
index 337a97e..0000000
--- a/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/network/Ec2NameResolver.java
+++ /dev/null
@@ -1,142 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.aws.network;
-
-import org.apache.lucene.util.IOUtils;
-import org.elasticsearch.ExceptionsHelper;
-import org.elasticsearch.cloud.aws.AwsEc2Service;
-import org.elasticsearch.common.component.AbstractComponent;
-import org.elasticsearch.common.network.NetworkService.CustomNameResolver;
-import org.elasticsearch.common.settings.Settings;
-
-import java.io.BufferedReader;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.InputStreamReader;
-import java.net.InetAddress;
-import java.net.URL;
-import java.net.URLConnection;
-import java.nio.charset.StandardCharsets;
-
-/**
- * Resolves certain ec2 related 'meta' hostnames into an actual hostname
- * obtained from ec2 meta-data.
- * <p/>
- * Valid config values for {@link Ec2HostnameType}s are -
- * <ul>
- * <li>_ec2_ - maps to privateIpv4</li>
- * <li>_ec2:privateIp_ - maps to privateIpv4</li>
- * <li>_ec2:privateIpv4_</li>
- * <li>_ec2:privateDns_</li>
- * <li>_ec2:publicIp_ - maps to publicIpv4</li>
- * <li>_ec2:publicIpv4_</li>
- * <li>_ec2:publicDns_</li>
- * </ul>
- *
- * @author Paul_Loy (keteracel)
- */
-public class Ec2NameResolver extends AbstractComponent implements CustomNameResolver {
-
-    /**
-     * enum that can be added to over time with more meta-data types (such as ipv6 when this is available)
-     *
-     * @author Paul_Loy
-     */
-    private static enum Ec2HostnameType {
-
-        PRIVATE_IPv4("ec2:privateIpv4", "local-ipv4"),
-        PRIVATE_DNS("ec2:privateDns", "local-hostname"),
-        PUBLIC_IPv4("ec2:publicIpv4", "public-ipv4"),
-        PUBLIC_DNS("ec2:publicDns", "public-hostname"),
-
-        // some less verbose defaults
-        PUBLIC_IP("ec2:publicIp", PUBLIC_IPv4.ec2Name),
-        PRIVATE_IP("ec2:privateIp", PRIVATE_IPv4.ec2Name),
-        EC2("ec2", PRIVATE_IPv4.ec2Name);
-
-        final String configName;
-        final String ec2Name;
-
-        private Ec2HostnameType(String configName, String ec2Name) {
-            this.configName = configName;
-            this.ec2Name = ec2Name;
-        }
-    }
-
-    /**
-     * Construct a {@link CustomNameResolver}.
-     */
-    public Ec2NameResolver(Settings settings) {
-        super(settings);
-    }
-
-    /**
-     * @param type the ec2 hostname type to discover.
-     * @return the appropriate host resolved from ec2 meta-data.
-     * @throws IOException if ec2 meta-data cannot be obtained.
-     * @see CustomNameResolver#resolveIfPossible(String)
-     */
-    public InetAddress[] resolve(Ec2HostnameType type, boolean warnOnFailure) {
-        URLConnection urlConnection = null;
-        InputStream in = null;
-        try {
-            URL url = new URL(AwsEc2Service.EC2_METADATA_URL + type.ec2Name);
-            logger.debug("obtaining ec2 hostname from ec2 meta-data url {}", url);
-            urlConnection = url.openConnection();
-            urlConnection.setConnectTimeout(2000);
-            in = urlConnection.getInputStream();
-            BufferedReader urlReader = new BufferedReader(new InputStreamReader(in, StandardCharsets.UTF_8));
-
-            String metadataResult = urlReader.readLine();
-            if (metadataResult == null || metadataResult.length() == 0) {
-                logger.error("no ec2 metadata returned from {}", url);
-                return null;
-            }
-            // only one address: because we explicitly ask for only one via the Ec2HostnameType
-            return new InetAddress[] { InetAddress.getByName(metadataResult) };
-        } catch (IOException e) {
-            if (warnOnFailure) {
-                logger.warn("failed to get metadata for [" + type.configName + "]: " + ExceptionsHelper.detailedMessage(e));
-            } else {
-                logger.debug("failed to get metadata for [" + type.configName + "]: " + ExceptionsHelper.detailedMessage(e));
-            }
-            return null;
-        } finally {
-            IOUtils.closeWhileHandlingException(in);
-        }
-    }
-
-    @Override
-    public InetAddress[] resolveDefault() {
-        return null; // using this, one has to explicitly specify _ec2_ in network setting
-//        return resolve(Ec2HostnameType.DEFAULT, false);
-    }
-
-    @Override
-    public InetAddress[] resolveIfPossible(String value) {
-        for (Ec2HostnameType type : Ec2HostnameType.values()) {
-            if (type.configName.equals(value)) {
-                return resolve(type, true);
-            }
-        }
-        return null;
-    }
-
-}
diff --git a/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/node/Ec2CustomNodeAttributes.java b/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/node/Ec2CustomNodeAttributes.java
deleted file mode 100644
index f8f66d0..0000000
--- a/plugins/cloud-aws/src/main/java/org/elasticsearch/cloud/aws/node/Ec2CustomNodeAttributes.java
+++ /dev/null
@@ -1,78 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.aws.node;
-
-import org.apache.lucene.util.IOUtils;
-import org.elasticsearch.ExceptionsHelper;
-import org.elasticsearch.cloud.aws.AwsEc2Service;
-import org.elasticsearch.cluster.node.DiscoveryNodeService;
-import org.elasticsearch.common.component.AbstractComponent;
-import org.elasticsearch.common.settings.Settings;
-
-import java.io.BufferedReader;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.InputStreamReader;
-import java.net.URL;
-import java.net.URLConnection;
-import java.nio.charset.StandardCharsets;
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- */
-public class Ec2CustomNodeAttributes extends AbstractComponent implements DiscoveryNodeService.CustomAttributesProvider {
-
-    public Ec2CustomNodeAttributes(Settings settings) {
-        super(settings);
-    }
-
-    @Override
-    public Map<String, String> buildAttributes() {
-        if (!settings.getAsBoolean("cloud.node.auto_attributes", false)) {
-            return null;
-        }
-        Map<String, String> ec2Attributes = new HashMap<>();
-
-        URLConnection urlConnection;
-        InputStream in = null;
-        try {
-            URL url = new URL(AwsEc2Service.EC2_METADATA_URL + "placement/availability-zone");
-            logger.debug("obtaining ec2 [placement/availability-zone] from ec2 meta-data url {}", url);
-            urlConnection = url.openConnection();
-            urlConnection.setConnectTimeout(2000);
-            in = urlConnection.getInputStream();
-            BufferedReader urlReader = new BufferedReader(new InputStreamReader(in, StandardCharsets.UTF_8));
-
-            String metadataResult = urlReader.readLine();
-            if (metadataResult == null || metadataResult.length() == 0) {
-                logger.error("no ec2 metadata returned from {}", url);
-                return null;
-            }
-            ec2Attributes.put("aws_availability_zone", metadataResult);
-        } catch (IOException e) {
-            logger.debug("failed to get metadata for [placement/availability-zone]: " + ExceptionsHelper.detailedMessage(e));
-        } finally {
-            IOUtils.closeWhileHandlingException(in);
-        }
-
-        return ec2Attributes;
-    }
-}
diff --git a/plugins/cloud-aws/src/main/java/org/elasticsearch/discovery/ec2/AwsEc2UnicastHostsProvider.java b/plugins/cloud-aws/src/main/java/org/elasticsearch/discovery/ec2/AwsEc2UnicastHostsProvider.java
deleted file mode 100644
index ab11541..0000000
--- a/plugins/cloud-aws/src/main/java/org/elasticsearch/discovery/ec2/AwsEc2UnicastHostsProvider.java
+++ /dev/null
@@ -1,201 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.ec2;
-
-import com.amazonaws.AmazonClientException;
-import com.amazonaws.services.ec2.AmazonEC2;
-import com.amazonaws.services.ec2.model.*;
-import org.elasticsearch.Version;
-import org.elasticsearch.cloud.aws.AwsEc2Service;
-import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.component.AbstractComponent;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.transport.TransportAddress;
-import org.elasticsearch.discovery.zen.ping.unicast.UnicastHostsProvider;
-import org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing;
-import org.elasticsearch.transport.TransportService;
-
-import java.util.*;
-
-/**
- *
- */
-public class AwsEc2UnicastHostsProvider extends AbstractComponent implements UnicastHostsProvider {
-
-    private static enum HostType {
-        PRIVATE_IP,
-        PUBLIC_IP,
-        PRIVATE_DNS,
-        PUBLIC_DNS
-    }
-
-    private final TransportService transportService;
-
-    private final AmazonEC2 client;
-
-    private final Version version;
-
-    private final boolean bindAnyGroup;
-
-    private final Set<String> groups;
-
-    private final Map<String, String> tags;
-
-    private final Set<String> availabilityZones;
-
-    private final HostType hostType;
-
-    @Inject
-    public AwsEc2UnicastHostsProvider(Settings settings, TransportService transportService, AwsEc2Service awsEc2Service, Version version) {
-        super(settings);
-        this.transportService = transportService;
-        this.client = awsEc2Service.client();
-        this.version = version;
-
-        this.hostType = HostType.valueOf(settings.get("discovery.ec2.host_type", "private_ip").toUpperCase(Locale.ROOT));
-
-        this.bindAnyGroup = settings.getAsBoolean("discovery.ec2.any_group", true);
-        this.groups = new HashSet<>();
-        groups.addAll(Arrays.asList(settings.getAsArray("discovery.ec2.groups")));
-
-        this.tags = settings.getByPrefix("discovery.ec2.tag.").getAsMap();
-
-        Set<String> availabilityZones = new HashSet();
-        availabilityZones.addAll(Arrays.asList(settings.getAsArray("discovery.ec2.availability_zones")));
-        if (settings.get("discovery.ec2.availability_zones") != null) {
-            availabilityZones.addAll(Strings.commaDelimitedListToSet(settings.get("discovery.ec2.availability_zones")));
-        }
-        this.availabilityZones = availabilityZones;
-
-        if (logger.isDebugEnabled()) {
-            logger.debug("using host_type [{}], tags [{}], groups [{}] with any_group [{}], availability_zones [{}]", hostType, tags, groups, bindAnyGroup, availabilityZones);
-        }
-    }
-
-    @Override
-    public List<DiscoveryNode> buildDynamicNodes() {
-        List<DiscoveryNode> discoNodes = new ArrayList<>();
-
-        DescribeInstancesResult descInstances;
-        try {
-            // Query EC2 API based on AZ, instance state, and tag.
-
-            // NOTE: we don't filter by security group during the describe instances request for two reasons:
-            // 1. differences in VPCs require different parameters during query (ID vs Name)
-            // 2. We want to use two different strategies: (all security groups vs. any security groups)
-            descInstances = client.describeInstances(buildDescribeInstancesRequest());
-        } catch (AmazonClientException e) {
-            logger.info("Exception while retrieving instance list from AWS API: {}", e.getMessage());
-            logger.debug("Full exception:", e);
-            return discoNodes;
-        }
-
-        logger.trace("building dynamic unicast discovery nodes...");
-        for (Reservation reservation : descInstances.getReservations()) {
-            for (Instance instance : reservation.getInstances()) {
-                // lets see if we can filter based on groups
-                if (!groups.isEmpty()) {
-                    List<GroupIdentifier> instanceSecurityGroups = instance.getSecurityGroups();
-                    ArrayList<String> securityGroupNames = new ArrayList<String>();
-                    ArrayList<String> securityGroupIds = new ArrayList<String>();
-                    for (GroupIdentifier sg : instanceSecurityGroups) {
-                        securityGroupNames.add(sg.getGroupName());
-                        securityGroupIds.add(sg.getGroupId());
-                    }
-                    if (bindAnyGroup) {
-                        // We check if we can find at least one group name or one group id in groups.
-                        if (Collections.disjoint(securityGroupNames, groups)
-                                && Collections.disjoint(securityGroupIds, groups)) {
-                            logger.trace("filtering out instance {} based on groups {}, not part of {}", instance.getInstanceId(), instanceSecurityGroups, groups);
-                            // continue to the next instance
-                            continue;
-                        }
-                    } else {
-                        // We need tp match all group names or group ids, otherwise we ignore this instance
-                        if (!(securityGroupNames.containsAll(groups) || securityGroupIds.containsAll(groups))) {
-                            logger.trace("filtering out instance {} based on groups {}, does not include all of {}", instance.getInstanceId(), instanceSecurityGroups, groups);
-                            // continue to the next instance
-                            continue;
-                        }
-                    }
-                }
-
-                String address = null;
-                switch (hostType) {
-                    case PRIVATE_DNS:
-                        address = instance.getPrivateDnsName();
-                        break;
-                    case PRIVATE_IP:
-                        address = instance.getPrivateIpAddress();
-                        break;
-                    case PUBLIC_DNS:
-                        address = instance.getPublicDnsName();
-                        break;
-                    case PUBLIC_IP:
-                        address = instance.getPublicIpAddress();
-                        break;
-                }
-                if (address != null) {
-                    try {
-                        // we only limit to 1 port per address, makes no sense to ping 100 ports
-                        TransportAddress[] addresses = transportService.addressesFromString(address, 1);
-                        for (int i = 0; i < addresses.length; i++) {
-                            logger.trace("adding {}, address {}, transport_address {}", instance.getInstanceId(), address, addresses[i]);
-                            discoNodes.add(new DiscoveryNode("#cloud-" + instance.getInstanceId() + "-" + i, addresses[i], version.minimumCompatibilityVersion()));
-                        }
-                    } catch (Exception e) {
-                        logger.warn("failed ot add {}, address {}", e, instance.getInstanceId(), address);
-                    }
-                } else {
-                    logger.trace("not adding {}, address is null, host_type {}", instance.getInstanceId(), hostType);
-                }
-            }
-        }
-
-        logger.debug("using dynamic discovery nodes {}", discoNodes);
-
-        return discoNodes;
-    }
-
-    private DescribeInstancesRequest buildDescribeInstancesRequest() {
-        DescribeInstancesRequest describeInstancesRequest = new DescribeInstancesRequest()
-            .withFilters(
-                new Filter("instance-state-name").withValues("running", "pending")
-            );
-
-        for (Map.Entry<String, String> tagFilter : tags.entrySet()) {
-            // for a given tag key, OR relationship for multiple different values
-            describeInstancesRequest.withFilters(
-                new Filter("tag:" + tagFilter.getKey()).withValues(tagFilter.getValue())
-            );
-        }
-
-        if (!availabilityZones.isEmpty()) {
-            // OR relationship amongst multiple values of the availability-zone filter
-            describeInstancesRequest.withFilters(
-                new Filter("availability-zone").withValues(availabilityZones)
-            );
-        }
-
-        return describeInstancesRequest;
-    }
-}
diff --git a/plugins/cloud-aws/src/main/java/org/elasticsearch/discovery/ec2/Ec2Discovery.java b/plugins/cloud-aws/src/main/java/org/elasticsearch/discovery/ec2/Ec2Discovery.java
deleted file mode 100755
index b599541..0000000
--- a/plugins/cloud-aws/src/main/java/org/elasticsearch/discovery/ec2/Ec2Discovery.java
+++ /dev/null
@@ -1,49 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.ec2;
-
-import org.elasticsearch.cluster.ClusterName;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.settings.ClusterDynamicSettings;
-import org.elasticsearch.cluster.settings.DynamicSettings;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.discovery.DiscoverySettings;
-import org.elasticsearch.discovery.zen.ZenDiscovery;
-import org.elasticsearch.discovery.zen.elect.ElectMasterService;
-import org.elasticsearch.discovery.zen.ping.ZenPingService;
-import org.elasticsearch.node.settings.NodeSettingsService;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportService;
-
-/**
- *
- */
-public class Ec2Discovery extends ZenDiscovery {
-
-    @Inject
-    public Ec2Discovery(Settings settings, ClusterName clusterName, ThreadPool threadPool, TransportService transportService,
-                        ClusterService clusterService, NodeSettingsService nodeSettingsService, ZenPingService pingService,
-                        DiscoverySettings discoverySettings,
-                        ElectMasterService electMasterService) {
-        super(settings, clusterName, threadPool, transportService, clusterService, nodeSettingsService,
-                pingService, electMasterService, discoverySettings);
-    }
-}
diff --git a/plugins/cloud-aws/src/main/java/org/elasticsearch/plugin/cloud/aws/CloudAwsPlugin.java b/plugins/cloud-aws/src/main/java/org/elasticsearch/plugin/cloud/aws/CloudAwsPlugin.java
deleted file mode 100644
index d5d7f6b..0000000
--- a/plugins/cloud-aws/src/main/java/org/elasticsearch/plugin/cloud/aws/CloudAwsPlugin.java
+++ /dev/null
@@ -1,86 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.cloud.aws;
-
-import org.elasticsearch.cloud.aws.AwsEc2Service;
-import org.elasticsearch.cloud.aws.AwsModule;
-import org.elasticsearch.common.component.LifecycleComponent;
-import org.elasticsearch.common.inject.Module;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.discovery.DiscoveryModule;
-import org.elasticsearch.discovery.ec2.Ec2Discovery;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository;
-import org.elasticsearch.repositories.RepositoriesModule;
-import org.elasticsearch.repositories.s3.S3Repository;
-
-import java.util.ArrayList;
-import java.util.Collection;
-
-/**
- *
- */
-public class CloudAwsPlugin extends Plugin {
-
-    private final Settings settings;
-
-    public CloudAwsPlugin(Settings settings) {
-        this.settings = settings;
-    }
-
-    @Override
-    public String name() {
-        return "cloud-aws";
-    }
-
-    @Override
-    public String description() {
-        return "Cloud AWS Plugin";
-    }
-
-    @Override
-    public Collection<Module> nodeModules() {
-        Collection<Module> modules = new ArrayList<>();
-        if (settings.getAsBoolean("cloud.enabled", true)) {
-            modules.add(new AwsModule());
-        }
-        return modules;
-    }
-
-    @Override
-    public Collection<Class<? extends LifecycleComponent>> nodeServices() {
-        Collection<Class<? extends LifecycleComponent>> services = new ArrayList<>();
-        if (settings.getAsBoolean("cloud.enabled", true)) {
-            services.add(AwsModule.getS3ServiceImpl());
-            services.add(AwsEc2Service.class);
-        }
-        return services;
-    }
-
-    public void onModule(RepositoriesModule repositoriesModule) {
-        if (settings.getAsBoolean("cloud.enabled", true)) {
-            repositoriesModule.registerRepository(S3Repository.TYPE, S3Repository.class, BlobStoreIndexShardRepository.class);
-        }
-    }
-
-    public void onModule(DiscoveryModule discoveryModule) {
-        discoveryModule.addDiscoveryType("ec2", Ec2Discovery.class);
-    }
-}
diff --git a/plugins/cloud-aws/src/main/java/org/elasticsearch/repositories/s3/S3Repository.java b/plugins/cloud-aws/src/main/java/org/elasticsearch/repositories/s3/S3Repository.java
deleted file mode 100644
index 4be35ba..0000000
--- a/plugins/cloud-aws/src/main/java/org/elasticsearch/repositories/s3/S3Repository.java
+++ /dev/null
@@ -1,167 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.repositories.s3;
-
-import org.elasticsearch.cloud.aws.AwsS3Service;
-import org.elasticsearch.cloud.aws.blobstore.S3BlobStore;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.blobstore.BlobPath;
-import org.elasticsearch.common.blobstore.BlobStore;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.unit.ByteSizeUnit;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.index.snapshots.IndexShardRepository;
-import org.elasticsearch.repositories.RepositoryException;
-import org.elasticsearch.repositories.RepositoryName;
-import org.elasticsearch.repositories.RepositorySettings;
-import org.elasticsearch.repositories.blobstore.BlobStoreRepository;
-
-import java.io.IOException;
-import java.util.Locale;
-
-/**
- * Shared file system implementation of the BlobStoreRepository
- * <p/>
- * Shared file system repository supports the following settings
- * <dl>
- * <dt>{@code bucket}</dt><dd>S3 bucket</dd>
- * <dt>{@code region}</dt><dd>S3 region. Defaults to us-east</dd>
- * <dt>{@code base_path}</dt><dd>Specifies the path within bucket to repository data. Defaults to root directory.</dd>
- * <dt>{@code concurrent_streams}</dt><dd>Number of concurrent read/write stream (per repository on each node). Defaults to 5.</dd>
- * <dt>{@code chunk_size}</dt><dd>Large file can be divided into chunks. This parameter specifies the chunk size. Defaults to not chucked.</dd>
- * <dt>{@code compress}</dt><dd>If set to true metadata files will be stored compressed. Defaults to false.</dd>
- * </dl>
- */
-public class S3Repository extends BlobStoreRepository {
-
-    public final static String TYPE = "s3";
-
-    private final S3BlobStore blobStore;
-
-    private final BlobPath basePath;
-
-    private ByteSizeValue chunkSize;
-
-    private boolean compress;
-
-    /**
-     * Constructs new shared file system repository
-     *
-     * @param name                 repository name
-     * @param repositorySettings   repository settings
-     * @param indexShardRepository index shard repository
-     * @param s3Service            S3 service
-     * @throws IOException
-     */
-    @Inject
-    public S3Repository(RepositoryName name, RepositorySettings repositorySettings, IndexShardRepository indexShardRepository, AwsS3Service s3Service) throws IOException {
-        super(name.getName(), repositorySettings, indexShardRepository);
-
-        String bucket = repositorySettings.settings().get("bucket", settings.get("repositories.s3.bucket"));
-        if (bucket == null) {
-            throw new RepositoryException(name.name(), "No bucket defined for s3 gateway");
-        }
-
-        String endpoint = repositorySettings.settings().get("endpoint", settings.get("repositories.s3.endpoint"));
-        String protocol = repositorySettings.settings().get("protocol", settings.get("repositories.s3.protocol"));
-
-        String region = repositorySettings.settings().get("region", settings.get("repositories.s3.region"));
-        if (region == null) {
-            // Bucket setting is not set - use global region setting
-            String regionSetting = repositorySettings.settings().get("cloud.aws.region", settings.get("cloud.aws.region"));
-            if (regionSetting != null) {
-                regionSetting = regionSetting.toLowerCase(Locale.ENGLISH);
-                if ("us-east".equals(regionSetting) || "us-east-1".equals(regionSetting)) {
-                    // Default bucket - setting region to null
-                    region = null;
-                } else if ("us-west".equals(regionSetting) || "us-west-1".equals(regionSetting)) {
-                    region = "us-west-1";
-                } else if ("us-west-2".equals(regionSetting)) {
-                    region = "us-west-2";
-                } else if ("ap-southeast".equals(regionSetting) || "ap-southeast-1".equals(regionSetting)) {
-                    region = "ap-southeast-1";
-                } else if ("ap-southeast-2".equals(regionSetting)) {
-                    region = "ap-southeast-2";
-                } else if ("ap-northeast".equals(regionSetting) || "ap-northeast-1".equals(regionSetting)) {
-                    region = "ap-northeast-1";
-                } else if ("eu-west".equals(regionSetting) || "eu-west-1".equals(regionSetting)) {
-                    region = "eu-west-1";
-                } else if ("eu-central".equals(regionSetting) || "eu-central-1".equals(regionSetting)) {
-                    region = "eu-central-1";
-                } else if ("sa-east".equals(regionSetting) || "sa-east-1".equals(regionSetting)) {
-                    region = "sa-east-1";
-                } else if ("cn-north".equals(regionSetting) || "cn-north-1".equals(regionSetting)) {
-                    region = "cn-north-1";
-                }
-            }
-        }
-
-        boolean serverSideEncryption = repositorySettings.settings().getAsBoolean("server_side_encryption", settings.getAsBoolean("repositories.s3.server_side_encryption", false));
-        ByteSizeValue bufferSize = repositorySettings.settings().getAsBytesSize("buffer_size", settings.getAsBytesSize("repositories.s3.buffer_size", null));
-        Integer maxRetries = repositorySettings.settings().getAsInt("max_retries", settings.getAsInt("repositories.s3.max_retries", 3));
-        this.chunkSize = repositorySettings.settings().getAsBytesSize("chunk_size", settings.getAsBytesSize("repositories.s3.chunk_size", new ByteSizeValue(100, ByteSizeUnit.MB)));
-        this.compress = repositorySettings.settings().getAsBoolean("compress", settings.getAsBoolean("repositories.s3.compress", false));
-
-        logger.debug("using bucket [{}], region [{}], endpoint [{}], protocol [{}], chunk_size [{}], server_side_encryption [{}], buffer_size [{}], max_retries [{}]",
-                bucket, region, endpoint, protocol, chunkSize, serverSideEncryption, bufferSize, maxRetries);
-
-        blobStore = new S3BlobStore(settings, s3Service.client(endpoint, protocol, region, repositorySettings.settings().get("access_key"), repositorySettings.settings().get("secret_key"), maxRetries), bucket, region, serverSideEncryption, bufferSize, maxRetries);
-        String basePath = repositorySettings.settings().get("base_path", settings.get("repositories.s3.base_path"));
-        if (Strings.hasLength(basePath)) {
-            BlobPath path = new BlobPath();
-            for(String elem : Strings.splitStringToArray(basePath, '/')) {
-                path = path.add(elem);
-            }
-            this.basePath = path;
-        } else {
-            this.basePath = BlobPath.cleanPath();
-        }
-    }
-
-    /**
-     * {@inheritDoc}
-     */
-    @Override
-    protected BlobStore blobStore() {
-        return blobStore;
-    }
-
-    @Override
-    protected BlobPath basePath() {
-        return basePath;
-    }
-
-    /**
-     * {@inheritDoc}
-     */
-    @Override
-    protected boolean isCompress() {
-        return compress;
-    }
-
-    /**
-     * {@inheritDoc}
-     */
-    @Override
-    protected ByteSizeValue chunkSize() {
-        return chunkSize;
-    }
-
-}
diff --git a/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/AWSSignersTest.java b/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/AWSSignersTest.java
deleted file mode 100644
index f0a10c0..0000000
--- a/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/AWSSignersTest.java
+++ /dev/null
@@ -1,53 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.aws;
-
-import com.amazonaws.ClientConfiguration;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import static org.hamcrest.CoreMatchers.is;
-
-public class AWSSignersTest extends ESTestCase {
-
-    @Test
-    public void testSigners() {
-        assertThat(signerTester(null), is(false));
-        assertThat(signerTester("QueryStringSignerType"), is(true));
-        assertThat(signerTester("AWS3SignerType"), is(true));
-        assertThat(signerTester("AWS4SignerType"), is(true));
-        assertThat(signerTester("NoOpSignerType"), is(true));
-        assertThat(signerTester("UndefinedSigner"), is(false));
-    }
-
-    /**
-     * Test a signer configuration
-     * @param signer signer name
-     * @return true if successful, false otherwise
-     */
-    private boolean signerTester(String signer) {
-        try {
-            AwsSigner.configureSigner(signer, new ClientConfiguration());
-            return true;
-        } catch (IllegalArgumentException e) {
-            return false;
-        }
-    }
-}
diff --git a/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTest.java b/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTest.java
deleted file mode 100644
index b66cd94..0000000
--- a/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTest.java
+++ /dev/null
@@ -1,101 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.aws;
-
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.PathUtils;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsException;
-import org.elasticsearch.plugin.cloud.aws.CloudAwsPlugin;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.elasticsearch.test.ESIntegTestCase.ThirdParty;
-import org.junit.After;
-import org.junit.Before;
-
-import java.util.Collection;
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- * Base class for AWS tests that require credentials.
- * <p>
- * You must specify {@code -Dtests.thirdparty=true -Dtests.config=/path/to/config}
- * in order to run these tests.
- */
-@ThirdParty
-public abstract class AbstractAwsTest extends ESIntegTestCase {
-
-    /**
-     * Those properties are set by the AWS SDK v1.9.4 and if not ignored,
-     * lead to tests failure (see AbstractRandomizedTest#IGNORED_INVARIANT_PROPERTIES)
-     */
-    private static final String[] AWS_INVARIANT_PROPERTIES = {
-            "com.sun.org.apache.xml.internal.dtm.DTMManager",
-            "javax.xml.parsers.DocumentBuilderFactory"
-    };
-
-    private Map<String, String> properties = new HashMap<>();
-
-    @Before
-    public void saveProperties() {
-        for (String p : AWS_INVARIANT_PROPERTIES) {
-            properties.put(p, System.getProperty(p));
-        }
-    }
-
-    @After
-    public void restoreProperties() {
-        for (String p : AWS_INVARIANT_PROPERTIES) {
-            if (properties.get(p) != null) {
-                System.setProperty(p, properties.get(p));
-            } else {
-                System.clearProperty(p);
-            }
-        }
-    }
-
-    @Override
-    protected Settings nodeSettings(int nodeOrdinal) {
-                Settings.Builder settings = Settings.builder()
-                .put(super.nodeSettings(nodeOrdinal))
-                .put("path.home", createTempDir())
-                .put("cloud.aws.test.random", randomInt())
-                .put("cloud.aws.test.write_failures", 0.1)
-                .put("cloud.aws.test.read_failures", 0.1);
-
-        // if explicit, just load it and don't load from env
-        try {
-            if (Strings.hasText(System.getProperty("tests.config"))) {
-                settings.loadFromPath(PathUtils.get(System.getProperty("tests.config")));
-            } else {
-                throw new IllegalStateException("to run integration tests, you need to set -Dtest.thirdparty=true and -Dtests.config=/path/to/elasticsearch.yml");
-            }
-        } catch (SettingsException exception) {
-            throw new IllegalStateException("your test configuration file is incorrect: " + System.getProperty("tests.config"), exception);
-        }
-        return settings.build();
-    }
-
-    @Override
-    protected Collection<Class<? extends Plugin>> nodePlugins() {
-        return pluginList(CloudAwsPlugin.class, TestAwsS3Service.TestPlugin.class);
-    }
-}
diff --git a/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/AmazonS3Wrapper.java b/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/AmazonS3Wrapper.java
deleted file mode 100644
index 24c7196..0000000
--- a/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/AmazonS3Wrapper.java
+++ /dev/null
@@ -1,631 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.aws;
-
-import com.amazonaws.AmazonClientException;
-import com.amazonaws.AmazonServiceException;
-import com.amazonaws.AmazonWebServiceRequest;
-import com.amazonaws.HttpMethod;
-import com.amazonaws.regions.Region;
-import com.amazonaws.services.s3.AmazonS3;
-import com.amazonaws.services.s3.S3ClientOptions;
-import com.amazonaws.services.s3.S3ResponseMetadata;
-import com.amazonaws.services.s3.model.*;
-
-import java.io.File;
-import java.io.InputStream;
-import java.net.URL;
-import java.util.Date;
-import java.util.List;
-import org.elasticsearch.common.SuppressForbidden;
-
-/**
- *
- */
-@SuppressForbidden(reason = "implements AWS api that uses java.io.File!")
-public class AmazonS3Wrapper implements AmazonS3 {
-
-    protected AmazonS3 delegate;
-
-    public AmazonS3Wrapper(AmazonS3 delegate) {
-        this.delegate = delegate;
-    }
-
-
-    @Override
-    public void setEndpoint(String endpoint) {
-        delegate.setEndpoint(endpoint);
-    }
-
-    @Override
-    public void setRegion(Region region) throws IllegalArgumentException {
-        delegate.setRegion(region);
-    }
-
-    @Override
-    public void setS3ClientOptions(S3ClientOptions clientOptions) {
-        delegate.setS3ClientOptions(clientOptions);
-    }
-
-    @Override
-    public void changeObjectStorageClass(String bucketName, String key, StorageClass newStorageClass) throws AmazonClientException, AmazonServiceException {
-        delegate.changeObjectStorageClass(bucketName, key, newStorageClass);
-    }
-
-    @Override
-    public void setObjectRedirectLocation(String bucketName, String key, String newRedirectLocation) throws AmazonClientException, AmazonServiceException {
-        delegate.setObjectRedirectLocation(bucketName, key, newRedirectLocation);
-    }
-
-    @Override
-    public ObjectListing listObjects(String bucketName) throws AmazonClientException, AmazonServiceException {
-        return delegate.listObjects(bucketName);
-    }
-
-    @Override
-    public ObjectListing listObjects(String bucketName, String prefix) throws AmazonClientException, AmazonServiceException {
-        return delegate.listObjects(bucketName, prefix);
-    }
-
-    @Override
-    public ObjectListing listObjects(ListObjectsRequest listObjectsRequest) throws AmazonClientException, AmazonServiceException {
-        return delegate.listObjects(listObjectsRequest);
-    }
-
-    @Override
-    public ObjectListing listNextBatchOfObjects(ObjectListing previousObjectListing) throws AmazonClientException, AmazonServiceException {
-        return delegate.listNextBatchOfObjects(previousObjectListing);
-    }
-
-    @Override
-    public VersionListing listVersions(String bucketName, String prefix) throws AmazonClientException, AmazonServiceException {
-        return delegate.listVersions(bucketName, prefix);
-    }
-
-    @Override
-    public VersionListing listNextBatchOfVersions(VersionListing previousVersionListing) throws AmazonClientException, AmazonServiceException {
-        return delegate.listNextBatchOfVersions(previousVersionListing);
-    }
-
-    @Override
-    public VersionListing listVersions(String bucketName, String prefix, String keyMarker, String versionIdMarker, String delimiter, Integer maxResults) throws AmazonClientException, AmazonServiceException {
-        return delegate.listVersions(bucketName, prefix, keyMarker, versionIdMarker, delimiter, maxResults);
-    }
-
-    @Override
-    public VersionListing listVersions(ListVersionsRequest listVersionsRequest) throws AmazonClientException, AmazonServiceException {
-        return delegate.listVersions(listVersionsRequest);
-    }
-
-    @Override
-    public Owner getS3AccountOwner() throws AmazonClientException, AmazonServiceException {
-        return delegate.getS3AccountOwner();
-    }
-
-    @Override
-    public boolean doesBucketExist(String bucketName) throws AmazonClientException, AmazonServiceException {
-        return delegate.doesBucketExist(bucketName);
-    }
-
-    @Override
-    public List<Bucket> listBuckets() throws AmazonClientException, AmazonServiceException {
-        return delegate.listBuckets();
-    }
-
-    @Override
-    public List<Bucket> listBuckets(ListBucketsRequest listBucketsRequest) throws AmazonClientException, AmazonServiceException {
-        return delegate.listBuckets(listBucketsRequest);
-    }
-
-    @Override
-    public String getBucketLocation(String bucketName) throws AmazonClientException, AmazonServiceException {
-        return delegate.getBucketLocation(bucketName);
-    }
-
-    @Override
-    public String getBucketLocation(GetBucketLocationRequest getBucketLocationRequest) throws AmazonClientException, AmazonServiceException {
-        return delegate.getBucketLocation(getBucketLocationRequest);
-    }
-
-    @Override
-    public Bucket createBucket(CreateBucketRequest createBucketRequest) throws AmazonClientException, AmazonServiceException {
-        return delegate.createBucket(createBucketRequest);
-    }
-
-    @Override
-    public Bucket createBucket(String bucketName) throws AmazonClientException, AmazonServiceException {
-        return delegate.createBucket(bucketName);
-    }
-
-    @Override
-    public Bucket createBucket(String bucketName, com.amazonaws.services.s3.model.Region region) throws AmazonClientException, AmazonServiceException {
-        return delegate.createBucket(bucketName, region);
-    }
-
-    @Override
-    public Bucket createBucket(String bucketName, String region) throws AmazonClientException, AmazonServiceException {
-        return delegate.createBucket(bucketName, region);
-    }
-
-    @Override
-    public AccessControlList getObjectAcl(String bucketName, String key) throws AmazonClientException, AmazonServiceException {
-        return delegate.getObjectAcl(bucketName, key);
-    }
-
-    @Override
-    public AccessControlList getObjectAcl(String bucketName, String key, String versionId) throws AmazonClientException, AmazonServiceException {
-        return delegate.getObjectAcl(bucketName, key, versionId);
-    }
-
-    @Override
-    public void setObjectAcl(String bucketName, String key, AccessControlList acl) throws AmazonClientException, AmazonServiceException {
-        delegate.setObjectAcl(bucketName, key, acl);
-    }
-
-    @Override
-    public void setObjectAcl(String bucketName, String key, CannedAccessControlList acl) throws AmazonClientException, AmazonServiceException {
-        delegate.setObjectAcl(bucketName, key, acl);
-    }
-
-    @Override
-    public void setObjectAcl(String bucketName, String key, String versionId, AccessControlList acl) throws AmazonClientException, AmazonServiceException {
-        delegate.setObjectAcl(bucketName, key, versionId, acl);
-    }
-
-    @Override
-    public void setObjectAcl(String bucketName, String key, String versionId, CannedAccessControlList acl) throws AmazonClientException, AmazonServiceException {
-        delegate.setObjectAcl(bucketName, key, versionId, acl);
-    }
-
-    @Override
-    public void setObjectAcl(SetObjectAclRequest setObjectAclRequest) throws AmazonClientException, AmazonServiceException {
-        delegate.setObjectAcl(setObjectAclRequest);
-    }
-
-    @Override
-    public AccessControlList getBucketAcl(String bucketName) throws AmazonClientException, AmazonServiceException {
-        return delegate.getBucketAcl(bucketName);
-    }
-
-    @Override
-    public void setBucketAcl(SetBucketAclRequest setBucketAclRequest) throws AmazonClientException, AmazonServiceException {
-        delegate.setBucketAcl(setBucketAclRequest);
-    }
-
-    @Override
-    public AccessControlList getBucketAcl(GetBucketAclRequest getBucketAclRequest) throws AmazonClientException, AmazonServiceException {
-        return delegate.getBucketAcl(getBucketAclRequest);
-    }
-
-    @Override
-    public void setBucketAcl(String bucketName, AccessControlList acl) throws AmazonClientException, AmazonServiceException {
-        delegate.setBucketAcl(bucketName, acl);
-    }
-
-    @Override
-    public void setBucketAcl(String bucketName, CannedAccessControlList acl) throws AmazonClientException, AmazonServiceException {
-        delegate.setBucketAcl(bucketName, acl);
-    }
-
-    @Override
-    public ObjectMetadata getObjectMetadata(String bucketName, String key) throws AmazonClientException, AmazonServiceException {
-        return delegate.getObjectMetadata(bucketName, key);
-    }
-
-    @Override
-    public ObjectMetadata getObjectMetadata(GetObjectMetadataRequest getObjectMetadataRequest) throws AmazonClientException, AmazonServiceException {
-        return delegate.getObjectMetadata(getObjectMetadataRequest);
-    }
-
-    @Override
-    public S3Object getObject(String bucketName, String key) throws AmazonClientException, AmazonServiceException {
-        return delegate.getObject(bucketName, key);
-    }
-
-    @Override
-    public S3Object getObject(GetObjectRequest getObjectRequest) throws AmazonClientException, AmazonServiceException {
-        return delegate.getObject(getObjectRequest);
-    }
-
-    @Override
-    public ObjectMetadata getObject(GetObjectRequest getObjectRequest, File destinationFile) throws AmazonClientException, AmazonServiceException {
-        return delegate.getObject(getObjectRequest, destinationFile);
-    }
-
-    @Override
-    public void deleteBucket(DeleteBucketRequest deleteBucketRequest) throws AmazonClientException, AmazonServiceException {
-        delegate.deleteBucket(deleteBucketRequest);
-    }
-
-    @Override
-    public void deleteBucket(String bucketName) throws AmazonClientException, AmazonServiceException {
-        delegate.deleteBucket(bucketName);
-    }
-
-    @Override
-    public void setBucketReplicationConfiguration(String bucketName, BucketReplicationConfiguration configuration) throws AmazonServiceException, AmazonClientException {
-        delegate.setBucketReplicationConfiguration(bucketName, configuration);
-    }
-
-    @Override
-    public void setBucketReplicationConfiguration(SetBucketReplicationConfigurationRequest setBucketReplicationConfigurationRequest) throws AmazonServiceException, AmazonClientException {
-        delegate.setBucketReplicationConfiguration(setBucketReplicationConfigurationRequest);
-    }
-
-    @Override
-    public BucketReplicationConfiguration getBucketReplicationConfiguration(String bucketName) throws AmazonServiceException, AmazonClientException {
-        return delegate.getBucketReplicationConfiguration(bucketName);
-    }
-
-    @Override
-    public void deleteBucketReplicationConfiguration(String bucketName) throws AmazonServiceException, AmazonClientException {
-        delegate.deleteBucketReplicationConfiguration(bucketName);
-    }
-
-    @Override
-    public PutObjectResult putObject(PutObjectRequest putObjectRequest) throws AmazonClientException, AmazonServiceException {
-        return delegate.putObject(putObjectRequest);
-    }
-
-    @Override
-    public PutObjectResult putObject(String bucketName, String key, File file) throws AmazonClientException, AmazonServiceException {
-        return delegate.putObject(bucketName, key, file);
-    }
-
-    @Override
-    public PutObjectResult putObject(String bucketName, String key, InputStream input, ObjectMetadata metadata) throws AmazonClientException, AmazonServiceException {
-        return delegate.putObject(bucketName, key, input, metadata);
-    }
-
-    @Override
-    public CopyObjectResult copyObject(String sourceBucketName, String sourceKey, String destinationBucketName, String destinationKey) throws AmazonClientException, AmazonServiceException {
-        return delegate.copyObject(sourceBucketName, sourceKey, destinationBucketName, destinationKey);
-    }
-
-    @Override
-    public CopyObjectResult copyObject(CopyObjectRequest copyObjectRequest) throws AmazonClientException, AmazonServiceException {
-        return delegate.copyObject(copyObjectRequest);
-    }
-
-    @Override
-    public CopyPartResult copyPart(CopyPartRequest copyPartRequest) throws AmazonClientException, AmazonServiceException {
-        return delegate.copyPart(copyPartRequest);
-    }
-
-    @Override
-    public void deleteObject(String bucketName, String key) throws AmazonClientException, AmazonServiceException {
-        delegate.deleteObject(bucketName, key);
-    }
-
-    @Override
-    public void deleteObject(DeleteObjectRequest deleteObjectRequest) throws AmazonClientException, AmazonServiceException {
-        delegate.deleteObject(deleteObjectRequest);
-    }
-
-    @Override
-    public DeleteObjectsResult deleteObjects(DeleteObjectsRequest deleteObjectsRequest) throws AmazonClientException, AmazonServiceException {
-        return delegate.deleteObjects(deleteObjectsRequest);
-    }
-
-    @Override
-    public void deleteVersion(String bucketName, String key, String versionId) throws AmazonClientException, AmazonServiceException {
-        delegate.deleteVersion(bucketName, key, versionId);
-    }
-
-    @Override
-    public void deleteVersion(DeleteVersionRequest deleteVersionRequest) throws AmazonClientException, AmazonServiceException {
-        delegate.deleteVersion(deleteVersionRequest);
-    }
-
-    @Override
-    public BucketLoggingConfiguration getBucketLoggingConfiguration(String bucketName) throws AmazonClientException, AmazonServiceException {
-        return delegate.getBucketLoggingConfiguration(bucketName);
-    }
-
-    @Override
-    public void setBucketLoggingConfiguration(SetBucketLoggingConfigurationRequest setBucketLoggingConfigurationRequest) throws AmazonClientException, AmazonServiceException {
-        delegate.setBucketLoggingConfiguration(setBucketLoggingConfigurationRequest);
-    }
-
-    @Override
-    public BucketVersioningConfiguration getBucketVersioningConfiguration(String bucketName) throws AmazonClientException, AmazonServiceException {
-        return delegate.getBucketVersioningConfiguration(bucketName);
-    }
-
-    @Override
-    public void setBucketVersioningConfiguration(SetBucketVersioningConfigurationRequest setBucketVersioningConfigurationRequest) throws AmazonClientException, AmazonServiceException {
-        delegate.setBucketVersioningConfiguration(setBucketVersioningConfigurationRequest);
-    }
-
-    @Override
-    public BucketLifecycleConfiguration getBucketLifecycleConfiguration(String bucketName) {
-        return delegate.getBucketLifecycleConfiguration(bucketName);
-    }
-
-    @Override
-    public void setBucketLifecycleConfiguration(String bucketName, BucketLifecycleConfiguration bucketLifecycleConfiguration) {
-        delegate.setBucketLifecycleConfiguration(bucketName, bucketLifecycleConfiguration);
-    }
-
-    @Override
-    public void setBucketLifecycleConfiguration(SetBucketLifecycleConfigurationRequest setBucketLifecycleConfigurationRequest) {
-        delegate.setBucketLifecycleConfiguration(setBucketLifecycleConfigurationRequest);
-    }
-
-    @Override
-    public void deleteBucketLifecycleConfiguration(String bucketName) {
-        delegate.deleteBucketLifecycleConfiguration(bucketName);
-    }
-
-    @Override
-    public void deleteBucketLifecycleConfiguration(DeleteBucketLifecycleConfigurationRequest deleteBucketLifecycleConfigurationRequest) {
-        delegate.deleteBucketLifecycleConfiguration(deleteBucketLifecycleConfigurationRequest);
-    }
-
-    @Override
-    public BucketCrossOriginConfiguration getBucketCrossOriginConfiguration(String bucketName) {
-        return delegate.getBucketCrossOriginConfiguration(bucketName);
-    }
-
-    @Override
-    public void setBucketCrossOriginConfiguration(String bucketName, BucketCrossOriginConfiguration bucketCrossOriginConfiguration) {
-        delegate.setBucketCrossOriginConfiguration(bucketName, bucketCrossOriginConfiguration);
-    }
-
-    @Override
-    public void setBucketCrossOriginConfiguration(SetBucketCrossOriginConfigurationRequest setBucketCrossOriginConfigurationRequest) {
-        delegate.setBucketCrossOriginConfiguration(setBucketCrossOriginConfigurationRequest);
-    }
-
-    @Override
-    public void deleteBucketCrossOriginConfiguration(String bucketName) {
-        delegate.deleteBucketCrossOriginConfiguration(bucketName);
-    }
-
-    @Override
-    public void deleteBucketCrossOriginConfiguration(DeleteBucketCrossOriginConfigurationRequest deleteBucketCrossOriginConfigurationRequest) {
-        delegate.deleteBucketCrossOriginConfiguration(deleteBucketCrossOriginConfigurationRequest);
-    }
-
-    @Override
-    public BucketTaggingConfiguration getBucketTaggingConfiguration(String bucketName) {
-        return delegate.getBucketTaggingConfiguration(bucketName);
-    }
-
-    @Override
-    public void setBucketTaggingConfiguration(String bucketName, BucketTaggingConfiguration bucketTaggingConfiguration) {
-        delegate.setBucketTaggingConfiguration(bucketName, bucketTaggingConfiguration);
-    }
-
-    @Override
-    public void setBucketTaggingConfiguration(SetBucketTaggingConfigurationRequest setBucketTaggingConfigurationRequest) {
-        delegate.setBucketTaggingConfiguration(setBucketTaggingConfigurationRequest);
-    }
-
-    @Override
-    public void deleteBucketTaggingConfiguration(String bucketName) {
-        delegate.deleteBucketTaggingConfiguration(bucketName);
-    }
-
-    @Override
-    public void deleteBucketTaggingConfiguration(DeleteBucketTaggingConfigurationRequest deleteBucketTaggingConfigurationRequest) {
-        delegate.deleteBucketTaggingConfiguration(deleteBucketTaggingConfigurationRequest);
-    }
-
-    @Override
-    public BucketNotificationConfiguration getBucketNotificationConfiguration(String bucketName) throws AmazonClientException, AmazonServiceException {
-        return delegate.getBucketNotificationConfiguration(bucketName);
-    }
-
-    @Override
-    public void setBucketNotificationConfiguration(SetBucketNotificationConfigurationRequest setBucketNotificationConfigurationRequest) throws AmazonClientException, AmazonServiceException {
-        delegate.setBucketNotificationConfiguration(setBucketNotificationConfigurationRequest);
-    }
-
-    @Override
-    public void setBucketNotificationConfiguration(String bucketName, BucketNotificationConfiguration bucketNotificationConfiguration) throws AmazonClientException, AmazonServiceException {
-        delegate.setBucketNotificationConfiguration(bucketName, bucketNotificationConfiguration);
-    }
-
-    @Override
-    public BucketWebsiteConfiguration getBucketWebsiteConfiguration(String bucketName) throws AmazonClientException, AmazonServiceException {
-        return delegate.getBucketWebsiteConfiguration(bucketName);
-    }
-
-    @Override
-    public BucketWebsiteConfiguration getBucketWebsiteConfiguration(GetBucketWebsiteConfigurationRequest getBucketWebsiteConfigurationRequest) throws AmazonClientException, AmazonServiceException {
-        return delegate.getBucketWebsiteConfiguration(getBucketWebsiteConfigurationRequest);
-    }
-
-    @Override
-    public void setBucketWebsiteConfiguration(String bucketName, BucketWebsiteConfiguration configuration) throws AmazonClientException, AmazonServiceException {
-        delegate.setBucketWebsiteConfiguration(bucketName, configuration);
-    }
-
-    @Override
-    public void setBucketWebsiteConfiguration(SetBucketWebsiteConfigurationRequest setBucketWebsiteConfigurationRequest) throws AmazonClientException, AmazonServiceException {
-        delegate.setBucketWebsiteConfiguration(setBucketWebsiteConfigurationRequest);
-    }
-
-    @Override
-    public void deleteBucketWebsiteConfiguration(String bucketName) throws AmazonClientException, AmazonServiceException {
-        delegate.deleteBucketWebsiteConfiguration(bucketName);
-    }
-
-    @Override
-    public void deleteBucketWebsiteConfiguration(DeleteBucketWebsiteConfigurationRequest deleteBucketWebsiteConfigurationRequest) throws AmazonClientException, AmazonServiceException {
-        delegate.deleteBucketWebsiteConfiguration(deleteBucketWebsiteConfigurationRequest);
-    }
-
-    @Override
-    public BucketPolicy getBucketPolicy(String bucketName) throws AmazonClientException, AmazonServiceException {
-        return delegate.getBucketPolicy(bucketName);
-    }
-
-    @Override
-    public BucketPolicy getBucketPolicy(GetBucketPolicyRequest getBucketPolicyRequest) throws AmazonClientException, AmazonServiceException {
-        return delegate.getBucketPolicy(getBucketPolicyRequest);
-    }
-
-    @Override
-    public void setBucketPolicy(String bucketName, String policyText) throws AmazonClientException, AmazonServiceException {
-        delegate.setBucketPolicy(bucketName, policyText);
-    }
-
-    @Override
-    public void setBucketPolicy(SetBucketPolicyRequest setBucketPolicyRequest) throws AmazonClientException, AmazonServiceException {
-        delegate.setBucketPolicy(setBucketPolicyRequest);
-    }
-
-    @Override
-    public void deleteBucketPolicy(String bucketName) throws AmazonClientException, AmazonServiceException {
-        delegate.deleteBucketPolicy(bucketName);
-    }
-
-    @Override
-    public void deleteBucketPolicy(DeleteBucketPolicyRequest deleteBucketPolicyRequest) throws AmazonClientException, AmazonServiceException {
-        delegate.deleteBucketPolicy(deleteBucketPolicyRequest);
-    }
-
-    @Override
-    public URL generatePresignedUrl(String bucketName, String key, Date expiration) throws AmazonClientException {
-        return delegate.generatePresignedUrl(bucketName, key, expiration);
-    }
-
-    @Override
-    public URL generatePresignedUrl(String bucketName, String key, Date expiration, HttpMethod method) throws AmazonClientException {
-        return delegate.generatePresignedUrl(bucketName, key, expiration, method);
-    }
-
-    @Override
-    public URL generatePresignedUrl(GeneratePresignedUrlRequest generatePresignedUrlRequest) throws AmazonClientException {
-        return delegate.generatePresignedUrl(generatePresignedUrlRequest);
-    }
-
-    @Override
-    public InitiateMultipartUploadResult initiateMultipartUpload(InitiateMultipartUploadRequest request) throws AmazonClientException, AmazonServiceException {
-        return delegate.initiateMultipartUpload(request);
-    }
-
-    @Override
-    public UploadPartResult uploadPart(UploadPartRequest request) throws AmazonClientException, AmazonServiceException {
-        return delegate.uploadPart(request);
-    }
-
-    @Override
-    public PartListing listParts(ListPartsRequest request) throws AmazonClientException, AmazonServiceException {
-        return delegate.listParts(request);
-    }
-
-    @Override
-    public void abortMultipartUpload(AbortMultipartUploadRequest request) throws AmazonClientException, AmazonServiceException {
-        delegate.abortMultipartUpload(request);
-    }
-
-    @Override
-    public CompleteMultipartUploadResult completeMultipartUpload(CompleteMultipartUploadRequest request) throws AmazonClientException, AmazonServiceException {
-        return delegate.completeMultipartUpload(request);
-    }
-
-    @Override
-    public MultipartUploadListing listMultipartUploads(ListMultipartUploadsRequest request) throws AmazonClientException, AmazonServiceException {
-        return delegate.listMultipartUploads(request);
-    }
-
-    @Override
-    public S3ResponseMetadata getCachedResponseMetadata(AmazonWebServiceRequest request) {
-        return delegate.getCachedResponseMetadata(request);
-    }
-
-    @Override
-    public void restoreObject(RestoreObjectRequest copyGlacierObjectRequest) throws AmazonServiceException {
-        delegate.restoreObject(copyGlacierObjectRequest);
-    }
-
-    @Override
-    public void restoreObject(String bucketName, String key, int expirationInDays) throws AmazonServiceException {
-        delegate.restoreObject(bucketName, key, expirationInDays);
-    }
-
-    @Override
-    public void enableRequesterPays(String bucketName) throws AmazonServiceException, AmazonClientException {
-        delegate.enableRequesterPays(bucketName);
-    }
-
-    @Override
-    public void disableRequesterPays(String bucketName) throws AmazonServiceException, AmazonClientException {
-        delegate.disableRequesterPays(bucketName);
-    }
-
-    @Override
-    public boolean isRequesterPaysEnabled(String bucketName) throws AmazonServiceException, AmazonClientException {
-        return delegate.isRequesterPaysEnabled(bucketName);
-    }
-
-    @Override
-    public ObjectListing listNextBatchOfObjects(ListNextBatchOfObjectsRequest listNextBatchOfObjectsRequest) throws AmazonClientException, AmazonServiceException {
-        return delegate.listNextBatchOfObjects(listNextBatchOfObjectsRequest);
-    }
-
-    @Override
-    public VersionListing listNextBatchOfVersions(ListNextBatchOfVersionsRequest listNextBatchOfVersionsRequest) throws AmazonClientException, AmazonServiceException {
-        return delegate.listNextBatchOfVersions(listNextBatchOfVersionsRequest);
-    }
-
-    @Override
-    public Owner getS3AccountOwner(GetS3AccountOwnerRequest getS3AccountOwnerRequest) throws AmazonClientException, AmazonServiceException {
-        return delegate.getS3AccountOwner(getS3AccountOwnerRequest);
-    }
-
-    @Override
-    public BucketLoggingConfiguration getBucketLoggingConfiguration(GetBucketLoggingConfigurationRequest getBucketLoggingConfigurationRequest) throws AmazonClientException, AmazonServiceException {
-        return delegate.getBucketLoggingConfiguration(getBucketLoggingConfigurationRequest);
-    }
-
-    @Override
-    public BucketVersioningConfiguration getBucketVersioningConfiguration(GetBucketVersioningConfigurationRequest getBucketVersioningConfigurationRequest) throws AmazonClientException, AmazonServiceException {
-        return delegate.getBucketVersioningConfiguration(getBucketVersioningConfigurationRequest);
-    }
-
-    @Override
-    public BucketLifecycleConfiguration getBucketLifecycleConfiguration(GetBucketLifecycleConfigurationRequest getBucketLifecycleConfigurationRequest) {
-        return delegate.getBucketLifecycleConfiguration(getBucketLifecycleConfigurationRequest);
-    }
-
-    @Override
-    public BucketCrossOriginConfiguration getBucketCrossOriginConfiguration(GetBucketCrossOriginConfigurationRequest getBucketCrossOriginConfigurationRequest) {
-        return delegate.getBucketCrossOriginConfiguration(getBucketCrossOriginConfigurationRequest);
-    }
-
-    @Override
-    public BucketTaggingConfiguration getBucketTaggingConfiguration(GetBucketTaggingConfigurationRequest getBucketTaggingConfigurationRequest) {
-        return delegate.getBucketTaggingConfiguration(getBucketTaggingConfigurationRequest);
-    }
-
-    @Override
-    public BucketNotificationConfiguration getBucketNotificationConfiguration(GetBucketNotificationConfigurationRequest getBucketNotificationConfigurationRequest) throws AmazonClientException, AmazonServiceException {
-        return delegate.getBucketNotificationConfiguration(getBucketNotificationConfigurationRequest);
-    }
-
-    @Override
-    public BucketReplicationConfiguration getBucketReplicationConfiguration(GetBucketReplicationConfigurationRequest getBucketReplicationConfigurationRequest) throws AmazonServiceException, AmazonClientException {
-        return delegate.getBucketReplicationConfiguration(getBucketReplicationConfigurationRequest);
-    }
-}
diff --git a/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/CloudAWSRestIT.java b/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/CloudAWSRestIT.java
deleted file mode 100644
index ce47b60..0000000
--- a/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/CloudAWSRestIT.java
+++ /dev/null
@@ -1,41 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.aws;
-
-import com.carrotsearch.randomizedtesting.annotations.Name;
-import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
-import org.elasticsearch.test.rest.ESRestTestCase;
-import org.elasticsearch.test.rest.RestTestCandidate;
-import org.elasticsearch.test.rest.parser.RestTestParseException;
-
-import java.io.IOException;
-
-public class CloudAWSRestIT extends ESRestTestCase {
-
-    public CloudAWSRestIT(@Name("yaml") RestTestCandidate testCandidate) {
-        super(testCandidate);
-    }
-
-    @ParametersFactory
-    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
-        return ESRestTestCase.createParameters(0, 1);
-    }
-}
-
diff --git a/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/TestAmazonS3.java b/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/TestAmazonS3.java
deleted file mode 100644
index d2ed3ba..0000000
--- a/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/TestAmazonS3.java
+++ /dev/null
@@ -1,155 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.aws;
-
-import com.amazonaws.AmazonClientException;
-import com.amazonaws.AmazonServiceException;
-import com.amazonaws.services.s3.AmazonS3;
-import com.amazonaws.services.s3.model.*;
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.Loggers;
-import org.elasticsearch.common.settings.Settings;
-
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.UnsupportedEncodingException;
-import java.security.MessageDigest;
-import java.security.NoSuchAlgorithmException;
-import java.util.concurrent.ConcurrentHashMap;
-import java.util.concurrent.ConcurrentMap;
-import java.util.concurrent.atomic.AtomicLong;
-
-import static com.carrotsearch.randomizedtesting.RandomizedTest.randomDouble;
-
-/**
- *
- */
-public class TestAmazonS3 extends AmazonS3Wrapper {
-
-    protected final ESLogger logger = Loggers.getLogger(getClass());
-
-    private double writeFailureRate = 0.0;
-    private double readFailureRate = 0.0;
-
-    private String randomPrefix;
-
-    ConcurrentMap<String, AtomicLong> accessCounts = new ConcurrentHashMap<String, AtomicLong>();
-
-    private long incrementAndGet(String path) {
-        AtomicLong value = accessCounts.get(path);
-        if (value == null) {
-            value = accessCounts.putIfAbsent(path, new AtomicLong(1));
-        }
-        if (value != null) {
-            return value.incrementAndGet();
-        }
-        return 1;
-    }
-
-    public TestAmazonS3(AmazonS3 delegate, Settings settings) {
-        super(delegate);
-        randomPrefix = settings.get("cloud.aws.test.random");
-        writeFailureRate = settings.getAsDouble("cloud.aws.test.write_failures", 0.0);
-        readFailureRate = settings.getAsDouble("cloud.aws.test.read_failures", 0.0);
-    }
-
-    @Override
-    public PutObjectResult putObject(String bucketName, String key, InputStream input, ObjectMetadata metadata) throws AmazonClientException, AmazonServiceException {
-        if (shouldFail(bucketName, key, writeFailureRate)) {
-            long length = metadata.getContentLength();
-            long partToRead = (long) (length * randomDouble());
-            byte[] buffer = new byte[1024];
-            for (long cur = 0; cur < partToRead; cur += buffer.length) {
-                try {
-                    input.read(buffer, 0, (int) (partToRead - cur > buffer.length ? buffer.length : partToRead - cur));
-                } catch (IOException ex) {
-                    throw new ElasticsearchException("cannot read input stream", ex);
-                }
-            }
-            logger.info("--> random write failure on putObject method: throwing an exception for [bucket={}, key={}]", bucketName, key);
-            AmazonS3Exception ex = new AmazonS3Exception("Random S3 exception");
-            ex.setStatusCode(400);
-            ex.setErrorCode("RequestTimeout");
-            throw ex;
-        } else {
-            return super.putObject(bucketName, key, input, metadata);
-        }
-    }
-
-    @Override
-    public UploadPartResult uploadPart(UploadPartRequest request) throws AmazonClientException, AmazonServiceException {
-        if (shouldFail(request.getBucketName(), request.getKey(), writeFailureRate)) {
-            long length = request.getPartSize();
-            long partToRead = (long) (length * randomDouble());
-            byte[] buffer = new byte[1024];
-            for (long cur = 0; cur < partToRead; cur += buffer.length) {
-                try (InputStream input = request.getInputStream()){
-                    input.read(buffer, 0, (int) (partToRead - cur > buffer.length ? buffer.length : partToRead - cur));
-                } catch (IOException ex) {
-                    throw new ElasticsearchException("cannot read input stream", ex);
-                }
-            }
-            logger.info("--> random write failure on uploadPart method: throwing an exception for [bucket={}, key={}]", request.getBucketName(), request.getKey());
-            AmazonS3Exception ex = new AmazonS3Exception("Random S3 write exception");
-            ex.setStatusCode(400);
-            ex.setErrorCode("RequestTimeout");
-            throw ex;
-        } else {
-            return super.uploadPart(request);
-        }
-    }
-
-    @Override
-    public S3Object getObject(String bucketName, String key) throws AmazonClientException, AmazonServiceException {
-        if (shouldFail(bucketName, key, readFailureRate)) {
-            logger.info("--> random read failure on getObject method: throwing an exception for [bucket={}, key={}]", bucketName, key);
-            AmazonS3Exception ex = new AmazonS3Exception("Random S3 read exception");
-            ex.setStatusCode(404);
-            throw ex;
-        } else {
-            return super.getObject(bucketName, key);
-        }
-    }
-
-    private boolean shouldFail(String bucketName, String key, double probability) {
-        if (probability > 0.0) {
-            String path = randomPrefix + "-" + bucketName + "+" + key;
-            path += "/" + incrementAndGet(path);
-            return Math.abs(hashCode(path)) < Integer.MAX_VALUE * probability;
-        } else {
-            return false;
-        }
-    }
-
-    private int hashCode(String path) {
-        try {
-            MessageDigest digest = MessageDigest.getInstance("MD5");
-            byte[] bytes = digest.digest(path.getBytes("UTF-8"));
-            int i = 0;
-            return ((bytes[i++] & 0xFF) << 24) | ((bytes[i++] & 0xFF) << 16)
-                    | ((bytes[i++] & 0xFF) << 8) | (bytes[i++] & 0xFF);
-        } catch (UnsupportedEncodingException ex) {
-            throw new ElasticsearchException("cannot calculate hashcode", ex);
-        } catch (NoSuchAlgorithmException ex) {
-            throw new ElasticsearchException("cannot calculate hashcode", ex);
-        }
-    }
-}
diff --git a/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/TestAwsS3Service.java b/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/TestAwsS3Service.java
deleted file mode 100644
index 92e4d72..0000000
--- a/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/TestAwsS3Service.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.cloud.aws;
-
-import com.amazonaws.services.s3.AmazonS3;
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.plugins.Plugin;
-
-import java.util.IdentityHashMap;
-
-public class TestAwsS3Service extends InternalAwsS3Service {
-    public static class TestPlugin extends Plugin {
-        @Override
-        public String name() {
-            return "mock-s3-service";
-        }
-        @Override
-        public String description() {
-            return "plugs in mock s3 service";
-        }
-        public void onModule(AwsModule awsModule) {
-            awsModule.s3ServiceImpl = TestAwsS3Service.class;
-        }
-    }
-
-    IdentityHashMap<AmazonS3, TestAmazonS3> clients = new IdentityHashMap<AmazonS3, TestAmazonS3>();
-
-    @Inject
-    public TestAwsS3Service(Settings settings) {
-        super(settings);
-    }
-
-
-    @Override
-    public synchronized AmazonS3 client() {
-        return cachedWrapper(super.client());
-    }
-
-    @Override
-    public synchronized AmazonS3 client(String endpoint, String protocol, String region, String account, String key) {
-        return cachedWrapper(super.client(endpoint, protocol, region, account, key));
-    }
-
-    @Override
-    public synchronized AmazonS3 client(String endpoint, String protocol, String region, String account, String key, Integer maxRetries) {
-        return cachedWrapper(super.client(endpoint, protocol, region, account, key, maxRetries));
-    }
-
-    private AmazonS3 cachedWrapper(AmazonS3 client) {
-        TestAmazonS3 wrapper = clients.get(client);
-        if (wrapper == null) {
-            wrapper = new TestAmazonS3(client, settings);
-            clients.put(client, wrapper);
-        }
-        return wrapper;
-    }
-
-    @Override
-    protected synchronized void doClose() throws ElasticsearchException {
-        super.doClose();
-        clients.clear();
-    }
-
-
-}
diff --git a/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/blobstore/MockDefaultS3OutputStream.java b/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/blobstore/MockDefaultS3OutputStream.java
deleted file mode 100644
index cd2450f..0000000
--- a/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/blobstore/MockDefaultS3OutputStream.java
+++ /dev/null
@@ -1,99 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.aws.blobstore;
-
-import com.amazonaws.services.s3.model.AmazonS3Exception;
-import com.amazonaws.services.s3.model.PartETag;
-import com.carrotsearch.randomizedtesting.RandomizedTest;
-import org.elasticsearch.common.io.Streams;
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.util.List;
-
-public class MockDefaultS3OutputStream extends DefaultS3OutputStream {
-
-    private ByteArrayOutputStream out = new ByteArrayOutputStream();
-
-    private boolean initialized = false;
-    private boolean completed = false;
-    private boolean aborted = false;
-
-    private int numberOfUploadRequests = 0;
-
-    public MockDefaultS3OutputStream(int bufferSizeInBytes) {
-        super(null, "test-bucket", "test-blobname", bufferSizeInBytes, 3, false);
-    }
-
-    @Override
-    protected void doUpload(S3BlobStore blobStore, String bucketName, String blobName, InputStream is, int length, boolean serverSideEncryption) throws AmazonS3Exception {
-        try {
-            long copied = Streams.copy(is, out);
-            if (copied != length) {
-                throw new AmazonS3Exception("Not all the bytes were copied");
-            }
-            numberOfUploadRequests++;
-        } catch (IOException e) {
-            throw new AmazonS3Exception(e.getMessage());
-        }
-    }
-
-    @Override
-    protected String doInitialize(S3BlobStore blobStore, String bucketName, String blobName, boolean serverSideEncryption) {
-        initialized = true;
-        return RandomizedTest.randomAsciiOfLength(50);
-    }
-
-    @Override
-    protected PartETag doUploadMultipart(S3BlobStore blobStore, String bucketName, String blobName, String uploadId, InputStream is, int length, boolean lastPart) throws AmazonS3Exception {
-        try {
-            long copied = Streams.copy(is, out);
-            if (copied != length) {
-                throw new AmazonS3Exception("Not all the bytes were copied");
-            }
-            return new PartETag(numberOfUploadRequests++, RandomizedTest.randomAsciiOfLength(50));
-        } catch (IOException e) {
-            throw new AmazonS3Exception(e.getMessage());
-        }
-    }
-
-    @Override
-    protected void doCompleteMultipart(S3BlobStore blobStore, String bucketName, String blobName, String uploadId, List<PartETag> parts) throws AmazonS3Exception {
-        completed = true;
-    }
-
-    @Override
-    protected void doAbortMultipart(S3BlobStore blobStore, String bucketName, String blobName, String uploadId) throws AmazonS3Exception {
-        aborted = true;
-    }
-
-    public int getNumberOfUploadRequests() {
-        return numberOfUploadRequests;
-    }
-
-    public boolean isMultipart() {
-        return (numberOfUploadRequests > 1) && initialized && completed && !aborted;
-    }
-
-    public byte[] toByteArray() {
-        return out.toByteArray();
-    }
-}
diff --git a/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/blobstore/S3OutputStreamTest.java b/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/blobstore/S3OutputStreamTest.java
deleted file mode 100644
index b2dc66b..0000000
--- a/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/blobstore/S3OutputStreamTest.java
+++ /dev/null
@@ -1,143 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.aws.blobstore;
-
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.util.Arrays;
-
-import static org.elasticsearch.common.io.Streams.copy;
-import static org.hamcrest.Matchers.equalTo;
-
-/**
- * Unit test for {@link S3OutputStream}.
- */
-public class S3OutputStreamTest extends ESTestCase {
-
-    private static final int BUFFER_SIZE = S3BlobStore.MIN_BUFFER_SIZE.bytesAsInt();
-
-    @Test
-    public void testWriteLessDataThanBufferSize() throws IOException {
-        MockDefaultS3OutputStream out = newS3OutputStream(BUFFER_SIZE);
-        byte[] content = randomUnicodeOfLengthBetween(1, 512).getBytes("UTF-8");
-        copy(content, out);
-
-        // Checks length & content
-        assertThat(out.getLength(), equalTo((long) content.length));
-        assertThat(Arrays.equals(content, out.toByteArray()), equalTo(true));
-
-        // Checks single/multi part upload
-        assertThat(out.getBufferSize(), equalTo(BUFFER_SIZE));
-        assertThat(out.getFlushCount(), equalTo(1));
-        assertThat(out.getNumberOfUploadRequests(), equalTo(1));
-        assertFalse(out.isMultipart());
-
-    }
-
-    @Test
-    public void testWriteSameDataThanBufferSize() throws IOException {
-        int size = randomIntBetween(BUFFER_SIZE, 2 * BUFFER_SIZE);
-        MockDefaultS3OutputStream out = newS3OutputStream(size);
-
-        ByteArrayOutputStream content = new ByteArrayOutputStream(size);
-        for (int i = 0; i < size; i++) {
-            content.write(randomByte());
-        }
-        copy(content.toByteArray(), out);
-
-        // Checks length & content
-        assertThat(out.getLength(), equalTo((long) size));
-        assertThat(Arrays.equals(content.toByteArray(), out.toByteArray()), equalTo(true));
-
-        // Checks single/multi part upload
-        assertThat(out.getBufferSize(), equalTo(size));
-        assertThat(out.getFlushCount(), equalTo(1));
-        assertThat(out.getNumberOfUploadRequests(), equalTo(1));
-        assertFalse(out.isMultipart());
-
-    }
-
-    @Test
-    public void testWriteExactlyNTimesMoreDataThanBufferSize() throws IOException {
-        int n = randomIntBetween(2, 3);
-        int length = n * BUFFER_SIZE;
-        ByteArrayOutputStream content = new ByteArrayOutputStream(length);
-
-        for (int i = 0; i < length; i++) {
-            content.write(randomByte());
-        }
-
-        MockDefaultS3OutputStream out = newS3OutputStream(BUFFER_SIZE);
-        copy(content.toByteArray(), out);
-
-        // Checks length & content
-        assertThat(out.getLength(), equalTo((long) length));
-        assertThat(Arrays.equals(content.toByteArray(), out.toByteArray()), equalTo(true));
-
-        // Checks single/multi part upload
-        assertThat(out.getBufferSize(), equalTo(BUFFER_SIZE));
-        assertThat(out.getFlushCount(), equalTo(n));
-
-        assertThat(out.getNumberOfUploadRequests(), equalTo(n));
-        assertTrue(out.isMultipart());
-    }
-
-    @Test
-    public void testWriteRandomNumberOfBytes() throws IOException {
-        Integer randomBufferSize = randomIntBetween(BUFFER_SIZE, 2 * BUFFER_SIZE);
-        MockDefaultS3OutputStream out = newS3OutputStream(randomBufferSize);
-
-        Integer randomLength = randomIntBetween(1, 2 * BUFFER_SIZE);
-        ByteArrayOutputStream content = new ByteArrayOutputStream(randomLength);
-        for (int i = 0; i < randomLength; i++) {
-            content.write(randomByte());
-        }
-
-        copy(content.toByteArray(), out);
-
-        // Checks length & content
-        assertThat(out.getLength(), equalTo((long) randomLength));
-        assertThat(Arrays.equals(content.toByteArray(), out.toByteArray()), equalTo(true));
-
-        assertThat(out.getBufferSize(), equalTo(randomBufferSize));
-        int times = (int) Math.ceil(randomLength.doubleValue() / randomBufferSize.doubleValue());
-        assertThat(out.getFlushCount(), equalTo(times));
-        if (times > 1) {
-            assertTrue(out.isMultipart());
-        } else {
-            assertFalse(out.isMultipart());
-        }
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testWrongBufferSize() throws IOException {
-        Integer randomBufferSize = randomIntBetween(1, 4 * 1024 * 1024);
-        MockDefaultS3OutputStream out = newS3OutputStream(randomBufferSize);
-        fail("Buffer size can't be smaller than 5mb");
-    }
-
-    private MockDefaultS3OutputStream newS3OutputStream(int bufferSizeInBytes) {
-        return new MockDefaultS3OutputStream(bufferSizeInBytes);
-    }
-
-}
diff --git a/plugins/cloud-aws/src/test/java/org/elasticsearch/discovery/ec2/Ec2DiscoveryITest.java b/plugins/cloud-aws/src/test/java/org/elasticsearch/discovery/ec2/Ec2DiscoveryITest.java
deleted file mode 100644
index 9af9e4d..0000000
--- a/plugins/cloud-aws/src/test/java/org/elasticsearch/discovery/ec2/Ec2DiscoveryITest.java
+++ /dev/null
@@ -1,57 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.ec2;
-
-
-import org.elasticsearch.cloud.aws.AbstractAwsTest;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.plugin.cloud.aws.CloudAwsPlugin;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
-import org.elasticsearch.test.ESIntegTestCase.Scope;
-import org.junit.Test;
-
-import java.util.Collection;
-
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-
-/**
- * Just an empty Node Start test to check eveything if fine when
- * starting.
- * This test requires AWS to run.
- */
-@ClusterScope(scope = Scope.TEST, numDataNodes = 0, numClientNodes = 0, transportClientRatio = 0.0)
-public class Ec2DiscoveryITest extends AbstractAwsTest {
-
-    @Override
-    protected Collection<Class<? extends Plugin>> nodePlugins() {
-        return pluginList(CloudAwsPlugin.class);
-    }
-
-    @Test
-    public void testStart() {
-        Settings nodeSettings = settingsBuilder()
-                .put("cloud.enabled", true)
-                .put("discovery.type", "ec2")
-                .build();
-        internalCluster().startNode(nodeSettings);
-    }
-
-}
diff --git a/plugins/cloud-aws/src/test/java/org/elasticsearch/discovery/ec2/Ec2DiscoveryUpdateSettingsITest.java b/plugins/cloud-aws/src/test/java/org/elasticsearch/discovery/ec2/Ec2DiscoveryUpdateSettingsITest.java
deleted file mode 100644
index 7dbe764..0000000
--- a/plugins/cloud-aws/src/test/java/org/elasticsearch/discovery/ec2/Ec2DiscoveryUpdateSettingsITest.java
+++ /dev/null
@@ -1,68 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.ec2;
-
-
-import org.elasticsearch.action.admin.cluster.settings.ClusterUpdateSettingsResponse;
-import org.elasticsearch.cloud.aws.AbstractAwsTest;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.plugin.cloud.aws.CloudAwsPlugin;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
-import org.elasticsearch.test.ESIntegTestCase.Scope;
-import org.junit.Test;
-
-import java.util.Collection;
-
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.hamcrest.CoreMatchers.is;
-
-/**
- * Just an empty Node Start test to check eveything if fine when
- * starting.
- * This test requires AWS to run.
- */
-@ClusterScope(scope = Scope.TEST, numDataNodes = 0, numClientNodes = 0, transportClientRatio = 0.0)
-public class Ec2DiscoveryUpdateSettingsITest extends AbstractAwsTest {
-
-    @Override
-    protected Collection<Class<? extends Plugin>> nodePlugins() {
-        return pluginList(CloudAwsPlugin.class);
-    }
-
-    @Test
-    public void testMinimumMasterNodesStart() {
-        Settings nodeSettings = settingsBuilder()
-                .put("cloud.enabled", true)
-                .put("discovery.type", "ec2")
-                .build();
-        internalCluster().startNode(nodeSettings);
-
-        // We try to update minimum_master_nodes now
-        ClusterUpdateSettingsResponse response = client().admin().cluster().prepareUpdateSettings()
-                .setPersistentSettings(settingsBuilder().put("discovery.zen.minimum_master_nodes", 1))
-                .setTransientSettings(settingsBuilder().put("discovery.zen.minimum_master_nodes", 1))
-                .get();
-
-        Integer min = response.getPersistentSettings().getAsInt("discovery.zen.minimum_master_nodes", null);
-        assertThat(min, is(1));
-    }
-
-}
diff --git a/plugins/cloud-aws/src/test/java/org/elasticsearch/repositories/s3/AbstractS3SnapshotRestoreTest.java b/plugins/cloud-aws/src/test/java/org/elasticsearch/repositories/s3/AbstractS3SnapshotRestoreTest.java
deleted file mode 100644
index 3932241..0000000
--- a/plugins/cloud-aws/src/test/java/org/elasticsearch/repositories/s3/AbstractS3SnapshotRestoreTest.java
+++ /dev/null
@@ -1,515 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.repositories.s3;
-
-import com.amazonaws.services.s3.AmazonS3;
-import com.amazonaws.services.s3.model.DeleteObjectsRequest;
-import com.amazonaws.services.s3.model.ObjectListing;
-import com.amazonaws.services.s3.model.S3ObjectSummary;
-
-import org.elasticsearch.action.admin.cluster.repositories.put.PutRepositoryResponse;
-import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;
-import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.ClusterAdminClient;
-import org.elasticsearch.cloud.aws.AbstractAwsTest;
-import org.elasticsearch.cloud.aws.AwsS3Service;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.plugin.cloud.aws.CloudAwsPlugin;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.repositories.RepositoryMissingException;
-import org.elasticsearch.repositories.RepositoryVerificationException;
-import org.elasticsearch.snapshots.SnapshotMissingException;
-import org.elasticsearch.snapshots.SnapshotState;
-import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
-import org.elasticsearch.test.ESIntegTestCase.Scope;
-import org.elasticsearch.test.store.MockFSDirectoryService;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.List;
-
-import static org.hamcrest.Matchers.*;
-
-/**
- */
-@ClusterScope(scope = Scope.SUITE, numDataNodes = 2, numClientNodes = 0, transportClientRatio = 0.0)
-abstract public class AbstractS3SnapshotRestoreTest extends AbstractAwsTest {
-
-    @Override
-    public Settings indexSettings() {
-        // During restore we frequently restore index to exactly the same state it was before, that might cause the same
-        // checksum file to be written twice during restore operation
-        return Settings.builder().put(super.indexSettings())
-                .put(MockFSDirectoryService.RANDOM_PREVENT_DOUBLE_WRITE, false)
-                .put(MockFSDirectoryService.RANDOM_NO_DELETE_OPEN_FILE, false)
-                .put("cloud.enabled", true)
-                .put("repositories.s3.base_path", basePath)
-                .build();
-    }
-
-    @Override
-    protected Collection<Class<? extends Plugin>> nodePlugins() {
-        return pluginList(CloudAwsPlugin.class);
-    }
-
-    private String basePath;
-
-    @Before
-    public final void wipeBefore() {
-        wipeRepositories();
-        basePath = "repo-" + randomInt();
-        cleanRepositoryFiles(basePath);
-    }
-
-    @After
-    public final void wipeAfter() {
-        wipeRepositories();
-        cleanRepositoryFiles(basePath);
-    }
-
-    @Test @AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch-cloud-aws/issues/211")
-    public void testSimpleWorkflow() {
-        Client client = client();
-        Settings.Builder settings = Settings.settingsBuilder()
-                .put("chunk_size", randomIntBetween(1000, 10000));
-
-        // We sometime test getting the base_path from node settings using repositories.s3.base_path
-        if (usually()) {
-            settings.put("base_path", basePath);
-        }
-
-        logger.info("-->  creating s3 repository with bucket[{}] and path [{}]", internalCluster().getInstance(Settings.class).get("repositories.s3.bucket"), basePath);
-        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository("test-repo")
-                .setType("s3").setSettings(settings
-                        ).get();
-        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
-
-        createIndex("test-idx-1", "test-idx-2", "test-idx-3");
-        ensureGreen();
-
-        logger.info("--> indexing some data");
-        for (int i = 0; i < 100; i++) {
-            index("test-idx-1", "doc", Integer.toString(i), "foo", "bar" + i);
-            index("test-idx-2", "doc", Integer.toString(i), "foo", "baz" + i);
-            index("test-idx-3", "doc", Integer.toString(i), "foo", "baz" + i);
-        }
-        refresh();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
-        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
-        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(100L));
-
-        logger.info("--> snapshot");
-        CreateSnapshotResponse createSnapshotResponse = client.admin().cluster().prepareCreateSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices("test-idx-*", "-test-idx-3").get();
-        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
-        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse.getSnapshotInfo().totalShards()));
-
-        assertThat(client.admin().cluster().prepareGetSnapshots("test-repo").setSnapshots("test-snap").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
-
-        logger.info("--> delete some data");
-        for (int i = 0; i < 50; i++) {
-            client.prepareDelete("test-idx-1", "doc", Integer.toString(i)).get();
-        }
-        for (int i = 50; i < 100; i++) {
-            client.prepareDelete("test-idx-2", "doc", Integer.toString(i)).get();
-        }
-        for (int i = 0; i < 100; i += 2) {
-            client.prepareDelete("test-idx-3", "doc", Integer.toString(i)).get();
-        }
-        refresh();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(50L));
-        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(50L));
-        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(50L));
-
-        logger.info("--> close indices");
-        client.admin().indices().prepareClose("test-idx-1", "test-idx-2").get();
-
-        logger.info("--> restore all indices from the snapshot");
-        RestoreSnapshotResponse restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setWaitForCompletion(true).execute().actionGet();
-        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
-
-        ensureGreen();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
-        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
-        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(50L));
-
-        // Test restore after index deletion
-        logger.info("--> delete indices");
-        cluster().wipeIndices("test-idx-1", "test-idx-2");
-        logger.info("--> restore one index after deletion");
-        restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices("test-idx-*", "-test-idx-2").execute().actionGet();
-        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
-        ensureGreen();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
-        ClusterState clusterState = client.admin().cluster().prepareState().get().getState();
-        assertThat(clusterState.getMetaData().hasIndex("test-idx-1"), equalTo(true));
-        assertThat(clusterState.getMetaData().hasIndex("test-idx-2"), equalTo(false));
-    }
-
-    @Test @AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch-cloud-aws/issues/211")
-    public void testEncryption() {
-        Client client = client();
-        logger.info("-->  creating s3 repository with bucket[{}] and path [{}]", internalCluster().getInstance(Settings.class).get("repositories.s3.bucket"), basePath);
-        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository("test-repo")
-                .setType("s3").setSettings(Settings.settingsBuilder()
-                        .put("base_path", basePath)
-                        .put("chunk_size", randomIntBetween(1000, 10000))
-                        .put("server_side_encryption", true)
-                        ).get();
-        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
-
-        createIndex("test-idx-1", "test-idx-2", "test-idx-3");
-        ensureGreen();
-
-        logger.info("--> indexing some data");
-        for (int i = 0; i < 100; i++) {
-            index("test-idx-1", "doc", Integer.toString(i), "foo", "bar" + i);
-            index("test-idx-2", "doc", Integer.toString(i), "foo", "baz" + i);
-            index("test-idx-3", "doc", Integer.toString(i), "foo", "baz" + i);
-        }
-        refresh();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
-        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
-        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(100L));
-
-        logger.info("--> snapshot");
-        CreateSnapshotResponse createSnapshotResponse = client.admin().cluster().prepareCreateSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices("test-idx-*", "-test-idx-3").get();
-        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
-        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse.getSnapshotInfo().totalShards()));
-
-        assertThat(client.admin().cluster().prepareGetSnapshots("test-repo").setSnapshots("test-snap").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
-
-        Settings settings = internalCluster().getInstance(Settings.class);
-        Settings bucket = settings.getByPrefix("repositories.s3.");
-        AmazonS3 s3Client = internalCluster().getInstance(AwsS3Service.class).client(
-                null,
-                null,
-                bucket.get("region", settings.get("repositories.s3.region")),
-                bucket.get("access_key", settings.get("cloud.aws.access_key")),
-                bucket.get("secret_key", settings.get("cloud.aws.secret_key")));
-
-        String bucketName = bucket.get("bucket");
-        logger.info("--> verify encryption for bucket [{}], prefix [{}]", bucketName, basePath);
-        List<S3ObjectSummary> summaries = s3Client.listObjects(bucketName, basePath).getObjectSummaries();
-        for (S3ObjectSummary summary : summaries) {
-            assertThat(s3Client.getObjectMetadata(bucketName, summary.getKey()).getSSEAlgorithm(), equalTo("AES256"));
-        }
-
-        logger.info("--> delete some data");
-        for (int i = 0; i < 50; i++) {
-            client.prepareDelete("test-idx-1", "doc", Integer.toString(i)).get();
-        }
-        for (int i = 50; i < 100; i++) {
-            client.prepareDelete("test-idx-2", "doc", Integer.toString(i)).get();
-        }
-        for (int i = 0; i < 100; i += 2) {
-            client.prepareDelete("test-idx-3", "doc", Integer.toString(i)).get();
-        }
-        refresh();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(50L));
-        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(50L));
-        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(50L));
-
-        logger.info("--> close indices");
-        client.admin().indices().prepareClose("test-idx-1", "test-idx-2").get();
-
-        logger.info("--> restore all indices from the snapshot");
-        RestoreSnapshotResponse restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setWaitForCompletion(true).execute().actionGet();
-        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
-
-        ensureGreen();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
-        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
-        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(50L));
-
-        // Test restore after index deletion
-        logger.info("--> delete indices");
-        cluster().wipeIndices("test-idx-1", "test-idx-2");
-        logger.info("--> restore one index after deletion");
-        restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices("test-idx-*", "-test-idx-2").execute().actionGet();
-        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
-        ensureGreen();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
-        ClusterState clusterState = client.admin().cluster().prepareState().get().getState();
-        assertThat(clusterState.getMetaData().hasIndex("test-idx-1"), equalTo(true));
-        assertThat(clusterState.getMetaData().hasIndex("test-idx-2"), equalTo(false));
-    }
-
-    /**
-     * This test verifies that the test configuration is set up in a manner that
-     * does not make the test {@link #testRepositoryWithCustomCredentials()} pointless.
-     */
-    @Test(expected = RepositoryVerificationException.class)
-    public void assertRepositoryWithCustomCredentialsIsNotAccessibleByDefaultCredentials() {
-        Client client = client();
-        Settings bucketSettings = internalCluster().getInstance(Settings.class).getByPrefix("repositories.s3.private-bucket.");
-        logger.info("-->  creating s3 repository with bucket[{}] and path [{}]", bucketSettings.get("bucket"), basePath);
-        client.admin().cluster().preparePutRepository("test-repo")
-        .setType("s3").setSettings(Settings.settingsBuilder()
-                .put("base_path", basePath)
-                .put("bucket", bucketSettings.get("bucket"))
-                ).get();
-        fail("repository verification should have raise an exception!");
-    }
-
-    @Test
-    public void testRepositoryWithCustomCredentials() {
-        Client client = client();
-        Settings bucketSettings = internalCluster().getInstance(Settings.class).getByPrefix("repositories.s3.private-bucket.");
-        logger.info("-->  creating s3 repository with bucket[{}] and path [{}]", bucketSettings.get("bucket"), basePath);
-        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository("test-repo")
-                .setType("s3").setSettings(Settings.settingsBuilder()
-                        .put("base_path", basePath)
-                        .put("region", bucketSettings.get("region"))
-                        .put("access_key", bucketSettings.get("access_key"))
-                        .put("secret_key", bucketSettings.get("secret_key"))
-                        .put("bucket", bucketSettings.get("bucket"))
-                        ).get();
-        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
-
-        assertRepositoryIsOperational(client, "test-repo");
-    }
-
-    @Test @AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch-cloud-aws/issues/211")
-    public void testRepositoryWithCustomEndpointProtocol() {
-        Client client = client();
-        Settings bucketSettings = internalCluster().getInstance(Settings.class).getByPrefix("repositories.s3.external-bucket.");
-        logger.info("--> creating s3 repostoriy with endpoint [{}], bucket[{}] and path [{}]", bucketSettings.get("endpoint"), bucketSettings.get("bucket"), basePath);
-        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository("test-repo")
-                .setType("s3").setSettings(Settings.settingsBuilder()
-                        .put("bucket", bucketSettings.get("bucket"))
-                        .put("endpoint", bucketSettings.get("endpoint"))
-                        .put("access_key", bucketSettings.get("access_key"))
-                        .put("secret_key", bucketSettings.get("secret_key"))
-                        .put("base_path", basePath)
-                        ).get();
-        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
-        assertRepositoryIsOperational(client, "test-repo");
-    }
-
-    /**
-     * This test verifies that the test configuration is set up in a manner that
-     * does not make the test {@link #testRepositoryInRemoteRegion()} pointless.
-     */
-    @Test(expected = RepositoryVerificationException.class)
-    public void assertRepositoryInRemoteRegionIsRemote() {
-        Client client = client();
-        Settings bucketSettings = internalCluster().getInstance(Settings.class).getByPrefix("repositories.s3.remote-bucket.");
-        logger.info("-->  creating s3 repository with bucket[{}] and path [{}]", bucketSettings.get("bucket"), basePath);
-        client.admin().cluster().preparePutRepository("test-repo")
-        .setType("s3").setSettings(Settings.settingsBuilder()
-                .put("base_path", basePath)
-                .put("bucket", bucketSettings.get("bucket"))
-                // Below setting intentionally omitted to assert bucket is not available in default region.
-                //                        .put("region", privateBucketSettings.get("region"))
-                ).get();
-
-        fail("repository verification should have raise an exception!");
-    }
-
-    @Test @AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch-cloud-aws/issues/211")
-    public void testRepositoryInRemoteRegion() {
-        Client client = client();
-        Settings settings = internalCluster().getInstance(Settings.class);
-        Settings bucketSettings = settings.getByPrefix("repositories.s3.remote-bucket.");
-        logger.info("-->  creating s3 repository with bucket[{}] and path [{}]", bucketSettings.get("bucket"), basePath);
-        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository("test-repo")
-                .setType("s3").setSettings(Settings.settingsBuilder()
-                        .put("base_path", basePath)
-                        .put("bucket", bucketSettings.get("bucket"))
-                        .put("region", bucketSettings.get("region"))
-                        ).get();
-        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
-
-        assertRepositoryIsOperational(client, "test-repo");
-    }
-
-    /**
-     * Test case for issue #86: https://github.com/elasticsearch/elasticsearch-cloud-aws/issues/86
-     */
-    @Test
-    public void testNonExistingRepo_86() {
-        Client client = client();
-        logger.info("-->  creating s3 repository with bucket[{}] and path [{}]", internalCluster().getInstance(Settings.class).get("repositories.s3.bucket"), basePath);
-        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository("test-repo")
-                .setType("s3").setSettings(Settings.settingsBuilder()
-                        .put("base_path", basePath)
-                ).get();
-        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
-
-        logger.info("--> restore non existing snapshot");
-        try {
-            client.admin().cluster().prepareRestoreSnapshot("test-repo", "no-existing-snapshot").setWaitForCompletion(true).execute().actionGet();
-            fail("Shouldn't be here");
-        } catch (SnapshotMissingException ex) {
-            // Expected
-        }
-    }
-
-    /**
-     * For issue #86: https://github.com/elasticsearch/elasticsearch-cloud-aws/issues/86
-     */
-    @Test
-    public void testGetDeleteNonExistingSnapshot_86() {
-        ClusterAdminClient client = client().admin().cluster();
-        logger.info("-->  creating s3 repository without any path");
-        PutRepositoryResponse putRepositoryResponse = client.preparePutRepository("test-repo")
-                .setType("s3").setSettings(Settings.settingsBuilder()
-                        .put("base_path", basePath)
-                        ).get();
-        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
-
-        try {
-            client.prepareGetSnapshots("test-repo").addSnapshots("no-existing-snapshot").get();
-            fail("Shouldn't be here");
-        } catch (SnapshotMissingException ex) {
-            // Expected
-        }
-
-        try {
-            client.prepareDeleteSnapshot("test-repo", "no-existing-snapshot").get();
-            fail("Shouldn't be here");
-        } catch (SnapshotMissingException ex) {
-            // Expected
-        }
-    }
-
-    private void assertRepositoryIsOperational(Client client, String repository) {
-        createIndex("test-idx-1");
-        ensureGreen();
-
-        logger.info("--> indexing some data");
-        for (int i = 0; i < 100; i++) {
-            index("test-idx-1", "doc", Integer.toString(i), "foo", "bar" + i);
-        }
-        refresh();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
-
-        logger.info("--> snapshot");
-        CreateSnapshotResponse createSnapshotResponse = client.admin().cluster().prepareCreateSnapshot(repository, "test-snap").setWaitForCompletion(true).setIndices("test-idx-*").get();
-        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
-        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse.getSnapshotInfo().totalShards()));
-
-        assertThat(client.admin().cluster().prepareGetSnapshots(repository).setSnapshots("test-snap").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
-
-        logger.info("--> delete some data");
-        for (int i = 0; i < 50; i++) {
-            client.prepareDelete("test-idx-1", "doc", Integer.toString(i)).get();
-        }
-        refresh();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(50L));
-
-        logger.info("--> close indices");
-        client.admin().indices().prepareClose("test-idx-1").get();
-
-        logger.info("--> restore all indices from the snapshot");
-        RestoreSnapshotResponse restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot(repository, "test-snap").setWaitForCompletion(true).execute().actionGet();
-        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
-
-        ensureGreen();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
-    }
-
-
-    /**
-     * Deletes repositories, supports wildcard notation.
-     */
-    public static void wipeRepositories(String... repositories) {
-        // if nothing is provided, delete all
-        if (repositories.length == 0) {
-            repositories = new String[]{"*"};
-        }
-        for (String repository : repositories) {
-            try {
-                client().admin().cluster().prepareDeleteRepository(repository).execute().actionGet();
-            } catch (RepositoryMissingException ex) {
-                // ignore
-            }
-        }
-    }
-
-    /**
-     * Deletes content of the repository files in the bucket
-     */
-    public void cleanRepositoryFiles(String basePath) {
-        Settings settings = internalCluster().getInstance(Settings.class);
-        Settings[] buckets = {
-                settings.getByPrefix("repositories.s3."),
-                settings.getByPrefix("repositories.s3.private-bucket."),
-                settings.getByPrefix("repositories.s3.remote-bucket."),
-                settings.getByPrefix("repositories.s3.external-bucket.")
-        };
-        for (Settings bucket : buckets) {
-            String endpoint = bucket.get("endpoint", settings.get("repositories.s3.endpoint"));
-            String protocol = bucket.get("protocol", settings.get("repositories.s3.protocol"));
-            String region = bucket.get("region", settings.get("repositories.s3.region"));
-            String accessKey = bucket.get("access_key", settings.get("cloud.aws.access_key"));
-            String secretKey = bucket.get("secret_key", settings.get("cloud.aws.secret_key"));
-            String bucketName = bucket.get("bucket");
-
-            // We check that settings has been set in elasticsearch.yml integration test file
-            // as described in README
-            assertThat("Your settings in elasticsearch.yml are incorrects. Check README file.", bucketName, notNullValue());
-            AmazonS3 client = internalCluster().getInstance(AwsS3Service.class).client(endpoint, protocol, region, accessKey, secretKey);
-            try {
-                ObjectListing prevListing = null;
-                //From http://docs.amazonwebservices.com/AmazonS3/latest/dev/DeletingMultipleObjectsUsingJava.html
-                //we can do at most 1K objects per delete
-                //We don't know the bucket name until first object listing
-                DeleteObjectsRequest multiObjectDeleteRequest = null;
-                ArrayList<DeleteObjectsRequest.KeyVersion> keys = new ArrayList<DeleteObjectsRequest.KeyVersion>();
-                while (true) {
-                    ObjectListing list;
-                    if (prevListing != null) {
-                        list = client.listNextBatchOfObjects(prevListing);
-                    } else {
-                        list = client.listObjects(bucketName, basePath);
-                        multiObjectDeleteRequest = new DeleteObjectsRequest(list.getBucketName());
-                    }
-                    for (S3ObjectSummary summary : list.getObjectSummaries()) {
-                        keys.add(new DeleteObjectsRequest.KeyVersion(summary.getKey()));
-                        //Every 500 objects batch the delete request
-                        if (keys.size() > 500) {
-                            multiObjectDeleteRequest.setKeys(keys);
-                            client.deleteObjects(multiObjectDeleteRequest);
-                            multiObjectDeleteRequest = new DeleteObjectsRequest(list.getBucketName());
-                            keys.clear();
-                        }
-                    }
-                    if (list.isTruncated()) {
-                        prevListing = list;
-                    } else {
-                        break;
-                    }
-                }
-                if (!keys.isEmpty()) {
-                    multiObjectDeleteRequest.setKeys(keys);
-                    client.deleteObjects(multiObjectDeleteRequest);
-                }
-            } catch (Throwable ex) {
-                logger.warn("Failed to delete S3 repository [{}] in [{}]", ex, bucketName, region);
-            }
-        }
-    }
-}
diff --git a/plugins/cloud-aws/src/test/java/org/elasticsearch/repositories/s3/S3ProxiedSnapshotRestoreOverHttpsTest.java b/plugins/cloud-aws/src/test/java/org/elasticsearch/repositories/s3/S3ProxiedSnapshotRestoreOverHttpsTest.java
deleted file mode 100644
index dbcb7d8..0000000
--- a/plugins/cloud-aws/src/test/java/org/elasticsearch/repositories/s3/S3ProxiedSnapshotRestoreOverHttpsTest.java
+++ /dev/null
@@ -1,47 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.repositories.s3;
-
-import org.elasticsearch.common.settings.Settings;
-import org.junit.Before;
-
-/**
- * This will only run if you define in your `elasticsearch.yml` file a s3 specific proxy
- * cloud.aws.s3.proxy_host: mys3proxy.company.com
- * cloud.aws.s3.proxy_port: 8080
- */
-public class S3ProxiedSnapshotRestoreOverHttpsTest extends AbstractS3SnapshotRestoreTest {
-
-    private boolean proxySet = false;
-
-    @Override
-    public Settings nodeSettings(int nodeOrdinal) {
-        Settings settings = super.nodeSettings(nodeOrdinal);
-        String proxyHost = settings.get("cloud.aws.s3.proxy_host");
-        proxySet = proxyHost != null;
-        return settings;
-    }
-
-    @Before
-    public void checkProxySettings() {
-        assumeTrue("we are expecting proxy settings in elasticsearch.yml file", proxySet);
-    }
-
-}
diff --git a/plugins/cloud-aws/src/test/java/org/elasticsearch/repositories/s3/S3SnapshotRestoreOverHttpTest.java b/plugins/cloud-aws/src/test/java/org/elasticsearch/repositories/s3/S3SnapshotRestoreOverHttpTest.java
deleted file mode 100644
index 045d18a..0000000
--- a/plugins/cloud-aws/src/test/java/org/elasticsearch/repositories/s3/S3SnapshotRestoreOverHttpTest.java
+++ /dev/null
@@ -1,34 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.repositories.s3;
-
-import org.elasticsearch.common.settings.Settings;
-
-/**
- */
-public class S3SnapshotRestoreOverHttpTest extends AbstractS3SnapshotRestoreTest {
-    @Override
-    public Settings nodeSettings(int nodeOrdinal) {
-        Settings.Builder settings = Settings.builder()
-                .put(super.nodeSettings(nodeOrdinal))
-                .put("cloud.aws.s3.protocol", "http");
-        return settings.build();
-    }
-}
diff --git a/plugins/cloud-aws/src/test/java/org/elasticsearch/repositories/s3/S3SnapshotRestoreOverHttpsTest.java b/plugins/cloud-aws/src/test/java/org/elasticsearch/repositories/s3/S3SnapshotRestoreOverHttpsTest.java
deleted file mode 100644
index ca098cb..0000000
--- a/plugins/cloud-aws/src/test/java/org/elasticsearch/repositories/s3/S3SnapshotRestoreOverHttpsTest.java
+++ /dev/null
@@ -1,34 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.repositories.s3;
-
-import org.elasticsearch.common.settings.Settings;
-
-/**
- */
-public class S3SnapshotRestoreOverHttpsTest extends AbstractS3SnapshotRestoreTest {
-    @Override
-    public Settings nodeSettings(int nodeOrdinal) {
-        Settings.Builder settings = Settings.builder()
-                .put(super.nodeSettings(nodeOrdinal))
-                .put("cloud.aws.s3.protocol", "https");
-        return settings.build();
-    }
-}
diff --git a/plugins/cloud-azure/pom.xml b/plugins/cloud-azure/pom.xml
index f7fa9dc..e5e1945 100644
--- a/plugins/cloud-azure/pom.xml
+++ b/plugins/cloud-azure/pom.xml
@@ -18,7 +18,7 @@ governing permissions and limitations under the License. -->
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>plugins</artifactId>
-        <version>2.1.0-SNAPSHOT</version>
+        <version>3.0.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>cloud-azure</artifactId>
diff --git a/plugins/cloud-azure/src/test/java/org/apache/lucene/store/SmbMMapDirectoryTest.java b/plugins/cloud-azure/src/test/java/org/apache/lucene/store/SmbMMapDirectoryTest.java
deleted file mode 100644
index eecaa4c..0000000
--- a/plugins/cloud-azure/src/test/java/org/apache/lucene/store/SmbMMapDirectoryTest.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.apache.lucene.store;
-
-import java.io.IOException;
-import java.nio.file.Path;
-
-public class SmbMMapDirectoryTest extends ESBaseDirectoryTestCase {
-
-    @Override
-    protected Directory getDirectory(Path file) throws IOException {
-        return new SmbDirectoryWrapper(new MMapDirectory(file));
-    }
-}
\ No newline at end of file
diff --git a/plugins/cloud-azure/src/test/java/org/apache/lucene/store/SmbMMapDirectoryTests.java b/plugins/cloud-azure/src/test/java/org/apache/lucene/store/SmbMMapDirectoryTests.java
new file mode 100644
index 0000000..43c61d8
--- /dev/null
+++ b/plugins/cloud-azure/src/test/java/org/apache/lucene/store/SmbMMapDirectoryTests.java
@@ -0,0 +1,31 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.lucene.store;
+
+import java.io.IOException;
+import java.nio.file.Path;
+
+public class SmbMMapDirectoryTests extends ESBaseDirectoryTestCase {
+
+    @Override
+    protected Directory getDirectory(Path file) throws IOException {
+        return new SmbDirectoryWrapper(new MMapDirectory(file));
+    }
+}
\ No newline at end of file
diff --git a/plugins/cloud-azure/src/test/java/org/apache/lucene/store/SmbSimpleFSDirectoryTest.java b/plugins/cloud-azure/src/test/java/org/apache/lucene/store/SmbSimpleFSDirectoryTest.java
deleted file mode 100644
index 58cdb5f..0000000
--- a/plugins/cloud-azure/src/test/java/org/apache/lucene/store/SmbSimpleFSDirectoryTest.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.apache.lucene.store;
-
-import java.io.IOException;
-import java.nio.file.Path;
-
-public class SmbSimpleFSDirectoryTest extends ESBaseDirectoryTestCase {
-
-    @Override
-    protected Directory getDirectory(Path file) throws IOException {
-        return new SmbDirectoryWrapper(new SimpleFSDirectory(file));
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/apache/lucene/store/SmbSimpleFSDirectoryTests.java b/plugins/cloud-azure/src/test/java/org/apache/lucene/store/SmbSimpleFSDirectoryTests.java
new file mode 100644
index 0000000..208eb6c
--- /dev/null
+++ b/plugins/cloud-azure/src/test/java/org/apache/lucene/store/SmbSimpleFSDirectoryTests.java
@@ -0,0 +1,31 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.lucene.store;
+
+import java.io.IOException;
+import java.nio.file.Path;
+
+public class SmbSimpleFSDirectoryTests extends ESBaseDirectoryTestCase {
+
+    @Override
+    protected Directory getDirectory(Path file) throws IOException {
+        return new SmbDirectoryWrapper(new SimpleFSDirectory(file));
+    }
+}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/azure/itest/AzureSimpleITest.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/azure/itest/AzureSimpleITest.java
deleted file mode 100644
index da916d5..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/azure/itest/AzureSimpleITest.java
+++ /dev/null
@@ -1,67 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.azure.itest;
-
-import org.elasticsearch.action.admin.cluster.state.ClusterStateResponse;
-import org.elasticsearch.cloud.azure.AbstractAzureTest;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.hamcrest.Matchers;
-import org.junit.Test;
-
-/**
- * This test needs Azure to run and -Dtests.thirdparty=true to be set
- * and -Des.config=/path/to/elasticsearch.yml
- * @see org.elasticsearch.cloud.azure.AbstractAzureTest
- */
-@ESIntegTestCase.ClusterScope(
-        scope = ESIntegTestCase.Scope.TEST,
-        numDataNodes = 1,
-        numClientNodes = 0,
-        transportClientRatio = 0.0)
-public class AzureSimpleITest extends AbstractAzureTest {
-
-    @Override
-    protected Settings nodeSettings(int nodeOrdinal) {
-        return Settings.builder()
-                .put(super.nodeSettings(nodeOrdinal))
-                // For now we let the user who runs tests to define if he wants or not to run discovery tests
-                // by setting in elasticsearch.yml: discovery.type: azure
-                // .put("discovery.type", "azure")
-                .build();
-    }
-
-    @Test
-    public void one_node_should_run() {
-        // Do nothing... Just start :-)
-        // but let's check that we have at least 1 node (local node)
-        ClusterStateResponse clusterState = client().admin().cluster().prepareState().execute().actionGet();
-
-        assertThat(clusterState.getState().getNodes().getSize(), Matchers.greaterThanOrEqualTo(1));
-    }
-
-    @Override
-    public Settings indexSettings() {
-        // During restore we frequently restore index to exactly the same state it was before, that might cause the same
-        // checksum file to be written twice during restore operation
-        return Settings.builder().put(super.indexSettings())
-                .build();
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/azure/itest/AzureSimpleTests.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/azure/itest/AzureSimpleTests.java
new file mode 100644
index 0000000..389f2a4
--- /dev/null
+++ b/plugins/cloud-azure/src/test/java/org/elasticsearch/azure/itest/AzureSimpleTests.java
@@ -0,0 +1,67 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.azure.itest;
+
+import org.elasticsearch.action.admin.cluster.state.ClusterStateResponse;
+import org.elasticsearch.cloud.azure.AbstractAzureTestCase;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.hamcrest.Matchers;
+import org.junit.Test;
+
+/**
+ * This test needs Azure to run and -Dtests.thirdparty=true to be set
+ * and -Des.config=/path/to/elasticsearch.yml
+ * @see AbstractAzureTestCase
+ */
+@ESIntegTestCase.ClusterScope(
+        scope = ESIntegTestCase.Scope.TEST,
+        numDataNodes = 1,
+        numClientNodes = 0,
+        transportClientRatio = 0.0)
+public class AzureSimpleTests extends AbstractAzureTestCase {
+
+    @Override
+    protected Settings nodeSettings(int nodeOrdinal) {
+        return Settings.builder()
+                .put(super.nodeSettings(nodeOrdinal))
+                // For now we let the user who runs tests to define if he wants or not to run discovery tests
+                // by setting in elasticsearch.yml: discovery.type: azure
+                // .put("discovery.type", "azure")
+                .build();
+    }
+
+    @Test
+    public void one_node_should_run() {
+        // Do nothing... Just start :-)
+        // but let's check that we have at least 1 node (local node)
+        ClusterStateResponse clusterState = client().admin().cluster().prepareState().execute().actionGet();
+
+        assertThat(clusterState.getState().getNodes().getSize(), Matchers.greaterThanOrEqualTo(1));
+    }
+
+    @Override
+    public Settings indexSettings() {
+        // During restore we frequently restore index to exactly the same state it was before, that might cause the same
+        // checksum file to be written twice during restore operation
+        return Settings.builder().put(super.indexSettings())
+                .build();
+    }
+}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureComputeServiceTest.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureComputeServiceTest.java
deleted file mode 100644
index 8df4df2..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureComputeServiceTest.java
+++ /dev/null
@@ -1,69 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.azure;
-
-import org.elasticsearch.action.admin.cluster.node.info.NodesInfoResponse;
-import org.elasticsearch.cloud.azure.management.AzureComputeService.Discovery;
-import org.elasticsearch.cloud.azure.management.AzureComputeService.Management;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.plugin.cloud.azure.CloudAzurePlugin;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.test.ESIntegTestCase;
-
-import java.util.Collection;
-
-public abstract class AbstractAzureComputeServiceTest extends ESIntegTestCase {
-
-    private Class<? extends Plugin> mockPlugin;
-
-    public AbstractAzureComputeServiceTest(Class<? extends Plugin> mockPlugin) {
-        // We want to inject the Azure API Mock
-        this.mockPlugin = mockPlugin;
-    }
-
-    @Override
-    protected Settings nodeSettings(int nodeOrdinal) {
-        Settings.Builder builder = Settings.settingsBuilder()
-            .put(super.nodeSettings(nodeOrdinal))
-            .put("discovery.type", "azure")
-                // We need the network to make the mock working
-            .put("node.mode", "network");
-
-        // We add a fake subscription_id to start mock compute service
-        builder.put(Management.SUBSCRIPTION_ID, "fake")
-            .put(Discovery.REFRESH, "5s")
-            .put(Management.KEYSTORE_PATH, "dummy")
-            .put(Management.KEYSTORE_PASSWORD, "dummy")
-            .put(Management.SERVICE_NAME, "dummy");
-        return builder.build();
-    }
-
-    @Override
-    protected Collection<Class<? extends Plugin>> nodePlugins() {
-        return pluginList(CloudAzurePlugin.class, mockPlugin);
-    }
-
-    protected void checkNumberOfNodes(int expected) {
-        NodesInfoResponse nodeInfos = client().admin().cluster().prepareNodesInfo().execute().actionGet();
-        assertNotNull(nodeInfos);
-        assertNotNull(nodeInfos.getNodes());
-        assertEquals(expected, nodeInfos.getNodes().length);
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureComputeServiceTestCase.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureComputeServiceTestCase.java
new file mode 100644
index 0000000..dd69e08
--- /dev/null
+++ b/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureComputeServiceTestCase.java
@@ -0,0 +1,69 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure;
+
+import org.elasticsearch.action.admin.cluster.node.info.NodesInfoResponse;
+import org.elasticsearch.cloud.azure.management.AzureComputeService.Discovery;
+import org.elasticsearch.cloud.azure.management.AzureComputeService.Management;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.plugin.cloud.azure.CloudAzurePlugin;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.test.ESIntegTestCase;
+
+import java.util.Collection;
+
+public abstract class AbstractAzureComputeServiceTestCase extends ESIntegTestCase {
+
+    private Class<? extends Plugin> mockPlugin;
+
+    public AbstractAzureComputeServiceTestCase(Class<? extends Plugin> mockPlugin) {
+        // We want to inject the Azure API Mock
+        this.mockPlugin = mockPlugin;
+    }
+
+    @Override
+    protected Settings nodeSettings(int nodeOrdinal) {
+        Settings.Builder builder = Settings.settingsBuilder()
+            .put(super.nodeSettings(nodeOrdinal))
+            .put("discovery.type", "azure")
+                // We need the network to make the mock working
+            .put("node.mode", "network");
+
+        // We add a fake subscription_id to start mock compute service
+        builder.put(Management.SUBSCRIPTION_ID, "fake")
+            .put(Discovery.REFRESH, "5s")
+            .put(Management.KEYSTORE_PATH, "dummy")
+            .put(Management.KEYSTORE_PASSWORD, "dummy")
+            .put(Management.SERVICE_NAME, "dummy");
+        return builder.build();
+    }
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return pluginList(CloudAzurePlugin.class, mockPlugin);
+    }
+
+    protected void checkNumberOfNodes(int expected) {
+        NodesInfoResponse nodeInfos = client().admin().cluster().prepareNodesInfo().execute().actionGet();
+        assertNotNull(nodeInfos);
+        assertNotNull(nodeInfos.getNodes());
+        assertEquals(expected, nodeInfos.getNodes().length);
+    }
+}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureRepositoryServiceTest.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureRepositoryServiceTest.java
deleted file mode 100644
index d4ae582..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureRepositoryServiceTest.java
+++ /dev/null
@@ -1,122 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.azure;
-
-import com.microsoft.azure.storage.StorageException;
-import org.elasticsearch.cloud.azure.storage.AzureStorageService;
-import org.elasticsearch.cloud.azure.storage.AzureStorageService.Storage;
-import org.elasticsearch.cloud.azure.storage.AzureStorageServiceMock;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.plugin.cloud.azure.CloudAzurePlugin;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.repositories.RepositoryMissingException;
-import org.elasticsearch.test.store.MockFSDirectoryService;
-import org.junit.After;
-import org.junit.Before;
-
-import java.net.URISyntaxException;
-import java.util.Collection;
-
-public abstract class AbstractAzureRepositoryServiceTest extends AbstractAzureTest {
-
-    public static class TestPlugin extends Plugin {
-        @Override
-        public String name() {
-            return "mock-stoarge-service";
-        }
-        @Override
-        public String description() {
-            return "plugs in a mock storage service for testing";
-        }
-        public void onModule(AzureModule azureModule) {
-            azureModule.storageServiceImpl = AzureStorageServiceMock.class;
-        }
-    }
-
-    protected String basePath;
-    private Class<? extends AzureStorageService> mock;
-
-    public AbstractAzureRepositoryServiceTest(String basePath) {
-        this.basePath = basePath;
-    }
-
-    /**
-     * Deletes repositories, supports wildcard notation.
-     */
-    public static void wipeRepositories(String... repositories) {
-        // if nothing is provided, delete all
-        if (repositories.length == 0) {
-            repositories = new String[]{"*"};
-        }
-        for (String repository : repositories) {
-            try {
-                client().admin().cluster().prepareDeleteRepository(repository).execute().actionGet();
-            } catch (RepositoryMissingException ex) {
-                // ignore
-            }
-        }
-    }
-
-    @Override
-    protected Settings nodeSettings(int nodeOrdinal) {
-        Settings.Builder builder = Settings.settingsBuilder()
-                .put(Storage.API_IMPLEMENTATION, mock)
-                .put(Storage.CONTAINER, "snapshots");
-
-        // We use sometime deprecated settings in tests
-        builder.put(Storage.ACCOUNT, "mock_azure_account")
-                .put(Storage.KEY, "mock_azure_key");
-
-        return builder.build();
-    }
-
-    @Override
-    protected Collection<Class<? extends Plugin>> nodePlugins() {
-        return pluginList(CloudAzurePlugin.class, TestPlugin.class);
-    }
-
-    @Override
-    public Settings indexSettings() {
-        // During restore we frequently restore index to exactly the same state it was before, that might cause the same
-        // checksum file to be written twice during restore operation
-        return Settings.builder().put(super.indexSettings())
-                .put(MockFSDirectoryService.RANDOM_PREVENT_DOUBLE_WRITE, false)
-                .put(MockFSDirectoryService.RANDOM_NO_DELETE_OPEN_FILE, false)
-                .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-    }
-
-    @Before @After
-    public final void wipe() throws StorageException, URISyntaxException {
-        wipeRepositories();
-        cleanRepositoryFiles(basePath);
-    }
-
-    /**
-     * Purge the test container
-     */
-    public void cleanRepositoryFiles(String path) throws StorageException, URISyntaxException {
-        String container = internalCluster().getInstance(Settings.class).get("repositories.azure.container");
-        logger.info("--> remove blobs in container [{}]", container);
-        AzureStorageService client = internalCluster().getInstance(AzureStorageService.class);
-        client.deleteFiles(container, path);
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureRepositoryServiceTestCase.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureRepositoryServiceTestCase.java
new file mode 100644
index 0000000..96fe8eb
--- /dev/null
+++ b/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureRepositoryServiceTestCase.java
@@ -0,0 +1,122 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure;
+
+import com.microsoft.azure.storage.StorageException;
+import org.elasticsearch.cloud.azure.storage.AzureStorageService;
+import org.elasticsearch.cloud.azure.storage.AzureStorageService.Storage;
+import org.elasticsearch.cloud.azure.storage.AzureStorageServiceMock;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.plugin.cloud.azure.CloudAzurePlugin;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.repositories.RepositoryMissingException;
+import org.elasticsearch.test.store.MockFSDirectoryService;
+import org.junit.After;
+import org.junit.Before;
+
+import java.net.URISyntaxException;
+import java.util.Collection;
+
+public abstract class AbstractAzureRepositoryServiceTestCase extends AbstractAzureTestCase {
+
+    public static class TestPlugin extends Plugin {
+        @Override
+        public String name() {
+            return "mock-stoarge-service";
+        }
+        @Override
+        public String description() {
+            return "plugs in a mock storage service for testing";
+        }
+        public void onModule(AzureModule azureModule) {
+            azureModule.storageServiceImpl = AzureStorageServiceMock.class;
+        }
+    }
+
+    protected String basePath;
+    private Class<? extends AzureStorageService> mock;
+
+    public AbstractAzureRepositoryServiceTestCase(String basePath) {
+        this.basePath = basePath;
+    }
+
+    /**
+     * Deletes repositories, supports wildcard notation.
+     */
+    public static void wipeRepositories(String... repositories) {
+        // if nothing is provided, delete all
+        if (repositories.length == 0) {
+            repositories = new String[]{"*"};
+        }
+        for (String repository : repositories) {
+            try {
+                client().admin().cluster().prepareDeleteRepository(repository).execute().actionGet();
+            } catch (RepositoryMissingException ex) {
+                // ignore
+            }
+        }
+    }
+
+    @Override
+    protected Settings nodeSettings(int nodeOrdinal) {
+        Settings.Builder builder = Settings.settingsBuilder()
+                .put(Storage.API_IMPLEMENTATION, mock)
+                .put(Storage.CONTAINER, "snapshots");
+
+        // We use sometime deprecated settings in tests
+        builder.put(Storage.ACCOUNT, "mock_azure_account")
+                .put(Storage.KEY, "mock_azure_key");
+
+        return builder.build();
+    }
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return pluginList(CloudAzurePlugin.class, TestPlugin.class);
+    }
+
+    @Override
+    public Settings indexSettings() {
+        // During restore we frequently restore index to exactly the same state it was before, that might cause the same
+        // checksum file to be written twice during restore operation
+        return Settings.builder().put(super.indexSettings())
+                .put(MockFSDirectoryService.RANDOM_PREVENT_DOUBLE_WRITE, false)
+                .put(MockFSDirectoryService.RANDOM_NO_DELETE_OPEN_FILE, false)
+                .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0)
+                .build();
+    }
+
+    @Before @After
+    public final void wipe() throws StorageException, URISyntaxException {
+        wipeRepositories();
+        cleanRepositoryFiles(basePath);
+    }
+
+    /**
+     * Purge the test container
+     */
+    public void cleanRepositoryFiles(String path) throws StorageException, URISyntaxException {
+        String container = internalCluster().getInstance(Settings.class).get("repositories.azure.container");
+        logger.info("--> remove blobs in container [{}]", container);
+        AzureStorageService client = internalCluster().getInstance(AzureStorageService.class);
+        client.deleteFiles(container, path);
+    }
+}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureTest.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureTest.java
deleted file mode 100644
index d99823b..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureTest.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.azure;
-
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.PathUtils;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsException;
-import org.elasticsearch.plugin.cloud.azure.CloudAzurePlugin;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.elasticsearch.test.ESIntegTestCase.ThirdParty;
-
-import java.util.Collection;
-
-/**
- * Base class for Azure tests that require credentials.
- * <p>
- * You must specify {@code -Dtests.thirdparty=true -Dtests.config=/path/to/config}
- * in order to run these tests.
- */
-@ThirdParty
-public abstract class AbstractAzureTest extends ESIntegTestCase {
-
-    @Override
-    protected Settings nodeSettings(int nodeOrdinal) {
-        return Settings.builder()
-                .put(super.nodeSettings(nodeOrdinal))
-                .put(readSettingsFromFile())
-                .build();
-    }
-
-    @Override
-    protected Collection<Class<? extends Plugin>> nodePlugins() {
-        return pluginList(CloudAzurePlugin.class);
-    }
-
-    protected Settings readSettingsFromFile() {
-        Settings.Builder settings = Settings.builder();
-        settings.put("path.home", createTempDir());
-
-        // if explicit, just load it and don't load from env
-        try {
-            if (Strings.hasText(System.getProperty("tests.config"))) {
-                settings.loadFromPath(PathUtils.get((System.getProperty("tests.config"))));
-            } else {
-                throw new IllegalStateException("to run integration tests, you need to set -Dtests.thirdparty=true and -Dtests.config=/path/to/elasticsearch.yml");
-            }
-        } catch (SettingsException exception) {
-          throw new IllegalStateException("your test configuration file is incorrect: " + System.getProperty("tests.config"), exception);
-        }
-        return settings.build();
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureTestCase.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureTestCase.java
new file mode 100644
index 0000000..39e6d1b
--- /dev/null
+++ b/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureTestCase.java
@@ -0,0 +1,71 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure;
+
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.io.PathUtils;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.settings.SettingsException;
+import org.elasticsearch.plugin.cloud.azure.CloudAzurePlugin;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.elasticsearch.test.ESIntegTestCase.ThirdParty;
+
+import java.util.Collection;
+
+/**
+ * Base class for Azure tests that require credentials.
+ * <p>
+ * You must specify {@code -Dtests.thirdparty=true -Dtests.config=/path/to/config}
+ * in order to run these tests.
+ */
+@ThirdParty
+public abstract class AbstractAzureTestCase extends ESIntegTestCase {
+
+    @Override
+    protected Settings nodeSettings(int nodeOrdinal) {
+        return Settings.builder()
+                .put(super.nodeSettings(nodeOrdinal))
+                .put(readSettingsFromFile())
+                .build();
+    }
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return pluginList(CloudAzurePlugin.class);
+    }
+
+    protected Settings readSettingsFromFile() {
+        Settings.Builder settings = Settings.builder();
+        settings.put("path.home", createTempDir());
+
+        // if explicit, just load it and don't load from env
+        try {
+            if (Strings.hasText(System.getProperty("tests.config"))) {
+                settings.loadFromPath(PathUtils.get((System.getProperty("tests.config"))));
+            } else {
+                throw new IllegalStateException("to run integration tests, you need to set -Dtests.thirdparty=true and -Dtests.config=/path/to/elasticsearch.yml");
+            }
+        } catch (SettingsException exception) {
+          throw new IllegalStateException("your test configuration file is incorrect: " + System.getProperty("tests.config"), exception);
+        }
+        return settings.build();
+    }
+}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureMinimumMasterNodesTest.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureMinimumMasterNodesTest.java
deleted file mode 100644
index d2d559b..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureMinimumMasterNodesTest.java
+++ /dev/null
@@ -1,90 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.azure;
-
-import org.elasticsearch.cloud.azure.AbstractAzureComputeServiceTest;
-import org.elasticsearch.cloud.azure.AzureComputeServiceTwoNodesMock;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.discovery.MasterNotDiscoveredException;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.junit.Test;
-import org.apache.lucene.util.LuceneTestCase.AwaitsFix;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
-
-/**
- * Reported issue in #15
- * (https://github.com/elasticsearch/elasticsearch-cloud-azure/issues/15)
- */
-@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.SUITE,
-        numDataNodes = 0,
-        transportClientRatio = 0.0,
-        numClientNodes = 0)
-@AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch-cloud-azure/issues/89")
-public class AzureMinimumMasterNodesTest extends AbstractAzureComputeServiceTest {
-
-    public AzureMinimumMasterNodesTest() {
-        super(AzureComputeServiceTwoNodesMock.TestPlugin.class);
-    }
-
-    @Override
-    protected Settings nodeSettings(int nodeOrdinal) {
-        Settings.Builder builder = Settings.settingsBuilder()
-                .put(super.nodeSettings(nodeOrdinal))
-                .put("discovery.zen.minimum_master_nodes", 2)
-                // Make the test run faster
-                .put("discovery.zen.join.timeout", "50ms")
-                .put("discovery.zen.ping.timeout", "10ms")
-                .put("discovery.initial_state_timeout", "100ms");
-        return builder.build();
-    }
-
-    @Test
-    public void simpleOnlyMasterNodeElection() throws IOException {
-        logger.info("--> start data node / non master node");
-        internalCluster().startNode();
-        try {
-            assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("100ms").execute().actionGet().getState().nodes().masterNodeId(), nullValue());
-            fail("should not be able to find master");
-        } catch (MasterNotDiscoveredException e) {
-            // all is well, no master elected
-        }
-        logger.info("--> start another node");
-        internalCluster().startNode();
-        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
-
-        logger.info("--> stop master node");
-        internalCluster().stopCurrentMasterNode();
-
-        try {
-            assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), nullValue());
-            fail("should not be able to find master");
-        } catch (MasterNotDiscoveredException e) {
-            // all is well, no master elected
-        }
-
-        logger.info("--> start another node");
-        internalCluster().startNode();
-        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureMinimumMasterNodesTests.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureMinimumMasterNodesTests.java
new file mode 100644
index 0000000..8465136
--- /dev/null
+++ b/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureMinimumMasterNodesTests.java
@@ -0,0 +1,90 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.azure;
+
+import org.elasticsearch.cloud.azure.AbstractAzureComputeServiceTestCase;
+import org.elasticsearch.cloud.azure.AzureComputeServiceTwoNodesMock;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.discovery.MasterNotDiscoveredException;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.junit.Test;
+import org.apache.lucene.util.LuceneTestCase.AwaitsFix;
+
+import java.io.IOException;
+
+import static org.hamcrest.Matchers.notNullValue;
+import static org.hamcrest.Matchers.nullValue;
+
+/**
+ * Reported issue in #15
+ * (https://github.com/elasticsearch/elasticsearch-cloud-azure/issues/15)
+ */
+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.SUITE,
+        numDataNodes = 0,
+        transportClientRatio = 0.0,
+        numClientNodes = 0)
+@AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch-cloud-azure/issues/89")
+public class AzureMinimumMasterNodesTests extends AbstractAzureComputeServiceTestCase {
+
+    public AzureMinimumMasterNodesTests() {
+        super(AzureComputeServiceTwoNodesMock.TestPlugin.class);
+    }
+
+    @Override
+    protected Settings nodeSettings(int nodeOrdinal) {
+        Settings.Builder builder = Settings.settingsBuilder()
+                .put(super.nodeSettings(nodeOrdinal))
+                .put("discovery.zen.minimum_master_nodes", 2)
+                // Make the test run faster
+                .put("discovery.zen.join.timeout", "50ms")
+                .put("discovery.zen.ping.timeout", "10ms")
+                .put("discovery.initial_state_timeout", "100ms");
+        return builder.build();
+    }
+
+    @Test
+    public void simpleOnlyMasterNodeElection() throws IOException {
+        logger.info("--> start data node / non master node");
+        internalCluster().startNode();
+        try {
+            assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("100ms").execute().actionGet().getState().nodes().masterNodeId(), nullValue());
+            fail("should not be able to find master");
+        } catch (MasterNotDiscoveredException e) {
+            // all is well, no master elected
+        }
+        logger.info("--> start another node");
+        internalCluster().startNode();
+        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
+
+        logger.info("--> stop master node");
+        internalCluster().stopCurrentMasterNode();
+
+        try {
+            assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), nullValue());
+            fail("should not be able to find master");
+        } catch (MasterNotDiscoveredException e) {
+            // all is well, no master elected
+        }
+
+        logger.info("--> start another node");
+        internalCluster().startNode();
+        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
+    }
+}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureSimpleTest.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureSimpleTest.java
deleted file mode 100644
index e46fadd..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureSimpleTest.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.azure;
-
-import org.elasticsearch.cloud.azure.AbstractAzureComputeServiceTest;
-import org.elasticsearch.cloud.azure.management.AzureComputeService.Discovery;
-import org.elasticsearch.cloud.azure.management.AzureComputeService.Management;
-import org.elasticsearch.cloud.azure.AzureComputeServiceSimpleMock;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.junit.Test;
-
-import static org.hamcrest.Matchers.notNullValue;
-
-@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST,
-        numDataNodes = 0,
-        transportClientRatio = 0.0,
-        numClientNodes = 0)
-public class AzureSimpleTest extends AbstractAzureComputeServiceTest {
-
-    public AzureSimpleTest() {
-        super(AzureComputeServiceSimpleMock.TestPlugin.class);
-    }
-
-    @Test
-    public void one_node_should_run_using_private_ip() {
-        Settings.Builder settings = Settings.settingsBuilder()
-                .put(Management.SERVICE_NAME, "dummy")
-                .put(Discovery.HOST_TYPE, "private_ip");
-
-        logger.info("--> start one node");
-        internalCluster().startNode(settings);
-        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
-
-        // We expect having 1 node as part of the cluster, let's test that
-        checkNumberOfNodes(1);
-    }
-
-    @Test
-    public void one_node_should_run_using_public_ip() {
-        Settings.Builder settings = Settings.settingsBuilder()
-                .put(Management.SERVICE_NAME, "dummy")
-                .put(Discovery.HOST_TYPE, "public_ip");
-
-        logger.info("--> start one node");
-        internalCluster().startNode(settings);
-        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
-
-        // We expect having 1 node as part of the cluster, let's test that
-        checkNumberOfNodes(1);
-    }
-
-    @Test
-    public void one_node_should_run_using_wrong_settings() {
-        Settings.Builder settings = Settings.settingsBuilder()
-                .put(Management.SERVICE_NAME, "dummy")
-                .put(Discovery.HOST_TYPE, "do_not_exist");
-
-        logger.info("--> start one node");
-        internalCluster().startNode(settings);
-        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
-
-        // We expect having 1 node as part of the cluster, let's test that
-        checkNumberOfNodes(1);
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureSimpleTests.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureSimpleTests.java
new file mode 100644
index 0000000..74daf1a
--- /dev/null
+++ b/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureSimpleTests.java
@@ -0,0 +1,83 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.azure;
+
+import org.elasticsearch.cloud.azure.AbstractAzureComputeServiceTestCase;
+import org.elasticsearch.cloud.azure.management.AzureComputeService.Discovery;
+import org.elasticsearch.cloud.azure.management.AzureComputeService.Management;
+import org.elasticsearch.cloud.azure.AzureComputeServiceSimpleMock;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.junit.Test;
+
+import static org.hamcrest.Matchers.notNullValue;
+
+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST,
+        numDataNodes = 0,
+        transportClientRatio = 0.0,
+        numClientNodes = 0)
+public class AzureSimpleTests extends AbstractAzureComputeServiceTestCase {
+
+    public AzureSimpleTests() {
+        super(AzureComputeServiceSimpleMock.TestPlugin.class);
+    }
+
+    @Test
+    public void one_node_should_run_using_private_ip() {
+        Settings.Builder settings = Settings.settingsBuilder()
+                .put(Management.SERVICE_NAME, "dummy")
+                .put(Discovery.HOST_TYPE, "private_ip");
+
+        logger.info("--> start one node");
+        internalCluster().startNode(settings);
+        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
+
+        // We expect having 1 node as part of the cluster, let's test that
+        checkNumberOfNodes(1);
+    }
+
+    @Test
+    public void one_node_should_run_using_public_ip() {
+        Settings.Builder settings = Settings.settingsBuilder()
+                .put(Management.SERVICE_NAME, "dummy")
+                .put(Discovery.HOST_TYPE, "public_ip");
+
+        logger.info("--> start one node");
+        internalCluster().startNode(settings);
+        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
+
+        // We expect having 1 node as part of the cluster, let's test that
+        checkNumberOfNodes(1);
+    }
+
+    @Test
+    public void one_node_should_run_using_wrong_settings() {
+        Settings.Builder settings = Settings.settingsBuilder()
+                .put(Management.SERVICE_NAME, "dummy")
+                .put(Discovery.HOST_TYPE, "do_not_exist");
+
+        logger.info("--> start one node");
+        internalCluster().startNode(settings);
+        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
+
+        // We expect having 1 node as part of the cluster, let's test that
+        checkNumberOfNodes(1);
+    }
+}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureTwoStartedNodesTest.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureTwoStartedNodesTest.java
deleted file mode 100644
index 3b6287c..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureTwoStartedNodesTest.java
+++ /dev/null
@@ -1,79 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.azure;
-
-import org.elasticsearch.cloud.azure.AbstractAzureComputeServiceTest;
-import org.elasticsearch.cloud.azure.management.AzureComputeService.Discovery;
-import org.elasticsearch.cloud.azure.management.AzureComputeService.Management;
-import org.elasticsearch.cloud.azure.AzureComputeServiceTwoNodesMock;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.junit.Test;
-
-import static org.hamcrest.Matchers.notNullValue;
-
-@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST,
-        numDataNodes = 0,
-        transportClientRatio = 0.0,
-        numClientNodes = 0)
-public class AzureTwoStartedNodesTest extends AbstractAzureComputeServiceTest {
-
-    public AzureTwoStartedNodesTest() {
-        super(AzureComputeServiceTwoNodesMock.TestPlugin.class);
-    }
-
-    @Test
-    @AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch/issues/11533")
-    public void two_nodes_should_run_using_private_ip() {
-        Settings.Builder settings = Settings.settingsBuilder()
-                .put(Management.SERVICE_NAME, "dummy")
-                .put(Discovery.HOST_TYPE, "private_ip");
-
-        logger.info("--> start first node");
-        internalCluster().startNode(settings);
-        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
-
-        logger.info("--> start another node");
-        internalCluster().startNode(settings);
-        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
-
-        // We expect having 2 nodes as part of the cluster, let's test that
-        checkNumberOfNodes(2);
-    }
-
-    @Test
-    @AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch/issues/11533")
-    public void two_nodes_should_run_using_public_ip() {
-        Settings.Builder settings = Settings.settingsBuilder()
-                .put(Management.SERVICE_NAME, "dummy")
-                .put(Discovery.HOST_TYPE, "public_ip");
-
-        logger.info("--> start first node");
-        internalCluster().startNode(settings);
-        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
-
-        logger.info("--> start another node");
-        internalCluster().startNode(settings);
-        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
-
-        // We expect having 2 nodes as part of the cluster, let's test that
-        checkNumberOfNodes(2);
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureTwoStartedNodesTests.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureTwoStartedNodesTests.java
new file mode 100644
index 0000000..dbccc4a
--- /dev/null
+++ b/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureTwoStartedNodesTests.java
@@ -0,0 +1,79 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.azure;
+
+import org.elasticsearch.cloud.azure.AbstractAzureComputeServiceTestCase;
+import org.elasticsearch.cloud.azure.management.AzureComputeService.Discovery;
+import org.elasticsearch.cloud.azure.management.AzureComputeService.Management;
+import org.elasticsearch.cloud.azure.AzureComputeServiceTwoNodesMock;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.junit.Test;
+
+import static org.hamcrest.Matchers.notNullValue;
+
+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST,
+        numDataNodes = 0,
+        transportClientRatio = 0.0,
+        numClientNodes = 0)
+public class AzureTwoStartedNodesTests extends AbstractAzureComputeServiceTestCase {
+
+    public AzureTwoStartedNodesTests() {
+        super(AzureComputeServiceTwoNodesMock.TestPlugin.class);
+    }
+
+    @Test
+    @AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch/issues/11533")
+    public void two_nodes_should_run_using_private_ip() {
+        Settings.Builder settings = Settings.settingsBuilder()
+                .put(Management.SERVICE_NAME, "dummy")
+                .put(Discovery.HOST_TYPE, "private_ip");
+
+        logger.info("--> start first node");
+        internalCluster().startNode(settings);
+        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
+
+        logger.info("--> start another node");
+        internalCluster().startNode(settings);
+        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
+
+        // We expect having 2 nodes as part of the cluster, let's test that
+        checkNumberOfNodes(2);
+    }
+
+    @Test
+    @AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch/issues/11533")
+    public void two_nodes_should_run_using_public_ip() {
+        Settings.Builder settings = Settings.settingsBuilder()
+                .put(Management.SERVICE_NAME, "dummy")
+                .put(Discovery.HOST_TYPE, "public_ip");
+
+        logger.info("--> start first node");
+        internalCluster().startNode(settings);
+        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
+
+        logger.info("--> start another node");
+        internalCluster().startNode(settings);
+        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
+
+        // We expect having 2 nodes as part of the cluster, let's test that
+        checkNumberOfNodes(2);
+    }
+}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/AbstractAzureFsTest.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/AbstractAzureFsTest.java
deleted file mode 100644
index 04407ce..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/AbstractAzureFsTest.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.store;
-
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.plugin.cloud.azure.CloudAzurePlugin;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.junit.Test;
-
-import java.util.Collection;
-
-import static org.hamcrest.Matchers.is;
-
-abstract public class AbstractAzureFsTest extends ESIntegTestCase {
-
-    @Override
-    protected Collection<Class<? extends Plugin>> nodePlugins() {
-        return pluginList(CloudAzurePlugin.class);
-    }
-
-    @Test
-    public void testAzureFs() {
-        // Create an index and index some documents
-        createIndex("test");
-        long nbDocs = randomIntBetween(10, 1000);
-        for (long i = 0; i < nbDocs; i++) {
-            index("test", "doc", "" + i, "foo", "bar");
-        }
-        refresh();
-        SearchResponse response = client().prepareSearch("test").get();
-        assertThat(response.getHits().totalHits(), is(nbDocs));
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/AbstractAzureFsTestCase.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/AbstractAzureFsTestCase.java
new file mode 100644
index 0000000..12eacc4
--- /dev/null
+++ b/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/AbstractAzureFsTestCase.java
@@ -0,0 +1,52 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.store;
+
+import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.plugin.cloud.azure.CloudAzurePlugin;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.junit.Test;
+
+import java.util.Collection;
+
+import static org.hamcrest.Matchers.is;
+
+abstract public class AbstractAzureFsTestCase extends ESIntegTestCase {
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return pluginList(CloudAzurePlugin.class);
+    }
+
+    @Test
+    public void testAzureFs() {
+        // Create an index and index some documents
+        createIndex("test");
+        long nbDocs = randomIntBetween(10, 1000);
+        for (long i = 0; i < nbDocs; i++) {
+            index("test", "doc", "" + i, "foo", "bar");
+        }
+        refresh();
+        SearchResponse response = client().prepareSearch("test").get();
+        assertThat(response.getHits().totalHits(), is(nbDocs));
+    }
+}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/SmbMMapFsTest.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/SmbMMapFsTest.java
deleted file mode 100644
index 820f500..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/SmbMMapFsTest.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.store;
-
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.plugin.cloud.azure.CloudAzurePlugin;
-
-
-public class SmbMMapFsTest extends AbstractAzureFsTest {
-
-    @Override
-    public Settings indexSettings() {
-        return Settings.builder()
-                .put(super.indexSettings())
-                .put("index.store.type", "smb_mmap_fs")
-                .build();
-    }
-
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/SmbMMapFsTests.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/SmbMMapFsTests.java
new file mode 100644
index 0000000..0f9e874
--- /dev/null
+++ b/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/SmbMMapFsTests.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.store;
+
+import org.elasticsearch.common.settings.Settings;
+
+
+public class SmbMMapFsTests extends AbstractAzureFsTestCase {
+
+    @Override
+    public Settings indexSettings() {
+        return Settings.builder()
+                .put(super.indexSettings())
+                .put("index.store.type", "smb_mmap_fs")
+                .build();
+    }
+
+}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/SmbSimpleFsTest.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/SmbSimpleFsTest.java
deleted file mode 100644
index d4bd02f..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/SmbSimpleFsTest.java
+++ /dev/null
@@ -1,34 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.store;
-
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.plugin.cloud.azure.CloudAzurePlugin;
-
-
-public class SmbSimpleFsTest extends AbstractAzureFsTest {
-    @Override
-    public Settings indexSettings() {
-        return Settings.builder()
-                .put(super.indexSettings())
-                .put("index.store.type", "smb_simple_fs")
-                .build();
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/SmbSimpleFsTests.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/SmbSimpleFsTests.java
new file mode 100644
index 0000000..ed15718
--- /dev/null
+++ b/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/SmbSimpleFsTests.java
@@ -0,0 +1,33 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.store;
+
+import org.elasticsearch.common.settings.Settings;
+
+
+public class SmbSimpleFsTests extends AbstractAzureFsTestCase {
+    @Override
+    public Settings indexSettings() {
+        return Settings.builder()
+                .put(super.indexSettings())
+                .put("index.store.type", "smb_simple_fs")
+                .build();
+    }
+}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreITest.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreITest.java
deleted file mode 100644
index 1326a0e..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreITest.java
+++ /dev/null
@@ -1,535 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.repositories.azure;
-
-
-import com.carrotsearch.randomizedtesting.RandomizedTest;
-import com.microsoft.azure.storage.StorageException;
-import org.elasticsearch.action.admin.cluster.repositories.put.PutRepositoryResponse;
-import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;
-import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.ClusterAdminClient;
-import org.elasticsearch.cloud.azure.AbstractAzureTest;
-import org.elasticsearch.cloud.azure.storage.AzureStorageService;
-import org.elasticsearch.cloud.azure.storage.AzureStorageServiceImpl;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.ByteSizeUnit;
-import org.elasticsearch.repositories.RepositoryMissingException;
-import org.elasticsearch.repositories.RepositoryVerificationException;
-import org.elasticsearch.repositories.azure.AzureRepository.Repository;
-import org.elasticsearch.snapshots.SnapshotMissingException;
-import org.elasticsearch.snapshots.SnapshotState;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.elasticsearch.test.store.MockFSDirectoryService;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.net.URISyntaxException;
-import java.util.Locale;
-import java.util.concurrent.TimeUnit;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-
-/**
- * This test needs Azure to run and -Dtests.thirdparty=true to be set
- * and -Dtests.config=/path/to/elasticsearch.yml
- * @see org.elasticsearch.cloud.azure.AbstractAzureTest
- */
-@ESIntegTestCase.ClusterScope(
-        scope = ESIntegTestCase.Scope.SUITE,
-        numDataNodes = 1,
-        transportClientRatio = 0.0)
-public class AzureSnapshotRestoreITest extends AbstractAzureTest {
-
-    private String getRepositoryPath() {
-        String testName = "it-".concat(Strings.toUnderscoreCase(getTestName()).replaceAll("_", "-"));
-        return testName.contains(" ") ? Strings.split(testName, " ")[0] : testName;
-    }
-
-    private static String getContainerName() {
-        String testName = "snapshot-itest-".concat(RandomizedTest.getContext().getRunnerSeedAsString().toLowerCase(Locale.ROOT));
-        return testName.contains(" ") ? Strings.split(testName, " ")[0] : testName;
-    }
-
-    @Override
-    protected Settings nodeSettings(int nodeOrdinal) {
-        return Settings.builder().put(super.nodeSettings(nodeOrdinal))
-                // In snapshot tests, we explicitly disable cloud discovery
-                .put("discovery.type", "local")
-                .build();
-    }
-
-    @Override
-    public Settings indexSettings() {
-        // During restore we frequently restore index to exactly the same state it was before, that might cause the same
-        // checksum file to be written twice during restore operation
-        return Settings.builder().put(super.indexSettings())
-                .put(MockFSDirectoryService.RANDOM_PREVENT_DOUBLE_WRITE, false)
-                .put(MockFSDirectoryService.RANDOM_NO_DELETE_OPEN_FILE, false)
-                .build();
-    }
-
-    @Before @After
-    public final void wipeAzureRepositories() throws StorageException, URISyntaxException {
-        wipeRepositories();
-        cleanRepositoryFiles(
-            getContainerName(),
-            getContainerName().concat("-1"),
-            getContainerName().concat("-2"));
-    }
-
-    @Test
-    public void testSimpleWorkflow() {
-        Client client = client();
-        logger.info("-->  creating azure repository with path [{}]", getRepositoryPath());
-        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository("test-repo")
-                .setType("azure").setSettings(Settings.settingsBuilder()
-                        .put(Repository.CONTAINER, getContainerName())
-                        .put(Repository.BASE_PATH, getRepositoryPath())
-                        .put(Repository.CHUNK_SIZE, randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
-                ).get();
-        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
-
-        createIndex("test-idx-1", "test-idx-2", "test-idx-3");
-        ensureGreen();
-
-        logger.info("--> indexing some data");
-        for (int i = 0; i < 100; i++) {
-            index("test-idx-1", "doc", Integer.toString(i), "foo", "bar" + i);
-            index("test-idx-2", "doc", Integer.toString(i), "foo", "baz" + i);
-            index("test-idx-3", "doc", Integer.toString(i), "foo", "baz" + i);
-        }
-        refresh();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
-        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
-        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(100L));
-
-        logger.info("--> snapshot");
-        CreateSnapshotResponse createSnapshotResponse = client.admin().cluster().prepareCreateSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices("test-idx-*", "-test-idx-3").get();
-        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
-        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse.getSnapshotInfo().totalShards()));
-
-        assertThat(client.admin().cluster().prepareGetSnapshots("test-repo").setSnapshots("test-snap").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
-
-        logger.info("--> delete some data");
-        for (int i = 0; i < 50; i++) {
-            client.prepareDelete("test-idx-1", "doc", Integer.toString(i)).get();
-        }
-        for (int i = 50; i < 100; i++) {
-            client.prepareDelete("test-idx-2", "doc", Integer.toString(i)).get();
-        }
-        for (int i = 0; i < 100; i += 2) {
-            client.prepareDelete("test-idx-3", "doc", Integer.toString(i)).get();
-        }
-        refresh();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(50L));
-        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(50L));
-        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(50L));
-
-        logger.info("--> close indices");
-        client.admin().indices().prepareClose("test-idx-1", "test-idx-2").get();
-
-        logger.info("--> restore all indices from the snapshot");
-        RestoreSnapshotResponse restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setWaitForCompletion(true).execute().actionGet();
-        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
-
-        ensureGreen();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
-        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
-        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(50L));
-
-        // Test restore after index deletion
-        logger.info("--> delete indices");
-        cluster().wipeIndices("test-idx-1", "test-idx-2");
-        logger.info("--> restore one index after deletion");
-        restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices("test-idx-*", "-test-idx-2").execute().actionGet();
-        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
-        ensureGreen();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
-        ClusterState clusterState = client.admin().cluster().prepareState().get().getState();
-        assertThat(clusterState.getMetaData().hasIndex("test-idx-1"), equalTo(true));
-        assertThat(clusterState.getMetaData().hasIndex("test-idx-2"), equalTo(false));
-    }
-
-    /**
-     * For issue #51: https://github.com/elasticsearch/elasticsearch-cloud-azure/issues/51
-     */
-    @Test
-    public void testMultipleSnapshots() throws URISyntaxException, StorageException {
-        final String indexName = "test-idx-1";
-        final String typeName = "doc";
-        final String repositoryName = "test-repo";
-        final String snapshot1Name = "test-snap-1";
-        final String snapshot2Name = "test-snap-2";
-
-        Client client = client();
-
-        logger.info("creating index [{}]", indexName);
-        createIndex(indexName);
-        ensureGreen();
-
-        logger.info("indexing first document");
-        index(indexName, typeName, Integer.toString(1), "foo", "bar " + Integer.toString(1));
-        refresh();
-        assertThat(client.prepareCount(indexName).get().getCount(), equalTo(1L));
-
-        logger.info("creating Azure repository with path [{}]", getRepositoryPath());
-        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository(repositoryName)
-                .setType("azure").setSettings(Settings.settingsBuilder()
-                                .put(Repository.CONTAINER, getContainerName())
-                                .put(Repository.BASE_PATH, getRepositoryPath())
-                                .put(Repository.BASE_PATH, randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
-                ).get();
-        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
-
-        logger.info("creating snapshot [{}]", snapshot1Name);
-        CreateSnapshotResponse createSnapshotResponse1 = client.admin().cluster().prepareCreateSnapshot(repositoryName, snapshot1Name).setWaitForCompletion(true).setIndices(indexName).get();
-        assertThat(createSnapshotResponse1.getSnapshotInfo().successfulShards(), greaterThan(0));
-        assertThat(createSnapshotResponse1.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse1.getSnapshotInfo().totalShards()));
-
-        assertThat(client.admin().cluster().prepareGetSnapshots(repositoryName).setSnapshots(snapshot1Name).get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
-
-        logger.info("indexing second document");
-        index(indexName, typeName, Integer.toString(2), "foo", "bar " + Integer.toString(2));
-        refresh();
-        assertThat(client.prepareCount(indexName).get().getCount(), equalTo(2L));
-
-        logger.info("creating snapshot [{}]", snapshot2Name);
-        CreateSnapshotResponse createSnapshotResponse2 = client.admin().cluster().prepareCreateSnapshot(repositoryName, snapshot2Name).setWaitForCompletion(true).setIndices(indexName).get();
-        assertThat(createSnapshotResponse2.getSnapshotInfo().successfulShards(), greaterThan(0));
-        assertThat(createSnapshotResponse2.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse2.getSnapshotInfo().totalShards()));
-
-        assertThat(client.admin().cluster().prepareGetSnapshots(repositoryName).setSnapshots(snapshot2Name).get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
-
-        logger.info("closing index [{}]", indexName);
-        client.admin().indices().prepareClose(indexName).get();
-
-        logger.info("attempting restore from snapshot [{}]", snapshot1Name);
-        RestoreSnapshotResponse restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot(repositoryName, snapshot1Name).setWaitForCompletion(true).execute().actionGet();
-        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
-        ensureGreen();
-        assertThat(client.prepareCount(indexName).get().getCount(), equalTo(1L));
-    }
-
-    @Test
-    public void testMultipleRepositories() {
-        Client client = client();
-        logger.info("-->  creating azure repository with path [{}]", getRepositoryPath());
-        PutRepositoryResponse putRepositoryResponse1 = client.admin().cluster().preparePutRepository("test-repo1")
-                .setType("azure").setSettings(Settings.settingsBuilder()
-                        .put(Repository.CONTAINER, getContainerName().concat("-1"))
-                        .put(Repository.BASE_PATH, getRepositoryPath())
-                        .put(Repository.CHUNK_SIZE, randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
-                ).get();
-        assertThat(putRepositoryResponse1.isAcknowledged(), equalTo(true));
-        PutRepositoryResponse putRepositoryResponse2 = client.admin().cluster().preparePutRepository("test-repo2")
-                .setType("azure").setSettings(Settings.settingsBuilder()
-                        .put(Repository.CONTAINER, getContainerName().concat("-2"))
-                        .put(Repository.BASE_PATH, getRepositoryPath())
-                        .put(Repository.CHUNK_SIZE, randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
-                ).get();
-        assertThat(putRepositoryResponse2.isAcknowledged(), equalTo(true));
-
-        createIndex("test-idx-1", "test-idx-2");
-        ensureGreen();
-
-        logger.info("--> indexing some data");
-        for (int i = 0; i < 100; i++) {
-            index("test-idx-1", "doc", Integer.toString(i), "foo", "bar" + i);
-            index("test-idx-2", "doc", Integer.toString(i), "foo", "baz" + i);
-        }
-        refresh();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
-        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
-
-        logger.info("--> snapshot 1");
-        CreateSnapshotResponse createSnapshotResponse1 = client.admin().cluster().prepareCreateSnapshot("test-repo1", "test-snap").setWaitForCompletion(true).setIndices("test-idx-1").get();
-        assertThat(createSnapshotResponse1.getSnapshotInfo().successfulShards(), greaterThan(0));
-        assertThat(createSnapshotResponse1.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse1.getSnapshotInfo().totalShards()));
-
-        logger.info("--> snapshot 2");
-        CreateSnapshotResponse createSnapshotResponse2 = client.admin().cluster().prepareCreateSnapshot("test-repo2", "test-snap").setWaitForCompletion(true).setIndices("test-idx-2").get();
-        assertThat(createSnapshotResponse2.getSnapshotInfo().successfulShards(), greaterThan(0));
-        assertThat(createSnapshotResponse2.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse2.getSnapshotInfo().totalShards()));
-
-        assertThat(client.admin().cluster().prepareGetSnapshots("test-repo1").setSnapshots("test-snap").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
-        assertThat(client.admin().cluster().prepareGetSnapshots("test-repo2").setSnapshots("test-snap").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
-
-        // Test restore after index deletion
-        logger.info("--> delete indices");
-        cluster().wipeIndices("test-idx-1", "test-idx-2");
-        logger.info("--> restore one index after deletion from snapshot 1");
-        RestoreSnapshotResponse restoreSnapshotResponse1 = client.admin().cluster().prepareRestoreSnapshot("test-repo1", "test-snap").setWaitForCompletion(true).setIndices("test-idx-1").execute().actionGet();
-        assertThat(restoreSnapshotResponse1.getRestoreInfo().totalShards(), greaterThan(0));
-        ensureGreen();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
-        ClusterState clusterState = client.admin().cluster().prepareState().get().getState();
-        assertThat(clusterState.getMetaData().hasIndex("test-idx-1"), equalTo(true));
-        assertThat(clusterState.getMetaData().hasIndex("test-idx-2"), equalTo(false));
-
-        logger.info("--> restore other index after deletion from snapshot 2");
-        RestoreSnapshotResponse restoreSnapshotResponse2 = client.admin().cluster().prepareRestoreSnapshot("test-repo2", "test-snap").setWaitForCompletion(true).setIndices("test-idx-2").execute().actionGet();
-        assertThat(restoreSnapshotResponse2.getRestoreInfo().totalShards(), greaterThan(0));
-        ensureGreen();
-        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
-        clusterState = client.admin().cluster().prepareState().get().getState();
-        assertThat(clusterState.getMetaData().hasIndex("test-idx-1"), equalTo(true));
-        assertThat(clusterState.getMetaData().hasIndex("test-idx-2"), equalTo(true));
-    }
-
-    /**
-     * For issue #26: https://github.com/elasticsearch/elasticsearch-cloud-azure/issues/26
-     */
-    @Test
-    public void testListBlobs_26() throws StorageException, URISyntaxException {
-        createIndex("test-idx-1", "test-idx-2", "test-idx-3");
-        ensureGreen();
-
-        logger.info("--> indexing some data");
-        for (int i = 0; i < 100; i++) {
-            index("test-idx-1", "doc", Integer.toString(i), "foo", "bar" + i);
-            index("test-idx-2", "doc", Integer.toString(i), "foo", "baz" + i);
-            index("test-idx-3", "doc", Integer.toString(i), "foo", "baz" + i);
-        }
-        refresh();
-
-        ClusterAdminClient client = client().admin().cluster();
-        logger.info("-->  creating azure repository without any path");
-        PutRepositoryResponse putRepositoryResponse = client.preparePutRepository("test-repo").setType("azure")
-                .setSettings(Settings.settingsBuilder()
-                        .put(Repository.CONTAINER, getContainerName())
-                ).get();
-        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
-
-        // Get all snapshots - should be empty
-        assertThat(client.prepareGetSnapshots("test-repo").get().getSnapshots().size(), equalTo(0));
-
-        logger.info("--> snapshot");
-        CreateSnapshotResponse createSnapshotResponse = client.prepareCreateSnapshot("test-repo", "test-snap-26").setWaitForCompletion(true).setIndices("test-idx-*").get();
-        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
-
-        // Get all snapshots - should have one
-        assertThat(client.prepareGetSnapshots("test-repo").get().getSnapshots().size(), equalTo(1));
-
-        // Clean the snapshot
-        client.prepareDeleteSnapshot("test-repo", "test-snap-26").get();
-        client.prepareDeleteRepository("test-repo").get();
-
-        logger.info("-->  creating azure repository path [{}]", getRepositoryPath());
-        putRepositoryResponse = client.preparePutRepository("test-repo").setType("azure")
-                .setSettings(Settings.settingsBuilder()
-                        .put(Repository.CONTAINER, getContainerName())
-                        .put(Repository.BASE_PATH, getRepositoryPath())
-        ).get();
-        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
-
-        // Get all snapshots - should be empty
-        assertThat(client.prepareGetSnapshots("test-repo").get().getSnapshots().size(), equalTo(0));
-
-        logger.info("--> snapshot");
-        createSnapshotResponse = client.prepareCreateSnapshot("test-repo", "test-snap-26").setWaitForCompletion(true).setIndices("test-idx-*").get();
-        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
-
-        // Get all snapshots - should have one
-        assertThat(client.prepareGetSnapshots("test-repo").get().getSnapshots().size(), equalTo(1));
-
-
-    }
-
-    /**
-     * For issue #28: https://github.com/elasticsearch/elasticsearch-cloud-azure/issues/28
-     */
-    @Test
-    public void testGetDeleteNonExistingSnapshot_28() throws StorageException, URISyntaxException {
-        ClusterAdminClient client = client().admin().cluster();
-        logger.info("-->  creating azure repository without any path");
-        PutRepositoryResponse putRepositoryResponse = client.preparePutRepository("test-repo").setType("azure")
-                .setSettings(Settings.settingsBuilder()
-                        .put(Repository.CONTAINER, getContainerName())
-                ).get();
-        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
-
-        try {
-            client.prepareGetSnapshots("test-repo").addSnapshots("nonexistingsnapshotname").get();
-            fail("Shouldn't be here");
-        } catch (SnapshotMissingException ex) {
-            // Expected
-        }
-
-        try {
-            client.prepareDeleteSnapshot("test-repo", "nonexistingsnapshotname").get();
-            fail("Shouldn't be here");
-        } catch (SnapshotMissingException ex) {
-            // Expected
-        }
-    }
-
-    /**
-     * For issue #21: https://github.com/elasticsearch/elasticsearch-cloud-azure/issues/21
-     */
-    @Test
-    public void testForbiddenContainerName() throws Exception {
-        checkContainerName("", false);
-        checkContainerName("es", false);
-        checkContainerName("-elasticsearch", false);
-        checkContainerName("elasticsearch--integration", false);
-        checkContainerName("elasticsearch_integration", false);
-        checkContainerName("ElAsTicsearch_integration", false);
-        checkContainerName("123456789-123456789-123456789-123456789-123456789-123456789-1234", false);
-        checkContainerName("123456789-123456789-123456789-123456789-123456789-123456789-123", true);
-        checkContainerName("elasticsearch-integration", true);
-        checkContainerName("elasticsearch-integration-007", true);
-    }
-
-    /**
-     * Create repository with wrong or correct container name
-     * @param container Container name we want to create
-     * @param correct Is this container name correct
-     */
-    private void checkContainerName(final String container, final boolean correct) throws Exception {
-        logger.info("-->  creating azure repository with container name [{}]", container);
-        // It could happen that we just removed from a previous test the same container so
-        // we can not create it yet.
-        assertBusy(new Runnable() {
-
-            public void run() {
-                try {
-                    PutRepositoryResponse putRepositoryResponse = client().admin().cluster().preparePutRepository("test-repo")
-                            .setType("azure").setSettings(Settings.settingsBuilder()
-                                            .put(Repository.CONTAINER, container)
-                                            .put(Repository.BASE_PATH, getRepositoryPath())
-                                            .put(Repository.CHUNK_SIZE, randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
-                            ).get();
-                    client().admin().cluster().prepareDeleteRepository("test-repo").get();
-                    try {
-                        logger.info("--> remove container [{}]", container);
-                        cleanRepositoryFiles(container);
-                    } catch (StorageException | URISyntaxException e) {
-                        // We can ignore that as we just try to clean after the test
-                    }
-                    assertTrue(putRepositoryResponse.isAcknowledged() == correct);
-                } catch (RepositoryVerificationException e) {
-                    if (correct) {
-                        logger.debug(" -> container is being removed. Let's wait a bit...");
-                        fail();
-                    }
-                }
-            }
-        }, 5, TimeUnit.MINUTES);
-    }
-
-    /**
-     * Test case for issue #23: https://github.com/elasticsearch/elasticsearch-cloud-azure/issues/23
-     */
-    @Test
-    public void testNonExistingRepo_23() {
-        Client client = client();
-        logger.info("-->  creating azure repository with path [{}]", getRepositoryPath());
-        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository("test-repo")
-                .setType("azure").setSettings(Settings.settingsBuilder()
-                        .put(Repository.CONTAINER, getContainerName())
-                        .put(Repository.BASE_PATH, getRepositoryPath())
-                        .put(Repository.CHUNK_SIZE, randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
-                ).get();
-        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
-
-        logger.info("--> restore non existing snapshot");
-        try {
-            client.admin().cluster().prepareRestoreSnapshot("test-repo", "no-existing-snapshot").setWaitForCompletion(true).execute().actionGet();
-            fail("Shouldn't be here");
-        } catch (SnapshotMissingException ex) {
-            // Expected
-        }
-    }
-
-    /**
-     * When a user remove a container you can not immediately create it again.
-     */
-    @Test
-    public void testRemoveAndCreateContainer() throws Exception {
-        final String container = getContainerName().concat("-testremove");
-        final AzureStorageService storageService = internalCluster().getInstance(AzureStorageService.class);
-
-        // It could happen that we run this test really close to a previous one
-        // so we might need some time to be able to create the container
-        assertBusy(new Runnable() {
-
-            public void run()  {
-                try {
-                    storageService.createContainer(container);
-                    logger.debug(" -> container created...");
-                } catch (URISyntaxException e) {
-                    // Incorrect URL. This should never happen.
-                    fail();
-                } catch (StorageException e) {
-                    // It could happen. Let's wait for a while.
-                    logger.debug(" -> container is being removed. Let's wait a bit...");
-                    fail();
-                }
-            }
-        }, 30, TimeUnit.SECONDS);
-        storageService.removeContainer(container);
-
-        ClusterAdminClient client = client().admin().cluster();
-        logger.info("-->  creating azure repository while container is being removed");
-        try {
-            client.preparePutRepository("test-repo").setType("azure")
-                    .setSettings(Settings.settingsBuilder()
-                            .put(Repository.CONTAINER, container)
-                    ).get();
-            fail("we should get a RepositoryVerificationException");
-        } catch (RepositoryVerificationException e) {
-            // Fine we expect that
-        }
-    }
-
-    /**
-     * Deletes repositories, supports wildcard notation.
-     */
-    public static void wipeRepositories(String... repositories) {
-        // if nothing is provided, delete all
-        if (repositories.length == 0) {
-            repositories = new String[]{"*"};
-        }
-        for (String repository : repositories) {
-            try {
-                client().admin().cluster().prepareDeleteRepository(repository).execute().actionGet();
-            } catch (RepositoryMissingException ex) {
-                // ignore
-            }
-        }
-    }
-
-    /**
-     * Purge the test containers
-     */
-    public void cleanRepositoryFiles(String... containers) throws StorageException, URISyntaxException {
-        Settings settings = readSettingsFromFile();
-        AzureStorageService client = new AzureStorageServiceImpl(settings);
-        for (String container : containers) {
-            client.removeContainer(container);
-        }
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreServiceTests.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreServiceTests.java
new file mode 100644
index 0000000..37a21a3
--- /dev/null
+++ b/plugins/cloud-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreServiceTests.java
@@ -0,0 +1,121 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.repositories.azure;
+
+
+import org.elasticsearch.action.admin.cluster.repositories.put.PutRepositoryResponse;
+import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;
+import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse;
+import org.elasticsearch.client.Client;
+import org.elasticsearch.cloud.azure.AbstractAzureRepositoryServiceTestCase;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.ByteSizeUnit;
+import org.elasticsearch.snapshots.SnapshotState;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.junit.Test;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.greaterThan;
+
+@ESIntegTestCase.ClusterScope(
+        scope = ESIntegTestCase.Scope.SUITE,
+        numDataNodes = 1,
+        numClientNodes = 0,
+        transportClientRatio = 0.0)
+public class AzureSnapshotRestoreServiceTests extends AbstractAzureRepositoryServiceTestCase {
+
+    public AzureSnapshotRestoreServiceTests() {
+        super("/snapshot-test/repo-" + randomInt());
+    }
+
+    @Test
+    public void testSimpleWorkflow() {
+        Client client = client();
+        logger.info("-->  creating azure repository with path [{}]", basePath);
+        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository("test-repo")
+                .setType("azure").setSettings(Settings.settingsBuilder()
+                        .put("base_path", basePath)
+                        .put("chunk_size", randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
+                ).get();
+        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
+
+        createIndex("test-idx-1", "test-idx-2", "test-idx-3");
+        ensureGreen();
+
+        logger.info("--> indexing some data");
+        for (int i = 0; i < 100; i++) {
+            index("test-idx-1", "doc", Integer.toString(i), "foo", "bar" + i);
+            index("test-idx-2", "doc", Integer.toString(i), "foo", "baz" + i);
+            index("test-idx-3", "doc", Integer.toString(i), "foo", "baz" + i);
+        }
+        refresh();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
+        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
+        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(100L));
+
+        logger.info("--> snapshot");
+        CreateSnapshotResponse createSnapshotResponse = client.admin().cluster().prepareCreateSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices("test-idx-*", "-test-idx-3").get();
+        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
+        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse.getSnapshotInfo().totalShards()));
+
+        assertThat(client.admin().cluster().prepareGetSnapshots("test-repo").setSnapshots("test-snap").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
+
+        logger.info("--> delete some data");
+        for (int i = 0; i < 50; i++) {
+            client.prepareDelete("test-idx-1", "doc", Integer.toString(i)).get();
+        }
+        for (int i = 50; i < 100; i++) {
+            client.prepareDelete("test-idx-2", "doc", Integer.toString(i)).get();
+        }
+        for (int i = 0; i < 100; i += 2) {
+            client.prepareDelete("test-idx-3", "doc", Integer.toString(i)).get();
+        }
+        refresh();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(50L));
+        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(50L));
+        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(50L));
+
+        logger.info("--> close indices");
+        client.admin().indices().prepareClose("test-idx-1", "test-idx-2").get();
+
+        logger.info("--> restore all indices from the snapshot");
+        RestoreSnapshotResponse restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setWaitForCompletion(true).execute().actionGet();
+        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
+
+        ensureGreen();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
+        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
+        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(50L));
+
+        // Test restore after index deletion
+        logger.info("--> delete indices");
+        cluster().wipeIndices("test-idx-1", "test-idx-2");
+        logger.info("--> restore one index after deletion");
+        restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices("test-idx-*", "-test-idx-2").execute().actionGet();
+        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
+        ensureGreen();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
+        ClusterState clusterState = client.admin().cluster().prepareState().get().getState();
+        assertThat(clusterState.getMetaData().hasIndex("test-idx-1"), equalTo(true));
+        assertThat(clusterState.getMetaData().hasIndex("test-idx-2"), equalTo(false));
+    }
+
+}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreTest.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreTest.java
deleted file mode 100644
index fec61b8..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreTest.java
+++ /dev/null
@@ -1,122 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.repositories.azure;
-
-
-import org.elasticsearch.action.admin.cluster.repositories.put.PutRepositoryResponse;
-import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;
-import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.cloud.azure.AbstractAzureRepositoryServiceTest;
-import org.elasticsearch.cloud.azure.storage.AzureStorageServiceMock;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.ByteSizeUnit;
-import org.elasticsearch.snapshots.SnapshotState;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.junit.Test;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-
-@ESIntegTestCase.ClusterScope(
-        scope = ESIntegTestCase.Scope.SUITE,
-        numDataNodes = 1,
-        numClientNodes = 0,
-        transportClientRatio = 0.0)
-public class AzureSnapshotRestoreTest extends AbstractAzureRepositoryServiceTest {
-
-    public AzureSnapshotRestoreTest() {
-        super("/snapshot-test/repo-" + randomInt());
-    }
-
-    @Test
-    public void testSimpleWorkflow() {
-        Client client = client();
-        logger.info("-->  creating azure repository with path [{}]", basePath);
-        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository("test-repo")
-                .setType("azure").setSettings(Settings.settingsBuilder()
-                        .put("base_path", basePath)
-                        .put("chunk_size", randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
-                ).get();
-        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
-
-        createIndex("test-idx-1", "test-idx-2", "test-idx-3");
-        ensureGreen();
-
-        logger.info("--> indexing some data");
-        for (int i = 0; i < 100; i++) {
-            index("test-idx-1", "doc", Integer.toString(i), "foo", "bar" + i);
-            index("test-idx-2", "doc", Integer.toString(i), "foo", "baz" + i);
-            index("test-idx-3", "doc", Integer.toString(i), "foo", "baz" + i);
-        }
-        refresh();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
-        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
-        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(100L));
-
-        logger.info("--> snapshot");
-        CreateSnapshotResponse createSnapshotResponse = client.admin().cluster().prepareCreateSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices("test-idx-*", "-test-idx-3").get();
-        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
-        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse.getSnapshotInfo().totalShards()));
-
-        assertThat(client.admin().cluster().prepareGetSnapshots("test-repo").setSnapshots("test-snap").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
-
-        logger.info("--> delete some data");
-        for (int i = 0; i < 50; i++) {
-            client.prepareDelete("test-idx-1", "doc", Integer.toString(i)).get();
-        }
-        for (int i = 50; i < 100; i++) {
-            client.prepareDelete("test-idx-2", "doc", Integer.toString(i)).get();
-        }
-        for (int i = 0; i < 100; i += 2) {
-            client.prepareDelete("test-idx-3", "doc", Integer.toString(i)).get();
-        }
-        refresh();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(50L));
-        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(50L));
-        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(50L));
-
-        logger.info("--> close indices");
-        client.admin().indices().prepareClose("test-idx-1", "test-idx-2").get();
-
-        logger.info("--> restore all indices from the snapshot");
-        RestoreSnapshotResponse restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setWaitForCompletion(true).execute().actionGet();
-        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
-
-        ensureGreen();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
-        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
-        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(50L));
-
-        // Test restore after index deletion
-        logger.info("--> delete indices");
-        cluster().wipeIndices("test-idx-1", "test-idx-2");
-        logger.info("--> restore one index after deletion");
-        restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices("test-idx-*", "-test-idx-2").execute().actionGet();
-        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
-        ensureGreen();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
-        ClusterState clusterState = client.admin().cluster().prepareState().get().getState();
-        assertThat(clusterState.getMetaData().hasIndex("test-idx-1"), equalTo(true));
-        assertThat(clusterState.getMetaData().hasIndex("test-idx-2"), equalTo(false));
-    }
-
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreTests.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreTests.java
new file mode 100644
index 0000000..6f8d0d0
--- /dev/null
+++ b/plugins/cloud-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreTests.java
@@ -0,0 +1,535 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.repositories.azure;
+
+
+import com.carrotsearch.randomizedtesting.RandomizedTest;
+import com.microsoft.azure.storage.StorageException;
+import org.elasticsearch.action.admin.cluster.repositories.put.PutRepositoryResponse;
+import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;
+import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse;
+import org.elasticsearch.client.Client;
+import org.elasticsearch.client.ClusterAdminClient;
+import org.elasticsearch.cloud.azure.AbstractAzureTestCase;
+import org.elasticsearch.cloud.azure.storage.AzureStorageService;
+import org.elasticsearch.cloud.azure.storage.AzureStorageServiceImpl;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.ByteSizeUnit;
+import org.elasticsearch.repositories.RepositoryMissingException;
+import org.elasticsearch.repositories.RepositoryVerificationException;
+import org.elasticsearch.repositories.azure.AzureRepository.Repository;
+import org.elasticsearch.snapshots.SnapshotMissingException;
+import org.elasticsearch.snapshots.SnapshotState;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.elasticsearch.test.store.MockFSDirectoryService;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import java.net.URISyntaxException;
+import java.util.Locale;
+import java.util.concurrent.TimeUnit;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.greaterThan;
+
+/**
+ * This test needs Azure to run and -Dtests.thirdparty=true to be set
+ * and -Dtests.config=/path/to/elasticsearch.yml
+ * @see AbstractAzureTestCase
+ */
+@ESIntegTestCase.ClusterScope(
+        scope = ESIntegTestCase.Scope.SUITE,
+        numDataNodes = 1,
+        transportClientRatio = 0.0)
+public class AzureSnapshotRestoreTests extends AbstractAzureTestCase {
+
+    private String getRepositoryPath() {
+        String testName = "it-".concat(Strings.toUnderscoreCase(getTestName()).replaceAll("_", "-"));
+        return testName.contains(" ") ? Strings.split(testName, " ")[0] : testName;
+    }
+
+    private static String getContainerName() {
+        String testName = "snapshot-itest-".concat(RandomizedTest.getContext().getRunnerSeedAsString().toLowerCase(Locale.ROOT));
+        return testName.contains(" ") ? Strings.split(testName, " ")[0] : testName;
+    }
+
+    @Override
+    protected Settings nodeSettings(int nodeOrdinal) {
+        return Settings.builder().put(super.nodeSettings(nodeOrdinal))
+                // In snapshot tests, we explicitly disable cloud discovery
+                .put("discovery.type", "local")
+                .build();
+    }
+
+    @Override
+    public Settings indexSettings() {
+        // During restore we frequently restore index to exactly the same state it was before, that might cause the same
+        // checksum file to be written twice during restore operation
+        return Settings.builder().put(super.indexSettings())
+                .put(MockFSDirectoryService.RANDOM_PREVENT_DOUBLE_WRITE, false)
+                .put(MockFSDirectoryService.RANDOM_NO_DELETE_OPEN_FILE, false)
+                .build();
+    }
+
+    @Before @After
+    public final void wipeAzureRepositories() throws StorageException, URISyntaxException {
+        wipeRepositories();
+        cleanRepositoryFiles(
+            getContainerName(),
+            getContainerName().concat("-1"),
+            getContainerName().concat("-2"));
+    }
+
+    @Test
+    public void testSimpleWorkflow() {
+        Client client = client();
+        logger.info("-->  creating azure repository with path [{}]", getRepositoryPath());
+        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository("test-repo")
+                .setType("azure").setSettings(Settings.settingsBuilder()
+                        .put(Repository.CONTAINER, getContainerName())
+                        .put(Repository.BASE_PATH, getRepositoryPath())
+                        .put(Repository.CHUNK_SIZE, randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
+                ).get();
+        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
+
+        createIndex("test-idx-1", "test-idx-2", "test-idx-3");
+        ensureGreen();
+
+        logger.info("--> indexing some data");
+        for (int i = 0; i < 100; i++) {
+            index("test-idx-1", "doc", Integer.toString(i), "foo", "bar" + i);
+            index("test-idx-2", "doc", Integer.toString(i), "foo", "baz" + i);
+            index("test-idx-3", "doc", Integer.toString(i), "foo", "baz" + i);
+        }
+        refresh();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
+        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
+        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(100L));
+
+        logger.info("--> snapshot");
+        CreateSnapshotResponse createSnapshotResponse = client.admin().cluster().prepareCreateSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices("test-idx-*", "-test-idx-3").get();
+        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
+        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse.getSnapshotInfo().totalShards()));
+
+        assertThat(client.admin().cluster().prepareGetSnapshots("test-repo").setSnapshots("test-snap").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
+
+        logger.info("--> delete some data");
+        for (int i = 0; i < 50; i++) {
+            client.prepareDelete("test-idx-1", "doc", Integer.toString(i)).get();
+        }
+        for (int i = 50; i < 100; i++) {
+            client.prepareDelete("test-idx-2", "doc", Integer.toString(i)).get();
+        }
+        for (int i = 0; i < 100; i += 2) {
+            client.prepareDelete("test-idx-3", "doc", Integer.toString(i)).get();
+        }
+        refresh();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(50L));
+        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(50L));
+        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(50L));
+
+        logger.info("--> close indices");
+        client.admin().indices().prepareClose("test-idx-1", "test-idx-2").get();
+
+        logger.info("--> restore all indices from the snapshot");
+        RestoreSnapshotResponse restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setWaitForCompletion(true).execute().actionGet();
+        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
+
+        ensureGreen();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
+        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
+        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(50L));
+
+        // Test restore after index deletion
+        logger.info("--> delete indices");
+        cluster().wipeIndices("test-idx-1", "test-idx-2");
+        logger.info("--> restore one index after deletion");
+        restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices("test-idx-*", "-test-idx-2").execute().actionGet();
+        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
+        ensureGreen();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
+        ClusterState clusterState = client.admin().cluster().prepareState().get().getState();
+        assertThat(clusterState.getMetaData().hasIndex("test-idx-1"), equalTo(true));
+        assertThat(clusterState.getMetaData().hasIndex("test-idx-2"), equalTo(false));
+    }
+
+    /**
+     * For issue #51: https://github.com/elasticsearch/elasticsearch-cloud-azure/issues/51
+     */
+    @Test
+    public void testMultipleSnapshots() throws URISyntaxException, StorageException {
+        final String indexName = "test-idx-1";
+        final String typeName = "doc";
+        final String repositoryName = "test-repo";
+        final String snapshot1Name = "test-snap-1";
+        final String snapshot2Name = "test-snap-2";
+
+        Client client = client();
+
+        logger.info("creating index [{}]", indexName);
+        createIndex(indexName);
+        ensureGreen();
+
+        logger.info("indexing first document");
+        index(indexName, typeName, Integer.toString(1), "foo", "bar " + Integer.toString(1));
+        refresh();
+        assertThat(client.prepareCount(indexName).get().getCount(), equalTo(1L));
+
+        logger.info("creating Azure repository with path [{}]", getRepositoryPath());
+        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository(repositoryName)
+                .setType("azure").setSettings(Settings.settingsBuilder()
+                                .put(Repository.CONTAINER, getContainerName())
+                                .put(Repository.BASE_PATH, getRepositoryPath())
+                                .put(Repository.BASE_PATH, randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
+                ).get();
+        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
+
+        logger.info("creating snapshot [{}]", snapshot1Name);
+        CreateSnapshotResponse createSnapshotResponse1 = client.admin().cluster().prepareCreateSnapshot(repositoryName, snapshot1Name).setWaitForCompletion(true).setIndices(indexName).get();
+        assertThat(createSnapshotResponse1.getSnapshotInfo().successfulShards(), greaterThan(0));
+        assertThat(createSnapshotResponse1.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse1.getSnapshotInfo().totalShards()));
+
+        assertThat(client.admin().cluster().prepareGetSnapshots(repositoryName).setSnapshots(snapshot1Name).get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
+
+        logger.info("indexing second document");
+        index(indexName, typeName, Integer.toString(2), "foo", "bar " + Integer.toString(2));
+        refresh();
+        assertThat(client.prepareCount(indexName).get().getCount(), equalTo(2L));
+
+        logger.info("creating snapshot [{}]", snapshot2Name);
+        CreateSnapshotResponse createSnapshotResponse2 = client.admin().cluster().prepareCreateSnapshot(repositoryName, snapshot2Name).setWaitForCompletion(true).setIndices(indexName).get();
+        assertThat(createSnapshotResponse2.getSnapshotInfo().successfulShards(), greaterThan(0));
+        assertThat(createSnapshotResponse2.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse2.getSnapshotInfo().totalShards()));
+
+        assertThat(client.admin().cluster().prepareGetSnapshots(repositoryName).setSnapshots(snapshot2Name).get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
+
+        logger.info("closing index [{}]", indexName);
+        client.admin().indices().prepareClose(indexName).get();
+
+        logger.info("attempting restore from snapshot [{}]", snapshot1Name);
+        RestoreSnapshotResponse restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot(repositoryName, snapshot1Name).setWaitForCompletion(true).execute().actionGet();
+        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
+        ensureGreen();
+        assertThat(client.prepareCount(indexName).get().getCount(), equalTo(1L));
+    }
+
+    @Test
+    public void testMultipleRepositories() {
+        Client client = client();
+        logger.info("-->  creating azure repository with path [{}]", getRepositoryPath());
+        PutRepositoryResponse putRepositoryResponse1 = client.admin().cluster().preparePutRepository("test-repo1")
+                .setType("azure").setSettings(Settings.settingsBuilder()
+                        .put(Repository.CONTAINER, getContainerName().concat("-1"))
+                        .put(Repository.BASE_PATH, getRepositoryPath())
+                        .put(Repository.CHUNK_SIZE, randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
+                ).get();
+        assertThat(putRepositoryResponse1.isAcknowledged(), equalTo(true));
+        PutRepositoryResponse putRepositoryResponse2 = client.admin().cluster().preparePutRepository("test-repo2")
+                .setType("azure").setSettings(Settings.settingsBuilder()
+                        .put(Repository.CONTAINER, getContainerName().concat("-2"))
+                        .put(Repository.BASE_PATH, getRepositoryPath())
+                        .put(Repository.CHUNK_SIZE, randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
+                ).get();
+        assertThat(putRepositoryResponse2.isAcknowledged(), equalTo(true));
+
+        createIndex("test-idx-1", "test-idx-2");
+        ensureGreen();
+
+        logger.info("--> indexing some data");
+        for (int i = 0; i < 100; i++) {
+            index("test-idx-1", "doc", Integer.toString(i), "foo", "bar" + i);
+            index("test-idx-2", "doc", Integer.toString(i), "foo", "baz" + i);
+        }
+        refresh();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
+        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
+
+        logger.info("--> snapshot 1");
+        CreateSnapshotResponse createSnapshotResponse1 = client.admin().cluster().prepareCreateSnapshot("test-repo1", "test-snap").setWaitForCompletion(true).setIndices("test-idx-1").get();
+        assertThat(createSnapshotResponse1.getSnapshotInfo().successfulShards(), greaterThan(0));
+        assertThat(createSnapshotResponse1.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse1.getSnapshotInfo().totalShards()));
+
+        logger.info("--> snapshot 2");
+        CreateSnapshotResponse createSnapshotResponse2 = client.admin().cluster().prepareCreateSnapshot("test-repo2", "test-snap").setWaitForCompletion(true).setIndices("test-idx-2").get();
+        assertThat(createSnapshotResponse2.getSnapshotInfo().successfulShards(), greaterThan(0));
+        assertThat(createSnapshotResponse2.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse2.getSnapshotInfo().totalShards()));
+
+        assertThat(client.admin().cluster().prepareGetSnapshots("test-repo1").setSnapshots("test-snap").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
+        assertThat(client.admin().cluster().prepareGetSnapshots("test-repo2").setSnapshots("test-snap").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
+
+        // Test restore after index deletion
+        logger.info("--> delete indices");
+        cluster().wipeIndices("test-idx-1", "test-idx-2");
+        logger.info("--> restore one index after deletion from snapshot 1");
+        RestoreSnapshotResponse restoreSnapshotResponse1 = client.admin().cluster().prepareRestoreSnapshot("test-repo1", "test-snap").setWaitForCompletion(true).setIndices("test-idx-1").execute().actionGet();
+        assertThat(restoreSnapshotResponse1.getRestoreInfo().totalShards(), greaterThan(0));
+        ensureGreen();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
+        ClusterState clusterState = client.admin().cluster().prepareState().get().getState();
+        assertThat(clusterState.getMetaData().hasIndex("test-idx-1"), equalTo(true));
+        assertThat(clusterState.getMetaData().hasIndex("test-idx-2"), equalTo(false));
+
+        logger.info("--> restore other index after deletion from snapshot 2");
+        RestoreSnapshotResponse restoreSnapshotResponse2 = client.admin().cluster().prepareRestoreSnapshot("test-repo2", "test-snap").setWaitForCompletion(true).setIndices("test-idx-2").execute().actionGet();
+        assertThat(restoreSnapshotResponse2.getRestoreInfo().totalShards(), greaterThan(0));
+        ensureGreen();
+        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
+        clusterState = client.admin().cluster().prepareState().get().getState();
+        assertThat(clusterState.getMetaData().hasIndex("test-idx-1"), equalTo(true));
+        assertThat(clusterState.getMetaData().hasIndex("test-idx-2"), equalTo(true));
+    }
+
+    /**
+     * For issue #26: https://github.com/elasticsearch/elasticsearch-cloud-azure/issues/26
+     */
+    @Test
+    public void testListBlobs_26() throws StorageException, URISyntaxException {
+        createIndex("test-idx-1", "test-idx-2", "test-idx-3");
+        ensureGreen();
+
+        logger.info("--> indexing some data");
+        for (int i = 0; i < 100; i++) {
+            index("test-idx-1", "doc", Integer.toString(i), "foo", "bar" + i);
+            index("test-idx-2", "doc", Integer.toString(i), "foo", "baz" + i);
+            index("test-idx-3", "doc", Integer.toString(i), "foo", "baz" + i);
+        }
+        refresh();
+
+        ClusterAdminClient client = client().admin().cluster();
+        logger.info("-->  creating azure repository without any path");
+        PutRepositoryResponse putRepositoryResponse = client.preparePutRepository("test-repo").setType("azure")
+                .setSettings(Settings.settingsBuilder()
+                        .put(Repository.CONTAINER, getContainerName())
+                ).get();
+        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
+
+        // Get all snapshots - should be empty
+        assertThat(client.prepareGetSnapshots("test-repo").get().getSnapshots().size(), equalTo(0));
+
+        logger.info("--> snapshot");
+        CreateSnapshotResponse createSnapshotResponse = client.prepareCreateSnapshot("test-repo", "test-snap-26").setWaitForCompletion(true).setIndices("test-idx-*").get();
+        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
+
+        // Get all snapshots - should have one
+        assertThat(client.prepareGetSnapshots("test-repo").get().getSnapshots().size(), equalTo(1));
+
+        // Clean the snapshot
+        client.prepareDeleteSnapshot("test-repo", "test-snap-26").get();
+        client.prepareDeleteRepository("test-repo").get();
+
+        logger.info("-->  creating azure repository path [{}]", getRepositoryPath());
+        putRepositoryResponse = client.preparePutRepository("test-repo").setType("azure")
+                .setSettings(Settings.settingsBuilder()
+                        .put(Repository.CONTAINER, getContainerName())
+                        .put(Repository.BASE_PATH, getRepositoryPath())
+        ).get();
+        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
+
+        // Get all snapshots - should be empty
+        assertThat(client.prepareGetSnapshots("test-repo").get().getSnapshots().size(), equalTo(0));
+
+        logger.info("--> snapshot");
+        createSnapshotResponse = client.prepareCreateSnapshot("test-repo", "test-snap-26").setWaitForCompletion(true).setIndices("test-idx-*").get();
+        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
+
+        // Get all snapshots - should have one
+        assertThat(client.prepareGetSnapshots("test-repo").get().getSnapshots().size(), equalTo(1));
+
+
+    }
+
+    /**
+     * For issue #28: https://github.com/elasticsearch/elasticsearch-cloud-azure/issues/28
+     */
+    @Test
+    public void testGetDeleteNonExistingSnapshot_28() throws StorageException, URISyntaxException {
+        ClusterAdminClient client = client().admin().cluster();
+        logger.info("-->  creating azure repository without any path");
+        PutRepositoryResponse putRepositoryResponse = client.preparePutRepository("test-repo").setType("azure")
+                .setSettings(Settings.settingsBuilder()
+                        .put(Repository.CONTAINER, getContainerName())
+                ).get();
+        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
+
+        try {
+            client.prepareGetSnapshots("test-repo").addSnapshots("nonexistingsnapshotname").get();
+            fail("Shouldn't be here");
+        } catch (SnapshotMissingException ex) {
+            // Expected
+        }
+
+        try {
+            client.prepareDeleteSnapshot("test-repo", "nonexistingsnapshotname").get();
+            fail("Shouldn't be here");
+        } catch (SnapshotMissingException ex) {
+            // Expected
+        }
+    }
+
+    /**
+     * For issue #21: https://github.com/elasticsearch/elasticsearch-cloud-azure/issues/21
+     */
+    @Test
+    public void testForbiddenContainerName() throws Exception {
+        checkContainerName("", false);
+        checkContainerName("es", false);
+        checkContainerName("-elasticsearch", false);
+        checkContainerName("elasticsearch--integration", false);
+        checkContainerName("elasticsearch_integration", false);
+        checkContainerName("ElAsTicsearch_integration", false);
+        checkContainerName("123456789-123456789-123456789-123456789-123456789-123456789-1234", false);
+        checkContainerName("123456789-123456789-123456789-123456789-123456789-123456789-123", true);
+        checkContainerName("elasticsearch-integration", true);
+        checkContainerName("elasticsearch-integration-007", true);
+    }
+
+    /**
+     * Create repository with wrong or correct container name
+     * @param container Container name we want to create
+     * @param correct Is this container name correct
+     */
+    private void checkContainerName(final String container, final boolean correct) throws Exception {
+        logger.info("-->  creating azure repository with container name [{}]", container);
+        // It could happen that we just removed from a previous test the same container so
+        // we can not create it yet.
+        assertBusy(new Runnable() {
+
+            public void run() {
+                try {
+                    PutRepositoryResponse putRepositoryResponse = client().admin().cluster().preparePutRepository("test-repo")
+                            .setType("azure").setSettings(Settings.settingsBuilder()
+                                            .put(Repository.CONTAINER, container)
+                                            .put(Repository.BASE_PATH, getRepositoryPath())
+                                            .put(Repository.CHUNK_SIZE, randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
+                            ).get();
+                    client().admin().cluster().prepareDeleteRepository("test-repo").get();
+                    try {
+                        logger.info("--> remove container [{}]", container);
+                        cleanRepositoryFiles(container);
+                    } catch (StorageException | URISyntaxException e) {
+                        // We can ignore that as we just try to clean after the test
+                    }
+                    assertTrue(putRepositoryResponse.isAcknowledged() == correct);
+                } catch (RepositoryVerificationException e) {
+                    if (correct) {
+                        logger.debug(" -> container is being removed. Let's wait a bit...");
+                        fail();
+                    }
+                }
+            }
+        }, 5, TimeUnit.MINUTES);
+    }
+
+    /**
+     * Test case for issue #23: https://github.com/elasticsearch/elasticsearch-cloud-azure/issues/23
+     */
+    @Test
+    public void testNonExistingRepo_23() {
+        Client client = client();
+        logger.info("-->  creating azure repository with path [{}]", getRepositoryPath());
+        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository("test-repo")
+                .setType("azure").setSettings(Settings.settingsBuilder()
+                        .put(Repository.CONTAINER, getContainerName())
+                        .put(Repository.BASE_PATH, getRepositoryPath())
+                        .put(Repository.CHUNK_SIZE, randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
+                ).get();
+        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
+
+        logger.info("--> restore non existing snapshot");
+        try {
+            client.admin().cluster().prepareRestoreSnapshot("test-repo", "no-existing-snapshot").setWaitForCompletion(true).execute().actionGet();
+            fail("Shouldn't be here");
+        } catch (SnapshotMissingException ex) {
+            // Expected
+        }
+    }
+
+    /**
+     * When a user remove a container you can not immediately create it again.
+     */
+    @Test
+    public void testRemoveAndCreateContainer() throws Exception {
+        final String container = getContainerName().concat("-testremove");
+        final AzureStorageService storageService = internalCluster().getInstance(AzureStorageService.class);
+
+        // It could happen that we run this test really close to a previous one
+        // so we might need some time to be able to create the container
+        assertBusy(new Runnable() {
+
+            public void run()  {
+                try {
+                    storageService.createContainer(container);
+                    logger.debug(" -> container created...");
+                } catch (URISyntaxException e) {
+                    // Incorrect URL. This should never happen.
+                    fail();
+                } catch (StorageException e) {
+                    // It could happen. Let's wait for a while.
+                    logger.debug(" -> container is being removed. Let's wait a bit...");
+                    fail();
+                }
+            }
+        }, 30, TimeUnit.SECONDS);
+        storageService.removeContainer(container);
+
+        ClusterAdminClient client = client().admin().cluster();
+        logger.info("-->  creating azure repository while container is being removed");
+        try {
+            client.preparePutRepository("test-repo").setType("azure")
+                    .setSettings(Settings.settingsBuilder()
+                            .put(Repository.CONTAINER, container)
+                    ).get();
+            fail("we should get a RepositoryVerificationException");
+        } catch (RepositoryVerificationException e) {
+            // Fine we expect that
+        }
+    }
+
+    /**
+     * Deletes repositories, supports wildcard notation.
+     */
+    public static void wipeRepositories(String... repositories) {
+        // if nothing is provided, delete all
+        if (repositories.length == 0) {
+            repositories = new String[]{"*"};
+        }
+        for (String repository : repositories) {
+            try {
+                client().admin().cluster().prepareDeleteRepository(repository).execute().actionGet();
+            } catch (RepositoryMissingException ex) {
+                // ignore
+            }
+        }
+    }
+
+    /**
+     * Purge the test containers
+     */
+    public void cleanRepositoryFiles(String... containers) throws StorageException, URISyntaxException {
+        Settings settings = readSettingsFromFile();
+        AzureStorageService client = new AzureStorageServiceImpl(settings);
+        for (String container : containers) {
+            client.removeContainer(container);
+        }
+    }
+}
diff --git a/plugins/cloud-gce/pom.xml b/plugins/cloud-gce/pom.xml
index 724036a..38fed6e 100644
--- a/plugins/cloud-gce/pom.xml
+++ b/plugins/cloud-gce/pom.xml
@@ -18,7 +18,7 @@ governing permissions and limitations under the License. -->
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>plugins</artifactId>
-        <version>2.1.0-SNAPSHOT</version>
+        <version>3.0.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>cloud-gce</artifactId>
diff --git a/plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoveryTest.java b/plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoveryTest.java
deleted file mode 100644
index a31597b..0000000
--- a/plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoveryTest.java
+++ /dev/null
@@ -1,217 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.gce;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.cloud.gce.GceComputeService;
-import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.network.NetworkService;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.transport.MockTransportService;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.local.LocalTransport;
-import org.junit.*;
-
-import java.util.List;
-import java.util.Locale;
-
-import static org.hamcrest.Matchers.hasSize;
-import static org.hamcrest.Matchers.is;
-
-/**
- * This test class uses a GCE HTTP Mock system which allows to simulate JSON Responses.
- *
- * To implement a new test you'll need to create an `instances.json` file which contains expected response
- * for a given project-id and zone under the src/test/resources/org/elasticsearch/discovery/gce with dir name:
- *
- * compute/v1/projects/[project-id]/zones/[zone]
- *
- * By default, project-id is the test method name, lowercase.
- *
- * For example, if you create a test `myNewAwesomeTest` with following settings:
- *
- * Settings nodeSettings = Settings.builder()
- *  .put(GceComputeService.Fields.PROJECT, projectName)
- *  .put(GceComputeService.Fields.ZONE, "europe-west1-b")
- *  .build();
- *
- *  You need to create a file under `src/test/resources/org/elasticsearch/discovery/gce/` named:
- *
- *  compute/v1/projects/mynewawesometest/zones/europe-west1-b/instances.json
- *
- */
-public class GceDiscoveryTest extends ESTestCase {
-
-    protected static ThreadPool threadPool;
-    protected MockTransportService transportService;
-    protected GceComputeService mock;
-    protected String projectName;
-
-    @BeforeClass
-    public static void createThreadPool() {
-        threadPool = new ThreadPool(GceDiscoveryTest.class.getName());
-    }
-
-    @AfterClass
-    public static void stopThreadPool() {
-        if (threadPool !=null) {
-            threadPool.shutdownNow();
-            threadPool = null;
-        }
-    }
-
-    @Before
-    public void setProjectName() {
-        projectName = getTestName().toLowerCase(Locale.ROOT);
-    }
-
-    @Before
-    public void createTransportService() {
-        transportService = new MockTransportService(
-                Settings.EMPTY,
-                new LocalTransport(Settings.EMPTY, threadPool, Version.CURRENT, new NamedWriteableRegistry()), threadPool);
-    }
-
-    @After
-    public void stopGceComputeService() {
-        if (mock != null) {
-            mock.stop();
-        }
-    }
-
-    protected List<DiscoveryNode> buildDynamicNodes(GceComputeService gceComputeService, Settings nodeSettings) {
-        GceUnicastHostsProvider provider = new GceUnicastHostsProvider(nodeSettings, gceComputeService,
-                transportService, new NetworkService(Settings.EMPTY), Version.CURRENT);
-
-        List<DiscoveryNode> discoveryNodes = provider.buildDynamicNodes();
-        logger.info("--> nodes found: {}", discoveryNodes);
-        return discoveryNodes;
-    }
-
-    @Test
-    public void nodesWithDifferentTagsAndNoTagSet() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(2));
-    }
-
-    @Test
-    public void nodesWithDifferentTagsAndOneTagSet() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
-                .putArray(GceComputeService.Fields.TAGS, "elasticsearch")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(1));
-        assertThat(discoveryNodes.get(0).getId(), is("#cloud-test2-0"));
-    }
-
-    @Test
-    public void nodesWithDifferentTagsAndTwoTagSet() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
-                .putArray(GceComputeService.Fields.TAGS, "elasticsearch", "dev")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(1));
-        assertThat(discoveryNodes.get(0).getId(), is("#cloud-test2-0"));
-    }
-
-    @Test
-    public void nodesWithSameTagsAndNoTagSet() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(2));
-    }
-
-    @Test
-    public void nodesWithSameTagsAndOneTagSet() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
-                .putArray(GceComputeService.Fields.TAGS, "elasticsearch")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(2));
-    }
-
-    @Test
-    public void nodesWithSameTagsAndTwoTagsSet() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
-                .putArray(GceComputeService.Fields.TAGS, "elasticsearch", "dev")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(2));
-    }
-
-    @Test
-    public void multipleZonesAndTwoNodesInSameZone() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .putArray(GceComputeService.Fields.ZONE, "us-central1-a", "europe-west1-b")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(2));
-    }
-
-    @Test
-    public void multipleZonesAndTwoNodesInDifferentZones() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .putArray(GceComputeService.Fields.ZONE, "us-central1-a", "europe-west1-b")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(2));
-    }
-
-    /**
-     * For issue https://github.com/elastic/elasticsearch-cloud-gce/issues/43
-     */
-    @Test
-    public void zeroNode43() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .putArray(GceComputeService.Fields.ZONE, "us-central1-a", "us-central1-b")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(0));
-    }
-}
diff --git a/plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoveryTests.java b/plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoveryTests.java
new file mode 100644
index 0000000..b18cca1
--- /dev/null
+++ b/plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoveryTests.java
@@ -0,0 +1,217 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.gce;
+
+import org.elasticsearch.Version;
+import org.elasticsearch.cloud.gce.GceComputeService;
+import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
+import org.elasticsearch.common.network.NetworkService;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.transport.MockTransportService;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.elasticsearch.transport.local.LocalTransport;
+import org.junit.*;
+
+import java.util.List;
+import java.util.Locale;
+
+import static org.hamcrest.Matchers.hasSize;
+import static org.hamcrest.Matchers.is;
+
+/**
+ * This test class uses a GCE HTTP Mock system which allows to simulate JSON Responses.
+ *
+ * To implement a new test you'll need to create an `instances.json` file which contains expected response
+ * for a given project-id and zone under the src/test/resources/org/elasticsearch/discovery/gce with dir name:
+ *
+ * compute/v1/projects/[project-id]/zones/[zone]
+ *
+ * By default, project-id is the test method name, lowercase.
+ *
+ * For example, if you create a test `myNewAwesomeTest` with following settings:
+ *
+ * Settings nodeSettings = Settings.builder()
+ *  .put(GceComputeService.Fields.PROJECT, projectName)
+ *  .put(GceComputeService.Fields.ZONE, "europe-west1-b")
+ *  .build();
+ *
+ *  You need to create a file under `src/test/resources/org/elasticsearch/discovery/gce/` named:
+ *
+ *  compute/v1/projects/mynewawesometest/zones/europe-west1-b/instances.json
+ *
+ */
+public class GceDiscoveryTests extends ESTestCase {
+
+    protected static ThreadPool threadPool;
+    protected MockTransportService transportService;
+    protected GceComputeService mock;
+    protected String projectName;
+
+    @BeforeClass
+    public static void createThreadPool() {
+        threadPool = new ThreadPool(GceDiscoveryTests.class.getName());
+    }
+
+    @AfterClass
+    public static void stopThreadPool() {
+        if (threadPool !=null) {
+            threadPool.shutdownNow();
+            threadPool = null;
+        }
+    }
+
+    @Before
+    public void setProjectName() {
+        projectName = getTestName().toLowerCase(Locale.ROOT);
+    }
+
+    @Before
+    public void createTransportService() {
+        transportService = new MockTransportService(
+                Settings.EMPTY,
+                new LocalTransport(Settings.EMPTY, threadPool, Version.CURRENT, new NamedWriteableRegistry()), threadPool);
+    }
+
+    @After
+    public void stopGceComputeService() {
+        if (mock != null) {
+            mock.stop();
+        }
+    }
+
+    protected List<DiscoveryNode> buildDynamicNodes(GceComputeService gceComputeService, Settings nodeSettings) {
+        GceUnicastHostsProvider provider = new GceUnicastHostsProvider(nodeSettings, gceComputeService,
+                transportService, new NetworkService(Settings.EMPTY), Version.CURRENT);
+
+        List<DiscoveryNode> discoveryNodes = provider.buildDynamicNodes();
+        logger.info("--> nodes found: {}", discoveryNodes);
+        return discoveryNodes;
+    }
+
+    @Test
+    public void nodesWithDifferentTagsAndNoTagSet() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(2));
+    }
+
+    @Test
+    public void nodesWithDifferentTagsAndOneTagSet() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
+                .putArray(GceComputeService.Fields.TAGS, "elasticsearch")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(1));
+        assertThat(discoveryNodes.get(0).getId(), is("#cloud-test2-0"));
+    }
+
+    @Test
+    public void nodesWithDifferentTagsAndTwoTagSet() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
+                .putArray(GceComputeService.Fields.TAGS, "elasticsearch", "dev")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(1));
+        assertThat(discoveryNodes.get(0).getId(), is("#cloud-test2-0"));
+    }
+
+    @Test
+    public void nodesWithSameTagsAndNoTagSet() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(2));
+    }
+
+    @Test
+    public void nodesWithSameTagsAndOneTagSet() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
+                .putArray(GceComputeService.Fields.TAGS, "elasticsearch")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(2));
+    }
+
+    @Test
+    public void nodesWithSameTagsAndTwoTagsSet() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
+                .putArray(GceComputeService.Fields.TAGS, "elasticsearch", "dev")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(2));
+    }
+
+    @Test
+    public void multipleZonesAndTwoNodesInSameZone() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .putArray(GceComputeService.Fields.ZONE, "us-central1-a", "europe-west1-b")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(2));
+    }
+
+    @Test
+    public void multipleZonesAndTwoNodesInDifferentZones() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .putArray(GceComputeService.Fields.ZONE, "us-central1-a", "europe-west1-b")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(2));
+    }
+
+    /**
+     * For issue https://github.com/elastic/elasticsearch-cloud-gce/issues/43
+     */
+    @Test
+    public void zeroNode43() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .putArray(GceComputeService.Fields.ZONE, "us-central1-a", "us-central1-b")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(0));
+    }
+}
diff --git a/plugins/delete-by-query/pom.xml b/plugins/delete-by-query/pom.xml
index 07d3bb8..105c9f9 100644
--- a/plugins/delete-by-query/pom.xml
+++ b/plugins/delete-by-query/pom.xml
@@ -18,7 +18,7 @@ governing permissions and limitations under the License. -->
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>plugins</artifactId>
-        <version>2.1.0-SNAPSHOT</version>
+        <version>3.0.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>delete-by-query</artifactId>
diff --git a/plugins/discovery-ec2/LICENSE.txt b/plugins/discovery-ec2/LICENSE.txt
new file mode 100644
index 0000000..d645695
--- /dev/null
+++ b/plugins/discovery-ec2/LICENSE.txt
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/plugins/discovery-ec2/NOTICE.txt b/plugins/discovery-ec2/NOTICE.txt
new file mode 100644
index 0000000..4880904
--- /dev/null
+++ b/plugins/discovery-ec2/NOTICE.txt
@@ -0,0 +1,8 @@
+Elasticsearch
+Copyright 2009-2015 Elasticsearch
+
+This product includes software developed by The Apache Software
+Foundation (http://www.apache.org/).
+
+The LICENSE and NOTICE files for all dependencies may be found in the licenses/
+directory.
diff --git a/plugins/discovery-ec2/licenses/aws-java-sdk-LICENSE.txt b/plugins/discovery-ec2/licenses/aws-java-sdk-LICENSE.txt
new file mode 100644
index 0000000..98d1f93
--- /dev/null
+++ b/plugins/discovery-ec2/licenses/aws-java-sdk-LICENSE.txt
@@ -0,0 +1,63 @@
+Apache License
+Version 2.0, January 2004
+
+TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+1. Definitions.
+
+"License" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.
+
+"Licensor" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.
+
+"Legal Entity" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, "control" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.
+
+"You" (or "Your") shall mean an individual or Legal Entity exercising permissions granted by this License.
+
+"Source" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.
+
+"Object" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.
+
+"Work" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).
+
+"Derivative Works" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.
+
+"Contribution" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, "submitted" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as "Not a Contribution."
+
+"Contributor" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.
+
+2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.
+
+3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.
+
+4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:
+
+   1.   You must give any other recipients of the Work or Derivative Works a copy of this License; and
+   2.   You must cause any modified files to carry prominent notices stating that You changed the files; and
+   3.   You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and
+   4.   If the Work includes a "NOTICE" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License.
+
+You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.
+
+5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.
+
+6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.
+
+7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.
+
+8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.
+
+9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.
+
+END OF TERMS AND CONDITIONS
+
+Note: Other license terms may apply to certain, identified software files contained within or distributed with the accompanying software if such terms are included in the directory containing the accompanying software. Such other license terms will then apply in lieu of the terms of the software license above.
+
+JSON processing code subject to the JSON License from JSON.org:
+
+Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
+
+The Software shall be used for Good, not Evil.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
diff --git a/plugins/discovery-ec2/licenses/aws-java-sdk-NOTICE.txt b/plugins/discovery-ec2/licenses/aws-java-sdk-NOTICE.txt
new file mode 100644
index 0000000..565bd60
--- /dev/null
+++ b/plugins/discovery-ec2/licenses/aws-java-sdk-NOTICE.txt
@@ -0,0 +1,15 @@
+AWS SDK for Java
+Copyright 2010-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.
+
+This product includes software developed by
+Amazon Technologies, Inc (http://www.amazon.com/).
+
+**********************
+THIRD PARTY COMPONENTS
+**********************
+This software includes third party software subject to the following copyrights:
+- XML parsing and utility functions from JetS3t - Copyright 2006-2009 James Murty.
+- JSON parsing and utility functions from JSON.org - Copyright 2002 JSON.org.
+- PKCS#1 PEM encoded private key parsing and utility functions from oauth.googlecode.com - Copyright 1998-2010 AOL Inc.
+
+The licenses for these third party components are included in LICENSE.txt
diff --git a/plugins/discovery-ec2/licenses/aws-java-sdk-core-1.10.12.jar.sha1 b/plugins/discovery-ec2/licenses/aws-java-sdk-core-1.10.12.jar.sha1
new file mode 100644
index 0000000..659b6cc
--- /dev/null
+++ b/plugins/discovery-ec2/licenses/aws-java-sdk-core-1.10.12.jar.sha1
@@ -0,0 +1 @@
+7ff51040bbcc9085dcb9a24a2c2a3cc7ac995988
diff --git a/plugins/discovery-ec2/licenses/aws-java-sdk-ec2-1.10.12.jar.sha1 b/plugins/discovery-ec2/licenses/aws-java-sdk-ec2-1.10.12.jar.sha1
new file mode 100644
index 0000000..60bae7e
--- /dev/null
+++ b/plugins/discovery-ec2/licenses/aws-java-sdk-ec2-1.10.12.jar.sha1
@@ -0,0 +1 @@
+b0712cc659e72b9da0f5b03872d2476ab4a695f7
diff --git a/plugins/discovery-ec2/licenses/commons-codec-1.6.jar.sha1 b/plugins/discovery-ec2/licenses/commons-codec-1.6.jar.sha1
new file mode 100644
index 0000000..bf78aff
--- /dev/null
+++ b/plugins/discovery-ec2/licenses/commons-codec-1.6.jar.sha1
@@ -0,0 +1 @@
+b7f0fc8f61ecadeb3695f0b9464755eee44374d4
diff --git a/plugins/discovery-ec2/licenses/commons-codec-LICENSE.txt b/plugins/discovery-ec2/licenses/commons-codec-LICENSE.txt
new file mode 100644
index 0000000..d645695
--- /dev/null
+++ b/plugins/discovery-ec2/licenses/commons-codec-LICENSE.txt
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/plugins/discovery-ec2/licenses/commons-codec-NOTICE.txt b/plugins/discovery-ec2/licenses/commons-codec-NOTICE.txt
new file mode 100644
index 0000000..5691644
--- /dev/null
+++ b/plugins/discovery-ec2/licenses/commons-codec-NOTICE.txt
@@ -0,0 +1,17 @@
+Apache Commons Codec
+Copyright 2002-2015 The Apache Software Foundation
+
+This product includes software developed at
+The Apache Software Foundation (http://www.apache.org/).
+
+src/test/org/apache/commons/codec/language/DoubleMetaphoneTest.java
+contains test data from http://aspell.net/test/orig/batch0.tab.
+Copyright (C) 2002 Kevin Atkinson (kevina@gnu.org)
+
+===============================================================================
+
+The content of package org.apache.commons.codec.language.bm has been translated
+from the original php source code available at http://stevemorse.org/phoneticinfo.htm
+with permission from the original authors.
+Original source copyright:
+Copyright (c) 2008 Alexander Beider & Stephen P. Morse.
diff --git a/plugins/discovery-ec2/licenses/commons-logging-1.1.3.jar.sha1 b/plugins/discovery-ec2/licenses/commons-logging-1.1.3.jar.sha1
new file mode 100644
index 0000000..c8756c4
--- /dev/null
+++ b/plugins/discovery-ec2/licenses/commons-logging-1.1.3.jar.sha1
@@ -0,0 +1 @@
+f6f66e966c70a83ffbdb6f17a0919eaf7c8aca7f
diff --git a/plugins/discovery-ec2/licenses/commons-logging-LICENSE.txt b/plugins/discovery-ec2/licenses/commons-logging-LICENSE.txt
new file mode 100644
index 0000000..57bc88a
--- /dev/null
+++ b/plugins/discovery-ec2/licenses/commons-logging-LICENSE.txt
@@ -0,0 +1,202 @@
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+
diff --git a/plugins/discovery-ec2/licenses/commons-logging-NOTICE.txt b/plugins/discovery-ec2/licenses/commons-logging-NOTICE.txt
new file mode 100644
index 0000000..72eb32a
--- /dev/null
+++ b/plugins/discovery-ec2/licenses/commons-logging-NOTICE.txt
@@ -0,0 +1,5 @@
+Apache Commons CLI
+Copyright 2001-2009 The Apache Software Foundation
+
+This product includes software developed by
+The Apache Software Foundation (http://www.apache.org/).
diff --git a/plugins/discovery-ec2/licenses/httpclient-4.3.6.jar.sha1 b/plugins/discovery-ec2/licenses/httpclient-4.3.6.jar.sha1
new file mode 100644
index 0000000..3d35ee9
--- /dev/null
+++ b/plugins/discovery-ec2/licenses/httpclient-4.3.6.jar.sha1
@@ -0,0 +1 @@
+4c47155e3e6c9a41a28db36680b828ced53b8af4
diff --git a/plugins/discovery-ec2/licenses/httpclient-LICENSE.txt b/plugins/discovery-ec2/licenses/httpclient-LICENSE.txt
new file mode 100644
index 0000000..32f01ed
--- /dev/null
+++ b/plugins/discovery-ec2/licenses/httpclient-LICENSE.txt
@@ -0,0 +1,558 @@
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+=========================================================================
+
+This project includes Public Suffix List copied from
+<https://publicsuffix.org/list/effective_tld_names.dat>
+licensed under the terms of the Mozilla Public License, v. 2.0
+
+Full license text: <http://mozilla.org/MPL/2.0/>
+
+Mozilla Public License Version 2.0
+==================================
+
+1. Definitions
+--------------
+
+1.1. "Contributor"
+    means each individual or legal entity that creates, contributes to
+    the creation of, or owns Covered Software.
+
+1.2. "Contributor Version"
+    means the combination of the Contributions of others (if any) used
+    by a Contributor and that particular Contributor's Contribution.
+
+1.3. "Contribution"
+    means Covered Software of a particular Contributor.
+
+1.4. "Covered Software"
+    means Source Code Form to which the initial Contributor has attached
+    the notice in Exhibit A, the Executable Form of such Source Code
+    Form, and Modifications of such Source Code Form, in each case
+    including portions thereof.
+
+1.5. "Incompatible With Secondary Licenses"
+    means
+
+    (a) that the initial Contributor has attached the notice described
+        in Exhibit B to the Covered Software; or
+
+    (b) that the Covered Software was made available under the terms of
+        version 1.1 or earlier of the License, but not also under the
+        terms of a Secondary License.
+
+1.6. "Executable Form"
+    means any form of the work other than Source Code Form.
+
+1.7. "Larger Work"
+    means a work that combines Covered Software with other material, in
+    a separate file or files, that is not Covered Software.
+
+1.8. "License"
+    means this document.
+
+1.9. "Licensable"
+    means having the right to grant, to the maximum extent possible,
+    whether at the time of the initial grant or subsequently, any and
+    all of the rights conveyed by this License.
+
+1.10. "Modifications"
+    means any of the following:
+
+    (a) any file in Source Code Form that results from an addition to,
+        deletion from, or modification of the contents of Covered
+        Software; or
+
+    (b) any new file in Source Code Form that contains any Covered
+        Software.
+
+1.11. "Patent Claims" of a Contributor
+    means any patent claim(s), including without limitation, method,
+    process, and apparatus claims, in any patent Licensable by such
+    Contributor that would be infringed, but for the grant of the
+    License, by the making, using, selling, offering for sale, having
+    made, import, or transfer of either its Contributions or its
+    Contributor Version.
+
+1.12. "Secondary License"
+    means either the GNU General Public License, Version 2.0, the GNU
+    Lesser General Public License, Version 2.1, the GNU Affero General
+    Public License, Version 3.0, or any later versions of those
+    licenses.
+
+1.13. "Source Code Form"
+    means the form of the work preferred for making modifications.
+
+1.14. "You" (or "Your")
+    means an individual or a legal entity exercising rights under this
+    License. For legal entities, "You" includes any entity that
+    controls, is controlled by, or is under common control with You. For
+    purposes of this definition, "control" means (a) the power, direct
+    or indirect, to cause the direction or management of such entity,
+    whether by contract or otherwise, or (b) ownership of more than
+    fifty percent (50%) of the outstanding shares or beneficial
+    ownership of such entity.
+
+2. License Grants and Conditions
+--------------------------------
+
+2.1. Grants
+
+Each Contributor hereby grants You a world-wide, royalty-free,
+non-exclusive license:
+
+(a) under intellectual property rights (other than patent or trademark)
+    Licensable by such Contributor to use, reproduce, make available,
+    modify, display, perform, distribute, and otherwise exploit its
+    Contributions, either on an unmodified basis, with Modifications, or
+    as part of a Larger Work; and
+
+(b) under Patent Claims of such Contributor to make, use, sell, offer
+    for sale, have made, import, and otherwise transfer either its
+    Contributions or its Contributor Version.
+
+2.2. Effective Date
+
+The licenses granted in Section 2.1 with respect to any Contribution
+become effective for each Contribution on the date the Contributor first
+distributes such Contribution.
+
+2.3. Limitations on Grant Scope
+
+The licenses granted in this Section 2 are the only rights granted under
+this License. No additional rights or licenses will be implied from the
+distribution or licensing of Covered Software under this License.
+Notwithstanding Section 2.1(b) above, no patent license is granted by a
+Contributor:
+
+(a) for any code that a Contributor has removed from Covered Software;
+    or
+
+(b) for infringements caused by: (i) Your and any other third party's
+    modifications of Covered Software, or (ii) the combination of its
+    Contributions with other software (except as part of its Contributor
+    Version); or
+
+(c) under Patent Claims infringed by Covered Software in the absence of
+    its Contributions.
+
+This License does not grant any rights in the trademarks, service marks,
+or logos of any Contributor (except as may be necessary to comply with
+the notice requirements in Section 3.4).
+
+2.4. Subsequent Licenses
+
+No Contributor makes additional grants as a result of Your choice to
+distribute the Covered Software under a subsequent version of this
+License (see Section 10.2) or under the terms of a Secondary License (if
+permitted under the terms of Section 3.3).
+
+2.5. Representation
+
+Each Contributor represents that the Contributor believes its
+Contributions are its original creation(s) or it has sufficient rights
+to grant the rights to its Contributions conveyed by this License.
+
+2.6. Fair Use
+
+This License is not intended to limit any rights You have under
+applicable copyright doctrines of fair use, fair dealing, or other
+equivalents.
+
+2.7. Conditions
+
+Sections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted
+in Section 2.1.
+
+3. Responsibilities
+-------------------
+
+3.1. Distribution of Source Form
+
+All distribution of Covered Software in Source Code Form, including any
+Modifications that You create or to which You contribute, must be under
+the terms of this License. You must inform recipients that the Source
+Code Form of the Covered Software is governed by the terms of this
+License, and how they can obtain a copy of this License. You may not
+attempt to alter or restrict the recipients' rights in the Source Code
+Form.
+
+3.2. Distribution of Executable Form
+
+If You distribute Covered Software in Executable Form then:
+
+(a) such Covered Software must also be made available in Source Code
+    Form, as described in Section 3.1, and You must inform recipients of
+    the Executable Form how they can obtain a copy of such Source Code
+    Form by reasonable means in a timely manner, at a charge no more
+    than the cost of distribution to the recipient; and
+
+(b) You may distribute such Executable Form under the terms of this
+    License, or sublicense it under different terms, provided that the
+    license for the Executable Form does not attempt to limit or alter
+    the recipients' rights in the Source Code Form under this License.
+
+3.3. Distribution of a Larger Work
+
+You may create and distribute a Larger Work under terms of Your choice,
+provided that You also comply with the requirements of this License for
+the Covered Software. If the Larger Work is a combination of Covered
+Software with a work governed by one or more Secondary Licenses, and the
+Covered Software is not Incompatible With Secondary Licenses, this
+License permits You to additionally distribute such Covered Software
+under the terms of such Secondary License(s), so that the recipient of
+the Larger Work may, at their option, further distribute the Covered
+Software under the terms of either this License or such Secondary
+License(s).
+
+3.4. Notices
+
+You may not remove or alter the substance of any license notices
+(including copyright notices, patent notices, disclaimers of warranty,
+or limitations of liability) contained within the Source Code Form of
+the Covered Software, except that You may alter any license notices to
+the extent required to remedy known factual inaccuracies.
+
+3.5. Application of Additional Terms
+
+You may choose to offer, and to charge a fee for, warranty, support,
+indemnity or liability obligations to one or more recipients of Covered
+Software. However, You may do so only on Your own behalf, and not on
+behalf of any Contributor. You must make it absolutely clear that any
+such warranty, support, indemnity, or liability obligation is offered by
+You alone, and You hereby agree to indemnify every Contributor for any
+liability incurred by such Contributor as a result of warranty, support,
+indemnity or liability terms You offer. You may include additional
+disclaimers of warranty and limitations of liability specific to any
+jurisdiction.
+
+4. Inability to Comply Due to Statute or Regulation
+---------------------------------------------------
+
+If it is impossible for You to comply with any of the terms of this
+License with respect to some or all of the Covered Software due to
+statute, judicial order, or regulation then You must: (a) comply with
+the terms of this License to the maximum extent possible; and (b)
+describe the limitations and the code they affect. Such description must
+be placed in a text file included with all distributions of the Covered
+Software under this License. Except to the extent prohibited by statute
+or regulation, such description must be sufficiently detailed for a
+recipient of ordinary skill to be able to understand it.
+
+5. Termination
+--------------
+
+5.1. The rights granted under this License will terminate automatically
+if You fail to comply with any of its terms. However, if You become
+compliant, then the rights granted under this License from a particular
+Contributor are reinstated (a) provisionally, unless and until such
+Contributor explicitly and finally terminates Your grants, and (b) on an
+ongoing basis, if such Contributor fails to notify You of the
+non-compliance by some reasonable means prior to 60 days after You have
+come back into compliance. Moreover, Your grants from a particular
+Contributor are reinstated on an ongoing basis if such Contributor
+notifies You of the non-compliance by some reasonable means, this is the
+first time You have received notice of non-compliance with this License
+from such Contributor, and You become compliant prior to 30 days after
+Your receipt of the notice.
+
+5.2. If You initiate litigation against any entity by asserting a patent
+infringement claim (excluding declaratory judgment actions,
+counter-claims, and cross-claims) alleging that a Contributor Version
+directly or indirectly infringes any patent, then the rights granted to
+You by any and all Contributors for the Covered Software under Section
+2.1 of this License shall terminate.
+
+5.3. In the event of termination under Sections 5.1 or 5.2 above, all
+end user license agreements (excluding distributors and resellers) which
+have been validly granted by You or Your distributors under this License
+prior to termination shall survive termination.
+
+************************************************************************
+*                                                                      *
+*  6. Disclaimer of Warranty                                           *
+*  -------------------------                                           *
+*                                                                      *
+*  Covered Software is provided under this License on an "as is"       *
+*  basis, without warranty of any kind, either expressed, implied, or  *
+*  statutory, including, without limitation, warranties that the       *
+*  Covered Software is free of defects, merchantable, fit for a        *
+*  particular purpose or non-infringing. The entire risk as to the     *
+*  quality and performance of the Covered Software is with You.        *
+*  Should any Covered Software prove defective in any respect, You     *
+*  (not any Contributor) assume the cost of any necessary servicing,   *
+*  repair, or correction. This disclaimer of warranty constitutes an   *
+*  essential part of this License. No use of any Covered Software is   *
+*  authorized under this License except under this disclaimer.         *
+*                                                                      *
+************************************************************************
+
+************************************************************************
+*                                                                      *
+*  7. Limitation of Liability                                          *
+*  --------------------------                                          *
+*                                                                      *
+*  Under no circumstances and under no legal theory, whether tort      *
+*  (including negligence), contract, or otherwise, shall any           *
+*  Contributor, or anyone who distributes Covered Software as          *
+*  permitted above, be liable to You for any direct, indirect,         *
+*  special, incidental, or consequential damages of any character      *
+*  including, without limitation, damages for lost profits, loss of    *
+*  goodwill, work stoppage, computer failure or malfunction, or any    *
+*  and all other commercial damages or losses, even if such party      *
+*  shall have been informed of the possibility of such damages. This   *
+*  limitation of liability shall not apply to liability for death or   *
+*  personal injury resulting from such party's negligence to the       *
+*  extent applicable law prohibits such limitation. Some               *
+*  jurisdictions do not allow the exclusion or limitation of           *
+*  incidental or consequential damages, so this exclusion and          *
+*  limitation may not apply to You.                                    *
+*                                                                      *
+************************************************************************
+
+8. Litigation
+-------------
+
+Any litigation relating to this License may be brought only in the
+courts of a jurisdiction where the defendant maintains its principal
+place of business and such litigation shall be governed by laws of that
+jurisdiction, without reference to its conflict-of-law provisions.
+Nothing in this Section shall prevent a party's ability to bring
+cross-claims or counter-claims.
+
+9. Miscellaneous
+----------------
+
+This License represents the complete agreement concerning the subject
+matter hereof. If any provision of this License is held to be
+unenforceable, such provision shall be reformed only to the extent
+necessary to make it enforceable. Any law or regulation which provides
+that the language of a contract shall be construed against the drafter
+shall not be used to construe this License against a Contributor.
+
+10. Versions of the License
+---------------------------
+
+10.1. New Versions
+
+Mozilla Foundation is the license steward. Except as provided in Section
+10.3, no one other than the license steward has the right to modify or
+publish new versions of this License. Each version will be given a
+distinguishing version number.
+
+10.2. Effect of New Versions
+
+You may distribute the Covered Software under the terms of the version
+of the License under which You originally received the Covered Software,
+or under the terms of any subsequent version published by the license
+steward.
+
+10.3. Modified Versions
+
+If you create software not governed by this License, and you want to
+create a new license for such software, you may create and use a
+modified version of this License if you rename the license and remove
+any references to the name of the license steward (except to note that
+such modified license differs from this License).
+
+10.4. Distributing Source Code Form that is Incompatible With Secondary
+Licenses
+
+If You choose to distribute Source Code Form that is Incompatible With
+Secondary Licenses under the terms of this version of the License, the
+notice described in Exhibit B of this License must be attached.
+
+Exhibit A - Source Code Form License Notice
+-------------------------------------------
+
+  This Source Code Form is subject to the terms of the Mozilla Public
+  License, v. 2.0. If a copy of the MPL was not distributed with this
+  file, You can obtain one at http://mozilla.org/MPL/2.0/.
+
+If it is not possible or desirable to put the notice in a particular
+file, then You may include the notice in a location (such as a LICENSE
+file in a relevant directory) where a recipient would be likely to look
+for such a notice.
+
+You may add additional accurate notices of copyright ownership.
+
+Exhibit B - "Incompatible With Secondary Licenses" Notice
+---------------------------------------------------------
+
+  This Source Code Form is "Incompatible With Secondary Licenses", as
+  defined by the Mozilla Public License, v. 2.0.
diff --git a/plugins/discovery-ec2/licenses/httpclient-NOTICE.txt b/plugins/discovery-ec2/licenses/httpclient-NOTICE.txt
new file mode 100644
index 0000000..4f60581
--- /dev/null
+++ b/plugins/discovery-ec2/licenses/httpclient-NOTICE.txt
@@ -0,0 +1,5 @@
+Apache HttpComponents Client
+Copyright 1999-2015 The Apache Software Foundation
+
+This product includes software developed at
+The Apache Software Foundation (http://www.apache.org/).
diff --git a/plugins/discovery-ec2/licenses/httpcore-4.3.3.jar.sha1 b/plugins/discovery-ec2/licenses/httpcore-4.3.3.jar.sha1
new file mode 100644
index 0000000..5d9c0e2
--- /dev/null
+++ b/plugins/discovery-ec2/licenses/httpcore-4.3.3.jar.sha1
@@ -0,0 +1 @@
+f91b7a4aadc5cf486df6e4634748d7dd7a73f06d
diff --git a/plugins/discovery-ec2/licenses/httpcore-LICENSE.txt b/plugins/discovery-ec2/licenses/httpcore-LICENSE.txt
new file mode 100644
index 0000000..72819a9
--- /dev/null
+++ b/plugins/discovery-ec2/licenses/httpcore-LICENSE.txt
@@ -0,0 +1,241 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+=========================================================================
+
+This project contains annotations in the package org.apache.http.annotation
+which are derived from JCIP-ANNOTATIONS
+Copyright (c) 2005 Brian Goetz and Tim Peierls.
+See http://www.jcip.net and the Creative Commons Attribution License
+(http://creativecommons.org/licenses/by/2.5)
+Full text: http://creativecommons.org/licenses/by/2.5/legalcode
+
+License
+
+THE WORK (AS DEFINED BELOW) IS PROVIDED UNDER THE TERMS OF THIS CREATIVE COMMONS PUBLIC LICENSE ("CCPL" OR "LICENSE"). THE WORK IS PROTECTED BY COPYRIGHT AND/OR OTHER APPLICABLE LAW. ANY USE OF THE WORK OTHER THAN AS AUTHORIZED UNDER THIS LICENSE OR COPYRIGHT LAW IS PROHIBITED.
+
+BY EXERCISING ANY RIGHTS TO THE WORK PROVIDED HERE, YOU ACCEPT AND AGREE TO BE BOUND BY THE TERMS OF THIS LICENSE. THE LICENSOR GRANTS YOU THE RIGHTS CONTAINED HERE IN CONSIDERATION OF YOUR ACCEPTANCE OF SUCH TERMS AND CONDITIONS.
+
+1. Definitions
+
+    "Collective Work" means a work, such as a periodical issue, anthology or encyclopedia, in which the Work in its entirety in unmodified form, along with a number of other contributions, constituting separate and independent works in themselves, are assembled into a collective whole. A work that constitutes a Collective Work will not be considered a Derivative Work (as defined below) for the purposes of this License.
+    "Derivative Work" means a work based upon the Work or upon the Work and other pre-existing works, such as a translation, musical arrangement, dramatization, fictionalization, motion picture version, sound recording, art reproduction, abridgment, condensation, or any other form in which the Work may be recast, transformed, or adapted, except that a work that constitutes a Collective Work will not be considered a Derivative Work for the purpose of this License. For the avoidance of doubt, where the Work is a musical composition or sound recording, the synchronization of the Work in timed-relation with a moving image ("synching") will be considered a Derivative Work for the purpose of this License.
+    "Licensor" means the individual or entity that offers the Work under the terms of this License.
+    "Original Author" means the individual or entity who created the Work.
+    "Work" means the copyrightable work of authorship offered under the terms of this License.
+    "You" means an individual or entity exercising rights under this License who has not previously violated the terms of this License with respect to the Work, or who has received express permission from the Licensor to exercise rights under this License despite a previous violation.
+
+2. Fair Use Rights. Nothing in this license is intended to reduce, limit, or restrict any rights arising from fair use, first sale or other limitations on the exclusive rights of the copyright owner under copyright law or other applicable laws.
+
+3. License Grant. Subject to the terms and conditions of this License, Licensor hereby grants You a worldwide, royalty-free, non-exclusive, perpetual (for the duration of the applicable copyright) license to exercise the rights in the Work as stated below:
+
+    to reproduce the Work, to incorporate the Work into one or more Collective Works, and to reproduce the Work as incorporated in the Collective Works;
+    to create and reproduce Derivative Works;
+    to distribute copies or phonorecords of, display publicly, perform publicly, and perform publicly by means of a digital audio transmission the Work including as incorporated in Collective Works;
+    to distribute copies or phonorecords of, display publicly, perform publicly, and perform publicly by means of a digital audio transmission Derivative Works.
+
+    For the avoidance of doubt, where the work is a musical composition:
+        Performance Royalties Under Blanket Licenses. Licensor waives the exclusive right to collect, whether individually or via a performance rights society (e.g. ASCAP, BMI, SESAC), royalties for the public performance or public digital performance (e.g. webcast) of the Work.
+        Mechanical Rights and Statutory Royalties. Licensor waives the exclusive right to collect, whether individually or via a music rights agency or designated agent (e.g. Harry Fox Agency), royalties for any phonorecord You create from the Work ("cover version") and distribute, subject to the compulsory license created by 17 USC Section 115 of the US Copyright Act (or the equivalent in other jurisdictions).
+    Webcasting Rights and Statutory Royalties. For the avoidance of doubt, where the Work is a sound recording, Licensor waives the exclusive right to collect, whether individually or via a performance-rights society (e.g. SoundExchange), royalties for the public digital performance (e.g. webcast) of the Work, subject to the compulsory license created by 17 USC Section 114 of the US Copyright Act (or the equivalent in other jurisdictions).
+
+The above rights may be exercised in all media and formats whether now known or hereafter devised. The above rights include the right to make such modifications as are technically necessary to exercise the rights in other media and formats. All rights not expressly granted by Licensor are hereby reserved.
+
+4. Restrictions.The license granted in Section 3 above is expressly made subject to and limited by the following restrictions:
+
+    You may distribute, publicly display, publicly perform, or publicly digitally perform the Work only under the terms of this License, and You must include a copy of, or the Uniform Resource Identifier for, this License with every copy or phonorecord of the Work You distribute, publicly display, publicly perform, or publicly digitally perform. You may not offer or impose any terms on the Work that alter or restrict the terms of this License or the recipients' exercise of the rights granted hereunder. You may not sublicense the Work. You must keep intact all notices that refer to this License and to the disclaimer of warranties. You may not distribute, publicly display, publicly perform, or publicly digitally perform the Work with any technological measures that control access or use of the Work in a manner inconsistent with the terms of this License Agreement. The above applies to the Work as incorporated in a Collective Work, but this does not require the Collective Work apart from the Work itself to be made subject to the terms of this License. If You create a Collective Work, upon notice from any Licensor You must, to the extent practicable, remove from the Collective Work any credit as required by clause 4(b), as requested. If You create a Derivative Work, upon notice from any Licensor You must, to the extent practicable, remove from the Derivative Work any credit as required by clause 4(b), as requested.
+    If you distribute, publicly display, publicly perform, or publicly digitally perform the Work or any Derivative Works or Collective Works, You must keep intact all copyright notices for the Work and provide, reasonable to the medium or means You are utilizing: (i) the name of the Original Author (or pseudonym, if applicable) if supplied, and/or (ii) if the Original Author and/or Licensor designate another party or parties (e.g. a sponsor institute, publishing entity, journal) for attribution in Licensor's copyright notice, terms of service or by other reasonable means, the name of such party or parties; the title of the Work if supplied; to the extent reasonably practicable, the Uniform Resource Identifier, if any, that Licensor specifies to be associated with the Work, unless such URI does not refer to the copyright notice or licensing information for the Work; and in the case of a Derivative Work, a credit identifying the use of the Work in the Derivative Work (e.g., "French translation of the Work by Original Author," or "Screenplay based on original Work by Original Author"). Such credit may be implemented in any reasonable manner; provided, however, that in the case of a Derivative Work or Collective Work, at a minimum such credit will appear where any other comparable authorship credit appears and in a manner at least as prominent as such other comparable authorship credit.
+
+5. Representations, Warranties and Disclaimer
+
+UNLESS OTHERWISE MUTUALLY AGREED TO BY THE PARTIES IN WRITING, LICENSOR OFFERS THE WORK AS-IS AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND CONCERNING THE WORK, EXPRESS, IMPLIED, STATUTORY OR OTHERWISE, INCLUDING, WITHOUT LIMITATION, WARRANTIES OF TITLE, MERCHANTIBILITY, FITNESS FOR A PARTICULAR PURPOSE, NONINFRINGEMENT, OR THE ABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OF ABSENCE OF ERRORS, WHETHER OR NOT DISCOVERABLE. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF IMPLIED WARRANTIES, SO SUCH EXCLUSION MAY NOT APPLY TO YOU.
+
+6. Limitation on Liability. EXCEPT TO THE EXTENT REQUIRED BY APPLICABLE LAW, IN NO EVENT WILL LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY FOR ANY SPECIAL, INCIDENTAL, CONSEQUENTIAL, PUNITIVE OR EXEMPLARY DAMAGES ARISING OUT OF THIS LICENSE OR THE USE OF THE WORK, EVEN IF LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+
+7. Termination
+
+    This License and the rights granted hereunder will terminate automatically upon any breach by You of the terms of this License. Individuals or entities who have received Derivative Works or Collective Works from You under this License, however, will not have their licenses terminated provided such individuals or entities remain in full compliance with those licenses. Sections 1, 2, 5, 6, 7, and 8 will survive any termination of this License.
+    Subject to the above terms and conditions, the license granted here is perpetual (for the duration of the applicable copyright in the Work). Notwithstanding the above, Licensor reserves the right to release the Work under different license terms or to stop distributing the Work at any time; provided, however that any such election will not serve to withdraw this License (or any other license that has been, or is required to be, granted under the terms of this License), and this License will continue in full force and effect unless terminated as stated above.
+
+8. Miscellaneous
+
+    Each time You distribute or publicly digitally perform the Work or a Collective Work, the Licensor offers to the recipient a license to the Work on the same terms and conditions as the license granted to You under this License.
+    Each time You distribute or publicly digitally perform a Derivative Work, Licensor offers to the recipient a license to the original Work on the same terms and conditions as the license granted to You under this License.
+    If any provision of this License is invalid or unenforceable under applicable law, it shall not affect the validity or enforceability of the remainder of the terms of this License, and without further action by the parties to this agreement, such provision shall be reformed to the minimum extent necessary to make such provision valid and enforceable.
+    No term or provision of this License shall be deemed waived and no breach consented to unless such waiver or consent shall be in writing and signed by the party to be charged with such waiver or consent.
+    This License constitutes the entire agreement between the parties with respect to the Work licensed here. There are no understandings, agreements or representations with respect to the Work not specified here. Licensor shall not be bound by any additional provisions that may appear in any communication from You. This License may not be modified without the mutual written agreement of the Licensor and You.
diff --git a/plugins/discovery-ec2/licenses/httpcore-NOTICE.txt b/plugins/discovery-ec2/licenses/httpcore-NOTICE.txt
new file mode 100644
index 0000000..c0be50a
--- /dev/null
+++ b/plugins/discovery-ec2/licenses/httpcore-NOTICE.txt
@@ -0,0 +1,8 @@
+Apache HttpComponents Core
+Copyright 2005-2014 The Apache Software Foundation
+
+This product includes software developed at
+The Apache Software Foundation (http://www.apache.org/).
+
+This project contains annotations derived from JCIP-ANNOTATIONS
+Copyright (c) 2005 Brian Goetz and Tim Peierls. See http://www.jcip.net
diff --git a/plugins/discovery-ec2/licenses/jackson-LICENSE b/plugins/discovery-ec2/licenses/jackson-LICENSE
new file mode 100644
index 0000000..f5f45d2
--- /dev/null
+++ b/plugins/discovery-ec2/licenses/jackson-LICENSE
@@ -0,0 +1,8 @@
+This copy of Jackson JSON processor streaming parser/generator is licensed under the
+Apache (Software) License, version 2.0 ("the License").
+See the License for details about distribution rights, and the
+specific rights regarding derivate works.
+
+You may obtain a copy of the License at:
+
+http://www.apache.org/licenses/LICENSE-2.0
diff --git a/plugins/discovery-ec2/licenses/jackson-NOTICE b/plugins/discovery-ec2/licenses/jackson-NOTICE
new file mode 100644
index 0000000..4c976b7
--- /dev/null
+++ b/plugins/discovery-ec2/licenses/jackson-NOTICE
@@ -0,0 +1,20 @@
+# Jackson JSON processor
+
+Jackson is a high-performance, Free/Open Source JSON processing library.
+It was originally written by Tatu Saloranta (tatu.saloranta@iki.fi), and has
+been in development since 2007.
+It is currently developed by a community of developers, as well as supported
+commercially by FasterXML.com.
+
+## Licensing
+
+Jackson core and extension components may licensed under different licenses.
+To find the details that apply to this artifact see the accompanying LICENSE file.
+For more information, including possible other licensing options, contact
+FasterXML.com (http://fasterxml.com).
+
+## Credits
+
+A list of contributors may be found from CREDITS file, which is included
+in some artifacts (usually source distributions); but is always available
+from the source code management (SCM) system project uses.
diff --git a/plugins/discovery-ec2/licenses/jackson-annotations-2.5.0.jar.sha1 b/plugins/discovery-ec2/licenses/jackson-annotations-2.5.0.jar.sha1
new file mode 100644
index 0000000..862ac6f
--- /dev/null
+++ b/plugins/discovery-ec2/licenses/jackson-annotations-2.5.0.jar.sha1
@@ -0,0 +1 @@
+a2a55a3375bc1cef830ca426d68d2ea22961190e
diff --git a/plugins/discovery-ec2/licenses/jackson-databind-2.5.3.jar.sha1 b/plugins/discovery-ec2/licenses/jackson-databind-2.5.3.jar.sha1
new file mode 100644
index 0000000..cdc6695
--- /dev/null
+++ b/plugins/discovery-ec2/licenses/jackson-databind-2.5.3.jar.sha1
@@ -0,0 +1 @@
+c37875ff66127d93e5f672708cb2dcc14c8232ab
diff --git a/plugins/discovery-ec2/pom.xml b/plugins/discovery-ec2/pom.xml
new file mode 100644
index 0000000..85958dd
--- /dev/null
+++ b/plugins/discovery-ec2/pom.xml
@@ -0,0 +1,50 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project xmlns="http://maven.apache.org/POM/4.0.0"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+
+    <parent>
+        <groupId>org.elasticsearch.plugin</groupId>
+        <artifactId>plugins</artifactId>
+        <version>3.0.0-SNAPSHOT</version>
+    </parent>
+
+    <artifactId>discovery-ec2</artifactId>
+    <name>Plugin: Discovery: EC2</name>
+    <description>The EC2 discovery plugin allows to use AWS API for the unicast discovery mechanism.</description>
+
+    <properties>
+        <elasticsearch.plugin.classname>org.elasticsearch.plugin.discovery.ec2.Ec2DiscoveryPlugin</elasticsearch.plugin.classname>
+        <amazonaws.version>1.10.12</amazonaws.version>
+        <tests.jvms>1</tests.jvms>
+        <tests.rest.suite>discovery_ec2</tests.rest.suite>
+        <tests.rest.load_packaged>false</tests.rest.load_packaged>
+    </properties>
+
+    <dependencies>
+        <!-- AWS SDK -->
+        <dependency>
+            <groupId>com.amazonaws</groupId>
+            <artifactId>aws-java-sdk-ec2</artifactId>
+            <version>${amazonaws.version}</version>
+        </dependency>
+        <!-- We need to force here the compile scope as it was defined as test scope in plugins/pom.xml -->
+        <!-- TODO: remove this dependency when we will have a REST Test module -->
+        <dependency>
+            <groupId>org.apache.httpcomponents</groupId>
+            <artifactId>httpclient</artifactId>
+            <scope>compile</scope>
+        </dependency>
+    </dependencies>
+
+    <build>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
diff --git a/plugins/discovery-ec2/rest-api-spec/test/discovery_ec2/10_basic.yaml b/plugins/discovery-ec2/rest-api-spec/test/discovery_ec2/10_basic.yaml
new file mode 100644
index 0000000..e3b7844
--- /dev/null
+++ b/plugins/discovery-ec2/rest-api-spec/test/discovery_ec2/10_basic.yaml
@@ -0,0 +1,14 @@
+# Integration tests for Discovery EC2 component
+#
+"Discovery EC2 loaded":
+    - do:
+        cluster.state: {}
+
+    # Get master node id
+    - set: { master_node: master }
+
+    - do:
+        nodes.info: {}
+
+    - match:  { nodes.$master.plugins.0.name: discovery-ec2  }
+    - match:  { nodes.$master.plugins.0.jvm: true  }
diff --git a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java
new file mode 100644
index 0000000..f8ecc3c
--- /dev/null
+++ b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java
@@ -0,0 +1,179 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.aws;
+
+import java.util.Locale;
+
+import com.amazonaws.ClientConfiguration;
+import com.amazonaws.Protocol;
+import com.amazonaws.auth.*;
+import com.amazonaws.internal.StaticCredentialsProvider;
+import com.amazonaws.services.ec2.AmazonEC2;
+import com.amazonaws.services.ec2.AmazonEC2Client;
+
+import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.cloud.aws.network.Ec2NameResolver;
+import org.elasticsearch.cloud.aws.node.Ec2CustomNodeAttributes;
+import org.elasticsearch.cluster.node.DiscoveryNodeService;
+import org.elasticsearch.common.component.AbstractLifecycleComponent;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.network.NetworkService;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.settings.SettingsFilter;
+
+/**
+ *
+ */
+public class AwsEc2Service extends AbstractLifecycleComponent<AwsEc2Service> {
+
+    public static final String EC2_METADATA_URL = "http://169.254.169.254/latest/meta-data/";
+
+    private AmazonEC2Client client;
+
+    @Inject
+    public AwsEc2Service(Settings settings, SettingsFilter settingsFilter, NetworkService networkService, DiscoveryNodeService discoveryNodeService) {
+        super(settings);
+        settingsFilter.addFilter("cloud.aws.access_key");
+        settingsFilter.addFilter("cloud.aws.secret_key");
+        // Filter repository-specific settings
+        settingsFilter.addFilter("access_key");
+        settingsFilter.addFilter("secret_key");
+        // add specific ec2 name resolver
+        networkService.addCustomNameResolver(new Ec2NameResolver(settings));
+        discoveryNodeService.addCustomAttributeProvider(new Ec2CustomNodeAttributes(settings));
+    }
+
+    public synchronized AmazonEC2 client() {
+        if (client != null) {
+            return client;
+        }
+
+        ClientConfiguration clientConfiguration = new ClientConfiguration();
+        // the response metadata cache is only there for diagnostics purposes,
+        // but can force objects from every response to the old generation.
+        clientConfiguration.setResponseMetadataCacheSize(0);
+        String protocol = settings.get("cloud.aws.protocol", "https").toLowerCase(Locale.ROOT);
+        protocol = settings.get("cloud.aws.ec2.protocol", protocol).toLowerCase(Locale.ROOT);
+        if ("http".equals(protocol)) {
+            clientConfiguration.setProtocol(Protocol.HTTP);
+        } else if ("https".equals(protocol)) {
+            clientConfiguration.setProtocol(Protocol.HTTPS);
+        } else {
+            throw new IllegalArgumentException("No protocol supported [" + protocol + "], can either be [http] or [https]");
+        }
+        String account = settings.get("cloud.aws.access_key");
+        String key = settings.get("cloud.aws.secret_key");
+
+        String proxyHost = settings.get("cloud.aws.proxy_host");
+        proxyHost = settings.get("cloud.aws.ec2.proxy_host", proxyHost);
+        if (proxyHost != null) {
+            String portString = settings.get("cloud.aws.proxy_port", "80");
+            portString = settings.get("cloud.aws.ec2.proxy_port", portString);
+            Integer proxyPort;
+            try {
+                proxyPort = Integer.parseInt(portString, 10);
+            } catch (NumberFormatException ex) {
+                throw new IllegalArgumentException("The configured proxy port value [" + portString + "] is invalid", ex);
+            }
+            clientConfiguration.withProxyHost(proxyHost).setProxyPort(proxyPort);
+        }
+
+        // #155: we might have 3rd party users using older EC2 API version
+        String awsSigner = settings.get("cloud.aws.ec2.signer", settings.get("cloud.aws.signer"));
+        if (awsSigner != null) {
+            logger.debug("using AWS API signer [{}]", awsSigner);
+            try {
+                AwsSigner.configureSigner(awsSigner, clientConfiguration);
+            } catch (IllegalArgumentException e) {
+                logger.warn("wrong signer set for [cloud.aws.ec2.signer] or [cloud.aws.signer]: [{}]", awsSigner);
+            }
+        }
+
+        AWSCredentialsProvider credentials;
+
+        if (account == null && key == null) {
+            credentials = new AWSCredentialsProviderChain(
+                    new EnvironmentVariableCredentialsProvider(),
+                    new SystemPropertiesCredentialsProvider(),
+                    new InstanceProfileCredentialsProvider()
+            );
+        } else {
+            credentials = new AWSCredentialsProviderChain(
+                    new StaticCredentialsProvider(new BasicAWSCredentials(account, key))
+            );
+        }
+
+        this.client = new AmazonEC2Client(credentials, clientConfiguration);
+
+        if (settings.get("cloud.aws.ec2.endpoint") != null) {
+            String endpoint = settings.get("cloud.aws.ec2.endpoint");
+            logger.debug("using explicit ec2 endpoint [{}]", endpoint);
+            client.setEndpoint(endpoint);
+        } else if (settings.get("cloud.aws.region") != null) {
+            String region = settings.get("cloud.aws.region").toLowerCase(Locale.ROOT);
+            String endpoint;
+            if (region.equals("us-east-1") || region.equals("us-east")) {
+                endpoint = "ec2.us-east-1.amazonaws.com";
+            } else if (region.equals("us-west") || region.equals("us-west-1")) {
+                endpoint = "ec2.us-west-1.amazonaws.com";
+            } else if (region.equals("us-west-2")) {
+                endpoint = "ec2.us-west-2.amazonaws.com";
+            } else if (region.equals("ap-southeast") || region.equals("ap-southeast-1")) {
+                endpoint = "ec2.ap-southeast-1.amazonaws.com";
+            } else if (region.equals("ap-southeast-2")) {
+                endpoint = "ec2.ap-southeast-2.amazonaws.com";
+            } else if (region.equals("ap-northeast") || region.equals("ap-northeast-1")) {
+                endpoint = "ec2.ap-northeast-1.amazonaws.com";
+            } else if (region.equals("eu-west") || region.equals("eu-west-1")) {
+                endpoint = "ec2.eu-west-1.amazonaws.com";
+            } else if (region.equals("eu-central") || region.equals("eu-central-1")) {
+                endpoint = "ec2.eu-central-1.amazonaws.com";
+            } else if (region.equals("sa-east") || region.equals("sa-east-1")) {
+                endpoint = "ec2.sa-east-1.amazonaws.com";
+            } else if (region.equals("cn-north") || region.equals("cn-north-1")) {
+                endpoint = "ec2.cn-north-1.amazonaws.com.cn";
+            } else {
+                throw new IllegalArgumentException("No automatic endpoint could be derived from region [" + region + "]");
+            }
+            if (endpoint != null) {
+                logger.debug("using ec2 region [{}], with endpoint [{}]", region, endpoint);
+                client.setEndpoint(endpoint);
+            }
+        }
+
+        return this.client;
+
+    }
+
+    @Override
+    protected void doStart() throws ElasticsearchException {
+    }
+
+    @Override
+    protected void doStop() throws ElasticsearchException {
+    }
+
+    @Override
+    protected void doClose() throws ElasticsearchException {
+        if (client != null) {
+            client.shutdown();
+        }
+    }
+}
diff --git a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsSigner.java b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsSigner.java
new file mode 100644
index 0000000..476bb74
--- /dev/null
+++ b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsSigner.java
@@ -0,0 +1,53 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.aws;
+
+import com.amazonaws.ClientConfiguration;
+import com.amazonaws.auth.SignerFactory;
+
+public class AwsSigner {
+
+    private AwsSigner() {
+
+    }
+
+    /**
+     * Add a AWS API Signer.
+     * @param signer Signer to use
+     * @param configuration AWS Client configuration
+     * @throws IllegalArgumentException if signer does not exist
+     */
+    public static void configureSigner(String signer, ClientConfiguration configuration)
+        throws IllegalArgumentException {
+
+        if (signer == null) {
+            throw new IllegalArgumentException("[null] signer set");
+        }
+
+        try {
+            // We check this signer actually exists in AWS SDK
+            // It throws a IllegalArgumentException if not found
+            SignerFactory.getSignerByTypeAndService(signer, null);
+            configuration.setSignerOverride(signer);
+        } catch (IllegalArgumentException e) {
+            throw new IllegalArgumentException("wrong signer set [" + signer + "]");
+        }
+    }
+}
diff --git a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/Ec2Module.java b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/Ec2Module.java
new file mode 100644
index 0000000..9852441
--- /dev/null
+++ b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/Ec2Module.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.aws;
+
+import org.elasticsearch.common.inject.AbstractModule;
+
+public class Ec2Module extends AbstractModule {
+
+    @Override
+    protected void configure() {
+        bind(AwsEc2Service.class).asEagerSingleton();
+    }
+}
diff --git a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/network/Ec2NameResolver.java b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/network/Ec2NameResolver.java
new file mode 100755
index 0000000..337a97e
--- /dev/null
+++ b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/network/Ec2NameResolver.java
@@ -0,0 +1,142 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.aws.network;
+
+import org.apache.lucene.util.IOUtils;
+import org.elasticsearch.ExceptionsHelper;
+import org.elasticsearch.cloud.aws.AwsEc2Service;
+import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.network.NetworkService.CustomNameResolver;
+import org.elasticsearch.common.settings.Settings;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.net.InetAddress;
+import java.net.URL;
+import java.net.URLConnection;
+import java.nio.charset.StandardCharsets;
+
+/**
+ * Resolves certain ec2 related 'meta' hostnames into an actual hostname
+ * obtained from ec2 meta-data.
+ * <p/>
+ * Valid config values for {@link Ec2HostnameType}s are -
+ * <ul>
+ * <li>_ec2_ - maps to privateIpv4</li>
+ * <li>_ec2:privateIp_ - maps to privateIpv4</li>
+ * <li>_ec2:privateIpv4_</li>
+ * <li>_ec2:privateDns_</li>
+ * <li>_ec2:publicIp_ - maps to publicIpv4</li>
+ * <li>_ec2:publicIpv4_</li>
+ * <li>_ec2:publicDns_</li>
+ * </ul>
+ *
+ * @author Paul_Loy (keteracel)
+ */
+public class Ec2NameResolver extends AbstractComponent implements CustomNameResolver {
+
+    /**
+     * enum that can be added to over time with more meta-data types (such as ipv6 when this is available)
+     *
+     * @author Paul_Loy
+     */
+    private static enum Ec2HostnameType {
+
+        PRIVATE_IPv4("ec2:privateIpv4", "local-ipv4"),
+        PRIVATE_DNS("ec2:privateDns", "local-hostname"),
+        PUBLIC_IPv4("ec2:publicIpv4", "public-ipv4"),
+        PUBLIC_DNS("ec2:publicDns", "public-hostname"),
+
+        // some less verbose defaults
+        PUBLIC_IP("ec2:publicIp", PUBLIC_IPv4.ec2Name),
+        PRIVATE_IP("ec2:privateIp", PRIVATE_IPv4.ec2Name),
+        EC2("ec2", PRIVATE_IPv4.ec2Name);
+
+        final String configName;
+        final String ec2Name;
+
+        private Ec2HostnameType(String configName, String ec2Name) {
+            this.configName = configName;
+            this.ec2Name = ec2Name;
+        }
+    }
+
+    /**
+     * Construct a {@link CustomNameResolver}.
+     */
+    public Ec2NameResolver(Settings settings) {
+        super(settings);
+    }
+
+    /**
+     * @param type the ec2 hostname type to discover.
+     * @return the appropriate host resolved from ec2 meta-data.
+     * @throws IOException if ec2 meta-data cannot be obtained.
+     * @see CustomNameResolver#resolveIfPossible(String)
+     */
+    public InetAddress[] resolve(Ec2HostnameType type, boolean warnOnFailure) {
+        URLConnection urlConnection = null;
+        InputStream in = null;
+        try {
+            URL url = new URL(AwsEc2Service.EC2_METADATA_URL + type.ec2Name);
+            logger.debug("obtaining ec2 hostname from ec2 meta-data url {}", url);
+            urlConnection = url.openConnection();
+            urlConnection.setConnectTimeout(2000);
+            in = urlConnection.getInputStream();
+            BufferedReader urlReader = new BufferedReader(new InputStreamReader(in, StandardCharsets.UTF_8));
+
+            String metadataResult = urlReader.readLine();
+            if (metadataResult == null || metadataResult.length() == 0) {
+                logger.error("no ec2 metadata returned from {}", url);
+                return null;
+            }
+            // only one address: because we explicitly ask for only one via the Ec2HostnameType
+            return new InetAddress[] { InetAddress.getByName(metadataResult) };
+        } catch (IOException e) {
+            if (warnOnFailure) {
+                logger.warn("failed to get metadata for [" + type.configName + "]: " + ExceptionsHelper.detailedMessage(e));
+            } else {
+                logger.debug("failed to get metadata for [" + type.configName + "]: " + ExceptionsHelper.detailedMessage(e));
+            }
+            return null;
+        } finally {
+            IOUtils.closeWhileHandlingException(in);
+        }
+    }
+
+    @Override
+    public InetAddress[] resolveDefault() {
+        return null; // using this, one has to explicitly specify _ec2_ in network setting
+//        return resolve(Ec2HostnameType.DEFAULT, false);
+    }
+
+    @Override
+    public InetAddress[] resolveIfPossible(String value) {
+        for (Ec2HostnameType type : Ec2HostnameType.values()) {
+            if (type.configName.equals(value)) {
+                return resolve(type, true);
+            }
+        }
+        return null;
+    }
+
+}
diff --git a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/node/Ec2CustomNodeAttributes.java b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/node/Ec2CustomNodeAttributes.java
new file mode 100644
index 0000000..f8f66d0
--- /dev/null
+++ b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/node/Ec2CustomNodeAttributes.java
@@ -0,0 +1,78 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.aws.node;
+
+import org.apache.lucene.util.IOUtils;
+import org.elasticsearch.ExceptionsHelper;
+import org.elasticsearch.cloud.aws.AwsEc2Service;
+import org.elasticsearch.cluster.node.DiscoveryNodeService;
+import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.settings.Settings;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.net.URL;
+import java.net.URLConnection;
+import java.nio.charset.StandardCharsets;
+import java.util.HashMap;
+import java.util.Map;
+
+/**
+ */
+public class Ec2CustomNodeAttributes extends AbstractComponent implements DiscoveryNodeService.CustomAttributesProvider {
+
+    public Ec2CustomNodeAttributes(Settings settings) {
+        super(settings);
+    }
+
+    @Override
+    public Map<String, String> buildAttributes() {
+        if (!settings.getAsBoolean("cloud.node.auto_attributes", false)) {
+            return null;
+        }
+        Map<String, String> ec2Attributes = new HashMap<>();
+
+        URLConnection urlConnection;
+        InputStream in = null;
+        try {
+            URL url = new URL(AwsEc2Service.EC2_METADATA_URL + "placement/availability-zone");
+            logger.debug("obtaining ec2 [placement/availability-zone] from ec2 meta-data url {}", url);
+            urlConnection = url.openConnection();
+            urlConnection.setConnectTimeout(2000);
+            in = urlConnection.getInputStream();
+            BufferedReader urlReader = new BufferedReader(new InputStreamReader(in, StandardCharsets.UTF_8));
+
+            String metadataResult = urlReader.readLine();
+            if (metadataResult == null || metadataResult.length() == 0) {
+                logger.error("no ec2 metadata returned from {}", url);
+                return null;
+            }
+            ec2Attributes.put("aws_availability_zone", metadataResult);
+        } catch (IOException e) {
+            logger.debug("failed to get metadata for [placement/availability-zone]: " + ExceptionsHelper.detailedMessage(e));
+        } finally {
+            IOUtils.closeWhileHandlingException(in);
+        }
+
+        return ec2Attributes;
+    }
+}
diff --git a/plugins/discovery-ec2/src/main/java/org/elasticsearch/discovery/ec2/AwsEc2UnicastHostsProvider.java b/plugins/discovery-ec2/src/main/java/org/elasticsearch/discovery/ec2/AwsEc2UnicastHostsProvider.java
new file mode 100644
index 0000000..ab11541
--- /dev/null
+++ b/plugins/discovery-ec2/src/main/java/org/elasticsearch/discovery/ec2/AwsEc2UnicastHostsProvider.java
@@ -0,0 +1,201 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.ec2;
+
+import com.amazonaws.AmazonClientException;
+import com.amazonaws.services.ec2.AmazonEC2;
+import com.amazonaws.services.ec2.model.*;
+import org.elasticsearch.Version;
+import org.elasticsearch.cloud.aws.AwsEc2Service;
+import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.transport.TransportAddress;
+import org.elasticsearch.discovery.zen.ping.unicast.UnicastHostsProvider;
+import org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing;
+import org.elasticsearch.transport.TransportService;
+
+import java.util.*;
+
+/**
+ *
+ */
+public class AwsEc2UnicastHostsProvider extends AbstractComponent implements UnicastHostsProvider {
+
+    private static enum HostType {
+        PRIVATE_IP,
+        PUBLIC_IP,
+        PRIVATE_DNS,
+        PUBLIC_DNS
+    }
+
+    private final TransportService transportService;
+
+    private final AmazonEC2 client;
+
+    private final Version version;
+
+    private final boolean bindAnyGroup;
+
+    private final Set<String> groups;
+
+    private final Map<String, String> tags;
+
+    private final Set<String> availabilityZones;
+
+    private final HostType hostType;
+
+    @Inject
+    public AwsEc2UnicastHostsProvider(Settings settings, TransportService transportService, AwsEc2Service awsEc2Service, Version version) {
+        super(settings);
+        this.transportService = transportService;
+        this.client = awsEc2Service.client();
+        this.version = version;
+
+        this.hostType = HostType.valueOf(settings.get("discovery.ec2.host_type", "private_ip").toUpperCase(Locale.ROOT));
+
+        this.bindAnyGroup = settings.getAsBoolean("discovery.ec2.any_group", true);
+        this.groups = new HashSet<>();
+        groups.addAll(Arrays.asList(settings.getAsArray("discovery.ec2.groups")));
+
+        this.tags = settings.getByPrefix("discovery.ec2.tag.").getAsMap();
+
+        Set<String> availabilityZones = new HashSet();
+        availabilityZones.addAll(Arrays.asList(settings.getAsArray("discovery.ec2.availability_zones")));
+        if (settings.get("discovery.ec2.availability_zones") != null) {
+            availabilityZones.addAll(Strings.commaDelimitedListToSet(settings.get("discovery.ec2.availability_zones")));
+        }
+        this.availabilityZones = availabilityZones;
+
+        if (logger.isDebugEnabled()) {
+            logger.debug("using host_type [{}], tags [{}], groups [{}] with any_group [{}], availability_zones [{}]", hostType, tags, groups, bindAnyGroup, availabilityZones);
+        }
+    }
+
+    @Override
+    public List<DiscoveryNode> buildDynamicNodes() {
+        List<DiscoveryNode> discoNodes = new ArrayList<>();
+
+        DescribeInstancesResult descInstances;
+        try {
+            // Query EC2 API based on AZ, instance state, and tag.
+
+            // NOTE: we don't filter by security group during the describe instances request for two reasons:
+            // 1. differences in VPCs require different parameters during query (ID vs Name)
+            // 2. We want to use two different strategies: (all security groups vs. any security groups)
+            descInstances = client.describeInstances(buildDescribeInstancesRequest());
+        } catch (AmazonClientException e) {
+            logger.info("Exception while retrieving instance list from AWS API: {}", e.getMessage());
+            logger.debug("Full exception:", e);
+            return discoNodes;
+        }
+
+        logger.trace("building dynamic unicast discovery nodes...");
+        for (Reservation reservation : descInstances.getReservations()) {
+            for (Instance instance : reservation.getInstances()) {
+                // lets see if we can filter based on groups
+                if (!groups.isEmpty()) {
+                    List<GroupIdentifier> instanceSecurityGroups = instance.getSecurityGroups();
+                    ArrayList<String> securityGroupNames = new ArrayList<String>();
+                    ArrayList<String> securityGroupIds = new ArrayList<String>();
+                    for (GroupIdentifier sg : instanceSecurityGroups) {
+                        securityGroupNames.add(sg.getGroupName());
+                        securityGroupIds.add(sg.getGroupId());
+                    }
+                    if (bindAnyGroup) {
+                        // We check if we can find at least one group name or one group id in groups.
+                        if (Collections.disjoint(securityGroupNames, groups)
+                                && Collections.disjoint(securityGroupIds, groups)) {
+                            logger.trace("filtering out instance {} based on groups {}, not part of {}", instance.getInstanceId(), instanceSecurityGroups, groups);
+                            // continue to the next instance
+                            continue;
+                        }
+                    } else {
+                        // We need tp match all group names or group ids, otherwise we ignore this instance
+                        if (!(securityGroupNames.containsAll(groups) || securityGroupIds.containsAll(groups))) {
+                            logger.trace("filtering out instance {} based on groups {}, does not include all of {}", instance.getInstanceId(), instanceSecurityGroups, groups);
+                            // continue to the next instance
+                            continue;
+                        }
+                    }
+                }
+
+                String address = null;
+                switch (hostType) {
+                    case PRIVATE_DNS:
+                        address = instance.getPrivateDnsName();
+                        break;
+                    case PRIVATE_IP:
+                        address = instance.getPrivateIpAddress();
+                        break;
+                    case PUBLIC_DNS:
+                        address = instance.getPublicDnsName();
+                        break;
+                    case PUBLIC_IP:
+                        address = instance.getPublicIpAddress();
+                        break;
+                }
+                if (address != null) {
+                    try {
+                        // we only limit to 1 port per address, makes no sense to ping 100 ports
+                        TransportAddress[] addresses = transportService.addressesFromString(address, 1);
+                        for (int i = 0; i < addresses.length; i++) {
+                            logger.trace("adding {}, address {}, transport_address {}", instance.getInstanceId(), address, addresses[i]);
+                            discoNodes.add(new DiscoveryNode("#cloud-" + instance.getInstanceId() + "-" + i, addresses[i], version.minimumCompatibilityVersion()));
+                        }
+                    } catch (Exception e) {
+                        logger.warn("failed ot add {}, address {}", e, instance.getInstanceId(), address);
+                    }
+                } else {
+                    logger.trace("not adding {}, address is null, host_type {}", instance.getInstanceId(), hostType);
+                }
+            }
+        }
+
+        logger.debug("using dynamic discovery nodes {}", discoNodes);
+
+        return discoNodes;
+    }
+
+    private DescribeInstancesRequest buildDescribeInstancesRequest() {
+        DescribeInstancesRequest describeInstancesRequest = new DescribeInstancesRequest()
+            .withFilters(
+                new Filter("instance-state-name").withValues("running", "pending")
+            );
+
+        for (Map.Entry<String, String> tagFilter : tags.entrySet()) {
+            // for a given tag key, OR relationship for multiple different values
+            describeInstancesRequest.withFilters(
+                new Filter("tag:" + tagFilter.getKey()).withValues(tagFilter.getValue())
+            );
+        }
+
+        if (!availabilityZones.isEmpty()) {
+            // OR relationship amongst multiple values of the availability-zone filter
+            describeInstancesRequest.withFilters(
+                new Filter("availability-zone").withValues(availabilityZones)
+            );
+        }
+
+        return describeInstancesRequest;
+    }
+}
diff --git a/plugins/discovery-ec2/src/main/java/org/elasticsearch/discovery/ec2/Ec2Discovery.java b/plugins/discovery-ec2/src/main/java/org/elasticsearch/discovery/ec2/Ec2Discovery.java
new file mode 100755
index 0000000..b599541
--- /dev/null
+++ b/plugins/discovery-ec2/src/main/java/org/elasticsearch/discovery/ec2/Ec2Discovery.java
@@ -0,0 +1,49 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.ec2;
+
+import org.elasticsearch.cluster.ClusterName;
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.cluster.settings.ClusterDynamicSettings;
+import org.elasticsearch.cluster.settings.DynamicSettings;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.discovery.DiscoverySettings;
+import org.elasticsearch.discovery.zen.ZenDiscovery;
+import org.elasticsearch.discovery.zen.elect.ElectMasterService;
+import org.elasticsearch.discovery.zen.ping.ZenPingService;
+import org.elasticsearch.node.settings.NodeSettingsService;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.elasticsearch.transport.TransportService;
+
+/**
+ *
+ */
+public class Ec2Discovery extends ZenDiscovery {
+
+    @Inject
+    public Ec2Discovery(Settings settings, ClusterName clusterName, ThreadPool threadPool, TransportService transportService,
+                        ClusterService clusterService, NodeSettingsService nodeSettingsService, ZenPingService pingService,
+                        DiscoverySettings discoverySettings,
+                        ElectMasterService electMasterService) {
+        super(settings, clusterName, threadPool, transportService, clusterService, nodeSettingsService,
+                pingService, electMasterService, discoverySettings);
+    }
+}
diff --git a/plugins/discovery-ec2/src/main/java/org/elasticsearch/plugin/discovery/ec2/Ec2DiscoveryPlugin.java b/plugins/discovery-ec2/src/main/java/org/elasticsearch/plugin/discovery/ec2/Ec2DiscoveryPlugin.java
new file mode 100644
index 0000000..180581b
--- /dev/null
+++ b/plugins/discovery-ec2/src/main/java/org/elasticsearch/plugin/discovery/ec2/Ec2DiscoveryPlugin.java
@@ -0,0 +1,65 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plugin.discovery.ec2;
+
+import org.elasticsearch.cloud.aws.AwsEc2Service;
+import org.elasticsearch.cloud.aws.Ec2Module;
+import org.elasticsearch.common.component.LifecycleComponent;
+import org.elasticsearch.common.inject.Module;
+import org.elasticsearch.discovery.DiscoveryModule;
+import org.elasticsearch.discovery.ec2.Ec2Discovery;
+import org.elasticsearch.plugins.Plugin;
+
+import java.util.ArrayList;
+import java.util.Collection;
+
+/**
+ *
+ */
+public class Ec2DiscoveryPlugin extends Plugin {
+
+    @Override
+    public String name() {
+        return "discovery-ec2";
+    }
+
+    @Override
+    public String description() {
+        return "EC2 Discovery Plugin";
+    }
+
+    @Override
+    public Collection<Module> nodeModules() {
+        Collection<Module> modules = new ArrayList<>();
+        modules.add(new Ec2Module());
+        return modules;
+    }
+
+    @Override
+    public Collection<Class<? extends LifecycleComponent>> nodeServices() {
+        Collection<Class<? extends LifecycleComponent>> services = new ArrayList<>();
+        services.add(AwsEc2Service.class);
+        return services;
+    }
+
+    public void onModule(DiscoveryModule discoveryModule) {
+        discoveryModule.addDiscoveryType("ec2", Ec2Discovery.class);
+    }
+}
diff --git a/plugins/discovery-ec2/src/test/java/org/elasticsearch/cloud/aws/AWSSignersTests.java b/plugins/discovery-ec2/src/test/java/org/elasticsearch/cloud/aws/AWSSignersTests.java
new file mode 100644
index 0000000..1c460c1
--- /dev/null
+++ b/plugins/discovery-ec2/src/test/java/org/elasticsearch/cloud/aws/AWSSignersTests.java
@@ -0,0 +1,53 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.aws;
+
+import com.amazonaws.ClientConfiguration;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import static org.hamcrest.CoreMatchers.is;
+
+public class AWSSignersTests extends ESTestCase {
+
+    @Test
+    public void testSigners() {
+        assertThat(signerTester(null), is(false));
+        assertThat(signerTester("QueryStringSignerType"), is(true));
+        assertThat(signerTester("AWS3SignerType"), is(true));
+        assertThat(signerTester("AWS4SignerType"), is(true));
+        assertThat(signerTester("NoOpSignerType"), is(true));
+        assertThat(signerTester("UndefinedSigner"), is(false));
+    }
+
+    /**
+     * Test a signer configuration
+     * @param signer signer name
+     * @return true if successful, false otherwise
+     */
+    private boolean signerTester(String signer) {
+        try {
+            AwsSigner.configureSigner(signer, new ClientConfiguration());
+            return true;
+        } catch (IllegalArgumentException e) {
+            return false;
+        }
+    }
+}
diff --git a/plugins/discovery-ec2/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTestCase.java b/plugins/discovery-ec2/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTestCase.java
new file mode 100644
index 0000000..47647e1
--- /dev/null
+++ b/plugins/discovery-ec2/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTestCase.java
@@ -0,0 +1,95 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.aws;
+
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.io.PathUtils;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.settings.SettingsException;
+import org.elasticsearch.plugin.discovery.ec2.Ec2DiscoveryPlugin;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.elasticsearch.test.ESIntegTestCase.ThirdParty;
+import org.junit.After;
+import org.junit.Before;
+
+import java.util.HashMap;
+import java.util.Map;
+
+/**
+ * Base class for AWS tests that require credentials.
+ * <p>
+ * You must specify {@code -Dtests.thirdparty=true -Dtests.config=/path/to/config}
+ * in order to run these tests.
+ */
+@ThirdParty
+public abstract class AbstractAwsTestCase extends ESIntegTestCase {
+
+    /**
+     * Those properties are set by the AWS SDK v1.9.4 and if not ignored,
+     * lead to tests failure (see AbstractRandomizedTest#IGNORED_INVARIANT_PROPERTIES)
+     */
+    private static final String[] AWS_INVARIANT_PROPERTIES = {
+            "com.sun.org.apache.xml.internal.dtm.DTMManager",
+            "javax.xml.parsers.DocumentBuilderFactory"
+    };
+
+    private Map<String, String> properties = new HashMap<>();
+
+    @Before
+    public void saveProperties() {
+        for (String p : AWS_INVARIANT_PROPERTIES) {
+            properties.put(p, System.getProperty(p));
+        }
+    }
+
+    @After
+    public void restoreProperties() {
+        for (String p : AWS_INVARIANT_PROPERTIES) {
+            if (properties.get(p) != null) {
+                System.setProperty(p, properties.get(p));
+            } else {
+                System.clearProperty(p);
+            }
+        }
+    }
+
+    @Override
+    protected Settings nodeSettings(int nodeOrdinal) {
+                Settings.Builder settings = Settings.builder()
+                .put(super.nodeSettings(nodeOrdinal))
+                .put("path.home", createTempDir())
+                .extendArray("plugin.types", Ec2DiscoveryPlugin.class.getName())
+                .put("cloud.aws.test.random", randomInt())
+                .put("cloud.aws.test.write_failures", 0.1)
+                .put("cloud.aws.test.read_failures", 0.1);
+
+        // if explicit, just load it and don't load from env
+        try {
+            if (Strings.hasText(System.getProperty("tests.config"))) {
+                settings.loadFromPath(PathUtils.get(System.getProperty("tests.config")));
+            } else {
+                throw new IllegalStateException("to run integration tests, you need to set -Dtest.thirdparty=true and -Dtests.config=/path/to/elasticsearch.yml");
+            }
+        } catch (SettingsException exception) {
+            throw new IllegalStateException("your test configuration file is incorrect: " + System.getProperty("tests.config"), exception);
+        }
+        return settings.build();
+    }
+}
diff --git a/plugins/discovery-ec2/src/test/java/org/elasticsearch/cloud/aws/DiscoveryEc2RestIT.java b/plugins/discovery-ec2/src/test/java/org/elasticsearch/cloud/aws/DiscoveryEc2RestIT.java
new file mode 100644
index 0000000..24ccf82
--- /dev/null
+++ b/plugins/discovery-ec2/src/test/java/org/elasticsearch/cloud/aws/DiscoveryEc2RestIT.java
@@ -0,0 +1,41 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.aws;
+
+import com.carrotsearch.randomizedtesting.annotations.Name;
+import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
+import org.elasticsearch.test.rest.ESRestTestCase;
+import org.elasticsearch.test.rest.RestTestCandidate;
+import org.elasticsearch.test.rest.parser.RestTestParseException;
+
+import java.io.IOException;
+
+public class DiscoveryEc2RestIT extends ESRestTestCase {
+
+    public DiscoveryEc2RestIT(@Name("yaml") RestTestCandidate testCandidate) {
+        super(testCandidate);
+    }
+
+    @ParametersFactory
+    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
+        return ESRestTestCase.createParameters(0, 1);
+    }
+}
+
diff --git a/plugins/discovery-ec2/src/test/java/org/elasticsearch/discovery/ec2/Ec2DiscoveryTests.java b/plugins/discovery-ec2/src/test/java/org/elasticsearch/discovery/ec2/Ec2DiscoveryTests.java
new file mode 100644
index 0000000..a794db6
--- /dev/null
+++ b/plugins/discovery-ec2/src/test/java/org/elasticsearch/discovery/ec2/Ec2DiscoveryTests.java
@@ -0,0 +1,49 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.ec2;
+
+
+import org.elasticsearch.cloud.aws.AbstractAwsTestCase;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.plugin.discovery.ec2.Ec2DiscoveryPlugin;
+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
+import org.elasticsearch.test.ESIntegTestCase.Scope;
+import org.junit.Test;
+
+import static org.elasticsearch.common.settings.Settings.settingsBuilder;
+
+/**
+ * Just an empty Node Start test to check everything if fine when
+ * starting.
+ * This test requires AWS to run.
+ */
+@ClusterScope(scope = Scope.TEST, numDataNodes = 0, numClientNodes = 0, transportClientRatio = 0.0)
+public class Ec2DiscoveryTests extends AbstractAwsTestCase {
+
+    @Test
+    public void testStart() {
+        Settings nodeSettings = settingsBuilder()
+                .put("plugin.types", Ec2DiscoveryPlugin.class.getName())
+                .put("discovery.type", "ec2")
+                .build();
+        internalCluster().startNode(nodeSettings);
+    }
+
+}
diff --git a/plugins/discovery-ec2/src/test/java/org/elasticsearch/discovery/ec2/Ec2DiscoveryUpdateSettingsTests.java b/plugins/discovery-ec2/src/test/java/org/elasticsearch/discovery/ec2/Ec2DiscoveryUpdateSettingsTests.java
new file mode 100644
index 0000000..de3efcf
--- /dev/null
+++ b/plugins/discovery-ec2/src/test/java/org/elasticsearch/discovery/ec2/Ec2DiscoveryUpdateSettingsTests.java
@@ -0,0 +1,61 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.ec2;
+
+
+import org.elasticsearch.action.admin.cluster.settings.ClusterUpdateSettingsResponse;
+import org.elasticsearch.cloud.aws.AbstractAwsTestCase;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.plugin.discovery.ec2.Ec2DiscoveryPlugin;
+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
+import org.elasticsearch.test.ESIntegTestCase.Scope;
+import org.junit.Test;
+
+import static org.elasticsearch.common.settings.Settings.settingsBuilder;
+import static org.hamcrest.CoreMatchers.is;
+
+/**
+ * Just an empty Node Start test to check eveything if fine when
+ * starting.
+ * This test requires AWS to run.
+ */
+@ClusterScope(scope = Scope.TEST, numDataNodes = 0, numClientNodes = 0, transportClientRatio = 0.0)
+public class Ec2DiscoveryUpdateSettingsTests extends AbstractAwsTestCase {
+
+    @Test
+    public void testMinimumMasterNodesStart() {
+        Settings nodeSettings = settingsBuilder()
+                .put("plugin.types", Ec2DiscoveryPlugin.class.getName())
+                .put("cloud.enabled", true)
+                .put("discovery.type", "ec2")
+                .build();
+        internalCluster().startNode(nodeSettings);
+
+        // We try to update minimum_master_nodes now
+        ClusterUpdateSettingsResponse response = client().admin().cluster().prepareUpdateSettings()
+                .setPersistentSettings(settingsBuilder().put("discovery.zen.minimum_master_nodes", 1))
+                .setTransientSettings(settingsBuilder().put("discovery.zen.minimum_master_nodes", 1))
+                .get();
+
+        Integer min = response.getPersistentSettings().getAsInt("discovery.zen.minimum_master_nodes", null);
+        assertThat(min, is(1));
+    }
+
+}
diff --git a/plugins/discovery-multicast/pom.xml b/plugins/discovery-multicast/pom.xml
index 937034a..eaa1e11 100644
--- a/plugins/discovery-multicast/pom.xml
+++ b/plugins/discovery-multicast/pom.xml
@@ -7,7 +7,7 @@
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>plugins</artifactId>
-        <version>2.1.0-SNAPSHOT</version>
+        <version>3.0.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>discovery-multicast</artifactId>
diff --git a/plugins/jvm-example/pom.xml b/plugins/jvm-example/pom.xml
index be67cf5..aa7d853 100644
--- a/plugins/jvm-example/pom.xml
+++ b/plugins/jvm-example/pom.xml
@@ -7,7 +7,7 @@
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>plugins</artifactId>
-        <version>2.1.0-SNAPSHOT</version>
+        <version>3.0.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>jvm-example</artifactId>
diff --git a/plugins/lang-javascript/pom.xml b/plugins/lang-javascript/pom.xml
index eaaa29a..a9851a8 100644
--- a/plugins/lang-javascript/pom.xml
+++ b/plugins/lang-javascript/pom.xml
@@ -7,7 +7,7 @@
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>plugins</artifactId>
-        <version>2.1.0-SNAPSHOT</version>
+        <version>3.0.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>lang-javascript</artifactId>
diff --git a/plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptMultiThreadedTest.java b/plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptMultiThreadedTest.java
deleted file mode 100644
index e56ff83..0000000
--- a/plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptMultiThreadedTest.java
+++ /dev/null
@@ -1,171 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.script.javascript;
-
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.script.CompiledScript;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.util.HashMap;
-import java.util.Map;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.CyclicBarrier;
-import java.util.concurrent.ThreadLocalRandom;
-import java.util.concurrent.atomic.AtomicBoolean;
-
-import static org.hamcrest.Matchers.equalTo;
-
-/**
- *
- */
-public class JavaScriptScriptMultiThreadedTest extends ESTestCase {
-
-    @Test
-    public void testExecutableNoRuntimeParams() throws Exception {
-        final JavaScriptScriptEngineService se = new JavaScriptScriptEngineService(Settings.Builder.EMPTY_SETTINGS);
-        final Object compiled = se.compile("x + y");
-        final AtomicBoolean failed = new AtomicBoolean();
-
-        Thread[] threads = new Thread[50];
-        final CountDownLatch latch = new CountDownLatch(threads.length);
-        final CyclicBarrier barrier = new CyclicBarrier(threads.length + 1);
-        for (int i = 0; i < threads.length; i++) {
-            threads[i] = new Thread(new Runnable() {
-                @Override
-                public void run() {
-                    try {
-                        barrier.await();
-                        long x = ThreadLocalRandom.current().nextInt();
-                        long y = ThreadLocalRandom.current().nextInt();
-                        long addition = x + y;
-                        Map<String, Object> vars = new HashMap<String, Object>();
-                        vars.put("x", x);
-                        vars.put("y", y);
-                        ExecutableScript script = se.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "testExecutableNoRuntimeParams", "js", compiled), vars);
-                        for (int i = 0; i < 100000; i++) {
-                            long result = ((Number) script.run()).longValue();
-                            assertThat(result, equalTo(addition));
-                        }
-                    } catch (Throwable t) {
-                        failed.set(true);
-                        logger.error("failed", t);
-                    } finally {
-                        latch.countDown();
-                    }
-                }
-            });
-        }
-        for (int i = 0; i < threads.length; i++) {
-            threads[i].start();
-        }
-        barrier.await();
-        latch.await();
-        assertThat(failed.get(), equalTo(false));
-    }
-
-
-    @Test
-    public void testExecutableWithRuntimeParams() throws Exception {
-        final JavaScriptScriptEngineService se = new JavaScriptScriptEngineService(Settings.Builder.EMPTY_SETTINGS);
-        final Object compiled = se.compile("x + y");
-        final AtomicBoolean failed = new AtomicBoolean();
-
-        Thread[] threads = new Thread[50];
-        final CountDownLatch latch = new CountDownLatch(threads.length);
-        final CyclicBarrier barrier = new CyclicBarrier(threads.length + 1);
-        for (int i = 0; i < threads.length; i++) {
-            threads[i] = new Thread(new Runnable() {
-                @Override
-                public void run() {
-                    try {
-                        barrier.await();
-                        long x = ThreadLocalRandom.current().nextInt();
-                        Map<String, Object> vars = new HashMap<String, Object>();
-                        vars.put("x", x);
-                        ExecutableScript script = se.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "testExecutableNoRuntimeParams", "js", compiled), vars);
-                        for (int i = 0; i < 100000; i++) {
-                            long y = ThreadLocalRandom.current().nextInt();
-                            long addition = x + y;
-                            script.setNextVar("y", y);
-                            long result = ((Number) script.run()).longValue();
-                            assertThat(result, equalTo(addition));
-                        }
-                    } catch (Throwable t) {
-                        failed.set(true);
-                        logger.error("failed", t);
-                    } finally {
-                        latch.countDown();
-                    }
-                }
-            });
-        }
-        for (int i = 0; i < threads.length; i++) {
-            threads[i].start();
-        }
-        barrier.await();
-        latch.await();
-        assertThat(failed.get(), equalTo(false));
-    }
-
-    @Test
-    public void testExecute() throws Exception {
-        final JavaScriptScriptEngineService se = new JavaScriptScriptEngineService(Settings.Builder.EMPTY_SETTINGS);
-        final Object compiled = se.compile("x + y");
-        final AtomicBoolean failed = new AtomicBoolean();
-
-        Thread[] threads = new Thread[50];
-        final CountDownLatch latch = new CountDownLatch(threads.length);
-        final CyclicBarrier barrier = new CyclicBarrier(threads.length + 1);
-        for (int i = 0; i < threads.length; i++) {
-            threads[i] = new Thread(new Runnable() {
-                @Override
-                public void run() {
-                    try {
-                        barrier.await();
-                        Map<String, Object> runtimeVars = new HashMap<String, Object>();
-                        for (int i = 0; i < 100000; i++) {
-                            long x = ThreadLocalRandom.current().nextInt();
-                            long y = ThreadLocalRandom.current().nextInt();
-                            long addition = x + y;
-                            runtimeVars.put("x", x);
-                            runtimeVars.put("y", y);
-                            long result = ((Number) se.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "testExecutableNoRuntimeParams", "js", compiled), runtimeVars)).longValue();
-                            assertThat(result, equalTo(addition));
-                        }
-                    } catch (Throwable t) {
-                        failed.set(true);
-                        logger.error("failed", t);
-                    } finally {
-                        latch.countDown();
-                    }
-                }
-            });
-        }
-        for (int i = 0; i < threads.length; i++) {
-            threads[i].start();
-        }
-        barrier.await();
-        latch.await();
-        assertThat(failed.get(), equalTo(false));
-    }
-}
diff --git a/plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptMultiThreadedTests.java b/plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptMultiThreadedTests.java
new file mode 100644
index 0000000..1d9090d
--- /dev/null
+++ b/plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptMultiThreadedTests.java
@@ -0,0 +1,171 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script.javascript;
+
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.script.CompiledScript;
+import org.elasticsearch.script.ExecutableScript;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.CyclicBarrier;
+import java.util.concurrent.ThreadLocalRandom;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+import static org.hamcrest.Matchers.equalTo;
+
+/**
+ *
+ */
+public class JavaScriptScriptMultiThreadedTests extends ESTestCase {
+
+    @Test
+    public void testExecutableNoRuntimeParams() throws Exception {
+        final JavaScriptScriptEngineService se = new JavaScriptScriptEngineService(Settings.Builder.EMPTY_SETTINGS);
+        final Object compiled = se.compile("x + y");
+        final AtomicBoolean failed = new AtomicBoolean();
+
+        Thread[] threads = new Thread[50];
+        final CountDownLatch latch = new CountDownLatch(threads.length);
+        final CyclicBarrier barrier = new CyclicBarrier(threads.length + 1);
+        for (int i = 0; i < threads.length; i++) {
+            threads[i] = new Thread(new Runnable() {
+                @Override
+                public void run() {
+                    try {
+                        barrier.await();
+                        long x = ThreadLocalRandom.current().nextInt();
+                        long y = ThreadLocalRandom.current().nextInt();
+                        long addition = x + y;
+                        Map<String, Object> vars = new HashMap<String, Object>();
+                        vars.put("x", x);
+                        vars.put("y", y);
+                        ExecutableScript script = se.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "testExecutableNoRuntimeParams", "js", compiled), vars);
+                        for (int i = 0; i < 100000; i++) {
+                            long result = ((Number) script.run()).longValue();
+                            assertThat(result, equalTo(addition));
+                        }
+                    } catch (Throwable t) {
+                        failed.set(true);
+                        logger.error("failed", t);
+                    } finally {
+                        latch.countDown();
+                    }
+                }
+            });
+        }
+        for (int i = 0; i < threads.length; i++) {
+            threads[i].start();
+        }
+        barrier.await();
+        latch.await();
+        assertThat(failed.get(), equalTo(false));
+    }
+
+
+    @Test
+    public void testExecutableWithRuntimeParams() throws Exception {
+        final JavaScriptScriptEngineService se = new JavaScriptScriptEngineService(Settings.Builder.EMPTY_SETTINGS);
+        final Object compiled = se.compile("x + y");
+        final AtomicBoolean failed = new AtomicBoolean();
+
+        Thread[] threads = new Thread[50];
+        final CountDownLatch latch = new CountDownLatch(threads.length);
+        final CyclicBarrier barrier = new CyclicBarrier(threads.length + 1);
+        for (int i = 0; i < threads.length; i++) {
+            threads[i] = new Thread(new Runnable() {
+                @Override
+                public void run() {
+                    try {
+                        barrier.await();
+                        long x = ThreadLocalRandom.current().nextInt();
+                        Map<String, Object> vars = new HashMap<String, Object>();
+                        vars.put("x", x);
+                        ExecutableScript script = se.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "testExecutableNoRuntimeParams", "js", compiled), vars);
+                        for (int i = 0; i < 100000; i++) {
+                            long y = ThreadLocalRandom.current().nextInt();
+                            long addition = x + y;
+                            script.setNextVar("y", y);
+                            long result = ((Number) script.run()).longValue();
+                            assertThat(result, equalTo(addition));
+                        }
+                    } catch (Throwable t) {
+                        failed.set(true);
+                        logger.error("failed", t);
+                    } finally {
+                        latch.countDown();
+                    }
+                }
+            });
+        }
+        for (int i = 0; i < threads.length; i++) {
+            threads[i].start();
+        }
+        barrier.await();
+        latch.await();
+        assertThat(failed.get(), equalTo(false));
+    }
+
+    @Test
+    public void testExecute() throws Exception {
+        final JavaScriptScriptEngineService se = new JavaScriptScriptEngineService(Settings.Builder.EMPTY_SETTINGS);
+        final Object compiled = se.compile("x + y");
+        final AtomicBoolean failed = new AtomicBoolean();
+
+        Thread[] threads = new Thread[50];
+        final CountDownLatch latch = new CountDownLatch(threads.length);
+        final CyclicBarrier barrier = new CyclicBarrier(threads.length + 1);
+        for (int i = 0; i < threads.length; i++) {
+            threads[i] = new Thread(new Runnable() {
+                @Override
+                public void run() {
+                    try {
+                        barrier.await();
+                        Map<String, Object> runtimeVars = new HashMap<String, Object>();
+                        for (int i = 0; i < 100000; i++) {
+                            long x = ThreadLocalRandom.current().nextInt();
+                            long y = ThreadLocalRandom.current().nextInt();
+                            long addition = x + y;
+                            runtimeVars.put("x", x);
+                            runtimeVars.put("y", y);
+                            long result = ((Number) se.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "testExecutableNoRuntimeParams", "js", compiled), runtimeVars)).longValue();
+                            assertThat(result, equalTo(addition));
+                        }
+                    } catch (Throwable t) {
+                        failed.set(true);
+                        logger.error("failed", t);
+                    } finally {
+                        latch.countDown();
+                    }
+                }
+            });
+        }
+        for (int i = 0; i < threads.length; i++) {
+            threads[i].start();
+        }
+        barrier.await();
+        latch.await();
+        assertThat(failed.get(), equalTo(false));
+    }
+}
diff --git a/plugins/lang-python/pom.xml b/plugins/lang-python/pom.xml
index 7b44f61..03f34bf 100644
--- a/plugins/lang-python/pom.xml
+++ b/plugins/lang-python/pom.xml
@@ -7,7 +7,7 @@
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>plugins</artifactId>
-        <version>2.1.0-SNAPSHOT</version>
+        <version>3.0.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>lang-python</artifactId>
diff --git a/plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonScriptMultiThreadedTest.java b/plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonScriptMultiThreadedTest.java
deleted file mode 100644
index c4eb985..0000000
--- a/plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonScriptMultiThreadedTest.java
+++ /dev/null
@@ -1,180 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.script.python;
-
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.script.CompiledScript;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.After;
-import org.junit.Test;
-
-import java.util.HashMap;
-import java.util.Map;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.CyclicBarrier;
-import java.util.concurrent.ThreadLocalRandom;
-import java.util.concurrent.atomic.AtomicBoolean;
-
-import static org.hamcrest.Matchers.equalTo;
-
-/**
- *
- */
-public class PythonScriptMultiThreadedTest extends ESTestCase {
-
-    @After
-    public void close() {
-        // We need to clear some system properties
-        System.clearProperty("python.cachedir.skip");
-        System.clearProperty("python.console.encoding");
-    }
-
-    @Test
-    public void testExecutableNoRuntimeParams() throws Exception {
-        final PythonScriptEngineService se = new PythonScriptEngineService(Settings.Builder.EMPTY_SETTINGS);
-        final Object compiled = se.compile("x + y");
-        final CompiledScript compiledScript = new CompiledScript(ScriptService.ScriptType.INLINE, "testExecutableNoRuntimeParams", "python", compiled);
-        final AtomicBoolean failed = new AtomicBoolean();
-
-        Thread[] threads = new Thread[4];
-        final CountDownLatch latch = new CountDownLatch(threads.length);
-        final CyclicBarrier barrier = new CyclicBarrier(threads.length + 1);
-        for (int i = 0; i < threads.length; i++) {
-            threads[i] = new Thread(new Runnable() {
-                @Override
-                public void run() {
-                    try {
-                        barrier.await();
-                        long x = ThreadLocalRandom.current().nextInt();
-                        long y = ThreadLocalRandom.current().nextInt();
-                        long addition = x + y;
-                        Map<String, Object> vars = new HashMap<String, Object>();
-                        vars.put("x", x);
-                        vars.put("y", y);
-                        ExecutableScript script = se.executable(compiledScript, vars);
-                        for (int i = 0; i < 10000; i++) {
-                            long result = ((Number) script.run()).longValue();
-                            assertThat(result, equalTo(addition));
-                        }
-                    } catch (Throwable t) {
-                        failed.set(true);
-                        logger.error("failed", t);
-                    } finally {
-                        latch.countDown();
-                    }
-                }
-            });
-        }
-        for (int i = 0; i < threads.length; i++) {
-            threads[i].start();
-        }
-        barrier.await();
-        latch.await();
-        assertThat(failed.get(), equalTo(false));
-    }
-
-
-//    @Test public void testExecutableWithRuntimeParams() throws Exception {
-//        final PythonScriptEngineService se = new PythonScriptEngineService(Settings.Builder.EMPTY_SETTINGS);
-//        final Object compiled = se.compile("x + y");
-//        final AtomicBoolean failed = new AtomicBoolean();
-//
-//        Thread[] threads = new Thread[50];
-//        final CountDownLatch latch = new CountDownLatch(threads.length);
-//        final CyclicBarrier barrier = new CyclicBarrier(threads.length + 1);
-//        for (int i = 0; i < threads.length; i++) {
-//            threads[i] = new Thread(new Runnable() {
-//                @Override public void run() {
-//                    try {
-//                        barrier.await();
-//                        long x = ThreadLocalRandom.current().nextInt();
-//                        Map<String, Object> vars = new HashMap<String, Object>();
-//                        vars.put("x", x);
-//                        ExecutableScript script = se.executable(compiled, vars);
-//                        Map<String, Object> runtimeVars = new HashMap<String, Object>();
-//                        for (int i = 0; i < 100000; i++) {
-//                            long y = ThreadLocalRandom.current().nextInt();
-//                            long addition = x + y;
-//                            runtimeVars.put("y", y);
-//                            long result = ((Number) script.run(runtimeVars)).longValue();
-//                            assertThat(result, equalTo(addition));
-//                        }
-//                    } catch (Throwable t) {
-//                        failed.set(true);
-//                        logger.error("failed", t);
-//                    } finally {
-//                        latch.countDown();
-//                    }
-//                }
-//            });
-//        }
-//        for (int i = 0; i < threads.length; i++) {
-//            threads[i].start();
-//        }
-//        barrier.await();
-//        latch.await();
-//        assertThat(failed.get(), equalTo(false));
-//    }
-
-    @Test
-    public void testExecute() throws Exception {
-        final PythonScriptEngineService se = new PythonScriptEngineService(Settings.Builder.EMPTY_SETTINGS);
-        final Object compiled = se.compile("x + y");
-        final CompiledScript compiledScript = new CompiledScript(ScriptService.ScriptType.INLINE, "testExecute", "python", compiled);
-        final AtomicBoolean failed = new AtomicBoolean();
-
-        Thread[] threads = new Thread[4];
-        final CountDownLatch latch = new CountDownLatch(threads.length);
-        final CyclicBarrier barrier = new CyclicBarrier(threads.length + 1);
-        for (int i = 0; i < threads.length; i++) {
-            threads[i] = new Thread(new Runnable() {
-                @Override
-                public void run() {
-                    try {
-                        barrier.await();
-                        Map<String, Object> runtimeVars = new HashMap<String, Object>();
-                        for (int i = 0; i < 10000; i++) {
-                            long x = ThreadLocalRandom.current().nextInt();
-                            long y = ThreadLocalRandom.current().nextInt();
-                            long addition = x + y;
-                            runtimeVars.put("x", x);
-                            runtimeVars.put("y", y);
-                            long result = ((Number) se.execute(compiledScript, runtimeVars)).longValue();
-                            assertThat(result, equalTo(addition));
-                        }
-                    } catch (Throwable t) {
-                        failed.set(true);
-                        logger.error("failed", t);
-                    } finally {
-                        latch.countDown();
-                    }
-                }
-            });
-        }
-        for (int i = 0; i < threads.length; i++) {
-            threads[i].start();
-        }
-        barrier.await();
-        latch.await();
-        assertThat(failed.get(), equalTo(false));
-    }
-}
diff --git a/plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonScriptMultiThreadedTests.java b/plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonScriptMultiThreadedTests.java
new file mode 100644
index 0000000..6798dab
--- /dev/null
+++ b/plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonScriptMultiThreadedTests.java
@@ -0,0 +1,180 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script.python;
+
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.script.CompiledScript;
+import org.elasticsearch.script.ExecutableScript;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.After;
+import org.junit.Test;
+
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.CyclicBarrier;
+import java.util.concurrent.ThreadLocalRandom;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+import static org.hamcrest.Matchers.equalTo;
+
+/**
+ *
+ */
+public class PythonScriptMultiThreadedTests extends ESTestCase {
+
+    @After
+    public void close() {
+        // We need to clear some system properties
+        System.clearProperty("python.cachedir.skip");
+        System.clearProperty("python.console.encoding");
+    }
+
+    @Test
+    public void testExecutableNoRuntimeParams() throws Exception {
+        final PythonScriptEngineService se = new PythonScriptEngineService(Settings.Builder.EMPTY_SETTINGS);
+        final Object compiled = se.compile("x + y");
+        final CompiledScript compiledScript = new CompiledScript(ScriptService.ScriptType.INLINE, "testExecutableNoRuntimeParams", "python", compiled);
+        final AtomicBoolean failed = new AtomicBoolean();
+
+        Thread[] threads = new Thread[4];
+        final CountDownLatch latch = new CountDownLatch(threads.length);
+        final CyclicBarrier barrier = new CyclicBarrier(threads.length + 1);
+        for (int i = 0; i < threads.length; i++) {
+            threads[i] = new Thread(new Runnable() {
+                @Override
+                public void run() {
+                    try {
+                        barrier.await();
+                        long x = ThreadLocalRandom.current().nextInt();
+                        long y = ThreadLocalRandom.current().nextInt();
+                        long addition = x + y;
+                        Map<String, Object> vars = new HashMap<String, Object>();
+                        vars.put("x", x);
+                        vars.put("y", y);
+                        ExecutableScript script = se.executable(compiledScript, vars);
+                        for (int i = 0; i < 10000; i++) {
+                            long result = ((Number) script.run()).longValue();
+                            assertThat(result, equalTo(addition));
+                        }
+                    } catch (Throwable t) {
+                        failed.set(true);
+                        logger.error("failed", t);
+                    } finally {
+                        latch.countDown();
+                    }
+                }
+            });
+        }
+        for (int i = 0; i < threads.length; i++) {
+            threads[i].start();
+        }
+        barrier.await();
+        latch.await();
+        assertThat(failed.get(), equalTo(false));
+    }
+
+
+//    @Test public void testExecutableWithRuntimeParams() throws Exception {
+//        final PythonScriptEngineService se = new PythonScriptEngineService(Settings.Builder.EMPTY_SETTINGS);
+//        final Object compiled = se.compile("x + y");
+//        final AtomicBoolean failed = new AtomicBoolean();
+//
+//        Thread[] threads = new Thread[50];
+//        final CountDownLatch latch = new CountDownLatch(threads.length);
+//        final CyclicBarrier barrier = new CyclicBarrier(threads.length + 1);
+//        for (int i = 0; i < threads.length; i++) {
+//            threads[i] = new Thread(new Runnable() {
+//                @Override public void run() {
+//                    try {
+//                        barrier.await();
+//                        long x = ThreadLocalRandom.current().nextInt();
+//                        Map<String, Object> vars = new HashMap<String, Object>();
+//                        vars.put("x", x);
+//                        ExecutableScript script = se.executable(compiled, vars);
+//                        Map<String, Object> runtimeVars = new HashMap<String, Object>();
+//                        for (int i = 0; i < 100000; i++) {
+//                            long y = ThreadLocalRandom.current().nextInt();
+//                            long addition = x + y;
+//                            runtimeVars.put("y", y);
+//                            long result = ((Number) script.run(runtimeVars)).longValue();
+//                            assertThat(result, equalTo(addition));
+//                        }
+//                    } catch (Throwable t) {
+//                        failed.set(true);
+//                        logger.error("failed", t);
+//                    } finally {
+//                        latch.countDown();
+//                    }
+//                }
+//            });
+//        }
+//        for (int i = 0; i < threads.length; i++) {
+//            threads[i].start();
+//        }
+//        barrier.await();
+//        latch.await();
+//        assertThat(failed.get(), equalTo(false));
+//    }
+
+    @Test
+    public void testExecute() throws Exception {
+        final PythonScriptEngineService se = new PythonScriptEngineService(Settings.Builder.EMPTY_SETTINGS);
+        final Object compiled = se.compile("x + y");
+        final CompiledScript compiledScript = new CompiledScript(ScriptService.ScriptType.INLINE, "testExecute", "python", compiled);
+        final AtomicBoolean failed = new AtomicBoolean();
+
+        Thread[] threads = new Thread[4];
+        final CountDownLatch latch = new CountDownLatch(threads.length);
+        final CyclicBarrier barrier = new CyclicBarrier(threads.length + 1);
+        for (int i = 0; i < threads.length; i++) {
+            threads[i] = new Thread(new Runnable() {
+                @Override
+                public void run() {
+                    try {
+                        barrier.await();
+                        Map<String, Object> runtimeVars = new HashMap<String, Object>();
+                        for (int i = 0; i < 10000; i++) {
+                            long x = ThreadLocalRandom.current().nextInt();
+                            long y = ThreadLocalRandom.current().nextInt();
+                            long addition = x + y;
+                            runtimeVars.put("x", x);
+                            runtimeVars.put("y", y);
+                            long result = ((Number) se.execute(compiledScript, runtimeVars)).longValue();
+                            assertThat(result, equalTo(addition));
+                        }
+                    } catch (Throwable t) {
+                        failed.set(true);
+                        logger.error("failed", t);
+                    } finally {
+                        latch.countDown();
+                    }
+                }
+            });
+        }
+        for (int i = 0; i < threads.length; i++) {
+            threads[i].start();
+        }
+        barrier.await();
+        latch.await();
+        assertThat(failed.get(), equalTo(false));
+    }
+}
diff --git a/plugins/mapper-murmur3/pom.xml b/plugins/mapper-murmur3/pom.xml
index 9c7440d..83dea67 100644
--- a/plugins/mapper-murmur3/pom.xml
+++ b/plugins/mapper-murmur3/pom.xml
@@ -18,7 +18,7 @@ governing permissions and limitations under the License. -->
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>plugins</artifactId>
-        <version>2.1.0-SNAPSHOT</version>
+        <version>3.0.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>mapper-murmur3</artifactId>
diff --git a/plugins/mapper-size/pom.xml b/plugins/mapper-size/pom.xml
index bd483e1..3e148cd 100644
--- a/plugins/mapper-size/pom.xml
+++ b/plugins/mapper-size/pom.xml
@@ -18,7 +18,7 @@ governing permissions and limitations under the License. -->
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>plugins</artifactId>
-        <version>2.1.0-SNAPSHOT</version>
+        <version>3.0.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>mapper-size</artifactId>
diff --git a/plugins/pom.xml b/plugins/pom.xml
index 63ba87d..cd13132 100644
--- a/plugins/pom.xml
+++ b/plugins/pom.xml
@@ -7,7 +7,7 @@
 
     <groupId>org.elasticsearch.plugin</groupId>
     <artifactId>plugins</artifactId>
-    <version>2.1.0-SNAPSHOT</version>
+    <version>3.0.0-SNAPSHOT</version>
     <packaging>pom</packaging>
     <name>Plugin: Parent POM</name>
     <inceptionYear>2009</inceptionYear>
@@ -16,7 +16,7 @@
     <parent>
         <groupId>org.elasticsearch</groupId>
         <artifactId>parent</artifactId>
-        <version>2.1.0-SNAPSHOT</version>
+        <version>3.0.0-SNAPSHOT</version>
     </parent>
 
     <properties>
@@ -425,20 +425,23 @@
     </build>
 
     <modules>
+        <module>analysis-icu</module>
         <module>analysis-kuromoji</module>
+        <module>analysis-phonetic</module>
         <module>analysis-smartcn</module>
         <module>analysis-stempel</module>
-        <module>analysis-phonetic</module>
-        <module>analysis-icu</module>
-        <module>cloud-gce</module>
         <module>cloud-azure</module>
-        <module>cloud-aws</module>
+        <module>cloud-gce</module>
         <module>delete-by-query</module>
+        <module>discovery-ec2</module>
         <module>discovery-multicast</module>
-        <module>lang-python</module>
         <module>lang-javascript</module>
+        <module>lang-python</module>
         <module>mapper-murmur3</module>
         <module>mapper-size</module>
+        <module>repository-s3</module>
+
+        <!-- Internal plugins for test only -->
         <module>jvm-example</module>
         <module>site-example</module>
     </modules>
diff --git a/plugins/repository-s3/LICENSE.txt b/plugins/repository-s3/LICENSE.txt
new file mode 100644
index 0000000..d645695
--- /dev/null
+++ b/plugins/repository-s3/LICENSE.txt
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/plugins/repository-s3/NOTICE.txt b/plugins/repository-s3/NOTICE.txt
new file mode 100644
index 0000000..4880904
--- /dev/null
+++ b/plugins/repository-s3/NOTICE.txt
@@ -0,0 +1,8 @@
+Elasticsearch
+Copyright 2009-2015 Elasticsearch
+
+This product includes software developed by The Apache Software
+Foundation (http://www.apache.org/).
+
+The LICENSE and NOTICE files for all dependencies may be found in the licenses/
+directory.
diff --git a/plugins/repository-s3/licenses/aws-java-sdk-LICENSE.txt b/plugins/repository-s3/licenses/aws-java-sdk-LICENSE.txt
new file mode 100644
index 0000000..98d1f93
--- /dev/null
+++ b/plugins/repository-s3/licenses/aws-java-sdk-LICENSE.txt
@@ -0,0 +1,63 @@
+Apache License
+Version 2.0, January 2004
+
+TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+1. Definitions.
+
+"License" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.
+
+"Licensor" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.
+
+"Legal Entity" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, "control" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.
+
+"You" (or "Your") shall mean an individual or Legal Entity exercising permissions granted by this License.
+
+"Source" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.
+
+"Object" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.
+
+"Work" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).
+
+"Derivative Works" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.
+
+"Contribution" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, "submitted" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as "Not a Contribution."
+
+"Contributor" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.
+
+2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.
+
+3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.
+
+4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:
+
+   1.   You must give any other recipients of the Work or Derivative Works a copy of this License; and
+   2.   You must cause any modified files to carry prominent notices stating that You changed the files; and
+   3.   You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and
+   4.   If the Work includes a "NOTICE" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License.
+
+You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.
+
+5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.
+
+6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.
+
+7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.
+
+8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.
+
+9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.
+
+END OF TERMS AND CONDITIONS
+
+Note: Other license terms may apply to certain, identified software files contained within or distributed with the accompanying software if such terms are included in the directory containing the accompanying software. Such other license terms will then apply in lieu of the terms of the software license above.
+
+JSON processing code subject to the JSON License from JSON.org:
+
+Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
+
+The Software shall be used for Good, not Evil.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
diff --git a/plugins/repository-s3/licenses/aws-java-sdk-NOTICE.txt b/plugins/repository-s3/licenses/aws-java-sdk-NOTICE.txt
new file mode 100644
index 0000000..565bd60
--- /dev/null
+++ b/plugins/repository-s3/licenses/aws-java-sdk-NOTICE.txt
@@ -0,0 +1,15 @@
+AWS SDK for Java
+Copyright 2010-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.
+
+This product includes software developed by
+Amazon Technologies, Inc (http://www.amazon.com/).
+
+**********************
+THIRD PARTY COMPONENTS
+**********************
+This software includes third party software subject to the following copyrights:
+- XML parsing and utility functions from JetS3t - Copyright 2006-2009 James Murty.
+- JSON parsing and utility functions from JSON.org - Copyright 2002 JSON.org.
+- PKCS#1 PEM encoded private key parsing and utility functions from oauth.googlecode.com - Copyright 1998-2010 AOL Inc.
+
+The licenses for these third party components are included in LICENSE.txt
diff --git a/plugins/repository-s3/licenses/aws-java-sdk-core-1.10.12.jar.sha1 b/plugins/repository-s3/licenses/aws-java-sdk-core-1.10.12.jar.sha1
new file mode 100644
index 0000000..659b6cc
--- /dev/null
+++ b/plugins/repository-s3/licenses/aws-java-sdk-core-1.10.12.jar.sha1
@@ -0,0 +1 @@
+7ff51040bbcc9085dcb9a24a2c2a3cc7ac995988
diff --git a/plugins/repository-s3/licenses/aws-java-sdk-kms-1.10.12.jar.sha1 b/plugins/repository-s3/licenses/aws-java-sdk-kms-1.10.12.jar.sha1
new file mode 100644
index 0000000..1948b0d
--- /dev/null
+++ b/plugins/repository-s3/licenses/aws-java-sdk-kms-1.10.12.jar.sha1
@@ -0,0 +1 @@
+31afbe46b65e9933316c7e8dfb8b88dc4b37b6ba
diff --git a/plugins/repository-s3/licenses/aws-java-sdk-s3-1.10.12.jar.sha1 b/plugins/repository-s3/licenses/aws-java-sdk-s3-1.10.12.jar.sha1
new file mode 100644
index 0000000..9814735
--- /dev/null
+++ b/plugins/repository-s3/licenses/aws-java-sdk-s3-1.10.12.jar.sha1
@@ -0,0 +1 @@
+c9e2593fdf398c5f8906a704db037d17b2de4b2a
diff --git a/plugins/repository-s3/licenses/commons-codec-1.6.jar.sha1 b/plugins/repository-s3/licenses/commons-codec-1.6.jar.sha1
new file mode 100644
index 0000000..bf78aff
--- /dev/null
+++ b/plugins/repository-s3/licenses/commons-codec-1.6.jar.sha1
@@ -0,0 +1 @@
+b7f0fc8f61ecadeb3695f0b9464755eee44374d4
diff --git a/plugins/repository-s3/licenses/commons-codec-LICENSE.txt b/plugins/repository-s3/licenses/commons-codec-LICENSE.txt
new file mode 100644
index 0000000..d645695
--- /dev/null
+++ b/plugins/repository-s3/licenses/commons-codec-LICENSE.txt
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/plugins/repository-s3/licenses/commons-codec-NOTICE.txt b/plugins/repository-s3/licenses/commons-codec-NOTICE.txt
new file mode 100644
index 0000000..5691644
--- /dev/null
+++ b/plugins/repository-s3/licenses/commons-codec-NOTICE.txt
@@ -0,0 +1,17 @@
+Apache Commons Codec
+Copyright 2002-2015 The Apache Software Foundation
+
+This product includes software developed at
+The Apache Software Foundation (http://www.apache.org/).
+
+src/test/org/apache/commons/codec/language/DoubleMetaphoneTest.java
+contains test data from http://aspell.net/test/orig/batch0.tab.
+Copyright (C) 2002 Kevin Atkinson (kevina@gnu.org)
+
+===============================================================================
+
+The content of package org.apache.commons.codec.language.bm has been translated
+from the original php source code available at http://stevemorse.org/phoneticinfo.htm
+with permission from the original authors.
+Original source copyright:
+Copyright (c) 2008 Alexander Beider & Stephen P. Morse.
diff --git a/plugins/repository-s3/licenses/commons-logging-1.1.3.jar.sha1 b/plugins/repository-s3/licenses/commons-logging-1.1.3.jar.sha1
new file mode 100644
index 0000000..c8756c4
--- /dev/null
+++ b/plugins/repository-s3/licenses/commons-logging-1.1.3.jar.sha1
@@ -0,0 +1 @@
+f6f66e966c70a83ffbdb6f17a0919eaf7c8aca7f
diff --git a/plugins/repository-s3/licenses/commons-logging-LICENSE.txt b/plugins/repository-s3/licenses/commons-logging-LICENSE.txt
new file mode 100644
index 0000000..57bc88a
--- /dev/null
+++ b/plugins/repository-s3/licenses/commons-logging-LICENSE.txt
@@ -0,0 +1,202 @@
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+
diff --git a/plugins/repository-s3/licenses/commons-logging-NOTICE.txt b/plugins/repository-s3/licenses/commons-logging-NOTICE.txt
new file mode 100644
index 0000000..72eb32a
--- /dev/null
+++ b/plugins/repository-s3/licenses/commons-logging-NOTICE.txt
@@ -0,0 +1,5 @@
+Apache Commons CLI
+Copyright 2001-2009 The Apache Software Foundation
+
+This product includes software developed by
+The Apache Software Foundation (http://www.apache.org/).
diff --git a/plugins/repository-s3/licenses/httpclient-4.3.6.jar.sha1 b/plugins/repository-s3/licenses/httpclient-4.3.6.jar.sha1
new file mode 100644
index 0000000..3d35ee9
--- /dev/null
+++ b/plugins/repository-s3/licenses/httpclient-4.3.6.jar.sha1
@@ -0,0 +1 @@
+4c47155e3e6c9a41a28db36680b828ced53b8af4
diff --git a/plugins/repository-s3/licenses/httpclient-LICENSE.txt b/plugins/repository-s3/licenses/httpclient-LICENSE.txt
new file mode 100644
index 0000000..32f01ed
--- /dev/null
+++ b/plugins/repository-s3/licenses/httpclient-LICENSE.txt
@@ -0,0 +1,558 @@
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+=========================================================================
+
+This project includes Public Suffix List copied from
+<https://publicsuffix.org/list/effective_tld_names.dat>
+licensed under the terms of the Mozilla Public License, v. 2.0
+
+Full license text: <http://mozilla.org/MPL/2.0/>
+
+Mozilla Public License Version 2.0
+==================================
+
+1. Definitions
+--------------
+
+1.1. "Contributor"
+    means each individual or legal entity that creates, contributes to
+    the creation of, or owns Covered Software.
+
+1.2. "Contributor Version"
+    means the combination of the Contributions of others (if any) used
+    by a Contributor and that particular Contributor's Contribution.
+
+1.3. "Contribution"
+    means Covered Software of a particular Contributor.
+
+1.4. "Covered Software"
+    means Source Code Form to which the initial Contributor has attached
+    the notice in Exhibit A, the Executable Form of such Source Code
+    Form, and Modifications of such Source Code Form, in each case
+    including portions thereof.
+
+1.5. "Incompatible With Secondary Licenses"
+    means
+
+    (a) that the initial Contributor has attached the notice described
+        in Exhibit B to the Covered Software; or
+
+    (b) that the Covered Software was made available under the terms of
+        version 1.1 or earlier of the License, but not also under the
+        terms of a Secondary License.
+
+1.6. "Executable Form"
+    means any form of the work other than Source Code Form.
+
+1.7. "Larger Work"
+    means a work that combines Covered Software with other material, in
+    a separate file or files, that is not Covered Software.
+
+1.8. "License"
+    means this document.
+
+1.9. "Licensable"
+    means having the right to grant, to the maximum extent possible,
+    whether at the time of the initial grant or subsequently, any and
+    all of the rights conveyed by this License.
+
+1.10. "Modifications"
+    means any of the following:
+
+    (a) any file in Source Code Form that results from an addition to,
+        deletion from, or modification of the contents of Covered
+        Software; or
+
+    (b) any new file in Source Code Form that contains any Covered
+        Software.
+
+1.11. "Patent Claims" of a Contributor
+    means any patent claim(s), including without limitation, method,
+    process, and apparatus claims, in any patent Licensable by such
+    Contributor that would be infringed, but for the grant of the
+    License, by the making, using, selling, offering for sale, having
+    made, import, or transfer of either its Contributions or its
+    Contributor Version.
+
+1.12. "Secondary License"
+    means either the GNU General Public License, Version 2.0, the GNU
+    Lesser General Public License, Version 2.1, the GNU Affero General
+    Public License, Version 3.0, or any later versions of those
+    licenses.
+
+1.13. "Source Code Form"
+    means the form of the work preferred for making modifications.
+
+1.14. "You" (or "Your")
+    means an individual or a legal entity exercising rights under this
+    License. For legal entities, "You" includes any entity that
+    controls, is controlled by, or is under common control with You. For
+    purposes of this definition, "control" means (a) the power, direct
+    or indirect, to cause the direction or management of such entity,
+    whether by contract or otherwise, or (b) ownership of more than
+    fifty percent (50%) of the outstanding shares or beneficial
+    ownership of such entity.
+
+2. License Grants and Conditions
+--------------------------------
+
+2.1. Grants
+
+Each Contributor hereby grants You a world-wide, royalty-free,
+non-exclusive license:
+
+(a) under intellectual property rights (other than patent or trademark)
+    Licensable by such Contributor to use, reproduce, make available,
+    modify, display, perform, distribute, and otherwise exploit its
+    Contributions, either on an unmodified basis, with Modifications, or
+    as part of a Larger Work; and
+
+(b) under Patent Claims of such Contributor to make, use, sell, offer
+    for sale, have made, import, and otherwise transfer either its
+    Contributions or its Contributor Version.
+
+2.2. Effective Date
+
+The licenses granted in Section 2.1 with respect to any Contribution
+become effective for each Contribution on the date the Contributor first
+distributes such Contribution.
+
+2.3. Limitations on Grant Scope
+
+The licenses granted in this Section 2 are the only rights granted under
+this License. No additional rights or licenses will be implied from the
+distribution or licensing of Covered Software under this License.
+Notwithstanding Section 2.1(b) above, no patent license is granted by a
+Contributor:
+
+(a) for any code that a Contributor has removed from Covered Software;
+    or
+
+(b) for infringements caused by: (i) Your and any other third party's
+    modifications of Covered Software, or (ii) the combination of its
+    Contributions with other software (except as part of its Contributor
+    Version); or
+
+(c) under Patent Claims infringed by Covered Software in the absence of
+    its Contributions.
+
+This License does not grant any rights in the trademarks, service marks,
+or logos of any Contributor (except as may be necessary to comply with
+the notice requirements in Section 3.4).
+
+2.4. Subsequent Licenses
+
+No Contributor makes additional grants as a result of Your choice to
+distribute the Covered Software under a subsequent version of this
+License (see Section 10.2) or under the terms of a Secondary License (if
+permitted under the terms of Section 3.3).
+
+2.5. Representation
+
+Each Contributor represents that the Contributor believes its
+Contributions are its original creation(s) or it has sufficient rights
+to grant the rights to its Contributions conveyed by this License.
+
+2.6. Fair Use
+
+This License is not intended to limit any rights You have under
+applicable copyright doctrines of fair use, fair dealing, or other
+equivalents.
+
+2.7. Conditions
+
+Sections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted
+in Section 2.1.
+
+3. Responsibilities
+-------------------
+
+3.1. Distribution of Source Form
+
+All distribution of Covered Software in Source Code Form, including any
+Modifications that You create or to which You contribute, must be under
+the terms of this License. You must inform recipients that the Source
+Code Form of the Covered Software is governed by the terms of this
+License, and how they can obtain a copy of this License. You may not
+attempt to alter or restrict the recipients' rights in the Source Code
+Form.
+
+3.2. Distribution of Executable Form
+
+If You distribute Covered Software in Executable Form then:
+
+(a) such Covered Software must also be made available in Source Code
+    Form, as described in Section 3.1, and You must inform recipients of
+    the Executable Form how they can obtain a copy of such Source Code
+    Form by reasonable means in a timely manner, at a charge no more
+    than the cost of distribution to the recipient; and
+
+(b) You may distribute such Executable Form under the terms of this
+    License, or sublicense it under different terms, provided that the
+    license for the Executable Form does not attempt to limit or alter
+    the recipients' rights in the Source Code Form under this License.
+
+3.3. Distribution of a Larger Work
+
+You may create and distribute a Larger Work under terms of Your choice,
+provided that You also comply with the requirements of this License for
+the Covered Software. If the Larger Work is a combination of Covered
+Software with a work governed by one or more Secondary Licenses, and the
+Covered Software is not Incompatible With Secondary Licenses, this
+License permits You to additionally distribute such Covered Software
+under the terms of such Secondary License(s), so that the recipient of
+the Larger Work may, at their option, further distribute the Covered
+Software under the terms of either this License or such Secondary
+License(s).
+
+3.4. Notices
+
+You may not remove or alter the substance of any license notices
+(including copyright notices, patent notices, disclaimers of warranty,
+or limitations of liability) contained within the Source Code Form of
+the Covered Software, except that You may alter any license notices to
+the extent required to remedy known factual inaccuracies.
+
+3.5. Application of Additional Terms
+
+You may choose to offer, and to charge a fee for, warranty, support,
+indemnity or liability obligations to one or more recipients of Covered
+Software. However, You may do so only on Your own behalf, and not on
+behalf of any Contributor. You must make it absolutely clear that any
+such warranty, support, indemnity, or liability obligation is offered by
+You alone, and You hereby agree to indemnify every Contributor for any
+liability incurred by such Contributor as a result of warranty, support,
+indemnity or liability terms You offer. You may include additional
+disclaimers of warranty and limitations of liability specific to any
+jurisdiction.
+
+4. Inability to Comply Due to Statute or Regulation
+---------------------------------------------------
+
+If it is impossible for You to comply with any of the terms of this
+License with respect to some or all of the Covered Software due to
+statute, judicial order, or regulation then You must: (a) comply with
+the terms of this License to the maximum extent possible; and (b)
+describe the limitations and the code they affect. Such description must
+be placed in a text file included with all distributions of the Covered
+Software under this License. Except to the extent prohibited by statute
+or regulation, such description must be sufficiently detailed for a
+recipient of ordinary skill to be able to understand it.
+
+5. Termination
+--------------
+
+5.1. The rights granted under this License will terminate automatically
+if You fail to comply with any of its terms. However, if You become
+compliant, then the rights granted under this License from a particular
+Contributor are reinstated (a) provisionally, unless and until such
+Contributor explicitly and finally terminates Your grants, and (b) on an
+ongoing basis, if such Contributor fails to notify You of the
+non-compliance by some reasonable means prior to 60 days after You have
+come back into compliance. Moreover, Your grants from a particular
+Contributor are reinstated on an ongoing basis if such Contributor
+notifies You of the non-compliance by some reasonable means, this is the
+first time You have received notice of non-compliance with this License
+from such Contributor, and You become compliant prior to 30 days after
+Your receipt of the notice.
+
+5.2. If You initiate litigation against any entity by asserting a patent
+infringement claim (excluding declaratory judgment actions,
+counter-claims, and cross-claims) alleging that a Contributor Version
+directly or indirectly infringes any patent, then the rights granted to
+You by any and all Contributors for the Covered Software under Section
+2.1 of this License shall terminate.
+
+5.3. In the event of termination under Sections 5.1 or 5.2 above, all
+end user license agreements (excluding distributors and resellers) which
+have been validly granted by You or Your distributors under this License
+prior to termination shall survive termination.
+
+************************************************************************
+*                                                                      *
+*  6. Disclaimer of Warranty                                           *
+*  -------------------------                                           *
+*                                                                      *
+*  Covered Software is provided under this License on an "as is"       *
+*  basis, without warranty of any kind, either expressed, implied, or  *
+*  statutory, including, without limitation, warranties that the       *
+*  Covered Software is free of defects, merchantable, fit for a        *
+*  particular purpose or non-infringing. The entire risk as to the     *
+*  quality and performance of the Covered Software is with You.        *
+*  Should any Covered Software prove defective in any respect, You     *
+*  (not any Contributor) assume the cost of any necessary servicing,   *
+*  repair, or correction. This disclaimer of warranty constitutes an   *
+*  essential part of this License. No use of any Covered Software is   *
+*  authorized under this License except under this disclaimer.         *
+*                                                                      *
+************************************************************************
+
+************************************************************************
+*                                                                      *
+*  7. Limitation of Liability                                          *
+*  --------------------------                                          *
+*                                                                      *
+*  Under no circumstances and under no legal theory, whether tort      *
+*  (including negligence), contract, or otherwise, shall any           *
+*  Contributor, or anyone who distributes Covered Software as          *
+*  permitted above, be liable to You for any direct, indirect,         *
+*  special, incidental, or consequential damages of any character      *
+*  including, without limitation, damages for lost profits, loss of    *
+*  goodwill, work stoppage, computer failure or malfunction, or any    *
+*  and all other commercial damages or losses, even if such party      *
+*  shall have been informed of the possibility of such damages. This   *
+*  limitation of liability shall not apply to liability for death or   *
+*  personal injury resulting from such party's negligence to the       *
+*  extent applicable law prohibits such limitation. Some               *
+*  jurisdictions do not allow the exclusion or limitation of           *
+*  incidental or consequential damages, so this exclusion and          *
+*  limitation may not apply to You.                                    *
+*                                                                      *
+************************************************************************
+
+8. Litigation
+-------------
+
+Any litigation relating to this License may be brought only in the
+courts of a jurisdiction where the defendant maintains its principal
+place of business and such litigation shall be governed by laws of that
+jurisdiction, without reference to its conflict-of-law provisions.
+Nothing in this Section shall prevent a party's ability to bring
+cross-claims or counter-claims.
+
+9. Miscellaneous
+----------------
+
+This License represents the complete agreement concerning the subject
+matter hereof. If any provision of this License is held to be
+unenforceable, such provision shall be reformed only to the extent
+necessary to make it enforceable. Any law or regulation which provides
+that the language of a contract shall be construed against the drafter
+shall not be used to construe this License against a Contributor.
+
+10. Versions of the License
+---------------------------
+
+10.1. New Versions
+
+Mozilla Foundation is the license steward. Except as provided in Section
+10.3, no one other than the license steward has the right to modify or
+publish new versions of this License. Each version will be given a
+distinguishing version number.
+
+10.2. Effect of New Versions
+
+You may distribute the Covered Software under the terms of the version
+of the License under which You originally received the Covered Software,
+or under the terms of any subsequent version published by the license
+steward.
+
+10.3. Modified Versions
+
+If you create software not governed by this License, and you want to
+create a new license for such software, you may create and use a
+modified version of this License if you rename the license and remove
+any references to the name of the license steward (except to note that
+such modified license differs from this License).
+
+10.4. Distributing Source Code Form that is Incompatible With Secondary
+Licenses
+
+If You choose to distribute Source Code Form that is Incompatible With
+Secondary Licenses under the terms of this version of the License, the
+notice described in Exhibit B of this License must be attached.
+
+Exhibit A - Source Code Form License Notice
+-------------------------------------------
+
+  This Source Code Form is subject to the terms of the Mozilla Public
+  License, v. 2.0. If a copy of the MPL was not distributed with this
+  file, You can obtain one at http://mozilla.org/MPL/2.0/.
+
+If it is not possible or desirable to put the notice in a particular
+file, then You may include the notice in a location (such as a LICENSE
+file in a relevant directory) where a recipient would be likely to look
+for such a notice.
+
+You may add additional accurate notices of copyright ownership.
+
+Exhibit B - "Incompatible With Secondary Licenses" Notice
+---------------------------------------------------------
+
+  This Source Code Form is "Incompatible With Secondary Licenses", as
+  defined by the Mozilla Public License, v. 2.0.
diff --git a/plugins/repository-s3/licenses/httpclient-NOTICE.txt b/plugins/repository-s3/licenses/httpclient-NOTICE.txt
new file mode 100644
index 0000000..4f60581
--- /dev/null
+++ b/plugins/repository-s3/licenses/httpclient-NOTICE.txt
@@ -0,0 +1,5 @@
+Apache HttpComponents Client
+Copyright 1999-2015 The Apache Software Foundation
+
+This product includes software developed at
+The Apache Software Foundation (http://www.apache.org/).
diff --git a/plugins/repository-s3/licenses/httpcore-4.3.3.jar.sha1 b/plugins/repository-s3/licenses/httpcore-4.3.3.jar.sha1
new file mode 100644
index 0000000..5d9c0e2
--- /dev/null
+++ b/plugins/repository-s3/licenses/httpcore-4.3.3.jar.sha1
@@ -0,0 +1 @@
+f91b7a4aadc5cf486df6e4634748d7dd7a73f06d
diff --git a/plugins/repository-s3/licenses/httpcore-LICENSE.txt b/plugins/repository-s3/licenses/httpcore-LICENSE.txt
new file mode 100644
index 0000000..72819a9
--- /dev/null
+++ b/plugins/repository-s3/licenses/httpcore-LICENSE.txt
@@ -0,0 +1,241 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+=========================================================================
+
+This project contains annotations in the package org.apache.http.annotation
+which are derived from JCIP-ANNOTATIONS
+Copyright (c) 2005 Brian Goetz and Tim Peierls.
+See http://www.jcip.net and the Creative Commons Attribution License
+(http://creativecommons.org/licenses/by/2.5)
+Full text: http://creativecommons.org/licenses/by/2.5/legalcode
+
+License
+
+THE WORK (AS DEFINED BELOW) IS PROVIDED UNDER THE TERMS OF THIS CREATIVE COMMONS PUBLIC LICENSE ("CCPL" OR "LICENSE"). THE WORK IS PROTECTED BY COPYRIGHT AND/OR OTHER APPLICABLE LAW. ANY USE OF THE WORK OTHER THAN AS AUTHORIZED UNDER THIS LICENSE OR COPYRIGHT LAW IS PROHIBITED.
+
+BY EXERCISING ANY RIGHTS TO THE WORK PROVIDED HERE, YOU ACCEPT AND AGREE TO BE BOUND BY THE TERMS OF THIS LICENSE. THE LICENSOR GRANTS YOU THE RIGHTS CONTAINED HERE IN CONSIDERATION OF YOUR ACCEPTANCE OF SUCH TERMS AND CONDITIONS.
+
+1. Definitions
+
+    "Collective Work" means a work, such as a periodical issue, anthology or encyclopedia, in which the Work in its entirety in unmodified form, along with a number of other contributions, constituting separate and independent works in themselves, are assembled into a collective whole. A work that constitutes a Collective Work will not be considered a Derivative Work (as defined below) for the purposes of this License.
+    "Derivative Work" means a work based upon the Work or upon the Work and other pre-existing works, such as a translation, musical arrangement, dramatization, fictionalization, motion picture version, sound recording, art reproduction, abridgment, condensation, or any other form in which the Work may be recast, transformed, or adapted, except that a work that constitutes a Collective Work will not be considered a Derivative Work for the purpose of this License. For the avoidance of doubt, where the Work is a musical composition or sound recording, the synchronization of the Work in timed-relation with a moving image ("synching") will be considered a Derivative Work for the purpose of this License.
+    "Licensor" means the individual or entity that offers the Work under the terms of this License.
+    "Original Author" means the individual or entity who created the Work.
+    "Work" means the copyrightable work of authorship offered under the terms of this License.
+    "You" means an individual or entity exercising rights under this License who has not previously violated the terms of this License with respect to the Work, or who has received express permission from the Licensor to exercise rights under this License despite a previous violation.
+
+2. Fair Use Rights. Nothing in this license is intended to reduce, limit, or restrict any rights arising from fair use, first sale or other limitations on the exclusive rights of the copyright owner under copyright law or other applicable laws.
+
+3. License Grant. Subject to the terms and conditions of this License, Licensor hereby grants You a worldwide, royalty-free, non-exclusive, perpetual (for the duration of the applicable copyright) license to exercise the rights in the Work as stated below:
+
+    to reproduce the Work, to incorporate the Work into one or more Collective Works, and to reproduce the Work as incorporated in the Collective Works;
+    to create and reproduce Derivative Works;
+    to distribute copies or phonorecords of, display publicly, perform publicly, and perform publicly by means of a digital audio transmission the Work including as incorporated in Collective Works;
+    to distribute copies or phonorecords of, display publicly, perform publicly, and perform publicly by means of a digital audio transmission Derivative Works.
+
+    For the avoidance of doubt, where the work is a musical composition:
+        Performance Royalties Under Blanket Licenses. Licensor waives the exclusive right to collect, whether individually or via a performance rights society (e.g. ASCAP, BMI, SESAC), royalties for the public performance or public digital performance (e.g. webcast) of the Work.
+        Mechanical Rights and Statutory Royalties. Licensor waives the exclusive right to collect, whether individually or via a music rights agency or designated agent (e.g. Harry Fox Agency), royalties for any phonorecord You create from the Work ("cover version") and distribute, subject to the compulsory license created by 17 USC Section 115 of the US Copyright Act (or the equivalent in other jurisdictions).
+    Webcasting Rights and Statutory Royalties. For the avoidance of doubt, where the Work is a sound recording, Licensor waives the exclusive right to collect, whether individually or via a performance-rights society (e.g. SoundExchange), royalties for the public digital performance (e.g. webcast) of the Work, subject to the compulsory license created by 17 USC Section 114 of the US Copyright Act (or the equivalent in other jurisdictions).
+
+The above rights may be exercised in all media and formats whether now known or hereafter devised. The above rights include the right to make such modifications as are technically necessary to exercise the rights in other media and formats. All rights not expressly granted by Licensor are hereby reserved.
+
+4. Restrictions.The license granted in Section 3 above is expressly made subject to and limited by the following restrictions:
+
+    You may distribute, publicly display, publicly perform, or publicly digitally perform the Work only under the terms of this License, and You must include a copy of, or the Uniform Resource Identifier for, this License with every copy or phonorecord of the Work You distribute, publicly display, publicly perform, or publicly digitally perform. You may not offer or impose any terms on the Work that alter or restrict the terms of this License or the recipients' exercise of the rights granted hereunder. You may not sublicense the Work. You must keep intact all notices that refer to this License and to the disclaimer of warranties. You may not distribute, publicly display, publicly perform, or publicly digitally perform the Work with any technological measures that control access or use of the Work in a manner inconsistent with the terms of this License Agreement. The above applies to the Work as incorporated in a Collective Work, but this does not require the Collective Work apart from the Work itself to be made subject to the terms of this License. If You create a Collective Work, upon notice from any Licensor You must, to the extent practicable, remove from the Collective Work any credit as required by clause 4(b), as requested. If You create a Derivative Work, upon notice from any Licensor You must, to the extent practicable, remove from the Derivative Work any credit as required by clause 4(b), as requested.
+    If you distribute, publicly display, publicly perform, or publicly digitally perform the Work or any Derivative Works or Collective Works, You must keep intact all copyright notices for the Work and provide, reasonable to the medium or means You are utilizing: (i) the name of the Original Author (or pseudonym, if applicable) if supplied, and/or (ii) if the Original Author and/or Licensor designate another party or parties (e.g. a sponsor institute, publishing entity, journal) for attribution in Licensor's copyright notice, terms of service or by other reasonable means, the name of such party or parties; the title of the Work if supplied; to the extent reasonably practicable, the Uniform Resource Identifier, if any, that Licensor specifies to be associated with the Work, unless such URI does not refer to the copyright notice or licensing information for the Work; and in the case of a Derivative Work, a credit identifying the use of the Work in the Derivative Work (e.g., "French translation of the Work by Original Author," or "Screenplay based on original Work by Original Author"). Such credit may be implemented in any reasonable manner; provided, however, that in the case of a Derivative Work or Collective Work, at a minimum such credit will appear where any other comparable authorship credit appears and in a manner at least as prominent as such other comparable authorship credit.
+
+5. Representations, Warranties and Disclaimer
+
+UNLESS OTHERWISE MUTUALLY AGREED TO BY THE PARTIES IN WRITING, LICENSOR OFFERS THE WORK AS-IS AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND CONCERNING THE WORK, EXPRESS, IMPLIED, STATUTORY OR OTHERWISE, INCLUDING, WITHOUT LIMITATION, WARRANTIES OF TITLE, MERCHANTIBILITY, FITNESS FOR A PARTICULAR PURPOSE, NONINFRINGEMENT, OR THE ABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OF ABSENCE OF ERRORS, WHETHER OR NOT DISCOVERABLE. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF IMPLIED WARRANTIES, SO SUCH EXCLUSION MAY NOT APPLY TO YOU.
+
+6. Limitation on Liability. EXCEPT TO THE EXTENT REQUIRED BY APPLICABLE LAW, IN NO EVENT WILL LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY FOR ANY SPECIAL, INCIDENTAL, CONSEQUENTIAL, PUNITIVE OR EXEMPLARY DAMAGES ARISING OUT OF THIS LICENSE OR THE USE OF THE WORK, EVEN IF LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+
+7. Termination
+
+    This License and the rights granted hereunder will terminate automatically upon any breach by You of the terms of this License. Individuals or entities who have received Derivative Works or Collective Works from You under this License, however, will not have their licenses terminated provided such individuals or entities remain in full compliance with those licenses. Sections 1, 2, 5, 6, 7, and 8 will survive any termination of this License.
+    Subject to the above terms and conditions, the license granted here is perpetual (for the duration of the applicable copyright in the Work). Notwithstanding the above, Licensor reserves the right to release the Work under different license terms or to stop distributing the Work at any time; provided, however that any such election will not serve to withdraw this License (or any other license that has been, or is required to be, granted under the terms of this License), and this License will continue in full force and effect unless terminated as stated above.
+
+8. Miscellaneous
+
+    Each time You distribute or publicly digitally perform the Work or a Collective Work, the Licensor offers to the recipient a license to the Work on the same terms and conditions as the license granted to You under this License.
+    Each time You distribute or publicly digitally perform a Derivative Work, Licensor offers to the recipient a license to the original Work on the same terms and conditions as the license granted to You under this License.
+    If any provision of this License is invalid or unenforceable under applicable law, it shall not affect the validity or enforceability of the remainder of the terms of this License, and without further action by the parties to this agreement, such provision shall be reformed to the minimum extent necessary to make such provision valid and enforceable.
+    No term or provision of this License shall be deemed waived and no breach consented to unless such waiver or consent shall be in writing and signed by the party to be charged with such waiver or consent.
+    This License constitutes the entire agreement between the parties with respect to the Work licensed here. There are no understandings, agreements or representations with respect to the Work not specified here. Licensor shall not be bound by any additional provisions that may appear in any communication from You. This License may not be modified without the mutual written agreement of the Licensor and You.
diff --git a/plugins/repository-s3/licenses/httpcore-NOTICE.txt b/plugins/repository-s3/licenses/httpcore-NOTICE.txt
new file mode 100644
index 0000000..c0be50a
--- /dev/null
+++ b/plugins/repository-s3/licenses/httpcore-NOTICE.txt
@@ -0,0 +1,8 @@
+Apache HttpComponents Core
+Copyright 2005-2014 The Apache Software Foundation
+
+This product includes software developed at
+The Apache Software Foundation (http://www.apache.org/).
+
+This project contains annotations derived from JCIP-ANNOTATIONS
+Copyright (c) 2005 Brian Goetz and Tim Peierls. See http://www.jcip.net
diff --git a/plugins/repository-s3/licenses/jackson-LICENSE b/plugins/repository-s3/licenses/jackson-LICENSE
new file mode 100644
index 0000000..f5f45d2
--- /dev/null
+++ b/plugins/repository-s3/licenses/jackson-LICENSE
@@ -0,0 +1,8 @@
+This copy of Jackson JSON processor streaming parser/generator is licensed under the
+Apache (Software) License, version 2.0 ("the License").
+See the License for details about distribution rights, and the
+specific rights regarding derivate works.
+
+You may obtain a copy of the License at:
+
+http://www.apache.org/licenses/LICENSE-2.0
diff --git a/plugins/repository-s3/licenses/jackson-NOTICE b/plugins/repository-s3/licenses/jackson-NOTICE
new file mode 100644
index 0000000..4c976b7
--- /dev/null
+++ b/plugins/repository-s3/licenses/jackson-NOTICE
@@ -0,0 +1,20 @@
+# Jackson JSON processor
+
+Jackson is a high-performance, Free/Open Source JSON processing library.
+It was originally written by Tatu Saloranta (tatu.saloranta@iki.fi), and has
+been in development since 2007.
+It is currently developed by a community of developers, as well as supported
+commercially by FasterXML.com.
+
+## Licensing
+
+Jackson core and extension components may licensed under different licenses.
+To find the details that apply to this artifact see the accompanying LICENSE file.
+For more information, including possible other licensing options, contact
+FasterXML.com (http://fasterxml.com).
+
+## Credits
+
+A list of contributors may be found from CREDITS file, which is included
+in some artifacts (usually source distributions); but is always available
+from the source code management (SCM) system project uses.
diff --git a/plugins/repository-s3/licenses/jackson-annotations-2.5.0.jar.sha1 b/plugins/repository-s3/licenses/jackson-annotations-2.5.0.jar.sha1
new file mode 100644
index 0000000..862ac6f
--- /dev/null
+++ b/plugins/repository-s3/licenses/jackson-annotations-2.5.0.jar.sha1
@@ -0,0 +1 @@
+a2a55a3375bc1cef830ca426d68d2ea22961190e
diff --git a/plugins/repository-s3/licenses/jackson-databind-2.5.3.jar.sha1 b/plugins/repository-s3/licenses/jackson-databind-2.5.3.jar.sha1
new file mode 100644
index 0000000..cdc6695
--- /dev/null
+++ b/plugins/repository-s3/licenses/jackson-databind-2.5.3.jar.sha1
@@ -0,0 +1 @@
+c37875ff66127d93e5f672708cb2dcc14c8232ab
diff --git a/plugins/repository-s3/pom.xml b/plugins/repository-s3/pom.xml
new file mode 100644
index 0000000..8b71e92
--- /dev/null
+++ b/plugins/repository-s3/pom.xml
@@ -0,0 +1,50 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project xmlns="http://maven.apache.org/POM/4.0.0"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+
+    <parent>
+        <groupId>org.elasticsearch.plugin</groupId>
+        <artifactId>plugins</artifactId>
+        <version>3.0.0-SNAPSHOT</version>
+    </parent>
+
+    <artifactId>repository-s3</artifactId>
+    <name>Plugin: Repository: S3</name>
+    <description>The S3 repository plugin adds S3 repositories.</description>
+
+    <properties>
+        <elasticsearch.plugin.classname>org.elasticsearch.plugin.repository.s3.S3RepositoryPlugin</elasticsearch.plugin.classname>
+        <amazonaws.version>1.10.12</amazonaws.version>
+        <tests.jvms>1</tests.jvms>
+        <tests.rest.suite>repository_s3</tests.rest.suite>
+        <tests.rest.load_packaged>false</tests.rest.load_packaged>
+    </properties>
+
+    <dependencies>
+        <!-- AWS SDK -->
+        <dependency>
+            <groupId>com.amazonaws</groupId>
+            <artifactId>aws-java-sdk-s3</artifactId>
+            <version>${amazonaws.version}</version>
+        </dependency>
+        <!-- We need to force here the compile scope as it was defined as test scope in plugins/pom.xml -->
+        <!-- TODO: remove this dependency when we will have a REST Test module -->
+        <dependency>
+            <groupId>org.apache.httpcomponents</groupId>
+            <artifactId>httpclient</artifactId>
+            <scope>compile</scope>
+        </dependency>
+    </dependencies>
+
+    <build>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
diff --git a/plugins/repository-s3/rest-api-spec/test/repository_s3/10_basic.yaml b/plugins/repository-s3/rest-api-spec/test/repository_s3/10_basic.yaml
new file mode 100644
index 0000000..811ff88
--- /dev/null
+++ b/plugins/repository-s3/rest-api-spec/test/repository_s3/10_basic.yaml
@@ -0,0 +1,14 @@
+# Integration tests for Repository S3 component
+#
+"Repository S3 loaded":
+    - do:
+        cluster.state: {}
+
+    # Get master node id
+    - set: { master_node: master }
+
+    - do:
+        nodes.info: {}
+
+    - match:  { nodes.$master.plugins.0.name: repository-s3  }
+    - match:  { nodes.$master.plugins.0.jvm: true  }
diff --git a/plugins/repository-s3/rest-api-spec/test/repository_s3/20_repository.yaml b/plugins/repository-s3/rest-api-spec/test/repository_s3/20_repository.yaml
new file mode 100644
index 0000000..69b50b6
--- /dev/null
+++ b/plugins/repository-s3/rest-api-spec/test/repository_s3/20_repository.yaml
@@ -0,0 +1,23 @@
+# Integration tests for Repository S3 component
+#
+"S3 repository can be registereed":
+    - do:
+        snapshot.create_repository:
+          repository: test_repo_s3_1
+          verify: false
+          body:
+            type: s3
+            settings:
+              bucket: "my_bucket_name"
+              access_key: "AKVAIQBF2RECL7FJWGJQ"
+              secret_key: "vExyMThREXeRMm/b/LRzEB8jWwvzQeXgjqMX+6br"
+
+    # Get repositry
+    - do:
+        snapshot.get_repository:
+          repository: test_repo_s3_1
+
+    - is_true: test_repo_s3_1
+    - is_true: test_repo_s3_1.settings.bucket
+    - is_false: test_repo_s3_1.settings.access_key
+    - is_false: test_repo_s3_1.settings.secret_key
diff --git a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/AwsS3Service.java b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/AwsS3Service.java
new file mode 100644
index 0000000..e5db2ed
--- /dev/null
+++ b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/AwsS3Service.java
@@ -0,0 +1,34 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.aws;
+
+import com.amazonaws.services.s3.AmazonS3;
+import org.elasticsearch.common.component.LifecycleComponent;
+
+/**
+ *
+ */
+public interface AwsS3Service extends LifecycleComponent<AwsS3Service> {
+    AmazonS3 client();
+
+    AmazonS3 client(String endpoint, String protocol, String region, String account, String key);
+
+    AmazonS3 client(String endpoint, String protocol, String region, String account, String key, Integer maxRetries);
+}
diff --git a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/AwsSigner.java b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/AwsSigner.java
new file mode 100644
index 0000000..476bb74
--- /dev/null
+++ b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/AwsSigner.java
@@ -0,0 +1,53 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.aws;
+
+import com.amazonaws.ClientConfiguration;
+import com.amazonaws.auth.SignerFactory;
+
+public class AwsSigner {
+
+    private AwsSigner() {
+
+    }
+
+    /**
+     * Add a AWS API Signer.
+     * @param signer Signer to use
+     * @param configuration AWS Client configuration
+     * @throws IllegalArgumentException if signer does not exist
+     */
+    public static void configureSigner(String signer, ClientConfiguration configuration)
+        throws IllegalArgumentException {
+
+        if (signer == null) {
+            throw new IllegalArgumentException("[null] signer set");
+        }
+
+        try {
+            // We check this signer actually exists in AWS SDK
+            // It throws a IllegalArgumentException if not found
+            SignerFactory.getSignerByTypeAndService(signer, null);
+            configuration.setSignerOverride(signer);
+        } catch (IllegalArgumentException e) {
+            throw new IllegalArgumentException("wrong signer set [" + signer + "]");
+        }
+    }
+}
diff --git a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java
new file mode 100644
index 0000000..5405ce5
--- /dev/null
+++ b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java
@@ -0,0 +1,222 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.aws;
+
+import com.amazonaws.ClientConfiguration;
+import com.amazonaws.Protocol;
+import com.amazonaws.auth.*;
+import com.amazonaws.http.IdleConnectionReaper;
+import com.amazonaws.internal.StaticCredentialsProvider;
+import com.amazonaws.services.s3.AmazonS3;
+import com.amazonaws.services.s3.AmazonS3Client;
+import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.common.collect.Tuple;
+import org.elasticsearch.common.component.AbstractLifecycleComponent;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.settings.SettingsFilter;
+
+import java.util.HashMap;
+import java.util.Locale;
+import java.util.Map;
+
+/**
+ *
+ */
+public class InternalAwsS3Service extends AbstractLifecycleComponent<AwsS3Service> implements AwsS3Service {
+
+    /**
+     * (acceskey, endpoint) -> client
+     */
+    private Map<Tuple<String, String>, AmazonS3Client> clients = new HashMap<Tuple<String,String>, AmazonS3Client>();
+
+    @Inject
+    public InternalAwsS3Service(Settings settings, SettingsFilter settingsFilter) {
+        super(settings);
+        settingsFilter.addFilter("cloud.aws.access_key");
+        settingsFilter.addFilter("cloud.aws.secret_key");
+        settingsFilter.addFilter("access_key");
+        settingsFilter.addFilter("secret_key");
+    }
+
+    @Override
+    public synchronized AmazonS3 client() {
+        String endpoint = getDefaultEndpoint();
+        String account = settings.get("cloud.aws.access_key");
+        String key = settings.get("cloud.aws.secret_key");
+
+        return getClient(endpoint, null, account, key, null);
+    }
+
+    @Override
+    public AmazonS3 client(String endpoint, String protocol, String region, String account, String key) {
+        return client(endpoint, protocol, region, account, key, null);
+    }
+
+    @Override
+    public synchronized AmazonS3 client(String endpoint, String protocol, String region, String account, String key, Integer maxRetries) {
+        if (region != null && endpoint == null) {
+            endpoint = getEndpoint(region);
+            logger.debug("using s3 region [{}], with endpoint [{}]", region, endpoint);
+        } else if (endpoint == null) {
+            endpoint = getDefaultEndpoint();
+        }
+        if (account == null || key == null) {
+            account = settings.get("cloud.aws.access_key");
+            key = settings.get("cloud.aws.secret_key");
+        }
+
+        return getClient(endpoint, protocol, account, key, maxRetries);
+    }
+
+
+    private synchronized AmazonS3 getClient(String endpoint, String protocol, String account, String key, Integer maxRetries) {
+        Tuple<String, String> clientDescriptor = new Tuple<String, String>(endpoint, account);
+        AmazonS3Client client = clients.get(clientDescriptor);
+        if (client != null) {
+            return client;
+        }
+
+        ClientConfiguration clientConfiguration = new ClientConfiguration();
+        // the response metadata cache is only there for diagnostics purposes,
+        // but can force objects from every response to the old generation.
+        clientConfiguration.setResponseMetadataCacheSize(0);
+        if (protocol == null) {
+            protocol = settings.get("cloud.aws.protocol", "https").toLowerCase(Locale.ROOT);
+            protocol = settings.get("cloud.aws.s3.protocol", protocol).toLowerCase(Locale.ROOT);
+        }
+
+        if ("http".equals(protocol)) {
+            clientConfiguration.setProtocol(Protocol.HTTP);
+        } else if ("https".equals(protocol)) {
+            clientConfiguration.setProtocol(Protocol.HTTPS);
+        } else {
+            throw new IllegalArgumentException("No protocol supported [" + protocol + "], can either be [http] or [https]");
+        }
+
+        String proxyHost = settings.get("cloud.aws.proxy_host");
+        proxyHost = settings.get("cloud.aws.s3.proxy_host", proxyHost);
+        if (proxyHost != null) {
+            String portString = settings.get("cloud.aws.proxy_port", "80");
+            portString = settings.get("cloud.aws.s3.proxy_port", portString);
+            Integer proxyPort;
+            try {
+                proxyPort = Integer.parseInt(portString, 10);
+            } catch (NumberFormatException ex) {
+                throw new IllegalArgumentException("The configured proxy port value [" + portString + "] is invalid", ex);
+            }
+            clientConfiguration.withProxyHost(proxyHost).setProxyPort(proxyPort);
+        }
+
+        if (maxRetries != null) {
+            // If not explicitly set, default to 3 with exponential backoff policy
+            clientConfiguration.setMaxErrorRetry(maxRetries);
+        }
+
+        // #155: we might have 3rd party users using older S3 API version
+        String awsSigner = settings.get("cloud.aws.s3.signer", settings.get("cloud.aws.signer"));
+        if (awsSigner != null) {
+            logger.debug("using AWS API signer [{}]", awsSigner);
+            try {
+                AwsSigner.configureSigner(awsSigner, clientConfiguration);
+            } catch (IllegalArgumentException e) {
+                logger.warn("wrong signer set for [cloud.aws.s3.signer] or [cloud.aws.signer]: [{}]", awsSigner);
+            }
+        }
+
+        AWSCredentialsProvider credentials;
+
+        if (account == null && key == null) {
+            credentials = new AWSCredentialsProviderChain(
+                    new EnvironmentVariableCredentialsProvider(),
+                    new SystemPropertiesCredentialsProvider(),
+                    new InstanceProfileCredentialsProvider()
+            );
+        } else {
+            credentials = new AWSCredentialsProviderChain(
+                    new StaticCredentialsProvider(new BasicAWSCredentials(account, key))
+            );
+        }
+        client = new AmazonS3Client(credentials, clientConfiguration);
+
+        if (endpoint != null) {
+            client.setEndpoint(endpoint);
+        }
+        clients.put(clientDescriptor, client);
+        return client;
+    }
+
+    private String getDefaultEndpoint() {
+        String endpoint = null;
+        if (settings.get("cloud.aws.s3.endpoint") != null) {
+            endpoint = settings.get("cloud.aws.s3.endpoint");
+            logger.debug("using explicit s3 endpoint [{}]", endpoint);
+        } else if (settings.get("cloud.aws.region") != null) {
+            String region = settings.get("cloud.aws.region").toLowerCase(Locale.ROOT);
+            endpoint = getEndpoint(region);
+            logger.debug("using s3 region [{}], with endpoint [{}]", region, endpoint);
+        }
+        return endpoint;
+    }
+
+    private static String getEndpoint(String region) {
+        if ("us-east".equals(region) || "us-east-1".equals(region)) {
+            return "s3.amazonaws.com";
+        } else if ("us-west".equals(region) || "us-west-1".equals(region)) {
+            return "s3-us-west-1.amazonaws.com";
+        } else if ("us-west-2".equals(region)) {
+            return "s3-us-west-2.amazonaws.com";
+        } else if ("ap-southeast".equals(region) || "ap-southeast-1".equals(region)) {
+            return "s3-ap-southeast-1.amazonaws.com";
+        } else if ("ap-southeast-2".equals(region)) {
+            return "s3-ap-southeast-2.amazonaws.com";
+        } else if ("ap-northeast".equals(region) || "ap-northeast-1".equals(region)) {
+            return "s3-ap-northeast-1.amazonaws.com";
+        } else if ("eu-west".equals(region) || "eu-west-1".equals(region)) {
+            return "s3-eu-west-1.amazonaws.com";
+        } else if ("eu-central".equals(region) || "eu-central-1".equals(region)) {
+            return "s3.eu-central-1.amazonaws.com";
+        } else if ("sa-east".equals(region) || "sa-east-1".equals(region)) {
+            return "s3-sa-east-1.amazonaws.com";
+        } else if ("cn-north".equals(region) || "cn-north-1".equals(region)) {
+            return "s3.cn-north-1.amazonaws.com.cn";
+        } else {
+            throw new IllegalArgumentException("No automatic endpoint could be derived from region [" + region + "]");
+        }
+    }
+
+    @Override
+    protected void doStart() throws ElasticsearchException {
+    }
+
+    @Override
+    protected void doStop() throws ElasticsearchException {
+    }
+
+    @Override
+    protected void doClose() throws ElasticsearchException {
+        for (AmazonS3Client client : clients.values()) {
+            client.shutdown();
+        }
+
+        // Ensure that IdleConnectionReaper is shutdown
+        IdleConnectionReaper.shutdown();
+    }
+}
diff --git a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/S3Module.java b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/S3Module.java
new file mode 100644
index 0000000..1129405
--- /dev/null
+++ b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/S3Module.java
@@ -0,0 +1,38 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.aws;
+
+import org.elasticsearch.common.inject.AbstractModule;
+
+public class S3Module extends AbstractModule {
+
+
+    // pkg private so it is settable by tests
+    static Class<? extends AwsS3Service> s3ServiceImpl = InternalAwsS3Service.class;
+
+    public static Class<? extends AwsS3Service> getS3ServiceImpl() {
+        return s3ServiceImpl;
+    }
+
+    @Override
+    protected void configure() {
+        bind(AwsS3Service.class).to(s3ServiceImpl).asEagerSingleton();
+    }
+}
diff --git a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/DefaultS3OutputStream.java b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/DefaultS3OutputStream.java
new file mode 100644
index 0000000..67e6889
--- /dev/null
+++ b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/DefaultS3OutputStream.java
@@ -0,0 +1,258 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.aws.blobstore;
+
+import com.amazonaws.AmazonClientException;
+import com.amazonaws.services.s3.model.*;
+import com.amazonaws.util.Base64;
+import org.elasticsearch.common.logging.ESLogger;
+import org.elasticsearch.common.logging.Loggers;
+import org.elasticsearch.common.unit.ByteSizeUnit;
+import org.elasticsearch.common.unit.ByteSizeValue;
+
+import java.io.ByteArrayInputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.security.DigestInputStream;
+import java.security.MessageDigest;
+import java.security.NoSuchAlgorithmException;
+import java.util.ArrayList;
+import java.util.List;
+
+/**
+ * DefaultS3OutputStream uploads data to the AWS S3 service using 2 modes: single and multi part.
+ * <p/>
+ * When the length of the chunk is lower than buffer_size, the chunk is uploaded with a single request.
+ * Otherwise multiple requests are made, each of buffer_size (except the last one which can be lower than buffer_size).
+ * <p/>
+ * Quick facts about S3:
+ * <p/>
+ * Maximum object size:                 5 TB
+ * Maximum number of parts per upload:  10,000
+ * Part numbers:                        1 to 10,000 (inclusive)
+ * Part size:                           5 MB to 5 GB, last part can be < 5 MB
+ * <p/>
+ * See http://docs.aws.amazon.com/AmazonS3/latest/dev/qfacts.html
+ * See http://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html
+ */
+public class DefaultS3OutputStream extends S3OutputStream {
+
+    private static final ByteSizeValue MULTIPART_MAX_SIZE = new ByteSizeValue(5, ByteSizeUnit.GB);
+    private static final ESLogger logger = Loggers.getLogger("cloud.aws");
+    /**
+     * Multipart Upload API data
+     */
+    private String multipartId;
+    private int multipartChunks;
+    private List<PartETag> multiparts;
+
+    public DefaultS3OutputStream(S3BlobStore blobStore, String bucketName, String blobName, int bufferSizeInBytes, int numberOfRetries, boolean serverSideEncryption) {
+        super(blobStore, bucketName, blobName, bufferSizeInBytes, numberOfRetries, serverSideEncryption);
+    }
+
+    @Override
+    public void flush(byte[] bytes, int off, int len, boolean closing) throws IOException {
+        if (len > MULTIPART_MAX_SIZE.getBytes()) {
+            throw new IOException("Unable to upload files larger than " + MULTIPART_MAX_SIZE + " to Amazon S3");
+        }
+
+        if (!closing) {
+            if (len < getBufferSize()) {
+                upload(bytes, off, len);
+            } else {
+                if (getFlushCount() == 0) {
+                    initializeMultipart();
+                }
+                uploadMultipart(bytes, off, len, false);
+            }
+        } else {
+            if (multipartId != null) {
+                uploadMultipart(bytes, off, len, true);
+                completeMultipart();
+            } else {
+                upload(bytes, off, len);
+            }
+        }
+    }
+
+    /**
+     * Upload data using a single request.
+     *
+     * @param bytes
+     * @param off
+     * @param len
+     * @throws IOException
+     */
+    private void upload(byte[] bytes, int off, int len) throws IOException {
+        try (ByteArrayInputStream is = new ByteArrayInputStream(bytes, off, len)) {
+            int retry = 0;
+            while (retry <= getNumberOfRetries()) {
+                try {
+                    doUpload(getBlobStore(), getBucketName(), getBlobName(), is, len, isServerSideEncryption());
+                    break;
+                } catch (AmazonClientException e) {
+                    if (getBlobStore().shouldRetry(e) && retry < getNumberOfRetries()) {
+                        is.reset();
+                        retry++;
+                    } else {
+                        throw new IOException("Unable to upload object " + getBlobName(), e);
+                    }
+                }
+            }
+        }
+    }
+
+    protected void doUpload(S3BlobStore blobStore, String bucketName, String blobName, InputStream is, int length,
+            boolean serverSideEncryption) throws AmazonS3Exception {
+        ObjectMetadata md = new ObjectMetadata();
+        if (serverSideEncryption) {
+            md.setSSEAlgorithm(ObjectMetadata.AES_256_SERVER_SIDE_ENCRYPTION);
+        }
+        md.setContentLength(length);
+
+        InputStream inputStream = is;
+
+        // We try to compute a MD5 while reading it
+        MessageDigest messageDigest;
+        try {
+            messageDigest = MessageDigest.getInstance("MD5");
+            inputStream = new DigestInputStream(is, messageDigest);
+        } catch (NoSuchAlgorithmException impossible) {
+            // Every implementation of the Java platform is required to support MD5 (see MessageDigest)
+            throw new RuntimeException(impossible);
+        }
+        PutObjectResult putObjectResult = blobStore.client().putObject(bucketName, blobName, inputStream, md);
+
+        String localMd5 = Base64.encodeAsString(messageDigest.digest());
+        String remoteMd5 = putObjectResult.getContentMd5();
+        if (!localMd5.equals(remoteMd5)) {
+            logger.debug("MD5 local [{}], remote [{}] are not equal...", localMd5, remoteMd5);
+            throw new AmazonS3Exception("MD5 local [" + localMd5 +
+                    "], remote [" + remoteMd5 +
+                    "] are not equal...");
+        }
+    }
+
+    private void initializeMultipart() {
+        int retry = 0;
+        while ((retry <= getNumberOfRetries()) && (multipartId == null)) {
+            try {
+                multipartId = doInitialize(getBlobStore(), getBucketName(), getBlobName(), isServerSideEncryption());
+                if (multipartId != null) {
+                    multipartChunks = 1;
+                    multiparts = new ArrayList<>();
+                }
+            } catch (AmazonClientException e) {
+                if (getBlobStore().shouldRetry(e) && retry < getNumberOfRetries()) {
+                    retry++;
+                } else {
+                    throw e;
+                }
+            }
+        }
+    }
+
+    protected String doInitialize(S3BlobStore blobStore, String bucketName, String blobName, boolean serverSideEncryption) {
+        InitiateMultipartUploadRequest request = new InitiateMultipartUploadRequest(bucketName, blobName);
+        if (serverSideEncryption) {
+            ObjectMetadata md = new ObjectMetadata();
+            md.setSSEAlgorithm(ObjectMetadata.AES_256_SERVER_SIDE_ENCRYPTION);
+            request.setObjectMetadata(md);
+        }
+        return blobStore.client().initiateMultipartUpload(request).getUploadId();
+    }
+
+    private void uploadMultipart(byte[] bytes, int off, int len, boolean lastPart) throws IOException {
+        try (ByteArrayInputStream is = new ByteArrayInputStream(bytes, off, len)) {
+            int retry = 0;
+            while (retry <= getNumberOfRetries()) {
+                try {
+                    PartETag partETag = doUploadMultipart(getBlobStore(), getBucketName(), getBlobName(), multipartId, is, len, lastPart);
+                    multiparts.add(partETag);
+                    multipartChunks++;
+                    return;
+                } catch (AmazonClientException e) {
+                    if (getBlobStore().shouldRetry(e) && retry < getNumberOfRetries()) {
+                        is.reset();
+                        retry++;
+                    } else {
+                        abortMultipart();
+                        throw e;
+                    }
+                }
+            }
+        }
+    }
+
+    protected PartETag doUploadMultipart(S3BlobStore blobStore, String bucketName, String blobName, String uploadId, InputStream is,
+            int length, boolean lastPart) throws AmazonS3Exception {
+        UploadPartRequest request = new UploadPartRequest()
+        .withBucketName(bucketName)
+        .withKey(blobName)
+        .withUploadId(uploadId)
+        .withPartNumber(multipartChunks)
+        .withInputStream(is)
+        .withPartSize(length)
+        .withLastPart(lastPart);
+
+        UploadPartResult response = blobStore.client().uploadPart(request);
+        return response.getPartETag();
+
+    }
+
+    private void completeMultipart() {
+        int retry = 0;
+        while (retry <= getNumberOfRetries()) {
+            try {
+                doCompleteMultipart(getBlobStore(), getBucketName(), getBlobName(), multipartId, multiparts);
+                multipartId = null;
+                return;
+            } catch (AmazonClientException e) {
+                if (getBlobStore().shouldRetry(e) && retry < getNumberOfRetries()) {
+                    retry++;
+                } else {
+                    abortMultipart();
+                    throw e;
+                }
+            }
+        }
+    }
+
+    protected void doCompleteMultipart(S3BlobStore blobStore, String bucketName, String blobName, String uploadId, List<PartETag> parts)
+            throws AmazonS3Exception {
+        CompleteMultipartUploadRequest request = new CompleteMultipartUploadRequest(bucketName, blobName, uploadId, parts);
+        blobStore.client().completeMultipartUpload(request);
+    }
+
+    private void abortMultipart() {
+        if (multipartId != null) {
+            try {
+                doAbortMultipart(getBlobStore(), getBucketName(), getBlobName(), multipartId);
+            } finally {
+                multipartId = null;
+            }
+        }
+    }
+
+    protected void doAbortMultipart(S3BlobStore blobStore, String bucketName, String blobName, String uploadId)
+            throws AmazonS3Exception {
+        blobStore.client().abortMultipartUpload(new AbortMultipartUploadRequest(bucketName, blobName, uploadId));
+    }
+}
diff --git a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobContainer.java b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobContainer.java
new file mode 100644
index 0000000..1c0b23e
--- /dev/null
+++ b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobContainer.java
@@ -0,0 +1,162 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.aws.blobstore;
+
+import com.amazonaws.AmazonClientException;
+import com.amazonaws.services.s3.model.*;
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.blobstore.BlobMetaData;
+import org.elasticsearch.common.blobstore.BlobPath;
+import org.elasticsearch.common.blobstore.BlobStoreException;
+import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;
+import org.elasticsearch.common.blobstore.support.PlainBlobMetaData;
+import org.elasticsearch.common.collect.MapBuilder;
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.OutputStream;
+import java.util.Map;
+
+/**
+ *
+ */
+public class S3BlobContainer extends AbstractBlobContainer {
+
+    protected final S3BlobStore blobStore;
+
+    protected final String keyPath;
+
+    public S3BlobContainer(BlobPath path, S3BlobStore blobStore) {
+        super(path);
+        this.blobStore = blobStore;
+        String keyPath = path.buildAsString("/");
+        if (!keyPath.isEmpty()) {
+            keyPath = keyPath + "/";
+        }
+        this.keyPath = keyPath;
+    }
+
+    @Override
+    public boolean blobExists(String blobName) {
+        try {
+            blobStore.client().getObjectMetadata(blobStore.bucket(), buildKey(blobName));
+            return true;
+        } catch (AmazonS3Exception e) {
+            return false;
+        } catch (Throwable e) {
+            throw new BlobStoreException("failed to check if blob exists", e);
+        }
+    }
+
+    @Override
+    public void deleteBlob(String blobName) throws IOException {
+        try {
+            blobStore.client().deleteObject(blobStore.bucket(), buildKey(blobName));
+        } catch (AmazonClientException e) {
+            throw new IOException("Exception when deleting blob [" + blobName + "]", e);
+        }
+    }
+
+    @Override
+    public InputStream openInput(String blobName) throws IOException {
+        int retry = 0;
+        while (retry <= blobStore.numberOfRetries()) {
+            try {
+                S3Object s3Object = blobStore.client().getObject(blobStore.bucket(), buildKey(blobName));
+                return s3Object.getObjectContent();
+            } catch (AmazonClientException e) {
+                if (blobStore.shouldRetry(e) && (retry < blobStore.numberOfRetries())) {
+                    retry++;
+                } else {
+                    if (e instanceof AmazonS3Exception) {
+                        if (404 == ((AmazonS3Exception) e).getStatusCode()) {
+                            throw new FileNotFoundException("Blob object [" + blobName + "] not found: " + e.getMessage());
+                        }
+                    }
+                    throw e;
+                }
+            }
+        }
+        throw new BlobStoreException("retries exhausted while attempting to access blob object [name:" + blobName + ", bucket:" + blobStore.bucket() +"]");
+    }
+
+    @Override
+    public OutputStream createOutput(final String blobName) throws IOException {
+        // UploadS3OutputStream does buffering & retry logic internally
+        return new DefaultS3OutputStream(blobStore, blobStore.bucket(), buildKey(blobName), blobStore.bufferSizeInBytes(), blobStore.numberOfRetries(), blobStore.serverSideEncryption());
+    }
+
+    @Override
+    public Map<String, BlobMetaData> listBlobsByPrefix(@Nullable String blobNamePrefix) throws IOException {
+        MapBuilder<String, BlobMetaData> blobsBuilder = MapBuilder.newMapBuilder();
+        ObjectListing prevListing = null;
+        while (true) {
+            ObjectListing list;
+            if (prevListing != null) {
+                list = blobStore.client().listNextBatchOfObjects(prevListing);
+            } else {
+                if (blobNamePrefix != null) {
+                    list = blobStore.client().listObjects(blobStore.bucket(), buildKey(blobNamePrefix));
+                } else {
+                    list = blobStore.client().listObjects(blobStore.bucket(), keyPath);
+                }
+            }
+            for (S3ObjectSummary summary : list.getObjectSummaries()) {
+                String name = summary.getKey().substring(keyPath.length());
+                blobsBuilder.put(name, new PlainBlobMetaData(name, summary.getSize()));
+            }
+            if (list.isTruncated()) {
+                prevListing = list;
+            } else {
+                break;
+            }
+        }
+        return blobsBuilder.immutableMap();
+    }
+
+    @Override
+    public void move(String sourceBlobName, String targetBlobName) throws IOException {
+        try {
+            CopyObjectRequest request = new CopyObjectRequest(blobStore.bucket(), buildKey(sourceBlobName),
+                    blobStore.bucket(), buildKey(targetBlobName));
+
+            if (blobStore.serverSideEncryption()) {
+                ObjectMetadata objectMetadata = new ObjectMetadata();
+                objectMetadata.setSSEAlgorithm(ObjectMetadata.AES_256_SERVER_SIDE_ENCRYPTION);
+                request.setNewObjectMetadata(objectMetadata);
+            }
+            blobStore.client().copyObject(request);
+            blobStore.client().deleteObject(blobStore.bucket(), buildKey(sourceBlobName));
+        } catch (AmazonS3Exception e){
+            throw new IOException(e);
+        }
+    }
+
+    @Override
+    public Map<String, BlobMetaData> listBlobs() throws IOException {
+        return listBlobsByPrefix(null);
+    }
+
+    protected String buildKey(String blobName) {
+        return keyPath + blobName;
+    }
+
+}
diff --git a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobStore.java b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobStore.java
new file mode 100644
index 0000000..91f06b6
--- /dev/null
+++ b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobStore.java
@@ -0,0 +1,185 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.aws.blobstore;
+
+import com.amazonaws.AmazonClientException;
+import com.amazonaws.services.s3.AmazonS3;
+import com.amazonaws.services.s3.model.AmazonS3Exception;
+import com.amazonaws.services.s3.model.DeleteObjectsRequest;
+import com.amazonaws.services.s3.model.DeleteObjectsRequest.KeyVersion;
+import com.amazonaws.services.s3.model.ObjectListing;
+import com.amazonaws.services.s3.model.S3ObjectSummary;
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.blobstore.BlobContainer;
+import org.elasticsearch.common.blobstore.BlobPath;
+import org.elasticsearch.common.blobstore.BlobStore;
+import org.elasticsearch.common.blobstore.BlobStoreException;
+import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.ByteSizeUnit;
+import org.elasticsearch.common.unit.ByteSizeValue;
+
+import java.util.ArrayList;
+
+/**
+ *
+ */
+public class S3BlobStore extends AbstractComponent implements BlobStore {
+
+    public static final ByteSizeValue MIN_BUFFER_SIZE = new ByteSizeValue(5, ByteSizeUnit.MB);
+
+    private final AmazonS3 client;
+
+    private final String bucket;
+
+    private final String region;
+
+    private final ByteSizeValue bufferSize;
+
+    private final boolean serverSideEncryption;
+
+    private final int numberOfRetries;
+
+    public S3BlobStore(Settings settings, AmazonS3 client, String bucket, @Nullable String region, boolean serverSideEncryption,
+                       ByteSizeValue bufferSize, int maxRetries) {
+        super(settings);
+        this.client = client;
+        this.bucket = bucket;
+        this.region = region;
+        this.serverSideEncryption = serverSideEncryption;
+
+        this.bufferSize = (bufferSize != null) ? bufferSize : MIN_BUFFER_SIZE;
+        if (this.bufferSize.getBytes() < MIN_BUFFER_SIZE.getBytes()) {
+            throw new BlobStoreException("Detected a buffer_size for the S3 storage lower than [" + MIN_BUFFER_SIZE + "]");
+        }
+
+        this.numberOfRetries = maxRetries;
+
+        // Note: the method client.doesBucketExist() may return 'true' is the bucket exists
+        // but we don't have access to it (ie, 403 Forbidden response code)
+        // Also, if invalid security credentials are used to execute this method, the
+        // client is not able to distinguish between bucket permission errors and
+        // invalid credential errors, and this method could return an incorrect result.
+        int retry = 0;
+        while (retry <= maxRetries) {
+            try {
+                if (!client.doesBucketExist(bucket)) {
+                    if (region != null) {
+                        client.createBucket(bucket, region);
+                    } else {
+                        client.createBucket(bucket);
+                    }
+                }
+                break;
+            } catch (AmazonClientException e) {
+                if (shouldRetry(e) && retry < maxRetries) {
+                    retry++;
+                } else {
+                    logger.debug("S3 client create bucket failed");
+                    throw e;
+                }
+            }
+        }
+    }
+
+    @Override
+    public String toString() {
+        return (region == null ? "" : region + "/") + bucket;
+    }
+
+    public AmazonS3 client() {
+        return client;
+    }
+
+    public String bucket() {
+        return bucket;
+    }
+
+    public boolean serverSideEncryption() { return serverSideEncryption; }
+
+    public int bufferSizeInBytes() {
+        return bufferSize.bytesAsInt();
+    }
+
+    public int numberOfRetries() {
+        return numberOfRetries;
+    }
+
+    @Override
+    public BlobContainer blobContainer(BlobPath path) {
+        return new S3BlobContainer(path, this);
+    }
+
+    @Override
+    public void delete(BlobPath path) {
+        ObjectListing prevListing = null;
+        //From http://docs.amazonwebservices.com/AmazonS3/latest/dev/DeletingMultipleObjectsUsingJava.html
+        //we can do at most 1K objects per delete
+        //We don't know the bucket name until first object listing
+        DeleteObjectsRequest multiObjectDeleteRequest = null;
+        ArrayList<KeyVersion> keys = new ArrayList<KeyVersion>();
+        while (true) {
+            ObjectListing list;
+            if (prevListing != null) {
+                list = client.listNextBatchOfObjects(prevListing);
+            } else {
+                String keyPath = path.buildAsString("/");
+                if (!keyPath.isEmpty()) {
+                    keyPath = keyPath + "/";
+                }
+                list = client.listObjects(bucket, keyPath);
+                multiObjectDeleteRequest = new DeleteObjectsRequest(list.getBucketName());
+            }
+            for (S3ObjectSummary summary : list.getObjectSummaries()) {
+                keys.add(new KeyVersion(summary.getKey()));
+                //Every 500 objects batch the delete request
+                if (keys.size() > 500) {
+                    multiObjectDeleteRequest.setKeys(keys);
+                    client.deleteObjects(multiObjectDeleteRequest);
+                    multiObjectDeleteRequest = new DeleteObjectsRequest(list.getBucketName());
+                    keys.clear();
+                }
+            }
+            if (list.isTruncated()) {
+                prevListing = list;
+            } else {
+                break;
+            }
+        }
+        if (!keys.isEmpty()) {
+            multiObjectDeleteRequest.setKeys(keys);
+            client.deleteObjects(multiObjectDeleteRequest);
+        }
+    }
+
+    protected boolean shouldRetry(AmazonClientException e) {
+        if (e instanceof AmazonS3Exception) {
+            AmazonS3Exception s3e = (AmazonS3Exception)e;
+            if (s3e.getStatusCode() == 400 && "RequestTimeout".equals(s3e.getErrorCode())) {
+                return true;
+            }
+        }
+        return e.isRetryable();
+    }
+
+    @Override
+    public void close() {
+    }
+}
diff --git a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3OutputStream.java b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3OutputStream.java
new file mode 100644
index 0000000..a1b66ad
--- /dev/null
+++ b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3OutputStream.java
@@ -0,0 +1,125 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.aws.blobstore;
+
+import org.elasticsearch.common.unit.ByteSizeUnit;
+import org.elasticsearch.common.unit.ByteSizeValue;
+
+import java.io.IOException;
+import java.io.OutputStream;
+
+/**
+ * S3OutputStream buffers data before flushing it to an underlying S3OutputStream.
+ */
+public abstract class S3OutputStream extends OutputStream {
+
+    /**
+     * Limit of upload allowed by AWS S3.
+     */
+    protected static final ByteSizeValue MULTIPART_MAX_SIZE = new ByteSizeValue(5, ByteSizeUnit.GB);
+    protected static final ByteSizeValue MULTIPART_MIN_SIZE = new ByteSizeValue(5, ByteSizeUnit.MB);
+
+    private S3BlobStore blobStore;
+    private String bucketName;
+    private String blobName;
+    private int numberOfRetries;
+    private boolean serverSideEncryption;
+
+    private byte[] buffer;
+    private int count;
+    private long length;
+
+    private int flushCount = 0;
+
+    public S3OutputStream(S3BlobStore blobStore, String bucketName, String blobName, int bufferSizeInBytes, int numberOfRetries, boolean serverSideEncryption) {
+        this.blobStore = blobStore;
+        this.bucketName = bucketName;
+        this.blobName = blobName;
+        this.numberOfRetries = numberOfRetries;
+        this.serverSideEncryption = serverSideEncryption;
+
+        if (bufferSizeInBytes < MULTIPART_MIN_SIZE.getBytes()) {
+            throw new IllegalArgumentException("Buffer size can't be smaller than " + MULTIPART_MIN_SIZE);
+        }
+        if (bufferSizeInBytes > MULTIPART_MAX_SIZE.getBytes()) {
+            throw new IllegalArgumentException("Buffer size can't be larger than " + MULTIPART_MAX_SIZE);
+        }
+
+        this.buffer = new byte[bufferSizeInBytes];
+    }
+
+    public abstract void flush(byte[] bytes, int off, int len, boolean closing) throws IOException;
+
+    private void flushBuffer(boolean closing) throws IOException {
+        flush(buffer, 0, count, closing);
+        flushCount++;
+        count = 0;
+    }
+
+    @Override
+    public void write(int b) throws IOException {
+        if (count >= buffer.length) {
+            flushBuffer(false);
+        }
+
+        buffer[count++] = (byte) b;
+        length++;
+    }
+
+    @Override
+    public void close() throws IOException {
+        if (count > 0) {
+            flushBuffer(true);
+            count = 0;
+        }
+    }
+
+    public S3BlobStore getBlobStore() {
+        return blobStore;
+    }
+
+    public String getBucketName() {
+        return bucketName;
+    }
+
+    public String getBlobName() {
+        return blobName;
+    }
+
+    public int getBufferSize() {
+        return buffer.length;
+    }
+
+    public int getNumberOfRetries() {
+        return numberOfRetries;
+    }
+
+    public boolean isServerSideEncryption() {
+        return serverSideEncryption;
+    }
+
+    public long getLength() {
+        return length;
+    }
+
+    public int getFlushCount() {
+        return flushCount;
+    }
+}
diff --git a/plugins/repository-s3/src/main/java/org/elasticsearch/plugin/repository/s3/S3RepositoryPlugin.java b/plugins/repository-s3/src/main/java/org/elasticsearch/plugin/repository/s3/S3RepositoryPlugin.java
new file mode 100644
index 0000000..2911e27
--- /dev/null
+++ b/plugins/repository-s3/src/main/java/org/elasticsearch/plugin/repository/s3/S3RepositoryPlugin.java
@@ -0,0 +1,64 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plugin.repository.s3;
+
+import org.elasticsearch.cloud.aws.S3Module;
+import org.elasticsearch.common.component.LifecycleComponent;
+import org.elasticsearch.common.inject.Module;
+import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.repositories.RepositoriesModule;
+import org.elasticsearch.repositories.s3.S3Repository;
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+
+/**
+ *
+ */
+public class S3RepositoryPlugin extends Plugin {
+
+    @Override
+    public String name() {
+        return "repository-s3";
+    }
+
+    @Override
+    public String description() {
+        return "S3 Repository Plugin";
+    }
+
+    @Override
+    public Collection<Module> nodeModules() {
+        Collection<Module> modules = new ArrayList<>();
+        modules.add(new S3Module());
+        return modules;
+    }
+
+    @Override
+    public Collection<Class<? extends LifecycleComponent>> nodeServices() {
+        return Collections.<Class<? extends LifecycleComponent>>singleton(S3Module.getS3ServiceImpl());
+    }
+
+    public void onModule(RepositoriesModule repositoriesModule) {
+        repositoriesModule.registerRepository(S3Repository.TYPE, S3Repository.class, BlobStoreIndexShardRepository.class);
+    }
+}
diff --git a/plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3Repository.java b/plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3Repository.java
new file mode 100644
index 0000000..4be35ba
--- /dev/null
+++ b/plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3Repository.java
@@ -0,0 +1,167 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.repositories.s3;
+
+import org.elasticsearch.cloud.aws.AwsS3Service;
+import org.elasticsearch.cloud.aws.blobstore.S3BlobStore;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.blobstore.BlobPath;
+import org.elasticsearch.common.blobstore.BlobStore;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.unit.ByteSizeUnit;
+import org.elasticsearch.common.unit.ByteSizeValue;
+import org.elasticsearch.index.snapshots.IndexShardRepository;
+import org.elasticsearch.repositories.RepositoryException;
+import org.elasticsearch.repositories.RepositoryName;
+import org.elasticsearch.repositories.RepositorySettings;
+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;
+
+import java.io.IOException;
+import java.util.Locale;
+
+/**
+ * Shared file system implementation of the BlobStoreRepository
+ * <p/>
+ * Shared file system repository supports the following settings
+ * <dl>
+ * <dt>{@code bucket}</dt><dd>S3 bucket</dd>
+ * <dt>{@code region}</dt><dd>S3 region. Defaults to us-east</dd>
+ * <dt>{@code base_path}</dt><dd>Specifies the path within bucket to repository data. Defaults to root directory.</dd>
+ * <dt>{@code concurrent_streams}</dt><dd>Number of concurrent read/write stream (per repository on each node). Defaults to 5.</dd>
+ * <dt>{@code chunk_size}</dt><dd>Large file can be divided into chunks. This parameter specifies the chunk size. Defaults to not chucked.</dd>
+ * <dt>{@code compress}</dt><dd>If set to true metadata files will be stored compressed. Defaults to false.</dd>
+ * </dl>
+ */
+public class S3Repository extends BlobStoreRepository {
+
+    public final static String TYPE = "s3";
+
+    private final S3BlobStore blobStore;
+
+    private final BlobPath basePath;
+
+    private ByteSizeValue chunkSize;
+
+    private boolean compress;
+
+    /**
+     * Constructs new shared file system repository
+     *
+     * @param name                 repository name
+     * @param repositorySettings   repository settings
+     * @param indexShardRepository index shard repository
+     * @param s3Service            S3 service
+     * @throws IOException
+     */
+    @Inject
+    public S3Repository(RepositoryName name, RepositorySettings repositorySettings, IndexShardRepository indexShardRepository, AwsS3Service s3Service) throws IOException {
+        super(name.getName(), repositorySettings, indexShardRepository);
+
+        String bucket = repositorySettings.settings().get("bucket", settings.get("repositories.s3.bucket"));
+        if (bucket == null) {
+            throw new RepositoryException(name.name(), "No bucket defined for s3 gateway");
+        }
+
+        String endpoint = repositorySettings.settings().get("endpoint", settings.get("repositories.s3.endpoint"));
+        String protocol = repositorySettings.settings().get("protocol", settings.get("repositories.s3.protocol"));
+
+        String region = repositorySettings.settings().get("region", settings.get("repositories.s3.region"));
+        if (region == null) {
+            // Bucket setting is not set - use global region setting
+            String regionSetting = repositorySettings.settings().get("cloud.aws.region", settings.get("cloud.aws.region"));
+            if (regionSetting != null) {
+                regionSetting = regionSetting.toLowerCase(Locale.ENGLISH);
+                if ("us-east".equals(regionSetting) || "us-east-1".equals(regionSetting)) {
+                    // Default bucket - setting region to null
+                    region = null;
+                } else if ("us-west".equals(regionSetting) || "us-west-1".equals(regionSetting)) {
+                    region = "us-west-1";
+                } else if ("us-west-2".equals(regionSetting)) {
+                    region = "us-west-2";
+                } else if ("ap-southeast".equals(regionSetting) || "ap-southeast-1".equals(regionSetting)) {
+                    region = "ap-southeast-1";
+                } else if ("ap-southeast-2".equals(regionSetting)) {
+                    region = "ap-southeast-2";
+                } else if ("ap-northeast".equals(regionSetting) || "ap-northeast-1".equals(regionSetting)) {
+                    region = "ap-northeast-1";
+                } else if ("eu-west".equals(regionSetting) || "eu-west-1".equals(regionSetting)) {
+                    region = "eu-west-1";
+                } else if ("eu-central".equals(regionSetting) || "eu-central-1".equals(regionSetting)) {
+                    region = "eu-central-1";
+                } else if ("sa-east".equals(regionSetting) || "sa-east-1".equals(regionSetting)) {
+                    region = "sa-east-1";
+                } else if ("cn-north".equals(regionSetting) || "cn-north-1".equals(regionSetting)) {
+                    region = "cn-north-1";
+                }
+            }
+        }
+
+        boolean serverSideEncryption = repositorySettings.settings().getAsBoolean("server_side_encryption", settings.getAsBoolean("repositories.s3.server_side_encryption", false));
+        ByteSizeValue bufferSize = repositorySettings.settings().getAsBytesSize("buffer_size", settings.getAsBytesSize("repositories.s3.buffer_size", null));
+        Integer maxRetries = repositorySettings.settings().getAsInt("max_retries", settings.getAsInt("repositories.s3.max_retries", 3));
+        this.chunkSize = repositorySettings.settings().getAsBytesSize("chunk_size", settings.getAsBytesSize("repositories.s3.chunk_size", new ByteSizeValue(100, ByteSizeUnit.MB)));
+        this.compress = repositorySettings.settings().getAsBoolean("compress", settings.getAsBoolean("repositories.s3.compress", false));
+
+        logger.debug("using bucket [{}], region [{}], endpoint [{}], protocol [{}], chunk_size [{}], server_side_encryption [{}], buffer_size [{}], max_retries [{}]",
+                bucket, region, endpoint, protocol, chunkSize, serverSideEncryption, bufferSize, maxRetries);
+
+        blobStore = new S3BlobStore(settings, s3Service.client(endpoint, protocol, region, repositorySettings.settings().get("access_key"), repositorySettings.settings().get("secret_key"), maxRetries), bucket, region, serverSideEncryption, bufferSize, maxRetries);
+        String basePath = repositorySettings.settings().get("base_path", settings.get("repositories.s3.base_path"));
+        if (Strings.hasLength(basePath)) {
+            BlobPath path = new BlobPath();
+            for(String elem : Strings.splitStringToArray(basePath, '/')) {
+                path = path.add(elem);
+            }
+            this.basePath = path;
+        } else {
+            this.basePath = BlobPath.cleanPath();
+        }
+    }
+
+    /**
+     * {@inheritDoc}
+     */
+    @Override
+    protected BlobStore blobStore() {
+        return blobStore;
+    }
+
+    @Override
+    protected BlobPath basePath() {
+        return basePath;
+    }
+
+    /**
+     * {@inheritDoc}
+     */
+    @Override
+    protected boolean isCompress() {
+        return compress;
+    }
+
+    /**
+     * {@inheritDoc}
+     */
+    @Override
+    protected ByteSizeValue chunkSize() {
+        return chunkSize;
+    }
+
+}
diff --git a/plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/AWSSignersTests.java b/plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/AWSSignersTests.java
new file mode 100644
index 0000000..1c460c1
--- /dev/null
+++ b/plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/AWSSignersTests.java
@@ -0,0 +1,53 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.aws;
+
+import com.amazonaws.ClientConfiguration;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import static org.hamcrest.CoreMatchers.is;
+
+public class AWSSignersTests extends ESTestCase {
+
+    @Test
+    public void testSigners() {
+        assertThat(signerTester(null), is(false));
+        assertThat(signerTester("QueryStringSignerType"), is(true));
+        assertThat(signerTester("AWS3SignerType"), is(true));
+        assertThat(signerTester("AWS4SignerType"), is(true));
+        assertThat(signerTester("NoOpSignerType"), is(true));
+        assertThat(signerTester("UndefinedSigner"), is(false));
+    }
+
+    /**
+     * Test a signer configuration
+     * @param signer signer name
+     * @return true if successful, false otherwise
+     */
+    private boolean signerTester(String signer) {
+        try {
+            AwsSigner.configureSigner(signer, new ClientConfiguration());
+            return true;
+        } catch (IllegalArgumentException e) {
+            return false;
+        }
+    }
+}
diff --git a/plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTestCase.java b/plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTestCase.java
new file mode 100644
index 0000000..9a49c67
--- /dev/null
+++ b/plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTestCase.java
@@ -0,0 +1,95 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.aws;
+
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.io.PathUtils;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.settings.SettingsException;
+import org.elasticsearch.plugin.repository.s3.S3RepositoryPlugin;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.elasticsearch.test.ESIntegTestCase.ThirdParty;
+import org.junit.After;
+import org.junit.Before;
+
+import java.util.HashMap;
+import java.util.Map;
+
+/**
+ * Base class for AWS tests that require credentials.
+ * <p>
+ * You must specify {@code -Dtests.thirdparty=true -Dtests.config=/path/to/config}
+ * in order to run these tests.
+ */
+@ThirdParty
+public abstract class AbstractAwsTestCase extends ESIntegTestCase {
+
+    /**
+     * Those properties are set by the AWS SDK v1.9.4 and if not ignored,
+     * lead to tests failure (see AbstractRandomizedTest#IGNORED_INVARIANT_PROPERTIES)
+     */
+    private static final String[] AWS_INVARIANT_PROPERTIES = {
+            "com.sun.org.apache.xml.internal.dtm.DTMManager",
+            "javax.xml.parsers.DocumentBuilderFactory"
+    };
+
+    private Map<String, String> properties = new HashMap<>();
+
+    @Before
+    public void saveProperties() {
+        for (String p : AWS_INVARIANT_PROPERTIES) {
+            properties.put(p, System.getProperty(p));
+        }
+    }
+
+    @After
+    public void restoreProperties() {
+        for (String p : AWS_INVARIANT_PROPERTIES) {
+            if (properties.get(p) != null) {
+                System.setProperty(p, properties.get(p));
+            } else {
+                System.clearProperty(p);
+            }
+        }
+    }
+
+    @Override
+    protected Settings nodeSettings(int nodeOrdinal) {
+                Settings.Builder settings = Settings.builder()
+                .put(super.nodeSettings(nodeOrdinal))
+                .put("path.home", createTempDir())
+                .extendArray("plugin.types", S3RepositoryPlugin.class.getName(), TestAwsS3Service.TestPlugin.class.getName())
+                .put("cloud.aws.test.random", randomInt())
+                .put("cloud.aws.test.write_failures", 0.1)
+                .put("cloud.aws.test.read_failures", 0.1);
+
+        // if explicit, just load it and don't load from env
+        try {
+            if (Strings.hasText(System.getProperty("tests.config"))) {
+                settings.loadFromPath(PathUtils.get(System.getProperty("tests.config")));
+            } else {
+                throw new IllegalStateException("to run integration tests, you need to set -Dtest.thirdparty=true and -Dtests.config=/path/to/elasticsearch.yml");
+            }
+        } catch (SettingsException exception) {
+            throw new IllegalStateException("your test configuration file is incorrect: " + System.getProperty("tests.config"), exception);
+        }
+        return settings.build();
+    }
+}
diff --git a/plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/AmazonS3Wrapper.java b/plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/AmazonS3Wrapper.java
new file mode 100644
index 0000000..846892b
--- /dev/null
+++ b/plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/AmazonS3Wrapper.java
@@ -0,0 +1,631 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.aws;
+
+import com.amazonaws.AmazonClientException;
+import com.amazonaws.AmazonServiceException;
+import com.amazonaws.AmazonWebServiceRequest;
+import com.amazonaws.HttpMethod;
+import com.amazonaws.regions.Region;
+import com.amazonaws.services.s3.AmazonS3;
+import com.amazonaws.services.s3.S3ClientOptions;
+import com.amazonaws.services.s3.S3ResponseMetadata;
+import com.amazonaws.services.s3.model.*;
+import org.elasticsearch.common.SuppressForbidden;
+
+import java.io.File;
+import java.io.InputStream;
+import java.net.URL;
+import java.util.Date;
+import java.util.List;
+
+/**
+ *
+ */
+@SuppressForbidden(reason = "implements AWS api that uses java.io.File!")
+public class AmazonS3Wrapper implements AmazonS3 {
+
+    protected AmazonS3 delegate;
+
+    public AmazonS3Wrapper(AmazonS3 delegate) {
+        this.delegate = delegate;
+    }
+
+
+    @Override
+    public void setEndpoint(String endpoint) {
+        delegate.setEndpoint(endpoint);
+    }
+
+    @Override
+    public void setRegion(Region region) throws IllegalArgumentException {
+        delegate.setRegion(region);
+    }
+
+    @Override
+    public void setS3ClientOptions(S3ClientOptions clientOptions) {
+        delegate.setS3ClientOptions(clientOptions);
+    }
+
+    @Override
+    public void changeObjectStorageClass(String bucketName, String key, StorageClass newStorageClass) throws AmazonClientException, AmazonServiceException {
+        delegate.changeObjectStorageClass(bucketName, key, newStorageClass);
+    }
+
+    @Override
+    public void setObjectRedirectLocation(String bucketName, String key, String newRedirectLocation) throws AmazonClientException, AmazonServiceException {
+        delegate.setObjectRedirectLocation(bucketName, key, newRedirectLocation);
+    }
+
+    @Override
+    public ObjectListing listObjects(String bucketName) throws AmazonClientException, AmazonServiceException {
+        return delegate.listObjects(bucketName);
+    }
+
+    @Override
+    public ObjectListing listObjects(String bucketName, String prefix) throws AmazonClientException, AmazonServiceException {
+        return delegate.listObjects(bucketName, prefix);
+    }
+
+    @Override
+    public ObjectListing listObjects(ListObjectsRequest listObjectsRequest) throws AmazonClientException, AmazonServiceException {
+        return delegate.listObjects(listObjectsRequest);
+    }
+
+    @Override
+    public ObjectListing listNextBatchOfObjects(ObjectListing previousObjectListing) throws AmazonClientException, AmazonServiceException {
+        return delegate.listNextBatchOfObjects(previousObjectListing);
+    }
+
+    @Override
+    public VersionListing listVersions(String bucketName, String prefix) throws AmazonClientException, AmazonServiceException {
+        return delegate.listVersions(bucketName, prefix);
+    }
+
+    @Override
+    public VersionListing listNextBatchOfVersions(VersionListing previousVersionListing) throws AmazonClientException, AmazonServiceException {
+        return delegate.listNextBatchOfVersions(previousVersionListing);
+    }
+
+    @Override
+    public VersionListing listVersions(String bucketName, String prefix, String keyMarker, String versionIdMarker, String delimiter, Integer maxResults) throws AmazonClientException, AmazonServiceException {
+        return delegate.listVersions(bucketName, prefix, keyMarker, versionIdMarker, delimiter, maxResults);
+    }
+
+    @Override
+    public VersionListing listVersions(ListVersionsRequest listVersionsRequest) throws AmazonClientException, AmazonServiceException {
+        return delegate.listVersions(listVersionsRequest);
+    }
+
+    @Override
+    public Owner getS3AccountOwner() throws AmazonClientException, AmazonServiceException {
+        return delegate.getS3AccountOwner();
+    }
+
+    @Override
+    public boolean doesBucketExist(String bucketName) throws AmazonClientException, AmazonServiceException {
+        return delegate.doesBucketExist(bucketName);
+    }
+
+    @Override
+    public List<Bucket> listBuckets() throws AmazonClientException, AmazonServiceException {
+        return delegate.listBuckets();
+    }
+
+    @Override
+    public List<Bucket> listBuckets(ListBucketsRequest listBucketsRequest) throws AmazonClientException, AmazonServiceException {
+        return delegate.listBuckets(listBucketsRequest);
+    }
+
+    @Override
+    public String getBucketLocation(String bucketName) throws AmazonClientException, AmazonServiceException {
+        return delegate.getBucketLocation(bucketName);
+    }
+
+    @Override
+    public String getBucketLocation(GetBucketLocationRequest getBucketLocationRequest) throws AmazonClientException, AmazonServiceException {
+        return delegate.getBucketLocation(getBucketLocationRequest);
+    }
+
+    @Override
+    public Bucket createBucket(CreateBucketRequest createBucketRequest) throws AmazonClientException, AmazonServiceException {
+        return delegate.createBucket(createBucketRequest);
+    }
+
+    @Override
+    public Bucket createBucket(String bucketName) throws AmazonClientException, AmazonServiceException {
+        return delegate.createBucket(bucketName);
+    }
+
+    @Override
+    public Bucket createBucket(String bucketName, com.amazonaws.services.s3.model.Region region) throws AmazonClientException, AmazonServiceException {
+        return delegate.createBucket(bucketName, region);
+    }
+
+    @Override
+    public Bucket createBucket(String bucketName, String region) throws AmazonClientException, AmazonServiceException {
+        return delegate.createBucket(bucketName, region);
+    }
+
+    @Override
+    public AccessControlList getObjectAcl(String bucketName, String key) throws AmazonClientException, AmazonServiceException {
+        return delegate.getObjectAcl(bucketName, key);
+    }
+
+    @Override
+    public AccessControlList getObjectAcl(String bucketName, String key, String versionId) throws AmazonClientException, AmazonServiceException {
+        return delegate.getObjectAcl(bucketName, key, versionId);
+    }
+
+    @Override
+    public void setObjectAcl(String bucketName, String key, AccessControlList acl) throws AmazonClientException, AmazonServiceException {
+        delegate.setObjectAcl(bucketName, key, acl);
+    }
+
+    @Override
+    public void setObjectAcl(String bucketName, String key, CannedAccessControlList acl) throws AmazonClientException, AmazonServiceException {
+        delegate.setObjectAcl(bucketName, key, acl);
+    }
+
+    @Override
+    public void setObjectAcl(String bucketName, String key, String versionId, AccessControlList acl) throws AmazonClientException, AmazonServiceException {
+        delegate.setObjectAcl(bucketName, key, versionId, acl);
+    }
+
+    @Override
+    public void setObjectAcl(String bucketName, String key, String versionId, CannedAccessControlList acl) throws AmazonClientException, AmazonServiceException {
+        delegate.setObjectAcl(bucketName, key, versionId, acl);
+    }
+
+    @Override
+    public void setObjectAcl(SetObjectAclRequest setObjectAclRequest) throws AmazonClientException, AmazonServiceException {
+        delegate.setObjectAcl(setObjectAclRequest);
+    }
+
+    @Override
+    public AccessControlList getBucketAcl(String bucketName) throws AmazonClientException, AmazonServiceException {
+        return delegate.getBucketAcl(bucketName);
+    }
+
+    @Override
+    public void setBucketAcl(SetBucketAclRequest setBucketAclRequest) throws AmazonClientException, AmazonServiceException {
+        delegate.setBucketAcl(setBucketAclRequest);
+    }
+
+    @Override
+    public AccessControlList getBucketAcl(GetBucketAclRequest getBucketAclRequest) throws AmazonClientException, AmazonServiceException {
+        return delegate.getBucketAcl(getBucketAclRequest);
+    }
+
+    @Override
+    public void setBucketAcl(String bucketName, AccessControlList acl) throws AmazonClientException, AmazonServiceException {
+        delegate.setBucketAcl(bucketName, acl);
+    }
+
+    @Override
+    public void setBucketAcl(String bucketName, CannedAccessControlList acl) throws AmazonClientException, AmazonServiceException {
+        delegate.setBucketAcl(bucketName, acl);
+    }
+
+    @Override
+    public ObjectMetadata getObjectMetadata(String bucketName, String key) throws AmazonClientException, AmazonServiceException {
+        return delegate.getObjectMetadata(bucketName, key);
+    }
+
+    @Override
+    public ObjectMetadata getObjectMetadata(GetObjectMetadataRequest getObjectMetadataRequest) throws AmazonClientException, AmazonServiceException {
+        return delegate.getObjectMetadata(getObjectMetadataRequest);
+    }
+
+    @Override
+    public S3Object getObject(String bucketName, String key) throws AmazonClientException, AmazonServiceException {
+        return delegate.getObject(bucketName, key);
+    }
+
+    @Override
+    public S3Object getObject(GetObjectRequest getObjectRequest) throws AmazonClientException, AmazonServiceException {
+        return delegate.getObject(getObjectRequest);
+    }
+
+    @Override
+    public ObjectMetadata getObject(GetObjectRequest getObjectRequest, File destinationFile) throws AmazonClientException, AmazonServiceException {
+        return delegate.getObject(getObjectRequest, destinationFile);
+    }
+
+    @Override
+    public void deleteBucket(DeleteBucketRequest deleteBucketRequest) throws AmazonClientException, AmazonServiceException {
+        delegate.deleteBucket(deleteBucketRequest);
+    }
+
+    @Override
+    public void deleteBucket(String bucketName) throws AmazonClientException, AmazonServiceException {
+        delegate.deleteBucket(bucketName);
+    }
+
+    @Override
+    public void setBucketReplicationConfiguration(String bucketName, BucketReplicationConfiguration configuration) throws AmazonServiceException, AmazonClientException {
+        delegate.setBucketReplicationConfiguration(bucketName, configuration);
+    }
+
+    @Override
+    public void setBucketReplicationConfiguration(SetBucketReplicationConfigurationRequest setBucketReplicationConfigurationRequest) throws AmazonServiceException, AmazonClientException {
+        delegate.setBucketReplicationConfiguration(setBucketReplicationConfigurationRequest);
+    }
+
+    @Override
+    public BucketReplicationConfiguration getBucketReplicationConfiguration(String bucketName) throws AmazonServiceException, AmazonClientException {
+        return delegate.getBucketReplicationConfiguration(bucketName);
+    }
+
+    @Override
+    public void deleteBucketReplicationConfiguration(String bucketName) throws AmazonServiceException, AmazonClientException {
+        delegate.deleteBucketReplicationConfiguration(bucketName);
+    }
+
+    @Override
+    public PutObjectResult putObject(PutObjectRequest putObjectRequest) throws AmazonClientException, AmazonServiceException {
+        return delegate.putObject(putObjectRequest);
+    }
+
+    @Override
+    public PutObjectResult putObject(String bucketName, String key, File file) throws AmazonClientException, AmazonServiceException {
+        return delegate.putObject(bucketName, key, file);
+    }
+
+    @Override
+    public PutObjectResult putObject(String bucketName, String key, InputStream input, ObjectMetadata metadata) throws AmazonClientException, AmazonServiceException {
+        return delegate.putObject(bucketName, key, input, metadata);
+    }
+
+    @Override
+    public CopyObjectResult copyObject(String sourceBucketName, String sourceKey, String destinationBucketName, String destinationKey) throws AmazonClientException, AmazonServiceException {
+        return delegate.copyObject(sourceBucketName, sourceKey, destinationBucketName, destinationKey);
+    }
+
+    @Override
+    public CopyObjectResult copyObject(CopyObjectRequest copyObjectRequest) throws AmazonClientException, AmazonServiceException {
+        return delegate.copyObject(copyObjectRequest);
+    }
+
+    @Override
+    public CopyPartResult copyPart(CopyPartRequest copyPartRequest) throws AmazonClientException, AmazonServiceException {
+        return delegate.copyPart(copyPartRequest);
+    }
+
+    @Override
+    public void deleteObject(String bucketName, String key) throws AmazonClientException, AmazonServiceException {
+        delegate.deleteObject(bucketName, key);
+    }
+
+    @Override
+    public void deleteObject(DeleteObjectRequest deleteObjectRequest) throws AmazonClientException, AmazonServiceException {
+        delegate.deleteObject(deleteObjectRequest);
+    }
+
+    @Override
+    public DeleteObjectsResult deleteObjects(DeleteObjectsRequest deleteObjectsRequest) throws AmazonClientException, AmazonServiceException {
+        return delegate.deleteObjects(deleteObjectsRequest);
+    }
+
+    @Override
+    public void deleteVersion(String bucketName, String key, String versionId) throws AmazonClientException, AmazonServiceException {
+        delegate.deleteVersion(bucketName, key, versionId);
+    }
+
+    @Override
+    public void deleteVersion(DeleteVersionRequest deleteVersionRequest) throws AmazonClientException, AmazonServiceException {
+        delegate.deleteVersion(deleteVersionRequest);
+    }
+
+    @Override
+    public BucketLoggingConfiguration getBucketLoggingConfiguration(String bucketName) throws AmazonClientException, AmazonServiceException {
+        return delegate.getBucketLoggingConfiguration(bucketName);
+    }
+
+    @Override
+    public void setBucketLoggingConfiguration(SetBucketLoggingConfigurationRequest setBucketLoggingConfigurationRequest) throws AmazonClientException, AmazonServiceException {
+        delegate.setBucketLoggingConfiguration(setBucketLoggingConfigurationRequest);
+    }
+
+    @Override
+    public BucketVersioningConfiguration getBucketVersioningConfiguration(String bucketName) throws AmazonClientException, AmazonServiceException {
+        return delegate.getBucketVersioningConfiguration(bucketName);
+    }
+
+    @Override
+    public void setBucketVersioningConfiguration(SetBucketVersioningConfigurationRequest setBucketVersioningConfigurationRequest) throws AmazonClientException, AmazonServiceException {
+        delegate.setBucketVersioningConfiguration(setBucketVersioningConfigurationRequest);
+    }
+
+    @Override
+    public BucketLifecycleConfiguration getBucketLifecycleConfiguration(String bucketName) {
+        return delegate.getBucketLifecycleConfiguration(bucketName);
+    }
+
+    @Override
+    public void setBucketLifecycleConfiguration(String bucketName, BucketLifecycleConfiguration bucketLifecycleConfiguration) {
+        delegate.setBucketLifecycleConfiguration(bucketName, bucketLifecycleConfiguration);
+    }
+
+    @Override
+    public void setBucketLifecycleConfiguration(SetBucketLifecycleConfigurationRequest setBucketLifecycleConfigurationRequest) {
+        delegate.setBucketLifecycleConfiguration(setBucketLifecycleConfigurationRequest);
+    }
+
+    @Override
+    public void deleteBucketLifecycleConfiguration(String bucketName) {
+        delegate.deleteBucketLifecycleConfiguration(bucketName);
+    }
+
+    @Override
+    public void deleteBucketLifecycleConfiguration(DeleteBucketLifecycleConfigurationRequest deleteBucketLifecycleConfigurationRequest) {
+        delegate.deleteBucketLifecycleConfiguration(deleteBucketLifecycleConfigurationRequest);
+    }
+
+    @Override
+    public BucketCrossOriginConfiguration getBucketCrossOriginConfiguration(String bucketName) {
+        return delegate.getBucketCrossOriginConfiguration(bucketName);
+    }
+
+    @Override
+    public void setBucketCrossOriginConfiguration(String bucketName, BucketCrossOriginConfiguration bucketCrossOriginConfiguration) {
+        delegate.setBucketCrossOriginConfiguration(bucketName, bucketCrossOriginConfiguration);
+    }
+
+    @Override
+    public void setBucketCrossOriginConfiguration(SetBucketCrossOriginConfigurationRequest setBucketCrossOriginConfigurationRequest) {
+        delegate.setBucketCrossOriginConfiguration(setBucketCrossOriginConfigurationRequest);
+    }
+
+    @Override
+    public void deleteBucketCrossOriginConfiguration(String bucketName) {
+        delegate.deleteBucketCrossOriginConfiguration(bucketName);
+    }
+
+    @Override
+    public void deleteBucketCrossOriginConfiguration(DeleteBucketCrossOriginConfigurationRequest deleteBucketCrossOriginConfigurationRequest) {
+        delegate.deleteBucketCrossOriginConfiguration(deleteBucketCrossOriginConfigurationRequest);
+    }
+
+    @Override
+    public BucketTaggingConfiguration getBucketTaggingConfiguration(String bucketName) {
+        return delegate.getBucketTaggingConfiguration(bucketName);
+    }
+
+    @Override
+    public void setBucketTaggingConfiguration(String bucketName, BucketTaggingConfiguration bucketTaggingConfiguration) {
+        delegate.setBucketTaggingConfiguration(bucketName, bucketTaggingConfiguration);
+    }
+
+    @Override
+    public void setBucketTaggingConfiguration(SetBucketTaggingConfigurationRequest setBucketTaggingConfigurationRequest) {
+        delegate.setBucketTaggingConfiguration(setBucketTaggingConfigurationRequest);
+    }
+
+    @Override
+    public void deleteBucketTaggingConfiguration(String bucketName) {
+        delegate.deleteBucketTaggingConfiguration(bucketName);
+    }
+
+    @Override
+    public void deleteBucketTaggingConfiguration(DeleteBucketTaggingConfigurationRequest deleteBucketTaggingConfigurationRequest) {
+        delegate.deleteBucketTaggingConfiguration(deleteBucketTaggingConfigurationRequest);
+    }
+
+    @Override
+    public BucketNotificationConfiguration getBucketNotificationConfiguration(String bucketName) throws AmazonClientException, AmazonServiceException {
+        return delegate.getBucketNotificationConfiguration(bucketName);
+    }
+
+    @Override
+    public void setBucketNotificationConfiguration(SetBucketNotificationConfigurationRequest setBucketNotificationConfigurationRequest) throws AmazonClientException, AmazonServiceException {
+        delegate.setBucketNotificationConfiguration(setBucketNotificationConfigurationRequest);
+    }
+
+    @Override
+    public void setBucketNotificationConfiguration(String bucketName, BucketNotificationConfiguration bucketNotificationConfiguration) throws AmazonClientException, AmazonServiceException {
+        delegate.setBucketNotificationConfiguration(bucketName, bucketNotificationConfiguration);
+    }
+
+    @Override
+    public BucketWebsiteConfiguration getBucketWebsiteConfiguration(String bucketName) throws AmazonClientException, AmazonServiceException {
+        return delegate.getBucketWebsiteConfiguration(bucketName);
+    }
+
+    @Override
+    public BucketWebsiteConfiguration getBucketWebsiteConfiguration(GetBucketWebsiteConfigurationRequest getBucketWebsiteConfigurationRequest) throws AmazonClientException, AmazonServiceException {
+        return delegate.getBucketWebsiteConfiguration(getBucketWebsiteConfigurationRequest);
+    }
+
+    @Override
+    public void setBucketWebsiteConfiguration(String bucketName, BucketWebsiteConfiguration configuration) throws AmazonClientException, AmazonServiceException {
+        delegate.setBucketWebsiteConfiguration(bucketName, configuration);
+    }
+
+    @Override
+    public void setBucketWebsiteConfiguration(SetBucketWebsiteConfigurationRequest setBucketWebsiteConfigurationRequest) throws AmazonClientException, AmazonServiceException {
+        delegate.setBucketWebsiteConfiguration(setBucketWebsiteConfigurationRequest);
+    }
+
+    @Override
+    public void deleteBucketWebsiteConfiguration(String bucketName) throws AmazonClientException, AmazonServiceException {
+        delegate.deleteBucketWebsiteConfiguration(bucketName);
+    }
+
+    @Override
+    public void deleteBucketWebsiteConfiguration(DeleteBucketWebsiteConfigurationRequest deleteBucketWebsiteConfigurationRequest) throws AmazonClientException, AmazonServiceException {
+        delegate.deleteBucketWebsiteConfiguration(deleteBucketWebsiteConfigurationRequest);
+    }
+
+    @Override
+    public BucketPolicy getBucketPolicy(String bucketName) throws AmazonClientException, AmazonServiceException {
+        return delegate.getBucketPolicy(bucketName);
+    }
+
+    @Override
+    public BucketPolicy getBucketPolicy(GetBucketPolicyRequest getBucketPolicyRequest) throws AmazonClientException, AmazonServiceException {
+        return delegate.getBucketPolicy(getBucketPolicyRequest);
+    }
+
+    @Override
+    public void setBucketPolicy(String bucketName, String policyText) throws AmazonClientException, AmazonServiceException {
+        delegate.setBucketPolicy(bucketName, policyText);
+    }
+
+    @Override
+    public void setBucketPolicy(SetBucketPolicyRequest setBucketPolicyRequest) throws AmazonClientException, AmazonServiceException {
+        delegate.setBucketPolicy(setBucketPolicyRequest);
+    }
+
+    @Override
+    public void deleteBucketPolicy(String bucketName) throws AmazonClientException, AmazonServiceException {
+        delegate.deleteBucketPolicy(bucketName);
+    }
+
+    @Override
+    public void deleteBucketPolicy(DeleteBucketPolicyRequest deleteBucketPolicyRequest) throws AmazonClientException, AmazonServiceException {
+        delegate.deleteBucketPolicy(deleteBucketPolicyRequest);
+    }
+
+    @Override
+    public URL generatePresignedUrl(String bucketName, String key, Date expiration) throws AmazonClientException {
+        return delegate.generatePresignedUrl(bucketName, key, expiration);
+    }
+
+    @Override
+    public URL generatePresignedUrl(String bucketName, String key, Date expiration, HttpMethod method) throws AmazonClientException {
+        return delegate.generatePresignedUrl(bucketName, key, expiration, method);
+    }
+
+    @Override
+    public URL generatePresignedUrl(GeneratePresignedUrlRequest generatePresignedUrlRequest) throws AmazonClientException {
+        return delegate.generatePresignedUrl(generatePresignedUrlRequest);
+    }
+
+    @Override
+    public InitiateMultipartUploadResult initiateMultipartUpload(InitiateMultipartUploadRequest request) throws AmazonClientException, AmazonServiceException {
+        return delegate.initiateMultipartUpload(request);
+    }
+
+    @Override
+    public UploadPartResult uploadPart(UploadPartRequest request) throws AmazonClientException, AmazonServiceException {
+        return delegate.uploadPart(request);
+    }
+
+    @Override
+    public PartListing listParts(ListPartsRequest request) throws AmazonClientException, AmazonServiceException {
+        return delegate.listParts(request);
+    }
+
+    @Override
+    public void abortMultipartUpload(AbortMultipartUploadRequest request) throws AmazonClientException, AmazonServiceException {
+        delegate.abortMultipartUpload(request);
+    }
+
+    @Override
+    public CompleteMultipartUploadResult completeMultipartUpload(CompleteMultipartUploadRequest request) throws AmazonClientException, AmazonServiceException {
+        return delegate.completeMultipartUpload(request);
+    }
+
+    @Override
+    public MultipartUploadListing listMultipartUploads(ListMultipartUploadsRequest request) throws AmazonClientException, AmazonServiceException {
+        return delegate.listMultipartUploads(request);
+    }
+
+    @Override
+    public S3ResponseMetadata getCachedResponseMetadata(AmazonWebServiceRequest request) {
+        return delegate.getCachedResponseMetadata(request);
+    }
+
+    @Override
+    public void restoreObject(RestoreObjectRequest copyGlacierObjectRequest) throws AmazonServiceException {
+        delegate.restoreObject(copyGlacierObjectRequest);
+    }
+
+    @Override
+    public void restoreObject(String bucketName, String key, int expirationInDays) throws AmazonServiceException {
+        delegate.restoreObject(bucketName, key, expirationInDays);
+    }
+
+    @Override
+    public void enableRequesterPays(String bucketName) throws AmazonServiceException, AmazonClientException {
+        delegate.enableRequesterPays(bucketName);
+    }
+
+    @Override
+    public void disableRequesterPays(String bucketName) throws AmazonServiceException, AmazonClientException {
+        delegate.disableRequesterPays(bucketName);
+    }
+
+    @Override
+    public boolean isRequesterPaysEnabled(String bucketName) throws AmazonServiceException, AmazonClientException {
+        return delegate.isRequesterPaysEnabled(bucketName);
+    }
+
+    @Override
+    public ObjectListing listNextBatchOfObjects(ListNextBatchOfObjectsRequest listNextBatchOfObjectsRequest) throws AmazonClientException, AmazonServiceException {
+        return delegate.listNextBatchOfObjects(listNextBatchOfObjectsRequest);
+    }
+
+    @Override
+    public VersionListing listNextBatchOfVersions(ListNextBatchOfVersionsRequest listNextBatchOfVersionsRequest) throws AmazonClientException, AmazonServiceException {
+        return delegate.listNextBatchOfVersions(listNextBatchOfVersionsRequest);
+    }
+
+    @Override
+    public Owner getS3AccountOwner(GetS3AccountOwnerRequest getS3AccountOwnerRequest) throws AmazonClientException, AmazonServiceException {
+        return delegate.getS3AccountOwner(getS3AccountOwnerRequest);
+    }
+
+    @Override
+    public BucketLoggingConfiguration getBucketLoggingConfiguration(GetBucketLoggingConfigurationRequest getBucketLoggingConfigurationRequest) throws AmazonClientException, AmazonServiceException {
+        return delegate.getBucketLoggingConfiguration(getBucketLoggingConfigurationRequest);
+    }
+
+    @Override
+    public BucketVersioningConfiguration getBucketVersioningConfiguration(GetBucketVersioningConfigurationRequest getBucketVersioningConfigurationRequest) throws AmazonClientException, AmazonServiceException {
+        return delegate.getBucketVersioningConfiguration(getBucketVersioningConfigurationRequest);
+    }
+
+    @Override
+    public BucketLifecycleConfiguration getBucketLifecycleConfiguration(GetBucketLifecycleConfigurationRequest getBucketLifecycleConfigurationRequest) {
+        return delegate.getBucketLifecycleConfiguration(getBucketLifecycleConfigurationRequest);
+    }
+
+    @Override
+    public BucketCrossOriginConfiguration getBucketCrossOriginConfiguration(GetBucketCrossOriginConfigurationRequest getBucketCrossOriginConfigurationRequest) {
+        return delegate.getBucketCrossOriginConfiguration(getBucketCrossOriginConfigurationRequest);
+    }
+
+    @Override
+    public BucketTaggingConfiguration getBucketTaggingConfiguration(GetBucketTaggingConfigurationRequest getBucketTaggingConfigurationRequest) {
+        return delegate.getBucketTaggingConfiguration(getBucketTaggingConfigurationRequest);
+    }
+
+    @Override
+    public BucketNotificationConfiguration getBucketNotificationConfiguration(GetBucketNotificationConfigurationRequest getBucketNotificationConfigurationRequest) throws AmazonClientException, AmazonServiceException {
+        return delegate.getBucketNotificationConfiguration(getBucketNotificationConfigurationRequest);
+    }
+
+    @Override
+    public BucketReplicationConfiguration getBucketReplicationConfiguration(GetBucketReplicationConfigurationRequest getBucketReplicationConfigurationRequest) throws AmazonServiceException, AmazonClientException {
+        return delegate.getBucketReplicationConfiguration(getBucketReplicationConfigurationRequest);
+    }
+}
diff --git a/plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/TestAmazonS3.java b/plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/TestAmazonS3.java
new file mode 100644
index 0000000..d2ed3ba
--- /dev/null
+++ b/plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/TestAmazonS3.java
@@ -0,0 +1,155 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.aws;
+
+import com.amazonaws.AmazonClientException;
+import com.amazonaws.AmazonServiceException;
+import com.amazonaws.services.s3.AmazonS3;
+import com.amazonaws.services.s3.model.*;
+import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.common.logging.ESLogger;
+import org.elasticsearch.common.logging.Loggers;
+import org.elasticsearch.common.settings.Settings;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.UnsupportedEncodingException;
+import java.security.MessageDigest;
+import java.security.NoSuchAlgorithmException;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ConcurrentMap;
+import java.util.concurrent.atomic.AtomicLong;
+
+import static com.carrotsearch.randomizedtesting.RandomizedTest.randomDouble;
+
+/**
+ *
+ */
+public class TestAmazonS3 extends AmazonS3Wrapper {
+
+    protected final ESLogger logger = Loggers.getLogger(getClass());
+
+    private double writeFailureRate = 0.0;
+    private double readFailureRate = 0.0;
+
+    private String randomPrefix;
+
+    ConcurrentMap<String, AtomicLong> accessCounts = new ConcurrentHashMap<String, AtomicLong>();
+
+    private long incrementAndGet(String path) {
+        AtomicLong value = accessCounts.get(path);
+        if (value == null) {
+            value = accessCounts.putIfAbsent(path, new AtomicLong(1));
+        }
+        if (value != null) {
+            return value.incrementAndGet();
+        }
+        return 1;
+    }
+
+    public TestAmazonS3(AmazonS3 delegate, Settings settings) {
+        super(delegate);
+        randomPrefix = settings.get("cloud.aws.test.random");
+        writeFailureRate = settings.getAsDouble("cloud.aws.test.write_failures", 0.0);
+        readFailureRate = settings.getAsDouble("cloud.aws.test.read_failures", 0.0);
+    }
+
+    @Override
+    public PutObjectResult putObject(String bucketName, String key, InputStream input, ObjectMetadata metadata) throws AmazonClientException, AmazonServiceException {
+        if (shouldFail(bucketName, key, writeFailureRate)) {
+            long length = metadata.getContentLength();
+            long partToRead = (long) (length * randomDouble());
+            byte[] buffer = new byte[1024];
+            for (long cur = 0; cur < partToRead; cur += buffer.length) {
+                try {
+                    input.read(buffer, 0, (int) (partToRead - cur > buffer.length ? buffer.length : partToRead - cur));
+                } catch (IOException ex) {
+                    throw new ElasticsearchException("cannot read input stream", ex);
+                }
+            }
+            logger.info("--> random write failure on putObject method: throwing an exception for [bucket={}, key={}]", bucketName, key);
+            AmazonS3Exception ex = new AmazonS3Exception("Random S3 exception");
+            ex.setStatusCode(400);
+            ex.setErrorCode("RequestTimeout");
+            throw ex;
+        } else {
+            return super.putObject(bucketName, key, input, metadata);
+        }
+    }
+
+    @Override
+    public UploadPartResult uploadPart(UploadPartRequest request) throws AmazonClientException, AmazonServiceException {
+        if (shouldFail(request.getBucketName(), request.getKey(), writeFailureRate)) {
+            long length = request.getPartSize();
+            long partToRead = (long) (length * randomDouble());
+            byte[] buffer = new byte[1024];
+            for (long cur = 0; cur < partToRead; cur += buffer.length) {
+                try (InputStream input = request.getInputStream()){
+                    input.read(buffer, 0, (int) (partToRead - cur > buffer.length ? buffer.length : partToRead - cur));
+                } catch (IOException ex) {
+                    throw new ElasticsearchException("cannot read input stream", ex);
+                }
+            }
+            logger.info("--> random write failure on uploadPart method: throwing an exception for [bucket={}, key={}]", request.getBucketName(), request.getKey());
+            AmazonS3Exception ex = new AmazonS3Exception("Random S3 write exception");
+            ex.setStatusCode(400);
+            ex.setErrorCode("RequestTimeout");
+            throw ex;
+        } else {
+            return super.uploadPart(request);
+        }
+    }
+
+    @Override
+    public S3Object getObject(String bucketName, String key) throws AmazonClientException, AmazonServiceException {
+        if (shouldFail(bucketName, key, readFailureRate)) {
+            logger.info("--> random read failure on getObject method: throwing an exception for [bucket={}, key={}]", bucketName, key);
+            AmazonS3Exception ex = new AmazonS3Exception("Random S3 read exception");
+            ex.setStatusCode(404);
+            throw ex;
+        } else {
+            return super.getObject(bucketName, key);
+        }
+    }
+
+    private boolean shouldFail(String bucketName, String key, double probability) {
+        if (probability > 0.0) {
+            String path = randomPrefix + "-" + bucketName + "+" + key;
+            path += "/" + incrementAndGet(path);
+            return Math.abs(hashCode(path)) < Integer.MAX_VALUE * probability;
+        } else {
+            return false;
+        }
+    }
+
+    private int hashCode(String path) {
+        try {
+            MessageDigest digest = MessageDigest.getInstance("MD5");
+            byte[] bytes = digest.digest(path.getBytes("UTF-8"));
+            int i = 0;
+            return ((bytes[i++] & 0xFF) << 24) | ((bytes[i++] & 0xFF) << 16)
+                    | ((bytes[i++] & 0xFF) << 8) | (bytes[i++] & 0xFF);
+        } catch (UnsupportedEncodingException ex) {
+            throw new ElasticsearchException("cannot calculate hashcode", ex);
+        } catch (NoSuchAlgorithmException ex) {
+            throw new ElasticsearchException("cannot calculate hashcode", ex);
+        }
+    }
+}
diff --git a/plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/TestAwsS3Service.java b/plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/TestAwsS3Service.java
new file mode 100644
index 0000000..81a0312
--- /dev/null
+++ b/plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/TestAwsS3Service.java
@@ -0,0 +1,84 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.cloud.aws;
+
+import com.amazonaws.services.s3.AmazonS3;
+import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.settings.SettingsFilter;
+import org.elasticsearch.plugins.Plugin;
+
+import java.util.IdentityHashMap;
+
+public class TestAwsS3Service extends InternalAwsS3Service {
+    public static class TestPlugin extends Plugin {
+        @Override
+        public String name() {
+            return "mock-s3-service";
+        }
+        @Override
+        public String description() {
+            return "plugs in mock s3 service";
+        }
+        public void onModule(S3Module s3Module) {
+            s3Module.s3ServiceImpl = TestAwsS3Service.class;
+        }
+    }
+
+    IdentityHashMap<AmazonS3, TestAmazonS3> clients = new IdentityHashMap<AmazonS3, TestAmazonS3>();
+
+    @Inject
+    public TestAwsS3Service(Settings settings, SettingsFilter settingsFilter) {
+        super(settings, settingsFilter);
+    }
+
+
+    @Override
+    public synchronized AmazonS3 client() {
+        return cachedWrapper(super.client());
+    }
+
+    @Override
+    public synchronized AmazonS3 client(String endpoint, String protocol, String region, String account, String key) {
+        return cachedWrapper(super.client(endpoint, protocol, region, account, key));
+    }
+
+    @Override
+    public synchronized AmazonS3 client(String endpoint, String protocol, String region, String account, String key, Integer maxRetries) {
+        return cachedWrapper(super.client(endpoint, protocol, region, account, key, maxRetries));
+    }
+
+    private AmazonS3 cachedWrapper(AmazonS3 client) {
+        TestAmazonS3 wrapper = clients.get(client);
+        if (wrapper == null) {
+            wrapper = new TestAmazonS3(client, settings);
+            clients.put(client, wrapper);
+        }
+        return wrapper;
+    }
+
+    @Override
+    protected synchronized void doClose() throws ElasticsearchException {
+        super.doClose();
+        clients.clear();
+    }
+
+
+}
diff --git a/plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/blobstore/MockDefaultS3OutputStream.java b/plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/blobstore/MockDefaultS3OutputStream.java
new file mode 100644
index 0000000..cd2450f
--- /dev/null
+++ b/plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/blobstore/MockDefaultS3OutputStream.java
@@ -0,0 +1,99 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.aws.blobstore;
+
+import com.amazonaws.services.s3.model.AmazonS3Exception;
+import com.amazonaws.services.s3.model.PartETag;
+import com.carrotsearch.randomizedtesting.RandomizedTest;
+import org.elasticsearch.common.io.Streams;
+
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.util.List;
+
+public class MockDefaultS3OutputStream extends DefaultS3OutputStream {
+
+    private ByteArrayOutputStream out = new ByteArrayOutputStream();
+
+    private boolean initialized = false;
+    private boolean completed = false;
+    private boolean aborted = false;
+
+    private int numberOfUploadRequests = 0;
+
+    public MockDefaultS3OutputStream(int bufferSizeInBytes) {
+        super(null, "test-bucket", "test-blobname", bufferSizeInBytes, 3, false);
+    }
+
+    @Override
+    protected void doUpload(S3BlobStore blobStore, String bucketName, String blobName, InputStream is, int length, boolean serverSideEncryption) throws AmazonS3Exception {
+        try {
+            long copied = Streams.copy(is, out);
+            if (copied != length) {
+                throw new AmazonS3Exception("Not all the bytes were copied");
+            }
+            numberOfUploadRequests++;
+        } catch (IOException e) {
+            throw new AmazonS3Exception(e.getMessage());
+        }
+    }
+
+    @Override
+    protected String doInitialize(S3BlobStore blobStore, String bucketName, String blobName, boolean serverSideEncryption) {
+        initialized = true;
+        return RandomizedTest.randomAsciiOfLength(50);
+    }
+
+    @Override
+    protected PartETag doUploadMultipart(S3BlobStore blobStore, String bucketName, String blobName, String uploadId, InputStream is, int length, boolean lastPart) throws AmazonS3Exception {
+        try {
+            long copied = Streams.copy(is, out);
+            if (copied != length) {
+                throw new AmazonS3Exception("Not all the bytes were copied");
+            }
+            return new PartETag(numberOfUploadRequests++, RandomizedTest.randomAsciiOfLength(50));
+        } catch (IOException e) {
+            throw new AmazonS3Exception(e.getMessage());
+        }
+    }
+
+    @Override
+    protected void doCompleteMultipart(S3BlobStore blobStore, String bucketName, String blobName, String uploadId, List<PartETag> parts) throws AmazonS3Exception {
+        completed = true;
+    }
+
+    @Override
+    protected void doAbortMultipart(S3BlobStore blobStore, String bucketName, String blobName, String uploadId) throws AmazonS3Exception {
+        aborted = true;
+    }
+
+    public int getNumberOfUploadRequests() {
+        return numberOfUploadRequests;
+    }
+
+    public boolean isMultipart() {
+        return (numberOfUploadRequests > 1) && initialized && completed && !aborted;
+    }
+
+    public byte[] toByteArray() {
+        return out.toByteArray();
+    }
+}
diff --git a/plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/blobstore/S3OutputStreamTests.java b/plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/blobstore/S3OutputStreamTests.java
new file mode 100644
index 0000000..f40065c
--- /dev/null
+++ b/plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/blobstore/S3OutputStreamTests.java
@@ -0,0 +1,143 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.aws.blobstore;
+
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.util.Arrays;
+
+import static org.elasticsearch.common.io.Streams.copy;
+import static org.hamcrest.Matchers.equalTo;
+
+/**
+ * Unit test for {@link S3OutputStream}.
+ */
+public class S3OutputStreamTests extends ESTestCase {
+
+    private static final int BUFFER_SIZE = S3BlobStore.MIN_BUFFER_SIZE.bytesAsInt();
+
+    @Test
+    public void testWriteLessDataThanBufferSize() throws IOException {
+        MockDefaultS3OutputStream out = newS3OutputStream(BUFFER_SIZE);
+        byte[] content = randomUnicodeOfLengthBetween(1, 512).getBytes("UTF-8");
+        copy(content, out);
+
+        // Checks length & content
+        assertThat(out.getLength(), equalTo((long) content.length));
+        assertThat(Arrays.equals(content, out.toByteArray()), equalTo(true));
+
+        // Checks single/multi part upload
+        assertThat(out.getBufferSize(), equalTo(BUFFER_SIZE));
+        assertThat(out.getFlushCount(), equalTo(1));
+        assertThat(out.getNumberOfUploadRequests(), equalTo(1));
+        assertFalse(out.isMultipart());
+
+    }
+
+    @Test
+    public void testWriteSameDataThanBufferSize() throws IOException {
+        int size = randomIntBetween(BUFFER_SIZE, 2 * BUFFER_SIZE);
+        MockDefaultS3OutputStream out = newS3OutputStream(size);
+
+        ByteArrayOutputStream content = new ByteArrayOutputStream(size);
+        for (int i = 0; i < size; i++) {
+            content.write(randomByte());
+        }
+        copy(content.toByteArray(), out);
+
+        // Checks length & content
+        assertThat(out.getLength(), equalTo((long) size));
+        assertThat(Arrays.equals(content.toByteArray(), out.toByteArray()), equalTo(true));
+
+        // Checks single/multi part upload
+        assertThat(out.getBufferSize(), equalTo(size));
+        assertThat(out.getFlushCount(), equalTo(1));
+        assertThat(out.getNumberOfUploadRequests(), equalTo(1));
+        assertFalse(out.isMultipart());
+
+    }
+
+    @Test
+    public void testWriteExactlyNTimesMoreDataThanBufferSize() throws IOException {
+        int n = randomIntBetween(2, 3);
+        int length = n * BUFFER_SIZE;
+        ByteArrayOutputStream content = new ByteArrayOutputStream(length);
+
+        for (int i = 0; i < length; i++) {
+            content.write(randomByte());
+        }
+
+        MockDefaultS3OutputStream out = newS3OutputStream(BUFFER_SIZE);
+        copy(content.toByteArray(), out);
+
+        // Checks length & content
+        assertThat(out.getLength(), equalTo((long) length));
+        assertThat(Arrays.equals(content.toByteArray(), out.toByteArray()), equalTo(true));
+
+        // Checks single/multi part upload
+        assertThat(out.getBufferSize(), equalTo(BUFFER_SIZE));
+        assertThat(out.getFlushCount(), equalTo(n));
+
+        assertThat(out.getNumberOfUploadRequests(), equalTo(n));
+        assertTrue(out.isMultipart());
+    }
+
+    @Test
+    public void testWriteRandomNumberOfBytes() throws IOException {
+        Integer randomBufferSize = randomIntBetween(BUFFER_SIZE, 2 * BUFFER_SIZE);
+        MockDefaultS3OutputStream out = newS3OutputStream(randomBufferSize);
+
+        Integer randomLength = randomIntBetween(1, 2 * BUFFER_SIZE);
+        ByteArrayOutputStream content = new ByteArrayOutputStream(randomLength);
+        for (int i = 0; i < randomLength; i++) {
+            content.write(randomByte());
+        }
+
+        copy(content.toByteArray(), out);
+
+        // Checks length & content
+        assertThat(out.getLength(), equalTo((long) randomLength));
+        assertThat(Arrays.equals(content.toByteArray(), out.toByteArray()), equalTo(true));
+
+        assertThat(out.getBufferSize(), equalTo(randomBufferSize));
+        int times = (int) Math.ceil(randomLength.doubleValue() / randomBufferSize.doubleValue());
+        assertThat(out.getFlushCount(), equalTo(times));
+        if (times > 1) {
+            assertTrue(out.isMultipart());
+        } else {
+            assertFalse(out.isMultipart());
+        }
+    }
+
+    @Test(expected = IllegalArgumentException.class)
+    public void testWrongBufferSize() throws IOException {
+        Integer randomBufferSize = randomIntBetween(1, 4 * 1024 * 1024);
+        MockDefaultS3OutputStream out = newS3OutputStream(randomBufferSize);
+        fail("Buffer size can't be smaller than 5mb");
+    }
+
+    private MockDefaultS3OutputStream newS3OutputStream(int bufferSizeInBytes) {
+        return new MockDefaultS3OutputStream(bufferSizeInBytes);
+    }
+
+}
diff --git a/plugins/repository-s3/src/test/java/org/elasticsearch/repositories/s3/AbstractS3SnapshotRestoreTest.java b/plugins/repository-s3/src/test/java/org/elasticsearch/repositories/s3/AbstractS3SnapshotRestoreTest.java
new file mode 100644
index 0000000..c47202d
--- /dev/null
+++ b/plugins/repository-s3/src/test/java/org/elasticsearch/repositories/s3/AbstractS3SnapshotRestoreTest.java
@@ -0,0 +1,509 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.repositories.s3;
+
+import com.amazonaws.services.s3.AmazonS3;
+import com.amazonaws.services.s3.model.DeleteObjectsRequest;
+import com.amazonaws.services.s3.model.ObjectListing;
+import com.amazonaws.services.s3.model.S3ObjectSummary;
+
+import org.elasticsearch.action.admin.cluster.repositories.put.PutRepositoryResponse;
+import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;
+import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse;
+import org.elasticsearch.client.Client;
+import org.elasticsearch.client.ClusterAdminClient;
+import org.elasticsearch.cloud.aws.AbstractAwsTestCase;
+import org.elasticsearch.cloud.aws.AwsS3Service;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.plugin.repository.s3.S3RepositoryPlugin;
+import org.elasticsearch.repositories.RepositoryMissingException;
+import org.elasticsearch.repositories.RepositoryVerificationException;
+import org.elasticsearch.snapshots.SnapshotMissingException;
+import org.elasticsearch.snapshots.SnapshotState;
+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
+import org.elasticsearch.test.ESIntegTestCase.Scope;
+import org.elasticsearch.test.store.MockFSDirectoryService;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import static org.hamcrest.Matchers.*;
+
+/**
+ */
+@ClusterScope(scope = Scope.SUITE, numDataNodes = 2, numClientNodes = 0, transportClientRatio = 0.0)
+abstract public class AbstractS3SnapshotRestoreTest extends AbstractAwsTestCase {
+
+    @Override
+    public Settings indexSettings() {
+        // During restore we frequently restore index to exactly the same state it was before, that might cause the same
+        // checksum file to be written twice during restore operation
+        return Settings.builder().put(super.indexSettings())
+                .put(MockFSDirectoryService.RANDOM_PREVENT_DOUBLE_WRITE, false)
+                .put(MockFSDirectoryService.RANDOM_NO_DELETE_OPEN_FILE, false)
+                .put("cloud.enabled", true)
+                .put("plugin.types", S3RepositoryPlugin.class.getName())
+                .put("repositories.s3.base_path", basePath)
+                .build();
+    }
+
+    private String basePath;
+
+    @Before
+    public final void wipeBefore() {
+        wipeRepositories();
+        basePath = "repo-" + randomInt();
+        cleanRepositoryFiles(basePath);
+    }
+
+    @After
+    public final void wipeAfter() {
+        wipeRepositories();
+        cleanRepositoryFiles(basePath);
+    }
+
+    @Test @AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch-cloud-aws/issues/211")
+    public void testSimpleWorkflow() {
+        Client client = client();
+        Settings.Builder settings = Settings.settingsBuilder()
+                .put("chunk_size", randomIntBetween(1000, 10000));
+
+        // We sometime test getting the base_path from node settings using repositories.s3.base_path
+        if (usually()) {
+            settings.put("base_path", basePath);
+        }
+
+        logger.info("-->  creating s3 repository with bucket[{}] and path [{}]", internalCluster().getInstance(Settings.class).get("repositories.s3.bucket"), basePath);
+        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository("test-repo")
+                .setType("s3").setSettings(settings
+                        ).get();
+        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
+
+        createIndex("test-idx-1", "test-idx-2", "test-idx-3");
+        ensureGreen();
+
+        logger.info("--> indexing some data");
+        for (int i = 0; i < 100; i++) {
+            index("test-idx-1", "doc", Integer.toString(i), "foo", "bar" + i);
+            index("test-idx-2", "doc", Integer.toString(i), "foo", "baz" + i);
+            index("test-idx-3", "doc", Integer.toString(i), "foo", "baz" + i);
+        }
+        refresh();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
+        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
+        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(100L));
+
+        logger.info("--> snapshot");
+        CreateSnapshotResponse createSnapshotResponse = client.admin().cluster().prepareCreateSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices("test-idx-*", "-test-idx-3").get();
+        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
+        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse.getSnapshotInfo().totalShards()));
+
+        assertThat(client.admin().cluster().prepareGetSnapshots("test-repo").setSnapshots("test-snap").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
+
+        logger.info("--> delete some data");
+        for (int i = 0; i < 50; i++) {
+            client.prepareDelete("test-idx-1", "doc", Integer.toString(i)).get();
+        }
+        for (int i = 50; i < 100; i++) {
+            client.prepareDelete("test-idx-2", "doc", Integer.toString(i)).get();
+        }
+        for (int i = 0; i < 100; i += 2) {
+            client.prepareDelete("test-idx-3", "doc", Integer.toString(i)).get();
+        }
+        refresh();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(50L));
+        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(50L));
+        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(50L));
+
+        logger.info("--> close indices");
+        client.admin().indices().prepareClose("test-idx-1", "test-idx-2").get();
+
+        logger.info("--> restore all indices from the snapshot");
+        RestoreSnapshotResponse restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setWaitForCompletion(true).execute().actionGet();
+        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
+
+        ensureGreen();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
+        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
+        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(50L));
+
+        // Test restore after index deletion
+        logger.info("--> delete indices");
+        cluster().wipeIndices("test-idx-1", "test-idx-2");
+        logger.info("--> restore one index after deletion");
+        restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices("test-idx-*", "-test-idx-2").execute().actionGet();
+        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
+        ensureGreen();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
+        ClusterState clusterState = client.admin().cluster().prepareState().get().getState();
+        assertThat(clusterState.getMetaData().hasIndex("test-idx-1"), equalTo(true));
+        assertThat(clusterState.getMetaData().hasIndex("test-idx-2"), equalTo(false));
+    }
+
+    @Test @AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch-cloud-aws/issues/211")
+    public void testEncryption() {
+        Client client = client();
+        logger.info("-->  creating s3 repository with bucket[{}] and path [{}]", internalCluster().getInstance(Settings.class).get("repositories.s3.bucket"), basePath);
+        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository("test-repo")
+                .setType("s3").setSettings(Settings.settingsBuilder()
+                        .put("base_path", basePath)
+                        .put("chunk_size", randomIntBetween(1000, 10000))
+                        .put("server_side_encryption", true)
+                        ).get();
+        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
+
+        createIndex("test-idx-1", "test-idx-2", "test-idx-3");
+        ensureGreen();
+
+        logger.info("--> indexing some data");
+        for (int i = 0; i < 100; i++) {
+            index("test-idx-1", "doc", Integer.toString(i), "foo", "bar" + i);
+            index("test-idx-2", "doc", Integer.toString(i), "foo", "baz" + i);
+            index("test-idx-3", "doc", Integer.toString(i), "foo", "baz" + i);
+        }
+        refresh();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
+        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
+        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(100L));
+
+        logger.info("--> snapshot");
+        CreateSnapshotResponse createSnapshotResponse = client.admin().cluster().prepareCreateSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices("test-idx-*", "-test-idx-3").get();
+        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
+        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse.getSnapshotInfo().totalShards()));
+
+        assertThat(client.admin().cluster().prepareGetSnapshots("test-repo").setSnapshots("test-snap").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
+
+        Settings settings = internalCluster().getInstance(Settings.class);
+        Settings bucket = settings.getByPrefix("repositories.s3.");
+        AmazonS3 s3Client = internalCluster().getInstance(AwsS3Service.class).client(
+                null,
+                null,
+                bucket.get("region", settings.get("repositories.s3.region")),
+                bucket.get("access_key", settings.get("cloud.aws.access_key")),
+                bucket.get("secret_key", settings.get("cloud.aws.secret_key")));
+
+        String bucketName = bucket.get("bucket");
+        logger.info("--> verify encryption for bucket [{}], prefix [{}]", bucketName, basePath);
+        List<S3ObjectSummary> summaries = s3Client.listObjects(bucketName, basePath).getObjectSummaries();
+        for (S3ObjectSummary summary : summaries) {
+            assertThat(s3Client.getObjectMetadata(bucketName, summary.getKey()).getSSEAlgorithm(), equalTo("AES256"));
+        }
+
+        logger.info("--> delete some data");
+        for (int i = 0; i < 50; i++) {
+            client.prepareDelete("test-idx-1", "doc", Integer.toString(i)).get();
+        }
+        for (int i = 50; i < 100; i++) {
+            client.prepareDelete("test-idx-2", "doc", Integer.toString(i)).get();
+        }
+        for (int i = 0; i < 100; i += 2) {
+            client.prepareDelete("test-idx-3", "doc", Integer.toString(i)).get();
+        }
+        refresh();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(50L));
+        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(50L));
+        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(50L));
+
+        logger.info("--> close indices");
+        client.admin().indices().prepareClose("test-idx-1", "test-idx-2").get();
+
+        logger.info("--> restore all indices from the snapshot");
+        RestoreSnapshotResponse restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setWaitForCompletion(true).execute().actionGet();
+        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
+
+        ensureGreen();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
+        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
+        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(50L));
+
+        // Test restore after index deletion
+        logger.info("--> delete indices");
+        cluster().wipeIndices("test-idx-1", "test-idx-2");
+        logger.info("--> restore one index after deletion");
+        restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices("test-idx-*", "-test-idx-2").execute().actionGet();
+        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
+        ensureGreen();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
+        ClusterState clusterState = client.admin().cluster().prepareState().get().getState();
+        assertThat(clusterState.getMetaData().hasIndex("test-idx-1"), equalTo(true));
+        assertThat(clusterState.getMetaData().hasIndex("test-idx-2"), equalTo(false));
+    }
+
+    /**
+     * This test verifies that the test configuration is set up in a manner that
+     * does not make the test {@link #testRepositoryWithCustomCredentials()} pointless.
+     */
+    @Test(expected = RepositoryVerificationException.class)
+    public void assertRepositoryWithCustomCredentialsIsNotAccessibleByDefaultCredentials() {
+        Client client = client();
+        Settings bucketSettings = internalCluster().getInstance(Settings.class).getByPrefix("repositories.s3.private-bucket.");
+        logger.info("-->  creating s3 repository with bucket[{}] and path [{}]", bucketSettings.get("bucket"), basePath);
+        client.admin().cluster().preparePutRepository("test-repo")
+        .setType("s3").setSettings(Settings.settingsBuilder()
+                .put("base_path", basePath)
+                .put("bucket", bucketSettings.get("bucket"))
+                ).get();
+        fail("repository verification should have raise an exception!");
+    }
+
+    @Test
+    public void testRepositoryWithCustomCredentials() {
+        Client client = client();
+        Settings bucketSettings = internalCluster().getInstance(Settings.class).getByPrefix("repositories.s3.private-bucket.");
+        logger.info("-->  creating s3 repository with bucket[{}] and path [{}]", bucketSettings.get("bucket"), basePath);
+        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository("test-repo")
+                .setType("s3").setSettings(Settings.settingsBuilder()
+                        .put("base_path", basePath)
+                        .put("region", bucketSettings.get("region"))
+                        .put("access_key", bucketSettings.get("access_key"))
+                        .put("secret_key", bucketSettings.get("secret_key"))
+                        .put("bucket", bucketSettings.get("bucket"))
+                        ).get();
+        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
+
+        assertRepositoryIsOperational(client, "test-repo");
+    }
+
+    @Test @AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch-cloud-aws/issues/211")
+    public void testRepositoryWithCustomEndpointProtocol() {
+        Client client = client();
+        Settings bucketSettings = internalCluster().getInstance(Settings.class).getByPrefix("repositories.s3.external-bucket.");
+        logger.info("--> creating s3 repostoriy with endpoint [{}], bucket[{}] and path [{}]", bucketSettings.get("endpoint"), bucketSettings.get("bucket"), basePath);
+        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository("test-repo")
+                .setType("s3").setSettings(Settings.settingsBuilder()
+                        .put("bucket", bucketSettings.get("bucket"))
+                        .put("endpoint", bucketSettings.get("endpoint"))
+                        .put("access_key", bucketSettings.get("access_key"))
+                        .put("secret_key", bucketSettings.get("secret_key"))
+                        .put("base_path", basePath)
+                        ).get();
+        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
+        assertRepositoryIsOperational(client, "test-repo");
+    }
+
+    /**
+     * This test verifies that the test configuration is set up in a manner that
+     * does not make the test {@link #testRepositoryInRemoteRegion()} pointless.
+     */
+    @Test(expected = RepositoryVerificationException.class)
+    public void assertRepositoryInRemoteRegionIsRemote() {
+        Client client = client();
+        Settings bucketSettings = internalCluster().getInstance(Settings.class).getByPrefix("repositories.s3.remote-bucket.");
+        logger.info("-->  creating s3 repository with bucket[{}] and path [{}]", bucketSettings.get("bucket"), basePath);
+        client.admin().cluster().preparePutRepository("test-repo")
+        .setType("s3").setSettings(Settings.settingsBuilder()
+                .put("base_path", basePath)
+                .put("bucket", bucketSettings.get("bucket"))
+                // Below setting intentionally omitted to assert bucket is not available in default region.
+                //                        .put("region", privateBucketSettings.get("region"))
+                ).get();
+
+        fail("repository verification should have raise an exception!");
+    }
+
+    @Test @AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch-cloud-aws/issues/211")
+    public void testRepositoryInRemoteRegion() {
+        Client client = client();
+        Settings settings = internalCluster().getInstance(Settings.class);
+        Settings bucketSettings = settings.getByPrefix("repositories.s3.remote-bucket.");
+        logger.info("-->  creating s3 repository with bucket[{}] and path [{}]", bucketSettings.get("bucket"), basePath);
+        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository("test-repo")
+                .setType("s3").setSettings(Settings.settingsBuilder()
+                        .put("base_path", basePath)
+                        .put("bucket", bucketSettings.get("bucket"))
+                        .put("region", bucketSettings.get("region"))
+                        ).get();
+        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
+
+        assertRepositoryIsOperational(client, "test-repo");
+    }
+
+    /**
+     * Test case for issue #86: https://github.com/elasticsearch/elasticsearch-cloud-aws/issues/86
+     */
+    @Test
+    public void testNonExistingRepo_86() {
+        Client client = client();
+        logger.info("-->  creating s3 repository with bucket[{}] and path [{}]", internalCluster().getInstance(Settings.class).get("repositories.s3.bucket"), basePath);
+        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository("test-repo")
+                .setType("s3").setSettings(Settings.settingsBuilder()
+                        .put("base_path", basePath)
+                ).get();
+        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
+
+        logger.info("--> restore non existing snapshot");
+        try {
+            client.admin().cluster().prepareRestoreSnapshot("test-repo", "no-existing-snapshot").setWaitForCompletion(true).execute().actionGet();
+            fail("Shouldn't be here");
+        } catch (SnapshotMissingException ex) {
+            // Expected
+        }
+    }
+
+    /**
+     * For issue #86: https://github.com/elasticsearch/elasticsearch-cloud-aws/issues/86
+     */
+    @Test
+    public void testGetDeleteNonExistingSnapshot_86() {
+        ClusterAdminClient client = client().admin().cluster();
+        logger.info("-->  creating s3 repository without any path");
+        PutRepositoryResponse putRepositoryResponse = client.preparePutRepository("test-repo")
+                .setType("s3").setSettings(Settings.settingsBuilder()
+                        .put("base_path", basePath)
+                        ).get();
+        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
+
+        try {
+            client.prepareGetSnapshots("test-repo").addSnapshots("no-existing-snapshot").get();
+            fail("Shouldn't be here");
+        } catch (SnapshotMissingException ex) {
+            // Expected
+        }
+
+        try {
+            client.prepareDeleteSnapshot("test-repo", "no-existing-snapshot").get();
+            fail("Shouldn't be here");
+        } catch (SnapshotMissingException ex) {
+            // Expected
+        }
+    }
+
+    private void assertRepositoryIsOperational(Client client, String repository) {
+        createIndex("test-idx-1");
+        ensureGreen();
+
+        logger.info("--> indexing some data");
+        for (int i = 0; i < 100; i++) {
+            index("test-idx-1", "doc", Integer.toString(i), "foo", "bar" + i);
+        }
+        refresh();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
+
+        logger.info("--> snapshot");
+        CreateSnapshotResponse createSnapshotResponse = client.admin().cluster().prepareCreateSnapshot(repository, "test-snap").setWaitForCompletion(true).setIndices("test-idx-*").get();
+        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
+        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse.getSnapshotInfo().totalShards()));
+
+        assertThat(client.admin().cluster().prepareGetSnapshots(repository).setSnapshots("test-snap").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
+
+        logger.info("--> delete some data");
+        for (int i = 0; i < 50; i++) {
+            client.prepareDelete("test-idx-1", "doc", Integer.toString(i)).get();
+        }
+        refresh();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(50L));
+
+        logger.info("--> close indices");
+        client.admin().indices().prepareClose("test-idx-1").get();
+
+        logger.info("--> restore all indices from the snapshot");
+        RestoreSnapshotResponse restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot(repository, "test-snap").setWaitForCompletion(true).execute().actionGet();
+        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
+
+        ensureGreen();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
+    }
+
+
+    /**
+     * Deletes repositories, supports wildcard notation.
+     */
+    public static void wipeRepositories(String... repositories) {
+        // if nothing is provided, delete all
+        if (repositories.length == 0) {
+            repositories = new String[]{"*"};
+        }
+        for (String repository : repositories) {
+            try {
+                client().admin().cluster().prepareDeleteRepository(repository).execute().actionGet();
+            } catch (RepositoryMissingException ex) {
+                // ignore
+            }
+        }
+    }
+
+    /**
+     * Deletes content of the repository files in the bucket
+     */
+    public void cleanRepositoryFiles(String basePath) {
+        Settings settings = internalCluster().getInstance(Settings.class);
+        Settings[] buckets = {
+                settings.getByPrefix("repositories.s3."),
+                settings.getByPrefix("repositories.s3.private-bucket."),
+                settings.getByPrefix("repositories.s3.remote-bucket."),
+                settings.getByPrefix("repositories.s3.external-bucket.")
+        };
+        for (Settings bucket : buckets) {
+            String endpoint = bucket.get("endpoint", settings.get("repositories.s3.endpoint"));
+            String protocol = bucket.get("protocol", settings.get("repositories.s3.protocol"));
+            String region = bucket.get("region", settings.get("repositories.s3.region"));
+            String accessKey = bucket.get("access_key", settings.get("cloud.aws.access_key"));
+            String secretKey = bucket.get("secret_key", settings.get("cloud.aws.secret_key"));
+            String bucketName = bucket.get("bucket");
+
+            // We check that settings has been set in elasticsearch.yml integration test file
+            // as described in README
+            assertThat("Your settings in elasticsearch.yml are incorrects. Check README file.", bucketName, notNullValue());
+            AmazonS3 client = internalCluster().getInstance(AwsS3Service.class).client(endpoint, protocol, region, accessKey, secretKey);
+            try {
+                ObjectListing prevListing = null;
+                //From http://docs.amazonwebservices.com/AmazonS3/latest/dev/DeletingMultipleObjectsUsingJava.html
+                //we can do at most 1K objects per delete
+                //We don't know the bucket name until first object listing
+                DeleteObjectsRequest multiObjectDeleteRequest = null;
+                ArrayList<DeleteObjectsRequest.KeyVersion> keys = new ArrayList<DeleteObjectsRequest.KeyVersion>();
+                while (true) {
+                    ObjectListing list;
+                    if (prevListing != null) {
+                        list = client.listNextBatchOfObjects(prevListing);
+                    } else {
+                        list = client.listObjects(bucketName, basePath);
+                        multiObjectDeleteRequest = new DeleteObjectsRequest(list.getBucketName());
+                    }
+                    for (S3ObjectSummary summary : list.getObjectSummaries()) {
+                        keys.add(new DeleteObjectsRequest.KeyVersion(summary.getKey()));
+                        //Every 500 objects batch the delete request
+                        if (keys.size() > 500) {
+                            multiObjectDeleteRequest.setKeys(keys);
+                            client.deleteObjects(multiObjectDeleteRequest);
+                            multiObjectDeleteRequest = new DeleteObjectsRequest(list.getBucketName());
+                            keys.clear();
+                        }
+                    }
+                    if (list.isTruncated()) {
+                        prevListing = list;
+                    } else {
+                        break;
+                    }
+                }
+                if (!keys.isEmpty()) {
+                    multiObjectDeleteRequest.setKeys(keys);
+                    client.deleteObjects(multiObjectDeleteRequest);
+                }
+            } catch (Throwable ex) {
+                logger.warn("Failed to delete S3 repository [{}] in [{}]", ex, bucketName, region);
+            }
+        }
+    }
+}
diff --git a/plugins/repository-s3/src/test/java/org/elasticsearch/repositories/s3/RepositoryS3RestIT.java b/plugins/repository-s3/src/test/java/org/elasticsearch/repositories/s3/RepositoryS3RestIT.java
new file mode 100644
index 0000000..d8e436b
--- /dev/null
+++ b/plugins/repository-s3/src/test/java/org/elasticsearch/repositories/s3/RepositoryS3RestIT.java
@@ -0,0 +1,41 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.repositories.s3;
+
+import com.carrotsearch.randomizedtesting.annotations.Name;
+import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
+import org.elasticsearch.test.rest.ESRestTestCase;
+import org.elasticsearch.test.rest.RestTestCandidate;
+import org.elasticsearch.test.rest.parser.RestTestParseException;
+
+import java.io.IOException;
+
+public class RepositoryS3RestIT extends ESRestTestCase {
+
+    public RepositoryS3RestIT(@Name("yaml") RestTestCandidate testCandidate) {
+        super(testCandidate);
+    }
+
+    @ParametersFactory
+    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
+        return ESRestTestCase.createParameters(0, 1);
+    }
+}
+
diff --git a/plugins/repository-s3/src/test/java/org/elasticsearch/repositories/s3/S3ProxiedSnapshotRestoreOverHttpsTests.java b/plugins/repository-s3/src/test/java/org/elasticsearch/repositories/s3/S3ProxiedSnapshotRestoreOverHttpsTests.java
new file mode 100644
index 0000000..667a756
--- /dev/null
+++ b/plugins/repository-s3/src/test/java/org/elasticsearch/repositories/s3/S3ProxiedSnapshotRestoreOverHttpsTests.java
@@ -0,0 +1,47 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.repositories.s3;
+
+import org.elasticsearch.common.settings.Settings;
+import org.junit.Before;
+
+/**
+ * This will only run if you define in your `elasticsearch.yml` file a s3 specific proxy
+ * cloud.aws.s3.proxy_host: mys3proxy.company.com
+ * cloud.aws.s3.proxy_port: 8080
+ */
+public class S3ProxiedSnapshotRestoreOverHttpsTests extends AbstractS3SnapshotRestoreTest {
+
+    private boolean proxySet = false;
+
+    @Override
+    public Settings nodeSettings(int nodeOrdinal) {
+        Settings settings = super.nodeSettings(nodeOrdinal);
+        String proxyHost = settings.get("cloud.aws.s3.proxy_host");
+        proxySet = proxyHost != null;
+        return settings;
+    }
+
+    @Before
+    public void checkProxySettings() {
+        assumeTrue("we are expecting proxy settings in elasticsearch.yml file", proxySet);
+    }
+
+}
diff --git a/plugins/repository-s3/src/test/java/org/elasticsearch/repositories/s3/S3SnapshotRestoreOverHttpTests.java b/plugins/repository-s3/src/test/java/org/elasticsearch/repositories/s3/S3SnapshotRestoreOverHttpTests.java
new file mode 100644
index 0000000..bcc430e
--- /dev/null
+++ b/plugins/repository-s3/src/test/java/org/elasticsearch/repositories/s3/S3SnapshotRestoreOverHttpTests.java
@@ -0,0 +1,34 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.repositories.s3;
+
+import org.elasticsearch.common.settings.Settings;
+
+/**
+ */
+public class S3SnapshotRestoreOverHttpTests extends AbstractS3SnapshotRestoreTest {
+    @Override
+    public Settings nodeSettings(int nodeOrdinal) {
+        Settings.Builder settings = Settings.builder()
+                .put(super.nodeSettings(nodeOrdinal))
+                .put("cloud.aws.s3.protocol", "http");
+        return settings.build();
+    }
+}
diff --git a/plugins/repository-s3/src/test/java/org/elasticsearch/repositories/s3/S3SnapshotRestoreOverHttpsTests.java b/plugins/repository-s3/src/test/java/org/elasticsearch/repositories/s3/S3SnapshotRestoreOverHttpsTests.java
new file mode 100644
index 0000000..8bb53ed
--- /dev/null
+++ b/plugins/repository-s3/src/test/java/org/elasticsearch/repositories/s3/S3SnapshotRestoreOverHttpsTests.java
@@ -0,0 +1,34 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.repositories.s3;
+
+import org.elasticsearch.common.settings.Settings;
+
+/**
+ */
+public class S3SnapshotRestoreOverHttpsTests extends AbstractS3SnapshotRestoreTest {
+    @Override
+    public Settings nodeSettings(int nodeOrdinal) {
+        Settings.Builder settings = Settings.builder()
+                .put(super.nodeSettings(nodeOrdinal))
+                .put("cloud.aws.s3.protocol", "https");
+        return settings.build();
+    }
+}
diff --git a/plugins/site-example/pom.xml b/plugins/site-example/pom.xml
index 3c25d5a..38b1b0d 100644
--- a/plugins/site-example/pom.xml
+++ b/plugins/site-example/pom.xml
@@ -7,7 +7,7 @@
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>plugins</artifactId>
-        <version>2.1.0-SNAPSHOT</version>
+        <version>3.0.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>site-example</artifactId>
diff --git a/pom.xml b/pom.xml
index 0cc34ed..3ba48a6 100644
--- a/pom.xml
+++ b/pom.xml
@@ -6,7 +6,7 @@
 
     <groupId>org.elasticsearch</groupId>
     <artifactId>parent</artifactId>
-    <version>2.1.0-SNAPSHOT</version>
+    <version>3.0.0-SNAPSHOT</version>
     <packaging>pom</packaging>
     <name>Elasticsearch: Parent POM</name>
     <description>Parent POM</description>
@@ -716,11 +716,9 @@
                                 </balancers>
                                 <includes>
                                     <include>**/*Tests.class</include>
-                                    <include>**/*Test.class</include>
                                 </includes>
                                 <excludes>
-                                    <exclude>**/Abstract*.class</exclude>
-                                    <exclude>**/*StressTest.class</exclude>
+                                    <exclude>**/*$*.class</exclude>
                                 </excludes>
                             </configuration>
                         </execution>
diff --git a/qa/pom.xml b/qa/pom.xml
index f8b1f38..e6bda14 100644
--- a/qa/pom.xml
+++ b/qa/pom.xml
@@ -7,7 +7,7 @@
 
     <groupId>org.elasticsearch.qa</groupId>
     <artifactId>elasticsearch-qa</artifactId>
-    <version>2.1.0-SNAPSHOT</version>
+    <version>3.0.0-SNAPSHOT</version>
     <packaging>pom</packaging>
     <name>QA: Parent POM</name>
     <inceptionYear>2015</inceptionYear>
@@ -15,7 +15,7 @@
     <parent>
         <groupId>org.elasticsearch</groupId>
         <artifactId>parent</artifactId>
-        <version>2.1.0-SNAPSHOT</version>
+        <version>3.0.0-SNAPSHOT</version>
     </parent>
 
     <properties>
@@ -147,6 +147,7 @@
     <modules>
         <module>smoke-test-plugins</module>
         <module>smoke-test-multinode</module>
+        <module>smoke-test-client</module>
     </modules>
 
     <profiles>
diff --git a/qa/smoke-test-client/pom.xml b/qa/smoke-test-client/pom.xml
new file mode 100644
index 0000000..82495c1
--- /dev/null
+++ b/qa/smoke-test-client/pom.xml
@@ -0,0 +1,129 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project xmlns="http://maven.apache.org/POM/4.0.0"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
+    <parent>
+        <artifactId>elasticsearch-qa</artifactId>
+        <groupId>org.elasticsearch.qa</groupId>
+        <version>3.0.0-SNAPSHOT</version>
+    </parent>
+    <modelVersion>4.0.0</modelVersion>
+
+    <!--
+      This test unzips and starts elasticsearch.
+      It then creates a Java Client and makes sure simple
+      Java API calls works nicely
+
+      It will also helps to better document the Java API reference guide
+      and keep it up-to-date when API is changing.
+    -->
+
+    <artifactId>smoke-test-client</artifactId>
+    <name>QA: Smoke Test Client</name>
+    <description>Test the Java Client against a running cluster</description>
+
+    <properties>
+        <skip.unit.tests>true</skip.unit.tests>
+    </properties>
+
+    <dependencies>
+        <dependency>
+            <groupId>org.elasticsearch</groupId>
+            <artifactId>elasticsearch</artifactId>
+            <scope>test</scope>
+        </dependency>
+        <dependency>
+            <groupId>log4j</groupId>
+            <artifactId>log4j</artifactId>
+            <scope>test</scope>
+        </dependency>
+    </dependencies>
+
+    <build>
+        <testResources>
+            <testResource>
+                <directory>src/test/resources</directory>
+            </testResource>
+            <!-- shared test resources like log4j.properties -->
+            <testResource>
+                <directory>${elasticsearch.tools.directory}/shared-test-resources</directory>
+                <filtering>false</filtering>
+            </testResource>
+        </testResources>
+
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-remote-resources-plugin</artifactId>
+            </plugin>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-dependency-plugin</artifactId>
+                <executions>
+                    <execution>
+                        <id>integ-setup-dependencies</id>
+                        <phase>pre-integration-test</phase>
+                        <goals>
+                            <goal>copy</goal>
+                        </goals>
+                        <configuration>
+                            <skip>${skip.integ.tests}</skip>
+                            <useBaseVersion>true</useBaseVersion>
+                            <outputDirectory>${integ.deps}/plugins</outputDirectory>
+
+                            <artifactItems>
+                                <!-- elasticsearch distribution -->
+                                <artifactItem>
+                                    <groupId>org.elasticsearch.distribution.zip</groupId>
+                                    <artifactId>elasticsearch</artifactId>
+                                    <version>${elasticsearch.version}</version>
+                                    <type>zip</type>
+                                    <overWrite>true</overWrite>
+                                    <outputDirectory>${integ.deps}</outputDirectory>
+                                </artifactItem>
+                            </artifactItems>
+                        </configuration>
+                    </execution>
+                </executions>
+            </plugin>
+
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-antrun-plugin</artifactId>
+                <executions>
+                    <!-- start up external cluster -->
+                    <execution>
+                        <id>integ-setup</id>
+                        <phase>pre-integration-test</phase>
+                        <goals>
+                            <goal>run</goal>
+                        </goals>
+                        <configuration>
+                            <skip>${skip.integ.tests}</skip>
+                            <target>
+                                <ant antfile="${elasticsearch.integ.antfile}" target="start-external-cluster">
+                                    <property name="tests.jvm.argline" value="${tests.jvm.argline}"/>
+                                </ant>
+                            </target>
+                        </configuration>
+                    </execution>
+                    <!-- shut down external cluster -->
+                    <execution>
+                        <id>integ-teardown</id>
+                        <phase>post-integration-test</phase>
+                        <goals>
+                            <goal>run</goal>
+                        </goals>
+                        <configuration>
+                            <skip>${skip.integ.tests}</skip>
+                            <target>
+                                <ant antfile="${elasticsearch.integ.antfile}" target="stop-external-cluster"/>
+                            </target>
+                        </configuration>
+                    </execution>
+                </executions>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
diff --git a/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java b/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java
new file mode 100644
index 0000000..e18ca34
--- /dev/null
+++ b/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java
@@ -0,0 +1,174 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.smoketest;
+
+import org.apache.lucene.util.LuceneTestCase;
+import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
+import org.elasticsearch.client.Client;
+import org.elasticsearch.client.transport.TransportClient;
+import org.elasticsearch.common.logging.ESLogger;
+import org.elasticsearch.common.logging.ESLoggerFactory;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.transport.InetSocketTransportAddress;
+import org.elasticsearch.common.transport.TransportAddress;
+import org.elasticsearch.node.internal.InternalSettingsPreparer;
+import org.junit.After;
+import org.junit.AfterClass;
+import org.junit.Before;
+import org.junit.BeforeClass;
+
+import java.net.InetAddress;
+import java.net.UnknownHostException;
+import java.nio.file.Path;
+import java.util.Locale;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import static com.carrotsearch.randomizedtesting.RandomizedTest.randomAsciiOfLength;
+import static org.hamcrest.Matchers.notNullValue;
+
+/**
+ * {@link ESSmokeClientTestCase} is an abstract base class to run integration
+ * tests against an external Elasticsearch Cluster.
+ * <p/>
+ * You can define a list of transport addresses from where you can reach your cluster
+ * by setting "tests.cluster" system property. It defaults to "localhost:9300".
+ * <p/>
+ * All tests can be run from maven using mvn install as maven will start an external cluster first.
+ * <p/>
+ * If you want to debug this module from your IDE, then start an external cluster by yourself
+ * then run JUnit. If you changed the default port, set "tests.cluster=localhost:PORT" when running
+ * your test.
+ */
+@LuceneTestCase.SuppressSysoutChecks(bugUrl = "we log a lot on purpose")
+public abstract class ESSmokeClientTestCase extends LuceneTestCase {
+
+    /**
+     * Key used to eventually switch to using an external cluster and provide its transport addresses
+     */
+    public static final String TESTS_CLUSTER = "tests.cluster";
+
+    /**
+     * Defaults to localhost:9300
+     */
+    public static final String TESTS_CLUSTER_DEFAULT = "localhost:9300";
+
+    protected static ESLogger logger = ESLoggerFactory.getLogger(ESSmokeClientTestCase.class.getName());
+
+    private static final AtomicInteger counter = new AtomicInteger();
+    private static Client client;
+    private static String clusterAddresses;
+    protected String index;
+
+    private static Client startClient(Path tempDir, TransportAddress... transportAddresses) {
+        Settings clientSettings = Settings.settingsBuilder()
+                .put("name", "qa_smoke_client_" + counter.getAndIncrement())
+                .put(InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING, true) // prevents any settings to be replaced by system properties.
+                .put("client.transport.ignore_cluster_name", true)
+                .put("path.home", tempDir)
+                .put("node.mode", "network").build(); // we require network here!
+
+        TransportClient.Builder transportClientBuilder = TransportClient.builder().settings(clientSettings);
+        TransportClient client = transportClientBuilder.build().addTransportAddresses(transportAddresses);
+
+        logger.info("--> Elasticsearch Java TransportClient started");
+
+        Exception clientException = null;
+        try {
+            ClusterHealthResponse health = client.admin().cluster().prepareHealth().get();
+            logger.info("--> connected to [{}] cluster which is running [{}] node(s).",
+                    health.getClusterName(), health.getNumberOfNodes());
+        } catch (Exception e) {
+            clientException = e;
+        }
+
+        assumeNoException("Sounds like your cluster is not running at " + clusterAddresses, clientException);
+
+        return client;
+    }
+
+    private static Client startClient() throws UnknownHostException {
+        String[] stringAddresses = clusterAddresses.split(",");
+        TransportAddress[] transportAddresses = new TransportAddress[stringAddresses.length];
+        int i = 0;
+        for (String stringAddress : stringAddresses) {
+            String[] split = stringAddress.split(":");
+            if (split.length < 2) {
+                throw new IllegalArgumentException("address [" + clusterAddresses + "] not valid");
+            }
+            try {
+                transportAddresses[i++] = new InetSocketTransportAddress(InetAddress.getByName(split[0]), Integer.valueOf(split[1]));
+            } catch (NumberFormatException e) {
+                throw new IllegalArgumentException("port is not valid, expected number but was [" + split[1] + "]");
+            }
+        }
+        return startClient(createTempDir(), transportAddresses);
+    }
+
+    public static Client getClient() {
+        if (client == null) {
+            try {
+                client = startClient();
+            } catch (UnknownHostException e) {
+                logger.error("can not start the client", e);
+            }
+            assertThat(client, notNullValue());
+        }
+        return client;
+    }
+
+    @BeforeClass
+    public static void initializeSettings() throws UnknownHostException {
+        clusterAddresses = System.getProperty(TESTS_CLUSTER);
+        if (clusterAddresses == null || clusterAddresses.isEmpty()) {
+            clusterAddresses = TESTS_CLUSTER_DEFAULT;
+            logger.info("[{}] not set. Falling back to [{}]", TESTS_CLUSTER, TESTS_CLUSTER_DEFAULT);
+        }
+    }
+
+    @AfterClass
+    public static void stopTransportClient() {
+        if (client != null) {
+            client.close();
+            client = null;
+        }
+    }
+
+    @Before
+    public void defineIndexName() {
+        doClean();
+        index = "qa-smoke-test-client-" + randomAsciiOfLength(10).toLowerCase(Locale.getDefault());
+    }
+
+    @After
+    public void cleanIndex() {
+        doClean();
+    }
+
+    private void doClean() {
+        if (client != null) {
+            try {
+                client.admin().indices().prepareDelete(index).get();
+            } catch (Exception e) {
+                // We ignore this cleanup exception
+            }
+        }
+    }
+
+}
diff --git a/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/SmokeTestClientIT.java b/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/SmokeTestClientIT.java
new file mode 100644
index 0000000..4c324b0
--- /dev/null
+++ b/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/SmokeTestClientIT.java
@@ -0,0 +1,71 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.smoketest;
+
+import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
+import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.client.Client;
+import org.junit.Test;
+
+import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.Matchers.greaterThan;
+
+public class SmokeTestClientIT extends ESSmokeClientTestCase {
+
+    /**
+     * Check that we are connected to a cluster named "elasticsearch".
+     */
+    @Test
+    public void testSimpleClient() {
+        Client client = getClient();
+
+        // START SNIPPET: java-doc-admin-cluster-health
+        ClusterHealthResponse health = client.admin().cluster().prepareHealth().setWaitForYellowStatus().get();
+        String clusterName = health.getClusterName();
+        int numberOfNodes = health.getNumberOfNodes();
+        // END SNIPPET: java-doc-admin-cluster-health
+        assertThat("cluster [" + clusterName + "] should have at least 1 node", numberOfNodes, greaterThan(0));
+    }
+
+    /**
+     * Create an index and index some docs
+     */
+    @Test
+    public void testPutDocument() {
+        Client client = getClient();
+
+        // START SNIPPET: java-doc-index-doc-simple
+        client.prepareIndex(index, "doc", "1")  // Index, Type, Id
+                .setSource("foo", "bar")        // Simple document: { "foo" : "bar" }
+                .get();                         // Execute and wait for the result
+        // END SNIPPET: java-doc-index-doc-simple
+
+        // START SNIPPET: java-doc-admin-indices-refresh
+        // Prepare a refresh action on a given index, execute and wait for the result
+        client.admin().indices().prepareRefresh(index).get();
+        // END SNIPPET: java-doc-admin-indices-refresh
+
+        // START SNIPPET: java-doc-search-simple
+        SearchResponse searchResponse = client.prepareSearch(index).get();
+        assertThat(searchResponse.getHits().getTotalHits(), is(1L));
+        // END SNIPPET: java-doc-search-simple
+    }
+}
+
diff --git a/qa/smoke-test-multinode/pom.xml b/qa/smoke-test-multinode/pom.xml
index 4c23f7b..a2aa7b0 100644
--- a/qa/smoke-test-multinode/pom.xml
+++ b/qa/smoke-test-multinode/pom.xml
@@ -8,7 +8,7 @@
   <parent>
     <groupId>org.elasticsearch.qa</groupId>
     <artifactId>elasticsearch-qa</artifactId>
-    <version>2.1.0-SNAPSHOT</version>
+    <version>3.0.0-SNAPSHOT</version>
   </parent>
 
   <!-- 
diff --git a/qa/smoke-test-plugins/pom.xml b/qa/smoke-test-plugins/pom.xml
index beae9eb..128dab1 100644
--- a/qa/smoke-test-plugins/pom.xml
+++ b/qa/smoke-test-plugins/pom.xml
@@ -8,7 +8,7 @@
   <parent>
     <groupId>org.elasticsearch.qa</groupId>
     <artifactId>elasticsearch-qa</artifactId>
-    <version>2.1.0-SNAPSHOT</version>
+    <version>3.0.0-SNAPSHOT</version>
   </parent>
 
   <!-- 
@@ -242,7 +242,7 @@
                  <!-- plugins -->
                  <artifactItem>
                    <groupId>org.elasticsearch.plugin</groupId>
-                   <artifactId>analysis-kuromoji</artifactId>
+                   <artifactId>analysis-icu</artifactId>
                    <version>${elasticsearch.version}</version>
                    <type>zip</type>
                    <overWrite>true</overWrite>
@@ -250,7 +250,7 @@
 
                  <artifactItem>
                    <groupId>org.elasticsearch.plugin</groupId>
-                   <artifactId>analysis-smartcn</artifactId>
+                   <artifactId>analysis-kuromoji</artifactId>
                    <version>${elasticsearch.version}</version>
                    <type>zip</type>
                    <overWrite>true</overWrite>
@@ -258,7 +258,7 @@
 
                  <artifactItem>
                    <groupId>org.elasticsearch.plugin</groupId>
-                   <artifactId>analysis-stempel</artifactId>
+                   <artifactId>analysis-phonetic</artifactId>
                    <version>${elasticsearch.version}</version>
                    <type>zip</type>
                    <overWrite>true</overWrite>
@@ -266,7 +266,7 @@
 
                  <artifactItem>
                    <groupId>org.elasticsearch.plugin</groupId>
-                   <artifactId>analysis-phonetic</artifactId>
+                   <artifactId>analysis-smartcn</artifactId>
                    <version>${elasticsearch.version}</version>
                    <type>zip</type>
                    <overWrite>true</overWrite>
@@ -274,7 +274,7 @@
 
                  <artifactItem>
                    <groupId>org.elasticsearch.plugin</groupId>
-                   <artifactId>analysis-icu</artifactId>
+                   <artifactId>analysis-stempel</artifactId>
                    <version>${elasticsearch.version}</version>
                    <type>zip</type>
                    <overWrite>true</overWrite>
@@ -282,7 +282,7 @@
 
                  <artifactItem>
                    <groupId>org.elasticsearch.plugin</groupId>
-                   <artifactId>cloud-gce</artifactId>
+                   <artifactId>cloud-azure</artifactId>
                    <version>${elasticsearch.version}</version>
                    <type>zip</type>
                    <overWrite>true</overWrite>
@@ -290,7 +290,7 @@
 
                  <artifactItem>
                    <groupId>org.elasticsearch.plugin</groupId>
-                   <artifactId>cloud-azure</artifactId>
+                   <artifactId>cloud-gce</artifactId>
                    <version>${elasticsearch.version}</version>
                    <type>zip</type>
                    <overWrite>true</overWrite>
@@ -298,7 +298,7 @@
 
                  <artifactItem>
                    <groupId>org.elasticsearch.plugin</groupId>
-                   <artifactId>cloud-aws</artifactId>
+                   <artifactId>delete-by-query</artifactId>
                    <version>${elasticsearch.version}</version>
                    <type>zip</type>
                    <overWrite>true</overWrite>
@@ -306,7 +306,7 @@
 
                  <artifactItem>
                    <groupId>org.elasticsearch.plugin</groupId>
-                   <artifactId>delete-by-query</artifactId>
+                   <artifactId>discovery-ec2</artifactId>
                    <version>${elasticsearch.version}</version>
                    <type>zip</type>
                    <overWrite>true</overWrite>
@@ -322,7 +322,7 @@
 
                  <artifactItem>
                    <groupId>org.elasticsearch.plugin</groupId>
-                   <artifactId>lang-python</artifactId>
+                   <artifactId>lang-javascript</artifactId>
                    <version>${elasticsearch.version}</version>
                    <type>zip</type>
                    <overWrite>true</overWrite>
@@ -330,7 +330,7 @@
 
                  <artifactItem>
                    <groupId>org.elasticsearch.plugin</groupId>
-                   <artifactId>lang-javascript</artifactId>
+                   <artifactId>lang-python</artifactId>
                    <version>${elasticsearch.version}</version>
                    <type>zip</type>
                    <overWrite>true</overWrite>
@@ -354,12 +354,13 @@
 
                  <artifactItem>
                    <groupId>org.elasticsearch.plugin</groupId>
-                   <artifactId>site-example</artifactId>
+                   <artifactId>repository-s3</artifactId>
                    <version>${elasticsearch.version}</version>
                    <type>zip</type>
                    <overWrite>true</overWrite>
                  </artifactItem>
 
+                 <!-- Internal plugins for test purpose only -->
                  <artifactItem>
                    <groupId>org.elasticsearch.plugin</groupId>
                    <artifactId>jvm-example</artifactId>
@@ -368,6 +369,14 @@
                    <overWrite>true</overWrite>
                  </artifactItem>
 
+                 <artifactItem>
+                   <groupId>org.elasticsearch.plugin</groupId>
+                   <artifactId>site-example</artifactId>
+                   <version>${elasticsearch.version}</version>
+                   <type>zip</type>
+                   <overWrite>true</overWrite>
+                 </artifactItem>
+
                </artifactItems>
              </configuration>
            </execution>
diff --git a/qa/vagrant/pom.xml b/qa/vagrant/pom.xml
index ed9422b..728b83d 100644
--- a/qa/vagrant/pom.xml
+++ b/qa/vagrant/pom.xml
@@ -6,7 +6,7 @@
     <parent>
         <groupId>org.elasticsearch.qa</groupId>
         <artifactId>elasticsearch-qa</artifactId>
-        <version>2.1.0-SNAPSHOT</version>
+        <version>3.0.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>qa-vagrant</artifactId>
diff --git a/rest-api-spec/pom.xml b/rest-api-spec/pom.xml
index 4adc460..e21e55f 100644
--- a/rest-api-spec/pom.xml
+++ b/rest-api-spec/pom.xml
@@ -2,7 +2,7 @@
   <modelVersion>4.0.0</modelVersion>
   <groupId>org.elasticsearch</groupId>
   <artifactId>rest-api-spec</artifactId>
-  <version>2.1.0-SNAPSHOT</version>
+  <version>3.0.0-SNAPSHOT</version>
   <name>Rest API Specification</name>
   <description>REST API Specification and tests for use with the Elasticsearch REST Test framework</description>
   <parent>
