diff --git a/core/pom.xml b/core/pom.xml
index 2a5f6de..dc0d644 100644
--- a/core/pom.xml
+++ b/core/pom.xml
@@ -334,11 +334,14 @@
                         <!-- Guice -->
                         <exclude>src/main/java/org/elasticsearch/common/inject/**</exclude>
                         <exclude>src/main/java/org/elasticsearch/common/geo/GeoHashUtils.java</exclude>
+                        <exclude>src/main/java/org/elasticsearch/common/network/InetAddresses.java</exclude>
                         <exclude>src/main/java/org/apache/lucene/**/X*.java</exclude>
                         <!-- t-digest -->
                         <exclude>src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestState.java</exclude>
                         <!-- netty pipelining -->
                         <exclude>src/main/java/org/elasticsearch/http/netty/pipelining/**</exclude>
+                        <exclude>src/test/java/org/elasticsearch/common/network/InetAddressesTests.java</exclude>
+                        <exclude>src/test/java/org/elasticsearch/common/collect/EvictingQueueTests.java</exclude>
                     </excludes>
                 </configuration>
             </plugin>
diff --git a/core/src/main/java/org/elasticsearch/ElasticsearchException.java b/core/src/main/java/org/elasticsearch/ElasticsearchException.java
index 87348f9..4c82c28 100644
--- a/core/src/main/java/org/elasticsearch/ElasticsearchException.java
+++ b/core/src/main/java/org/elasticsearch/ElasticsearchException.java
@@ -482,7 +482,7 @@ public class ElasticsearchException extends RuntimeException implements ToXConte
         RESOURCE_NOT_FOUND_EXCEPTION(org.elasticsearch.ResourceNotFoundException.class, org.elasticsearch.ResourceNotFoundException::new, 19),
         ACTION_TRANSPORT_EXCEPTION(org.elasticsearch.transport.ActionTransportException.class, org.elasticsearch.transport.ActionTransportException::new, 20),
         ELASTICSEARCH_GENERATION_EXCEPTION(org.elasticsearch.ElasticsearchGenerationException.class, org.elasticsearch.ElasticsearchGenerationException::new, 21),
-        CREATE_FAILED_ENGINE_EXCEPTION(org.elasticsearch.index.engine.CreateFailedEngineException.class, org.elasticsearch.index.engine.CreateFailedEngineException::new, 22),
+        //      22 was CreateFailedEngineException
         INDEX_SHARD_STARTED_EXCEPTION(org.elasticsearch.index.shard.IndexShardStartedException.class, org.elasticsearch.index.shard.IndexShardStartedException::new, 23),
         SEARCH_CONTEXT_MISSING_EXCEPTION(org.elasticsearch.search.SearchContextMissingException.class, org.elasticsearch.search.SearchContextMissingException::new, 24),
         SCRIPT_EXCEPTION(org.elasticsearch.script.ScriptException.class, org.elasticsearch.script.ScriptException::new, 25),
@@ -514,7 +514,7 @@ public class ElasticsearchException extends RuntimeException implements ToXConte
         INDEX_SHARD_ALREADY_EXISTS_EXCEPTION(org.elasticsearch.index.IndexShardAlreadyExistsException.class, org.elasticsearch.index.IndexShardAlreadyExistsException::new, 51),
         VERSION_CONFLICT_ENGINE_EXCEPTION(org.elasticsearch.index.engine.VersionConflictEngineException.class, org.elasticsearch.index.engine.VersionConflictEngineException::new, 52),
         ENGINE_EXCEPTION(org.elasticsearch.index.engine.EngineException.class, org.elasticsearch.index.engine.EngineException::new, 53),
-        DOCUMENT_ALREADY_EXISTS_EXCEPTION(org.elasticsearch.index.engine.DocumentAlreadyExistsException.class, org.elasticsearch.index.engine.DocumentAlreadyExistsException::new, 54),
+        // 54 was DocumentAlreadyExistsException, which is superseded by VersionConflictEngineException
         NO_SUCH_NODE_EXCEPTION(org.elasticsearch.action.NoSuchNodeException.class, org.elasticsearch.action.NoSuchNodeException::new, 55),
         SETTINGS_EXCEPTION(org.elasticsearch.common.settings.SettingsException.class, org.elasticsearch.common.settings.SettingsException::new, 56),
         INDEX_TEMPLATE_MISSING_EXCEPTION(org.elasticsearch.indices.IndexTemplateMissingException.class, org.elasticsearch.indices.IndexTemplateMissingException::new, 57),
diff --git a/core/src/main/java/org/elasticsearch/Version.java b/core/src/main/java/org/elasticsearch/Version.java
index 3fa9533..a610d8d 100644
--- a/core/src/main/java/org/elasticsearch/Version.java
+++ b/core/src/main/java/org/elasticsearch/Version.java
@@ -259,6 +259,8 @@ public class Version {
     public static final Version V_2_0_0_beta1 = new Version(V_2_0_0_beta1_ID, false, org.apache.lucene.util.Version.LUCENE_5_2_1);
     public static final int V_2_0_0_beta2_ID = 2000002;
     public static final Version V_2_0_0_beta2 = new Version(V_2_0_0_beta2_ID, false, org.apache.lucene.util.Version.LUCENE_5_2_1);
+    public static final int V_2_0_0_rc1_ID = 2000051;
+    public static final Version V_2_0_0_rc1 = new Version(V_2_0_0_rc1_ID, false, org.apache.lucene.util.Version.LUCENE_5_2_1);
     public static final int V_2_0_0_ID = 2000099;
     public static final Version V_2_0_0 = new Version(V_2_0_0_ID, true, org.apache.lucene.util.Version.LUCENE_5_2_1);
     public static final int V_2_1_0_ID = 2010099;
@@ -287,6 +289,8 @@ public class Version {
                 return V_2_1_0;
             case V_2_0_0_ID:
                 return V_2_0_0;
+            case V_2_0_0_rc1_ID:
+                return V_2_0_0_rc1;
             case V_2_0_0_beta2_ID:
                 return V_2_0_0_beta2;
             case V_2_0_0_beta1_ID:
diff --git a/core/src/main/java/org/elasticsearch/action/UnavailableShardsException.java b/core/src/main/java/org/elasticsearch/action/UnavailableShardsException.java
index dd0968e..ff31bb7 100644
--- a/core/src/main/java/org/elasticsearch/action/UnavailableShardsException.java
+++ b/core/src/main/java/org/elasticsearch/action/UnavailableShardsException.java
@@ -21,6 +21,7 @@ package org.elasticsearch.action;
 
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.collect.HppcMaps;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.rest.RestStatus;
@@ -32,8 +33,8 @@ import java.io.IOException;
  */
 public class UnavailableShardsException extends ElasticsearchException {
 
-    public UnavailableShardsException(@Nullable ShardId shardId, String message) {
-        super(buildMessage(shardId, message));
+    public UnavailableShardsException(@Nullable ShardId shardId, String message, Object... args) {
+        super(buildMessage(shardId, message), args);
     }
 
     private static String buildMessage(ShardId shardId, String message) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexResponse.java b/core/src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexResponse.java
index 6eac403..0930f8f 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexResponse.java
@@ -122,7 +122,7 @@ public class GetIndexResponse extends ActionResponse {
                         in.readString(),
                         in.readStringArray(),
                         in.readOptionalBoolean(),
-                        in.readBoolean() ? new IndexWarmersMetaData.SearchSource(in) : null)
+                        in.readBytesReference())
                 );
             }
             warmersMapBuilder.put(key, Collections.unmodifiableList(warmerEntryBuilder));
@@ -173,11 +173,7 @@ public class GetIndexResponse extends ActionResponse {
                 out.writeString(warmerEntry.name());
                 out.writeStringArray(warmerEntry.types());
                 out.writeOptionalBoolean(warmerEntry.requestCache());
-                boolean hasSource = warmerEntry.source() != null;
-                out.writeBoolean(hasSource);
-                if (hasSource) {
-                    warmerEntry.source().writeTo(out);
-                }
+                out.writeBytesReference(warmerEntry.source());
             }
         }
         out.writeVInt(mappings.size());
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/segments/IndexShardSegments.java b/core/src/main/java/org/elasticsearch/action/admin/indices/segments/IndexShardSegments.java
index 4132c7b..7cbb664 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/segments/IndexShardSegments.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/segments/IndexShardSegments.java
@@ -19,9 +19,9 @@
 
 package org.elasticsearch.action.admin.indices.segments;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.index.shard.ShardId;
 
+import java.util.Arrays;
 import java.util.Iterator;
 
 public class IndexShardSegments implements Iterable<ShardSegments> {
@@ -49,6 +49,6 @@ public class IndexShardSegments implements Iterable<ShardSegments> {
 
     @Override
     public Iterator<ShardSegments> iterator() {
-        return Iterators.forArray(shards);
+        return Arrays.stream(shards).iterator();
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndexShardStats.java b/core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndexShardStats.java
index e599468..dd4cc64 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndexShardStats.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndexShardStats.java
@@ -19,13 +19,13 @@
 
 package org.elasticsearch.action.admin.indices.stats;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
 import org.elasticsearch.index.shard.ShardId;
 
 import java.io.IOException;
+import java.util.Arrays;
 import java.util.Iterator;
 
 /**
@@ -57,7 +57,7 @@ public class IndexShardStats implements Iterable<ShardStats>, Streamable {
 
     @Override
     public Iterator<ShardStats> iterator() {
-        return Iterators.forArray(shards);
+        return Arrays.stream(shards).iterator();
     }
 
     private CommonStats total = null;
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/IndexShardUpgradeStatus.java b/core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/IndexShardUpgradeStatus.java
index e1cd163..c4b5820 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/IndexShardUpgradeStatus.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/IndexShardUpgradeStatus.java
@@ -19,9 +19,9 @@
 
 package org.elasticsearch.action.admin.indices.upgrade.get;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.index.shard.ShardId;
 
+import java.util.Arrays;
 import java.util.Iterator;
 
 public class IndexShardUpgradeStatus implements Iterable<ShardUpgradeStatus> {
@@ -49,7 +49,7 @@ public class IndexShardUpgradeStatus implements Iterable<ShardUpgradeStatus> {
 
     @Override
     public Iterator<ShardUpgradeStatus> iterator() {
-        return Iterators.forArray(shards);
+        return Arrays.stream(shards).iterator();
     }
 
     public long getTotalBytes() {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/get/GetWarmersResponse.java b/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/get/GetWarmersResponse.java
index 57e0b74..3ed444c 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/get/GetWarmersResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/get/GetWarmersResponse.java
@@ -20,8 +20,9 @@
 package org.elasticsearch.action.admin.indices.warmer.get;
 
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
-
+import org.elasticsearch.Version;
 import org.elasticsearch.action.ActionResponse;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
@@ -68,10 +69,7 @@ public class GetWarmersResponse extends ActionResponse {
             for (int j = 0; j < valueSize; j++) {
                 String name = in.readString();
                 String[] types = in.readStringArray();
-                IndexWarmersMetaData.SearchSource source = null;
-                if (in.readBoolean()) {
-                    source = new IndexWarmersMetaData.SearchSource(in);
-                }
+                BytesReference source = in.readBytesReference();
                 Boolean queryCache = null;
                 queryCache = in.readOptionalBoolean();
                 warmerEntryBuilder.add(new IndexWarmersMetaData.Entry(
@@ -96,11 +94,7 @@ public class GetWarmersResponse extends ActionResponse {
             for (IndexWarmersMetaData.Entry warmerEntry : indexEntry.value) {
                 out.writeString(warmerEntry.name());
                 out.writeStringArray(warmerEntry.types());
-                boolean hasWarmerSource = warmerEntry != null;
-                out.writeBoolean(hasWarmerSource);
-                if (hasWarmerSource) {
-                    warmerEntry.source().writeTo(out);
-                }
+                out.writeBytesReference(warmerEntry.source());
                 out.writeOptionalBoolean(warmerEntry.requestCache());
             }
         }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/put/TransportPutWarmerAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/put/TransportPutWarmerAction.java
index d72be81..18246f6 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/put/TransportPutWarmerAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/put/TransportPutWarmerAction.java
@@ -38,7 +38,6 @@ import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.IndexNotFoundException;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.warmer.IndexWarmersMetaData;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
@@ -115,9 +114,11 @@ public class TransportPutWarmerAction extends TransportMasterNodeAction<PutWarme
                         MetaData metaData = currentState.metaData();
                         String[] concreteIndices = indexNameExpressionResolver.concreteIndices(currentState, request.searchRequest().indicesOptions(), request.searchRequest().indices());
 
-                        IndexWarmersMetaData.SearchSource source = null;
-                        if (request.searchRequest().source() != null) {
-                            source = new IndexWarmersMetaData.SearchSource(request.searchRequest().source());
+                        BytesReference source = null;
+                        if (request.searchRequest().source() != null && request.searchRequest().source().length() > 0) {
+                            source = request.searchRequest().source();
+                        } else if (request.searchRequest().extraSource() != null && request.searchRequest().extraSource().length() > 0) {
+                            source = request.searchRequest().extraSource();
                         }
 
                         // now replace it on the metadata
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/BulkResponse.java b/core/src/main/java/org/elasticsearch/action/bulk/BulkResponse.java
index 50525fb..2076086 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/BulkResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/BulkResponse.java
@@ -19,13 +19,13 @@
 
 package org.elasticsearch.action.bulk;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.unit.TimeValue;
 
 import java.io.IOException;
+import java.util.Arrays;
 import java.util.Iterator;
 
 /**
@@ -95,7 +95,7 @@ public class BulkResponse extends ActionResponse implements Iterable<BulkItemRes
 
     @Override
     public Iterator<BulkItemResponse> iterator() {
-        return Iterators.forArray(responses);
+        return Arrays.stream(responses).iterator();
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java b/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java
index c071885..0f00b87 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java
@@ -47,7 +47,6 @@ import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.VersionType;
-import org.elasticsearch.index.engine.DocumentAlreadyExistsException;
 import org.elasticsearch.index.engine.Engine;
 import org.elasticsearch.index.engine.VersionConflictEngineException;
 import org.elasticsearch.index.mapper.Mapping;
@@ -97,6 +96,7 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
     protected TransportRequestOptions transportOptions() {
         return BulkAction.INSTANCE.transportOptions(settings);
     }
+
     @Override
     protected BulkShardResponse newResponseInstance() {
         return new BulkShardResponse();
@@ -416,7 +416,7 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
                 } catch (Throwable t) {
                     t = ExceptionsHelper.unwrapCause(t);
                     boolean retry = false;
-                    if (t instanceof VersionConflictEngineException || (t instanceof DocumentAlreadyExistsException && translate.operation() == UpdateHelper.Operation.UPSERT)) {
+                    if (t instanceof VersionConflictEngineException) {
                         retry = true;
                     }
                     return new UpdateResult(translate, indexRequest, retry, t, null);
@@ -460,20 +460,12 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
                     SourceToParse sourceToParse = SourceToParse.source(SourceToParse.Origin.REPLICA, indexRequest.source()).index(shardId.getIndex()).type(indexRequest.type()).id(indexRequest.id())
                             .routing(indexRequest.routing()).parent(indexRequest.parent()).timestamp(indexRequest.timestamp()).ttl(indexRequest.ttl());
 
-                    final Engine.IndexingOperation operation;
-                    if (indexRequest.opType() == IndexRequest.OpType.INDEX) {
-                        operation = indexShard.prepareIndex(sourceToParse, indexRequest.version(), indexRequest.versionType(), Engine.Operation.Origin.REPLICA);
-                    } else {
-                        assert indexRequest.opType() == IndexRequest.OpType.CREATE : indexRequest.opType();
-                        operation = indexShard.prepareCreate(sourceToParse,
-                                indexRequest.version(), indexRequest.versionType(),
-                                Engine.Operation.Origin.REPLICA);
-                    }
+                    final Engine.Index operation = indexShard.prepareIndex(sourceToParse, indexRequest.version(), indexRequest.versionType(), Engine.Operation.Origin.REPLICA);
                     Mapping update = operation.parsedDoc().dynamicMappingsUpdate();
                     if (update != null) {
                         throw new RetryOnReplicaException(shardId, "Mappings are not available on the replica yet, triggered update: " + update);
                     }
-                    operation.execute(indexShard);
+                    indexShard.index(operation);
                     location = locationToSync(location, operation.getTranslogLocation());
                 } catch (Throwable e) {
                     // if its not an ignore replica failure, we need to make sure to bubble up the failure
@@ -500,7 +492,7 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
             }
         }
 
-       processAfter(request.refresh(), indexShard, location);
+        processAfter(request.refresh(), indexShard, location);
     }
 
     private void applyVersion(BulkItemRequest item, long version, VersionType versionType) {
diff --git a/core/src/main/java/org/elasticsearch/action/count/CountRequest.java b/core/src/main/java/org/elasticsearch/action/count/CountRequest.java
index fd78687..05e193a 100644
--- a/core/src/main/java/org/elasticsearch/action/count/CountRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/count/CountRequest.java
@@ -19,21 +19,34 @@
 
 package org.elasticsearch.action.count;
 
+import org.elasticsearch.ElasticsearchGenerationException;
 import org.elasticsearch.action.search.SearchRequest;
+import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.action.support.broadcast.BroadcastRequest;
+import org.elasticsearch.client.Requests;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.index.query.QueryBuilder;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
 
 import java.io.IOException;
 import java.util.Arrays;
+import java.util.Map;
+
+import static org.elasticsearch.search.internal.SearchContext.DEFAULT_TERMINATE_AFTER;
 
 /**
  * A request to count the number of documents matching a specific query. Best created with
  * {@link org.elasticsearch.client.Requests#countRequest(String...)}.
+ * <p>
+ * The request requires the query source to be set either using {@link #source(QuerySourceBuilder)},
+ * or {@link #source(byte[])}.
  *
  * @see CountResponse
  * @see org.elasticsearch.client.Client#count(CountRequest)
@@ -41,15 +54,21 @@ import java.util.Arrays;
  */
 public class CountRequest extends BroadcastRequest<CountRequest> {
 
+    public static final float DEFAULT_MIN_SCORE = -1f;
+
+    private float minScore = DEFAULT_MIN_SCORE;
+
     @Nullable
     protected String routing;
 
     @Nullable
     private String preference;
 
+    private BytesReference source;
+
     private String[] types = Strings.EMPTY_ARRAY;
 
-    private SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();
+    private int terminateAfter = DEFAULT_TERMINATE_AFTER;
 
     /**
      * Constructs a new count request against the provided indices. No indices provided means it will
@@ -57,14 +76,13 @@ public class CountRequest extends BroadcastRequest<CountRequest> {
      */
     public CountRequest(String... indices) {
         super(indices);
-        searchSourceBuilder.size(0);
     }
 
     /**
      * The minimum score of the documents to include in the count.
      */
-    public Float minScore() {
-        return searchSourceBuilder.minScore();
+    public float minScore() {
+        return minScore;
     }
 
     /**
@@ -72,16 +90,69 @@ public class CountRequest extends BroadcastRequest<CountRequest> {
      * documents will be included in the count.
      */
     public CountRequest minScore(float minScore) {
-        this.searchSourceBuilder.minScore(minScore);
+        this.minScore = minScore;
+        return this;
+    }
+
+    /**
+     * The source to execute.
+     */
+    public BytesReference source() {
+        return source;
+    }
+
+    /**
+     * The source to execute.
+     */
+    public CountRequest source(QuerySourceBuilder sourceBuilder) {
+        this.source = sourceBuilder.buildAsBytes(Requests.CONTENT_TYPE);
+        return this;
+    }
+
+    /**
+     * The source to execute in the form of a map.
+     */
+    @SuppressWarnings("unchecked")
+    public CountRequest source(Map querySource) {
+        try {
+            XContentBuilder builder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
+            builder.map(querySource);
+            return source(builder);
+        } catch (IOException e) {
+            throw new ElasticsearchGenerationException("Failed to generate [" + querySource + "]", e);
+        }
+    }
+
+    public CountRequest source(XContentBuilder builder) {
+        this.source = builder.bytes();
+        return this;
+    }
+
+    /**
+     * The source to execute. It is preferable to use either {@link #source(byte[])}
+     * or {@link #source(QuerySourceBuilder)}.
+     */
+    public CountRequest source(String querySource) {
+        this.source = new BytesArray(querySource);
         return this;
     }
 
+    /**
+     * The source to execute.
+     */
+    public CountRequest source(byte[] querySource) {
+        return source(querySource, 0, querySource.length);
+    }
 
     /**
-     * The query to execute
+     * The source to execute.
      */
-    public CountRequest query(QueryBuilder<?> queryBuilder) {
-        this.searchSourceBuilder.query(queryBuilder);
+    public CountRequest source(byte[] querySource, int offset, int length) {
+        return source(new BytesArray(querySource, offset, length));
+    }
+
+    public CountRequest source(BytesReference querySource) {
+        this.source = querySource;
         return this;
     }
 
@@ -136,12 +207,15 @@ public class CountRequest extends BroadcastRequest<CountRequest> {
      * Upon reaching <code>terminateAfter</code> counts, the count request will early terminate
      */
     public CountRequest terminateAfter(int terminateAfterCount) {
-        this.searchSourceBuilder.terminateAfter(terminateAfterCount);
+        if (terminateAfterCount <= 0) {
+            throw new IllegalArgumentException("terminateAfter must be > 0");
+        }
+        this.terminateAfter = terminateAfterCount;
         return this;
     }
 
     public int terminateAfter() {
-        return this.searchSourceBuilder.terminateAfter();
+        return this.terminateAfter;
     }
 
     @Override
@@ -156,20 +230,31 @@ public class CountRequest extends BroadcastRequest<CountRequest> {
 
     @Override
     public String toString() {
-        return "count request indices:" + Arrays.toString(indices) +
-                ", types:" + Arrays.toString(types) +
-                ", routing: " + routing +
-                ", preference: " + preference +
-                ", source:" + searchSourceBuilder.toString();
+        String sSource = "_na_";
+        try {
+            sSource = XContentHelper.convertToJson(source, false);
+        } catch (Exception e) {
+            // ignore
+        }
+        return "[" + Arrays.toString(indices) + "]" + Arrays.toString(types) + ", source[" + sSource + "]";
     }
 
     public SearchRequest toSearchRequest() {
         SearchRequest searchRequest = new SearchRequest(indices());
-        searchRequest.source(searchSourceBuilder);
         searchRequest.indicesOptions(indicesOptions());
         searchRequest.types(types());
         searchRequest.routing(routing());
         searchRequest.preference(preference());
+        searchRequest.source(source());
+        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();
+        searchSourceBuilder.size(0);
+        if (minScore() != DEFAULT_MIN_SCORE) {
+            searchSourceBuilder.minScore(minScore());
+        }
+        if (terminateAfter() != DEFAULT_TERMINATE_AFTER) {
+            searchSourceBuilder.terminateAfter(terminateAfter());
+        }
+        searchRequest.extraSource(searchSourceBuilder);
         return searchRequest;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/count/CountRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/count/CountRequestBuilder.java
index 2f5d914..54c60e5 100644
--- a/core/src/main/java/org/elasticsearch/action/count/CountRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/count/CountRequestBuilder.java
@@ -19,12 +19,17 @@
 
 package org.elasticsearch.action.count;
 
+import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.search.SearchAction;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.support.DelegatingActionListener;
+import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.action.support.broadcast.BroadcastOperationRequestBuilder;
 import org.elasticsearch.client.ElasticsearchClient;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.index.query.QueryBuilder;
 
 /**
@@ -32,6 +37,8 @@ import org.elasticsearch.index.query.QueryBuilder;
  */
 public class CountRequestBuilder extends BroadcastOperationRequestBuilder<CountRequest, CountResponse, CountRequestBuilder> {
 
+    private QuerySourceBuilder sourceBuilder;
+
     public CountRequestBuilder(ElasticsearchClient client, CountAction action) {
         super(client, action, new CountRequest());
     }
@@ -82,9 +89,43 @@ public class CountRequestBuilder extends BroadcastOperationRequestBuilder<CountR
 
     /**
      * The query source to execute.
+     *
+     * @see org.elasticsearch.index.query.QueryBuilders
+     */
+    public CountRequestBuilder setQuery(QueryBuilder queryBuilder) {
+        sourceBuilder().setQuery(queryBuilder);
+        return this;
+    }
+
+    /**
+     * The query binary to execute
+     */
+    public CountRequestBuilder setQuery(BytesReference queryBinary) {
+        sourceBuilder().setQuery(queryBinary);
+        return this;
+    }
+
+    /**
+     * Constructs a new builder with a raw search query.
+     */
+    public CountRequestBuilder setQuery(XContentBuilder query) {
+        return setQuery(query.bytes());
+    }
+
+
+    /**
+     * The source to execute.
+     */
+    public CountRequestBuilder setSource(BytesReference source) {
+        request().source(source);
+        return this;
+    }
+
+    /**
+     * The query source to execute.
      */
-    public CountRequestBuilder setQuery(QueryBuilder<?> builder) {
-        request.query(builder);
+    public CountRequestBuilder setSource(byte[] querySource) {
+        request.source(querySource);
         return this;
     }
 
@@ -94,6 +135,21 @@ public class CountRequestBuilder extends BroadcastOperationRequestBuilder<CountR
     }
 
     @Override
+    protected CountRequest beforeExecute(CountRequest request) {
+        if (sourceBuilder != null) {
+            request.source(sourceBuilder);
+        }
+        return request;
+    }
+
+    private QuerySourceBuilder sourceBuilder() {
+        if (sourceBuilder == null) {
+            sourceBuilder = new QuerySourceBuilder();
+        }
+        return sourceBuilder;
+    }
+
+    @Override
     public void execute(ActionListener<CountResponse> listener) {
         CountRequest countRequest = beforeExecute(request);
         client.execute(SearchAction.INSTANCE, countRequest.toSearchRequest(), new DelegatingActionListener<SearchResponse, CountResponse>(listener) {
@@ -106,6 +162,16 @@ public class CountRequestBuilder extends BroadcastOperationRequestBuilder<CountR
 
     @Override
     public String toString() {
-        return request.toString();
+        if (sourceBuilder != null) {
+            return sourceBuilder.toString();
+        }
+        if (request.source() != null) {
+            try {
+                return XContentHelper.convertToJson(request.source().toBytesArray(), false, true);
+            } catch (Exception e) {
+                return "{ \"error\" : \"" + ExceptionsHelper.detailedMessage(e) + "\"}";
+            }
+        }
+        return new QuerySourceBuilder().toString();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/get/MultiGetRequest.java b/core/src/main/java/org/elasticsearch/action/get/MultiGetRequest.java
index 0d03bbb..030b0ee 100644
--- a/core/src/main/java/org/elasticsearch/action/get/MultiGetRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/get/MultiGetRequest.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.action.get;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.action.*;
 import org.elasticsearch.action.support.IndicesOptions;
@@ -27,6 +26,7 @@ import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.collect.Iterators;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
@@ -37,10 +37,7 @@ import org.elasticsearch.index.VersionType;
 import org.elasticsearch.search.fetch.source.FetchSourceContext;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Iterator;
-import java.util.List;
+import java.util.*;
 
 public class MultiGetRequest extends ActionRequest<MultiGetRequest> implements Iterable<MultiGetRequest.Item>, CompositeIndicesRequest, RealtimeRequest {
 
@@ -498,7 +495,7 @@ public class MultiGetRequest extends ActionRequest<MultiGetRequest> implements I
 
     @Override
     public Iterator<Item> iterator() {
-        return Iterators.unmodifiableIterator(items.iterator());
+        return Collections.unmodifiableCollection(items).iterator();
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/action/get/MultiGetResponse.java b/core/src/main/java/org/elasticsearch/action/get/MultiGetResponse.java
index 7abfb2b..32e10b8 100644
--- a/core/src/main/java/org/elasticsearch/action/get/MultiGetResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/get/MultiGetResponse.java
@@ -19,10 +19,8 @@
 
 package org.elasticsearch.action.get;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.ActionResponse;
-import org.elasticsearch.action.percolate.PercolateResponse;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
@@ -31,7 +29,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
 
 import java.io.IOException;
-import java.util.Collections;
+import java.util.Arrays;
 import java.util.Iterator;
 
 public class MultiGetResponse extends ActionResponse implements Iterable<MultiGetItemResponse>, ToXContent {
@@ -126,7 +124,7 @@ public class MultiGetResponse extends ActionResponse implements Iterable<MultiGe
 
     @Override
     public Iterator<MultiGetItemResponse> iterator() {
-        return Iterators.forArray(responses);
+        return Arrays.stream(responses).iterator();
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java b/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java
index c171ae9..ad7b9c1 100644
--- a/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java
@@ -49,14 +49,14 @@ import static org.elasticsearch.action.ValidateActions.addValidationError;
 /**
  * Index request to index a typed JSON document into a specific index and make it searchable. Best
  * created using {@link org.elasticsearch.client.Requests#indexRequest(String)}.
- * <p>
+ *
  * The index requires the {@link #index()}, {@link #type(String)}, {@link #id(String)} and
  * {@link #source(byte[])} to be set.
- * <p>
+ *
  * The source (content to index) can be set in its bytes form using ({@link #source(byte[])}),
  * its string form ({@link #source(String)}) or using a {@link org.elasticsearch.common.xcontent.XContentBuilder}
  * ({@link #source(org.elasticsearch.common.xcontent.XContentBuilder)}).
- * <p>
+ *
  * If the {@link #id(String)} is not set, it will be automatically generated.
  *
  * @see IndexResponse
@@ -114,7 +114,7 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
 
         public static OpType fromString(String sOpType) {
             String lowersOpType = sOpType.toLowerCase(Locale.ROOT);
-            switch(lowersOpType){
+            switch (lowersOpType) {
                 case "create":
                     return OpType.CREATE;
                 case "index":
@@ -216,6 +216,14 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
         if (source == null) {
             validationException = addValidationError("source is missing", validationException);
         }
+
+        if (opType() == OpType.CREATE) {
+            if (versionType != VersionType.INTERNAL || version != Versions.MATCH_DELETED) {
+                validationException = addValidationError("create operations do not support versioning. use index instead", validationException);
+                return validationException;
+            }
+        }
+
         if (!versionType.validateVersionForWrites(version)) {
             validationException = addValidationError("illegal version value [" + version + "] for version type [" + versionType.name() + "]", validationException);
         }
@@ -370,7 +378,7 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
 
     /**
      * Sets the document source to index.
-     * <p>
+     *
      * Note, its preferable to either set it using {@link #source(org.elasticsearch.common.xcontent.XContentBuilder)}
      * or using the {@link #source(byte[])}.
      */
@@ -480,6 +488,10 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
      */
     public IndexRequest opType(OpType opType) {
         this.opType = opType;
+        if (opType == OpType.CREATE) {
+            version(Versions.MATCH_DELETED);
+            versionType(VersionType.INTERNAL);
+        }
         return this;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java b/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java
index 3e98f1a..63b8237 100644
--- a/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java
+++ b/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java
@@ -54,7 +54,7 @@ import org.elasticsearch.transport.TransportService;
 
 /**
  * Performs the index operation.
- * <p>
+ *
  * Allows for the following settings:
  * <ul>
  * <li><b>autoCreateIndex</b>: When set to <tt>true</tt>, will automatically create an index if one does not exists.
@@ -167,6 +167,7 @@ public class TransportIndexAction extends TransportReplicationAction<IndexReques
         IndexShard indexShard = indexService.getShard(shardRequest.shardId.id());
 
         final WriteResult<IndexResponse> result = executeIndexRequestOnPrimary(null, request, indexShard);
+
         final IndexResponse response = result.response;
         final Translog.Location location = result.location;
         processAfter(request.refresh(), indexShard, location);
@@ -180,18 +181,12 @@ public class TransportIndexAction extends TransportReplicationAction<IndexReques
         SourceToParse sourceToParse = SourceToParse.source(SourceToParse.Origin.REPLICA, request.source()).index(shardId.getIndex()).type(request.type()).id(request.id())
                 .routing(request.routing()).parent(request.parent()).timestamp(request.timestamp()).ttl(request.ttl());
 
-        final Engine.IndexingOperation operation;
-        if (request.opType() == IndexRequest.OpType.INDEX) {
-            operation = indexShard.prepareIndex(sourceToParse, request.version(), request.versionType(), Engine.Operation.Origin.REPLICA);
-        } else {
-            assert request.opType() == IndexRequest.OpType.CREATE : request.opType();
-            operation = indexShard.prepareCreate(sourceToParse, request.version(), request.versionType(), Engine.Operation.Origin.REPLICA);
-        }
+        final Engine.Index operation = indexShard.prepareIndex(sourceToParse, request.version(), request.versionType(), Engine.Operation.Origin.REPLICA);
         Mapping update = operation.parsedDoc().dynamicMappingsUpdate();
         if (update != null) {
             throw new RetryOnReplicaException(shardId, "Mappings are not available on the replica yet, triggered update: " + update);
         }
-        operation.execute(indexShard);
+        indexShard.index(operation);
         processAfter(request.refresh(), indexShard, operation.getTranslogLocation());
     }
 
diff --git a/core/src/main/java/org/elasticsearch/action/percolate/MultiPercolateResponse.java b/core/src/main/java/org/elasticsearch/action/percolate/MultiPercolateResponse.java
index 4a32fc1..812d525 100644
--- a/core/src/main/java/org/elasticsearch/action/percolate/MultiPercolateResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/percolate/MultiPercolateResponse.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.action.percolate;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.common.Nullable;
@@ -30,6 +29,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
 
 import java.io.IOException;
+import java.util.Arrays;
 import java.util.Iterator;
 
 /**
@@ -52,7 +52,7 @@ public class MultiPercolateResponse extends ActionResponse implements Iterable<M
 
     @Override
     public Iterator<Item> iterator() {
-        return Iterators.forArray(items);
+        return Arrays.stream(items).iterator();
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/search/MultiSearchRequest.java b/core/src/main/java/org/elasticsearch/action/search/MultiSearchRequest.java
index a3236e9..d754d96 100644
--- a/core/src/main/java/org/elasticsearch/action/search/MultiSearchRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/search/MultiSearchRequest.java
@@ -24,14 +24,22 @@ import org.elasticsearch.action.ActionRequestValidationException;
 import org.elasticsearch.action.CompositeIndicesRequest;
 import org.elasticsearch.action.IndicesRequest;
 import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.xcontent.XContent;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
+import java.util.Map;
 
 import static org.elasticsearch.action.ValidateActions.addValidationError;
+import static org.elasticsearch.common.xcontent.support.XContentMapValues.*;
 
 /**
  * A multi search API request.
@@ -60,6 +68,107 @@ public class MultiSearchRequest extends ActionRequest<MultiSearchRequest> implem
         return this;
     }
 
+    public MultiSearchRequest add(byte[] data, int from, int length,
+            boolean isTemplateRequest, @Nullable String[] indices, @Nullable String[] types, @Nullable String searchType) throws Exception {
+        return add(new BytesArray(data, from, length), isTemplateRequest, indices, types, searchType, null, IndicesOptions.strictExpandOpenAndForbidClosed(), true);
+    }
+
+    public MultiSearchRequest add(BytesReference data, boolean isTemplateRequest, @Nullable String[] indices, @Nullable String[] types, @Nullable String searchType, IndicesOptions indicesOptions) throws Exception {
+        return add(data, isTemplateRequest, indices, types, searchType, null, indicesOptions, true);
+    }
+
+    public MultiSearchRequest add(BytesReference data, boolean isTemplateRequest, @Nullable String[] indices, @Nullable String[] types, @Nullable String searchType, @Nullable String routing, IndicesOptions indicesOptions, boolean allowExplicitIndex) throws Exception {
+        XContent xContent = XContentFactory.xContent(data);
+        int from = 0;
+        int length = data.length();
+        byte marker = xContent.streamSeparator();
+        while (true) {
+            int nextMarker = findNextMarker(marker, from, data, length);
+            if (nextMarker == -1) {
+                break;
+            }
+            // support first line with \n
+            if (nextMarker == 0) {
+                from = nextMarker + 1;
+                continue;
+            }
+
+            SearchRequest searchRequest = new SearchRequest();
+            if (indices != null) {
+                searchRequest.indices(indices);
+            }
+            if (indicesOptions != null) {
+                searchRequest.indicesOptions(indicesOptions);
+            }
+            if (types != null && types.length > 0) {
+                searchRequest.types(types);
+            }
+            if (routing != null) {
+                searchRequest.routing(routing);
+            }
+            searchRequest.searchType(searchType);
+
+            IndicesOptions defaultOptions = IndicesOptions.strictExpandOpenAndForbidClosed();
+
+
+            // now parse the action
+            if (nextMarker - from > 0) {
+                try (XContentParser parser = xContent.createParser(data.slice(from, nextMarker - from))) {
+                    Map<String, Object> source = parser.map();
+                    for (Map.Entry<String, Object> entry : source.entrySet()) {
+                        Object value = entry.getValue();
+                        if ("index".equals(entry.getKey()) || "indices".equals(entry.getKey())) {
+                            if (!allowExplicitIndex) {
+                                throw new IllegalArgumentException("explicit index in multi percolate is not allowed");
+                            }
+                            searchRequest.indices(nodeStringArrayValue(value));
+                        } else if ("type".equals(entry.getKey()) || "types".equals(entry.getKey())) {
+                            searchRequest.types(nodeStringArrayValue(value));
+                        } else if ("search_type".equals(entry.getKey()) || "searchType".equals(entry.getKey())) {
+                            searchRequest.searchType(nodeStringValue(value, null));
+                        } else if ("request_cache".equals(entry.getKey()) || "requestCache".equals(entry.getKey())) {
+                            searchRequest.requestCache(nodeBooleanValue(value));
+                        } else if ("preference".equals(entry.getKey())) {
+                            searchRequest.preference(nodeStringValue(value, null));
+                        } else if ("routing".equals(entry.getKey())) {
+                            searchRequest.routing(nodeStringValue(value, null));
+                        }
+                    }
+                    defaultOptions = IndicesOptions.fromMap(source, defaultOptions);
+                }
+            }
+            searchRequest.indicesOptions(defaultOptions);
+
+            // move pointers
+            from = nextMarker + 1;
+            // now for the body
+            nextMarker = findNextMarker(marker, from, data, length);
+            if (nextMarker == -1) {
+                break;
+            }
+            if (isTemplateRequest) {
+                searchRequest.templateSource(data.slice(from,  nextMarker - from));
+            } else {
+                searchRequest.source(data.slice(from, nextMarker - from));
+            }
+            // move pointers
+            from = nextMarker + 1;
+
+            add(searchRequest);
+        }
+
+        return this;
+    }
+
+    private int findNextMarker(byte marker, int from, BytesReference data, int length) {
+        for (int i = from; i < length; i++) {
+            if (data.get(i) == marker) {
+                return i;
+            }
+        }
+        return -1;
+    }
+
     public List<SearchRequest> requests() {
         return this.requests;
     }
diff --git a/core/src/main/java/org/elasticsearch/action/search/MultiSearchResponse.java b/core/src/main/java/org/elasticsearch/action/search/MultiSearchResponse.java
index 8074565..0a9d619 100644
--- a/core/src/main/java/org/elasticsearch/action/search/MultiSearchResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/search/MultiSearchResponse.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.action.search;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.common.Nullable;
@@ -32,7 +31,7 @@ import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.common.xcontent.XContentFactory;
 
 import java.io.IOException;
-import java.util.Collections;
+import java.util.Arrays;
 import java.util.Iterator;
 
 /**
@@ -122,7 +121,7 @@ public class MultiSearchResponse extends ActionResponse implements Iterable<Mult
 
     @Override
     public Iterator<Item> iterator() {
-        return Iterators.forArray(items);
+        return Arrays.stream(items).iterator();
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/search/SearchRequest.java b/core/src/main/java/org/elasticsearch/action/search/SearchRequest.java
index 60f565b..9348185 100644
--- a/core/src/main/java/org/elasticsearch/action/search/SearchRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/search/SearchRequest.java
@@ -19,21 +19,31 @@
 
 package org.elasticsearch.action.search;
 
+import org.elasticsearch.ElasticsearchGenerationException;
 import org.elasticsearch.action.ActionRequest;
 import org.elasticsearch.action.ActionRequestValidationException;
 import org.elasticsearch.action.IndicesRequest;
 import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.client.Requests;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.Template;
+import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 import org.elasticsearch.search.Scroll;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
 
 import java.io.IOException;
+import java.util.Map;
 
 import static org.elasticsearch.search.Scroll.readScroll;
 
@@ -43,7 +53,9 @@ import static org.elasticsearch.search.Scroll.readScroll;
  * <p>
  * Note, the search {@link #source(org.elasticsearch.search.builder.SearchSourceBuilder)}
  * is required. The search source is the different search options, including aggregations and such.
- * </p>
+ * <p>
+ * There is an option to specify an addition search source using the {@link #extraSource(org.elasticsearch.search.builder.SearchSourceBuilder)}.
+ *
  * @see org.elasticsearch.client.Requests#searchRequest(String...)
  * @see org.elasticsearch.client.Client#search(SearchRequest)
  * @see SearchResponse
@@ -59,8 +71,12 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
     @Nullable
     private String preference;
 
-    private SearchSourceBuilder source;
+    private BytesReference templateSource;
+    private Template template;
 
+    private BytesReference source;
+
+    private BytesReference extraSource;
     private Boolean requestCache;
 
     private Scroll scroll;
@@ -71,8 +87,6 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
 
     private IndicesOptions indicesOptions = DEFAULT_INDICES_OPTIONS;
 
-    private Template template;
-
     public SearchRequest() {
     }
 
@@ -86,8 +100,10 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
         this.indices = searchRequest.indices;
         this.routing = searchRequest.routing;
         this.preference = searchRequest.preference;
+        this.templateSource = searchRequest.templateSource;
         this.template = searchRequest.template;
         this.source = searchRequest.source;
+        this.extraSource = searchRequest.extraSource;
         this.requestCache = searchRequest.requestCache;
         this.scroll = searchRequest.scroll;
         this.types = searchRequest.types;
@@ -113,9 +129,9 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
     /**
      * Constructs a new search request against the provided indices with the given search source.
      */
-    public SearchRequest(String[] indices, SearchSourceBuilder source) {
+    public SearchRequest(String[] indices, byte[] source) {
         indices(indices);
-        this.source = source;
+        this.source = new BytesArray(source);
     }
 
     @Override
@@ -231,17 +247,60 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
      * The source of the search request.
      */
     public SearchRequest source(SearchSourceBuilder sourceBuilder) {
-        this.source = sourceBuilder;
+        this.source = sourceBuilder.buildAsBytes(Requests.CONTENT_TYPE);
+        return this;
+    }
+
+    /**
+     * The search source to execute.
+     */
+    public SearchRequest source(BytesReference source) {
+        this.source = source;
         return this;
     }
 
+
     /**
      * The search source to execute.
      */
-    public SearchSourceBuilder source() {
+    public BytesReference source() {
         return source;
     }
 
+    /**
+     * The search source template to execute.
+     */
+    public BytesReference templateSource() {
+        return templateSource;
+    }
+
+    /**
+     * Allows to provide additional source that will be used as well.
+     */
+    public SearchRequest extraSource(SearchSourceBuilder sourceBuilder) {
+        if (sourceBuilder == null) {
+            extraSource = null;
+            return this;
+        }
+        this.extraSource = sourceBuilder.buildAsBytes(Requests.CONTENT_TYPE);
+        return this;
+    }
+
+    /**
+     * Allows to provide template as source.
+     */
+    public SearchRequest templateSource(BytesReference template) {
+        this.templateSource = template;
+        return this;
+    }
+
+    /**
+     * The template of the search request.
+     */
+    public SearchRequest templateSource(String template) {
+        this.templateSource = new BytesArray(template);
+        return this;
+    }
 
     /**
      * The stored template
@@ -258,6 +317,88 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
     }
 
     /**
+     * The name of the stored template
+     * 
+     * @deprecated use {@link #template(Template)} instead.
+     */
+    @Deprecated
+    public void templateName(String templateName) {
+        updateOrCreateScript(templateName, null, null, null);
+    }
+
+    /**
+     * The type of the stored template
+     * 
+     * @deprecated use {@link #template(Template)} instead.
+     */
+    @Deprecated
+    public void templateType(ScriptService.ScriptType templateType) {
+        updateOrCreateScript(null, templateType, null, null);
+    }
+
+    /**
+     * Template parameters used for rendering
+     * 
+     * @deprecated use {@link #template(Template)} instead.
+     */
+    @Deprecated
+    public void templateParams(Map<String, Object> params) {
+        updateOrCreateScript(null, null, null, params);
+    }
+
+    /**
+     * The name of the stored template
+     * 
+     * @deprecated use {@link #template()} instead.
+     */
+    @Deprecated
+    public String templateName() {
+        return template == null ? null : template.getScript();
+    }
+
+    /**
+     * The name of the stored template
+     * 
+     * @deprecated use {@link #template()} instead.
+     */
+    @Deprecated
+    public ScriptService.ScriptType templateType() {
+        return template == null ? null : template.getType();
+    }
+
+    /**
+     * Template parameters used for rendering
+     * 
+     * @deprecated use {@link #template()} instead.
+     */
+    @Deprecated
+    public Map<String, Object> templateParams() {
+        return template == null ? null : template.getParams();
+    }
+
+    private void updateOrCreateScript(String templateContent, ScriptType type, String lang, Map<String, Object> params) {
+        Template template = template();
+        if (template == null) {
+            template = new Template(templateContent == null ? "" : templateContent, type == null ? ScriptType.INLINE : type, lang, null,
+                    params);
+        } else {
+            String newTemplateContent = templateContent == null ? template.getScript() : templateContent;
+            ScriptType newTemplateType = type == null ? template.getType() : type;
+            String newTemplateLang = lang == null ? template.getLang() : lang;
+            Map<String, Object> newTemplateParams = params == null ? template.getParams() : params;
+            template = new Template(newTemplateContent, newTemplateType, MustacheScriptEngineService.NAME, null, newTemplateParams);
+        }
+        template(template);
+    }
+
+    /**
+     * Additional search source to execute.
+     */
+    public BytesReference extraSource() {
+        return this.extraSource;
+    }
+
+    /**
      * The tye of search to execute.
      */
     public SearchType searchType() {
@@ -331,15 +472,18 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
         if (in.readBoolean()) {
             scroll = readScroll(in);
         }
-        if (in.readBoolean()) {
-            source = SearchSourceBuilder.readSearchSourceFrom(in);
-        }
+
+        source = in.readBytesReference();
+        extraSource = in.readBytesReference();
 
         types = in.readStringArray();
         indicesOptions = IndicesOptions.readIndicesOptions(in);
 
+        templateSource = in.readBytesReference();
+        if (in.readBoolean()) {
+            template = Template.readTemplate(in);
+        }
         requestCache = in.readOptionalBoolean();
-        template = in.readOptionalStreamable(new Template());
     }
 
     @Override
@@ -361,15 +505,18 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
             out.writeBoolean(true);
             scroll.writeTo(out);
         }
-        if (source == null) {
-            out.writeBoolean(false);
-        } else {
-            out.writeBoolean(true);
-            source.writeTo(out);
-        }
+        out.writeBytesReference(source);
+        out.writeBytesReference(extraSource);
         out.writeStringArray(types);
         indicesOptions.writeIndicesOptions(out);
+
+        out.writeBytesReference(templateSource);
+        boolean hasTemplate = template != null;
+        out.writeBoolean(hasTemplate);
+        if (hasTemplate) {
+            template.writeTo(out);
+        }
+
         out.writeOptionalBoolean(requestCache);
-        out.writeOptionalStreamable(template);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
index 2e3084c..a570080 100644
--- a/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
@@ -19,13 +19,18 @@
 
 package org.elasticsearch.action.search;
 
+import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.ActionRequestBuilder;
 import org.elasticsearch.action.support.IndicesOptions;
 import org.elasticsearch.client.ElasticsearchClient;
 import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.script.Script;
+import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.script.Template;
 import org.elasticsearch.search.Scroll;
 import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
@@ -37,14 +42,15 @@ import org.elasticsearch.search.sort.SortBuilder;
 import org.elasticsearch.search.sort.SortOrder;
 import org.elasticsearch.search.suggest.SuggestBuilder;
 
-import java.util.Arrays;
-import java.util.List;
+import java.util.Map;
 
 /**
  * A search action request builder.
  */
 public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, SearchResponse, SearchRequestBuilder> {
 
+    private SearchSourceBuilder sourceBuilder;
+
     public SearchRequestBuilder(ElasticsearchClient client, SearchAction action) {
         super(client, action, new SearchRequest());
     }
@@ -117,6 +123,14 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
     }
 
     /**
+     * An optional timeout to control how long search is allowed to take.
+     */
+    public SearchRequestBuilder setTimeout(String timeout) {
+        sourceBuilder().timeout(timeout);
+        return this;
+    }
+
+    /**
      * An optional document count, upon collecting which the search
      * query will early terminate
      */
@@ -166,16 +180,118 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
      *
      * @see org.elasticsearch.index.query.QueryBuilders
      */
-    public SearchRequestBuilder setQuery(QueryBuilder<?> queryBuilder) {
+    public SearchRequestBuilder setQuery(QueryBuilder queryBuilder) {
         sourceBuilder().query(queryBuilder);
         return this;
     }
 
     /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchRequestBuilder setQuery(String query) {
+        sourceBuilder().query(query);
+        return this;
+    }
+
+    /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchRequestBuilder setQuery(BytesReference queryBinary) {
+        sourceBuilder().query(queryBinary);
+        return this;
+    }
+
+    /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchRequestBuilder setQuery(byte[] queryBinary) {
+        sourceBuilder().query(queryBinary);
+        return this;
+    }
+
+    /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchRequestBuilder setQuery(byte[] queryBinary, int queryBinaryOffset, int queryBinaryLength) {
+        sourceBuilder().query(queryBinary, queryBinaryOffset, queryBinaryLength);
+        return this;
+    }
+
+    /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchRequestBuilder setQuery(XContentBuilder query) {
+        sourceBuilder().query(query);
+        return this;
+    }
+
+    /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchRequestBuilder setQuery(Map query) {
+        sourceBuilder().query(query);
+        return this;
+    }
+
+    /**
      * Sets a filter that will be executed after the query has been executed and only has affect on the search hits
      * (not aggregations). This filter is always executed as last filtering mechanism.
      */
-    public SearchRequestBuilder setPostFilter(QueryBuilder<?> postFilter) {
+    public SearchRequestBuilder setPostFilter(QueryBuilder postFilter) {
+        sourceBuilder().postFilter(postFilter);
+        return this;
+    }
+
+    /**
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
+     */
+    public SearchRequestBuilder setPostFilter(String postFilter) {
+        sourceBuilder().postFilter(postFilter);
+        return this;
+    }
+
+    /**
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
+     */
+    public SearchRequestBuilder setPostFilter(BytesReference postFilter) {
+        sourceBuilder().postFilter(postFilter);
+        return this;
+    }
+
+    /**
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
+     */
+    public SearchRequestBuilder setPostFilter(byte[] postFilter) {
+        sourceBuilder().postFilter(postFilter);
+        return this;
+    }
+
+    /**
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
+     */
+    public SearchRequestBuilder setPostFilter(byte[] postFilter, int postFilterOffset, int postFilterLength) {
+        sourceBuilder().postFilter(postFilter, postFilterOffset, postFilterLength);
+        return this;
+    }
+
+    /**
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
+     */
+    public SearchRequestBuilder setPostFilter(XContentBuilder postFilter) {
+        sourceBuilder().postFilter(postFilter);
+        return this;
+    }
+
+    /**
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
+     */
+    public SearchRequestBuilder setPostFilter(Map postFilter) {
         sourceBuilder().postFilter(postFilter);
         return this;
     }
@@ -237,14 +353,6 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
      * The stats groups this request will be aggregated under.
      */
     public SearchRequestBuilder setStats(String... statsGroups) {
-        sourceBuilder().stats(Arrays.asList(statsGroups));
-        return this;
-    }
-
-    /**
-     * The stats groups this request will be aggregated under.
-     */
-    public SearchRequestBuilder setStats(List<String> statsGroups) {
         sourceBuilder().stats(statsGroups);
         return this;
     }
@@ -357,7 +465,7 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
      * the source of the document will be returned.
      */
     public SearchRequestBuilder addFields(String... fields) {
-        sourceBuilder().fields(Arrays.asList(fields));
+        sourceBuilder().fields(fields);
         return this;
     }
 
@@ -369,23 +477,267 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
         return this;
     }
 
-    public SearchRequestBuilder highlighter(HighlightBuilder highlightBuilder) {
-        sourceBuilder().highlighter(highlightBuilder);
+    /**
+     * Sets a raw (xcontent) binary representation of addAggregation to use.
+     */
+    public SearchRequestBuilder setAggregations(BytesReference aggregations) {
+        sourceBuilder().aggregations(aggregations);
+        return this;
+    }
+
+    /**
+     * Sets a raw (xcontent) binary representation of addAggregation to use.
+     */
+    public SearchRequestBuilder setAggregations(byte[] aggregations) {
+        sourceBuilder().aggregations(aggregations);
+        return this;
+    }
+
+    /**
+     * Sets a raw (xcontent) binary representation of addAggregation to use.
+     */
+    public SearchRequestBuilder setAggregations(byte[] aggregations, int aggregationsOffset, int aggregationsLength) {
+        sourceBuilder().aggregations(aggregations, aggregationsOffset, aggregationsLength);
+        return this;
+    }
+
+    /**
+     * Sets a raw (xcontent) binary representation of addAggregation to use.
+     */
+    public SearchRequestBuilder setAggregations(XContentBuilder aggregations) {
+        sourceBuilder().aggregations(aggregations);
+        return this;
+    }
+
+    /**
+     * Sets a raw (xcontent) binary representation of addAggregation to use.
+     */
+    public SearchRequestBuilder setAggregations(Map aggregations) {
+        sourceBuilder().aggregations(aggregations);
+        return this;
+    }
+
+    /**
+     * Adds a field to be highlighted with default fragment size of 100 characters, and
+     * default number of fragments of 5.
+     *
+     * @param name The field to highlight
+     */
+    public SearchRequestBuilder addHighlightedField(String name) {
+        highlightBuilder().field(name);
+        return this;
+    }
+
+
+    /**
+     * Adds a field to be highlighted with a provided fragment size (in characters), and
+     * default number of fragments of 5.
+     *
+     * @param name         The field to highlight
+     * @param fragmentSize The size of a fragment in characters
+     */
+    public SearchRequestBuilder addHighlightedField(String name, int fragmentSize) {
+        highlightBuilder().field(name, fragmentSize);
+        return this;
+    }
+
+    /**
+     * Adds a field to be highlighted with a provided fragment size (in characters), and
+     * a provided (maximum) number of fragments.
+     *
+     * @param name              The field to highlight
+     * @param fragmentSize      The size of a fragment in characters
+     * @param numberOfFragments The (maximum) number of fragments
+     */
+    public SearchRequestBuilder addHighlightedField(String name, int fragmentSize, int numberOfFragments) {
+        highlightBuilder().field(name, fragmentSize, numberOfFragments);
+        return this;
+    }
+
+    /**
+     * Adds a field to be highlighted with a provided fragment size (in characters),
+     * a provided (maximum) number of fragments and an offset for the highlight.
+     *
+     * @param name              The field to highlight
+     * @param fragmentSize      The size of a fragment in characters
+     * @param numberOfFragments The (maximum) number of fragments
+     */
+    public SearchRequestBuilder addHighlightedField(String name, int fragmentSize, int numberOfFragments,
+                                                    int fragmentOffset) {
+        highlightBuilder().field(name, fragmentSize, numberOfFragments, fragmentOffset);
+        return this;
+    }
+
+    /**
+     * Adds a highlighted field.
+     */
+    public SearchRequestBuilder addHighlightedField(HighlightBuilder.Field field) {
+        highlightBuilder().field(field);
+        return this;
+    }
+
+    /**
+     * Set a tag scheme that encapsulates a built in pre and post tags. The allows schemes
+     * are <tt>styled</tt> and <tt>default</tt>.
+     *
+     * @param schemaName The tag scheme name
+     */
+    public SearchRequestBuilder setHighlighterTagsSchema(String schemaName) {
+        highlightBuilder().tagsSchema(schemaName);
+        return this;
+    }
+
+    public SearchRequestBuilder setHighlighterFragmentSize(Integer fragmentSize) {
+        highlightBuilder().fragmentSize(fragmentSize);
+        return this;
+    }
+
+    public SearchRequestBuilder setHighlighterNumOfFragments(Integer numOfFragments) {
+        highlightBuilder().numOfFragments(numOfFragments);
+        return this;
+    }
+
+    public SearchRequestBuilder setHighlighterFilter(Boolean highlightFilter) {
+        highlightBuilder().highlightFilter(highlightFilter);
+        return this;
+    }
+
+    /**
+     * The encoder to set for highlighting
+     */
+    public SearchRequestBuilder setHighlighterEncoder(String encoder) {
+        highlightBuilder().encoder(encoder);
+        return this;
+    }
+
+    /**
+     * Explicitly set the pre tags that will be used for highlighting.
+     */
+    public SearchRequestBuilder setHighlighterPreTags(String... preTags) {
+        highlightBuilder().preTags(preTags);
+        return this;
+    }
+
+    /**
+     * Explicitly set the post tags that will be used for highlighting.
+     */
+    public SearchRequestBuilder setHighlighterPostTags(String... postTags) {
+        highlightBuilder().postTags(postTags);
+        return this;
+    }
+
+    /**
+     * The order of fragments per field. By default, ordered by the order in the
+     * highlighted text. Can be <tt>score</tt>, which then it will be ordered
+     * by score of the fragments.
+     */
+    public SearchRequestBuilder setHighlighterOrder(String order) {
+        highlightBuilder().order(order);
+        return this;
+    }
+
+    public SearchRequestBuilder setHighlighterRequireFieldMatch(boolean requireFieldMatch) {
+        highlightBuilder().requireFieldMatch(requireFieldMatch);
+        return this;
+    }
+
+    public SearchRequestBuilder setHighlighterBoundaryMaxScan(Integer boundaryMaxScan) {
+        highlightBuilder().boundaryMaxScan(boundaryMaxScan);
+        return this;
+    }
+
+    public SearchRequestBuilder setHighlighterBoundaryChars(char[] boundaryChars) {
+        highlightBuilder().boundaryChars(boundaryChars);
+        return this;
+    }
+
+    /**
+     * The highlighter type to use.
+     */
+    public SearchRequestBuilder setHighlighterType(String type) {
+        highlightBuilder().highlighterType(type);
+        return this;
+    }
+
+    public SearchRequestBuilder setHighlighterFragmenter(String fragmenter) {
+        highlightBuilder().fragmenter(fragmenter);
+        return this;
+    }
+
+    /**
+     * Sets a query to be used for highlighting all fields instead of the search query.
+     */
+    public SearchRequestBuilder setHighlighterQuery(QueryBuilder highlightQuery) {
+        highlightBuilder().highlightQuery(highlightQuery);
         return this;
     }
 
     /**
-     * Delegates to
-     * {@link org.elasticsearch.search.suggest.SuggestBuilder#addSuggestion(org.elasticsearch.search.suggest.SuggestBuilder.SuggestionBuilder)}
-     * .
+     * Sets the size of the fragment to return from the beginning of the field if there are no matches to
+     * highlight and the field doesn't also define noMatchSize.
+     *
+     * @param noMatchSize integer to set or null to leave out of request.  default is null.
+     * @return this builder for chaining
      */
-    public SearchRequestBuilder suggest(SuggestBuilder suggestBuilder) {
-        sourceBuilder().suggest(suggestBuilder);
+    public SearchRequestBuilder setHighlighterNoMatchSize(Integer noMatchSize) {
+        highlightBuilder().noMatchSize(noMatchSize);
         return this;
     }
 
-    public SearchRequestBuilder innerHits(InnerHitsBuilder innerHitsBuilder) {
-        sourceBuilder().innerHits(innerHitsBuilder);
+    /**
+     * Sets the maximum number of phrases the fvh will consider if the field doesn't also define phraseLimit.
+     */
+    public SearchRequestBuilder setHighlighterPhraseLimit(Integer phraseLimit) {
+        highlightBuilder().phraseLimit(phraseLimit);
+        return this;
+    }
+
+    public SearchRequestBuilder setHighlighterOptions(Map<String, Object> options) {
+        highlightBuilder().options(options);
+        return this;
+    }
+
+    /**
+     * Forces to highlight fields based on the source even if fields are stored separately.
+     */
+    public SearchRequestBuilder setHighlighterForceSource(Boolean forceSource) {
+        highlightBuilder().forceSource(forceSource);
+        return this;
+    }
+
+    /**
+     * Send the fields to be highlighted using a syntax that is specific about the order in which they should be highlighted.
+     *
+     * @return this for chaining
+     */
+    public SearchRequestBuilder setHighlighterExplicitFieldOrder(boolean explicitFieldOrder) {
+        highlightBuilder().useExplicitFieldOrder(explicitFieldOrder);
+        return this;
+    }
+
+    public SearchRequestBuilder addParentChildInnerHits(String name, String type,  InnerHitsBuilder.InnerHit innerHit) {
+        innerHitsBuilder().addParentChildInnerHits(name, type, innerHit);
+        return this;
+    }
+
+    public SearchRequestBuilder addNestedInnerHits(String name, String path,  InnerHitsBuilder.InnerHit innerHit) {
+        innerHitsBuilder().addNestedInnerHits(name, path, innerHit);
+        return this;
+    }
+
+    /**
+     * Delegates to {@link org.elasticsearch.search.suggest.SuggestBuilder#setText(String)}.
+     */
+    public SearchRequestBuilder setSuggestText(String globalText) {
+        suggestBuilder().setText(globalText);
+        return this;
+    }
+
+    /**
+     * Delegates to {@link org.elasticsearch.search.suggest.SuggestBuilder#addSuggestion(org.elasticsearch.search.suggest.SuggestBuilder.SuggestionBuilder)}.
+     */
+    public SearchRequestBuilder addSuggestion(SuggestBuilder.SuggestionBuilder<?> suggestion) {
+        suggestBuilder().addSuggestion(suggestion);
         return this;
     }
 
@@ -448,7 +800,20 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
     }
 
     /**
-     * Sets the source of the request as a SearchSourceBuilder.
+     * Sets the rescore window for all rescorers that don't specify a window when added.
+     *
+     * @param window rescore window
+     * @return this for chaining
+     */
+    public SearchRequestBuilder setRescoreWindow(int window) {
+        sourceBuilder().defaultRescoreWindowSize(window);
+        return this;
+    }
+
+    /**
+     * Sets the source of the request as a SearchSourceBuilder. Note, settings anything other
+     * than the search type will cause this source to be overridden, consider using
+     * {@link #setExtraSource(SearchSourceBuilder)} instead.
      */
     public SearchRequestBuilder setSource(SearchSourceBuilder source) {
         request.source(source);
@@ -456,6 +821,26 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
     }
 
     /**
+     * Sets the source of the request as a json string. Note, settings anything other
+     * than the search type will cause this source to be overridden, consider using
+     * {@link #setExtraSource(SearchSourceBuilder)} instead.
+     */
+    public SearchRequestBuilder setSource(BytesReference source) {
+        request.source(source);
+        return this;
+    }
+
+    /**
+     * Sets the an addtional source of the request as a SearchSourceBuilder. All values and
+     * settings set on the extra source will override the corresponding settings on the specified
+     * source.
+     */
+    public SearchRequestBuilder setExtraSource(SearchSourceBuilder source) {
+        request.extraSource(source);
+        return this;
+    }
+
+    /**
      * template stuff
      */
     public SearchRequestBuilder setTemplate(Template template) {
@@ -463,6 +848,16 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
         return this;
     }
 
+    public SearchRequestBuilder setTemplateSource(String source) {
+        request.templateSource(source);
+        return this;
+    }
+
+    public SearchRequestBuilder setTemplateSource(BytesReference source) {
+        request.templateSource(source);
+        return this;
+    }
+
     /**
      * Sets if this request should use the request cache or not, assuming that it can (for
      * example, if "now" is used, it will never be cached). By default (not set, or null,
@@ -473,18 +868,70 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
         return this;
     }
 
+    /**
+     * Sets the source builder to be used with this request. Note, any operations done
+     * on this require builder before are discarded as this internal builder replaces
+     * what has been built up until this point.
+     */
+    public SearchRequestBuilder internalBuilder(SearchSourceBuilder sourceBuilder) {
+        this.sourceBuilder = sourceBuilder;
+        return this;
+    }
+
+    /**
+     * Returns the internal search source builder used to construct the request.
+     */
+    public SearchSourceBuilder internalBuilder() {
+        return sourceBuilder();
+    }
+
     @Override
     public String toString() {
+        if (sourceBuilder != null) {
+            return sourceBuilder.toString();
+        }
         if (request.source() != null) {
-            return request.source().toString();
+            try {
+                return XContentHelper.convertToJson(request.source().toBytesArray(), false, true);
+            } catch (Exception e) {
+                return "{ \"error\" : \"" + ExceptionsHelper.detailedMessage(e) + "\"}";
+            }
         }
         return new SearchSourceBuilder().toString();
     }
 
+    @Override
+    public SearchRequest request() {
+        if (sourceBuilder != null) {
+            request.source(sourceBuilder());
+        }
+        return request;
+    }
+
+    @Override
+    protected SearchRequest beforeExecute(SearchRequest request) {
+        if (sourceBuilder != null) {
+            request.source(sourceBuilder());
+        }
+        return request;
+    }
+
     private SearchSourceBuilder sourceBuilder() {
-        if (request.source() == null) {
-            request.source(new SearchSourceBuilder());
+        if (sourceBuilder == null) {
+            sourceBuilder = new SearchSourceBuilder();
         }
-        return request.source();
+        return sourceBuilder;
+    }
+
+    private HighlightBuilder highlightBuilder() {
+        return sourceBuilder().highlighter();
+    }
+
+    private InnerHitsBuilder innerHitsBuilder() {
+        return sourceBuilder().innerHitsBuilder();
+    }
+
+    private SuggestBuilder suggestBuilder() {
+        return sourceBuilder().suggest();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesAction.java b/core/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesAction.java
index 118e112..0597c26 100644
--- a/core/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesAction.java
+++ b/core/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesAction.java
@@ -30,6 +30,7 @@ import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
+import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.*;
@@ -95,17 +96,22 @@ public abstract class TransportNodesAction<NodesRequest extends BaseNodesRequest
 
         private final NodesRequest request;
         private final String[] nodesIds;
+        private final DiscoveryNode[] nodes;
         private final ActionListener<NodesResponse> listener;
-        private final ClusterState clusterState;
         private final AtomicReferenceArray<Object> responses;
         private final AtomicInteger counter = new AtomicInteger();
 
         private AsyncAction(NodesRequest request, ActionListener<NodesResponse> listener) {
             this.request = request;
             this.listener = listener;
-            clusterState = clusterService.state();
+            ClusterState clusterState = clusterService.state();
             String[] nodesIds = resolveNodes(request, clusterState);
             this.nodesIds = filterNodeIds(clusterState.nodes(), nodesIds);
+            ImmutableOpenMap<String, DiscoveryNode> nodes = clusterState.nodes().nodes();
+            this.nodes = new DiscoveryNode[nodesIds.length];
+            for (int i = 0; i < nodesIds.length; i++) {
+                this.nodes[i] = nodes.get(nodesIds[i]);
+            }
             this.responses = new AtomicReferenceArray<>(this.nodesIds.length);
         }
 
@@ -128,7 +134,7 @@ public abstract class TransportNodesAction<NodesRequest extends BaseNodesRequest
             for (int i = 0; i < nodesIds.length; i++) {
                 final String nodeId = nodesIds[i];
                 final int idx = i;
-                final DiscoveryNode node = clusterState.nodes().nodes().get(nodeId);
+                final DiscoveryNode node = nodes[i];
                 try {
                     if (node == null) {
                         onFailure(idx, nodeId, new NoSuchNodeException(nodeId));
diff --git a/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java b/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
index d7fca31..f3db2b6 100644
--- a/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
+++ b/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
@@ -56,7 +56,6 @@ import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.AbstractRunnable;
 import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
 import org.elasticsearch.index.IndexService;
-import org.elasticsearch.index.engine.DocumentAlreadyExistsException;
 import org.elasticsearch.index.engine.Engine;
 import org.elasticsearch.index.engine.VersionConflictEngineException;
 import org.elasticsearch.index.mapper.Mapping;
@@ -188,9 +187,6 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
         if (cause instanceof VersionConflictEngineException) {
             return true;
         }
-        if (cause instanceof DocumentAlreadyExistsException) {
-            return true;
-        }
         return false;
     }
 
@@ -1036,22 +1032,17 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
 
     /** Utility method to create either an index or a create operation depending
      *  on the {@link OpType} of the request. */
-    private final Engine.IndexingOperation prepareIndexOperationOnPrimary(BulkShardRequest shardRequest, IndexRequest request, IndexShard indexShard) {
+    private final Engine.Index prepareIndexOperationOnPrimary(BulkShardRequest shardRequest, IndexRequest request, IndexShard indexShard) {
         SourceToParse sourceToParse = SourceToParse.source(SourceToParse.Origin.PRIMARY, request.source()).index(request.index()).type(request.type()).id(request.id())
                 .routing(request.routing()).parent(request.parent()).timestamp(request.timestamp()).ttl(request.ttl());
-        if (request.opType() == IndexRequest.OpType.INDEX) {
             return indexShard.prepareIndex(sourceToParse, request.version(), request.versionType(), Engine.Operation.Origin.PRIMARY);
-        } else {
-            assert request.opType() == IndexRequest.OpType.CREATE : request.opType();
-            return indexShard.prepareCreate(sourceToParse,
-                    request.version(), request.versionType(), Engine.Operation.Origin.PRIMARY);
-        }
+
     }
 
     /** Execute the given {@link IndexRequest} on a primary shard, throwing a
      *  {@link RetryOnPrimaryException} if the operation needs to be re-tried. */
     protected final WriteResult<IndexResponse> executeIndexRequestOnPrimary(BulkShardRequest shardRequest, IndexRequest request, IndexShard indexShard) throws Throwable {
-        Engine.IndexingOperation operation = prepareIndexOperationOnPrimary(shardRequest, request, indexShard);
+        Engine.Index operation = prepareIndexOperationOnPrimary(shardRequest, request, indexShard);
         Mapping update = operation.parsedDoc().dynamicMappingsUpdate();
         final ShardId shardId = indexShard.shardId();
         if (update != null) {
@@ -1064,7 +1055,7 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
                         "Dynamics mappings are not available on the node that holds the primary yet");
             }
         }
-        final boolean created = operation.execute(indexShard);
+        final boolean created = indexShard.index(operation);
 
         // update the version on request so it will happen on the replicas
         final long version = operation.version();
diff --git a/core/src/main/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationAction.java b/core/src/main/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationAction.java
index 2e815da..5f4f942 100644
--- a/core/src/main/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationAction.java
+++ b/core/src/main/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationAction.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.support.single.instance;
 
+import org.elasticsearch.ElasticsearchTimeoutException;
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.action.UnavailableShardsException;
@@ -35,6 +36,7 @@ import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.cluster.routing.ShardIterator;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.logging.support.LoggerMessageFormat;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.index.shard.ShardId;
@@ -42,6 +44,7 @@ import org.elasticsearch.node.NodeClosedException;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.*;
 
+import java.util.concurrent.TimeoutException;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.function.Supplier;
 
@@ -111,9 +114,8 @@ public abstract class TransportInstanceSingleOperationAction<Request extends Ins
         private volatile ClusterStateObserver observer;
         private ShardIterator shardIt;
         private DiscoveryNodes nodes;
-        private final AtomicBoolean operationStarted = new AtomicBoolean();
 
-        private AsyncSingleAction(Request request, ActionListener<Response> listener) {
+        AsyncSingleAction(Request request, ActionListener<Response> listener) {
             this.request = request;
             this.listener = listener;
         }
@@ -123,14 +125,14 @@ public abstract class TransportInstanceSingleOperationAction<Request extends Ins
             doStart();
         }
 
-        protected boolean doStart() {
+        protected void doStart() {
             nodes = observer.observedState().nodes();
             try {
                 ClusterBlockException blockException = checkGlobalBlock(observer.observedState());
                 if (blockException != null) {
                     if (blockException.retryable()) {
                         retry(blockException);
-                        return false;
+                        return;
                     } else {
                         throw blockException;
                     }
@@ -138,13 +140,14 @@ public abstract class TransportInstanceSingleOperationAction<Request extends Ins
                 request.concreteIndex(indexNameExpressionResolver.concreteSingleIndex(observer.observedState(), request));
                 // check if we need to execute, and if not, return
                 if (!resolveRequest(observer.observedState(), request, listener)) {
-                    return true;
+                    listener.onFailure(new IllegalStateException(LoggerMessageFormat.format("{} request {} could not be resolved", new ShardId(request.index, request.shardId), actionName)));
+                    return;
                 }
                 blockException = checkRequestBlock(observer.observedState(), request);
                 if (blockException != null) {
                     if (blockException.retryable()) {
                         retry(blockException);
-                        return false;
+                        return;
                     } else {
                         throw blockException;
                     }
@@ -152,13 +155,13 @@ public abstract class TransportInstanceSingleOperationAction<Request extends Ins
                 shardIt = shards(observer.observedState(), request);
             } catch (Throwable e) {
                 listener.onFailure(e);
-                return true;
+                return;
             }
 
             // no shardIt, might be in the case between index gateway recovery and shardIt initialization
             if (shardIt.size() == 0) {
                 retry(null);
-                return false;
+                return;
             }
 
             // this transport only make sense with an iterator that returns a single shard routing (like primary)
@@ -169,11 +172,7 @@ public abstract class TransportInstanceSingleOperationAction<Request extends Ins
 
             if (!shard.active()) {
                 retry(null);
-                return false;
-            }
-
-            if (!operationStarted.compareAndSet(false, true)) {
-                return true;
+                return;
             }
 
             request.shardId = shardIt.shardId().id();
@@ -197,24 +196,30 @@ public abstract class TransportInstanceSingleOperationAction<Request extends Ins
 
                 @Override
                 public void handleException(TransportException exp) {
+                    Throwable cause = exp.unwrapCause();
                     // if we got disconnected from the node, or the node / shard is not in the right state (being closed)
-                    if (exp.unwrapCause() instanceof ConnectTransportException || exp.unwrapCause() instanceof NodeClosedException ||
+                    if (cause instanceof ConnectTransportException || cause instanceof NodeClosedException ||
                             retryOnFailure(exp)) {
-                        operationStarted.set(false);
-                        // we already marked it as started when we executed it (removed the listener) so pass false
-                        // to re-add to the cluster listener
-                        retry(null);
+                        retry(cause);
                     } else {
                         listener.onFailure(exp);
                     }
                 }
             });
-            return true;
         }
 
         void retry(final @Nullable Throwable failure) {
             if (observer.isTimedOut()) {
                 // we running as a last attempt after a timeout has happened. don't retry
+                Throwable listenFailure = failure;
+                if (listenFailure == null) {
+                    if (shardIt == null) {
+                        listenFailure = new UnavailableShardsException(new ShardId(request.concreteIndex(), -1), "Timeout waiting for [{}], request: {}", request.timeout(), actionName);
+                    } else {
+                        listenFailure = new UnavailableShardsException(shardIt.shardId(), "[{}] shardIt, [{}] active : Timeout waiting for [{}], request: {}", shardIt.size(), shardIt.sizeActive(), request.timeout(), actionName);
+                    }
+                }
+                listener.onFailure(listenFailure);
                 return;
             }
 
@@ -232,17 +237,7 @@ public abstract class TransportInstanceSingleOperationAction<Request extends Ins
                 @Override
                 public void onTimeout(TimeValue timeout) {
                     // just to be on the safe side, see if we can start it now?
-                    if (!doStart()) {
-                        Throwable listenFailure = failure;
-                        if (listenFailure == null) {
-                            if (shardIt == null) {
-                                listenFailure = new UnavailableShardsException(new ShardId(request.concreteIndex(), -1), "Timeout waiting for [" + timeout + "], request: " + request.toString());
-                            } else {
-                                listenFailure = new UnavailableShardsException(shardIt.shardId(), "[" + shardIt.size() + "] shardIt, [" + shardIt.sizeActive() + "] active : Timeout waiting for [" + timeout + "], request: " + request.toString());
-                            }
-                        }
-                        listener.onFailure(listenFailure);
-                    }
+                    doStart();
                 }
             }, request.timeout());
         }
diff --git a/core/src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsRequest.java b/core/src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsRequest.java
index c6db738..6d880a7 100644
--- a/core/src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsRequest.java
@@ -19,11 +19,11 @@
 
 package org.elasticsearch.action.termvectors;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.action.*;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.collect.Iterators;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentFactory;
@@ -74,7 +74,7 @@ public class MultiTermVectorsRequest extends ActionRequest<MultiTermVectorsReque
 
     @Override
     public Iterator<TermVectorsRequest> iterator() {
-        return Iterators.unmodifiableIterator(requests.iterator());
+        return Collections.unmodifiableCollection(requests).iterator();
     }
 
     public boolean isEmpty() {
diff --git a/core/src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsResponse.java b/core/src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsResponse.java
index fe013d5..6eb3b32 100644
--- a/core/src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsResponse.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.action.termvectors;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.common.io.stream.StreamInput;
@@ -30,6 +29,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
 
 import java.io.IOException;
+import java.util.Arrays;
 import java.util.Iterator;
 
 public class MultiTermVectorsResponse extends ActionResponse implements Iterable<MultiTermVectorsItemResponse>, ToXContent {
@@ -120,7 +120,7 @@ public class MultiTermVectorsResponse extends ActionResponse implements Iterable
 
     @Override
     public Iterator<MultiTermVectorsItemResponse> iterator() {
-        return Iterators.forArray(responses);
+        return Arrays.stream(responses).iterator();
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/action/termvectors/dfs/DfsOnlyRequest.java b/core/src/main/java/org/elasticsearch/action/termvectors/dfs/DfsOnlyRequest.java
index b21227f..86d575d 100644
--- a/core/src/main/java/org/elasticsearch/action/termvectors/dfs/DfsOnlyRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/termvectors/dfs/DfsOnlyRequest.java
@@ -27,6 +27,7 @@ import org.elasticsearch.action.search.SearchRequest;
 import org.elasticsearch.action.support.broadcast.BroadcastRequest;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.index.query.BoolQueryBuilder;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
@@ -103,8 +104,10 @@ public class DfsOnlyRequest extends BroadcastRequest<DfsOnlyRequest> {
     @Override
     public String toString() {
         String sSource = "_na_";
-        if (searchRequest.source() != null) {
-            sSource = searchRequest.source().toString();
+        try {
+            sSource = XContentHelper.convertToJson(searchRequest.source(), false);
+        } catch (IOException e) {
+            // ignore
         }
         return "[" + Arrays.toString(indices) + "]" + Arrays.toString(types()) + ", source[" + sSource + "]";
     }
diff --git a/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java b/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java
index 7479416..2a639c8 100644
--- a/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java
+++ b/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java
@@ -48,9 +48,8 @@ import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentType;
-import org.elasticsearch.index.engine.DocumentAlreadyExistsException;
-import org.elasticsearch.index.engine.VersionConflictEngineException;
 import org.elasticsearch.index.IndexService;
+import org.elasticsearch.index.engine.VersionConflictEngineException;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.indices.IndexAlreadyExistsException;
 import org.elasticsearch.indices.IndicesService;
@@ -170,7 +169,7 @@ public class TransportUpdateAction extends TransportInstanceSingleOperationActio
         final UpdateHelper.Result result = updateHelper.prepare(request, indexShard);
         switch (result.operation()) {
             case UPSERT:
-                IndexRequest upsertRequest = new IndexRequest((IndexRequest)result.action(), request);
+                IndexRequest upsertRequest = new IndexRequest(result.action(), request);
                 // we fetch it from the index request so we don't generate the bytes twice, its already done in the index request
                 final BytesReference upsertSourceBytes = upsertRequest.source();
                 indexAction.execute(upsertRequest, new ActionListener<IndexResponse>() {
@@ -189,7 +188,7 @@ public class TransportUpdateAction extends TransportInstanceSingleOperationActio
                     @Override
                     public void onFailure(Throwable e) {
                         e = ExceptionsHelper.unwrapCause(e);
-                        if (e instanceof VersionConflictEngineException || e instanceof DocumentAlreadyExistsException) {
+                        if (e instanceof VersionConflictEngineException) {
                             if (retryCount < request.retryOnConflict()) {
                                 threadPool.executor(executor()).execute(new ActionRunnable<UpdateResponse>(listener) {
                                     @Override
@@ -205,7 +204,7 @@ public class TransportUpdateAction extends TransportInstanceSingleOperationActio
                 });
                 break;
             case INDEX:
-                IndexRequest indexRequest = new IndexRequest((IndexRequest)result.action(), request);
+                IndexRequest indexRequest = new IndexRequest(result.action(), request);
                 // we fetch it from the index request so we don't generate the bytes twice, its already done in the index request
                 final BytesReference indexSourceBytes = indexRequest.source();
                 indexAction.execute(indexRequest, new ActionListener<IndexResponse>() {
@@ -235,7 +234,7 @@ public class TransportUpdateAction extends TransportInstanceSingleOperationActio
                 });
                 break;
             case DELETE:
-                DeleteRequest deleteRequest = new DeleteRequest((DeleteRequest)result.action(), request);
+                DeleteRequest deleteRequest = new DeleteRequest(result.action(), request);
                 deleteAction.execute(deleteRequest, new ActionListener<DeleteResponse>() {
                     @Override
                     public void onResponse(DeleteResponse response) {
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java b/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
index 542444b..9ebb2c9 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
@@ -26,7 +26,6 @@ import org.elasticsearch.common.PidFile;
 import org.elasticsearch.common.SuppressForbidden;
 import org.elasticsearch.common.cli.CliTool;
 import org.elasticsearch.common.cli.Terminal;
-import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.inject.CreationException;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.logging.ESLogger;
@@ -249,13 +248,13 @@ final class Bootstrap {
 
         Environment environment = initialSettings(foreground);
         Settings settings = environment.settings();
+        setupLogging(settings, environment);
+        checkForCustomConfFile();
 
         if (environment.pidFile() != null) {
             PidFile.create(environment.pidFile(), true);
         }
 
-        setupLogging(settings, environment);
-
         if (System.getProperty("es.max-open-files", "false").equals("true")) {
             ESLogger logger = Loggers.getLogger(Bootstrap.class);
             logger.info("max_open_files [{}]", ProcessProbe.getInstance().getMaxFileDescriptorCount());
@@ -330,4 +329,21 @@ final class Bootstrap {
             System.err.flush();
         }
     }
+
+    private static void checkForCustomConfFile() {
+        String confFileSetting = System.getProperty("es.default.config");
+        checkUnsetAndMaybeExit(confFileSetting, "es.default.config");
+        confFileSetting = System.getProperty("es.config");
+        checkUnsetAndMaybeExit(confFileSetting, "es.config");
+        confFileSetting = System.getProperty("elasticsearch.config");
+        checkUnsetAndMaybeExit(confFileSetting, "elasticsearch.config");
+    }
+
+    private static void checkUnsetAndMaybeExit(String confFileSetting, String settingName) {
+        if (confFileSetting != null && confFileSetting.isEmpty() == false) {
+            ESLogger logger = Loggers.getLogger(Bootstrap.class);
+            logger.info("{} is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed.", settingName);
+            System.exit(1);
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/Security.java b/core/src/main/java/org/elasticsearch/bootstrap/Security.java
index b27048d..66dda6e 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/Security.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/Security.java
@@ -165,7 +165,7 @@ final class Security {
         Map<String,String> m = new HashMap<>();
         m.put("repository-s3",       "org.elasticsearch.plugin.repository.s3.S3RepositoryPlugin");
         m.put("discovery-ec2",       "org.elasticsearch.plugin.discovery.ec2.Ec2DiscoveryPlugin");
-        m.put("cloud-gce",           "org.elasticsearch.plugin.cloud.gce.CloudGcePlugin");
+        m.put("discovery-gce",       "org.elasticsearch.plugin.discovery.gce.GceDiscoveryPlugin");
         m.put("lang-expression",     "org.elasticsearch.script.expression.ExpressionPlugin");
         m.put("lang-groovy",         "org.elasticsearch.script.groovy.GroovyPlugin");
         m.put("lang-javascript",     "org.elasticsearch.plugin.javascript.JavaScriptPlugin");
diff --git a/core/src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java b/core/src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java
index b13c799..e3925aa 100644
--- a/core/src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java
+++ b/core/src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java
@@ -73,7 +73,7 @@ public class MappingUpdatedAction extends AbstractComponent {
             throw new IllegalArgumentException("_default_ mapping should not be updated");
         }
         return client.preparePutMapping(index).setType(type).setSource(mappingUpdate.toString())
-            .setMasterNodeTimeout(timeout).setTimeout(timeout);
+                .setMasterNodeTimeout(timeout).setTimeout(timeout);
     }
 
     public void updateMappingOnMaster(String index, String type, Mapping mappingUpdate, final TimeValue timeout, final MappingUpdateListener listener) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java
index a17fe04..6e29f31 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java
@@ -34,7 +34,7 @@ import org.elasticsearch.index.Index;
 import org.elasticsearch.index.analysis.AnalysisService;
 import org.elasticsearch.index.analysis.NamedAnalyzer;
 import org.elasticsearch.index.mapper.MapperService;
-import org.elasticsearch.index.similarity.SimilarityLookupService;
+import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.index.store.IndexStoreModule;
 import org.elasticsearch.script.ScriptService;
 
@@ -322,11 +322,11 @@ public class MetaDataIndexUpgradeService extends AbstractComponent {
         Index index = new Index(indexMetaData.getIndex());
         Settings settings = indexMetaData.settings();
         try {
-            SimilarityLookupService similarityLookupService = new SimilarityLookupService(index, settings);
+            SimilarityService similarityService = new SimilarityService(index, settings);
             // We cannot instantiate real analysis server at this point because the node might not have
             // been started yet. However, we don't really need real analyzers at this stage - so we can fake it
             try (AnalysisService analysisService = new FakeAnalysisService(index, settings)) {
-                try (MapperService mapperService = new MapperService(index, settings, analysisService, similarityLookupService, scriptService)) {
+                try (MapperService mapperService = new MapperService(index, settings, analysisService, similarityService, scriptService)) {
                     for (ObjectCursor<MappingMetaData> cursor : indexMetaData.getMappings().values()) {
                         MappingMetaData mappingMetaData = cursor.value;
                         mapperService.merge(mappingMetaData.type(), mappingMetaData.source(), false, false);
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNode.java b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNode.java
index 43ad6af..596bb97 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNode.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNode.java
@@ -19,13 +19,10 @@
 
 package org.elasticsearch.cluster.routing;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.common.collect.Iterators;
 
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Iterator;
-import java.util.List;
+import java.util.*;
 
 /**
  * A {@link RoutingNode} represents a cluster node associated with a single {@link DiscoveryNode} including all shards
@@ -51,7 +48,7 @@ public class RoutingNode implements Iterable<ShardRouting> {
 
     @Override
     public Iterator<ShardRouting> iterator() {
-        return Iterators.unmodifiableIterator(shards.iterator());
+        return Collections.unmodifiableCollection(shards).iterator();
     }
 
     Iterator<ShardRouting> mutableIterator() {
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java
index 227d59f..39c2d03 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java
@@ -21,13 +21,13 @@ package org.elasticsearch.cluster.routing;
 
 import com.carrotsearch.hppc.ObjectIntHashMap;
 import com.carrotsearch.hppc.cursors.ObjectCursor;
-import com.google.common.collect.Iterators;
 import org.apache.lucene.util.CollectionUtil;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.block.ClusterBlocks;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
+import org.elasticsearch.common.collect.Iterators;
 import org.elasticsearch.index.shard.ShardId;
 
 import java.util.*;
@@ -144,7 +144,7 @@ public class RoutingNodes implements Iterable<RoutingNode> {
 
     @Override
     public Iterator<RoutingNode> iterator() {
-        return Iterators.unmodifiableIterator(nodesToShards.values().iterator());
+        return Collections.unmodifiableCollection(nodesToShards.values()).iterator();
     }
 
     public RoutingTable routingTable() {
diff --git a/core/src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java b/core/src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java
index 1477179..add383b 100644
--- a/core/src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java
+++ b/core/src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java
@@ -311,10 +311,6 @@ public class PagedBytesReference implements BytesReference {
             return true;
         }
 
-        if (obj == null) {
-            return false;
-        }
-
         if (!(obj instanceof PagedBytesReference)) {
             return BytesReference.Helper.bytesEqual(this, (BytesReference) obj);
         }
diff --git a/core/src/main/java/org/elasticsearch/common/collect/EvictingQueue.java b/core/src/main/java/org/elasticsearch/common/collect/EvictingQueue.java
new file mode 100644
index 0000000..51cc08d
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/common/collect/EvictingQueue.java
@@ -0,0 +1,176 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.collect;
+
+import java.util.ArrayDeque;
+import java.util.Collection;
+import java.util.Iterator;
+import java.util.Queue;
+
+/**
+ * An {@code EvictingQueue} is a non-blocking queue which is limited to a maximum size; when new elements are added to a
+ * full queue, elements are evicted from the head of the queue to accommodate the new elements.
+ *
+ * @param <T> The type of elements in the queue.
+ */
+public class EvictingQueue<T> implements Queue<T> {
+    private final int maximumSize;
+    private final ArrayDeque<T> queue;
+
+    /**
+     * Construct a new {@code EvictingQueue} that holds {@code maximumSize} elements.
+     *
+     * @param maximumSize The maximum number of elements that the queue can hold
+     * @throws IllegalArgumentException if {@code maximumSize} is less than zero
+     */
+    public EvictingQueue(int maximumSize) {
+        if (maximumSize < 0) {
+            throw new IllegalArgumentException("maximumSize < 0");
+        }
+        this.maximumSize = maximumSize;
+        this.queue = new ArrayDeque<>(maximumSize);
+    }
+
+    /**
+     * @return the number of additional elements that the queue can accommodate before evictions occur
+     */
+    public int remainingCapacity() {
+        return this.maximumSize - this.size();
+    }
+
+    /**
+     * Add the given element to the queue, possibly forcing an eviction from the head if {@link #remainingCapacity()} is
+     * zero.
+     *
+     * @param t the element to add
+     * @return true if the element was added (always the case for {@code EvictingQueue}
+     */
+    @Override
+    public boolean add(T t) {
+        if (maximumSize == 0) {
+            return true;
+        }
+        if (queue.size() == maximumSize) {
+            queue.remove();
+        }
+        queue.add(t);
+        return true;
+    }
+
+    /**
+     * @see #add(Object)
+     */
+    @Override
+    public boolean offer(T t) {
+        return add(t);
+    }
+
+    @Override
+    public T remove() {
+        return queue.remove();
+    }
+
+
+    @Override
+    public T poll() {
+        return queue.poll();
+    }
+
+    @Override
+    public T element() {
+        return queue.element();
+    }
+
+    @Override
+    public T peek() {
+        return queue.peek();
+    }
+
+    @Override
+    public int size() {
+        return queue.size();
+    }
+
+    @Override
+    public boolean isEmpty() {
+        return queue.isEmpty();
+    }
+
+    @Override
+    public boolean contains(Object o) {
+        return queue.contains(o);
+    }
+
+    @Override
+    public Iterator<T> iterator() {
+        return queue.iterator();
+    }
+
+    @Override
+    public Object[] toArray() {
+        return queue.toArray();
+    }
+
+    @Override
+    public <T1> T1[] toArray(T1[] a) {
+        return queue.toArray(a);
+    }
+
+    @Override
+    public boolean remove(Object o) {
+        return queue.remove(o);
+    }
+
+    @Override
+    public boolean containsAll(Collection<?> c) {
+        return queue.containsAll(c);
+    }
+
+    /**
+     * Add the given elements to the queue, possibly forcing evictions from the head if {@link #remainingCapacity()} is
+     * zero or becomes zero during the execution of this method.
+     *
+     * @param c the collection of elements to add
+     * @return true if any elements were added to the queue
+     */
+    @Override
+    public boolean addAll(Collection<? extends T> c) {
+        boolean modified = false;
+        for (T e : c)
+            if (add(e))
+                modified = true;
+        return modified;
+    }
+
+    @Override
+    public boolean removeAll(Collection<?> c) {
+        return queue.removeAll(c);
+    }
+
+    @Override
+    public boolean retainAll(Collection<?> c) {
+        return queue.retainAll(c);
+    }
+
+    @Override
+    public void clear() {
+        queue.clear();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/common/collect/Iterators.java b/core/src/main/java/org/elasticsearch/common/collect/Iterators.java
new file mode 100644
index 0000000..3454612
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/common/collect/Iterators.java
@@ -0,0 +1,68 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.collect;
+
+import java.util.Iterator;
+import java.util.NoSuchElementException;
+
+public class Iterators {
+    public static <T> Iterator<T> concat(Iterator<? extends T>... iterators) {
+        if (iterators == null) {
+            throw new NullPointerException("iterators");
+        }
+
+        return new ConcatenatedIterator<>(iterators);
+    }
+
+    static class ConcatenatedIterator<T> implements Iterator<T> {
+        private final Iterator<? extends T>[] iterators;
+        private int index = 0;
+
+        public ConcatenatedIterator(Iterator<? extends T>... iterators) {
+            if (iterators == null) {
+                throw new NullPointerException("iterators");
+            }
+            for (int i = 0; i < iterators.length; i++) {
+                if (iterators[i] == null) {
+                    throw new NullPointerException("iterators[" + i  + "]");
+                }
+            }
+            this.iterators = iterators;
+        }
+
+        @Override
+        public boolean hasNext() {
+            boolean hasNext = false;
+            while (index < iterators.length && !(hasNext = iterators[index].hasNext())) {
+                index++;
+            }
+
+            return hasNext;
+        }
+
+        @Override
+        public T next() {
+            if (!hasNext()) {
+                throw new NoSuchElementException();
+            }
+            return iterators[index].next();
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/common/io/FileSystemUtils.java b/core/src/main/java/org/elasticsearch/common/io/FileSystemUtils.java
index cb5ca5f..e53e7a7 100644
--- a/core/src/main/java/org/elasticsearch/common/io/FileSystemUtils.java
+++ b/core/src/main/java/org/elasticsearch/common/io/FileSystemUtils.java
@@ -19,9 +19,8 @@
 
 package org.elasticsearch.common.io;
 
-import com.google.common.collect.Iterators;
-
 import org.apache.lucene.util.IOUtils;
+import org.elasticsearch.common.collect.Iterators;
 import org.elasticsearch.common.logging.ESLogger;
 
 import java.io.BufferedReader;
@@ -35,6 +34,7 @@ import java.nio.file.*;
 import java.nio.file.attribute.BasicFileAttributes;
 import java.util.Arrays;
 import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.stream.StreamSupport;
 
 import static java.nio.file.FileVisitResult.CONTINUE;
 import static java.nio.file.FileVisitResult.SKIP_SUBTREE;
@@ -328,7 +328,7 @@ public final class FileSystemUtils {
      */
     public static Path[] files(Path from, DirectoryStream.Filter<Path> filter) throws IOException {
         try (DirectoryStream<Path> stream = Files.newDirectoryStream(from, filter)) {
-            return Iterators.toArray(stream.iterator(), Path.class);
+            return toArray(stream);
         }
     }
 
@@ -337,7 +337,7 @@ public final class FileSystemUtils {
      */
     public static Path[] files(Path directory) throws IOException {
         try (DirectoryStream<Path> stream = Files.newDirectoryStream(directory)) {
-            return Iterators.toArray(stream.iterator(), Path.class);
+            return toArray(stream);
         }
     }
 
@@ -346,8 +346,12 @@ public final class FileSystemUtils {
      */
     public static Path[] files(Path directory, String glob) throws IOException {
         try (DirectoryStream<Path> stream = Files.newDirectoryStream(directory, glob)) {
-            return Iterators.toArray(stream.iterator(), Path.class);
+            return toArray(stream);
         }
     }
 
+    private static Path[] toArray(DirectoryStream<Path> stream) {
+        return StreamSupport.stream(stream.spliterator(), false).toArray(length -> new Path[length]);
+    }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/uid/Versions.java b/core/src/main/java/org/elasticsearch/common/lucene/uid/Versions.java
index 77eb218..55586d8 100644
--- a/core/src/main/java/org/elasticsearch/common/lucene/uid/Versions.java
+++ b/core/src/main/java/org/elasticsearch/common/lucene/uid/Versions.java
@@ -33,10 +33,24 @@ import java.util.concurrent.ConcurrentMap;
 /** Utility class to resolve the Lucene doc ID and version for a given uid. */
 public class Versions {
 
-    public static final long MATCH_ANY = -3L; // Version was not specified by the user
+    /** used to indicate the write operation should succeed regardless of current version **/
+    public static final long MATCH_ANY = -3L;
+
+    /** indicates that the current document was not found in lucene and in the version map */
     public static final long NOT_FOUND = -1L;
+
+    /**
+     * used when the document is old and doesn't contain any version information in the index
+     * see {@link PerThreadIDAndVersionLookup#lookup(org.apache.lucene.util.BytesRef)}
+     */
     public static final long NOT_SET = -2L;
 
+    /**
+     * used to indicate that the write operation should be executed if the document is currently deleted
+     * i.e., not found in the index and/or found as deleted (with version) in the version map
+     */
+    public static final long MATCH_DELETED = -4L;
+
     // TODO: is there somewhere else we can store these?
     private static final ConcurrentMap<IndexReader, CloseableThreadLocal<PerThreadIDAndVersionLookup>> lookupStates = ConcurrentCollections.newConcurrentMapWithAggressiveConcurrency();
 
diff --git a/core/src/main/java/org/elasticsearch/common/network/InetAddresses.java b/core/src/main/java/org/elasticsearch/common/network/InetAddresses.java
new file mode 100644
index 0000000..4d3d140
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/common/network/InetAddresses.java
@@ -0,0 +1,357 @@
+/*
+ * Copyright (C) 2008 The Guava Authors
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.elasticsearch.common.network;
+
+import java.net.Inet4Address;
+import java.net.Inet6Address;
+import java.net.InetAddress;
+import java.net.UnknownHostException;
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import java.util.Locale;
+
+public class InetAddresses {
+    private static int IPV4_PART_COUNT = 4;
+    private static int IPV6_PART_COUNT = 8;
+
+    public static boolean isInetAddress(String ipString) {
+        return ipStringToBytes(ipString) != null;
+    }
+
+    private static byte[] ipStringToBytes(String ipString) {
+        // Make a first pass to categorize the characters in this string.
+        boolean hasColon = false;
+        boolean hasDot = false;
+        for (int i = 0; i < ipString.length(); i++) {
+            char c = ipString.charAt(i);
+            if (c == '.') {
+                hasDot = true;
+            } else if (c == ':') {
+                if (hasDot) {
+                    return null;  // Colons must not appear after dots.
+                }
+                hasColon = true;
+            } else if (Character.digit(c, 16) == -1) {
+                return null;  // Everything else must be a decimal or hex digit.
+            }
+        }
+
+        // Now decide which address family to parse.
+        if (hasColon) {
+            if (hasDot) {
+                ipString = convertDottedQuadToHex(ipString);
+                if (ipString == null) {
+                    return null;
+                }
+            }
+            return textToNumericFormatV6(ipString);
+        } else if (hasDot) {
+            return textToNumericFormatV4(ipString);
+        }
+        return null;
+    }
+
+    private static String convertDottedQuadToHex(String ipString) {
+        int lastColon = ipString.lastIndexOf(':');
+        String initialPart = ipString.substring(0, lastColon + 1);
+        String dottedQuad = ipString.substring(lastColon + 1);
+        byte[] quad = textToNumericFormatV4(dottedQuad);
+        if (quad == null) {
+            return null;
+        }
+        String penultimate = Integer.toHexString(((quad[0] & 0xff) << 8) | (quad[1] & 0xff));
+        String ultimate = Integer.toHexString(((quad[2] & 0xff) << 8) | (quad[3] & 0xff));
+        return initialPart + penultimate + ":" + ultimate;
+    }
+
+    private static byte[] textToNumericFormatV4(String ipString) {
+        String[] address = ipString.split("\\.", IPV4_PART_COUNT + 1);
+        if (address.length != IPV4_PART_COUNT) {
+            return null;
+        }
+
+        byte[] bytes = new byte[IPV4_PART_COUNT];
+        try {
+            for (int i = 0; i < bytes.length; i++) {
+                bytes[i] = parseOctet(address[i]);
+            }
+        } catch (NumberFormatException ex) {
+            return null;
+        }
+
+        return bytes;
+    }
+
+    private static byte parseOctet(String ipPart) {
+        // Note: we already verified that this string contains only hex digits.
+        int octet = Integer.parseInt(ipPart);
+        // Disallow leading zeroes, because no clear standard exists on
+        // whether these should be interpreted as decimal or octal.
+        if (octet > 255 || (ipPart.startsWith("0") && ipPart.length() > 1)) {
+            throw new NumberFormatException();
+        }
+        return (byte) octet;
+    }
+
+    private static byte[] textToNumericFormatV6(String ipString) {
+        // An address can have [2..8] colons, and N colons make N+1 parts.
+        String[] parts = ipString.split(":", IPV6_PART_COUNT + 2);
+        if (parts.length < 3 || parts.length > IPV6_PART_COUNT + 1) {
+            return null;
+        }
+
+        // Disregarding the endpoints, find "::" with nothing in between.
+        // This indicates that a run of zeroes has been skipped.
+        int skipIndex = -1;
+        for (int i = 1; i < parts.length - 1; i++) {
+            if (parts[i].length() == 0) {
+                if (skipIndex >= 0) {
+                    return null;  // Can't have more than one ::
+                }
+                skipIndex = i;
+            }
+        }
+
+        int partsHi;  // Number of parts to copy from above/before the "::"
+        int partsLo;  // Number of parts to copy from below/after the "::"
+        if (skipIndex >= 0) {
+            // If we found a "::", then check if it also covers the endpoints.
+            partsHi = skipIndex;
+            partsLo = parts.length - skipIndex - 1;
+            if (parts[0].length() == 0 && --partsHi != 0) {
+                return null;  // ^: requires ^::
+            }
+            if (parts[parts.length - 1].length() == 0 && --partsLo != 0) {
+                return null;  // :$ requires ::$
+            }
+        } else {
+            // Otherwise, allocate the entire address to partsHi.  The endpoints
+            // could still be empty, but parseHextet() will check for that.
+            partsHi = parts.length;
+            partsLo = 0;
+        }
+
+        // If we found a ::, then we must have skipped at least one part.
+        // Otherwise, we must have exactly the right number of parts.
+        int partsSkipped = IPV6_PART_COUNT - (partsHi + partsLo);
+        if (!(skipIndex >= 0 ? partsSkipped >= 1 : partsSkipped == 0)) {
+            return null;
+        }
+
+        // Now parse the hextets into a byte array.
+        ByteBuffer rawBytes = ByteBuffer.allocate(2 * IPV6_PART_COUNT);
+        try {
+            for (int i = 0; i < partsHi; i++) {
+                rawBytes.putShort(parseHextet(parts[i]));
+            }
+            for (int i = 0; i < partsSkipped; i++) {
+                rawBytes.putShort((short) 0);
+            }
+            for (int i = partsLo; i > 0; i--) {
+                rawBytes.putShort(parseHextet(parts[parts.length - i]));
+            }
+        } catch (NumberFormatException ex) {
+            return null;
+        }
+        return rawBytes.array();
+    }
+
+    private static short parseHextet(String ipPart) {
+        // Note: we already verified that this string contains only hex digits.
+        int hextet = Integer.parseInt(ipPart, 16);
+        if (hextet > 0xffff) {
+            throw new NumberFormatException();
+        }
+        return (short) hextet;
+    }
+
+    /**
+     * Returns the string representation of an {@link InetAddress} suitable
+     * for inclusion in a URI.
+     *
+     * <p>For IPv4 addresses, this is identical to
+     * {@link InetAddress#getHostAddress()}, but for IPv6 addresses it
+     * compresses zeroes and surrounds the text with square brackets; for example
+     * {@code "[2001:db8::1]"}.
+     *
+     * <p>Per section 3.2.2 of
+     * <a target="_parent"
+     *    href="http://tools.ietf.org/html/rfc3986#section-3.2.2"
+     *  >http://tools.ietf.org/html/rfc3986</a>,
+     * a URI containing an IPv6 string literal is of the form
+     * {@code "http://[2001:db8::1]:8888/index.html"}.
+     *
+     * <p>Use of either {@link InetAddresses#toAddrString},
+     * {@link InetAddress#getHostAddress()}, or this method is recommended over
+     * {@link InetAddress#toString()} when an IP address string literal is
+     * desired.  This is because {@link InetAddress#toString()} prints the
+     * hostname and the IP address string joined by a "/".
+     *
+     * @param ip {@link InetAddress} to be converted to URI string literal
+     * @return {@code String} containing URI-safe string literal
+     */
+    public static String toUriString(InetAddress ip) {
+        if (ip instanceof Inet6Address) {
+            return "[" + toAddrString(ip) + "]";
+        }
+        return toAddrString(ip);
+    }
+
+    /**
+     * Returns the string representation of an {@link InetAddress}.
+     *
+     * <p>For IPv4 addresses, this is identical to
+     * {@link InetAddress#getHostAddress()}, but for IPv6 addresses, the output
+     * follows <a href="http://tools.ietf.org/html/rfc5952">RFC 5952</a>
+     * section 4.  The main difference is that this method uses "::" for zero
+     * compression, while Java's version uses the uncompressed form.
+     *
+     * <p>This method uses hexadecimal for all IPv6 addresses, including
+     * IPv4-mapped IPv6 addresses such as "::c000:201".  The output does not
+     * include a Scope ID.
+     *
+     * @param ip {@link InetAddress} to be converted to an address string
+     * @return {@code String} containing the text-formatted IP address
+     * @since 10.0
+     */
+    public static String toAddrString(InetAddress ip) {
+        if (ip == null) {
+            throw new NullPointerException("ip");
+        }
+        if (ip instanceof Inet4Address) {
+            // For IPv4, Java's formatting is good enough.
+            byte[] bytes = ip.getAddress();
+            return (bytes[0] & 0xff) + "." + (bytes[1] & 0xff) + "." + (bytes[2] & 0xff) + "." + (bytes[3] & 0xff);
+        }
+        if (!(ip instanceof Inet6Address)) {
+            throw new IllegalArgumentException("ip");
+        }
+        byte[] bytes = ip.getAddress();
+        int[] hextets = new int[IPV6_PART_COUNT];
+        for (int i = 0; i < hextets.length; i++) {
+            hextets[i] =  (bytes[2 * i] & 255) << 8 | bytes[2 * i + 1] & 255;
+        }
+        compressLongestRunOfZeroes(hextets);
+        return hextetsToIPv6String(hextets);
+    }
+
+    /**
+     * Identify and mark the longest run of zeroes in an IPv6 address.
+     *
+     * <p>Only runs of two or more hextets are considered.  In case of a tie, the
+     * leftmost run wins.  If a qualifying run is found, its hextets are replaced
+     * by the sentinel value -1.
+     *
+     * @param hextets {@code int[]} mutable array of eight 16-bit hextets
+     */
+    private static void compressLongestRunOfZeroes(int[] hextets) {
+        int bestRunStart = -1;
+        int bestRunLength = -1;
+        int runStart = -1;
+        for (int i = 0; i < hextets.length + 1; i++) {
+            if (i < hextets.length && hextets[i] == 0) {
+                if (runStart < 0) {
+                    runStart = i;
+                }
+            } else if (runStart >= 0) {
+                int runLength = i - runStart;
+                if (runLength > bestRunLength) {
+                    bestRunStart = runStart;
+                    bestRunLength = runLength;
+                }
+                runStart = -1;
+            }
+        }
+        if (bestRunLength >= 2) {
+            Arrays.fill(hextets, bestRunStart, bestRunStart + bestRunLength, -1);
+        }
+    }
+
+    /**
+     * Convert a list of hextets into a human-readable IPv6 address.
+     *
+     * <p>In order for "::" compression to work, the input should contain negative
+     * sentinel values in place of the elided zeroes.
+     *
+     * @param hextets {@code int[]} array of eight 16-bit hextets, or -1s
+     */
+    private static String hextetsToIPv6String(int[] hextets) {
+    /*
+     * While scanning the array, handle these state transitions:
+     *   start->num => "num"     start->gap => "::"
+     *   num->num   => ":num"    num->gap   => "::"
+     *   gap->num   => "num"     gap->gap   => ""
+     */
+        StringBuilder buf = new StringBuilder(39);
+        boolean lastWasNumber = false;
+        for (int i = 0; i < hextets.length; i++) {
+            boolean thisIsNumber = hextets[i] >= 0;
+            if (thisIsNumber) {
+                if (lastWasNumber) {
+                    buf.append(':');
+                }
+                buf.append(Integer.toHexString(hextets[i]));
+            } else {
+                if (i == 0 || lastWasNumber) {
+                    buf.append("::");
+                }
+            }
+            lastWasNumber = thisIsNumber;
+        }
+        return buf.toString();
+    }
+
+    /**
+     * Returns the {@link InetAddress} having the given string representation.
+     *
+     * <p>This deliberately avoids all nameservice lookups (e.g. no DNS).
+     *
+     * @param ipString {@code String} containing an IPv4 or IPv6 string literal, e.g.
+     *     {@code "192.168.0.1"} or {@code "2001:db8::1"}
+     * @return {@link InetAddress} representing the argument
+     * @throws IllegalArgumentException if the argument is not a valid IP string literal
+     */
+    public static InetAddress forString(String ipString) {
+        byte[] addr = ipStringToBytes(ipString);
+
+        // The argument was malformed, i.e. not an IP string literal.
+        if (addr == null) {
+            throw new IllegalArgumentException(String.format(Locale.ROOT, "'%s' is not an IP string literal.", ipString));
+        }
+
+        return bytesToInetAddress(addr);
+    }
+
+    /**
+     * Convert a byte array into an InetAddress.
+     *
+     * {@link InetAddress#getByAddress} is documented as throwing a checked
+     * exception "if IP address is of illegal length."  We replace it with
+     * an unchecked exception, for use by callers who already know that addr
+     * is an array of length 4 or 16.
+     *
+     * @param addr the raw 4-byte or 16-byte IP address in big-endian order
+     * @return an InetAddress object created from the raw IP address
+     */
+    private static InetAddress bytesToInetAddress(byte[] addr) {
+        try {
+            return InetAddress.getByAddress(addr);
+        } catch (UnknownHostException e) {
+            throw new AssertionError(e);
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/common/network/NetworkAddress.java b/core/src/main/java/org/elasticsearch/common/network/NetworkAddress.java
index 91eda6b..3dcaeeb 100644
--- a/core/src/main/java/org/elasticsearch/common/network/NetworkAddress.java
+++ b/core/src/main/java/org/elasticsearch/common/network/NetworkAddress.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.common.network;
 
-import com.google.common.net.InetAddresses;
-
 import org.elasticsearch.common.SuppressForbidden;
 
 import java.net.Inet6Address;
diff --git a/core/src/main/java/org/elasticsearch/common/network/NetworkService.java b/core/src/main/java/org/elasticsearch/common/network/NetworkService.java
index 8eff70e..cd46d14 100644
--- a/core/src/main/java/org/elasticsearch/common/network/NetworkService.java
+++ b/core/src/main/java/org/elasticsearch/common/network/NetworkService.java
@@ -27,7 +27,6 @@ import org.elasticsearch.common.unit.TimeValue;
 
 import java.io.IOException;
 import java.net.InetAddress;
-import java.net.UnknownHostException;
 import java.util.List;
 import java.util.concurrent.CopyOnWriteArrayList;
 import java.util.concurrent.TimeUnit;
@@ -73,7 +72,7 @@ public class NetworkService extends AbstractComponent {
         /**
          * Resolves a custom value handling, return <tt>null</tt> if can't handle it.
          */
-        InetAddress[] resolveIfPossible(String value);
+        InetAddress[] resolveIfPossible(String value) throws IOException;
     }
 
     private final List<CustomNameResolver> customNameResolvers = new CopyOnWriteArrayList<>();
@@ -162,7 +161,7 @@ public class NetworkService extends AbstractComponent {
         return address;
     }
 
-    private InetAddress[] resolveInetAddress(String host) throws UnknownHostException, IOException {
+    private InetAddress[] resolveInetAddress(String host) throws IOException {
         if ((host.startsWith("#") && host.endsWith("#")) || (host.startsWith("_") && host.endsWith("_"))) {
             host = host.substring(1, host.length() - 1);
             // allow custom resolvers to have special names
diff --git a/core/src/main/java/org/elasticsearch/common/util/CollectionUtils.java b/core/src/main/java/org/elasticsearch/common/util/CollectionUtils.java
index e4a074b..fd0f64d 100644
--- a/core/src/main/java/org/elasticsearch/common/util/CollectionUtils.java
+++ b/core/src/main/java/org/elasticsearch/common/util/CollectionUtils.java
@@ -23,15 +23,12 @@ import com.carrotsearch.hppc.DoubleArrayList;
 import com.carrotsearch.hppc.FloatArrayList;
 import com.carrotsearch.hppc.LongArrayList;
 import com.carrotsearch.hppc.ObjectArrayList;
-import com.google.common.collect.Iterators;
 import org.apache.lucene.util.*;
 
 import java.util.*;
 
 /** Collections-related utility methods. */
-public enum CollectionUtils {
-    CollectionUtils;
-
+public class CollectionUtils {
     public static void sort(LongArrayList list) {
         sort(list.buffer, list.size());
     }
@@ -366,13 +363,6 @@ public enum CollectionUtils {
 
     }
 
-    /**
-     * Combines multiple iterators into a single iterator.
-     */
-    public static <T> Iterator<T> concat(Iterator<? extends T>... iterators) {
-        return Iterators.<T>concat(iterators);
-    }
-
     public static <E> ArrayList<E> iterableAsArrayList(Iterable<? extends E> elements) {
         if (elements == null) {
             throw new NullPointerException("elements");
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/XContentGenerator.java b/core/src/main/java/org/elasticsearch/common/xcontent/XContentGenerator.java
index a84ee83..a17ef93 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/XContentGenerator.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/XContentGenerator.java
@@ -21,7 +21,6 @@ package org.elasticsearch.common.xcontent;
 
 import org.elasticsearch.common.bytes.BytesReference;
 
-import java.io.Closeable;
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.OutputStream;
@@ -29,7 +28,7 @@ import java.io.OutputStream;
 /**
  *
  */
-public interface XContentGenerator extends Closeable {
+public interface XContentGenerator {
 
     XContentType contentType();
 
diff --git a/core/src/main/java/org/elasticsearch/index/IndexServicesProvider.java b/core/src/main/java/org/elasticsearch/index/IndexServicesProvider.java
index fe84284..0d5c3cb 100644
--- a/core/src/main/java/org/elasticsearch/index/IndexServicesProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/IndexServicesProvider.java
@@ -16,6 +16,7 @@
  * specific language governing permissions and limitations
  * under the License.
  */
+
 package org.elasticsearch.index;
 
 import org.elasticsearch.common.Nullable;
@@ -34,6 +35,7 @@ import org.elasticsearch.index.termvectors.TermVectorsService;
 import org.elasticsearch.indices.IndicesLifecycle;
 import org.elasticsearch.indices.IndicesWarmer;
 import org.elasticsearch.indices.cache.query.IndicesQueryCache;
+import org.elasticsearch.indices.memory.IndexingMemoryController;
 import org.elasticsearch.threadpool.ThreadPool;
 
 /**
@@ -58,9 +60,10 @@ public final class IndexServicesProvider {
     private final EngineFactory factory;
     private final BigArrays bigArrays;
     private final IndexSearcherWrapper indexSearcherWrapper;
+    private final IndexingMemoryController indexingMemoryController;
 
     @Inject
-    public IndexServicesProvider(IndicesLifecycle indicesLifecycle, ThreadPool threadPool, MapperService mapperService, IndexQueryParserService queryParserService, IndexCache indexCache, IndexAliasesService indexAliasesService, IndicesQueryCache indicesQueryCache, CodecService codecService, TermVectorsService termVectorsService, IndexFieldDataService indexFieldDataService, @Nullable IndicesWarmer warmer, SimilarityService similarityService, EngineFactory factory, BigArrays bigArrays, @Nullable IndexSearcherWrapper indexSearcherWrapper) {
+    public IndexServicesProvider(IndicesLifecycle indicesLifecycle, ThreadPool threadPool, MapperService mapperService, IndexQueryParserService queryParserService, IndexCache indexCache, IndexAliasesService indexAliasesService, IndicesQueryCache indicesQueryCache, CodecService codecService, TermVectorsService termVectorsService, IndexFieldDataService indexFieldDataService, @Nullable IndicesWarmer warmer, SimilarityService similarityService, EngineFactory factory, BigArrays bigArrays, @Nullable IndexSearcherWrapper indexSearcherWrapper, IndexingMemoryController indexingMemoryController) {
         this.indicesLifecycle = indicesLifecycle;
         this.threadPool = threadPool;
         this.mapperService = mapperService;
@@ -76,6 +79,7 @@ public final class IndexServicesProvider {
         this.factory = factory;
         this.bigArrays = bigArrays;
         this.indexSearcherWrapper = indexSearcherWrapper;
+        this.indexingMemoryController = indexingMemoryController;
     }
 
     public IndicesLifecycle getIndicesLifecycle() {
@@ -134,5 +138,11 @@ public final class IndexServicesProvider {
         return bigArrays;
     }
 
-    public IndexSearcherWrapper getIndexSearcherWrapper() { return indexSearcherWrapper; }
+    public IndexSearcherWrapper getIndexSearcherWrapper() {
+        return indexSearcherWrapper;
+    }
+
+    public IndexingMemoryController getIndexingMemoryController() {
+        return indexingMemoryController;
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/LocalNodeId.java b/core/src/main/java/org/elasticsearch/index/LocalNodeId.java
deleted file mode 100644
index a045636..0000000
--- a/core/src/main/java/org/elasticsearch/index/LocalNodeId.java
+++ /dev/null
@@ -1,40 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index;
-
-import org.elasticsearch.common.inject.BindingAnnotation;
-
-import java.lang.annotation.Documented;
-import java.lang.annotation.Retention;
-import java.lang.annotation.Target;
-
-import static java.lang.annotation.ElementType.FIELD;
-import static java.lang.annotation.ElementType.PARAMETER;
-import static java.lang.annotation.RetentionPolicy.RUNTIME;
-
-/**
- *
- */
-@BindingAnnotation
-@Target({FIELD, PARAMETER})
-@Retention(RUNTIME)
-@Documented
-public @interface LocalNodeId {
-}
diff --git a/core/src/main/java/org/elasticsearch/index/LocalNodeIdModule.java b/core/src/main/java/org/elasticsearch/index/LocalNodeIdModule.java
deleted file mode 100644
index 82e36cd..0000000
--- a/core/src/main/java/org/elasticsearch/index/LocalNodeIdModule.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index;
-
-import org.elasticsearch.common.inject.AbstractModule;
-
-/**
- *
- */
-public class LocalNodeIdModule extends AbstractModule {
-
-    private final String localNodeId;
-
-    public LocalNodeIdModule(String localNodeId) {
-        this.localNodeId = localNodeId;
-    }
-
-    @Override
-    protected void configure() {
-        bind(String.class).annotatedWith(LocalNodeId.class).toInstance(localNodeId);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/VersionType.java b/core/src/main/java/org/elasticsearch/index/VersionType.java
index a5d8cae..b8f998b 100644
--- a/core/src/main/java/org/elasticsearch/index/VersionType.java
+++ b/core/src/main/java/org/elasticsearch/index/VersionType.java
@@ -31,24 +31,37 @@ import java.io.IOException;
 public enum VersionType implements Writeable<VersionType> {
     INTERNAL((byte) 0) {
         @Override
-        public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion) {
-            return isVersionConflict(currentVersion, expectedVersion);
+        public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion, boolean deleted) {
+            return isVersionConflict(currentVersion, expectedVersion, deleted);
+        }
+
+        @Override
+        public String explainConflictForWrites(long currentVersion, long expectedVersion, boolean deleted) {
+            if (expectedVersion == Versions.MATCH_DELETED) {
+                return "document already exists (current version [" + currentVersion + "])";
+            }
+            return "current version [" + currentVersion + "] is different than the one provided [" + expectedVersion + "]";
         }
 
         @Override
         public boolean isVersionConflictForReads(long currentVersion, long expectedVersion) {
-            return isVersionConflict(currentVersion, expectedVersion);
+            return isVersionConflict(currentVersion, expectedVersion, false);
         }
 
-        private boolean isVersionConflict(long currentVersion, long expectedVersion) {
+        @Override
+        public String explainConflictForReads(long currentVersion, long expectedVersion) {
+            return "current version [" + currentVersion + "] is different than the one provided [" + expectedVersion + "]";
+        }
+
+        private boolean isVersionConflict(long currentVersion, long expectedVersion, boolean deleted) {
             if (currentVersion == Versions.NOT_SET) {
                 return false;
             }
             if (expectedVersion == Versions.MATCH_ANY) {
                 return false;
             }
-            if (currentVersion == Versions.NOT_FOUND) {
-                return true;
+            if (expectedVersion == Versions.MATCH_DELETED) {
+                return deleted == false;
             }
             if (currentVersion != expectedVersion) {
                 return true;
@@ -63,8 +76,7 @@ public enum VersionType implements Writeable<VersionType> {
 
         @Override
         public boolean validateVersionForWrites(long version) {
-            // not allowing Versions.NOT_FOUND as it is not a valid input value.
-            return version > 0L || version == Versions.MATCH_ANY;
+            return version > 0L || version == Versions.MATCH_ANY || version == Versions.MATCH_DELETED;
         }
 
         @Override
@@ -82,7 +94,7 @@ public enum VersionType implements Writeable<VersionType> {
     },
     EXTERNAL((byte) 1) {
         @Override
-        public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion) {
+        public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion, boolean deleted) {
             if (currentVersion == Versions.NOT_SET) {
                 return false;
             }
@@ -99,6 +111,11 @@ public enum VersionType implements Writeable<VersionType> {
         }
 
         @Override
+        public String explainConflictForWrites(long currentVersion, long expectedVersion, boolean deleted) {
+            return "current version [" + currentVersion + "] is higher or equal to the one provided [" + expectedVersion + "]";
+        }
+
+        @Override
         public boolean isVersionConflictForReads(long currentVersion, long expectedVersion) {
             if (currentVersion == Versions.NOT_SET) {
                 return false;
@@ -116,6 +133,11 @@ public enum VersionType implements Writeable<VersionType> {
         }
 
         @Override
+        public String explainConflictForReads(long currentVersion, long expectedVersion) {
+            return "current version [" + currentVersion + "] is different than the one provided [" + expectedVersion + "]";
+        }
+
+        @Override
         public long updateVersion(long currentVersion, long expectedVersion) {
             return expectedVersion;
         }
@@ -133,7 +155,7 @@ public enum VersionType implements Writeable<VersionType> {
     },
     EXTERNAL_GTE((byte) 2) {
         @Override
-        public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion) {
+        public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion, boolean deleted) {
             if (currentVersion == Versions.NOT_SET) {
                 return false;
             }
@@ -150,6 +172,11 @@ public enum VersionType implements Writeable<VersionType> {
         }
 
         @Override
+        public String explainConflictForWrites(long currentVersion, long expectedVersion, boolean deleted) {
+            return "current version [" + currentVersion + "] is higher than the one provided [" + expectedVersion + "]";
+        }
+
+        @Override
         public boolean isVersionConflictForReads(long currentVersion, long expectedVersion) {
             if (currentVersion == Versions.NOT_SET) {
                 return false;
@@ -167,6 +194,11 @@ public enum VersionType implements Writeable<VersionType> {
         }
 
         @Override
+        public String explainConflictForReads(long currentVersion, long expectedVersion) {
+            return "current version [" + currentVersion + "] is different than the one provided [" + expectedVersion + "]";
+        }
+
+        @Override
         public long updateVersion(long currentVersion, long expectedVersion) {
             return expectedVersion;
         }
@@ -187,7 +219,7 @@ public enum VersionType implements Writeable<VersionType> {
      */
     FORCE((byte) 3) {
         @Override
-        public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion) {
+        public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion, boolean deleted) {
             if (currentVersion == Versions.NOT_SET) {
                 return false;
             }
@@ -195,17 +227,27 @@ public enum VersionType implements Writeable<VersionType> {
                 return false;
             }
             if (expectedVersion == Versions.MATCH_ANY) {
-                return true;
+                throw new IllegalStateException("you must specify a version when use VersionType.FORCE");
             }
             return false;
         }
 
         @Override
+        public String explainConflictForWrites(long currentVersion, long expectedVersion, boolean deleted) {
+            throw new AssertionError("VersionType.FORCE should never result in a write conflict");
+        }
+
+        @Override
         public boolean isVersionConflictForReads(long currentVersion, long expectedVersion) {
             return false;
         }
 
         @Override
+        public String explainConflictForReads(long currentVersion, long expectedVersion) {
+            throw new AssertionError("VersionType.FORCE should never result in a read conflict");
+        }
+
+        @Override
         public long updateVersion(long currentVersion, long expectedVersion) {
             return expectedVersion;
         }
@@ -237,18 +279,47 @@ public enum VersionType implements Writeable<VersionType> {
     /**
      * Checks whether the current version conflicts with the expected version, based on the current version type.
      *
+     * @param currentVersion  the current version for the document
+     * @param expectedVersion the version specified for the write operation
+     * @param deleted         true if the document is currently deleted (note that #currentVersion will typically be
+     *                        {@link Versions#NOT_FOUND}, but may be something else if the document was recently deleted
      * @return true if versions conflict false o.w.
      */
-    public abstract boolean isVersionConflictForWrites(long currentVersion, long expectedVersion);
+    public abstract boolean isVersionConflictForWrites(long currentVersion, long expectedVersion, boolean deleted);
+
+
+    /**
+     * Returns a human readable explanation for a version conflict on write.
+     *
+     * Note that this method is only called if {@link #isVersionConflictForWrites(long, long, boolean)} returns true;
+     *
+     * @param currentVersion  the current version for the document
+     * @param expectedVersion the version specified for the write operation
+     * @param deleted         true if the document is currently deleted (note that #currentVersion will typically be
+     *                        {@link Versions#NOT_FOUND}, but may be something else if the document was recently deleted
+     */
+    public abstract String explainConflictForWrites(long currentVersion, long expectedVersion, boolean deleted);
 
     /**
      * Checks whether the current version conflicts with the expected version, based on the current version type.
      *
+     * @param currentVersion  the current version for the document
+     * @param expectedVersion the version specified for the read operation
      * @return true if versions conflict false o.w.
      */
     public abstract boolean isVersionConflictForReads(long currentVersion, long expectedVersion);
 
     /**
+     * Returns a human readable explanation for a version conflict on read.
+     *
+     * Note that this method is only called if {@link #isVersionConflictForReads(long, long)} returns true;
+     *
+     * @param currentVersion  the current version for the document
+     * @param expectedVersion the version specified for the read operation
+     */
+    public abstract String explainConflictForReads(long currentVersion, long expectedVersion);
+
+    /**
      * Returns the new version for a document, based on its current one and the specified in the request
      *
      * @return new version
diff --git a/core/src/main/java/org/elasticsearch/index/codec/postingsformat/Elasticsearch090PostingsFormat.java b/core/src/main/java/org/elasticsearch/index/codec/postingsformat/Elasticsearch090PostingsFormat.java
index b6ceba8..f7926c7 100644
--- a/core/src/main/java/org/elasticsearch/index/codec/postingsformat/Elasticsearch090PostingsFormat.java
+++ b/core/src/main/java/org/elasticsearch/index/codec/postingsformat/Elasticsearch090PostingsFormat.java
@@ -18,22 +18,17 @@
  */
 package org.elasticsearch.index.codec.postingsformat;
 
-import com.google.common.collect.Iterators;
 import org.apache.lucene.codecs.FieldsConsumer;
 import org.apache.lucene.codecs.FieldsProducer;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.FilterLeafReader;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
 import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.util.BloomFilter;
-import org.elasticsearch.index.codec.postingsformat.BloomFilterPostingsFormat.BloomFilteredFieldsConsumer;
 import org.elasticsearch.index.mapper.internal.UidFieldMapper;
 
 import java.io.IOException;
-import java.util.Iterator;
 import java.util.function.Predicate;
 
 /**
diff --git a/core/src/main/java/org/elasticsearch/index/engine/CreateFailedEngineException.java b/core/src/main/java/org/elasticsearch/index/engine/CreateFailedEngineException.java
deleted file mode 100644
index 32d9ee6..0000000
--- a/core/src/main/java/org/elasticsearch/index/engine/CreateFailedEngineException.java
+++ /dev/null
@@ -1,66 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.engine;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.index.shard.ShardId;
-
-import java.io.IOException;
-import java.util.Objects;
-
-/**
- *
- */
-public class CreateFailedEngineException extends EngineException {
-
-    private final String type;
-
-    private final String id;
-
-    public CreateFailedEngineException(ShardId shardId, String type, String id, Throwable cause) {
-        super(shardId, "Create failed for [" + type + "#" + id + "]", cause);
-        Objects.requireNonNull(type, "type must not be null");
-        Objects.requireNonNull(id, "id must not be null");
-        this.type = type;
-        this.id = id;
-    }
-
-    public CreateFailedEngineException(StreamInput in) throws IOException{
-        super(in);
-        type = in.readString();
-        id = in.readString();
-    }
-
-    public String type() {
-        return this.type;
-    }
-
-    public String id() {
-        return this.id;
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        out.writeString(type);
-        out.writeString(id);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/engine/DocumentAlreadyExistsException.java b/core/src/main/java/org/elasticsearch/index/engine/DocumentAlreadyExistsException.java
deleted file mode 100644
index 467dd8c..0000000
--- a/core/src/main/java/org/elasticsearch/index/engine/DocumentAlreadyExistsException.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.engine;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.rest.RestStatus;
-
-import java.io.IOException;
-
-/**
- *
- */
-public class DocumentAlreadyExistsException extends EngineException {
-
-    public DocumentAlreadyExistsException(ShardId shardId, String type, String id) {
-        super(shardId, "[" + type + "][" + id + "]: document already exists");
-    }
-
-    public DocumentAlreadyExistsException(StreamInput in) throws IOException{
-        super(in);
-    }
-
-    @Override
-    public RestStatus status() {
-        return RestStatus.CONFLICT;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/engine/Engine.java b/core/src/main/java/org/elasticsearch/index/engine/Engine.java
index 1330ef0..e9d61e1 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/Engine.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/Engine.java
@@ -45,7 +45,6 @@ import org.elasticsearch.index.mapper.ParseContext.Document;
 import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.Uid;
 import org.elasticsearch.index.merge.MergeStats;
-import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.index.store.Store;
 import org.elasticsearch.index.translog.Translog;
@@ -60,7 +59,6 @@ import java.util.concurrent.locks.Lock;
 import java.util.concurrent.locks.ReentrantLock;
 import java.util.concurrent.locks.ReentrantReadWriteLock;
 import java.util.function.Function;
-import java.util.function.Supplier;
 
 /**
  *
@@ -144,7 +142,8 @@ public abstract class Engine implements Closeable {
         return new MergeStats();
     }
 
-    /** A throttling class that can be activated, causing the
+    /**
+     * A throttling class that can be activated, causing the
      * {@code acquireThrottle} method to block on a lock when throttling
      * is enabled
      */
@@ -203,9 +202,7 @@ public abstract class Engine implements Closeable {
         }
     }
 
-    public abstract void create(Create create) throws EngineException;
-
-    public abstract boolean index(Index index) throws EngineException;
+    public abstract boolean index(Index operation) throws EngineException;
 
     public abstract void delete(Delete delete) throws EngineException;
 
@@ -216,7 +213,8 @@ public abstract class Engine implements Closeable {
     /**
      * Attempts to do a special commit where the given syncID is put into the commit data. The attempt
      * succeeds if there are not pending writes in lucene and the current point is equal to the expected one.
-     * @param syncId id of this sync
+     *
+     * @param syncId           id of this sync
      * @param expectedCommitId the expected value of
      * @return true if the sync commit was made, false o.w.
      */
@@ -243,7 +241,8 @@ public abstract class Engine implements Closeable {
             if (get.versionType().isVersionConflictForReads(docIdAndVersion.version, get.version())) {
                 Releasables.close(searcher);
                 Uid uid = Uid.createUid(get.uid().text());
-                throw new VersionConflictEngineException(shardId, uid.type(), uid.id(), docIdAndVersion.version, get.version());
+                throw new VersionConflictEngineException(shardId, uid.type(), uid.id(),
+                        get.versionType().explainConflictForReads(docIdAndVersion.version, get.version()));
             }
         }
 
@@ -328,7 +327,7 @@ public abstract class Engine implements Closeable {
         } catch (IOException e) {
             // Fall back to reading from the store if reading from the commit fails
             try {
-                return store. readLastCommittedSegmentsInfo();
+                return store.readLastCommittedSegmentsInfo();
             } catch (IOException e2) {
                 e2.addSuppressed(e);
                 throw e2;
@@ -366,6 +365,9 @@ public abstract class Engine implements Closeable {
         stats.addIndexWriterMaxMemoryInBytes(0);
     }
 
+    /** How much heap Lucene's IndexWriter is using */
+    abstract public long indexWriterRAMBytesUsed();
+
     protected Segment[] getSegmentInfo(SegmentInfos lastCommittedSegmentInfos, boolean verbose) {
         ensureOpen();
         Map<String, Segment> segments = new HashMap<>();
@@ -469,7 +471,8 @@ public abstract class Engine implements Closeable {
 
     /**
      * Flushes the state of the engine including the transaction log, clearing memory.
-     * @param force if <code>true</code> a lucene commit is executed even if no changes need to be committed.
+     *
+     * @param force         if <code>true</code> a lucene commit is executed even if no changes need to be committed.
      * @param waitIfOngoing if <code>true</code> this call will block until all currently running flushes have finished.
      *                      Otherwise this call will return without blocking.
      * @return the commit Id for the resulting commit
@@ -607,89 +610,43 @@ public abstract class Engine implements Closeable {
         }
     }
 
-    public static interface Operation {
-        static enum Type {
-            CREATE,
-            INDEX,
-            DELETE
-        }
-
-        static enum Origin {
-            PRIMARY,
-            REPLICA,
-            RECOVERY
-        }
-
-        Type opType();
-
-        Origin origin();
-    }
-
-    public static abstract class IndexingOperation implements Operation {
-
+    public static abstract class Operation {
         private final Term uid;
-        private final ParsedDocument doc;
         private long version;
         private final VersionType versionType;
         private final Origin origin;
         private Translog.Location location;
-
         private final long startTime;
         private long endTime;
 
-        public IndexingOperation(Term uid, ParsedDocument doc, long version, VersionType versionType, Origin origin, long startTime) {
+        public Operation(Term uid, long version, VersionType versionType, Origin origin, long startTime) {
             this.uid = uid;
-            this.doc = doc;
             this.version = version;
             this.versionType = versionType;
             this.origin = origin;
             this.startTime = startTime;
         }
 
-        public IndexingOperation(Term uid, ParsedDocument doc) {
-            this(uid, doc, Versions.MATCH_ANY, VersionType.INTERNAL, Origin.PRIMARY, System.nanoTime());
+        public static enum Origin {
+            PRIMARY,
+            REPLICA,
+            RECOVERY
         }
 
-        @Override
         public Origin origin() {
             return this.origin;
         }
 
-        public ParsedDocument parsedDoc() {
-            return this.doc;
-        }
-
         public Term uid() {
             return this.uid;
         }
 
-        public String type() {
-            return this.doc.type();
-        }
-
-        public String id() {
-            return this.doc.id();
-        }
-
-        public String routing() {
-            return this.doc.routing();
-        }
-
-        public long timestamp() {
-            return this.doc.timestamp();
-        }
-
-        public long ttl() {
-            return this.doc.ttl();
-        }
-
         public long version() {
             return this.version;
         }
 
         public void updateVersion(long version) {
             this.version = version;
-            this.doc.version().setLongValue(version);
         }
 
         public void setTranslogLocation(Translog.Location location) {
@@ -704,18 +661,6 @@ public abstract class Engine implements Closeable {
             return this.versionType;
         }
 
-        public String parent() {
-            return this.doc.parent();
-        }
-
-        public List<Document> docs() {
-            return this.doc.docs();
-        }
-
-        public BytesReference source() {
-            return this.doc.source();
-        }
-
         /**
          * Returns operation start time in nanoseconds.
          */
@@ -733,78 +678,77 @@ public abstract class Engine implements Closeable {
         public long endTime() {
             return this.endTime;
         }
-
-        /**
-         * Execute this operation against the provided {@link IndexShard} and
-         * return whether the document was created.
-         */
-        public abstract boolean execute(IndexShard shard);
     }
 
-    public static final class Create extends IndexingOperation {
+    public static class Index extends Operation {
+
+        private final ParsedDocument doc;
 
-        public Create(Term uid, ParsedDocument doc, long version, VersionType versionType, Origin origin, long startTime) {
-            super(uid, doc, version, versionType, origin, startTime);
+        public Index(Term uid, ParsedDocument doc, long version, VersionType versionType, Origin origin, long startTime) {
+            super(uid, version, versionType, origin, startTime);
+            this.doc = doc;
         }
 
-        public Create(Term uid, ParsedDocument doc) {
-            super(uid, doc);
+        public Index(Term uid, ParsedDocument doc) {
+            this(uid, doc, Versions.MATCH_ANY);
         }
 
-        @Override
-        public Type opType() {
-            return Type.CREATE;
+        public Index(Term uid, ParsedDocument doc, long version) {
+            this(uid, doc, version, VersionType.INTERNAL, Origin.PRIMARY, System.nanoTime());
         }
 
-        @Override
-        public boolean execute(IndexShard shard) {
-            shard.create(this);
-            return true;
+        public ParsedDocument parsedDoc() {
+            return this.doc;
         }
-    }
 
-    public static final class Index extends IndexingOperation {
+        public String type() {
+            return this.doc.type();
+        }
 
-        public Index(Term uid, ParsedDocument doc, long version, VersionType versionType, Origin origin, long startTime) {
-            super(uid, doc, version, versionType, origin, startTime);
+        public String id() {
+            return this.doc.id();
         }
 
-        public Index(Term uid, ParsedDocument doc) {
-            super(uid, doc);
+        public String routing() {
+            return this.doc.routing();
         }
 
-        @Override
-        public Type opType() {
-            return Type.INDEX;
+        public long timestamp() {
+            return this.doc.timestamp();
+        }
+
+        public long ttl() {
+            return this.doc.ttl();
         }
 
         @Override
-        public boolean execute(IndexShard shard) {
-            return shard.index(this);
+        public void updateVersion(long version) {
+            super.updateVersion(version);
+            this.doc.version().setLongValue(version);
+        }
+
+        public String parent() {
+            return this.doc.parent();
+        }
+
+        public List<Document> docs() {
+            return this.doc.docs();
+        }
+
+        public BytesReference source() {
+            return this.doc.source();
         }
     }
 
-    public static class Delete implements Operation {
+    public static class Delete extends Operation {
         private final String type;
         private final String id;
-        private final Term uid;
-        private long version;
-        private final VersionType versionType;
-        private final Origin origin;
         private boolean found;
 
-        private final long startTime;
-        private long endTime;
-        private Translog.Location location;
-
         public Delete(String type, String id, Term uid, long version, VersionType versionType, Origin origin, long startTime, boolean found) {
+            super(uid, version, versionType, origin, startTime);
             this.type = type;
             this.id = id;
-            this.uid = uid;
-            this.version = version;
-            this.versionType = versionType;
-            this.origin = origin;
-            this.startTime = startTime;
             this.found = found;
         }
 
@@ -816,16 +760,6 @@ public abstract class Engine implements Closeable {
             this(template.type(), template.id(), template.uid(), template.version(), versionType, template.origin(), template.startTime(), template.found());
         }
 
-        @Override
-        public Type opType() {
-            return Type.DELETE;
-        }
-
-        @Override
-        public Origin origin() {
-            return this.origin;
-        }
-
         public String type() {
             return this.type;
         }
@@ -834,55 +768,14 @@ public abstract class Engine implements Closeable {
             return this.id;
         }
 
-        public Term uid() {
-            return this.uid;
-        }
-
         public void updateVersion(long version, boolean found) {
-            this.version = version;
+            updateVersion(version);
             this.found = found;
         }
 
-        /**
-         * before delete execution this is the version to be deleted. After this is the version of the "delete" transaction record.
-         */
-        public long version() {
-            return this.version;
-        }
-
-        public VersionType versionType() {
-            return this.versionType;
-        }
-
         public boolean found() {
             return this.found;
         }
-
-        /**
-         * Returns operation start time in nanoseconds.
-         */
-        public long startTime() {
-            return this.startTime;
-        }
-
-        public void endTime(long endTime) {
-            this.endTime = endTime;
-        }
-
-        /**
-         * Returns operation end time in nanoseconds.
-         */
-        public long endTime() {
-            return this.endTime;
-        }
-
-        public void setTranslogLocation(Translog.Location location) {
-            this.location = location;
-        }
-
-        public Translog.Location getTranslogLocation() {
-            return this.location;
-        }
     }
 
     public static class DeleteByQuery {
@@ -1135,12 +1028,18 @@ public abstract class Engine implements Closeable {
 
         @Override
         public boolean equals(Object o) {
-            if (this == o) return true;
-            if (o == null || getClass() != o.getClass()) return false;
+            if (this == o) {
+                return true;
+            }
+            if (o == null || getClass() != o.getClass()) {
+                return false;
+            }
 
             CommitId commitId = (CommitId) o;
 
-            if (!Arrays.equals(id, commitId.id)) return false;
+            if (!Arrays.equals(id, commitId.id)) {
+                return false;
+            }
 
             return true;
         }
@@ -1151,5 +1050,6 @@ public abstract class Engine implements Closeable {
         }
     }
 
-    public void onSettingsChanged() {}
+    public void onSettingsChanged() {
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/engine/EngineConfig.java b/core/src/main/java/org/elasticsearch/index/engine/EngineConfig.java
index a79587e..fd4b5da 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/EngineConfig.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/EngineConfig.java
@@ -40,6 +40,7 @@ import org.elasticsearch.index.shard.TranslogRecoveryPerformer;
 import org.elasticsearch.index.store.Store;
 import org.elasticsearch.index.translog.TranslogConfig;
 import org.elasticsearch.indices.IndicesWarmer;
+import org.elasticsearch.indices.memory.IndexingMemoryController;
 import org.elasticsearch.threadpool.ThreadPool;
 
 import java.util.concurrent.TimeUnit;
@@ -107,8 +108,6 @@ public final class EngineConfig {
 
     public static final TimeValue DEFAULT_REFRESH_INTERVAL = new TimeValue(1, TimeUnit.SECONDS);
     public static final TimeValue DEFAULT_GC_DELETES = TimeValue.timeValueSeconds(60);
-    public static final ByteSizeValue DEFAULT_INDEX_BUFFER_SIZE = new ByteSizeValue(64, ByteSizeUnit.MB);
-    public static final ByteSizeValue INACTIVE_SHARD_INDEXING_BUFFER = ByteSizeValue.parseBytesSizeValue("500kb", "INACTIVE_SHARD_INDEXING_BUFFER");
 
     public static final String DEFAULT_VERSION_MAP_SIZE = "25%";
 
@@ -139,7 +138,8 @@ public final class EngineConfig {
         this.failedEngineListener = failedEngineListener;
         this.compoundOnFlush = indexSettings.getAsBoolean(EngineConfig.INDEX_COMPOUND_ON_FLUSH, compoundOnFlush);
         codecName = indexSettings.get(EngineConfig.INDEX_CODEC_SETTING, EngineConfig.DEFAULT_CODEC_NAME);
-        indexingBufferSize = DEFAULT_INDEX_BUFFER_SIZE;
+        // We start up inactive and rely on IndexingMemoryController to give us our fair share once we start indexing:
+        indexingBufferSize = IndexingMemoryController.INACTIVE_SHARD_INDEXING_BUFFER;
         gcDeletesInMillis = indexSettings.getAsTime(INDEX_GC_DELETES_SETTING, EngineConfig.DEFAULT_GC_DELETES).millis();
         versionMapSizeSetting = indexSettings.get(INDEX_VERSION_MAP_SIZE, DEFAULT_VERSION_MAP_SIZE);
         updateVersionMapSize();
@@ -258,10 +258,10 @@ public final class EngineConfig {
 
     /**
      * Returns a {@link org.elasticsearch.index.indexing.ShardIndexingService} used inside the engine to inform about
-     * pre and post index and create operations. The operations are used for statistic purposes etc.
+     * pre and post index. The operations are used for statistic purposes etc.
      *
-     * @see org.elasticsearch.index.indexing.ShardIndexingService#postCreate(org.elasticsearch.index.engine.Engine.Create)
-     * @see org.elasticsearch.index.indexing.ShardIndexingService#preCreate(org.elasticsearch.index.engine.Engine.Create)
+     * @see org.elasticsearch.index.indexing.ShardIndexingService#postIndex(Engine.Index)
+     * @see org.elasticsearch.index.indexing.ShardIndexingService#preIndex(Engine.Index)
      *
      */
     public ShardIndexingService getIndexingService() {
diff --git a/core/src/main/java/org/elasticsearch/index/engine/EngineException.java b/core/src/main/java/org/elasticsearch/index/engine/EngineException.java
index d7487ef..23f6be7 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/EngineException.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/EngineException.java
@@ -30,16 +30,16 @@ import java.io.IOException;
  */
 public class EngineException extends ElasticsearchException {
 
-    public EngineException(ShardId shardId, String msg) {
-        this(shardId, msg, null);
+    public EngineException(ShardId shardId, String msg, Object... params) {
+        this(shardId, msg, null, params);
     }
 
-    public EngineException(ShardId shardId, String msg, Throwable cause) {
-        super(msg, cause);
+    public EngineException(ShardId shardId, String msg, Throwable cause, Object... params) {
+        super(msg, cause, params);
         setShard(shardId);
     }
 
-    public EngineException(StreamInput in) throws IOException{
+    public EngineException(StreamInput in) throws IOException {
         super(in);
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java b/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java
index 227212d..237857e 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java
@@ -316,7 +316,8 @@ public class InternalEngine extends Engine {
                     }
                     if (get.versionType().isVersionConflictForReads(versionValue.version(), get.version())) {
                         Uid uid = Uid.createUid(get.uid().text());
-                        throw new VersionConflictEngineException(shardId, uid.type(), uid.id(), versionValue.version(), get.version());
+                        throw new VersionConflictEngineException(shardId, uid.type(), uid.id(),
+                                get.versionType().explainConflictForReads(versionValue.version(), get.version()));
                     }
                     Translog.Operation op = translog.read(versionValue.translogLocation());
                     if (op != null) {
@@ -331,96 +332,7 @@ public class InternalEngine extends Engine {
     }
 
     @Override
-    public void create(Create create) throws EngineException {
-        try (ReleasableLock lock = readLock.acquire()) {
-            ensureOpen();
-            if (create.origin() == Operation.Origin.RECOVERY) {
-                // Don't throttle recovery operations
-                innerCreate(create);
-            } else {
-                try (Releasable r = throttle.acquireThrottle()) {
-                    innerCreate(create);
-                }
-            }
-        } catch (OutOfMemoryError | IllegalStateException | IOException t) {
-            maybeFailEngine("create", t);
-            throw new CreateFailedEngineException(shardId, create.type(), create.id(), t);
-        }
-        checkVersionMapRefresh();
-    }
-
-    private void innerCreate(Create create) throws IOException {
-        synchronized (dirtyLock(create.uid())) {
-            final long currentVersion;
-            final VersionValue versionValue;
-            versionValue = versionMap.getUnderLock(create.uid().bytes());
-            if (versionValue == null) {
-                currentVersion = loadCurrentVersionFromIndex(create.uid());
-            } else {
-                if (engineConfig.isEnableGcDeletes() && versionValue.delete() && (engineConfig.getThreadPool().estimatedTimeInMillis() - versionValue.time()) > engineConfig.getGcDeletesInMillis()) {
-                    currentVersion = Versions.NOT_FOUND; // deleted, and GC
-                } else {
-                    currentVersion = versionValue.version();
-                }
-            }
-            innerCreateUnderLock(create, currentVersion, versionValue);
-        }
-    }
-
-    private void innerCreateUnderLock(Create create, long currentVersion, VersionValue versionValue) throws IOException {
-
-        // same logic as index
-        long updatedVersion;
-        long expectedVersion = create.version();
-        if (create.versionType().isVersionConflictForWrites(currentVersion, expectedVersion)) {
-            if (create.origin() == Operation.Origin.RECOVERY) {
-                return;
-            } else {
-                throw new VersionConflictEngineException(shardId, create.type(), create.id(), currentVersion, expectedVersion);
-            }
-        }
-        updatedVersion = create.versionType().updateVersion(currentVersion, expectedVersion);
-
-        // if the doc exists
-        boolean doUpdate = false;
-        if ((versionValue != null && versionValue.delete() == false) || (versionValue == null && currentVersion != Versions.NOT_FOUND)) {
-            if (create.origin() == Operation.Origin.RECOVERY) {
-                return;
-            } else if (create.origin() == Operation.Origin.REPLICA) {
-                // #7142: the primary already determined it's OK to index this document, and we confirmed above that the version doesn't
-                // conflict, so we must also update here on the replica to remain consistent:
-                doUpdate = true;
-            } else {
-                // On primary, we throw DAEE if the _uid is already in the index with an older version:
-                assert create.origin() == Operation.Origin.PRIMARY;
-                throw new DocumentAlreadyExistsException(shardId, create.type(), create.id());
-            }
-        }
-
-        create.updateVersion(updatedVersion);
-
-        if (doUpdate) {
-            if (create.docs().size() > 1) {
-                indexWriter.updateDocuments(create.uid(), create.docs());
-            } else {
-                indexWriter.updateDocument(create.uid(), create.docs().get(0));
-            }
-        } else {
-            if (create.docs().size() > 1) {
-                indexWriter.addDocuments(create.docs());
-            } else {
-                indexWriter.addDocument(create.docs().get(0));
-            }
-        }
-        Translog.Location translogLocation = translog.add(new Translog.Create(create));
-
-        versionMap.putUnderLock(create.uid().bytes(), new VersionValue(updatedVersion, translogLocation));
-        create.setTranslogLocation(translogLocation);
-        indexingService.postCreateUnderLock(create);
-    }
-
-    @Override
-    public boolean index(Index index) throws EngineException {
+    public boolean index(Index index) {
         final boolean created;
         try (ReleasableLock lock = readLock.acquire()) {
             ensureOpen();
@@ -440,40 +352,16 @@ public class InternalEngine extends Engine {
         return created;
     }
 
-    /**
-     * Forces a refresh if the versionMap is using too much RAM
-     */
-    private void checkVersionMapRefresh() {
-        if (versionMap.ramBytesUsedForRefresh() > config().getVersionMapSize().bytes() && versionMapRefreshPending.getAndSet(true) == false) {
-            try {
-                if (isClosed.get()) {
-                    // no point...
-                    return;
-                }
-                // Now refresh to clear versionMap:
-                engineConfig.getThreadPool().executor(ThreadPool.Names.REFRESH).execute(new Runnable() {
-                    @Override
-                    public void run() {
-                        try {
-                            refresh("version_table_full");
-                        } catch (EngineClosedException ex) {
-                            // ignore
-                        }
-                    }
-                });
-            } catch (EsRejectedExecutionException ex) {
-                // that is fine too.. we might be shutting down
-            }
-        }
-    }
-
     private boolean innerIndex(Index index) throws IOException {
         synchronized (dirtyLock(index.uid())) {
             final long currentVersion;
+            final boolean deleted;
             VersionValue versionValue = versionMap.getUnderLock(index.uid().bytes());
             if (versionValue == null) {
                 currentVersion = loadCurrentVersionFromIndex(index.uid());
+                deleted = currentVersion == Versions.NOT_FOUND;
             } else {
+                deleted = versionValue.delete();
                 if (engineConfig.isEnableGcDeletes() && versionValue.delete() && (engineConfig.getThreadPool().estimatedTimeInMillis() - versionValue.time()) > engineConfig.getGcDeletesInMillis()) {
                     currentVersion = Versions.NOT_FOUND; // deleted, and GC
                 } else {
@@ -481,19 +369,20 @@ public class InternalEngine extends Engine {
                 }
             }
 
-            long updatedVersion;
             long expectedVersion = index.version();
-            if (index.versionType().isVersionConflictForWrites(currentVersion, expectedVersion)) {
+            if (index.versionType().isVersionConflictForWrites(currentVersion, expectedVersion, deleted)) {
                 if (index.origin() == Operation.Origin.RECOVERY) {
                     return false;
                 } else {
-                    throw new VersionConflictEngineException(shardId, index.type(), index.id(), currentVersion, expectedVersion);
+                    throw new VersionConflictEngineException(shardId, index.type(), index.id(),
+                            index.versionType().explainConflictForWrites(currentVersion, expectedVersion, deleted));
                 }
             }
-            updatedVersion = index.versionType().updateVersion(currentVersion, expectedVersion);
+            long updatedVersion = index.versionType().updateVersion(currentVersion, expectedVersion);
 
             final boolean created;
             index.updateVersion(updatedVersion);
+
             if (currentVersion == Versions.NOT_FOUND) {
                 // document does not exists, we can optimize for create
                 created = true;
@@ -518,11 +407,39 @@ public class InternalEngine extends Engine {
 
             versionMap.putUnderLock(index.uid().bytes(), new VersionValue(updatedVersion, translogLocation));
             index.setTranslogLocation(translogLocation);
+
             indexingService.postIndexUnderLock(index);
             return created;
         }
     }
 
+    /**
+     * Forces a refresh if the versionMap is using too much RAM
+     */
+    private void checkVersionMapRefresh() {
+        if (versionMap.ramBytesUsedForRefresh() > config().getVersionMapSize().bytes() && versionMapRefreshPending.getAndSet(true) == false) {
+            try {
+                if (isClosed.get()) {
+                    // no point...
+                    return;
+                }
+                // Now refresh to clear versionMap:
+                engineConfig.getThreadPool().executor(ThreadPool.Names.REFRESH).execute(new Runnable() {
+                    @Override
+                    public void run() {
+                        try {
+                            refresh("version_table_full");
+                        } catch (EngineClosedException ex) {
+                            // ignore
+                        }
+                    }
+                });
+            } catch (EsRejectedExecutionException ex) {
+                // that is fine too.. we might be shutting down
+            }
+        }
+    }
+
     @Override
     public void delete(Delete delete) throws EngineException {
         try (ReleasableLock lock = readLock.acquire()) {
@@ -549,10 +466,13 @@ public class InternalEngine extends Engine {
     private void innerDelete(Delete delete) throws IOException {
         synchronized (dirtyLock(delete.uid())) {
             final long currentVersion;
+            final boolean deleted;
             VersionValue versionValue = versionMap.getUnderLock(delete.uid().bytes());
             if (versionValue == null) {
                 currentVersion = loadCurrentVersionFromIndex(delete.uid());
+                deleted = currentVersion == Versions.NOT_FOUND;
             } else {
+                deleted = versionValue.delete();
                 if (engineConfig.isEnableGcDeletes() && versionValue.delete() && (engineConfig.getThreadPool().estimatedTimeInMillis() - versionValue.time()) > engineConfig.getGcDeletesInMillis()) {
                     currentVersion = Versions.NOT_FOUND; // deleted, and GC
                 } else {
@@ -562,11 +482,12 @@ public class InternalEngine extends Engine {
 
             long updatedVersion;
             long expectedVersion = delete.version();
-            if (delete.versionType().isVersionConflictForWrites(currentVersion, expectedVersion)) {
+            if (delete.versionType().isVersionConflictForWrites(currentVersion, expectedVersion, deleted)) {
                 if (delete.origin() == Operation.Origin.RECOVERY) {
                     return;
                 } else {
-                    throw new VersionConflictEngineException(shardId, delete.type(), delete.id(), currentVersion, expectedVersion);
+                    throw new VersionConflictEngineException(shardId, delete.type(), delete.id(),
+                            delete.versionType().explainConflictForWrites(currentVersion, expectedVersion, deleted));
                 }
             }
             updatedVersion = delete.versionType().updateVersion(currentVersion, expectedVersion);
@@ -905,6 +826,11 @@ public class InternalEngine extends Engine {
     }
 
     @Override
+    public long indexWriterRAMBytesUsed() {
+        return indexWriter.ramBytesUsed();
+    }
+
+    @Override
     public List<Segment> segments(boolean verbose) {
         try (ReleasableLock lock = readLock.acquire()) {
             Segment[] segmentsArr = getSegmentInfo(lastCommittedSegmentInfos, verbose);
diff --git a/core/src/main/java/org/elasticsearch/index/engine/Segment.java b/core/src/main/java/org/elasticsearch/index/engine/Segment.java
index cbccaa1..7d3882f 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/Segment.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/Segment.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.index.engine;
 
-import com.google.common.collect.Iterators;
 import org.apache.lucene.util.Accountable;
 import org.apache.lucene.util.Accountables;
 import org.elasticsearch.common.Nullable;
@@ -32,7 +31,6 @@ import org.elasticsearch.common.unit.ByteSizeValue;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collection;
-import java.util.Iterator;
 import java.util.List;
 
 public class Segment implements Streamable {
diff --git a/core/src/main/java/org/elasticsearch/index/engine/ShadowEngine.java b/core/src/main/java/org/elasticsearch/index/engine/ShadowEngine.java
index 7588ffa..3892958 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/ShadowEngine.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/ShadowEngine.java
@@ -103,11 +103,6 @@ public class ShadowEngine extends Engine {
 
 
     @Override
-    public void create(Create create) throws EngineException {
-        throw new UnsupportedOperationException(shardId + " create operation not allowed on shadow engine");
-    }
-
-    @Override
     public boolean index(Index index) throws EngineException {
         throw new UnsupportedOperationException(shardId + " index operation not allowed on shadow engine");
     }
@@ -245,4 +240,9 @@ public class ShadowEngine extends Engine {
         return lastCommittedSegmentInfos;
     }
 
+    @Override
+    public long indexWriterRAMBytesUsed() {
+        // No IndexWriter
+        throw new UnsupportedOperationException("ShadowEngine has no IndexWriter");
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/engine/VersionConflictEngineException.java b/core/src/main/java/org/elasticsearch/index/engine/VersionConflictEngineException.java
index 8c2d352..9b038c6 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/VersionConflictEngineException.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/VersionConflictEngineException.java
@@ -29,8 +29,16 @@ import java.io.IOException;
  */
 public class VersionConflictEngineException extends EngineException {
 
-    public VersionConflictEngineException(ShardId shardId, String type, String id, long current, long provided) {
-        super(shardId, "[" + type + "][" + id + "]: version conflict, current [" + current + "], provided [" + provided + "]");
+    public VersionConflictEngineException(ShardId shardId, String type, String id, String explanation) {
+        this(shardId, null, type, id, explanation);
+    }
+
+    public VersionConflictEngineException(ShardId shardId, Throwable cause, String type, String id, String explanation) {
+        this(shardId, "[{}][{}]: version conflict, {}", cause, type, id, explanation);
+    }
+
+    public VersionConflictEngineException(ShardId shardId, String msg, Throwable cause, Object... params) {
+        super(shardId, msg, cause, params);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/index/indexing/IndexingOperationListener.java b/core/src/main/java/org/elasticsearch/index/indexing/IndexingOperationListener.java
index 858453f..651bc40 100644
--- a/core/src/main/java/org/elasticsearch/index/indexing/IndexingOperationListener.java
+++ b/core/src/main/java/org/elasticsearch/index/indexing/IndexingOperationListener.java
@@ -28,39 +28,8 @@ public abstract class IndexingOperationListener {
     /**
      * Called before the indexing occurs.
      */
-    public Engine.Create preCreate(Engine.Create create) {
-        return create;
-    }
-
-    /**
-     * Called after the indexing occurs, under a locking scheme to maintain
-     * concurrent updates to the same doc.
-     * <p>
-     * Note, long operations should not occur under this callback.
-     */
-    public void postCreateUnderLock(Engine.Create create) {
-
-    }
-
-    /**
-     * Called after create index operation occurred.
-     */
-    public void postCreate(Engine.Create create) {
-
-    }
-
-    /**
-     * Called after create index operation occurred with exception.
-     */
-    public void postCreate(Engine.Create create, Throwable ex) {
-
-    }
-
-    /**
-     * Called before the indexing occurs.
-     */
-    public Engine.Index preIndex(Engine.Index index) {
-        return index;
+    public Engine.Index preIndex(Engine.Index operation) {
+        return operation;
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/index/indexing/IndexingSlowLog.java b/core/src/main/java/org/elasticsearch/index/indexing/IndexingSlowLog.java
index ea45db2..292c2a1 100644
--- a/core/src/main/java/org/elasticsearch/index/indexing/IndexingSlowLog.java
+++ b/core/src/main/java/org/elasticsearch/index/indexing/IndexingSlowLog.java
@@ -128,10 +128,6 @@ public final class IndexingSlowLog {
         postIndexing(index.parsedDoc(), tookInNanos);
     }
 
-    void postCreate(Engine.Create create, long tookInNanos) {
-        postIndexing(create.parsedDoc(), tookInNanos);
-    }
-
     /**
      * Reads how much of the source to log. The user can specify any value they
      * like and numbers are interpreted the maximum number of characters to log
diff --git a/core/src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java b/core/src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java
index 4f0ea77..d1abbf1 100644
--- a/core/src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java
+++ b/core/src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java
@@ -85,25 +85,6 @@ public class ShardIndexingService extends AbstractIndexShardComponent {
         listeners.remove(listener);
     }
 
-    public Engine.Create preCreate(Engine.Create create) {
-        totalStats.indexCurrent.inc();
-        typeStats(create.type()).indexCurrent.inc();
-        for (IndexingOperationListener listener : listeners) {
-            create = listener.preCreate(create);
-        }
-        return create;
-    }
-
-    public void postCreateUnderLock(Engine.Create create) {
-        for (IndexingOperationListener listener : listeners) {
-            try {
-                listener.postCreateUnderLock(create);
-            } catch (Exception e) {
-                logger.warn("postCreateUnderLock listener [{}] failed", e, listener);
-            }
-        }
-    }
-
     public void throttlingActivated() {
         totalStats.setThrottled(true);
     }
@@ -112,40 +93,13 @@ public class ShardIndexingService extends AbstractIndexShardComponent {
         totalStats.setThrottled(false);
     }
 
-    public void postCreate(Engine.Create create) {
-        long took = create.endTime() - create.startTime();
-        totalStats.indexMetric.inc(took);
-        totalStats.indexCurrent.dec();
-        StatsHolder typeStats = typeStats(create.type());
-        typeStats.indexMetric.inc(took);
-        typeStats.indexCurrent.dec();
-        slowLog.postCreate(create, took);
-        for (IndexingOperationListener listener : listeners) {
-            try {
-                listener.postCreate(create);
-            } catch (Exception e) {
-                logger.warn("postCreate listener [{}] failed", e, listener);
-            }
-        }
-    }
-
-    public void postCreate(Engine.Create create, Throwable ex) {
-        for (IndexingOperationListener listener : listeners) {
-            try {
-                listener.postCreate(create, ex);
-            } catch (Throwable t) {
-                logger.warn("postCreate listener [{}] failed", t, listener);
-            }
-        }
-    }
-
-    public Engine.Index preIndex(Engine.Index index) {
+    public Engine.Index preIndex(Engine.Index operation) {
         totalStats.indexCurrent.inc();
-        typeStats(index.type()).indexCurrent.inc();
+        typeStats(operation.type()).indexCurrent.inc();
         for (IndexingOperationListener listener : listeners) {
-            index = listener.preIndex(index);
+            operation = listener.preIndex(operation);
         }
-        return index;
+        return operation;
     }
 
     public void postIndexUnderLock(Engine.Index index) {
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java b/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java
index bb991ef..82ff5fb 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java
@@ -43,7 +43,7 @@ import org.elasticsearch.index.mapper.ip.IpFieldMapper;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.index.mapper.object.RootObjectMapper;
 import org.elasticsearch.index.settings.IndexSettings;
-import org.elasticsearch.index.similarity.SimilarityLookupService;
+import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptService;
 
@@ -57,7 +57,7 @@ public class DocumentMapperParser {
     final MapperService mapperService;
     final AnalysisService analysisService;
     private static final ESLogger logger = Loggers.getLogger(DocumentMapperParser.class);
-    private final SimilarityLookupService similarityLookupService;
+    private final SimilarityService similarityService;
     private final ScriptService scriptService;
 
     private final RootObjectMapper.TypeParser rootObjectTypeParser = new RootObjectMapper.TypeParser();
@@ -71,12 +71,12 @@ public class DocumentMapperParser {
     private volatile SortedMap<String, Mapper.TypeParser> additionalRootMappers;
 
     public DocumentMapperParser(@IndexSettings Settings indexSettings, MapperService mapperService, AnalysisService analysisService,
-                                SimilarityLookupService similarityLookupService, ScriptService scriptService) {
+                                SimilarityService similarityService, ScriptService scriptService) {
         this.indexSettings = indexSettings;
         this.parseFieldMatcher = new ParseFieldMatcher(indexSettings);
         this.mapperService = mapperService;
         this.analysisService = analysisService;
-        this.similarityLookupService = similarityLookupService;
+        this.similarityService = similarityService;
         this.scriptService = scriptService;
         MapBuilder<String, Mapper.TypeParser> typeParsersBuilder = new MapBuilder<String, Mapper.TypeParser>()
                 .put(ByteFieldMapper.CONTENT_TYPE, new ByteFieldMapper.TypeParser())
@@ -142,7 +142,7 @@ public class DocumentMapperParser {
     }
 
     public Mapper.TypeParser.ParserContext parserContext(String type) {
-        return new Mapper.TypeParser.ParserContext(type, analysisService, similarityLookupService, mapperService, typeParsers, indexVersionCreated, parseFieldMatcher);
+        return new Mapper.TypeParser.ParserContext(type, analysisService, similarityService::getSimilarity, mapperService, typeParsers::get, indexVersionCreated, parseFieldMatcher);
     }
 
     public DocumentMapper parse(String source) throws MapperParsingException {
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java b/core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java
index db2919e..97435e0 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java
@@ -122,7 +122,7 @@ class DocumentParser implements Closeable {
                 // entire type is disabled
                 parser.skipChildren();
             } else if (emptyDoc == false) {
-                Mapper update = parseObject(context, mapping.root);
+                Mapper update = parseObject(context, mapping.root, true);
                 if (update != null) {
                     context.addDynamicMappingsUpdate(update);
                 }
@@ -194,7 +194,7 @@ class DocumentParser implements Closeable {
         return doc;
     }
 
-    static ObjectMapper parseObject(ParseContext context, ObjectMapper mapper) throws IOException {
+    static ObjectMapper parseObject(ParseContext context, ObjectMapper mapper, boolean atRoot) throws IOException {
         if (mapper.isEnabled() == false) {
             context.parser().skipChildren();
             return null;
@@ -202,6 +202,10 @@ class DocumentParser implements Closeable {
         XContentParser parser = context.parser();
 
         String currentFieldName = parser.currentName();
+        if (atRoot && MapperService.isMetadataField(currentFieldName) &&
+            Version.indexCreated(context.indexSettings()).onOrAfter(Version.V_2_0_0_beta1)) {
+            throw new MapperParsingException("Field [" + currentFieldName + "] is a metadata field and cannot be added inside a document. Use the index API request parameters.");
+        }
         XContentParser.Token token = parser.currentToken();
         if (token == XContentParser.Token.VALUE_NULL) {
             // the object is null ("obj1" : null), simply bail
@@ -302,7 +306,7 @@ class DocumentParser implements Closeable {
 
     private static Mapper parseObjectOrField(ParseContext context, Mapper mapper) throws IOException {
         if (mapper instanceof ObjectMapper) {
-            return parseObject(context, (ObjectMapper) mapper);
+            return parseObject(context, (ObjectMapper) mapper, false);
         } else {
             FieldMapper fieldMapper = (FieldMapper)mapper;
             Mapper update = fieldMapper.parse(context);
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java
index ec53fba..45bef68 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java
@@ -34,8 +34,8 @@ import org.elasticsearch.index.analysis.NamedAnalyzer;
 import org.elasticsearch.index.fielddata.FieldDataType;
 import org.elasticsearch.index.mapper.core.TypeParsers;
 import org.elasticsearch.index.mapper.internal.AllFieldMapper;
-import org.elasticsearch.index.similarity.SimilarityLookupService;
 import org.elasticsearch.index.similarity.SimilarityProvider;
+import org.elasticsearch.index.similarity.SimilarityService;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -447,7 +447,7 @@ public abstract class FieldMapper extends Mapper {
         if (fieldType().similarity() != null) {
             builder.field("similarity", fieldType().similarity().name());
         } else if (includeDefaults) {
-            builder.field("similarity", SimilarityLookupService.DEFAULT_SIMILARITY);
+            builder.field("similarity", SimilarityService.DEFAULT_SIMILARITY);
         }
 
         if (includeDefaults || hasCustomFieldDataSettings()) {
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/Mapper.java b/core/src/main/java/org/elasticsearch/index/mapper/Mapper.java
index ab56146..9ca34e1 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/Mapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/Mapper.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.index.mapper;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseFieldMatcher;
@@ -27,9 +26,10 @@ import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.index.analysis.AnalysisService;
-import org.elasticsearch.index.similarity.SimilarityLookupService;
+import org.elasticsearch.index.similarity.SimilarityProvider;
 
 import java.util.Map;
+import java.util.function.Function;
 
 public abstract class Mapper implements ToXContent, Iterable<Mapper> {
 
@@ -85,18 +85,18 @@ public abstract class Mapper implements ToXContent, Iterable<Mapper> {
 
             private final AnalysisService analysisService;
 
-            private final SimilarityLookupService similarityLookupService;
+            private final Function<String, SimilarityProvider> similarityLookupService;
 
             private final MapperService mapperService;
 
-            private final ImmutableMap<String, TypeParser> typeParsers;
+            private final Function<String, TypeParser> typeParsers;
 
             private final Version indexVersionCreated;
 
             private final ParseFieldMatcher parseFieldMatcher;
 
-            public ParserContext(String type, AnalysisService analysisService, SimilarityLookupService similarityLookupService,
-                                 MapperService mapperService, ImmutableMap<String, TypeParser> typeParsers,
+            public ParserContext(String type, AnalysisService analysisService,  Function<String, SimilarityProvider> similarityLookupService,
+                                 MapperService mapperService, Function<String, TypeParser> typeParsers,
                                  Version indexVersionCreated, ParseFieldMatcher parseFieldMatcher) {
                 this.type = type;
                 this.analysisService = analysisService;
@@ -115,8 +115,8 @@ public abstract class Mapper implements ToXContent, Iterable<Mapper> {
                 return analysisService;
             }
 
-            public SimilarityLookupService similarityLookupService() {
-                return similarityLookupService;
+            public SimilarityProvider getSimilarity(String name) {
+                return similarityLookupService.apply(name);
             }
 
             public MapperService mapperService() {
@@ -124,7 +124,7 @@ public abstract class Mapper implements ToXContent, Iterable<Mapper> {
             }
 
             public TypeParser typeParser(String type) {
-                return typeParsers.get(Strings.toUnderscoreCase(type));
+                return typeParsers.apply(Strings.toUnderscoreCase(type));
             }
 
             public Version indexVersionCreated() {
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/MapperService.java b/core/src/main/java/org/elasticsearch/index/mapper/MapperService.java
index dd31c6e..256a673 100755
--- a/core/src/main/java/org/elasticsearch/index/mapper/MapperService.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/MapperService.java
@@ -21,7 +21,6 @@ package org.elasticsearch.index.mapper;
 
 import com.carrotsearch.hppc.ObjectHashSet;
 import com.google.common.collect.ImmutableMap;
-import com.google.common.collect.Iterators;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.DelegatingAnalyzerWrapper;
@@ -39,6 +38,7 @@ import org.elasticsearch.ElasticsearchGenerationException;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
+import org.elasticsearch.common.collect.Iterators;
 import org.elasticsearch.common.compress.CompressedXContent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.lucene.search.Queries;
@@ -52,7 +52,7 @@ import org.elasticsearch.index.mapper.Mapper.BuilderContext;
 import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.index.settings.IndexSettings;
-import org.elasticsearch.index.similarity.SimilarityLookupService;
+import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.indices.InvalidTypeNameException;
 import org.elasticsearch.indices.TypeMissingException;
 import org.elasticsearch.percolator.PercolatorService;
@@ -72,6 +72,7 @@ import java.util.Set;
 import java.util.concurrent.CopyOnWriteArrayList;
 import java.util.concurrent.locks.ReentrantReadWriteLock;
 import java.util.function.Function;
+import java.util.stream.Collectors;
 
 import static java.util.Collections.emptySet;
 import static java.util.Collections.unmodifiableSet;
@@ -124,12 +125,12 @@ public class MapperService extends AbstractIndexComponent implements Closeable {
 
     @Inject
     public MapperService(Index index, @IndexSettings Settings indexSettings, AnalysisService analysisService,
-                         SimilarityLookupService similarityLookupService,
+                         SimilarityService similarityService,
                          ScriptService scriptService) {
         super(index, indexSettings);
         this.analysisService = analysisService;
         this.fieldTypes = new FieldTypeLookup();
-        this.documentParser = new DocumentMapperParser(indexSettings, this, analysisService, similarityLookupService, scriptService);
+        this.documentParser = new DocumentMapperParser(indexSettings, this, analysisService, similarityService, scriptService);
         this.indexAnalyzer = new MapperAnalyzerWrapper(analysisService.defaultIndexAnalyzer(), p -> p.indexAnalyzer());
         this.searchAnalyzer = new MapperAnalyzerWrapper(analysisService.defaultSearchAnalyzer(), p -> p.searchAnalyzer());
         this.searchQuoteAnalyzer = new MapperAnalyzerWrapper(analysisService.defaultSearchQuoteAnalyzer(), p -> p.searchQuoteAnalyzer());
@@ -184,13 +185,13 @@ public class MapperService extends AbstractIndexComponent implements Closeable {
      */
     public Iterable<DocumentMapper> docMappers(final boolean includingDefaultMapping) {
         return () -> {
-            final Iterator<DocumentMapper> iterator;
+            final Collection<DocumentMapper> documentMappers;
             if (includingDefaultMapping) {
-                iterator = mappers.values().iterator();
+                documentMappers = mappers.values();
             } else {
-                iterator = mappers.values().stream().filter(mapper -> !DEFAULT_MAPPING.equals(mapper.type())).iterator();
+                documentMappers = mappers.values().stream().filter(mapper -> !DEFAULT_MAPPING.equals(mapper.type())).collect(Collectors.toList());
             }
-            return Iterators.unmodifiableIterator(iterator);
+            return Collections.unmodifiableCollection(documentMappers).iterator();
         };
     }
 
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java b/core/src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java
index 0588bd1..3f142cc 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java
@@ -173,7 +173,7 @@ public class TypeParsers {
                 builder.omitNorms(nodeBooleanValue(propNode));
                 iterator.remove();
             } else if (propName.equals("similarity")) {
-                builder.similarity(parserContext.similarityLookupService().similarity(propNode.toString()));
+                builder.similarity(parserContext.getSimilarity(propNode.toString()));
                 iterator.remove();
             } else if (parseMultiField(builder, name, parserContext, propName, propNode)) {
                 iterator.remove();
@@ -277,7 +277,7 @@ public class TypeParsers {
                 // ignore for old indexes
                 iterator.remove();
             } else if (propName.equals("similarity")) {
-                builder.similarity(parserContext.similarityLookupService().similarity(propNode.toString()));
+                builder.similarity(parserContext.getSimilarity(propNode.toString()));
                 iterator.remove();
             } else if (propName.equals("fielddata")) {
                 final Settings settings = Settings.builder().put(SettingsLoader.Helper.loadNestedFromMap(nodeMapValue(propNode, "fielddata"))).build();
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java
index 4111786..b264bfa 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java
@@ -21,7 +21,6 @@ package org.elasticsearch.index.mapper.geo;
 
 import com.carrotsearch.hppc.ObjectHashSet;
 import com.carrotsearch.hppc.cursors.ObjectCursor;
-import com.google.common.collect.Iterators;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexOptions;
 import org.apache.lucene.util.BytesRef;
@@ -30,6 +29,7 @@ import org.apache.lucene.util.XGeoHashUtils;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.Explicit;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.collect.Iterators;
 import org.elasticsearch.common.geo.GeoDistance;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.geo.GeoUtils;
@@ -39,14 +39,7 @@ import org.elasticsearch.common.util.ByteUtils;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.support.XContentMapValues;
-import org.elasticsearch.index.mapper.ContentPath;
-import org.elasticsearch.index.mapper.FieldMapper;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.Mapper;
-import org.elasticsearch.index.mapper.MapperParsingException;
-import org.elasticsearch.index.mapper.MergeMappingException;
-import org.elasticsearch.index.mapper.MergeResult;
-import org.elasticsearch.index.mapper.ParseContext;
+import org.elasticsearch.index.mapper.*;
 import org.elasticsearch.index.mapper.core.DoubleFieldMapper;
 import org.elasticsearch.index.mapper.core.NumberFieldMapper;
 import org.elasticsearch.index.mapper.core.NumberFieldMapper.CustomNumericDocValuesField;
@@ -54,18 +47,10 @@ import org.elasticsearch.index.mapper.core.StringFieldMapper;
 import org.elasticsearch.index.mapper.object.ArrayValueMapperParser;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-
-import static org.elasticsearch.index.mapper.MapperBuilders.doubleField;
-import static org.elasticsearch.index.mapper.MapperBuilders.geoPointField;
-import static org.elasticsearch.index.mapper.MapperBuilders.stringField;
-import static org.elasticsearch.index.mapper.core.TypeParsers.parseField;
-import static org.elasticsearch.index.mapper.core.TypeParsers.parseMultiField;
-import static org.elasticsearch.index.mapper.core.TypeParsers.parsePathType;
+import java.util.*;
+
+import static org.elasticsearch.index.mapper.MapperBuilders.*;
+import static org.elasticsearch.index.mapper.core.TypeParsers.*;
 
 /**
  * Parsing: We handle:
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java
index e538a00..59b664d 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java
@@ -41,7 +41,7 @@ import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.MetadataFieldMapper;
 import org.elasticsearch.index.mapper.ParseContext;
 import org.elasticsearch.index.query.QueryShardContext;
-import org.elasticsearch.index.similarity.SimilarityLookupService;
+import org.elasticsearch.index.similarity.SimilarityService;
 
 import java.io.IOException;
 import java.util.Iterator;
@@ -300,7 +300,7 @@ public class AllFieldMapper extends MetadataFieldMapper {
         if (fieldType().similarity() != null) {
             builder.field("similarity", fieldType().similarity().name());
         } else if (includeDefaults) {
-            builder.field("similarity", SimilarityLookupService.DEFAULT_SIMILARITY);
+            builder.field("similarity", SimilarityService.DEFAULT_SIMILARITY);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java
index 1ac34df..1d73139 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.index.mapper.ip;
 
-import com.google.common.net.InetAddresses;
 import org.apache.lucene.analysis.NumericTokenStream;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexOptions;
@@ -29,6 +28,7 @@ import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.BytesRefBuilder;
 import org.apache.lucene.util.NumericUtils;
 import org.elasticsearch.common.Explicit;
+import org.elasticsearch.common.network.InetAddresses;
 import org.elasticsearch.common.Numbers;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.settings.Settings;
diff --git a/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java b/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java
index d811f1f..1f8a4c6 100644
--- a/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java
+++ b/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java
@@ -242,29 +242,12 @@ public final class PercolatorQueriesRegistry extends AbstractIndexShardComponent
     private class RealTimePercolatorOperationListener extends IndexingOperationListener {
 
         @Override
-        public Engine.Create preCreate(Engine.Create create) {
+        public Engine.Index preIndex(Engine.Index operation) {
             // validate the query here, before we index
-            if (PercolatorService.TYPE_NAME.equals(create.type())) {
-                parsePercolatorDocument(create.id(), create.source());
+            if (PercolatorService.TYPE_NAME.equals(operation.type())) {
+                parsePercolatorDocument(operation.id(), operation.source());
             }
-            return create;
-        }
-
-        @Override
-        public void postCreateUnderLock(Engine.Create create) {
-            // add the query under a doc lock
-            if (PercolatorService.TYPE_NAME.equals(create.type())) {
-                addPercolateQuery(create.id(), create.source());
-            }
-        }
-
-        @Override
-        public Engine.Index preIndex(Engine.Index index) {
-            // validate the query here, before we index
-            if (PercolatorService.TYPE_NAME.equals(index.type())) {
-                parsePercolatorDocument(index.id(), index.source());
-            }
-            return index;
+            return operation;
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java
index 9a224fa..a265427 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java
@@ -202,7 +202,7 @@ public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistan
     }
     
     /** Returns validation method for coordinates. */
-    public GeoValidationMethod getValidationMethod(GeoValidationMethod method) {
+    public GeoValidationMethod getValidationMethod() {
         return this.validationMethod;
     }
 
@@ -221,6 +221,7 @@ public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistan
             }
         }
 
+        GeoPoint point = new GeoPoint(this.point);
         if (GeoValidationMethod.isCoerce(validationMethod)) {
             GeoUtils.normalizePoint(point, true, true);
         }
diff --git a/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java
index 3339c97..1de8db2 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java
@@ -30,12 +30,7 @@ import org.elasticsearch.index.mapper.Uid;
 import org.elasticsearch.index.mapper.internal.UidFieldMapper;
 
 import java.io.IOException;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.Objects;
-import java.util.Set;
+import java.util.*;
 
 /**
  * A query that will return only documents matching specific ids (and a type).
@@ -133,14 +128,14 @@ public class IdsQueryBuilder extends AbstractQueryBuilder<IdsQueryBuilder> {
 
     @Override
     protected IdsQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        IdsQueryBuilder idsQueryBuilder = new IdsQueryBuilder(in.readOptionalStringArray());
+        IdsQueryBuilder idsQueryBuilder = new IdsQueryBuilder(in.readStringArray());
         idsQueryBuilder.addIds(in.readStringArray());
         return idsQueryBuilder;
     }
 
     @Override
     protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeOptionalStringArray(types);
+        out.writeStringArray(types);
         out.writeStringArray(ids.toArray(new String[ids.size()]));
     }
 
diff --git a/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java b/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java
index 5be406e..bbd9f84 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java
@@ -149,31 +149,10 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         return this.queryStringLenient;
     }
 
-    public IndicesQueriesRegistry indicesQueriesRegistry() {
+    IndicesQueriesRegistry indicesQueriesRegistry() {
         return indicesQueriesRegistry;
     }
 
-    public ParsedQuery parse(QueryBuilder<?> queryBuilder) {
-        QueryShardContext context = cache.get();
-        context.reset();
-        context.parseFieldMatcher(parseFieldMatcher);
-        try {
-            return innerParse(context, queryBuilder);
-        } catch (ParsingException e) {
-            throw e;
-        } catch (Exception e) {
-            throw new QueryShardException(context, "failed to create query: {}", e, queryBuilder);
-        }
-    }
-
-    private static ParsedQuery innerParse(QueryShardContext context, QueryBuilder<?> queryBuilder) throws IOException, QueryShardException {
-        Query query = queryBuilder.toQuery(context);
-        if (query == null) {
-            query = Queries.newMatchNoDocsQuery();
-        }
-        return new ParsedQuery(query, context.copyNamedQueries());
-    }
-
     public ParsedQuery parse(BytesReference source) {
         QueryShardContext context = cache.get();
         XContentParser parser = null;
@@ -279,7 +258,7 @@ public class IndexQueryParserService extends AbstractIndexComponent {
 
     public Query parseInnerQuery(QueryShardContext context) throws IOException {
         return toQuery(context.parseContext().parseInnerQueryBuilder(), context);
-            }
+    }
 
     public ParsedQuery toQuery(QueryBuilder<?> queryBuilder) {
         QueryShardContext context = cache.get();
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java b/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java
index 7862bb2..5b12b2d 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java
@@ -148,7 +148,7 @@ public class QueryShardContext {
     }
 
     public Similarity searchSimilarity() {
-        return indexQueryParser.similarityService != null ? indexQueryParser.similarityService.similarity() : null;
+        return indexQueryParser.similarityService != null ? indexQueryParser.similarityService.similarity(indexQueryParser.mapperService) : null;
     }
 
     public String defaultField() {
diff --git a/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
index 3c72adf..0df2460 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
@@ -18,25 +18,16 @@
  */
 package org.elasticsearch.index.query;
 
-import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.common.HasContextAndHeaders;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.lease.Releasables;
-import org.elasticsearch.common.xcontent.XContent;
-import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.script.*;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.script.Template;
 
 import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
 
-import static org.elasticsearch.common.Strings.hasLength;
-
 /**
  * In the simplest case, parse template string and variables from the request,
  * compile the template and execute the template against the given variables.
@@ -99,6 +90,4 @@ public class TemplateQueryParser implements QueryParser<TemplateQueryBuilder> {
     public TemplateQueryBuilder getBuilderPrototype() {
         return TemplateQueryBuilder.PROTOTYPE;
     }
-
-
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/exp/ExponentialDecayFunctionBuilder.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/exp/ExponentialDecayFunctionBuilder.java
index 3c81393..e133abd 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/exp/ExponentialDecayFunctionBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/exp/ExponentialDecayFunctionBuilder.java
@@ -27,7 +27,7 @@ import org.elasticsearch.index.query.functionscore.DecayFunctionBuilder;
 
 public class ExponentialDecayFunctionBuilder extends DecayFunctionBuilder<ExponentialDecayFunctionBuilder> {
 
-    private static final DecayFunction EXP_DECAY_FUNCTION = new ExponentialDecayScoreFunction();
+    public static final DecayFunction EXP_DECAY_FUNCTION = new ExponentialDecayScoreFunction();
 
     public ExponentialDecayFunctionBuilder(String fieldName, Object origin, Object scale, Object offset) {
         super(fieldName, origin, scale, offset);
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/gauss/GaussDecayFunctionBuilder.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/gauss/GaussDecayFunctionBuilder.java
index 621b22a..618503a 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/gauss/GaussDecayFunctionBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/gauss/GaussDecayFunctionBuilder.java
@@ -27,7 +27,7 @@ import org.elasticsearch.index.query.functionscore.DecayFunctionBuilder;
 
 public class GaussDecayFunctionBuilder extends DecayFunctionBuilder<GaussDecayFunctionBuilder> {
 
-    private static final DecayFunction GAUSS_DECAY_FUNCTION = new GaussScoreFunction();
+    public static final DecayFunction GAUSS_DECAY_FUNCTION = new GaussScoreFunction();
 
     public GaussDecayFunctionBuilder(String fieldName, Object origin, Object scale, Object offset) {
         super(fieldName, origin, scale, offset);
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/lin/LinearDecayFunctionBuilder.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/lin/LinearDecayFunctionBuilder.java
index 2e63aed..f321ee1 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/lin/LinearDecayFunctionBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/lin/LinearDecayFunctionBuilder.java
@@ -26,7 +26,7 @@ import org.elasticsearch.index.query.functionscore.DecayFunctionBuilder;
 
 public class LinearDecayFunctionBuilder extends DecayFunctionBuilder<LinearDecayFunctionBuilder> {
 
-    private static final DecayFunction LINEAR_DECAY_FUNCTION = new LinearDecayScoreFunction();
+    public static final DecayFunction LINEAR_DECAY_FUNCTION = new LinearDecayScoreFunction();
 
     public LinearDecayFunctionBuilder(String fieldName, Object origin, Object scale, Object offset) {
         super(fieldName, origin, scale, offset);
diff --git a/core/src/main/java/org/elasticsearch/index/search/stats/SearchSlowLog.java b/core/src/main/java/org/elasticsearch/index/search/stats/SearchSlowLog.java
index 108dab4..cfb7402 100644
--- a/core/src/main/java/org/elasticsearch/index/search/stats/SearchSlowLog.java
+++ b/core/src/main/java/org/elasticsearch/index/search/stats/SearchSlowLog.java
@@ -189,11 +189,24 @@ public final class SearchSlowLog{
                 sb.append("], ");
             }
             sb.append("search_type[").append(context.searchType()).append("], total_shards[").append(context.numberOfShards()).append("], ");
-            if (context.request().source() != null) {
-                sb.append("source[").append(context.request().source()).append("], ");
+            if (context.request().source() != null && context.request().source().length() > 0) {
+                try {
+                    sb.append("source[").append(XContentHelper.convertToJson(context.request().source(), reformat)).append("], ");
+                } catch (IOException e) {
+                    sb.append("source[_failed_to_convert_], ");
+                }
             } else {
                 sb.append("source[], ");
             }
+            if (context.request().extraSource() != null && context.request().extraSource().length() > 0) {
+                try {
+                    sb.append("extra_source[").append(XContentHelper.convertToJson(context.request().extraSource(), reformat)).append("], ");
+                } catch (IOException e) {
+                    sb.append("extra_source[_failed_to_convert_], ");
+                }
+            } else {
+                sb.append("extra_source[], ");
+            }
             return sb.toString();
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
index ea2d555..2497231 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
@@ -33,7 +33,6 @@ import org.elasticsearch.action.admin.indices.optimize.OptimizeRequest;
 import org.elasticsearch.action.admin.indices.upgrade.post.UpgradeRequest;
 import org.elasticsearch.action.termvectors.TermVectorsRequest;
 import org.elasticsearch.action.termvectors.TermVectorsResponse;
-import org.elasticsearch.bootstrap.Elasticsearch;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.routing.ShardRouting;
@@ -43,6 +42,7 @@ import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
 import org.elasticsearch.common.logging.ESLogger;
+import org.elasticsearch.common.logging.support.LoggerMessageFormat;
 import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.metrics.MeanMetric;
 import org.elasticsearch.common.settings.Settings;
@@ -84,8 +84,8 @@ import org.elasticsearch.index.settings.IndexSettings;
 import org.elasticsearch.index.settings.IndexSettingsService;
 import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.index.snapshots.IndexShardRepository;
-import org.elasticsearch.index.store.Store;
 import org.elasticsearch.index.store.Store.MetadataSnapshot;
+import org.elasticsearch.index.store.Store;
 import org.elasticsearch.index.store.StoreFileMetaData;
 import org.elasticsearch.index.store.StoreStats;
 import org.elasticsearch.index.suggest.stats.ShardSuggestMetric;
@@ -100,6 +100,7 @@ import org.elasticsearch.index.warmer.WarmerStats;
 import org.elasticsearch.indices.IndicesWarmer;
 import org.elasticsearch.indices.InternalIndicesLifecycle;
 import org.elasticsearch.indices.cache.query.IndicesQueryCache;
+import org.elasticsearch.indices.memory.IndexingMemoryController;
 import org.elasticsearch.indices.recovery.RecoveryFailedException;
 import org.elasticsearch.indices.recovery.RecoveryState;
 import org.elasticsearch.percolator.PercolatorService;
@@ -118,6 +119,7 @@ import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicReference;
 
+
 public class IndexShard extends AbstractIndexShardComponent implements IndexSettingsService.Listener {
 
     private final ThreadPool threadPool;
@@ -190,6 +192,13 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
 
     private final IndexSearcherWrapper searcherWrapper;
 
+    /** True if this shard is still indexing (recently) and false if we've been idle for long enough (as periodically checked by {@link
+     *  IndexingMemoryController}). */
+    private final AtomicBoolean active = new AtomicBoolean();
+
+    private volatile long lastWriteNS;
+    private final IndexingMemoryController indexingMemoryController;
+
     @Inject
     public IndexShard(ShardId shardId, @IndexSettings Settings indexSettings, ShardPath path, Store store, IndexServicesProvider provider) {
         super(shardId, indexSettings);
@@ -242,11 +251,16 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
         this.flushThresholdSize = indexSettings.getAsBytesSize(INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE, new ByteSizeValue(512, ByteSizeUnit.MB));
         this.disableFlush = indexSettings.getAsBoolean(INDEX_TRANSLOG_DISABLE_FLUSH, false);
         this.indexShardOperationCounter = new IndexShardOperationCounter(logger, shardId);
+        this.indexingMemoryController = provider.getIndexingMemoryController();
+
         this.searcherWrapper = provider.getIndexSearcherWrapper();
         this.percolatorQueriesRegistry = new PercolatorQueriesRegistry(shardId, indexSettings, queryParserService, indexingService, mapperService, indexFieldDataService);
         if (mapperService.hasMapping(PercolatorService.TYPE_NAME)) {
             percolatorQueriesRegistry.enableRealTimePercolator();
         }
+
+        // We start up inactive
+        active.set(false);
     }
 
     public Store store() {
@@ -278,7 +292,9 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
         return indexFieldDataService;
     }
 
-    public MapperService mapperService() { return mapperService;}
+    public MapperService mapperService() {
+        return mapperService;
+    }
 
     public ShardSearchStats searchService() {
         return this.searchService;
@@ -423,40 +439,6 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
         return previousState;
     }
 
-    public Engine.Create prepareCreate(SourceToParse source, long version, VersionType versionType, Engine.Operation.Origin origin) {
-        try {
-            return prepareCreate(docMapper(source.type()), source, version, versionType, origin);
-        } catch (Throwable t) {
-            verifyNotClosed(t);
-            throw t;
-        }
-    }
-
-    static Engine.Create prepareCreate(DocumentMapperForType docMapper, SourceToParse source, long version, VersionType versionType, Engine.Operation.Origin origin) {
-        long startTime = System.nanoTime();
-        ParsedDocument doc = docMapper.getDocumentMapper().parse(source);
-        if (docMapper.getMapping() != null) {
-            doc.addDynamicMappingsUpdate(docMapper.getMapping());
-        }
-        return new Engine.Create(docMapper.getDocumentMapper().uidMapper().term(doc.uid().stringValue()), doc, version, versionType, origin, startTime);
-    }
-
-    public void create(Engine.Create create) {
-        writeAllowed(create.origin());
-        create = indexingService.preCreate(create);
-        try {
-            if (logger.isTraceEnabled()) {
-                logger.trace("index [{}][{}]{}", create.type(), create.id(), create.docs());
-            }
-            getEngine().create(create);
-            create.endTime(System.nanoTime());
-        } catch (Throwable ex) {
-            indexingService.postCreate(create, ex);
-            throw ex;
-        }
-        indexingService.postCreate(create);
-    }
-
     public Engine.Index prepareIndex(SourceToParse source, long version, VersionType versionType, Engine.Operation.Origin origin) {
         try {
             return prepareIndex(docMapper(source.type()), source, version, versionType, origin);
@@ -480,7 +462,8 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
      * updated.
      */
     public boolean index(Engine.Index index) {
-        writeAllowed(index.origin());
+        ensureWriteAllowed(index);
+        markLastWrite(index);
         index = indexingService.preIndex(index);
         final boolean created;
         try {
@@ -504,7 +487,8 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
     }
 
     public void delete(Engine.Delete delete) {
-        writeAllowed(delete.origin());
+        ensureWriteAllowed(delete);
+        markLastWrite(delete);
         delete = indexingService.preDelete(delete);
         try {
             if (logger.isTraceEnabled()) {
@@ -914,7 +898,24 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
         }
     }
 
-    private void writeAllowed(Engine.Operation.Origin origin) throws IllegalIndexShardStateException {
+    /** Returns timestamp of last indexing operation */
+    public long getLastWriteNS() {
+        return lastWriteNS;
+    }
+
+    /** Records timestamp of the last write operation, possibly switching {@code active} to true if we were inactive. */
+    private void markLastWrite(Engine.Operation op) {
+        lastWriteNS = op.startTime();
+        if (active.getAndSet(true) == false) {
+            // We are currently inactive, but a new write operation just showed up, so we now notify IMC
+            // to wake up and fix our indexing buffer.  We could do this async instead, but cost should
+            // be low, and it's rare this happens.
+            indexingMemoryController.forceCheck();
+        }
+    }
+
+    private void ensureWriteAllowed(Engine.Operation op) throws IllegalIndexShardStateException {
+        Engine.Operation.Origin origin = op.origin();
         IndexShardState state = this.state; // one time volatile read
 
         if (origin == Engine.Operation.Origin.PRIMARY) {
@@ -976,6 +977,8 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
         this.failedEngineListener.delegates.add(failedEngineListener);
     }
 
+    /** Change the indexing and translog buffer sizes.  If {@code IndexWriter} is currently using more than
+     *  the new buffering indexing size then we do a refresh to free up the heap. */
     public void updateBufferSize(ByteSizeValue shardIndexingBufferSize, ByteSizeValue shardTranslogBufferSize) {
 
         final EngineConfig config = engineConfig;
@@ -994,27 +997,50 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
             // so we push changes these changes down to IndexWriter:
             engine.onSettingsChanged();
 
-            if (shardIndexingBufferSize == EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER) {
-                // it's inactive: make sure we do a refresh / full IW flush in this case, since the memory
-                // changes only after a "data" change has happened to the writer
-                // the index writer lazily allocates memory and a refresh will clean it all up.
-                logger.debug("updating index_buffer_size from [{}] to (inactive) [{}]", preValue, shardIndexingBufferSize);
+            long iwBytesUsed = engine.indexWriterRAMBytesUsed();
+
+            String message = LoggerMessageFormat.format("updating index_buffer_size from [{}] to [{}]; IndexWriter now using [{}] bytes",
+                                                        preValue, shardIndexingBufferSize, iwBytesUsed);
+
+            if (iwBytesUsed > shardIndexingBufferSize.bytes()) {
+                // our allowed buffer was changed to less than we are currently using; we ask IW to refresh
+                // so it clears its buffers (otherwise it won't clear until the next indexing/delete op)
+                logger.debug(message + "; now refresh to clear IndexWriter memory");
+
+                // TODO: should IW have an API to move segments to disk, but not refresh?  Its flush method is protected...
                 try {
                     refresh("update index buffer");
                 } catch (Throwable e) {
-                    logger.warn("failed to refresh after setting shard to inactive", e);
+                    logger.warn("failed to refresh after decreasing index buffer", e);
                 }
             } else {
-                logger.debug("updating index_buffer_size from [{}] to [{}]", preValue, shardIndexingBufferSize);
+                logger.debug(message);
             }
         }
 
         engine.getTranslog().updateBuffer(shardTranslogBufferSize);
     }
 
-    public void markAsInactive() {
-        updateBufferSize(EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER, TranslogConfig.INACTIVE_SHARD_TRANSLOG_BUFFER);
-        indicesLifecycle.onShardInactive(this);
+    /** Called by {@link IndexingMemoryController} to check whether more than {@code inactiveTimeNS} has passed since the last
+     *  indexing operation, and become inactive (reducing indexing and translog buffers to tiny values) if so.  This returns true
+     *  if the shard is inactive. */
+    public boolean checkIdle(long inactiveTimeNS) {
+        if (System.nanoTime() - lastWriteNS >= inactiveTimeNS) {
+            boolean wasActive = active.getAndSet(false);
+            if (wasActive) {
+                updateBufferSize(IndexingMemoryController.INACTIVE_SHARD_INDEXING_BUFFER, IndexingMemoryController.INACTIVE_SHARD_TRANSLOG_BUFFER);
+                logger.debug("shard is now inactive");
+                indicesLifecycle.onShardInactive(this);
+            }
+        }
+
+        return active.get() == false;
+    }
+
+    /** Returns {@code true} if this shard is active (has seen indexing ops in the last {@link
+     *  IndexingMemoryController#SHARD_INACTIVE_TIME_SETTING} (default 5 minutes), else {@code false}. */
+    public boolean getActive() {
+        return active.get();
     }
 
     public final boolean isFlushOnClose() {
@@ -1426,7 +1452,7 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
         };
         return new EngineConfig(shardId,
                 threadPool, indexingService, indexSettings, warmer, store, deletionPolicy, mergePolicyConfig.getMergePolicy(), mergeSchedulerConfig,
-                mapperService.indexAnalyzer(), similarityService.similarity(), codecService, failedEngineListener, translogRecoveryPerformer, indexCache.query(), cachingPolicy, translogConfig);
+                mapperService.indexAnalyzer(), similarityService.similarity(mapperService), codecService, failedEngineListener, translogRecoveryPerformer, indexCache.query(), cachingPolicy, translogConfig);
     }
 
     private static class IndexShardOperationCounter extends AbstractRefCounted {
@@ -1499,6 +1525,7 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
     /**
      * Schedules a flush if needed but won't schedule more than one flush concurrently. The flush will be executed on the
      * Flush thread-pool asynchronously.
+     *
      * @return <code>true</code> if a new flush is scheduled otherwise <code>false</code>.
      */
     public boolean maybeFlush() {
diff --git a/core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java b/core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java
index c81b9e5..8bdf1fb 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java
@@ -18,6 +18,8 @@
  */
 package org.elasticsearch.index.shard;
 
+import java.io.IOException;
+
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.IndexServicesProvider;
@@ -26,8 +28,7 @@ import org.elasticsearch.index.engine.EngineConfig;
 import org.elasticsearch.index.merge.MergeStats;
 import org.elasticsearch.index.settings.IndexSettings;
 import org.elasticsearch.index.store.Store;
-
-import java.io.IOException;
+import org.elasticsearch.index.translog.TranslogStats;
 
 /**
  * ShadowIndexShard extends {@link IndexShard} to add file synchronization
@@ -82,4 +83,9 @@ public final class ShadowIndexShard extends IndexShard {
     public boolean allowsPrimaryPromotion() {
         return false;
     }
+
+    @Override
+    public TranslogStats translogStats() {
+        return null; // shadow engine has no translog
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java b/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java
index f893ec4..23551df 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java
@@ -145,19 +145,7 @@ public class TranslogRecoveryPerformer {
     public void performRecoveryOperation(Engine engine, Translog.Operation operation, boolean allowMappingUpdates) {
         try {
             switch (operation.opType()) {
-                case CREATE:
-                    Translog.Create create = (Translog.Create) operation;
-                    Engine.Create engineCreate = IndexShard.prepareCreate(docMapper(create.type()),
-                            source(create.source()).index(shardId.getIndex()).type(create.type()).id(create.id())
-                                    .routing(create.routing()).parent(create.parent()).timestamp(create.timestamp()).ttl(create.ttl()),
-                            create.version(), create.versionType().versionTypeForReplicationAndRecovery(), Engine.Operation.Origin.RECOVERY);
-                    maybeAddMappingUpdate(engineCreate.type(), engineCreate.parsedDoc().dynamicMappingsUpdate(), engineCreate.id(), allowMappingUpdates);
-                    if (logger.isTraceEnabled()) {
-                        logger.trace("[translog] recover [create] op of [{}][{}]", create.type(), create.id());
-                    }
-                    engine.create(engineCreate);
-                    break;
-                case SAVE:
+                case INDEX:
                     Translog.Index index = (Translog.Index) operation;
                     Engine.Index engineIndex = IndexShard.prepareIndex(docMapper(index.type()), source(index.source()).type(index.type()).id(index.id())
                                     .routing(index.routing()).parent(index.parent()).timestamp(index.timestamp()).ttl(index.ttl()),
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/BM25SimilarityProvider.java b/core/src/main/java/org/elasticsearch/index/similarity/BM25SimilarityProvider.java
index 1983c4e..68e50da 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/BM25SimilarityProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/BM25SimilarityProvider.java
@@ -21,8 +21,6 @@ package org.elasticsearch.index.similarity;
 
 import org.apache.lucene.search.similarities.BM25Similarity;
 import org.apache.lucene.search.similarities.Similarity;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.inject.assistedinject.Assisted;
 import org.elasticsearch.common.settings.Settings;
 
 /**
@@ -40,8 +38,7 @@ public class BM25SimilarityProvider extends AbstractSimilarityProvider {
 
     private final BM25Similarity similarity;
 
-    @Inject
-    public BM25SimilarityProvider(@Assisted String name, @Assisted Settings settings) {
+    public BM25SimilarityProvider(String name, Settings settings) {
         super(name);
         float k1 = settings.getAsFloat("k1", 1.2f);
         float b = settings.getAsFloat("b", 0.75f);
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/DFRSimilarityProvider.java b/core/src/main/java/org/elasticsearch/index/similarity/DFRSimilarityProvider.java
index b5a5cdc..10ba1d4 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/DFRSimilarityProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/DFRSimilarityProvider.java
@@ -62,8 +62,7 @@ public class DFRSimilarityProvider extends AbstractSimilarityProvider {
 
     private final DFRSimilarity similarity;
 
-    @Inject
-    public DFRSimilarityProvider(@Assisted String name, @Assisted Settings settings) {
+    public DFRSimilarityProvider(String name, Settings settings) {
         super(name);
         BasicModel basicModel = parseBasicModel(settings);
         AfterEffect afterEffect = parseAfterEffect(settings);
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/DefaultSimilarityProvider.java b/core/src/main/java/org/elasticsearch/index/similarity/DefaultSimilarityProvider.java
index 0f9feba..3acbd98 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/DefaultSimilarityProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/DefaultSimilarityProvider.java
@@ -20,8 +20,6 @@
 package org.elasticsearch.index.similarity;
 
 import org.apache.lucene.search.similarities.DefaultSimilarity;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.inject.assistedinject.Assisted;
 import org.elasticsearch.common.settings.Settings;
 
 /**
@@ -37,8 +35,7 @@ public class DefaultSimilarityProvider extends AbstractSimilarityProvider {
 
     private final DefaultSimilarity similarity = new DefaultSimilarity();
 
-    @Inject
-    public DefaultSimilarityProvider(@Assisted String name, @Assisted Settings settings) {
+    public DefaultSimilarityProvider(String name, Settings settings) {
         super(name);
         boolean discountOverlaps = settings.getAsBoolean("discount_overlaps", true);
         this.similarity.setDiscountOverlaps(discountOverlaps);
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/IBSimilarityProvider.java b/core/src/main/java/org/elasticsearch/index/similarity/IBSimilarityProvider.java
index 161ca9c..eb8d20a 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/IBSimilarityProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/IBSimilarityProvider.java
@@ -22,8 +22,6 @@ package org.elasticsearch.index.similarity;
 import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.search.similarities.*;
 import org.elasticsearch.common.collect.MapBuilder;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.inject.assistedinject.Assisted;
 import org.elasticsearch.common.settings.Settings;
 
 /**
@@ -56,8 +54,7 @@ public class IBSimilarityProvider extends AbstractSimilarityProvider {
 
     private final IBSimilarity similarity;
 
-    @Inject
-    public IBSimilarityProvider(@Assisted String name, @Assisted Settings settings) {
+    public IBSimilarityProvider(String name, Settings settings) {
         super(name);
         Distribution distribution = parseDistribution(settings);
         Lambda lambda = parseLambda(settings);
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/LMDirichletSimilarityProvider.java b/core/src/main/java/org/elasticsearch/index/similarity/LMDirichletSimilarityProvider.java
index efea285..24494dc 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/LMDirichletSimilarityProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/LMDirichletSimilarityProvider.java
@@ -21,8 +21,6 @@ package org.elasticsearch.index.similarity;
 
 import org.apache.lucene.search.similarities.LMDirichletSimilarity;
 import org.apache.lucene.search.similarities.Similarity;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.inject.assistedinject.Assisted;
 import org.elasticsearch.common.settings.Settings;
 
 /**
@@ -38,8 +36,7 @@ public class LMDirichletSimilarityProvider extends AbstractSimilarityProvider {
 
     private final LMDirichletSimilarity similarity;
 
-    @Inject
-    public LMDirichletSimilarityProvider(@Assisted String name, @Assisted Settings settings) {
+    public LMDirichletSimilarityProvider(String name, Settings settings) {
         super(name);
         float mu = settings.getAsFloat("mu", 2000f);
         this.similarity = new LMDirichletSimilarity(mu);
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/LMJelinekMercerSimilarityProvider.java b/core/src/main/java/org/elasticsearch/index/similarity/LMJelinekMercerSimilarityProvider.java
index 5d30b30..3d5a40f 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/LMJelinekMercerSimilarityProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/LMJelinekMercerSimilarityProvider.java
@@ -38,8 +38,7 @@ public class LMJelinekMercerSimilarityProvider extends AbstractSimilarityProvide
 
     private final LMJelinekMercerSimilarity similarity;
 
-    @Inject
-    public LMJelinekMercerSimilarityProvider(@Assisted String name, @Assisted Settings settings) {
+    public LMJelinekMercerSimilarityProvider(String name, Settings settings) {
         super(name);
         float lambda = settings.getAsFloat("lambda", 0.1f);
         this.similarity = new LMJelinekMercerSimilarity(lambda);
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/PreBuiltSimilarityProvider.java b/core/src/main/java/org/elasticsearch/index/similarity/PreBuiltSimilarityProvider.java
deleted file mode 100644
index 4b3f0cc..0000000
--- a/core/src/main/java/org/elasticsearch/index/similarity/PreBuiltSimilarityProvider.java
+++ /dev/null
@@ -1,73 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.similarity;
-
-import org.apache.lucene.search.similarities.Similarity;
-import org.elasticsearch.common.settings.Settings;
-
-/**
- * {@link SimilarityProvider} for pre-built Similarities
- */
-public class PreBuiltSimilarityProvider extends AbstractSimilarityProvider {
-
-    public static class Factory implements SimilarityProvider.Factory {
-
-        private final PreBuiltSimilarityProvider similarity;
-
-        public Factory(String name, Similarity similarity) {
-            this.similarity = new PreBuiltSimilarityProvider(name, similarity);
-        }
-
-        @Override
-        public SimilarityProvider create(String name, Settings settings) {
-            return similarity;
-        }
-
-        public String name() {
-            return similarity.name();
-        }
-
-        public SimilarityProvider get() {
-            return similarity;
-        }
-    }
-
-    private final Similarity similarity;
-
-    /**
-     * Creates a new {@link PreBuiltSimilarityProvider} with the given name and given
-     * pre-built Similarity
-     *
-     * @param name Name of the Provider
-     * @param similarity Pre-built Similarity
-     */
-    public PreBuiltSimilarityProvider(String name, Similarity similarity) {
-        super(name);
-        this.similarity = similarity;
-    }
-
-    /**
-     * {@inheritDoc}
-     */
-    @Override
-    public Similarity get() {
-        return similarity;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/Similarities.java b/core/src/main/java/org/elasticsearch/index/similarity/Similarities.java
deleted file mode 100644
index b40acb8..0000000
--- a/core/src/main/java/org/elasticsearch/index/similarity/Similarities.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.similarity;
-
-import com.google.common.collect.ImmutableMap;
-import org.apache.lucene.search.similarities.BM25Similarity;
-import org.apache.lucene.search.similarities.DefaultSimilarity;
-import org.elasticsearch.common.collect.MapBuilder;
-
-import java.util.Collection;
-
-/**
- * Cache of pre-defined Similarities
- */
-public class Similarities {
-
-    private static final ImmutableMap<String, PreBuiltSimilarityProvider.Factory> PRE_BUILT_SIMILARITIES;
-
-    static {
-        MapBuilder<String, PreBuiltSimilarityProvider.Factory> similarities = MapBuilder.newMapBuilder();
-        similarities.put(SimilarityLookupService.DEFAULT_SIMILARITY,
-                new PreBuiltSimilarityProvider.Factory(SimilarityLookupService.DEFAULT_SIMILARITY, new DefaultSimilarity()));
-        similarities.put("BM25", new PreBuiltSimilarityProvider.Factory("BM25", new BM25Similarity()));
-
-        PRE_BUILT_SIMILARITIES = similarities.immutableMap();
-    }
-
-    private Similarities() {
-    }
-
-    /**
-     * Returns the list of pre-defined SimilarityProvider Factories
-     *
-     * @return Pre-defined SimilarityProvider Factories
-     */
-    public static Collection<PreBuiltSimilarityProvider.Factory> listFactories() {
-        return PRE_BUILT_SIMILARITIES.values();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/SimilarityLookupService.java b/core/src/main/java/org/elasticsearch/index/similarity/SimilarityLookupService.java
deleted file mode 100644
index 8b8b688..0000000
--- a/core/src/main/java/org/elasticsearch/index/similarity/SimilarityLookupService.java
+++ /dev/null
@@ -1,85 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.similarity;
-
-import com.google.common.collect.ImmutableMap;
-import org.elasticsearch.common.collect.MapBuilder;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.AbstractIndexComponent;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.settings.IndexSettings;
-
-import java.util.Map;
-
-/**
- * Service for looking up configured {@link SimilarityProvider} implementations by name.
- * <p>
- * The service instantiates the Providers through their Factories using configuration
- * values found with the {@link SimilarityModule#SIMILARITY_SETTINGS_PREFIX} prefix.
- */
-public class SimilarityLookupService extends AbstractIndexComponent {
-
-    public final static String DEFAULT_SIMILARITY = "default";
-
-    private final ImmutableMap<String, SimilarityProvider> similarities;
-
-    public SimilarityLookupService(Index index, Settings indexSettings) {
-        this(index, indexSettings, ImmutableMap.<String, SimilarityProvider.Factory>of());
-    }
-
-    @Inject
-    public SimilarityLookupService(Index index, @IndexSettings Settings indexSettings, Map<String, SimilarityProvider.Factory> similarities) {
-        super(index, indexSettings);
-
-        MapBuilder<String, SimilarityProvider> providers = MapBuilder.newMapBuilder();
-
-        Map<String, Settings> similaritySettings = indexSettings.getGroups(SimilarityModule.SIMILARITY_SETTINGS_PREFIX);
-        for (Map.Entry<String, SimilarityProvider.Factory> entry : similarities.entrySet()) {
-            String name = entry.getKey();
-            SimilarityProvider.Factory factory = entry.getValue();
-
-            Settings settings = similaritySettings.get(name);
-            if (settings == null) {
-                settings = Settings.Builder.EMPTY_SETTINGS;
-            }
-            providers.put(name, factory.create(name, settings));
-        }
-
-        // For testing
-        for (PreBuiltSimilarityProvider.Factory factory : Similarities.listFactories()) {
-            if (!providers.containsKey(factory.name())) {
-                providers.put(factory.name(), factory.get());
-            }
-        }
-
-        this.similarities = providers.immutableMap();
-    }
-
-    /**
-     * Returns the {@link SimilarityProvider} with the given name
-     *
-     * @param name Name of the SimilarityProvider to find
-     * @return {@link SimilarityProvider} with the given name, or {@code null} if no Provider exists
-     */
-    public SimilarityProvider similarity(String name) {
-        return similarities.get(name);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/SimilarityModule.java b/core/src/main/java/org/elasticsearch/index/similarity/SimilarityModule.java
index 6e03bcf..29312f2 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/SimilarityModule.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/SimilarityModule.java
@@ -20,19 +20,18 @@
 package org.elasticsearch.index.similarity;
 
 import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Scopes;
-import org.elasticsearch.common.inject.assistedinject.FactoryProvider;
-import org.elasticsearch.common.inject.multibindings.MapBinder;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.Index;
 
 import java.util.HashMap;
 import java.util.Map;
+import java.util.function.BiFunction;
 
 /**
  * {@link SimilarityModule} is responsible gathering registered and configured {@link SimilarityProvider}
- * implementations and making them available through the {@link SimilarityLookupService} and {@link SimilarityService}.
+ * implementations and making them available through the {@link SimilarityService}.
  *
- * New {@link SimilarityProvider} implementations can be registered through {@link #addSimilarity(String, Class)}
+ * New {@link SimilarityProvider} implementations can be registered through {@link #addSimilarity(String, BiFunction)}
  * while existing Providers can be referenced through Settings under the {@link #SIMILARITY_SETTINGS_PREFIX} prefix
  * along with the "type" value.  For example, to reference the {@link BM25SimilarityProvider}, the configuration
  * <tt>"index.similarity.my_similarity.type : "BM25"</tt> can be used.
@@ -42,16 +41,12 @@ public class SimilarityModule extends AbstractModule {
     public static final String SIMILARITY_SETTINGS_PREFIX = "index.similarity";
 
     private final Settings settings;
-    private final Map<String, Class<? extends SimilarityProvider>> similarities = new HashMap<>();
+    private final Map<String, BiFunction<String, Settings, SimilarityProvider>> similarities = new HashMap<>();
+    private final Index index;
 
-    public SimilarityModule(Settings settings) {
+    public SimilarityModule(Index index, Settings settings) {
         this.settings = settings;
-        addSimilarity("default", DefaultSimilarityProvider.class);
-        addSimilarity("BM25", BM25SimilarityProvider.class);
-        addSimilarity("DFR", DFRSimilarityProvider.class);
-        addSimilarity("IB", IBSimilarityProvider.class);
-        addSimilarity("LMDirichlet", LMDirichletSimilarityProvider.class);
-        addSimilarity("LMJelinekMercer", LMJelinekMercerSimilarityProvider.class);
+        this.index = index;
     }
 
     /**
@@ -60,36 +55,16 @@ public class SimilarityModule extends AbstractModule {
      * @param name Name of the SimilarityProvider
      * @param similarity SimilarityProvider to register
      */
-    public void addSimilarity(String name, Class<? extends SimilarityProvider> similarity) {
+    public void addSimilarity(String name, BiFunction<String, Settings, SimilarityProvider> similarity) {
+        if (similarities.containsKey(name) || SimilarityService.BUILT_IN.containsKey(name)) {
+            throw new IllegalArgumentException("similarity for name: [" + name + " is already registered");
+        }
         similarities.put(name, similarity);
     }
 
     @Override
     protected void configure() {
-        MapBinder<String, SimilarityProvider.Factory> similarityBinder =
-            MapBinder.newMapBinder(binder(), String.class, SimilarityProvider.Factory.class);
-
-        Map<String, Settings> similaritySettings = settings.getGroups(SIMILARITY_SETTINGS_PREFIX);
-        for (Map.Entry<String, Settings> entry : similaritySettings.entrySet()) {
-            String name = entry.getKey();
-            Settings settings = entry.getValue();
-
-            String typeName = settings.get("type");
-            if (typeName == null) {
-                throw new IllegalArgumentException("Similarity [" + name + "] must have an associated type");
-            } else if (similarities.containsKey(typeName) == false) {
-                throw new IllegalArgumentException("Unknown Similarity type [" + typeName + "] for [" + name + "]");
-            }
-            similarityBinder.addBinding(entry.getKey()).toProvider(FactoryProvider.newFactory(SimilarityProvider.Factory.class, similarities.get(typeName))).in(Scopes.SINGLETON);
-        }
-
-        for (PreBuiltSimilarityProvider.Factory factory : Similarities.listFactories()) {
-            if (!similarities.containsKey(factory.name())) {
-                similarityBinder.addBinding(factory.name()).toInstance(factory);
-            }
-        }
-
-        bind(SimilarityLookupService.class).asEagerSingleton();
-        bind(SimilarityService.class).asEagerSingleton();
+        SimilarityService service = new SimilarityService(index, settings, new HashMap<>(similarities));
+        bind(SimilarityService.class).toInstance(service);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/SimilarityProvider.java b/core/src/main/java/org/elasticsearch/index/similarity/SimilarityProvider.java
index 38f56af..6433181 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/SimilarityProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/SimilarityProvider.java
@@ -40,19 +40,4 @@ public interface SimilarityProvider {
      * @return Provided {@link Similarity}
      */
     Similarity get();
-
-    /**
-     * Factory for creating {@link SimilarityProvider} instances
-     */
-    public static interface Factory {
-
-        /**
-         * Creates a new {@link SimilarityProvider} instance
-         *
-         * @param name Name of the provider
-         * @param settings Settings to be used by the Provider
-         * @return {@link SimilarityProvider} instance created by the Factory
-         */
-        SimilarityProvider create(String name, Settings settings);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/SimilarityService.java b/core/src/main/java/org/elasticsearch/index/similarity/SimilarityService.java
index 98faa87..a77a2de 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/SimilarityService.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/SimilarityService.java
@@ -25,55 +25,96 @@ import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.AbstractIndexComponent;
 import org.elasticsearch.index.Index;
-import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.settings.IndexSettings;
 
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.function.BiFunction;
+
 /**
  *
  */
 public class SimilarityService extends AbstractIndexComponent {
 
-    private final SimilarityLookupService similarityLookupService;
-    private final MapperService mapperService;
-
-    private final Similarity perFieldSimilarity;
-
+    public final static String DEFAULT_SIMILARITY = "default";
+    private final Similarity defaultSimilarity;
+    private final Similarity baseSimilarity;
+    private final Map<String, SimilarityProvider> similarities;
+    static final Map<String, BiFunction<String, Settings, SimilarityProvider>> DEFAULTS;
+    static final Map<String, BiFunction<String, Settings, SimilarityProvider>> BUILT_IN;
+    static {
+        Map<String, BiFunction<String, Settings, SimilarityProvider>> defaults = new HashMap<>();
+        Map<String, BiFunction<String, Settings, SimilarityProvider>> buildIn = new HashMap<>();
+        defaults.put("default", DefaultSimilarityProvider::new);
+        defaults.put("BM25", BM25SimilarityProvider::new);
+        buildIn.put("default", DefaultSimilarityProvider::new);
+        buildIn.put("BM25", BM25SimilarityProvider::new);
+        buildIn.put("DFR", DFRSimilarityProvider::new);
+        buildIn.put("IB", IBSimilarityProvider::new);
+        buildIn.put("LMDirichlet", LMDirichletSimilarityProvider::new);
+        buildIn.put("LMJelinekMercer", LMJelinekMercerSimilarityProvider::new);
+        DEFAULTS = Collections.unmodifiableMap(defaults);
+        BUILT_IN = Collections.unmodifiableMap(buildIn);
+    }
     public SimilarityService(Index index) {
         this(index, Settings.Builder.EMPTY_SETTINGS);
     }
 
     public SimilarityService(Index index, Settings settings) {
-        this(index, settings, new SimilarityLookupService(index, settings), null);
+        this(index, settings, Collections.EMPTY_MAP);
     }
 
     @Inject
-    public SimilarityService(Index index, @IndexSettings Settings indexSettings,
-                             final SimilarityLookupService similarityLookupService, final MapperService mapperService) {
+    public SimilarityService(Index index, @IndexSettings Settings indexSettings, Map<String, BiFunction<String, Settings, SimilarityProvider>> similarities) {
         super(index, indexSettings);
-        this.similarityLookupService = similarityLookupService;
-        this.mapperService = mapperService;
-
-        Similarity defaultSimilarity = similarityLookupService.similarity(SimilarityLookupService.DEFAULT_SIMILARITY).get();
+        Map<String, SimilarityProvider> providers = new HashMap<>(similarities.size());
+        Map<String, Settings> similaritySettings = indexSettings.getGroups(SimilarityModule.SIMILARITY_SETTINGS_PREFIX);
+        for (Map.Entry<String, Settings> entry : similaritySettings.entrySet()) {
+            String name = entry.getKey();
+            Settings settings = entry.getValue();
+            String typeName = settings.get("type");
+            if (typeName == null) {
+                throw new IllegalArgumentException("Similarity [" + name + "] must have an associated type");
+            } else if ((similarities.containsKey(typeName) || BUILT_IN.containsKey(typeName)) == false) {
+                throw new IllegalArgumentException("Unknown Similarity type [" + typeName + "] for [" + name + "]");
+            }
+            BiFunction<String, Settings, SimilarityProvider> factory = similarities.getOrDefault(typeName, BUILT_IN.get(typeName));
+            if (settings == null) {
+                settings = Settings.Builder.EMPTY_SETTINGS;
+            }
+            providers.put(name, factory.apply(name, settings));
+        }
+        addSimilarities(similaritySettings, providers, DEFAULTS);
+        this.similarities = providers;
+        defaultSimilarity = providers.get(SimilarityService.DEFAULT_SIMILARITY).get();
         // Expert users can configure the base type as being different to default, but out-of-box we use default.
-        Similarity baseSimilarity = (similarityLookupService.similarity("base") != null) ? similarityLookupService.similarity("base").get() :
-                defaultSimilarity;
-
-        this.perFieldSimilarity = (mapperService != null) ? new PerFieldSimilarity(defaultSimilarity, baseSimilarity, mapperService) :
+        baseSimilarity = (providers.get("base") != null) ? providers.get("base").get() :
                 defaultSimilarity;
     }
 
-    public Similarity similarity() {
-        return perFieldSimilarity;
+    public Similarity similarity(MapperService mapperService) {
+        // TODO we can maybe factor out MapperService here entirely by introducing an interface for the lookup?
+        return (mapperService != null) ? new PerFieldSimilarity(defaultSimilarity, baseSimilarity, mapperService) :
+                defaultSimilarity;
     }
 
-    public SimilarityLookupService similarityLookupService() {
-        return similarityLookupService;
+    private void addSimilarities(Map<String, Settings>  similaritySettings, Map<String, SimilarityProvider> providers, Map<String, BiFunction<String, Settings, SimilarityProvider>> similarities)  {
+        for (Map.Entry<String, BiFunction<String, Settings, SimilarityProvider>> entry : similarities.entrySet()) {
+            String name = entry.getKey();
+            BiFunction<String, Settings, SimilarityProvider> factory = entry.getValue();
+            Settings settings = similaritySettings.get(name);
+            if (settings == null) {
+                settings = Settings.Builder.EMPTY_SETTINGS;
+            }
+            providers.put(name, factory.apply(name, settings));
+        }
     }
 
-    public MapperService mapperService() {
-        return mapperService;
+    public SimilarityProvider getSimilarity(String name) {
+        return similarities.get(name);
     }
 
     static class PerFieldSimilarity extends PerFieldSimilarityWrapper {
diff --git a/core/src/main/java/org/elasticsearch/index/translog/Translog.java b/core/src/main/java/org/elasticsearch/index/translog/Translog.java
index 5084895..a8f2801 100644
--- a/core/src/main/java/org/elasticsearch/index/translog/Translog.java
+++ b/core/src/main/java/org/elasticsearch/index/translog/Translog.java
@@ -465,11 +465,10 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
     }
 
     /**
-     * Adds a created / delete / index operations to the transaction log.
+     * Adds a delete / index operations to the transaction log.
      *
      * @see org.elasticsearch.index.translog.Translog.Operation
-     * @see org.elasticsearch.index.translog.Translog.Create
-     * @see org.elasticsearch.index.translog.Translog.Index
+     * @see Index
      * @see org.elasticsearch.index.translog.Translog.Delete
      */
     public Location add(Operation operation) throws TranslogException {
@@ -874,8 +873,9 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
      */
     public interface Operation extends Streamable {
         enum Type {
+            @Deprecated
             CREATE((byte) 1),
-            SAVE((byte) 2),
+            INDEX((byte) 2),
             DELETE((byte) 3),
             DELETE_BY_QUERY((byte) 4);
 
@@ -894,7 +894,7 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
                     case 1:
                         return CREATE;
                     case 2:
-                        return SAVE;
+                        return INDEX;
                     case 3:
                         return DELETE;
                     case 4:
@@ -929,199 +929,6 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
         }
     }
 
-    public static class Create implements Operation {
-        public static final int SERIALIZATION_FORMAT = 6;
-
-        private String id;
-        private String type;
-        private BytesReference source;
-        private String routing;
-        private String parent;
-        private long timestamp;
-        private long ttl;
-        private long version = Versions.MATCH_ANY;
-        private VersionType versionType = VersionType.INTERNAL;
-
-        public Create() {
-        }
-
-        public Create(Engine.Create create) {
-            this.id = create.id();
-            this.type = create.type();
-            this.source = create.source();
-            this.routing = create.routing();
-            this.parent = create.parent();
-            this.timestamp = create.timestamp();
-            this.ttl = create.ttl();
-            this.version = create.version();
-            this.versionType = create.versionType();
-        }
-
-        public Create(String type, String id, byte[] source) {
-            this.id = id;
-            this.type = type;
-            this.source = new BytesArray(source);
-        }
-
-        @Override
-        public Type opType() {
-            return Type.CREATE;
-        }
-
-        @Override
-        public long estimateSize() {
-            return ((id.length() + type.length()) * 2) + source.length() + 12;
-        }
-
-        public String id() {
-            return this.id;
-        }
-
-        public BytesReference source() {
-            return this.source;
-        }
-
-        public String type() {
-            return this.type;
-        }
-
-        public String routing() {
-            return this.routing;
-        }
-
-        public String parent() {
-            return this.parent;
-        }
-
-        public long timestamp() {
-            return this.timestamp;
-        }
-
-        public long ttl() {
-            return this.ttl;
-        }
-
-        public long version() {
-            return this.version;
-        }
-
-        public VersionType versionType() {
-            return versionType;
-        }
-
-        @Override
-        public Source getSource() {
-            return new Source(source, routing, parent, timestamp, ttl);
-        }
-
-        @Override
-        public void readFrom(StreamInput in) throws IOException {
-            int version = in.readVInt(); // version
-            id = in.readString();
-            type = in.readString();
-            source = in.readBytesReference();
-            if (version >= 1) {
-                if (in.readBoolean()) {
-                    routing = in.readString();
-                }
-            }
-            if (version >= 2) {
-                if (in.readBoolean()) {
-                    parent = in.readString();
-                }
-            }
-            if (version >= 3) {
-                this.version = in.readLong();
-            }
-            if (version >= 4) {
-                this.timestamp = in.readLong();
-            }
-            if (version >= 5) {
-                this.ttl = in.readLong();
-            }
-            if (version >= 6) {
-                this.versionType = VersionType.fromValue(in.readByte());
-            }
-
-            assert versionType.validateVersionForWrites(version);
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeVInt(SERIALIZATION_FORMAT);
-            out.writeString(id);
-            out.writeString(type);
-            out.writeBytesReference(source);
-            if (routing == null) {
-                out.writeBoolean(false);
-            } else {
-                out.writeBoolean(true);
-                out.writeString(routing);
-            }
-            if (parent == null) {
-                out.writeBoolean(false);
-            } else {
-                out.writeBoolean(true);
-                out.writeString(parent);
-            }
-            out.writeLong(version);
-            out.writeLong(timestamp);
-            out.writeLong(ttl);
-            out.writeByte(versionType.getValue());
-        }
-
-        @Override
-        public boolean equals(Object o) {
-            if (this == o) {
-                return true;
-            }
-            if (o == null || getClass() != o.getClass()) {
-                return false;
-            }
-
-            Create create = (Create) o;
-
-            if (timestamp != create.timestamp ||
-                    ttl != create.ttl ||
-                    version != create.version ||
-                    id.equals(create.id) == false ||
-                    type.equals(create.type) == false ||
-                    source.equals(create.source) == false) {
-                return false;
-            }
-            if (routing != null ? !routing.equals(create.routing) : create.routing != null) {
-                return false;
-            }
-            if (parent != null ? !parent.equals(create.parent) : create.parent != null) {
-                return false;
-            }
-            return versionType == create.versionType;
-
-        }
-
-        @Override
-        public int hashCode() {
-            int result = id.hashCode();
-            result = 31 * result + type.hashCode();
-            result = 31 * result + source.hashCode();
-            result = 31 * result + (routing != null ? routing.hashCode() : 0);
-            result = 31 * result + (parent != null ? parent.hashCode() : 0);
-            result = 31 * result + (int) (timestamp ^ (timestamp >>> 32));
-            result = 31 * result + (int) (ttl ^ (ttl >>> 32));
-            result = 31 * result + (int) (version ^ (version >>> 32));
-            result = 31 * result + versionType.hashCode();
-            return result;
-        }
-
-        @Override
-        public String toString() {
-            return "Create{" +
-                    "id='" + id + '\'' +
-                    ", type='" + type + '\'' +
-                    '}';
-        }
-    }
-
     public static class Index implements Operation {
         public static final int SERIALIZATION_FORMAT = 6;
 
@@ -1158,7 +965,7 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
 
         @Override
         public Type opType() {
-            return Type.SAVE;
+            return Type.INDEX;
         }
 
         @Override
@@ -1667,13 +1474,14 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
     static Translog.Operation newOperationFromType(Translog.Operation.Type type) throws IOException {
         switch (type) {
             case CREATE:
-                return new Translog.Create();
+                // the deserialization logic in Index was identical to that of Create when create was deprecated
+                return new Index();
             case DELETE:
                 return new Translog.Delete();
             case DELETE_BY_QUERY:
                 return new Translog.DeleteByQuery();
-            case SAVE:
-                return new Translog.Index();
+            case INDEX:
+                return new Index();
             default:
                 throw new IOException("No type for [" + type + "]");
         }
diff --git a/core/src/main/java/org/elasticsearch/index/translog/TranslogConfig.java b/core/src/main/java/org/elasticsearch/index/translog/TranslogConfig.java
index 4d74961..30ab814 100644
--- a/core/src/main/java/org/elasticsearch/index/translog/TranslogConfig.java
+++ b/core/src/main/java/org/elasticsearch/index/translog/TranslogConfig.java
@@ -27,6 +27,7 @@ import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.index.settings.IndexSettings;
 import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.index.translog.Translog.TranslogGeneration;
+import org.elasticsearch.indices.memory.IndexingMemoryController;
 import org.elasticsearch.threadpool.ThreadPool;
 
 import java.nio.file.Path;
@@ -42,7 +43,6 @@ public final class TranslogConfig {
     public static final String INDEX_TRANSLOG_FS_TYPE = "index.translog.fs.type";
     public static final String INDEX_TRANSLOG_BUFFER_SIZE = "index.translog.fs.buffer_size";
     public static final String INDEX_TRANSLOG_SYNC_INTERVAL = "index.translog.sync_interval";
-    public static final ByteSizeValue INACTIVE_SHARD_TRANSLOG_BUFFER = ByteSizeValue.parseBytesSizeValue("1kb", "INACTIVE_SHARD_TRANSLOG_BUFFER");
 
     private final TimeValue syncInterval;
     private final BigArrays bigArrays;
@@ -73,7 +73,7 @@ public final class TranslogConfig {
         this.threadPool = threadPool;
         this.bigArrays = bigArrays;
         this.type = TranslogWriter.Type.fromString(indexSettings.get(INDEX_TRANSLOG_FS_TYPE, TranslogWriter.Type.BUFFERED.name()));
-        this.bufferSize = (int) indexSettings.getAsBytesSize(INDEX_TRANSLOG_BUFFER_SIZE, ByteSizeValue.parseBytesSizeValue("64k", INDEX_TRANSLOG_BUFFER_SIZE)).bytes(); // Not really interesting, updated by IndexingMemoryController...
+        this.bufferSize = (int) indexSettings.getAsBytesSize(INDEX_TRANSLOG_BUFFER_SIZE, IndexingMemoryController.INACTIVE_SHARD_TRANSLOG_BUFFER).bytes(); // Not really interesting, updated by IndexingMemoryController...
 
         syncInterval = indexSettings.getAsTime(INDEX_TRANSLOG_SYNC_INTERVAL, TimeValue.timeValueSeconds(5));
         if (syncInterval.millis() > 0 && threadPool != null) {
diff --git a/core/src/main/java/org/elasticsearch/index/translog/TranslogStats.java b/core/src/main/java/org/elasticsearch/index/translog/TranslogStats.java
index 1af0a74..a4431b5 100644
--- a/core/src/main/java/org/elasticsearch/index/translog/TranslogStats.java
+++ b/core/src/main/java/org/elasticsearch/index/translog/TranslogStats.java
@@ -18,11 +18,10 @@
  */
 package org.elasticsearch.index.translog;
 
+import org.elasticsearch.action.support.ToXContentToBytes;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
 
@@ -31,17 +30,23 @@ import java.io.IOException;
 /**
  *
  */
-public class TranslogStats implements ToXContent, Streamable {
+public class TranslogStats extends ToXContentToBytes implements Streamable {
 
-    private long translogSizeInBytes = 0;
-    private int estimatedNumberOfOperations = -1;
+    private long translogSizeInBytes;
+    private int numberOfOperations;
 
     public TranslogStats() {
     }
 
-    public TranslogStats(int estimatedNumberOfOperations, long translogSizeInBytes) {
+    public TranslogStats(int numberOfOperations, long translogSizeInBytes) {
+        if (numberOfOperations < 0) {
+            throw new IllegalArgumentException("numberOfOperations must be >= 0");
+        }
+        if (translogSizeInBytes < 0) {
+            throw new IllegalArgumentException("translogSizeInBytes must be >= 0");
+        }
         assert translogSizeInBytes >= 0 : "translogSizeInBytes must be >= 0, got [" + translogSizeInBytes + "]";
-        this.estimatedNumberOfOperations = estimatedNumberOfOperations;
+        this.numberOfOperations = numberOfOperations;
         this.translogSizeInBytes = translogSizeInBytes;
     }
 
@@ -50,22 +55,22 @@ public class TranslogStats implements ToXContent, Streamable {
             return;
         }
 
-        this.estimatedNumberOfOperations += translogStats.estimatedNumberOfOperations;
-        this.translogSizeInBytes = +translogStats.translogSizeInBytes;
+        this.numberOfOperations += translogStats.numberOfOperations;
+        this.translogSizeInBytes += translogStats.translogSizeInBytes;
     }
 
-    public ByteSizeValue translogSizeInBytes() {
-        return new ByteSizeValue(translogSizeInBytes);
+    public long getTranslogSizeInBytes() {
+        return translogSizeInBytes;
     }
 
     public long estimatedNumberOfOperations() {
-        return estimatedNumberOfOperations;
+        return numberOfOperations;
     }
 
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
         builder.startObject(Fields.TRANSLOG);
-        builder.field(Fields.OPERATIONS, estimatedNumberOfOperations);
+        builder.field(Fields.OPERATIONS, numberOfOperations);
         builder.byteSizeField(Fields.SIZE_IN_BYTES, Fields.SIZE, translogSizeInBytes);
         builder.endObject();
         return builder;
@@ -80,13 +85,13 @@ public class TranslogStats implements ToXContent, Streamable {
 
     @Override
     public void readFrom(StreamInput in) throws IOException {
-        estimatedNumberOfOperations = in.readVInt();
+        numberOfOperations = in.readVInt();
         translogSizeInBytes = in.readVLong();
     }
 
     @Override
     public void writeTo(StreamOutput out) throws IOException {
-        out.writeVInt(estimatedNumberOfOperations);
+        out.writeVInt(numberOfOperations);
         out.writeVLong(translogSizeInBytes);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/indices/IndicesService.java b/core/src/main/java/org/elasticsearch/indices/IndicesService.java
index ae69eee..8601d76 100644
--- a/core/src/main/java/org/elasticsearch/indices/IndicesService.java
+++ b/core/src/main/java/org/elasticsearch/indices/IndicesService.java
@@ -52,7 +52,6 @@ import org.elasticsearch.index.IndexModule;
 import org.elasticsearch.index.IndexNameModule;
 import org.elasticsearch.index.IndexNotFoundException;
 import org.elasticsearch.index.IndexService;
-import org.elasticsearch.index.LocalNodeIdModule;
 import org.elasticsearch.index.analysis.AnalysisModule;
 import org.elasticsearch.index.analysis.AnalysisService;
 import org.elasticsearch.index.cache.IndexCache;
@@ -330,7 +329,6 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
 
         ModulesBuilder modules = new ModulesBuilder();
         modules.add(new IndexNameModule(index));
-        modules.add(new LocalNodeIdModule(localNodeId));
         modules.add(new IndexSettingsModule(index, indexSettings));
         // plugin modules must be added here, before others or we can get crazy injection errors...
         for (Module pluginModule : pluginsService.indexModules(indexSettings)) {
@@ -338,7 +336,7 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
         }
         modules.add(new IndexStoreModule(indexSettings));
         modules.add(new AnalysisModule(indexSettings, indicesAnalysisService));
-        modules.add(new SimilarityModule(indexSettings));
+        modules.add(new SimilarityModule(index, indexSettings));
         modules.add(new IndexCacheModule(indexSettings));
         modules.add(new IndexModule());
         
diff --git a/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java b/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java
index ddaf3c8..4ab4691 100644
--- a/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java
+++ b/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java
@@ -60,6 +60,8 @@ import java.util.concurrent.Callable;
 import java.util.concurrent.ConcurrentMap;
 import java.util.concurrent.TimeUnit;
 
+import static org.elasticsearch.common.Strings.hasLength;
+
 /**
  * The indices request cache allows to cache a shard level request stage responses, helping with improving
  * similar requests that are potentially expensive (because of aggs for example). The cache is fully coherent
@@ -205,7 +207,8 @@ public class IndicesRequestCache extends AbstractComponent implements RemovalLis
      * Can the shard request be cached at all?
      */
     public boolean canCache(ShardSearchRequest request, SearchContext context) {
-        if (request.template() != null) {
+        // TODO: for now, template is not supported, though we could use the generated bytes as the key
+        if (hasLength(request.templateSource())) {
             return false;
         }
 
diff --git a/core/src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java b/core/src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java
index a84fff3..90bb4c4 100644
--- a/core/src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java
+++ b/core/src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java
@@ -29,12 +29,10 @@ import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.FutureUtils;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.engine.EngineClosedException;
-import org.elasticsearch.index.engine.EngineConfig;
 import org.elasticsearch.index.engine.FlushNotAllowedEngineException;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.shard.IndexShardState;
 import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.index.translog.Translog;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.monitor.jvm.JvmInfo;
 import org.elasticsearch.threadpool.ThreadPool;
@@ -42,9 +40,6 @@ import org.elasticsearch.threadpool.ThreadPool;
 import java.util.*;
 import java.util.concurrent.ScheduledFuture;
 
-/**
- *
- */
 public class IndexingMemoryController extends AbstractLifecycleComponent<IndexingMemoryController> {
 
     /** How much heap (% or bytes) we will share across all actively indexing shards on this node (default: 10%). */
@@ -83,6 +78,12 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
     /** How frequently we check shards to find inactive ones (default: 30 seconds). */
     public static final String SHARD_INACTIVE_INTERVAL_TIME_SETTING = "indices.memory.interval";
 
+    /** Once a shard becomes inactive, we reduce the {@code IndexWriter} buffer to this value (500 KB) to let active shards use the heap instead. */
+    public static final ByteSizeValue INACTIVE_SHARD_INDEXING_BUFFER = ByteSizeValue.parseBytesSizeValue("500kb", "INACTIVE_SHARD_INDEXING_BUFFER");
+
+    /** Once a shard becomes inactive, we reduce the {@code Translog} buffer to this value (1 KB) to let active shards use the heap instead. */
+    public static final ByteSizeValue INACTIVE_SHARD_TRANSLOG_BUFFER = ByteSizeValue.parseBytesSizeValue("1kb", "INACTIVE_SHARD_TRANSLOG_BUFFER");
+
     private final ThreadPool threadPool;
     private final IndicesService indicesService;
 
@@ -164,7 +165,6 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
 
         this.statusChecker = new ShardsIndicesStatusChecker();
 
-
         logger.debug("using indexing buffer size [{}], with {} [{}], {} [{}], {} [{}], {} [{}]",
                 this.indexingBuffer,
                 MIN_SHARD_INDEX_BUFFER_SIZE_SETTING, this.minShardIndexBufferSize,
@@ -175,7 +175,7 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
 
     @Override
     protected void doStart() {
-        // its fine to run it on the scheduler thread, no busy work
+        // it's fine to run it on the scheduler thread, no busy work
         this.scheduler = threadPool.scheduleWithFixedDelay(statusChecker, interval);
     }
 
@@ -240,6 +240,7 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
         return null;
     }
 
+    /** set new indexing and translog buffers on this shard.  this may cause the shard to refresh to free up heap. */
     protected void updateShardBuffers(ShardId shardId, ByteSizeValue shardIndexingBufferSize, ByteSizeValue shardTranslogBufferSize) {
         final IndexShard shard = getShard(shardId);
         if (shard != null) {
@@ -255,105 +256,86 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
         }
     }
 
-
-    /** returns the current translog status (generation id + ops) for the given shard id. Returns null if unavailable. */
-    protected ShardIndexingStatus getTranslogStatus(ShardId shardId) {
+    /** returns {@link IndexShard#getActive} if the shard exists, else null */
+    protected Boolean getShardActive(ShardId shardId) {
         final IndexShard indexShard = getShard(shardId);
         if (indexShard == null) {
             return null;
         }
-        final Translog translog;
-        try {
-            translog = indexShard.getTranslog();
-        } catch (EngineClosedException e) {
-            // not ready yet to be checked for activity
-            return null;
-        }
-
-        ShardIndexingStatus status = new ShardIndexingStatus();
-        status.translogId = translog.currentFileGeneration();
-        status.translogNumberOfOperations = translog.totalOperations();
-        return status;
+        return indexShard.getActive();
     }
 
-    // used for tests
-    void forceCheck() {
+    /** check if any shards active status changed, now. */
+    public void forceCheck() {
         statusChecker.run();
     }
 
     class ShardsIndicesStatusChecker implements Runnable {
 
-        private final Map<ShardId, ShardIndexingStatus> shardsIndicesStatus = new HashMap<>();
+        // True if the shard was active last time we checked
+        private final Map<ShardId,Boolean> shardWasActive = new HashMap<>();
 
         @Override
-        public void run() {
+        public synchronized void run() {
             EnumSet<ShardStatusChangeType> changes = purgeDeletedAndClosedShards();
 
-            final List<ShardId> activeToInactiveIndexingShards = new ArrayList<>();
-            final int activeShards = updateShardStatuses(changes, activeToInactiveIndexingShards);
-            for (ShardId indexShard : activeToInactiveIndexingShards) {
-                markShardAsInactive(indexShard);
-            }
+            updateShardStatuses(changes);
 
             if (changes.isEmpty() == false) {
                 // Something changed: recompute indexing buffers:
-                calcAndSetShardBuffers(activeShards, "[" + changes + "]");
+                calcAndSetShardBuffers("[" + changes + "]");
             }
         }
 
         /**
-         * goes through all existing shards and check whether the changes their active status
-         *
-         * @return the current count of active shards
+         * goes through all existing shards and check whether there are changes in their active status
          */
-        private int updateShardStatuses(EnumSet<ShardStatusChangeType> changes, List<ShardId> activeToInactiveIndexingShards) {
-            int activeShards = 0;
+        private void updateShardStatuses(EnumSet<ShardStatusChangeType> changes) {
             for (ShardId shardId : availableShards()) {
 
-                final ShardIndexingStatus currentStatus = getTranslogStatus(shardId);
+                // Is the shard active now?
+                Boolean isActive = getShardActive(shardId);
 
-                if (currentStatus == null) {
+                if (isActive == null) {
                     // shard was closed..
                     continue;
                 }
 
-                ShardIndexingStatus status = shardsIndicesStatus.get(shardId);
-                if (status == null) {
-                    status = currentStatus;
-                    shardsIndicesStatus.put(shardId, status);
+                // Was the shard active last time we checked?
+                Boolean wasActive = shardWasActive.get(shardId);
+
+                if (wasActive == null) {
+                    // First time we are seeing this shard
+                    shardWasActive.put(shardId, isActive);
                     changes.add(ShardStatusChangeType.ADDED);
-                } else {
-                    final boolean lastActiveIndexing = status.activeIndexing;
-                    status.updateWith(currentTimeInNanos(), currentStatus, inactiveTime.nanos());
-                    if (lastActiveIndexing && (status.activeIndexing == false)) {
-                        activeToInactiveIndexingShards.add(shardId);
-                        changes.add(ShardStatusChangeType.BECAME_INACTIVE);
-                        logger.debug("marking shard {} as inactive (inactive_time[{}]) indexing wise, setting size to [{}]",
-                                shardId,
-                                inactiveTime, EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER);
-                    } else if ((lastActiveIndexing == false) && status.activeIndexing) {
+                } else if (isActive) {
+                    // Shard is active now
+                    if (wasActive == false) {
+                        // Shard became active itself, since we last checked (due to new indexing op arriving)
                         changes.add(ShardStatusChangeType.BECAME_ACTIVE);
                         logger.debug("marking shard {} as active indexing wise", shardId);
+                        shardWasActive.put(shardId, true);
+                    } else if (checkIdle(shardId, inactiveTime.nanos()) == Boolean.TRUE) {
+                        // Make shard inactive now
+                        changes.add(ShardStatusChangeType.BECAME_INACTIVE);
+                        logger.debug("marking shard {} as inactive (inactive_time[{}]) indexing wise",
+                                     shardId,
+                                     inactiveTime);
+                        shardWasActive.put(shardId, false);
                     }
                 }
-
-                if (status.activeIndexing) {
-                    activeShards++;
-                }
             }
-
-            return activeShards;
         }
 
         /**
          * purge any existing statuses that are no longer updated
          *
-         * @return true if any change
+         * @return the changes applied
          */
         private EnumSet<ShardStatusChangeType> purgeDeletedAndClosedShards() {
             EnumSet<ShardStatusChangeType> changes = EnumSet.noneOf(ShardStatusChangeType.class);
 
-            Iterator<ShardId> statusShardIdIterator = shardsIndicesStatus.keySet().iterator();
+            Iterator<ShardId> statusShardIdIterator = shardWasActive.keySet().iterator();
             while (statusShardIdIterator.hasNext()) {
                 ShardId shardId = statusShardIdIterator.next();
                 if (shardAvailable(shardId) == false) {
@@ -364,12 +346,25 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
             return changes;
         }
 
-        private void calcAndSetShardBuffers(int activeShards, String reason) {
-            if (activeShards == 0) {
+        private void calcAndSetShardBuffers(String reason) {
+
+            // Count how many shards are now active:
+            int activeShardCount = 0;
+            for (Map.Entry<ShardId,Boolean> ent : shardWasActive.entrySet()) {
+                if (ent.getValue()) {
+                    activeShardCount++;
+                }
+            }
+
+            // TODO: we could be smarter here by taking into account how RAM the IndexWriter on each shard
+            // is actually using (using IW.ramBytesUsed), so that small indices (e.g. Marvel) would not
+            // get the same indexing buffer as large indices.  But it quickly gets tricky...
+            if (activeShardCount == 0) {
                 logger.debug("no active shards (reason={})", reason);
                 return;
             }
-            ByteSizeValue shardIndexingBufferSize = new ByteSizeValue(indexingBuffer.bytes() / activeShards);
+
+            ByteSizeValue shardIndexingBufferSize = new ByteSizeValue(indexingBuffer.bytes() / activeShardCount);
             if (shardIndexingBufferSize.bytes() < minShardIndexBufferSize.bytes()) {
                 shardIndexingBufferSize = minShardIndexBufferSize;
             }
@@ -377,7 +372,7 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
                 shardIndexingBufferSize = maxShardIndexBufferSize;
             }
 
-            ByteSizeValue shardTranslogBufferSize = new ByteSizeValue(translogBuffer.bytes() / activeShards);
+            ByteSizeValue shardTranslogBufferSize = new ByteSizeValue(translogBuffer.bytes() / activeShardCount);
             if (shardTranslogBufferSize.bytes() < minShardTranslogBufferSize.bytes()) {
                 shardTranslogBufferSize = minShardTranslogBufferSize;
             }
@@ -385,11 +380,12 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
                 shardTranslogBufferSize = maxShardTranslogBufferSize;
             }
 
-            logger.debug("recalculating shard indexing buffer (reason={}), total is [{}] with [{}] active shards, each shard set to indexing=[{}], translog=[{}]", reason, indexingBuffer, activeShards, shardIndexingBufferSize, shardTranslogBufferSize);
-            for (ShardId shardId : availableShards()) {
-                ShardIndexingStatus status = shardsIndicesStatus.get(shardId);
-                if (status == null || status.activeIndexing) {
-                    updateShardBuffers(shardId, shardIndexingBufferSize, shardTranslogBufferSize);
+            logger.debug("recalculating shard indexing buffer (reason={}), total is [{}] with [{}] active shards, each shard set to indexing=[{}], translog=[{}]", reason, indexingBuffer, activeShardCount, shardIndexingBufferSize, shardTranslogBufferSize);
+
+            for (Map.Entry<ShardId,Boolean> ent : shardWasActive.entrySet()) {
+                if (ent.getValue()) {
+                    // This shard is active
+                    updateShardBuffers(ent.getKey(), shardIndexingBufferSize, shardTranslogBufferSize);
                 }
             }
         }
@@ -399,13 +395,14 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
         return System.nanoTime();
     }
 
-    // update inactive indexing buffer size
-    protected void markShardAsInactive(ShardId shardId) {
+    /** ask this shard to check now whether it is inactive, and reduces its indexing and translog buffers if so.  returns Boolean.TRUE if
+     *  it did deactive, Boolean.FALSE if it did not, and null if the shard is unknown */
+    protected Boolean checkIdle(ShardId shardId, long inactiveTimeNS) {
         String ignoreReason = null;
         final IndexShard shard = getShard(shardId);
         if (shard != null) {
             try {
-                shard.markAsInactive();
+                return shard.checkIdle(inactiveTimeNS);
             } catch (EngineClosedException e) {
                 // ignore
                 ignoreReason = "EngineClosedException";
@@ -419,47 +416,10 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
         if (ignoreReason != null) {
             logger.trace("ignore [{}] while marking shard {} as inactive", ignoreReason, shardId);
         }
+        return null;
     }
 
     private static enum ShardStatusChangeType {
         ADDED, DELETED, BECAME_ACTIVE, BECAME_INACTIVE
     }
-
-    static class ShardIndexingStatus {
-        long translogId = -1;
-        long translogNumberOfOperations = -1;
-        boolean activeIndexing = true;
-        long idleSinceNanoTime = -1; // contains the first time we saw this shard with no operations done on it
-
-
-        /** update status based on a new sample. updates all internal variables */
-        public void updateWith(long currentNanoTime, ShardIndexingStatus current, long inactiveNanoInterval) {
-            final boolean idle = (translogId == current.translogId && translogNumberOfOperations == current.translogNumberOfOperations);
-            if (activeIndexing && idle) {
-                // no indexing activity detected.
-                if (idleSinceNanoTime < 0) {
-                    // first time we see this, start the clock.
-                    idleSinceNanoTime = currentNanoTime;
-                } else if ((currentNanoTime - idleSinceNanoTime) > inactiveNanoInterval) {
-                    // shard is inactive. mark it as such.
-                    activeIndexing = false;
-                }
-            } else if (activeIndexing == false  // we weren't indexing before
-                    && idle == false // but we do now
-                    && current.translogNumberOfOperations > 0 // but only if we're really sure - see note bellow
-                    ) {
-                // since we sync flush once a shard becomes inactive, the translog id can change, however that
-                // doesn't mean the an indexing operation has happened. Note that if we're really unlucky and a flush happens
-                // immediately after an indexing operation we may not become active immediately. The following
-                // indexing operation will mark the shard as active, so it's OK. If that one doesn't come, we might as well stay
-                // inactive
-
-                activeIndexing = true;
-                idleSinceNanoTime = -1;
-            }
-
-            translogId = current.translogId;
-            translogNumberOfOperations = current.translogNumberOfOperations;
-        }
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/monitor/fs/FsInfo.java b/core/src/main/java/org/elasticsearch/monitor/fs/FsInfo.java
index 6fcbec5..b97457a 100644
--- a/core/src/main/java/org/elasticsearch/monitor/fs/FsInfo.java
+++ b/core/src/main/java/org/elasticsearch/monitor/fs/FsInfo.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.monitor.fs;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
@@ -30,6 +29,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
 
 import java.io.IOException;
+import java.util.Arrays;
 import java.util.HashSet;
 import java.util.Iterator;
 import java.util.Set;
@@ -235,7 +235,7 @@ public class FsInfo implements Iterable<FsInfo.Path>, Streamable, ToXContent {
 
     @Override
     public Iterator<Path> iterator() {
-        return Iterators.forArray(paths);
+        return Arrays.stream(paths).iterator();
     }
 
     public static FsInfo readFsInfo(StreamInput in) throws IOException {
diff --git a/core/src/main/java/org/elasticsearch/monitor/jvm/JvmStats.java b/core/src/main/java/org/elasticsearch/monitor/jvm/JvmStats.java
index d219488..c695e26 100644
--- a/core/src/main/java/org/elasticsearch/monitor/jvm/JvmStats.java
+++ b/core/src/main/java/org/elasticsearch/monitor/jvm/JvmStats.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.monitor.jvm;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
@@ -32,6 +31,7 @@ import org.elasticsearch.common.xcontent.XContentBuilderString;
 import java.io.IOException;
 import java.lang.management.*;
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.Iterator;
 import java.util.List;
 import java.util.concurrent.TimeUnit;
@@ -378,7 +378,7 @@ public class JvmStats implements Streamable, ToXContent {
 
         @Override
         public Iterator<GarbageCollector> iterator() {
-            return Iterators.forArray(collectors);
+            return Arrays.stream(collectors).iterator();
         }
     }
 
@@ -546,7 +546,7 @@ public class JvmStats implements Streamable, ToXContent {
 
         @Override
         public Iterator<MemoryPool> iterator() {
-            return Iterators.forArray(pools);
+            return Arrays.stream(pools).iterator();
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java b/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java
index 7aacde5..3f35ddf 100644
--- a/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java
+++ b/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java
@@ -83,42 +83,20 @@ public class InternalSettingsPreparer {
         initializeSettings(output, input, true);
         Environment environment = new Environment(output.build());
 
-        // TODO: can we simplify all of this and have a single filename, which is looked up in the config dir?
-        boolean loadFromEnv = true;
-        if (useSystemProperties(input)) {
-            // if its default, then load it, but also load form env
-            if (Strings.hasText(System.getProperty("es.default.config"))) {
-                // TODO: we don't allow multiple config files, but having loadFromEnv true here allows just that
-                loadFromEnv = true;
-                output.loadFromPath(environment.configFile().resolve(System.getProperty("es.default.config")));
-            }
-            // TODO: these should be elseifs so that multiple files cannot be loaded
-            // if explicit, just load it and don't load from env
-            if (Strings.hasText(System.getProperty("es.config"))) {
-                loadFromEnv = false;
-                output.loadFromPath(environment.configFile().resolve(System.getProperty("es.config")));
-            }
-            if (Strings.hasText(System.getProperty("elasticsearch.config"))) {
-                loadFromEnv = false;
-                output.loadFromPath(environment.configFile().resolve(System.getProperty("elasticsearch.config")));
-            }
-        }
-        if (loadFromEnv) {
-            boolean settingsFileFound = false;
-            Set<String> foundSuffixes = new HashSet<>();
-            for (String allowedSuffix : ALLOWED_SUFFIXES) {
-                Path path = environment.configFile().resolve("elasticsearch" + allowedSuffix);
-                if (Files.exists(path)) {
-                    if (!settingsFileFound) {
-                        output.loadFromPath(path);
-                    }
-                    settingsFileFound = true;
-                    foundSuffixes.add(allowedSuffix);
+        boolean settingsFileFound = false;
+        Set<String> foundSuffixes = new HashSet<>();
+        for (String allowedSuffix : ALLOWED_SUFFIXES) {
+            Path path = environment.configFile().resolve("elasticsearch" + allowedSuffix);
+            if (Files.exists(path)) {
+                if (!settingsFileFound) {
+                    output.loadFromPath(path);
                 }
+                settingsFileFound = true;
+                foundSuffixes.add(allowedSuffix);
             }
-            if (foundSuffixes.size() > 1) {
-                throw new SettingsException("multiple settings files found with suffixes: " + Strings.collectionToDelimitedString(foundSuffixes, ","));
-            }
+        }
+        if (foundSuffixes.size() > 1) {
+            throw new SettingsException("multiple settings files found with suffixes: " + Strings.collectionToDelimitedString(foundSuffixes, ","));
         }
 
         // re-initialize settings now that the config file has been loaded
diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginManager.java b/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
index 9e7a6fc..2267d5e 100644
--- a/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
+++ b/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
@@ -19,14 +19,8 @@
 
 package org.elasticsearch.plugins;
 
-import com.google.common.collect.Iterators;
-
 import org.apache.lucene.util.IOUtils;
-import org.elasticsearch.Build;
-import org.elasticsearch.ElasticsearchCorruptionException;
-import org.elasticsearch.ElasticsearchTimeoutException;
-import org.elasticsearch.ExceptionsHelper;
-import org.elasticsearch.Version;
+import org.elasticsearch.*;
 import org.elasticsearch.bootstrap.JarHell;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.cli.Terminal;
@@ -41,21 +35,12 @@ import java.io.IOException;
 import java.io.OutputStream;
 import java.net.MalformedURLException;
 import java.net.URL;
-import java.nio.file.DirectoryStream;
-import java.nio.file.FileVisitResult;
-import java.nio.file.Files;
-import java.nio.file.Path;
-import java.nio.file.SimpleFileVisitor;
+import java.nio.file.*;
 import java.nio.file.attribute.BasicFileAttributes;
 import java.nio.file.attribute.PosixFileAttributeView;
 import java.nio.file.attribute.PosixFilePermission;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Locale;
-import java.util.Random;
-import java.util.Set;
+import java.util.*;
+import java.util.stream.StreamSupport;
 import java.util.zip.ZipEntry;
 import java.util.zip.ZipInputStream;
 
@@ -90,10 +75,10 @@ public class PluginManager {
             "analysis-phonetic",
             "analysis-smartcn",
             "analysis-stempel",
-            "cloud-gce",
             "delete-by-query",
             "discovery-azure",
             "discovery-ec2",
+            "discovery-gce",
             "discovery-multicast",
             "lang-expression",
             "lang-groovy",
@@ -225,7 +210,6 @@ public class PluginManager {
     }
 
     private void extract(PluginHandle pluginHandle, Terminal terminal, Path pluginFile) throws IOException {
-
         // unzip plugin to a staging temp dir, named for the plugin
         Path tmp = Files.createTempDirectory(environment.tmpFile(), null);
         Path root = tmp.resolve(pluginHandle.name);
@@ -255,22 +239,74 @@ public class PluginManager {
         terminal.println("Installed %s into %s", pluginHandle.name, extractLocation.toAbsolutePath());
 
         // cleanup
-        IOUtils.rm(tmp, pluginFile);
+        tryToDeletePath(terminal, tmp, pluginFile);
 
         // take care of bin/ by moving and applying permissions if needed
-        Path binFile = extractLocation.resolve("bin");
-        if (Files.isDirectory(binFile)) {
-            Path toLocation = pluginHandle.binDir(environment);
-            terminal.println(VERBOSE, "Found bin, moving to %s", toLocation.toAbsolutePath());
-            if (Files.exists(toLocation)) {
-                IOUtils.rm(toLocation);
+        Path sourcePluginBinDirectory = extractLocation.resolve("bin");
+        Path destPluginBinDirectory = pluginHandle.binDir(environment);
+        boolean needToCopyBinDirectory = Files.exists(sourcePluginBinDirectory);
+        if (needToCopyBinDirectory) {
+            if (Files.exists(destPluginBinDirectory) && !Files.isDirectory(destPluginBinDirectory)) {
+                tryToDeletePath(terminal, extractLocation);
+                throw new IOException("plugin bin directory " + destPluginBinDirectory + " is not a directory");
+            }
+
+            try {
+                copyBinDirectory(sourcePluginBinDirectory, destPluginBinDirectory, pluginHandle.name, terminal);
+            } catch (IOException e) {
+                // rollback and remove potentially before installed leftovers
+                terminal.printError("Error copying bin directory [%s] to [%s], cleaning up, reason: %s", sourcePluginBinDirectory, pluginHandle.binDir(environment), e.getMessage());
+                tryToDeletePath(terminal, extractLocation, pluginHandle.binDir(environment));
+                throw e;
+            }
+
+        }
+
+        Path sourceConfigDirectory = extractLocation.resolve("config");
+        Path destConfigDirectory = pluginHandle.configDir(environment);
+        boolean needToCopyConfigDirectory = Files.exists(sourceConfigDirectory);
+        if (needToCopyConfigDirectory) {
+            if (Files.exists(destConfigDirectory) && !Files.isDirectory(destConfigDirectory)) {
+                tryToDeletePath(terminal, extractLocation, pluginHandle.binDir(environment));
+                throw new IOException("plugin config directory " + destConfigDirectory + " is not a directory");
+            }
+
+            try {
+                terminal.println(VERBOSE, "Found config, moving to %s", destConfigDirectory.toAbsolutePath());
+                moveFilesWithoutOverwriting(sourceConfigDirectory, destConfigDirectory, ".new");
+                terminal.println(VERBOSE, "Installed %s into %s", pluginHandle.name, destConfigDirectory.toAbsolutePath());
+            } catch (IOException e) {
+                terminal.printError("Error copying config directory [%s] to [%s], cleaning up, reason: %s", sourceConfigDirectory, pluginHandle.binDir(environment), e.getMessage());
+                tryToDeletePath(terminal, extractLocation, pluginHandle.binDir(environment), pluginHandle.configDir(environment));
+                throw e;
+            }
+        }
+    }
+
+    private void tryToDeletePath(Terminal terminal, Path ... paths) {
+        for (Path path : paths) {
+            try {
+                IOUtils.rm(path);
+            } catch (IOException e) {
+                terminal.printError(e);
+            }
+        }
+    }
+
+    private void copyBinDirectory(Path sourcePluginBinDirectory, Path destPluginBinDirectory, String pluginName, Terminal terminal) throws IOException {
+        boolean canCopyFromSource = Files.exists(sourcePluginBinDirectory) && Files.isReadable(sourcePluginBinDirectory) && Files.isDirectory(sourcePluginBinDirectory);
+        if (canCopyFromSource) {
+            terminal.println(VERBOSE, "Found bin, moving to %s", destPluginBinDirectory.toAbsolutePath());
+            if (Files.exists(destPluginBinDirectory)) {
+                IOUtils.rm(destPluginBinDirectory);
             }
             try {
-                FileSystemUtils.move(binFile, toLocation);
+                Files.createDirectories(destPluginBinDirectory.getParent());
+                FileSystemUtils.move(sourcePluginBinDirectory, destPluginBinDirectory);
             } catch (IOException e) {
-                throw new IOException("Could not move [" + binFile + "] to [" + toLocation + "]", e);
+                throw new IOException("Could not move [" + sourcePluginBinDirectory + "] to [" + destPluginBinDirectory + "]", e);
             }
-            if (Environment.getFileStore(toLocation).supportsFileAttributeView(PosixFileAttributeView.class)) {
+            if (Environment.getFileStore(destPluginBinDirectory).supportsFileAttributeView(PosixFileAttributeView.class)) {
                 // add read and execute permissions to existing perms, so execution will work.
                 // read should generally be set already, but set it anyway: don't rely on umask...
                 final Set<PosixFilePermission> executePerms = new HashSet<>();
@@ -280,7 +316,7 @@ public class PluginManager {
                 executePerms.add(PosixFilePermission.OWNER_EXECUTE);
                 executePerms.add(PosixFilePermission.GROUP_EXECUTE);
                 executePerms.add(PosixFilePermission.OTHERS_EXECUTE);
-                Files.walkFileTree(toLocation, new SimpleFileVisitor<Path>() {
+                Files.walkFileTree(destPluginBinDirectory, new SimpleFileVisitor<Path>() {
                     @Override
                     public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException {
                         if (attrs.isRegularFile()) {
@@ -294,15 +330,7 @@ public class PluginManager {
             } else {
                 terminal.println(VERBOSE, "Skipping posix permissions - filestore doesn't support posix permission");
             }
-            terminal.println(VERBOSE, "Installed %s into %s", pluginHandle.name, toLocation.toAbsolutePath());
-        }
-
-        Path configFile = extractLocation.resolve("config");
-        if (Files.isDirectory(configFile)) {
-            Path configDestLocation = pluginHandle.configDir(environment);
-            terminal.println(VERBOSE, "Found config, moving to %s", configDestLocation.toAbsolutePath());
-            moveFilesWithoutOverwriting(configFile, configDestLocation, ".new");
-            terminal.println(VERBOSE, "Installed %s into %s", pluginHandle.name, configDestLocation.toAbsolutePath());
+            terminal.println(VERBOSE, "Installed %s into %s", pluginName, destPluginBinDirectory.toAbsolutePath());
         }
     }
 
@@ -437,7 +465,7 @@ public class PluginManager {
         }
 
         try (DirectoryStream<Path> stream = Files.newDirectoryStream(environment.pluginsFile())) {
-            return Iterators.toArray(stream.iterator(), Path.class);
+            return StreamSupport.stream(stream.spliterator(), false).toArray(length -> new Path[length]);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/hotthreads/RestNodesHotThreadsAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/hotthreads/RestNodesHotThreadsAction.java
index 5e37407..c1a39cc 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/hotthreads/RestNodesHotThreadsAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/hotthreads/RestNodesHotThreadsAction.java
@@ -58,6 +58,7 @@ public class RestNodesHotThreadsAction extends BaseRestHandler {
         nodesHotThreadsRequest.type(request.param("type", nodesHotThreadsRequest.type()));
         nodesHotThreadsRequest.interval(TimeValue.parseTimeValue(request.param("interval"), nodesHotThreadsRequest.interval(), "interval"));
         nodesHotThreadsRequest.snapshots(request.paramAsInt("snapshots", nodesHotThreadsRequest.snapshots()));
+        nodesHotThreadsRequest.timeout(request.param("timeout"));
         client.admin().cluster().nodesHotThreads(nodesHotThreadsRequest, new RestResponseListener<NodesHotThreadsResponse>(channel) {
             @Override
             public RestResponse buildResponse(NodesHotThreadsResponse response) throws Exception {
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/info/RestNodesInfoAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/info/RestNodesInfoAction.java
index aed9514..f2c5185 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/info/RestNodesInfoAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/info/RestNodesInfoAction.java
@@ -87,6 +87,7 @@ public class RestNodesInfoAction extends BaseRestHandler {
         }
 
         final NodesInfoRequest nodesInfoRequest = new NodesInfoRequest(nodeIds);
+        nodesInfoRequest.timeout(request.param("timeout"));
         // shortcut, dont do checks if only all is specified
         if (metrics.size() == 1 && metrics.contains("_all")) {
             nodesInfoRequest.all();
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/stats/RestNodesStatsAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/stats/RestNodesStatsAction.java
index fa146b5..2e3927e 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/stats/RestNodesStatsAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/stats/RestNodesStatsAction.java
@@ -60,6 +60,7 @@ public class RestNodesStatsAction extends BaseRestHandler {
         Set<String> metrics = Strings.splitStringByCommaToSet(request.param("metric", "_all"));
 
         NodesStatsRequest nodesStatsRequest = new NodesStatsRequest(nodesIds);
+        nodesStatsRequest.timeout(request.param("timeout"));
 
         if (metrics.size() == 1 && metrics.contains("_all")) {
             nodesStatsRequest.all();
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/stats/RestClusterStatsAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/stats/RestClusterStatsAction.java
index 572a48d..975c460 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/stats/RestClusterStatsAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/stats/RestClusterStatsAction.java
@@ -43,6 +43,7 @@ public class RestClusterStatsAction extends BaseRestHandler {
     @Override
     public void handleRequest(final RestRequest request, final RestChannel channel, final Client client) {
         ClusterStatsRequest clusterStatsRequest = new ClusterStatsRequest().nodesIds(request.paramAsStringArray("nodeId", null));
+        clusterStatsRequest.timeout(request.param("timeout"));
         client.admin().cluster().clusterStats(clusterStatsRequest, new RestToXContentListener<ClusterStatsResponse>(channel));
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/query/RestValidateQueryAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/query/RestValidateQueryAction.java
index 85d6c57..6766196 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/query/RestValidateQueryAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/query/RestValidateQueryAction.java
@@ -29,13 +29,7 @@ import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.BytesRestResponse;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.rest.RestResponse;
+import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestBuilderListener;
 
@@ -67,10 +61,8 @@ public class RestValidateQueryAction extends BaseRestHandler {
         if (RestActions.hasBodyContent(request)) {
             validateQueryRequest.source(RestActions.getRestContent(request));
         } else {
-            QueryBuilder<?> queryBuilder = RestActions.urlParamsToQueryBuilder(request);
-            if (queryBuilder != null) {
-                QuerySourceBuilder querySourceBuilder = new QuerySourceBuilder();
-                querySourceBuilder.setQuery(queryBuilder);
+            QuerySourceBuilder querySourceBuilder = RestActions.parseQuerySource(request);
+            if (querySourceBuilder != null) {
                 validateQueryRequest.source(querySourceBuilder);
             }
         }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/put/RestPutWarmerAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/put/RestPutWarmerAction.java
index 2a4650b..4c421cc 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/put/RestPutWarmerAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/put/RestPutWarmerAction.java
@@ -19,24 +19,17 @@
 package org.elasticsearch.rest.action.admin.indices.warmer.put;
 
 import org.elasticsearch.action.admin.indices.warmer.put.PutWarmerRequest;
+import org.elasticsearch.action.admin.indices.warmer.put.PutWarmerResponse;
 import org.elasticsearch.action.search.SearchRequest;
 import org.elasticsearch.action.support.IndicesOptions;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
+import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.AcknowledgedRestListener;
-import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
 
-import java.io.IOException;
-
 import static org.elasticsearch.rest.RestRequest.Method.POST;
 import static org.elasticsearch.rest.RestRequest.Method.PUT;
 
@@ -44,12 +37,9 @@ import static org.elasticsearch.rest.RestRequest.Method.PUT;
  */
 public class RestPutWarmerAction extends BaseRestHandler {
 
-    private final IndicesQueriesRegistry queryRegistry;
-
     @Inject
-    public RestPutWarmerAction(Settings settings, RestController controller, Client client, IndicesQueriesRegistry queryRegistry) {
+    public RestPutWarmerAction(Settings settings, RestController controller, Client client) {
         super(settings, controller, client);
-        this.queryRegistry = queryRegistry;
         controller.registerHandler(PUT, "/_warmer/{name}", this);
         controller.registerHandler(PUT, "/{index}/_warmer/{name}", this);
         controller.registerHandler(PUT, "/{index}/{type}/_warmer/{name}", this);
@@ -68,14 +58,12 @@ public class RestPutWarmerAction extends BaseRestHandler {
     }
 
     @Override
-    public void handleRequest(final RestRequest request, final RestChannel channel, final Client client) throws IOException {
+    public void handleRequest(final RestRequest request, final RestChannel channel, final Client client) {
         PutWarmerRequest putWarmerRequest = new PutWarmerRequest(request.param("name"));
-
-        BytesReference sourceBytes = RestActions.getRestContent(request);
-        SearchSourceBuilder source = RestActions.getRestSearchSource(sourceBytes, queryRegistry);
         SearchRequest searchRequest = new SearchRequest(Strings.splitStringByCommaToArray(request.param("index")))
                 .types(Strings.splitStringByCommaToArray(request.param("type")))
-                .requestCache(request.paramAsBoolean("request_cache", null)).source(source);
+                .requestCache(request.paramAsBoolean("request_cache", null))
+                .source(request.content());
         searchRequest.indicesOptions(IndicesOptions.fromRequest(request, searchRequest.indicesOptions()));
         putWarmerRequest.searchRequest(searchRequest);
         putWarmerRequest.timeout(request.paramAsTime("timeout", putWarmerRequest.timeout()));
diff --git a/core/src/main/java/org/elasticsearch/rest/action/cat/RestCountAction.java b/core/src/main/java/org/elasticsearch/rest/action/cat/RestCountAction.java
index 356e81e..72057a9 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/cat/RestCountAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/cat/RestCountAction.java
@@ -25,16 +25,9 @@ import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.Table;
-import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.rest.RestResponse;
+import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestResponseListener;
 import org.elasticsearch.rest.action.support.RestTable;
@@ -47,14 +40,11 @@ import static org.elasticsearch.rest.RestRequest.Method.GET;
 
 public class RestCountAction extends AbstractCatAction {
 
-    private final IndicesQueriesRegistry indicesQueriesRegistry;
-
     @Inject
-    public RestCountAction(Settings settings, RestController restController, RestController controller, Client client, IndicesQueriesRegistry indicesQueriesRegistry) {
+    public RestCountAction(Settings settings, RestController restController, RestController controller, Client client) {
         super(settings, controller, client);
         restController.registerHandler(GET, "/_cat/count", this);
         restController.registerHandler(GET, "/_cat/count/{index}", this);
-        this.indicesQueriesRegistry = indicesQueriesRegistry;
     }
 
     @Override
@@ -69,16 +59,14 @@ public class RestCountAction extends AbstractCatAction {
         CountRequest countRequest = new CountRequest(indices);
         String source = request.param("source");
         if (source != null) {
-            QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
-            countRequest.query(RestActions.getQueryContent(new BytesArray(source), context));
+            countRequest.source(source);
         } else {
-            QueryBuilder<?> queryBuilder = RestActions.urlParamsToQueryBuilder(request);
-            if (queryBuilder != null) {
-                QuerySourceBuilder querySourceBuilder = new QuerySourceBuilder();
-                querySourceBuilder.setQuery(queryBuilder);
-                countRequest.query(queryBuilder);
+            QuerySourceBuilder querySourceBuilder = RestActions.parseQuerySource(request);
+            if (querySourceBuilder != null) {
+                countRequest.source(querySourceBuilder);
             }
         }
+
         client.count(countRequest, new RestResponseListener<CountResponse>(channel) {
             @Override
             public RestResponse buildResponse(CountResponse countResponse) throws Exception {
diff --git a/core/src/main/java/org/elasticsearch/rest/action/count/RestCountAction.java b/core/src/main/java/org/elasticsearch/rest/action/count/RestCountAction.java
index 15bf2bf..677f3af 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/count/RestCountAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/count/RestCountAction.java
@@ -22,38 +22,29 @@ package org.elasticsearch.rest.action.count;
 import org.elasticsearch.action.count.CountRequest;
 import org.elasticsearch.action.count.CountResponse;
 import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.BytesRestResponse;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.rest.RestResponse;
+import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestBuilderListener;
 
+import static org.elasticsearch.action.count.CountRequest.DEFAULT_MIN_SCORE;
+import static org.elasticsearch.search.internal.SearchContext.DEFAULT_TERMINATE_AFTER;
 import static org.elasticsearch.rest.RestRequest.Method.GET;
 import static org.elasticsearch.rest.RestRequest.Method.POST;
 import static org.elasticsearch.rest.action.support.RestActions.buildBroadcastShardsHeader;
-import static org.elasticsearch.search.internal.SearchContext.DEFAULT_TERMINATE_AFTER;
 
 /**
  *
  */
 public class RestCountAction extends BaseRestHandler {
 
-    private final IndicesQueriesRegistry indicesQueriesRegistry;
-
     @Inject
-    public RestCountAction(Settings settings, RestController controller, Client client, IndicesQueriesRegistry indicesQueriesRegistry) {
+    public RestCountAction(Settings settings, RestController controller, Client client) {
         super(settings, controller, client);
         controller.registerHandler(POST, "/_count", this);
         controller.registerHandler(GET, "/_count", this);
@@ -61,7 +52,6 @@ public class RestCountAction extends BaseRestHandler {
         controller.registerHandler(GET, "/{index}/_count", this);
         controller.registerHandler(POST, "/{index}/{type}/_count", this);
         controller.registerHandler(GET, "/{index}/{type}/_count", this);
-        this.indicesQueriesRegistry = indicesQueriesRegistry;
     }
 
     @Override
@@ -69,20 +59,15 @@ public class RestCountAction extends BaseRestHandler {
         CountRequest countRequest = new CountRequest(Strings.splitStringByCommaToArray(request.param("index")));
         countRequest.indicesOptions(IndicesOptions.fromRequest(request, countRequest.indicesOptions()));
         if (RestActions.hasBodyContent(request)) {
-            BytesReference restContent = RestActions.getRestContent(request);
-            QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
-            countRequest.query(RestActions.getQueryContent(restContent, context));
+            countRequest.source(RestActions.getRestContent(request));
         } else {
-            QueryBuilder<?> queryBuilder = RestActions.urlParamsToQueryBuilder(request);
-            if (queryBuilder != null) {
-                countRequest.query(queryBuilder);
+            QuerySourceBuilder querySourceBuilder = RestActions.parseQuerySource(request);
+            if (querySourceBuilder != null) {
+                countRequest.source(querySourceBuilder);
             }
         }
         countRequest.routing(request.param("routing"));
-        float minScore = request.paramAsFloat("min_score", -1f);
-        if (minScore != -1f) {
-            countRequest.minScore(minScore);
-        }
+        countRequest.minScore(request.paramAsFloat("min_score", DEFAULT_MIN_SCORE));
         countRequest.types(Strings.splitStringByCommaToArray(request.param("type")));
         countRequest.preference(request.param("preference"));
 
diff --git a/core/src/main/java/org/elasticsearch/rest/action/exists/RestExistsAction.java b/core/src/main/java/org/elasticsearch/rest/action/exists/RestExistsAction.java
index 04f548b..7cfe7ca 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/exists/RestExistsAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/exists/RestExistsAction.java
@@ -27,14 +27,7 @@ import org.elasticsearch.client.Client;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.BytesRestResponse;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.rest.RestResponse;
-import org.elasticsearch.rest.RestStatus;
+import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestBuilderListener;
 
@@ -58,11 +51,9 @@ public class RestExistsAction extends BaseRestHandler {
         if (RestActions.hasBodyContent(request)) {
             existsRequest.source(RestActions.getRestContent(request));
         } else {
-            QueryBuilder<?> queryBuilder = RestActions.urlParamsToQueryBuilder(request);
-            if (queryBuilder != null) {
-                QuerySourceBuilder querySourceBuilder = new QuerySourceBuilder();
-                querySourceBuilder.setQuery(queryBuilder);
-                existsRequest.source(querySourceBuilder.buildAsBytes());
+            QuerySourceBuilder querySourceBuilder = RestActions.parseQuerySource(request);
+            if (querySourceBuilder != null) {
+                existsRequest.source(querySourceBuilder);
             }
         }
         existsRequest.routing(request.param("routing"));
diff --git a/core/src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java b/core/src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java
index 2921e91..af1f2f4 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java
@@ -20,34 +20,16 @@
 package org.elasticsearch.rest.action.search;
 
 import org.elasticsearch.action.search.MultiSearchRequest;
-import org.elasticsearch.action.search.SearchRequest;
+import org.elasticsearch.action.search.MultiSearchResponse;
 import org.elasticsearch.action.support.IndicesOptions;
 import org.elasticsearch.client.Client;
-import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContent;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.index.query.TemplateQueryParser;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
+import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestToXContentListener;
-import org.elasticsearch.script.Template;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 
-import java.util.Map;
-
-import static org.elasticsearch.common.xcontent.support.XContentMapValues.nodeBooleanValue;
-import static org.elasticsearch.common.xcontent.support.XContentMapValues.nodeStringArrayValue;
-import static org.elasticsearch.common.xcontent.support.XContentMapValues.nodeStringValue;
 import static org.elasticsearch.rest.RestRequest.Method.GET;
 import static org.elasticsearch.rest.RestRequest.Method.POST;
 
@@ -56,11 +38,9 @@ import static org.elasticsearch.rest.RestRequest.Method.POST;
 public class RestMultiSearchAction extends BaseRestHandler {
 
     private final boolean allowExplicitIndex;
-    private final IndicesQueriesRegistry indicesQueriesRegistry;
-
 
     @Inject
-    public RestMultiSearchAction(Settings settings, RestController controller, Client client, IndicesQueriesRegistry indicesQueriesRegistry) {
+    public RestMultiSearchAction(Settings settings, RestController controller, Client client) {
         super(settings, controller, client);
 
         controller.registerHandler(GET, "/_msearch", this);
@@ -78,7 +58,6 @@ public class RestMultiSearchAction extends BaseRestHandler {
         controller.registerHandler(POST, "/{index}/{type}/_msearch/template", this);
 
         this.allowExplicitIndex = settings.getAsBoolean("rest.action.multi.allow_explicit_index", true);
-        this.indicesQueriesRegistry = indicesQueriesRegistry;
     }
 
     @Override
@@ -90,117 +69,12 @@ public class RestMultiSearchAction extends BaseRestHandler {
         String path = request.path();
         boolean isTemplateRequest = isTemplateRequest(path);
         IndicesOptions indicesOptions = IndicesOptions.fromRequest(request, multiSearchRequest.indicesOptions());
-        parseRequest(multiSearchRequest, RestActions.getRestContent(request), isTemplateRequest, indices, types, request.param("search_type"), request.param("routing"), indicesOptions, allowExplicitIndex, indicesQueriesRegistry);
-        client.multiSearch(multiSearchRequest, new RestToXContentListener<>(channel));
+        multiSearchRequest.add(RestActions.getRestContent(request), isTemplateRequest, indices, types, request.param("search_type"), request.param("routing"), indicesOptions, allowExplicitIndex);
+
+        client.multiSearch(multiSearchRequest, new RestToXContentListener<MultiSearchResponse>(channel));
     }
 
     private boolean isTemplateRequest(String path) {
         return (path != null && path.endsWith("/template"));
     }
-
-    public static MultiSearchRequest parseRequest(MultiSearchRequest msr, BytesReference data, boolean isTemplateRequest,
-                                                   @Nullable String[] indices,
-                                                   @Nullable String[] types,
-                                                   @Nullable String searchType,
-                                                   @Nullable String routing,
-                                                   IndicesOptions indicesOptions,
-                                                   boolean allowExplicitIndex, IndicesQueriesRegistry indicesQueriesRegistry) throws Exception {
-        XContent xContent = XContentFactory.xContent(data);
-        int from = 0;
-        int length = data.length();
-        byte marker = xContent.streamSeparator();
-        final QueryParseContext queryParseContext = new QueryParseContext(indicesQueriesRegistry);
-        while (true) {
-            int nextMarker = findNextMarker(marker, from, data, length);
-            if (nextMarker == -1) {
-                break;
-            }
-            // support first line with \n
-            if (nextMarker == 0) {
-                from = nextMarker + 1;
-                continue;
-            }
-
-            SearchRequest searchRequest = new SearchRequest();
-            if (indices != null) {
-                searchRequest.indices(indices);
-            }
-            if (indicesOptions != null) {
-                searchRequest.indicesOptions(indicesOptions);
-            }
-            if (types != null && types.length > 0) {
-                searchRequest.types(types);
-            }
-            if (routing != null) {
-                searchRequest.routing(routing);
-            }
-            searchRequest.searchType(searchType);
-
-            IndicesOptions defaultOptions = IndicesOptions.strictExpandOpenAndForbidClosed();
-
-
-            // now parse the action
-            if (nextMarker - from > 0) {
-                try (XContentParser parser = xContent.createParser(data.slice(from, nextMarker - from))) {
-                    Map<String, Object> source = parser.map();
-                    for (Map.Entry<String, Object> entry : source.entrySet()) {
-                        Object value = entry.getValue();
-                        if ("index".equals(entry.getKey()) || "indices".equals(entry.getKey())) {
-                            if (!allowExplicitIndex) {
-                                throw new IllegalArgumentException("explicit index in multi percolate is not allowed");
-                            }
-                            searchRequest.indices(nodeStringArrayValue(value));
-                        } else if ("type".equals(entry.getKey()) || "types".equals(entry.getKey())) {
-                            searchRequest.types(nodeStringArrayValue(value));
-                        } else if ("search_type".equals(entry.getKey()) || "searchType".equals(entry.getKey())) {
-                            searchRequest.searchType(nodeStringValue(value, null));
-                        } else if ("request_cache".equals(entry.getKey()) || "requestCache".equals(entry.getKey())) {
-                            searchRequest.requestCache(nodeBooleanValue(value));
-                        } else if ("preference".equals(entry.getKey())) {
-                            searchRequest.preference(nodeStringValue(value, null));
-                        } else if ("routing".equals(entry.getKey())) {
-                            searchRequest.routing(nodeStringValue(value, null));
-                        }
-                    }
-                    defaultOptions = IndicesOptions.fromMap(source, defaultOptions);
-                }
-            }
-            searchRequest.indicesOptions(defaultOptions);
-
-            // move pointers
-            from = nextMarker + 1;
-            // now for the body
-            nextMarker = findNextMarker(marker, from, data, length);
-            if (nextMarker == -1) {
-                break;
-            }
-            final BytesReference slice = data.slice(from, nextMarker - from);
-            if (isTemplateRequest) {
-                try (XContentParser parser = XContentFactory.xContent(slice).createParser(slice)) {
-                    queryParseContext.reset(parser);
-                    Template template = TemplateQueryParser.parse(parser, queryParseContext.parseFieldMatcher(), "params", "template");
-                    searchRequest.template(template);
-                }
-            } else {
-                try (XContentParser requestParser = XContentFactory.xContent(slice).createParser(slice)) {
-                    queryParseContext.reset(requestParser);
-                    searchRequest.source(SearchSourceBuilder.parseSearchSource(requestParser, queryParseContext));
-                }
-            }
-            // move pointers
-            from = nextMarker + 1;
-
-            msr.add(searchRequest);
-        }
-        return msr;
-    }
-
-    private static int findNextMarker(byte marker, int from, BytesReference data, int length) {
-        for (int i = from; i < length; i++) {
-            if (data.get(i) == marker) {
-                return i;
-            }
-        }
-        return -1;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java b/core/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java
index 0d8caa5..03a33e0 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java
@@ -20,20 +20,15 @@
 package org.elasticsearch.rest.action.search;
 
 import org.elasticsearch.action.search.SearchRequest;
+import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.search.SearchType;
 import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.index.query.TemplateQueryParser;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.rest.BaseRestHandler;
 import org.elasticsearch.rest.RestChannel;
 import org.elasticsearch.rest.RestController;
@@ -41,16 +36,11 @@ import org.elasticsearch.rest.RestRequest;
 import org.elasticsearch.rest.action.exists.RestExistsAction;
 import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestStatusToXContentListener;
-import org.elasticsearch.script.Template;
 import org.elasticsearch.search.Scroll;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.fetch.source.FetchSourceContext;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.search.sort.SortOrder;
-import org.elasticsearch.search.suggest.SuggestBuilder;
-
-import java.io.IOException;
-import java.util.Arrays;
 
 import static org.elasticsearch.common.unit.TimeValue.parseTimeValue;
 import static org.elasticsearch.rest.RestRequest.Method.GET;
@@ -62,12 +52,9 @@ import static org.elasticsearch.search.suggest.SuggestBuilders.termSuggestion;
  */
 public class RestSearchAction extends BaseRestHandler {
 
-    private final IndicesQueriesRegistry queryRegistry;
-
     @Inject
-    public RestSearchAction(Settings settings, RestController controller, Client client, IndicesQueriesRegistry queryRegistry) {
+    public RestSearchAction(Settings settings, RestController controller, Client client) {
         super(settings, controller, client);
-        this.queryRegistry = queryRegistry;
         controller.registerHandler(GET, "/_search", this);
         controller.registerHandler(POST, "/_search", this);
         controller.registerHandler(GET, "/{index}/_search", this);
@@ -91,34 +78,24 @@ public class RestSearchAction extends BaseRestHandler {
     }
 
     @Override
-    public void handleRequest(final RestRequest request, final RestChannel channel, final Client client) throws IOException {
+    public void handleRequest(final RestRequest request, final RestChannel channel, final Client client) {
         SearchRequest searchRequest;
-        searchRequest = RestSearchAction.parseSearchRequest(queryRegistry, request, parseFieldMatcher);
-        client.search(searchRequest, new RestStatusToXContentListener<>(channel));
+        searchRequest = RestSearchAction.parseSearchRequest(request, parseFieldMatcher);
+        client.search(searchRequest, new RestStatusToXContentListener<SearchResponse>(channel));
     }
 
-    public static SearchRequest parseSearchRequest(IndicesQueriesRegistry indicesQueriesRegistry,  RestRequest request, ParseFieldMatcher parseFieldMatcher) throws IOException {
+    public static SearchRequest parseSearchRequest(RestRequest request, ParseFieldMatcher parseFieldMatcher) {
         String[] indices = Strings.splitStringByCommaToArray(request.param("index"));
         SearchRequest searchRequest = new SearchRequest(indices);
         // get the content, and put it in the body
         // add content/source as template if template flag is set
         boolean isTemplateRequest = request.path().endsWith("/template");
-        final SearchSourceBuilder builder;
         if (RestActions.hasBodyContent(request)) {
-            BytesReference restContent = RestActions.getRestContent(request);
-            QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
             if (isTemplateRequest) {
-                try (XContentParser parser = XContentFactory.xContent(restContent).createParser(restContent)) {
-                    context.reset(parser);
-                    Template template = TemplateQueryParser.parse(parser, context.parseFieldMatcher(), "params", "template");
-                    searchRequest.template(template);
-                }
-                builder = null;
+                searchRequest.templateSource(RestActions.getRestContent(request));
             } else {
-                builder = RestActions.getRestSearchSource(restContent, indicesQueriesRegistry);
+                searchRequest.source(RestActions.getRestContent(request));
             }
-        } else {
-            builder = null;
         }
 
         // do not allow 'query_and_fetch' or 'dfs_query_and_fetch' search types
@@ -131,15 +108,8 @@ public class RestSearchAction extends BaseRestHandler {
         } else {
             searchRequest.searchType(searchType);
         }
-        if (builder == null) {
-            SearchSourceBuilder extraBuilder = new SearchSourceBuilder();
-            if (parseSearchSource(extraBuilder, request)) {
-                searchRequest.source(extraBuilder);
-            }
-        } else {
-            parseSearchSource(builder, request);
-            searchRequest.source(builder);
-        }
+
+        searchRequest.extraSource(parseSearchSource(request));
         searchRequest.requestCache(request.paramAsBoolean("request_cache", null));
 
         String scroll = request.param("scroll");
@@ -155,89 +125,111 @@ public class RestSearchAction extends BaseRestHandler {
         return searchRequest;
     }
 
-    public static boolean parseSearchSource(final SearchSourceBuilder searchSourceBuilder, RestRequest request) {
+    public static SearchSourceBuilder parseSearchSource(RestRequest request) {
+        SearchSourceBuilder searchSourceBuilder = null;
 
-        boolean modified = false;
-        QueryBuilder<?> queryBuilder = RestActions.urlParamsToQueryBuilder(request);
-        if (queryBuilder != null) {
-            searchSourceBuilder.query(queryBuilder);
-            modified = true;
+        QuerySourceBuilder querySourceBuilder = RestActions.parseQuerySource(request);
+        if (querySourceBuilder != null) {
+            searchSourceBuilder = new SearchSourceBuilder();
+            searchSourceBuilder.query(querySourceBuilder);
         }
 
         int from = request.paramAsInt("from", -1);
         if (from != -1) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             searchSourceBuilder.from(from);
-            modified = true;
         }
         int size = request.paramAsInt("size", -1);
         if (size != -1) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             searchSourceBuilder.size(size);
-            modified = true;
         }
 
         if (request.hasParam("explain")) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             searchSourceBuilder.explain(request.paramAsBoolean("explain", null));
-            modified = true;
         }
         if (request.hasParam("version")) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             searchSourceBuilder.version(request.paramAsBoolean("version", null));
-            modified = true;
         }
         if (request.hasParam("timeout")) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             searchSourceBuilder.timeout(request.paramAsTime("timeout", null));
-            modified = true;
         }
         if (request.hasParam("terminate_after")) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             int terminateAfter = request.paramAsInt("terminate_after",
                     SearchContext.DEFAULT_TERMINATE_AFTER);
             if (terminateAfter < 0) {
                 throw new IllegalArgumentException("terminateAfter must be > 0");
             } else if (terminateAfter > 0) {
                 searchSourceBuilder.terminateAfter(terminateAfter);
-                modified = true;
             }
         }
 
         String sField = request.param("fields");
         if (sField != null) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             if (!Strings.hasText(sField)) {
                 searchSourceBuilder.noFields();
-                modified = true;
             } else {
                 String[] sFields = Strings.splitStringByCommaToArray(sField);
                 if (sFields != null) {
                     for (String field : sFields) {
                         searchSourceBuilder.field(field);
-                        modified = true;
                     }
                 }
             }
         }
         String sFieldDataFields = request.param("fielddata_fields");
         if (sFieldDataFields != null) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             if (Strings.hasText(sFieldDataFields)) {
                 String[] sFields = Strings.splitStringByCommaToArray(sFieldDataFields);
                 if (sFields != null) {
                     for (String field : sFields) {
                         searchSourceBuilder.fieldDataField(field);
-                        modified = true;
                     }
                 }
             }
         }
         FetchSourceContext fetchSourceContext = FetchSourceContext.parseFromRestRequest(request);
         if (fetchSourceContext != null) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             searchSourceBuilder.fetchSource(fetchSourceContext);
-            modified = true;
         }
 
         if (request.hasParam("track_scores")) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             searchSourceBuilder.trackScores(request.paramAsBoolean("track_scores", false));
-            modified = true;
         }
 
         String sSorts = request.param("sort");
         if (sSorts != null) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             String[] sorts = Strings.splitStringByCommaToArray(sSorts);
             for (String sort : sorts) {
                 int delimiter = sort.lastIndexOf(":");
@@ -246,33 +238,37 @@ public class RestSearchAction extends BaseRestHandler {
                     String reverse = sort.substring(delimiter + 1);
                     if ("asc".equals(reverse)) {
                         searchSourceBuilder.sort(sortField, SortOrder.ASC);
-                        modified = true;
                     } else if ("desc".equals(reverse)) {
                         searchSourceBuilder.sort(sortField, SortOrder.DESC);
-                        modified = true;
                     }
                 } else {
                     searchSourceBuilder.sort(sort);
-                    modified = true;
                 }
             }
         }
 
         String sStats = request.param("stats");
         if (sStats != null) {
-            searchSourceBuilder.stats(Arrays.asList(Strings.splitStringByCommaToArray(sStats)));
-            modified = true;
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
+            searchSourceBuilder.stats(Strings.splitStringByCommaToArray(sStats));
         }
 
         String suggestField = request.param("suggest_field");
         if (suggestField != null) {
             String suggestText = request.param("suggest_text", request.param("q"));
             int suggestSize = request.paramAsInt("suggest_size", 5);
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             String suggestMode = request.param("suggest_mode");
-            searchSourceBuilder.suggest(new SuggestBuilder().addSuggestion(
-                    termSuggestion(suggestField).field(suggestField).text(suggestText).size(suggestSize).suggestMode(suggestMode)));
-            modified = true;
+            searchSourceBuilder.suggest().addSuggestion(
+                    termSuggestion(suggestField).field(suggestField).text(suggestText).size(suggestSize)
+                            .suggestMode(suggestMode)
+            );
         }
-        return modified;
+
+        return searchSourceBuilder;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java b/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java
index 8610879..674aa69 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java
@@ -19,27 +19,18 @@
 
 package org.elasticsearch.rest.action.support;
 
-import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.ShardOperationFailedException;
+import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.action.support.broadcast.BroadcastResponse;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.lucene.uid.Versions;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentBuilderString;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentType;
+import org.elasticsearch.common.xcontent.*;
 import org.elasticsearch.index.query.Operator;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryStringQueryBuilder;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 
 import java.io.IOException;
 
@@ -94,7 +85,7 @@ public class RestActions {
         builder.endObject();
     }
 
-    public static QueryBuilder<?> urlParamsToQueryBuilder(RestRequest request) {
+    public static QuerySourceBuilder parseQuerySource(RestRequest request) {
         String queryString = request.param("q");
         if (queryString == null) {
             return null;
@@ -109,16 +100,7 @@ public class RestActions {
         if (defaultOperator != null) {
             queryBuilder.defaultOperator(Operator.fromString(defaultOperator));
         }
-        return queryBuilder;
-    }
-
-    public static SearchSourceBuilder getRestSearchSource(BytesReference sourceBytes, IndicesQueriesRegistry queryRegistry)
-            throws IOException {
-        XContentParser parser = XContentFactory.xContent(sourceBytes).createParser(sourceBytes);
-        QueryParseContext queryParseContext = new QueryParseContext(queryRegistry);
-        queryParseContext.reset(parser);
-        SearchSourceBuilder source = SearchSourceBuilder.parseSearchSource(parser, queryParseContext);
-        return source;
+        return new QuerySourceBuilder().setQuery(queryBuilder);
     }
 
     /**
@@ -140,17 +122,6 @@ public class RestActions {
         return content;
     }
 
-    public static QueryBuilder<?> getQueryContent(BytesReference source, QueryParseContext context) {
-        try (XContentParser requestParser = XContentFactory.xContent(source).createParser(source)) {
-            context.reset(requestParser);
-            return context.parseInnerQueryBuilder();
-        } catch (IOException e) {
-            throw new ElasticsearchException("failed to parse source", e);
-        } finally {
-            context.reset(null);
-        }
-    }
-
     /**
      * guesses the content type from either payload or source parameter
      * @param request Rest request
diff --git a/core/src/main/java/org/elasticsearch/script/Template.java b/core/src/main/java/org/elasticsearch/script/Template.java
index babe488..293a8b3 100644
--- a/core/src/main/java/org/elasticsearch/script/Template.java
+++ b/core/src/main/java/org/elasticsearch/script/Template.java
@@ -46,7 +46,7 @@ public class Template extends Script {
     /**
      * Constructor for simple inline template. The template will have no lang,
      * content type or params set.
-     *
+     * 
      * @param template
      *            The inline template.
      */
@@ -56,7 +56,7 @@ public class Template extends Script {
 
     /**
      * Constructor for Template.
-     *
+     * 
      * @param template
      *            The cache key of the template to be compiled/executed. For
      *            inline templates this is the actual templates source code. For
@@ -73,13 +73,13 @@ public class Template extends Script {
      */
     public Template(String template, ScriptType type, @Nullable String lang, @Nullable XContentType xContentType,
             @Nullable Map<String, Object> params) {
-        super(template, type, lang == null ? MustacheScriptEngineService.NAME : lang, params);
+        super(template, type, lang, params);
         this.contentType = xContentType;
     }
 
     /**
      * Method for getting the {@link XContentType} of the template.
-     *
+     * 
      * @return The {@link XContentType} of the template.
      */
     public XContentType getContentType() {
diff --git a/core/src/main/java/org/elasticsearch/search/SearchService.java b/core/src/main/java/org/elasticsearch/search/SearchService.java
index d00e8dd..403f4a5 100644
--- a/core/src/main/java/org/elasticsearch/search/SearchService.java
+++ b/core/src/main/java/org/elasticsearch/search/SearchService.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.search;
 
-import com.carrotsearch.hppc.ObjectFloatHashMap;
 import com.carrotsearch.hppc.ObjectHashSet;
 import com.carrotsearch.hppc.ObjectSet;
 import com.carrotsearch.hppc.cursors.ObjectCursor;
@@ -29,6 +28,7 @@ import org.apache.lucene.index.IndexOptions;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.search.TopDocs;
+import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.search.SearchType;
 import org.elasticsearch.cache.recycler.PageCacheRecycler;
@@ -39,6 +39,7 @@ import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
@@ -46,8 +47,8 @@ import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
 import org.elasticsearch.common.util.concurrent.ConcurrentMapLong;
 import org.elasticsearch.common.util.concurrent.FutureUtils;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentLocation;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.Index;
@@ -62,7 +63,7 @@ import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.MappedFieldType.Loading;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.internal.ParentFieldMapper;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.query.TemplateQueryParser;
 import org.elasticsearch.index.search.stats.ShardSearchStats;
 import org.elasticsearch.index.search.stats.StatsGroupsParseElement;
 import org.elasticsearch.index.settings.IndexSettings;
@@ -75,10 +76,11 @@ import org.elasticsearch.indices.IndicesWarmer.WarmerContext;
 import org.elasticsearch.indices.cache.request.IndicesRequestCache;
 import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.script.ExecutableScript;
+import org.elasticsearch.script.Script.ScriptParseException;
 import org.elasticsearch.script.ScriptContext;
 import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.script.SearchScript;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
+import org.elasticsearch.script.Template;
+import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 import org.elasticsearch.search.dfs.DfsPhase;
 import org.elasticsearch.search.dfs.DfsSearchResult;
 import org.elasticsearch.search.fetch.FetchPhase;
@@ -86,10 +88,6 @@ import org.elasticsearch.search.fetch.FetchSearchResult;
 import org.elasticsearch.search.fetch.QueryFetchSearchResult;
 import org.elasticsearch.search.fetch.ScrollQueryFetchSearchResult;
 import org.elasticsearch.search.fetch.ShardFetchRequest;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsContext;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsContext.FieldDataField;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsFetchSubPhase;
-import org.elasticsearch.search.fetch.script.ScriptFieldsContext.ScriptField;
 import org.elasticsearch.search.internal.DefaultSearchContext;
 import org.elasticsearch.search.internal.InternalScrollSearchRequest;
 import org.elasticsearch.search.internal.ScrollContext;
@@ -105,6 +103,7 @@ import org.elasticsearch.search.query.ScrollQuerySearchResult;
 import org.elasticsearch.search.warmer.IndexWarmersMetaData;
 import org.elasticsearch.threadpool.ThreadPool;
 
+import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
 import java.util.concurrent.CountDownLatch;
@@ -113,6 +112,7 @@ import java.util.concurrent.Executor;
 import java.util.concurrent.ScheduledFuture;
 import java.util.concurrent.atomic.AtomicLong;
 
+import static org.elasticsearch.common.Strings.hasLength;
 import static org.elasticsearch.common.unit.TimeValue.timeValueMillis;
 import static org.elasticsearch.common.unit.TimeValue.timeValueMinutes;
 
@@ -572,16 +572,10 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
                 context.scrollContext(new ScrollContext());
                 context.scrollContext().scroll = request.scroll();
             }
-            if (request.template() != null) {
-                ExecutableScript executable = this.scriptService.executable(request.template(), ScriptContext.Standard.SEARCH, context);
-                BytesReference run = (BytesReference) executable.run();
-                try (XContentParser parser = XContentFactory.xContent(run).createParser(run)) {
-                    QueryParseContext queryParseContext = new QueryParseContext(indexService.queryParserService().indicesQueriesRegistry());
-                    queryParseContext.reset(parser);
-                    parseSource(context, SearchSourceBuilder.parseSearchSource(parser, queryParseContext));
-                }
-            }
+
+            parseTemplate(request, context);
             parseSource(context, request.source());
+            parseSource(context, request.extraSource());
 
             // if the from and size are still not set, default them
             if (context.from() == -1) {
@@ -670,229 +664,113 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
         }
     }
 
-    private void parseSource(SearchContext context, SearchSourceBuilder source) throws SearchParseException {
-        // nothing to parse...
-        if (source == null) {
-            return;
-        }
+    private void parseTemplate(ShardSearchRequest request, SearchContext searchContext) {
 
-        context.from(source.from());
-        context.size(source.size());
-        ObjectFloatHashMap<String> indexBoostMap = source.indexBoost();
-        if (indexBoostMap != null) {
-            Float indexBoost = indexBoostMap.get(context.shardTarget().index());
-            if (indexBoost != null) {
-                context.queryBoost(indexBoost);
-            }
-        }
-        if (source.query() != null) {
-            context.parsedQuery(context.queryParserService().parse(source.query()));
-        }
-        if (source.postFilter() != null) {
-            context.parsedPostFilter(context.queryParserService().parse(source.postFilter()));
-        }
-        if (source.sorts() != null) {
-            XContentParser completeSortParser = null;
-            try {
-                XContentBuilder completeSortBuilder = XContentFactory.jsonBuilder();
-                completeSortBuilder.startObject();
-                completeSortBuilder.startArray("sort");
-                for (BytesReference sort : source.sorts()) {
-                    XContentParser parser = XContentFactory.xContent(sort).createParser(sort);
-                    parser.nextToken();
-                    completeSortBuilder.copyCurrentStructure(parser);
-                }
-                completeSortBuilder.endArray();
-                completeSortBuilder.endObject();
-                BytesReference completeSortBytes = completeSortBuilder.bytes();
-                completeSortParser = XContentFactory.xContent(completeSortBytes).createParser(completeSortBytes);
-                completeSortParser.nextToken();
-                completeSortParser.nextToken();
-                completeSortParser.nextToken();
-                this.elementParsers.get("sort").parse(completeSortParser, context);
-            } catch (Exception e) {
-                String sSource = "_na_";
-                try {
-                    sSource = source.toString();
-                } catch (Throwable e1) {
-                    // ignore
-                }
-                XContentLocation location = completeSortParser != null ? completeSortParser.getTokenLocation() : null;
-                throw new SearchParseException(context, "failed to parse sort source [" + sSource + "]", location, e);
-            } // NORELEASE fix this to be more elegant
-        }
-        context.trackScores(source.trackScores());
-        if (source.minScore() != null) {
-            context.minimumScore(source.minScore());
-        }
-        context.timeoutInMillis(source.timeoutInMillis());
-        context.terminateAfter(source.terminateAfter());
-        if (source.aggregations() != null) {
-            XContentParser completeAggregationsParser = null;
-            try {
-                XContentBuilder completeAggregationsBuilder = XContentFactory.jsonBuilder();
-                completeAggregationsBuilder.startObject();
-                for (BytesReference agg : source.aggregations()) {
-                    XContentParser parser = XContentFactory.xContent(agg).createParser(agg);
-                    parser.nextToken();
-                    parser.nextToken();
-                    completeAggregationsBuilder.field(parser.currentName());
-                    parser.nextToken();
-                    completeAggregationsBuilder.copyCurrentStructure(parser);
-                }
-                completeAggregationsBuilder.endObject();
-                BytesReference completeAggregationsBytes = completeAggregationsBuilder.bytes();
-                completeAggregationsParser = XContentFactory.xContent(completeAggregationsBytes).createParser(completeAggregationsBytes);
-                completeAggregationsParser.nextToken();
-                this.elementParsers.get("aggregations").parse(completeAggregationsParser, context);
-            } catch (Exception e) {
-                String sSource = "_na_";
-                try {
-                    sSource = source.toString();
-                } catch (Throwable e1) {
-                    // ignore
-                }
-                XContentLocation location = completeAggregationsParser != null ? completeAggregationsParser.getTokenLocation() : null;
-                throw new SearchParseException(context, "failed to parse rescore source [" + sSource + "]", location, e);
-            } // NORELEASE fix this to be more elegant
-        }
-        if (source.suggest() != null) {
-            XContentParser suggestParser = null;
-            try {
-                suggestParser = XContentFactory.xContent(source.suggest()).createParser(source.suggest());
-                suggestParser.nextToken();
-                this.elementParsers.get("suggest").parse(suggestParser, context);
-            } catch (Exception e) {
-                String sSource = "_na_";
-                try {
-                    sSource = source.toString();
-                } catch (Throwable e1) {
-                    // ignore
-                }
-                XContentLocation location = suggestParser != null ? suggestParser.getTokenLocation() : null;
-                throw new SearchParseException(context, "failed to parse suggest source [" + sSource + "]", location, e);
+        BytesReference processedQuery;
+        if (request.template() != null) {
+            ExecutableScript executable = this.scriptService.executable(request.template(), ScriptContext.Standard.SEARCH, searchContext);
+            processedQuery = (BytesReference) executable.run();
+        } else {
+            if (!hasLength(request.templateSource())) {
+                return;
             }
-        }
-        if (source.rescores() != null) {
-            XContentParser completeRescoreParser = null;
+            XContentParser parser = null;
+            Template template = null;
+
             try {
-                XContentBuilder completeRescoreBuilder = XContentFactory.jsonBuilder();
-                completeRescoreBuilder.startObject();
-                completeRescoreBuilder.startArray("rescore");
-                for (BytesReference rescore : source.rescores()) {
-                    XContentParser parser = XContentFactory.xContent(rescore).createParser(rescore);
-                    parser.nextToken();
-                    completeRescoreBuilder.copyCurrentStructure(parser);
-                }
-                completeRescoreBuilder.endArray();
-                completeRescoreBuilder.endObject();
-                BytesReference completeRescoreBytes = completeRescoreBuilder.bytes();
-                completeRescoreParser = XContentFactory.xContent(completeRescoreBytes).createParser(completeRescoreBytes);
-                completeRescoreParser.nextToken();
-                completeRescoreParser.nextToken();
-                completeRescoreParser.nextToken();
-                this.elementParsers.get("rescore").parse(completeRescoreParser, context);
-            } catch (Exception e) {
-                String sSource = "_na_";
-                try {
-                    sSource = source.toString();
-                } catch (Throwable e1) {
-                    // ignore
+                parser = XContentFactory.xContent(request.templateSource()).createParser(request.templateSource());
+                template = TemplateQueryParser.parse(parser, searchContext.parseFieldMatcher(), "params", "template");
+
+                if (template.getType() == ScriptService.ScriptType.INLINE) {
+                    //Try to double parse for nested template id/file
+                    parser = null;
+                    try {
+                        ExecutableScript executable = this.scriptService.executable(template, ScriptContext.Standard.SEARCH, searchContext);
+                        processedQuery = (BytesReference) executable.run();
+                        parser = XContentFactory.xContent(processedQuery).createParser(processedQuery);
+                    } catch (ElasticsearchParseException epe) {
+                        //This was an non-nested template, the parse failure was due to this, it is safe to assume this refers to a file
+                        //for backwards compatibility and keep going
+                        template = new Template(template.getScript(), ScriptService.ScriptType.FILE, MustacheScriptEngineService.NAME,
+                                null, template.getParams());
+                        ExecutableScript executable = this.scriptService.executable(template, ScriptContext.Standard.SEARCH, searchContext);
+                        processedQuery = (BytesReference) executable.run();
+                    }
+                    if (parser != null) {
+                        try {
+                            Template innerTemplate = TemplateQueryParser.parse(parser, searchContext.parseFieldMatcher());
+                            if (hasLength(innerTemplate.getScript()) && !innerTemplate.getType().equals(ScriptService.ScriptType.INLINE)) {
+                                //An inner template referring to a filename or id
+                                template = new Template(innerTemplate.getScript(), innerTemplate.getType(),
+                                        MustacheScriptEngineService.NAME, null, template.getParams());
+                                ExecutableScript executable = this.scriptService.executable(template, ScriptContext.Standard.SEARCH,
+                                        searchContext);
+                                processedQuery = (BytesReference) executable.run();
+                            }
+                        } catch (ScriptParseException e) {
+                            // No inner template found, use original template from above
+                        }
+                    }
+                } else {
+                    ExecutableScript executable = this.scriptService.executable(template, ScriptContext.Standard.SEARCH, searchContext);
+                    processedQuery = (BytesReference) executable.run();
                 }
-                XContentLocation location = completeRescoreParser != null ? completeRescoreParser.getTokenLocation() : null;
-                throw new SearchParseException(context, "failed to parse rescore source [" + sSource + "]", location, e);
-            } // NORELEASE fix this to be more elegant
-        }
-        if (source.fields() != null) {
-            context.fieldNames().addAll(source.fields());
-        }
-        if (source.explain() != null) {
-            context.explain(source.explain());
-        }
-        if (source.fetchSource() != null) {
-            context.fetchSourceContext(source.fetchSource());
-        }
-        if (source.fieldDataFields() != null) {
-            FieldDataFieldsContext fieldDataFieldsContext = context.getFetchSubPhaseContext(FieldDataFieldsFetchSubPhase.CONTEXT_FACTORY);
-            for (String field : source.fieldDataFields()) {
-                fieldDataFieldsContext.add(new FieldDataField(field));
+            } catch (IOException e) {
+                throw new ElasticsearchParseException("Failed to parse template", e);
+            } finally {
+                Releasables.closeWhileHandlingException(parser);
             }
-            fieldDataFieldsContext.setHitExecutionNeeded(true);
-        }
-        if (source.highlighter() != null) {
-            XContentParser highlighterParser = null;
-            try {
-                highlighterParser = XContentFactory.xContent(source.highlighter()).createParser(source.highlighter());
-                this.elementParsers.get("highlight").parse(highlighterParser, context);
-            } catch (Exception e) {
-                String sSource = "_na_";
-                try {
-                    sSource = source.toString();
-                } catch (Throwable e1) {
-                    // ignore
-                }
-                XContentLocation location = highlighterParser != null ? highlighterParser.getTokenLocation() : null;
-                throw new SearchParseException(context, "failed to parse suggest source [" + sSource + "]", location, e);
+
+            if (!hasLength(template.getScript())) {
+                throw new ElasticsearchParseException("Template must have [template] field configured");
             }
         }
-        if (source.innerHits() != null) {
-            XContentParser innerHitsParser = null;
-            try {
-                innerHitsParser = XContentFactory.xContent(source.innerHits()).createParser(source.innerHits());
-                innerHitsParser.nextToken();
-                this.elementParsers.get("inner_hits").parse(innerHitsParser, context);
-            } catch (Exception e) {
-                String sSource = "_na_";
-                try {
-                    sSource = source.toString();
-                } catch (Throwable e1) {
-                    // ignore
-                }
-                XContentLocation location = innerHitsParser != null ? innerHitsParser.getTokenLocation() : null;
-                throw new SearchParseException(context, "failed to parse suggest source [" + sSource + "]", location, e);
-            }
+        request.source(processedQuery);
+    }
+
+    private void parseSource(SearchContext context, BytesReference source) throws SearchParseException {
+        // nothing to parse...
+        if (source == null || source.length() == 0) {
+            return;
         }
-        if (source.scriptFields() != null) {
-            for (org.elasticsearch.search.builder.SearchSourceBuilder.ScriptField field : source.scriptFields()) {
-                SearchScript searchScript = context.scriptService().search(context.lookup(), field.script(), ScriptContext.Standard.SEARCH);
-                context.scriptFields().add(new ScriptField(field.fieldName(), searchScript, field.ignoreFailure()));
+        XContentParser parser = null;
+        try {
+            parser = XContentFactory.xContent(source).createParser(source);
+            XContentParser.Token token;
+            token = parser.nextToken();
+            if (token != XContentParser.Token.START_OBJECT) {
+                throw new ElasticsearchParseException("failed to parse search source. source must be an object, but found [{}] instead", token.name());
             }
-        }
-        if (source.ext() != null) {
-            XContentParser extParser = null;
-            try {
-                extParser = XContentFactory.xContent(source.ext()).createParser(source.ext());
-                XContentParser.Token token = extParser.nextToken();
-                String currentFieldName = null;
-                while ((token = extParser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                    if (token == XContentParser.Token.FIELD_NAME) {
-                        currentFieldName = extParser.currentName();
+            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                if (token == XContentParser.Token.FIELD_NAME) {
+                    String fieldName = parser.currentName();
+                    parser.nextToken();
+                    SearchParseElement element = elementParsers.get(fieldName);
+                    if (element == null) {
+                        throw new SearchParseException(context, "failed to parse search source. unknown search element [" + fieldName + "]", parser.getTokenLocation());
+                    }
+                    element.parse(parser, context);
+                } else {
+                    if (token == null) {
+                        throw new ElasticsearchParseException("failed to parse search source. end of query source reached but query is not complete.");
                     } else {
-                        SearchParseElement parseElement = this.elementParsers.get(currentFieldName);
-                        if (parseElement == null) {
-                            throw new SearchParseException(context, "Unknown element [" + currentFieldName + "] in [ext]",
-                                    extParser.getTokenLocation());
-                        } else {
-                            parseElement.parse(extParser, context);
-                        }
+                        throw new ElasticsearchParseException("failed to parse search source. expected field name but got [{}]", token);
                     }
                 }
-            } catch (Exception e) {
-                String sSource = "_na_";
-                try {
-                    sSource = source.toString();
-                } catch (Throwable e1) {
-                    // ignore
-                }
-                XContentLocation location = extParser != null ? extParser.getTokenLocation() : null;
-                throw new SearchParseException(context, "failed to parse ext source [" + sSource + "]", location, e);
             }
-        }
-        if (source.version() != null) {
-            context.version(source.version());
-        }
-        if (source.stats() != null) {
-            context.groupStats(source.stats());
+        } catch (Throwable e) {
+            String sSource = "_na_";
+            try {
+                sSource = XContentHelper.convertToJson(source, false);
+            } catch (Throwable e1) {
+                // ignore
+            }
+            XContentLocation location = parser != null ? parser.getTokenLocation() : null;
+            throw new SearchParseException(context, "failed to parse search source [" + sSource + "]", location, e);
+        } finally {
+            if (parser != null) {
+                parser.close();
+            }
         }
     }
 
@@ -1186,23 +1064,17 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
                         SearchContext context = null;
                         try {
                             long now = System.nanoTime();
-                            final IndexService indexService = indicesService.indexServiceSafe(indexShard.shardId().index().name());
                             ShardSearchRequest request = new ShardSearchLocalRequest(indexShard.shardId(), indexMetaData.numberOfShards(),
-                                    SearchType.QUERY_THEN_FETCH, entry.source().build(new QueryParseContext(indexService.queryParserService().indicesQueriesRegistry())), entry.types(), entry.requestCache());
+                                    SearchType.QUERY_THEN_FETCH, entry.source(), entry.types(), entry.requestCache());
                             context = createContext(request, warmerContext.searcher());
-                            // if we use sort, we need to do query to sort on
-                            // it and load relevant field data
-                            // if not, we might as well set size=0 (and cache
-                            // if needed)
+                            // if we use sort, we need to do query to sort on it and load relevant field data
+                            // if not, we might as well set size=0 (and cache if needed)
                             if (context.sort() == null) {
                                 context.size(0);
                             }
                             boolean canCache = indicesQueryCache.canCache(request, context);
-                            // early terminate when we can cache, since we
-                            // can only do proper caching on top level searcher
-                            // also, if we can't cache, and its top, we don't
-                            // need to execute it, since we already did when its
-                            // not top
+                            // early terminate when we can cache, since we can only do proper caching on top level searcher
+                            // also, if we can't cache, and its top, we don't need to execute it, since we already did when its not top
                             if (canCache != top) {
                                 return;
                             }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsBuilder.java
index a5202c6..62bd22a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsBuilder.java
@@ -19,8 +19,8 @@
 package org.elasticsearch.search.aggregations.metrics.tophits;
 
 import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
@@ -29,6 +29,7 @@ import org.elasticsearch.search.sort.SortBuilder;
 import org.elasticsearch.search.sort.SortOrder;
 
 import java.io.IOException;
+import java.util.Map;
 
 /**
  * Builder for the {@link TopHits} aggregation.
@@ -172,6 +173,185 @@ public class TopHitsBuilder extends AbstractAggregationBuilder {
         return this;
     }
 
+    /**
+     * Adds a field to be highlighted with default fragment size of 100 characters, and
+     * default number of fragments of 5.
+     *
+     * @param name The field to highlight
+     */
+    public TopHitsBuilder addHighlightedField(String name) {
+        highlightBuilder().field(name);
+        return this;
+    }
+
+
+    /**
+     * Adds a field to be highlighted with a provided fragment size (in characters), and
+     * default number of fragments of 5.
+     *
+     * @param name         The field to highlight
+     * @param fragmentSize The size of a fragment in characters
+     */
+    public TopHitsBuilder addHighlightedField(String name, int fragmentSize) {
+        highlightBuilder().field(name, fragmentSize);
+        return this;
+    }
+
+    /**
+     * Adds a field to be highlighted with a provided fragment size (in characters), and
+     * a provided (maximum) number of fragments.
+     *
+     * @param name              The field to highlight
+     * @param fragmentSize      The size of a fragment in characters
+     * @param numberOfFragments The (maximum) number of fragments
+     */
+    public TopHitsBuilder addHighlightedField(String name, int fragmentSize, int numberOfFragments) {
+        highlightBuilder().field(name, fragmentSize, numberOfFragments);
+        return this;
+    }
+
+    /**
+     * Adds a field to be highlighted with a provided fragment size (in characters),
+     * a provided (maximum) number of fragments and an offset for the highlight.
+     *
+     * @param name              The field to highlight
+     * @param fragmentSize      The size of a fragment in characters
+     * @param numberOfFragments The (maximum) number of fragments
+     */
+    public TopHitsBuilder addHighlightedField(String name, int fragmentSize, int numberOfFragments,
+                                                    int fragmentOffset) {
+        highlightBuilder().field(name, fragmentSize, numberOfFragments, fragmentOffset);
+        return this;
+    }
+
+    /**
+     * Adds a highlighted field.
+     */
+    public TopHitsBuilder addHighlightedField(HighlightBuilder.Field field) {
+        highlightBuilder().field(field);
+        return this;
+    }
+
+    /**
+     * Set a tag scheme that encapsulates a built in pre and post tags. The allows schemes
+     * are <tt>styled</tt> and <tt>default</tt>.
+     *
+     * @param schemaName The tag scheme name
+     */
+    public TopHitsBuilder setHighlighterTagsSchema(String schemaName) {
+        highlightBuilder().tagsSchema(schemaName);
+        return this;
+    }
+
+    public TopHitsBuilder setHighlighterFragmentSize(Integer fragmentSize) {
+        highlightBuilder().fragmentSize(fragmentSize);
+        return this;
+    }
+
+    public TopHitsBuilder setHighlighterNumOfFragments(Integer numOfFragments) {
+        highlightBuilder().numOfFragments(numOfFragments);
+        return this;
+    }
+
+    public TopHitsBuilder setHighlighterFilter(Boolean highlightFilter) {
+        highlightBuilder().highlightFilter(highlightFilter);
+        return this;
+    }
+
+    /**
+     * The encoder to set for highlighting
+     */
+    public TopHitsBuilder setHighlighterEncoder(String encoder) {
+        highlightBuilder().encoder(encoder);
+        return this;
+    }
+
+    /**
+     * Explicitly set the pre tags that will be used for highlighting.
+     */
+    public TopHitsBuilder setHighlighterPreTags(String... preTags) {
+        highlightBuilder().preTags(preTags);
+        return this;
+    }
+
+    /**
+     * Explicitly set the post tags that will be used for highlighting.
+     */
+    public TopHitsBuilder setHighlighterPostTags(String... postTags) {
+        highlightBuilder().postTags(postTags);
+        return this;
+    }
+
+    /**
+     * The order of fragments per field. By default, ordered by the order in the
+     * highlighted text. Can be <tt>score</tt>, which then it will be ordered
+     * by score of the fragments.
+     */
+    public TopHitsBuilder setHighlighterOrder(String order) {
+        highlightBuilder().order(order);
+        return this;
+    }
+
+    public TopHitsBuilder setHighlighterRequireFieldMatch(boolean requireFieldMatch) {
+        highlightBuilder().requireFieldMatch(requireFieldMatch);
+        return this;
+    }
+
+    public TopHitsBuilder setHighlighterBoundaryMaxScan(Integer boundaryMaxScan) {
+        highlightBuilder().boundaryMaxScan(boundaryMaxScan);
+        return this;
+    }
+
+    public TopHitsBuilder setHighlighterBoundaryChars(char[] boundaryChars) {
+        highlightBuilder().boundaryChars(boundaryChars);
+        return this;
+    }
+
+    /**
+     * The highlighter type to use.
+     */
+    public TopHitsBuilder setHighlighterType(String type) {
+        highlightBuilder().highlighterType(type);
+        return this;
+    }
+
+    public TopHitsBuilder setHighlighterFragmenter(String fragmenter) {
+        highlightBuilder().fragmenter(fragmenter);
+        return this;
+    }
+
+    /**
+     * Sets a query to be used for highlighting all fields instead of the search query.
+     */
+    public TopHitsBuilder setHighlighterQuery(QueryBuilder highlightQuery) {
+        highlightBuilder().highlightQuery(highlightQuery);
+        return this;
+    }
+
+    /**
+     * Sets the size of the fragment to return from the beginning of the field if there are no matches to
+     * highlight and the field doesn't also define noMatchSize.
+     * @param noMatchSize integer to set or null to leave out of request.  default is null.
+     * @return this builder for chaining
+     */
+    public TopHitsBuilder setHighlighterNoMatchSize(Integer noMatchSize) {
+        highlightBuilder().noMatchSize(noMatchSize);
+        return this;
+    }
+
+    /**
+     * Sets the maximum number of phrases the fvh will consider if the field doesn't also define phraseLimit.
+     */
+    public TopHitsBuilder setHighlighterPhraseLimit(Integer phraseLimit) {
+        highlightBuilder().phraseLimit(phraseLimit);
+        return this;
+    }
+
+    public TopHitsBuilder setHighlighterOptions(Map<String, Object> options) {
+        highlightBuilder().options(options);
+        return this;
+    }
+
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
         builder.startObject(getName()).field(type);
@@ -186,12 +366,7 @@ public class TopHitsBuilder extends AbstractAggregationBuilder {
         return sourceBuilder;
     }
 
-    public BytesReference highlighter() {
+    public HighlightBuilder highlightBuilder() {
         return sourceBuilder().highlighter();
     }
-
-    public TopHitsBuilder highlighter(HighlightBuilder highlightBuilder) {
-        sourceBuilder().highlighter(highlightBuilder);
-        return this;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java
index a681bc7..48686b9 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java
@@ -19,7 +19,7 @@
 
 package org.elasticsearch.search.aggregations.pipeline.movavg;
 
-import com.google.common.collect.EvictingQueue;
+import org.elasticsearch.common.collect.EvictingQueue;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.search.aggregations.Aggregation;
@@ -102,7 +102,7 @@ public class MovAvgPipelineAggregator extends PipelineAggregator {
         InternalHistogram.Factory<? extends InternalHistogram.Bucket> factory = histo.getFactory();
 
         List newBuckets = new ArrayList<>();
-        EvictingQueue<Double> values = EvictingQueue.create(this.window);
+        EvictingQueue<Double> values = new EvictingQueue<>(this.window);
 
         long lastValidKey = 0;
         int lastValidPosition = 0;
@@ -202,7 +202,7 @@ public class MovAvgPipelineAggregator extends PipelineAggregator {
     private MovAvgModel minimize(List<? extends InternalHistogram.Bucket> buckets, InternalHistogram histo, MovAvgModel model) {
 
         int counter = 0;
-        EvictingQueue<Double> values = EvictingQueue.create(window);
+        EvictingQueue<Double> values = new EvictingQueue<>(this.window);
 
         double[] test = new double[window];
         ListIterator<? extends InternalHistogram.Bucket> iter = buckets.listIterator(buckets.size());
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/SimulatedAnealingMinimizer.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/SimulatedAnealingMinimizer.java
index bb04502..711ee22 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/SimulatedAnealingMinimizer.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/SimulatedAnealingMinimizer.java
@@ -19,7 +19,7 @@
 
 package org.elasticsearch.search.aggregations.pipeline.movavg;
 
-import com.google.common.collect.EvictingQueue;
+import org.elasticsearch.common.collect.EvictingQueue;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModel;
 
 /**
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java
index d00f064..5df97d3 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java
@@ -19,8 +19,8 @@
 
 package org.elasticsearch.search.aggregations.pipeline.serialdiff;
 
-import com.google.common.collect.EvictingQueue;
 import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.collect.EvictingQueue;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -86,7 +86,7 @@ public class SerialDiffPipelineAggregator extends PipelineAggregator {
         InternalHistogram.Factory<? extends InternalHistogram.Bucket> factory = histo.getFactory();
 
         List newBuckets = new ArrayList<>();
-        EvictingQueue<Double> lagWindow = EvictingQueue.create(lag);
+        EvictingQueue<Double> lagWindow = new EvictingQueue<>(lag);
         int counter = 0;
 
         for (InternalHistogram.Bucket bucket : buckets) {
diff --git a/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java b/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
index 47d8a7f..3b87030 100644
--- a/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
@@ -20,25 +20,19 @@
 package org.elasticsearch.search.builder;
 
 import com.carrotsearch.hppc.ObjectFloatHashMap;
-import com.carrotsearch.hppc.cursors.ObjectCursor;
-
+import java.nio.charset.StandardCharsets;
+import org.elasticsearch.ElasticsearchGenerationException;
+import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.action.support.ToXContentToBytes;
+import org.elasticsearch.client.Requests;
 import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
 import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
 import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
@@ -54,8 +48,9 @@ import org.elasticsearch.search.suggest.SuggestBuilder;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collections;
+import java.util.Iterator;
 import java.util.List;
-import java.util.Objects;
+import java.util.Map;
 
 /**
  * A search source builder allowing to easily build search source. Simple
@@ -64,43 +59,7 @@ import java.util.Objects;
  *
  * @see org.elasticsearch.action.search.SearchRequest#source(SearchSourceBuilder)
  */
-public final class SearchSourceBuilder extends ToXContentToBytes implements Writeable<SearchSourceBuilder> {
-
-    public static final ParseField FROM_FIELD = new ParseField("from");
-    public static final ParseField SIZE_FIELD = new ParseField("size");
-    public static final ParseField TIMEOUT_FIELD = new ParseField("timeout");
-    public static final ParseField TERMINATE_AFTER_FIELD = new ParseField("terminate_after");
-    public static final ParseField QUERY_FIELD = new ParseField("query");
-    public static final ParseField POST_FILTER_FIELD = new ParseField("post_filter");
-    public static final ParseField MIN_SCORE_FIELD = new ParseField("min_score");
-    public static final ParseField VERSION_FIELD = new ParseField("version");
-    public static final ParseField EXPLAIN_FIELD = new ParseField("explain");
-    public static final ParseField _SOURCE_FIELD = new ParseField("_source");
-    public static final ParseField FIELDS_FIELD = new ParseField("fields");
-    public static final ParseField FIELDDATA_FIELDS_FIELD = new ParseField("fielddata_fields");
-    public static final ParseField SCRIPT_FIELDS_FIELD = new ParseField("script_fields");
-    public static final ParseField SCRIPT_FIELD = new ParseField("script");
-    public static final ParseField IGNORE_FAILURE_FIELD = new ParseField("ignore_failure");
-    public static final ParseField SORT_FIELD = new ParseField("sort");
-    public static final ParseField TRACK_SCORES_FIELD = new ParseField("track_scores");
-    public static final ParseField INDICES_BOOST_FIELD = new ParseField("indices_boost");
-    public static final ParseField AGGREGATIONS_FIELD = new ParseField("aggregations", "aggs");
-    public static final ParseField HIGHLIGHT_FIELD = new ParseField("highlight");
-    public static final ParseField INNER_HITS_FIELD = new ParseField("inner_hits");
-    public static final ParseField SUGGEST_FIELD = new ParseField("suggest");
-    public static final ParseField RESCORE_FIELD = new ParseField("rescore");
-    public static final ParseField STATS_FIELD = new ParseField("stats");
-    public static final ParseField EXT_FIELD = new ParseField("ext");
-
-    private static final SearchSourceBuilder PROTOTYPE = new SearchSourceBuilder();
-
-    public static SearchSourceBuilder readSearchSourceFrom(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
-
-    public static SearchSourceBuilder parseSearchSource(XContentParser parser, QueryParseContext context) throws IOException {
-        return PROTOTYPE.fromXContent(parser, context);
-    }
+public class SearchSourceBuilder extends ToXContentToBytes {
 
     /**
      * A static factory method to construct a new search source.
@@ -116,9 +75,11 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
         return new HighlightBuilder();
     }
 
-    private QueryBuilder<?> queryBuilder;
+    private QuerySourceBuilder querySourceBuilder;
+
+    private QueryBuilder postQueryBuilder;
 
-    private QueryBuilder<?> postQueryBuilder;
+    private BytesReference filterBinary;
 
     private int from = -1;
 
@@ -128,7 +89,7 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
 
     private Boolean version;
 
-    private List<BytesReference> sorts;
+    private List<SortBuilder> sorts;
 
     private boolean trackScores = false;
 
@@ -142,21 +103,21 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     private List<ScriptField> scriptFields;
     private FetchSourceContext fetchSourceContext;
 
-    private List<BytesReference> aggregations;
+    private List<AbstractAggregationBuilder> aggregations;
+    private BytesReference aggregationsBinary;
 
-    private BytesReference highlightBuilder;
+    private HighlightBuilder highlightBuilder;
 
-    private BytesReference suggestBuilder;
+    private SuggestBuilder suggestBuilder;
 
-    private BytesReference innerHitsBuilder;
+    private InnerHitsBuilder innerHitsBuilder;
 
-    private List<BytesReference> rescoreBuilders;
+    private List<RescoreBuilder> rescoreBuilders;
+    private Integer defaultRescoreWindowSize;
 
     private ObjectFloatHashMap<String> indexBoost = null;
 
-    private List<String> stats;
-
-    private BytesReference ext = null;
+    private String[] stats;
 
     /**
      * Constructs a new search source builder.
@@ -165,20 +126,77 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Sets the search query for this request.
+     * Sets the query provided as a {@link QuerySourceBuilder}
+     */
+    public SearchSourceBuilder query(QuerySourceBuilder querySourceBuilder) {
+        this.querySourceBuilder = querySourceBuilder;
+        return this;
+    }
+
+    /**
+     * Constructs a new search source builder with a search query.
      *
      * @see org.elasticsearch.index.query.QueryBuilders
      */
-    public SearchSourceBuilder query(QueryBuilder<?> query) {
-        this.queryBuilder = query;
+    public SearchSourceBuilder query(QueryBuilder query) {
+        if (this.querySourceBuilder == null) {
+            this.querySourceBuilder = new QuerySourceBuilder();
+        }
+        this.querySourceBuilder.setQuery(query);
+        return this;
+    }
+
+    /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchSourceBuilder query(byte[] queryBinary) {
+        return query(queryBinary, 0, queryBinary.length);
+    }
+
+    /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchSourceBuilder query(byte[] queryBinary, int queryBinaryOffset, int queryBinaryLength) {
+        return query(new BytesArray(queryBinary, queryBinaryOffset, queryBinaryLength));
+    }
+
+    /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchSourceBuilder query(BytesReference queryBinary) {
+        if (this.querySourceBuilder == null) {
+            this.querySourceBuilder = new QuerySourceBuilder();
+        }
+        this.querySourceBuilder.setQuery(queryBinary);
         return this;
     }
 
     /**
-     * Gets the query for this request
+     * Constructs a new search source builder with a raw search query.
      */
-    public QueryBuilder<?> query() {
-        return queryBuilder;
+    public SearchSourceBuilder query(String queryString) {
+        return query(queryString.getBytes(StandardCharsets.UTF_8));
+    }
+
+    /**
+     * Constructs a new search source builder with a query from a builder.
+     */
+    public SearchSourceBuilder query(XContentBuilder query) {
+        return query(query.bytes());
+    }
+
+    /**
+     * Constructs a new search source builder with a query from a map.
+     */
+    @SuppressWarnings("unchecked")
+    public SearchSourceBuilder query(Map query) {
+        try {
+            XContentBuilder builder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
+            builder.map(query);
+            return query(builder);
+        } catch (IOException e) {
+            throw new ElasticsearchGenerationException("Failed to generate [" + query + "]", e);
+        }
     }
 
     /**
@@ -186,78 +204,96 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
      * only has affect on the search hits (not aggregations). This filter is
      * always executed as last filtering mechanism.
      */
-    public SearchSourceBuilder postFilter(QueryBuilder<?> postFilter) {
+    public SearchSourceBuilder postFilter(QueryBuilder postFilter) {
         this.postQueryBuilder = postFilter;
         return this;
     }
 
     /**
-     * Gets the post filter for this request
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
      */
-    public QueryBuilder<?> postFilter() {
-        return postQueryBuilder;
+    public SearchSourceBuilder postFilter(String postFilterString) {
+        return postFilter(postFilterString.getBytes(StandardCharsets.UTF_8));
     }
 
     /**
-     * From index to start the search from. Defaults to <tt>0</tt>.
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
      */
-    public SearchSourceBuilder from(int from) {
-        this.from = from;
-        return this;
+    public SearchSourceBuilder postFilter(byte[] postFilter) {
+        return postFilter(postFilter, 0, postFilter.length);
     }
 
     /**
-     * Gets the from index to start the search from.
-     **/
-    public int from() {
-        return from;
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
+     */
+    public SearchSourceBuilder postFilter(byte[] postFilterBinary, int postFilterBinaryOffset, int postFilterBinaryLength) {
+        return postFilter(new BytesArray(postFilterBinary, postFilterBinaryOffset, postFilterBinaryLength));
     }
 
     /**
-     * The number of search hits to return. Defaults to <tt>10</tt>.
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
      */
-    public SearchSourceBuilder size(int size) {
-        this.size = size;
+    public SearchSourceBuilder postFilter(BytesReference postFilterBinary) {
+        this.filterBinary = postFilterBinary;
         return this;
     }
 
     /**
-     * Gets the number of search hits to return.
+     * Constructs a new search source builder with a query from a builder.
      */
-    public int size() {
-        return size;
+    public SearchSourceBuilder postFilter(XContentBuilder postFilter) {
+        return postFilter(postFilter.bytes());
     }
 
     /**
-     * Sets the minimum score below which docs will be filtered out.
+     * Constructs a new search source builder with a query from a map.
      */
-    public SearchSourceBuilder minScore(float minScore) {
-        this.minScore = minScore;
+    @SuppressWarnings("unchecked")
+    public SearchSourceBuilder postFilter(Map postFilter) {
+        try {
+            XContentBuilder builder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
+            builder.map(postFilter);
+            return postFilter(builder);
+        } catch (IOException e) {
+            throw new ElasticsearchGenerationException("Failed to generate [" + postFilter + "]", e);
+        }
+    }
+
+    /**
+     * From index to start the search from. Defaults to <tt>0</tt>.
+     */
+    public SearchSourceBuilder from(int from) {
+        this.from = from;
         return this;
     }
 
     /**
-     * Gets the minimum score below which docs will be filtered out.
+     * The number of search hits to return. Defaults to <tt>10</tt>.
      */
-    public Float minScore() {
-        return minScore;
+    public SearchSourceBuilder size(int size) {
+        this.size = size;
+        return this;
     }
 
     /**
-     * Should each {@link org.elasticsearch.search.SearchHit} be returned with
-     * an explanation of the hit (ranking).
+     * Sets the minimum score below which docs will be filtered out.
      */
-    public SearchSourceBuilder explain(Boolean explain) {
-        this.explain = explain;
+    public SearchSourceBuilder minScore(float minScore) {
+        this.minScore = minScore;
         return this;
     }
 
     /**
-     * Indicates whether each search hit will be returned with an explanation of
-     * the hit (ranking)
+     * Should each {@link org.elasticsearch.search.SearchHit} be returned with
+     * an explanation of the hit (ranking).
      */
-    public Boolean explain() {
-        return explain;
+    public SearchSourceBuilder explain(Boolean explain) {
+        this.explain = explain;
+        return this;
     }
 
     /**
@@ -270,14 +306,6 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Indicates whether the document's version will be included in the search
-     * hits.
-     */
-    public Boolean version() {
-        return version;
-    }
-
-    /**
      * An optional timeout to control how long search is allowed to take.
      */
     public SearchSourceBuilder timeout(TimeValue timeout) {
@@ -286,10 +314,11 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Gets the timeout to control how long search is allowed to take.
+     * An optional timeout to control how long search is allowed to take.
      */
-    public long timeoutInMillis() {
-        return timeoutInMillis;
+    public SearchSourceBuilder timeout(String timeout) {
+        this.timeoutInMillis = TimeValue.parseTimeValue(timeout, null, getClass().getSimpleName() + ".timeout").millis();
+        return this;
     }
 
     /**
@@ -297,7 +326,7 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
      * <code>terminateAfter</code> documents
      */
     public  SearchSourceBuilder terminateAfter(int terminateAfter) {
-        if (terminateAfter < 0) {
+        if (terminateAfter <= 0) {
             throw new IllegalArgumentException("terminateAfter must be > 0");
         }
         this.terminateAfter = terminateAfter;
@@ -305,13 +334,6 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Gets the number of documents to terminate after collecting.
-     */
-    public int terminateAfter() {
-        return terminateAfter;
-    }
-
-    /**
      * Adds a sort against the given field name and the sort ordering.
      *
      * @param name
@@ -337,26 +359,11 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
      * Adds a sort builder.
      */
     public SearchSourceBuilder sort(SortBuilder sort) {
-        try {
-            if (sorts == null) {
-                sorts = new ArrayList<>();
-            }
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            sort.toXContent(builder, EMPTY_PARAMS);
-            builder.endObject();
-            sorts.add(builder.bytes());
-            return this;
-        } catch (IOException e) {
-            throw new RuntimeException(e);
+        if (sorts == null) {
+            sorts = new ArrayList<>();
         }
-    }
-
-    /**
-     * Gets the bytes representing the sort builders for this request.
-     */
-    public List<BytesReference> sorts() {
-        return sorts;
+        sorts.add(sort);
+        return this;
     }
 
     /**
@@ -369,113 +376,102 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Indicates whether scores will be tracked for this request.
+     * Add an get to perform as part of the search.
      */
-    public boolean trackScores() {
-        return trackScores;
+    public SearchSourceBuilder aggregation(AbstractAggregationBuilder aggregation) {
+        if (aggregations == null) {
+            aggregations = new ArrayList<>();
+        }
+        aggregations.add(aggregation);
+        return this;
     }
 
     /**
-     * Add an aggregation to perform as part of the search.
+     * Sets a raw (xcontent / json) addAggregation.
      */
-    public SearchSourceBuilder aggregation(AbstractAggregationBuilder aggregation) {
-        try {
-            if (aggregations == null) {
-                aggregations = new ArrayList<>();
-            }
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            aggregation.toXContent(builder, EMPTY_PARAMS);
-            builder.endObject();
-            aggregations.add(builder.bytes());
-            return this;
-        } catch (IOException e) {
-            throw new RuntimeException(e);
-        }
+    public SearchSourceBuilder aggregations(byte[] aggregationsBinary) {
+        return aggregations(aggregationsBinary, 0, aggregationsBinary.length);
     }
 
     /**
-     * Gets the bytes representing the aggregation builders for this request.
+     * Sets a raw (xcontent / json) addAggregation.
      */
-    public List<BytesReference> aggregations() {
-        return aggregations;
+    public SearchSourceBuilder aggregations(byte[] aggregationsBinary, int aggregationsBinaryOffset, int aggregationsBinaryLength) {
+        return aggregations(new BytesArray(aggregationsBinary, aggregationsBinaryOffset, aggregationsBinaryLength));
     }
 
     /**
-     * Adds highlight to perform as part of the search.
+     * Sets a raw (xcontent / json) addAggregation.
      */
-    public SearchSourceBuilder highlighter(HighlightBuilder highlightBuilder) {
-        try {
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            highlightBuilder.innerXContent(builder, EMPTY_PARAMS);
-            builder.endObject();
-            this.highlightBuilder = builder.bytes();
-            return this;
-        } catch (IOException e) {
-            throw new RuntimeException(e);
-        }
+    public SearchSourceBuilder aggregations(BytesReference aggregationsBinary) {
+        this.aggregationsBinary = aggregationsBinary;
+        return this;
     }
 
     /**
-     * Gets the bytes representing the hightlighter builder for this request.
+     * Sets a raw (xcontent / json) addAggregation.
      */
-    public BytesReference highlighter() {
-        return highlightBuilder;
+    public SearchSourceBuilder aggregations(XContentBuilder aggs) {
+        return aggregations(aggs.bytes());
     }
 
-    public SearchSourceBuilder innerHits(InnerHitsBuilder innerHitsBuilder) {
+    /**
+     * Set the rescore window size for rescores that don't specify their window.
+     */
+    public SearchSourceBuilder defaultRescoreWindowSize(int defaultRescoreWindowSize) {
+        this.defaultRescoreWindowSize = defaultRescoreWindowSize;
+        return this;
+    }
+
+    /**
+     * Sets a raw (xcontent / json) addAggregation.
+     */
+    @SuppressWarnings("unchecked")
+    public SearchSourceBuilder aggregations(Map aggregations) {
         try {
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            innerHitsBuilder.innerXContent(builder, EMPTY_PARAMS);
-            builder.endObject();
-            this.innerHitsBuilder = builder.bytes();
-            return this;
+            XContentBuilder builder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
+            builder.map(aggregations);
+            return aggregations(builder);
         } catch (IOException e) {
-            throw new RuntimeException(e);
+            throw new ElasticsearchGenerationException("Failed to generate [" + aggregations + "]", e);
         }
     }
 
+    public HighlightBuilder highlighter() {
+        if (highlightBuilder == null) {
+            highlightBuilder = new HighlightBuilder();
+        }
+        return highlightBuilder;
+    }
+
     /**
-     * Gets the bytes representing the inner hits builder for this request.
+     * Adds highlight to perform as part of the search.
      */
-    public BytesReference innerHits() {
-        return innerHitsBuilder;
+    public SearchSourceBuilder highlight(HighlightBuilder highlightBuilder) {
+        this.highlightBuilder = highlightBuilder;
+        return this;
     }
 
-    public SearchSourceBuilder suggest(SuggestBuilder suggestBuilder) {
-        try {
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            suggestBuilder.toXContent(builder, EMPTY_PARAMS);
-            this.suggestBuilder = builder.bytes();
-            return this;
-        } catch (IOException e) {
-            throw new RuntimeException(e);
+    public InnerHitsBuilder innerHitsBuilder() {
+        if (innerHitsBuilder == null) {
+            innerHitsBuilder = new InnerHitsBuilder();
         }
+        return innerHitsBuilder;
     }
 
-    /**
-     * Gets the bytes representing the suggester builder for this request.
-     */
-    public BytesReference suggest() {
+    public SuggestBuilder suggest() {
+        if (suggestBuilder == null) {
+            suggestBuilder = new SuggestBuilder("suggest");
+        }
         return suggestBuilder;
     }
 
     public SearchSourceBuilder addRescorer(RescoreBuilder rescoreBuilder) {
-        try {
-            if (rescoreBuilders == null) {
-                rescoreBuilders = new ArrayList<>();
-            }
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            rescoreBuilder.toXContent(builder, EMPTY_PARAMS);
-            builder.endObject();
-            rescoreBuilders.add(builder.bytes());
-            return this;
-        } catch (IOException e) {
-            throw new RuntimeException(e);
+        if (rescoreBuilders == null) {
+            rescoreBuilders = new ArrayList<>();
         }
+        rescoreBuilders.add(rescoreBuilder);
+        return this;
     }
 
     public SearchSourceBuilder clearRescorers() {
@@ -484,13 +480,6 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Gets the bytes representing the rescore builders for this request.
-     */
-    public List<BytesReference> rescores() {
-        return rescoreBuilders;
-    }
-
-    /**
      * Indicates whether the response should contain the stored _source for
      * every hit
      */
@@ -546,23 +535,11 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Gets the {@link FetchSourceContext} which defines how the _source should
-     * be fetched.
-     */
-    public FetchSourceContext fetchSource() {
-        return fetchSourceContext;
-    }
-
-    /**
-     * Adds a field to load and return (note, it must be stored) as part of the
-     * search request. If none are specified, the source of the document will be
-     * return.
+     * Sets no fields to be loaded, resulting in only id and type to be returned
+     * per field.
      */
-    public SearchSourceBuilder field(String name) {
-        if (fieldNames == null) {
-            fieldNames = new ArrayList<>();
-        }
-        fieldNames.add(name);
+    public SearchSourceBuilder noFields() {
+        this.fieldNames = Collections.emptyList();
         return this;
     }
 
@@ -576,19 +553,28 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Sets no fields to be loaded, resulting in only id and type to be returned
-     * per field.
+     * Adds the fields to load and return as part of the search request. If none
+     * are specified, the source of the document will be returned.
      */
-    public SearchSourceBuilder noFields() {
-        this.fieldNames = Collections.emptyList();
+    public SearchSourceBuilder fields(String... fields) {
+        if (fieldNames == null) {
+            fieldNames = new ArrayList<>();
+        }
+        Collections.addAll(fieldNames, fields);
         return this;
     }
 
     /**
-     * Gets the fields to load and return as part of the search request.
+     * Adds a field to load and return (note, it must be stored) as part of the
+     * search request. If none are specified, the source of the document will be
+     * return.
      */
-    public List<String> fields() {
-        return fieldNames;
+    public SearchSourceBuilder field(String name) {
+        if (fieldNames == null) {
+            fieldNames = new ArrayList<>();
+        }
+        fieldNames.add(name);
+        return this;
     }
 
     /**
@@ -604,13 +590,6 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Gets the field-data fields.
-     */
-    public List<String> fieldDataFields() {
-        return fieldDataFields;
-    }
-
-    /**
      * Adds a script field under the given name with the provided script.
      *
      * @param name
@@ -619,34 +598,14 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
      *            The script
      */
     public SearchSourceBuilder scriptField(String name, Script script) {
-        scriptField(name, script, false);
-        return this;
-    }
-
-    /**
-     * Adds a script field under the given name with the provided script.
-     *
-     * @param name
-     *            The name of the field
-     * @param script
-     *            The script
-     */
-    public SearchSourceBuilder scriptField(String name, Script script, boolean ignoreFailure) {
         if (scriptFields == null) {
             scriptFields = new ArrayList<>();
         }
-        scriptFields.add(new ScriptField(name, script, ignoreFailure));
+        scriptFields.add(new ScriptField(name, script));
         return this;
     }
 
     /**
-     * Gets the script fields.
-     */
-    public List<ScriptField> scriptFields() {
-        return scriptFields;
-    }
-
-    /**
      * Sets the boost a specific index will receive when the query is executeed
      * against it.
      *
@@ -664,242 +623,13 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Gets the boost a specific indices will receive when the query is
-     * executeed against them.
-     */
-    public ObjectFloatHashMap<String> indexBoost() {
-        return indexBoost;
-    }
-
-    /**
      * The stats groups this request will be aggregated under.
      */
-    public SearchSourceBuilder stats(List<String> statsGroups) {
+    public SearchSourceBuilder stats(String... statsGroups) {
         this.stats = statsGroups;
         return this;
     }
 
-    /**
-     * The stats groups this request will be aggregated under.
-     */
-    public List<String> stats() {
-        return stats;
-    }
-
-    public SearchSourceBuilder ext(XContentBuilder ext) {
-        this.ext = ext.bytes();
-        return this;
-    }
-
-    public BytesReference ext() {
-        return ext;
-    }
-
-    public SearchSourceBuilder fromXContent(XContentParser parser, QueryParseContext context) throws IOException {
-        SearchSourceBuilder builder = new SearchSourceBuilder();
-        XContentParser.Token token;
-        String currentFieldName = null;
-        if ((token = parser.nextToken()) != XContentParser.Token.START_OBJECT) {
-            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.START_OBJECT + "] but found [" + token + "]",
-                    parser.getTokenLocation());
-        }
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                currentFieldName = parser.currentName();
-            } else if (token.isValue()) {
-                if (context.parseFieldMatcher().match(currentFieldName, FROM_FIELD)) {
-                    builder.from = parser.intValue();
-                } else if (context.parseFieldMatcher().match(currentFieldName, SIZE_FIELD)) {
-                    builder.size = parser.intValue();
-                } else if (context.parseFieldMatcher().match(currentFieldName, TIMEOUT_FIELD)) {
-                    builder.timeoutInMillis = parser.longValue();
-                } else if (context.parseFieldMatcher().match(currentFieldName, TERMINATE_AFTER_FIELD)) {
-                    builder.terminateAfter = parser.intValue();
-                } else if (context.parseFieldMatcher().match(currentFieldName, MIN_SCORE_FIELD)) {
-                    builder.minScore = parser.floatValue();
-                } else if (context.parseFieldMatcher().match(currentFieldName, VERSION_FIELD)) {
-                    builder.version = parser.booleanValue();
-                } else if (context.parseFieldMatcher().match(currentFieldName, EXPLAIN_FIELD)) {
-                    builder.explain = parser.booleanValue();
-                } else if (context.parseFieldMatcher().match(currentFieldName, TRACK_SCORES_FIELD)) {
-                    builder.trackScores = parser.booleanValue();
-                } else if (context.parseFieldMatcher().match(currentFieldName, _SOURCE_FIELD)) {
-                    FetchSourceContext fetchSourceContext = FetchSourceContext.parse(parser, context);
-                    builder.fetchSourceContext = fetchSourceContext;
-                } else if (context.parseFieldMatcher().match(currentFieldName, FIELDS_FIELD)) {
-                    List<String> fieldNames = new ArrayList<>();
-                    fieldNames.add(parser.text());
-                    builder.fieldNames = fieldNames;
-                } else if (context.parseFieldMatcher().match(currentFieldName, SORT_FIELD)) {
-                    builder.sort(parser.text());
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                            parser.getTokenLocation());
-                }
-            } else if (token == XContentParser.Token.START_OBJECT) {
-                if (context.parseFieldMatcher().match(currentFieldName, QUERY_FIELD)) {
-                    builder.queryBuilder = context.parseInnerQueryBuilder();
-                } else if (context.parseFieldMatcher().match(currentFieldName, POST_FILTER_FIELD)) {
-                    builder.postQueryBuilder = context.parseInnerQueryBuilder();
-                } else if (context.parseFieldMatcher().match(currentFieldName, _SOURCE_FIELD)) {
-                    FetchSourceContext fetchSourceContext = FetchSourceContext.parse(parser, context);
-                    builder.fetchSourceContext = fetchSourceContext;
-                } else if (context.parseFieldMatcher().match(currentFieldName, SCRIPT_FIELDS_FIELD)) {
-                    List<ScriptField> scriptFields = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                        String scriptFieldName = parser.currentName();
-                        token = parser.nextToken();
-                        if (token == XContentParser.Token.START_OBJECT) {
-                            Script script = null;
-                            boolean ignoreFailure = false;
-                            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                                if (token == XContentParser.Token.FIELD_NAME) {
-                                    currentFieldName = parser.currentName();
-                                } else if (token.isValue()) {
-                                    if (context.parseFieldMatcher().match(currentFieldName, SCRIPT_FIELD)) {
-                                        script = Script.parse(parser, context.parseFieldMatcher());
-                                    } else if (context.parseFieldMatcher().match(currentFieldName, IGNORE_FAILURE_FIELD)) {
-                                        ignoreFailure = parser.booleanValue();
-                                    } else {
-                                        throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName
-                                                + "].", parser.getTokenLocation());
-                                    }
-                                } else if (token == XContentParser.Token.START_OBJECT) {
-                                    if (context.parseFieldMatcher().match(currentFieldName, SCRIPT_FIELD)) {
-                                        script = Script.parse(parser, context.parseFieldMatcher());
-                                    } else {
-                                        throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName
-                                                + "].", parser.getTokenLocation());
-                                    }
-                                } else {
-                                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName
-                                            + "].", parser.getTokenLocation());
-                                }
-                            }
-                            scriptFields.add(new ScriptField(scriptFieldName, script, ignoreFailure));
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.START_OBJECT + "] in ["
-                                    + currentFieldName + "] but found [" + token + "]", parser.getTokenLocation());
-                        }
-                    }
-                    builder.scriptFields = scriptFields;
-                } else if (context.parseFieldMatcher().match(currentFieldName, INDICES_BOOST_FIELD)) {
-                    ObjectFloatHashMap<String> indexBoost = new ObjectFloatHashMap<String>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                        if (token == XContentParser.Token.FIELD_NAME) {
-                            currentFieldName = parser.currentName();
-                        } else if (token.isValue()) {
-                            indexBoost.put(currentFieldName, parser.floatValue());
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                                    parser.getTokenLocation());
-                        }
-                    }
-                    builder.indexBoost = indexBoost;
-                } else if (context.parseFieldMatcher().match(currentFieldName, AGGREGATIONS_FIELD)) {
-                    List<BytesReference> aggregations = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                        currentFieldName = parser.currentName();
-                        token = parser.nextToken();
-                        if (token == XContentParser.Token.START_OBJECT) {
-                            XContentBuilder xContentBuilder = XContentFactory.contentBuilder(parser.contentType());
-                            xContentBuilder.startObject();
-                            xContentBuilder.field(currentFieldName);
-                            xContentBuilder.copyCurrentStructure(parser);
-                            xContentBuilder.endObject();
-                            aggregations.add(xContentBuilder.bytes());
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                                    parser.getTokenLocation());
-                        }
-                    }
-                    builder.aggregations = aggregations;
-                } else if (context.parseFieldMatcher().match(currentFieldName, HIGHLIGHT_FIELD)) {
-                    XContentBuilder xContentBuilder = XContentFactory.contentBuilder(parser.contentType()).copyCurrentStructure(parser);
-                    builder.highlightBuilder = xContentBuilder.bytes();
-                } else if (context.parseFieldMatcher().match(currentFieldName, INNER_HITS_FIELD)) {
-                    XContentBuilder xContentBuilder = XContentFactory.contentBuilder(parser.contentType()).copyCurrentStructure(parser);
-                    builder.innerHitsBuilder = xContentBuilder.bytes();
-                } else if (context.parseFieldMatcher().match(currentFieldName, SUGGEST_FIELD)) {
-                    XContentBuilder xContentBuilder = XContentFactory.contentBuilder(parser.contentType());
-                    xContentBuilder.copyCurrentStructure(parser);
-                    builder.suggestBuilder = xContentBuilder.bytes();
-                } else if (context.parseFieldMatcher().match(currentFieldName, SORT_FIELD)) {
-                    List<BytesReference> sorts = new ArrayList<>();
-                    XContentBuilder xContentBuilder = XContentFactory.contentBuilder(parser.contentType()).copyCurrentStructure(parser);
-                    sorts.add(xContentBuilder.bytes());
-                    builder.sorts = sorts;
-                } else if (context.parseFieldMatcher().match(currentFieldName, EXT_FIELD)) {
-                    XContentBuilder xContentBuilder = XContentFactory.contentBuilder(parser.contentType()).copyCurrentStructure(parser);
-                    builder.ext = xContentBuilder.bytes();
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                            parser.getTokenLocation());
-                }
-            } else if (token == XContentParser.Token.START_ARRAY) {
-
-                if (context.parseFieldMatcher().match(currentFieldName, FIELDS_FIELD)) {
-                    List<String> fieldNames = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        if (token == XContentParser.Token.VALUE_STRING) {
-                            fieldNames.add(parser.text());
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.VALUE_STRING + "] in ["
-                                    + currentFieldName + "] but found [" + token + "]", parser.getTokenLocation());
-                        }
-                    }
-                    builder.fieldNames = fieldNames;
-                } else if (context.parseFieldMatcher().match(currentFieldName, FIELDDATA_FIELDS_FIELD)) {
-                    List<String> fieldDataFields = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        if (token == XContentParser.Token.VALUE_STRING) {
-                            fieldDataFields.add(parser.text());
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.VALUE_STRING + "] in ["
-                                    + currentFieldName + "] but found [" + token + "]", parser.getTokenLocation());
-                        }
-                    }
-                    builder.fieldDataFields = fieldDataFields;
-                } else if (context.parseFieldMatcher().match(currentFieldName, SORT_FIELD)) {
-                    List<BytesReference> sorts = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        XContentBuilder xContentBuilder = XContentFactory.contentBuilder(parser.contentType()).copyCurrentStructure(parser);
-                        sorts.add(xContentBuilder.bytes());
-                    }
-                    builder.sorts = sorts;
-                } else if (context.parseFieldMatcher().match(currentFieldName, RESCORE_FIELD)) {
-                    List<BytesReference> rescoreBuilders = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        XContentBuilder xContentBuilder = XContentFactory.contentBuilder(parser.contentType()).copyCurrentStructure(parser);
-                        rescoreBuilders.add(xContentBuilder.bytes());
-                    }
-                    builder.rescoreBuilders = rescoreBuilders;
-                } else if (context.parseFieldMatcher().match(currentFieldName, STATS_FIELD)) {
-                    List<String> stats = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        if (token == XContentParser.Token.VALUE_STRING) {
-                            stats.add(parser.text());
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.VALUE_STRING + "] in ["
-                                    + currentFieldName + "] but found [" + token + "]", parser.getTokenLocation());
-                        }
-                    }
-                    builder.stats = stats;
-                } else if (context.parseFieldMatcher().match(currentFieldName, _SOURCE_FIELD)) {
-                    FetchSourceContext fetchSourceContext = FetchSourceContext.parse(parser, context);
-                    builder.fetchSourceContext = fetchSourceContext;
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                            parser.getTokenLocation());
-                }
-            } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                        parser.getTokenLocation());
-            }
-        }
-        return builder;
-    }
-
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
         builder.startObject();
@@ -910,49 +640,65 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
 
     public void innerToXContent(XContentBuilder builder, Params params) throws IOException {
         if (from != -1) {
-            builder.field(FROM_FIELD.getPreferredName(), from);
+            builder.field("from", from);
         }
         if (size != -1) {
-            builder.field(SIZE_FIELD.getPreferredName(), size);
+            builder.field("size", size);
         }
 
         if (timeoutInMillis != -1) {
-            builder.field(TIMEOUT_FIELD.getPreferredName(), timeoutInMillis);
+            builder.field("timeout", timeoutInMillis);
         }
 
         if (terminateAfter != SearchContext.DEFAULT_TERMINATE_AFTER) {
-            builder.field(TERMINATE_AFTER_FIELD.getPreferredName(), terminateAfter);
+            builder.field("terminate_after", terminateAfter);
         }
 
-        if (queryBuilder != null) {
-            builder.field(QUERY_FIELD.getPreferredName(), queryBuilder);
+        if (querySourceBuilder != null) {
+            querySourceBuilder.innerToXContent(builder, params);
         }
 
         if (postQueryBuilder != null) {
-            builder.field(POST_FILTER_FIELD.getPreferredName(), postQueryBuilder);
+            builder.field("post_filter");
+            postQueryBuilder.toXContent(builder, params);
+        }
+
+        if (filterBinary != null) {
+            if (XContentFactory.xContentType(filterBinary) == builder.contentType()) {
+                builder.rawField("filter", filterBinary);
+            } else {
+                builder.field("filter_binary", filterBinary);
+            }
         }
 
         if (minScore != null) {
-            builder.field(MIN_SCORE_FIELD.getPreferredName(), minScore);
+            builder.field("min_score", minScore);
         }
 
         if (version != null) {
-            builder.field(VERSION_FIELD.getPreferredName(), version);
+            builder.field("version", version);
         }
 
         if (explain != null) {
-            builder.field(EXPLAIN_FIELD.getPreferredName(), explain);
+            builder.field("explain", explain);
         }
 
         if (fetchSourceContext != null) {
-            builder.field(_SOURCE_FIELD.getPreferredName(), fetchSourceContext);
+            if (!fetchSourceContext.fetchSource()) {
+                builder.field("_source", false);
+            } else {
+                builder.startObject("_source");
+                builder.array("includes", fetchSourceContext.includes());
+                builder.array("excludes", fetchSourceContext.excludes());
+                builder.endObject();
+            }
         }
 
         if (fieldNames != null) {
             if (fieldNames.size() == 1) {
-                builder.field(FIELDS_FIELD.getPreferredName(), fieldNames.get(0));
+                builder.field("fields", fieldNames.get(0));
             } else {
-                builder.startArray(FIELDS_FIELD.getPreferredName());
+                builder.startArray("fields");
                 for (String fieldName : fieldNames) {
                     builder.value(fieldName);
                 }
@@ -961,37 +707,39 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
         }
 
         if (fieldDataFields != null) {
-            builder.startArray(FIELDDATA_FIELDS_FIELD.getPreferredName());
-            for (String fieldDataField : fieldDataFields) {
-                builder.value(fieldDataField);
+            builder.startArray("fielddata_fields");
+            for (String fieldName : fieldDataFields) {
+                builder.value(fieldName);
             }
             builder.endArray();
         }
 
         if (scriptFields != null) {
-            builder.startObject(SCRIPT_FIELDS_FIELD.getPreferredName());
+            builder.startObject("script_fields");
             for (ScriptField scriptField : scriptFields) {
-                scriptField.toXContent(builder, params);
+                builder.startObject(scriptField.fieldName());
+                builder.field("script", scriptField.script());
+                builder.endObject();
             }
             builder.endObject();
         }
 
         if (sorts != null) {
-            builder.startArray(SORT_FIELD.getPreferredName());
-            for (BytesReference sort : sorts) {
-                XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(sort);
-                parser.nextToken();
-                builder.copyCurrentStructure(parser);
+            builder.startArray("sort");
+            for (SortBuilder sort : sorts) {
+                builder.startObject();
+                sort.toXContent(builder, params);
+                builder.endObject();
             }
             builder.endArray();
         }
 
         if (trackScores) {
-            builder.field(TRACK_SCORES_FIELD.getPreferredName(), true);
+            builder.field("track_scores", true);
         }
 
         if (indexBoost != null) {
-            builder.startObject(INDICES_BOOST_FIELD.getPreferredName());
+            builder.startObject("indices_boost");
             assert !indexBoost.containsKey(null);
             final Object[] keys = indexBoost.keys;
             final float[] values = indexBoost.values;
@@ -1004,76 +752,82 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
         }
 
         if (aggregations != null) {
-            builder.field(AGGREGATIONS_FIELD.getPreferredName());
+            builder.field("aggregations");
             builder.startObject();
-            for (BytesReference aggregation : aggregations) {
-                XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(aggregation);
-                parser.nextToken();
-                parser.nextToken();
-                builder.copyCurrentStructure(parser);
+            for (AbstractAggregationBuilder aggregation : aggregations) {
+                aggregation.toXContent(builder, params);
             }
             builder.endObject();
         }
 
+        if (aggregationsBinary != null) {
+            if (XContentFactory.xContentType(aggregationsBinary) == builder.contentType()) {
+                builder.rawField("aggregations", aggregationsBinary);
+            } else {
+                builder.field("aggregations_binary", aggregationsBinary);
+            }
+        }
+
         if (highlightBuilder != null) {
-            builder.field(HIGHLIGHT_FIELD.getPreferredName());
-            XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(highlightBuilder);
-            parser.nextToken();
-            builder.copyCurrentStructure(parser);
+            highlightBuilder.toXContent(builder, params);
         }
 
         if (innerHitsBuilder != null) {
-            builder.field(INNER_HITS_FIELD.getPreferredName());
-            XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(innerHitsBuilder);
-            parser.nextToken();
-            builder.copyCurrentStructure(parser);
+            innerHitsBuilder.toXContent(builder, params);
         }
 
         if (suggestBuilder != null) {
-            builder.field(SUGGEST_FIELD.getPreferredName());
-            XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(suggestBuilder);
-            parser.nextToken();
-            builder.copyCurrentStructure(parser);
+            suggestBuilder.toXContent(builder, params);
         }
 
         if (rescoreBuilders != null) {
-            builder.startArray(RESCORE_FIELD.getPreferredName());
-            for (BytesReference rescoreBuilder : rescoreBuilders) {
-                XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(rescoreBuilder);
-                parser.nextToken();
-                builder.copyCurrentStructure(parser);
+            // Strip empty rescoreBuilders from the request
+            Iterator<RescoreBuilder> itr = rescoreBuilders.iterator();
+            while (itr.hasNext()) {
+                if (itr.next().isEmpty()) {
+                    itr.remove();
+                }
             }
-            builder.endArray();
-        }
 
-        if (stats != null) {
-            builder.field(STATS_FIELD.getPreferredName(), stats);
+            // Now build the request taking care to skip empty lists and only send the object form
+            // if there is just one builder.
+            if (rescoreBuilders.size() == 1) {
+                builder.startObject("rescore");
+                rescoreBuilders.get(0).toXContent(builder, params);
+                if (rescoreBuilders.get(0).windowSize() == null && defaultRescoreWindowSize != null) {
+                    builder.field("window_size", defaultRescoreWindowSize);
+                }
+                builder.endObject();
+            } else if (!rescoreBuilders.isEmpty()) {
+                builder.startArray("rescore");
+                for (RescoreBuilder rescoreBuilder : rescoreBuilders) {
+                    builder.startObject();
+                    rescoreBuilder.toXContent(builder, params);
+                    if (rescoreBuilder.windowSize() == null && defaultRescoreWindowSize != null) {
+                        builder.field("window_size", defaultRescoreWindowSize);
+                    }
+                    builder.endObject();
+                }
+                builder.endArray();
+            }
         }
 
-        if (ext != null) {
-            builder.field(EXT_FIELD.getPreferredName());
-            XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(ext);
-            parser.nextToken();
-            builder.copyCurrentStructure(parser);
+        if (stats != null) {
+            builder.startArray("stats");
+            for (String stat : stats) {
+                builder.value(stat);
+            }
+            builder.endArray();
         }
     }
 
-    public static class ScriptField implements Writeable<ScriptField>, ToXContent {
-
-        public static final ScriptField PROTOTYPE = new ScriptField(null, null);
-
-        private final boolean ignoreFailure;
+    private static class ScriptField {
         private final String fieldName;
         private final Script script;
 
         private ScriptField(String fieldName, Script script) {
-            this(fieldName, script, false);
-        }
-
-        private ScriptField(String fieldName, Script script, boolean ignoreFailure) {
             this.fieldName = fieldName;
             this.script = script;
-            this.ignoreFailure = ignoreFailure;
         }
 
         public String fieldName() {
@@ -1083,303 +837,5 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
         public Script script() {
             return script;
         }
-
-        public boolean ignoreFailure() {
-            return ignoreFailure;
-        }
-
-        @Override
-        public ScriptField readFrom(StreamInput in) throws IOException {
-            return new ScriptField(in.readString(), Script.readScript(in), in.readBoolean());
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeString(fieldName);
-            script.writeTo(out);
-            out.writeBoolean(ignoreFailure);
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(fieldName);
-            builder.field(SCRIPT_FIELD.getPreferredName(), script);
-            builder.field(IGNORE_FAILURE_FIELD.getPreferredName(), ignoreFailure);
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hash(fieldName, script, ignoreFailure);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            ScriptField other = (ScriptField) obj;
-            return Objects.equals(fieldName, other.fieldName)
-                    && Objects.equals(script, other.script)
-                    && Objects.equals(ignoreFailure, other.ignoreFailure);
-        }
-    }
-
-    @Override
-    public SearchSourceBuilder readFrom(StreamInput in) throws IOException {
-        SearchSourceBuilder builder = new SearchSourceBuilder();
-        if (in.readBoolean()) {
-            int size = in.readVInt();
-            List<BytesReference> aggregations = new ArrayList<>(size);
-            for (int i = 0; i < size; i++) {
-                aggregations.add(in.readBytesReference());
-            }
-            builder.aggregations = aggregations;
-        }
-        builder.explain = in.readOptionalBoolean();
-        builder.fetchSourceContext = FetchSourceContext.optionalReadFromStream(in);
-        boolean hasFieldDataFields = in.readBoolean();
-        if (hasFieldDataFields) {
-            int size = in.readVInt();
-            List<String> fieldDataFields = new ArrayList<>(size);
-            for (int i = 0; i < size; i++) {
-                fieldDataFields.add(in.readString());
-            }
-            builder.fieldDataFields = fieldDataFields;
-        }
-        boolean hasFieldNames = in.readBoolean();
-        if (hasFieldNames) {
-            int size = in.readVInt();
-            List<String> fieldNames = new ArrayList<>(size);
-            for (int i = 0; i < size; i++) {
-                fieldNames.add(in.readString());
-            }
-            builder.fieldNames = fieldNames;
-        }
-        builder.from = in.readVInt();
-        if (in.readBoolean()) {
-            builder.highlightBuilder = in.readBytesReference();
-        }
-        boolean hasIndexBoost = in.readBoolean();
-        if (hasIndexBoost) {
-            int size = in.readVInt();
-            ObjectFloatHashMap<String> indexBoost = new ObjectFloatHashMap<String>(size);
-            for (int i = 0; i < size; i++) {
-                indexBoost.put(in.readString(), in.readFloat());
-            }
-            builder.indexBoost = indexBoost;
-        }
-        if (in.readBoolean()) {
-            builder.innerHitsBuilder = in.readBytesReference();
-        }
-        if (in.readBoolean()) {
-            builder.minScore = in.readFloat();
-        }
-        if (in.readBoolean()) {
-            builder.postQueryBuilder = in.readQuery();
-        }
-        if (in.readBoolean()) {
-            builder.queryBuilder = in.readQuery();
-        }
-        if (in.readBoolean()) {
-            int size = in.readVInt();
-            List<BytesReference> rescoreBuilders = new ArrayList<>();
-            for (int i = 0; i < size; i++) {
-                rescoreBuilders.add(in.readBytesReference());
-            }
-            builder.rescoreBuilders = rescoreBuilders;
-        }
-        if (in.readBoolean()) {
-            int size = in.readVInt();
-            List<ScriptField> scriptFields = new ArrayList<>(size);
-            for (int i = 0; i < size; i++) {
-                scriptFields.add(ScriptField.PROTOTYPE.readFrom(in));
-            }
-            builder.scriptFields = scriptFields;
-        }
-        builder.size = in.readVInt();
-        if (in.readBoolean()) {
-            int size = in.readVInt();
-            List<BytesReference> sorts = new ArrayList<>();
-            for (int i = 0; i < size; i++) {
-                sorts.add(in.readBytesReference());
-            }
-            builder.sorts = sorts;
-        }
-        if (in.readBoolean()) {
-            int size = in.readVInt();
-            List<String> stats = new ArrayList<>();
-            for (int i = 0; i < size; i++) {
-                stats.add(in.readString());
-            }
-            builder.stats = stats;
-        }
-        if (in.readBoolean()) {
-            builder.suggestBuilder = in.readBytesReference();
-        }
-        builder.terminateAfter = in.readVInt();
-        builder.timeoutInMillis = in.readLong();
-        builder.trackScores = in.readBoolean();
-        builder.version = in.readOptionalBoolean();
-        if (in.readBoolean()) {
-            builder.ext = in.readBytesReference();
-        }
-        return builder;
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        boolean hasAggregations = aggregations != null;
-        out.writeBoolean(hasAggregations);
-        if (hasAggregations) {
-            out.writeVInt(aggregations.size());
-            for (BytesReference aggregation : aggregations) {
-                out.writeBytesReference(aggregation);
-            }
-        }
-        out.writeOptionalBoolean(explain);
-        FetchSourceContext.optionalWriteToStream(fetchSourceContext, out);
-        boolean hasFieldDataFields = fieldDataFields != null;
-        out.writeBoolean(hasFieldDataFields);
-        if (hasFieldDataFields) {
-            out.writeVInt(fieldDataFields.size());
-            for (String field : fieldDataFields) {
-                out.writeString(field);
-            }
-        }
-        boolean hasFieldNames = fieldNames != null;
-        out.writeBoolean(hasFieldNames);
-        if (hasFieldNames) {
-            out.writeVInt(fieldNames.size());
-            for (String field : fieldNames) {
-                out.writeString(field);
-            }
-        }
-        out.writeVInt(from);
-        boolean hasHighlightBuilder = highlightBuilder != null;
-        out.writeBoolean(hasHighlightBuilder);
-        if (hasHighlightBuilder) {
-            out.writeBytesReference(highlightBuilder);
-        }
-        boolean hasIndexBoost = indexBoost != null;
-        out.writeBoolean(hasIndexBoost);
-        if (hasIndexBoost) {
-            out.writeVInt(indexBoost.size());
-            for (ObjectCursor<String> key : indexBoost.keys()) {
-                out.writeString(key.value);
-                out.writeFloat(indexBoost.get(key.value));
-            }
-        }
-        boolean hasInnerHitsBuilder = innerHitsBuilder != null;
-        out.writeBoolean(hasInnerHitsBuilder);
-        if (hasInnerHitsBuilder) {
-            out.writeBytesReference(innerHitsBuilder);
-        }
-        boolean hasMinScore = minScore != null;
-        out.writeBoolean(hasMinScore);
-        if (hasMinScore) {
-            out.writeFloat(minScore);
-        }
-        boolean hasPostQuery = postQueryBuilder != null;
-        out.writeBoolean(hasPostQuery);
-        if (hasPostQuery) {
-            out.writeQuery(postQueryBuilder);
-        }
-        boolean hasQuery = queryBuilder != null;
-        out.writeBoolean(hasQuery);
-        if (hasQuery) {
-            out.writeQuery(queryBuilder);
-        }
-        boolean hasRescoreBuilders = rescoreBuilders != null;
-        out.writeBoolean(hasRescoreBuilders);
-        if (hasRescoreBuilders) {
-            out.writeVInt(rescoreBuilders.size());
-            for (BytesReference rescoreBuilder : rescoreBuilders) {
-                out.writeBytesReference(rescoreBuilder);
-            }
-        }
-        boolean hasScriptFields = scriptFields != null;
-        out.writeBoolean(hasScriptFields);
-        if (hasScriptFields) {
-            out.writeVInt(scriptFields.size());
-            for (ScriptField scriptField : scriptFields) {
-                scriptField.writeTo(out);
-            }
-        }
-        out.writeVInt(size);
-        boolean hasSorts = sorts != null;
-        out.writeBoolean(hasSorts);
-        if (hasSorts) {
-            out.writeVInt(sorts.size());
-            for (BytesReference sort : sorts) {
-                out.writeBytesReference(sort);
-            }
-        }
-        boolean hasStats = stats != null;
-        out.writeBoolean(hasStats);
-        if (hasStats) {
-            out.writeVInt(stats.size());
-            for (String stat : stats) {
-                out.writeString(stat);
-            }
-        }
-        boolean hasSuggestBuilder = suggestBuilder != null;
-        out.writeBoolean(hasSuggestBuilder);
-        if (hasSuggestBuilder) {
-            out.writeBytesReference(suggestBuilder);
-        }
-        out.writeVInt(terminateAfter);
-        out.writeLong(timeoutInMillis);
-        out.writeBoolean(trackScores);
-        out.writeOptionalBoolean(version);
-        boolean hasExt = ext != null;
-        out.writeBoolean(hasExt);
-        if (hasExt) {
-            out.writeBytesReference(ext);
-        }
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(aggregations, explain, fetchSourceContext, fieldDataFields, fieldNames, from,
-                highlightBuilder, indexBoost, innerHitsBuilder, minScore, postQueryBuilder, queryBuilder, rescoreBuilders, scriptFields,
-                size, sorts, stats, suggestBuilder, terminateAfter, timeoutInMillis, trackScores, version);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (obj.getClass() != getClass()) {
-            return false;
-        }
-        SearchSourceBuilder other = (SearchSourceBuilder) obj;
-        return Objects.equals(aggregations, other.aggregations)
-                && Objects.equals(explain, other.explain)
-                && Objects.equals(fetchSourceContext, other.fetchSourceContext)
-                && Objects.equals(fieldDataFields, other.fieldDataFields)
-                && Objects.equals(fieldNames, other.fieldNames)
-                && Objects.equals(from, other.from)
-                && Objects.equals(highlightBuilder, other.highlightBuilder)
-                && Objects.equals(indexBoost, other.indexBoost)
-                && Objects.equals(innerHitsBuilder, other.innerHitsBuilder)
-                && Objects.equals(minScore, other.minScore)
-                && Objects.equals(postQueryBuilder, other.postQueryBuilder)
-                && Objects.equals(queryBuilder, other.queryBuilder)
-                && Objects.equals(rescoreBuilders, other.rescoreBuilders)
-                && Objects.equals(scriptFields, other.scriptFields)
-                && Objects.equals(size, other.size)
-                && Objects.equals(sorts, other.sorts)
-                && Objects.equals(stats, other.stats)
-                && Objects.equals(suggestBuilder, other.suggestBuilder)
-                && Objects.equals(terminateAfter, other.terminateAfter)
-                && Objects.equals(timeoutInMillis, other.timeoutInMillis)
-                && Objects.equals(trackScores, other.trackScores)
-                && Objects.equals(version, other.version);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/FetchSubPhaseContext.java b/core/src/main/java/org/elasticsearch/search/fetch/FetchSubPhaseContext.java
index 981408b..237f435 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/FetchSubPhaseContext.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/FetchSubPhaseContext.java
@@ -33,7 +33,7 @@ public class FetchSubPhaseContext {
     /**
      * Set if this phase should be executed at all.
      */
-    public void setHitExecutionNeeded(boolean hitExecutionNeeded) {
+    void setHitExecutionNeeded(boolean hitExecutionNeeded) {
         this.hitExecutionNeeded = hitExecutionNeeded;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsBuilder.java b/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsBuilder.java
index 7941e17..a14fdfe 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsBuilder.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.search.fetch.innerhits;
 
 import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.query.QueryBuilder;
@@ -43,16 +42,12 @@ public class InnerHitsBuilder implements ToXContent {
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
         builder.startObject("inner_hits");
-        innerXContent(builder, params);
-        return builder.endObject();
-    }
-
-    public void innerXContent(XContentBuilder builder, Params params) throws IOException {
         for (Map.Entry<String, InnerHitsHolder> entry : innerHits.entrySet()) {
             builder.startObject(entry.getKey());
             entry.getValue().toXContent(builder, params);
             builder.endObject();
         }
+        return builder.endObject();
     }
 
     /**
@@ -266,12 +261,187 @@ public class InnerHitsBuilder implements ToXContent {
             return this;
         }
 
-        public BytesReference highlighter() {
+        public HighlightBuilder highlightBuilder() {
             return sourceBuilder().highlighter();
         }
 
-        public InnerHit highlighter(HighlightBuilder highlightBuilder) {
-            sourceBuilder().highlighter(highlightBuilder);
+        /**
+         * Adds a field to be highlighted with default fragment size of 100 characters, and
+         * default number of fragments of 5.
+         *
+         * @param name The field to highlight
+         */
+        public InnerHit addHighlightedField(String name) {
+            highlightBuilder().field(name);
+            return this;
+        }
+
+
+        /**
+         * Adds a field to be highlighted with a provided fragment size (in characters), and
+         * default number of fragments of 5.
+         *
+         * @param name         The field to highlight
+         * @param fragmentSize The size of a fragment in characters
+         */
+        public InnerHit addHighlightedField(String name, int fragmentSize) {
+            highlightBuilder().field(name, fragmentSize);
+            return this;
+        }
+
+        /**
+         * Adds a field to be highlighted with a provided fragment size (in characters), and
+         * a provided (maximum) number of fragments.
+         *
+         * @param name              The field to highlight
+         * @param fragmentSize      The size of a fragment in characters
+         * @param numberOfFragments The (maximum) number of fragments
+         */
+        public InnerHit addHighlightedField(String name, int fragmentSize, int numberOfFragments) {
+            highlightBuilder().field(name, fragmentSize, numberOfFragments);
+            return this;
+        }
+
+        /**
+         * Adds a field to be highlighted with a provided fragment size (in characters),
+         * a provided (maximum) number of fragments and an offset for the highlight.
+         *
+         * @param name              The field to highlight
+         * @param fragmentSize      The size of a fragment in characters
+         * @param numberOfFragments The (maximum) number of fragments
+         */
+        public InnerHit addHighlightedField(String name, int fragmentSize, int numberOfFragments,
+                                            int fragmentOffset) {
+            highlightBuilder().field(name, fragmentSize, numberOfFragments, fragmentOffset);
+            return this;
+        }
+
+        /**
+         * Adds a highlighted field.
+         */
+        public InnerHit addHighlightedField(HighlightBuilder.Field field) {
+            highlightBuilder().field(field);
+            return this;
+        }
+
+        /**
+         * Set a tag scheme that encapsulates a built in pre and post tags. The allows schemes
+         * are <tt>styled</tt> and <tt>default</tt>.
+         *
+         * @param schemaName The tag scheme name
+         */
+        public InnerHit setHighlighterTagsSchema(String schemaName) {
+            highlightBuilder().tagsSchema(schemaName);
+            return this;
+        }
+
+        public InnerHit setHighlighterFragmentSize(Integer fragmentSize) {
+            highlightBuilder().fragmentSize(fragmentSize);
+            return this;
+        }
+
+        public InnerHit setHighlighterNumOfFragments(Integer numOfFragments) {
+            highlightBuilder().numOfFragments(numOfFragments);
+            return this;
+        }
+
+        public InnerHit setHighlighterFilter(Boolean highlightFilter) {
+            highlightBuilder().highlightFilter(highlightFilter);
+            return this;
+        }
+
+        /**
+         * The encoder to set for highlighting
+         */
+        public InnerHit setHighlighterEncoder(String encoder) {
+            highlightBuilder().encoder(encoder);
+            return this;
+        }
+
+        /**
+         * Explicitly set the pre tags that will be used for highlighting.
+         */
+        public InnerHit setHighlighterPreTags(String... preTags) {
+            highlightBuilder().preTags(preTags);
+            return this;
+        }
+
+        /**
+         * Explicitly set the post tags that will be used for highlighting.
+         */
+        public InnerHit setHighlighterPostTags(String... postTags) {
+            highlightBuilder().postTags(postTags);
+            return this;
+        }
+
+        /**
+         * The order of fragments per field. By default, ordered by the order in the
+         * highlighted text. Can be <tt>score</tt>, which then it will be ordered
+         * by score of the fragments.
+         */
+        public InnerHit setHighlighterOrder(String order) {
+            highlightBuilder().order(order);
+            return this;
+        }
+
+        public InnerHit setHighlighterRequireFieldMatch(boolean requireFieldMatch) {
+            highlightBuilder().requireFieldMatch(requireFieldMatch);
+            return this;
+        }
+
+        public InnerHit setHighlighterBoundaryMaxScan(Integer boundaryMaxScan) {
+            highlightBuilder().boundaryMaxScan(boundaryMaxScan);
+            return this;
+        }
+
+        public InnerHit setHighlighterBoundaryChars(char[] boundaryChars) {
+            highlightBuilder().boundaryChars(boundaryChars);
+            return this;
+        }
+
+        /**
+         * The highlighter type to use.
+         */
+        public InnerHit setHighlighterType(String type) {
+            highlightBuilder().highlighterType(type);
+            return this;
+        }
+
+        public InnerHit setHighlighterFragmenter(String fragmenter) {
+            highlightBuilder().fragmenter(fragmenter);
+            return this;
+        }
+
+        /**
+         * Sets a query to be used for highlighting all fields instead of the search query.
+         */
+        public InnerHit setHighlighterQuery(QueryBuilder highlightQuery) {
+            highlightBuilder().highlightQuery(highlightQuery);
+            return this;
+        }
+
+        /**
+         * Sets the size of the fragment to return from the beginning of the field if there are no matches to
+         * highlight and the field doesn't also define noMatchSize.
+         *
+         * @param noMatchSize integer to set or null to leave out of request.  default is null.
+         * @return this builder for chaining
+         */
+        public InnerHit setHighlighterNoMatchSize(Integer noMatchSize) {
+            highlightBuilder().noMatchSize(noMatchSize);
+            return this;
+        }
+
+        /**
+         * Sets the maximum number of phrases the fvh will consider if the field doesn't also define phraseLimit.
+         */
+        public InnerHit setHighlighterPhraseLimit(Integer phraseLimit) {
+            highlightBuilder().phraseLimit(phraseLimit);
+            return this;
+        }
+
+        public InnerHit setHighlighterOptions(Map<String, Object> options) {
+            highlightBuilder().options(options);
             return this;
         }
 
@@ -290,8 +460,24 @@ public class InnerHitsBuilder implements ToXContent {
             return this;
         }
 
-        public InnerHit innerHits(InnerHitsBuilder innerHitsBuilder) {
-            sourceBuilder().innerHits(innerHitsBuilder);
+
+
+
+        /**
+         * Adds a nested inner hit definition that collects inner hits for hits
+         * on this inner hit level.
+         */
+        public InnerHit addNestedInnerHits(String name, String path, InnerHit innerHit) {
+            sourceBuilder().innerHitsBuilder().addNestedInnerHits(name, path, innerHit);
+            return this;
+        }
+
+        /**
+         * Adds a nested inner hit definition that collects inner hits for hits
+         * on this inner hit level.
+         */
+        public InnerHit addParentChildInnerHits(String name, String type, InnerHit innerHit) {
+            sourceBuilder().innerHitsBuilder().addParentChildInnerHits(name, type, innerHit);
             return this;
         }
 
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/source/FetchSourceContext.java b/core/src/main/java/org/elasticsearch/search/fetch/source/FetchSourceContext.java
index ae0a71d..9db7aea 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/source/FetchSourceContext.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/source/FetchSourceContext.java
@@ -19,30 +19,20 @@
 
 package org.elasticsearch.search.fetch.source;
 
+import org.elasticsearch.Version;
 import org.elasticsearch.common.Booleans;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.rest.RestRequest;
 
 import java.io.IOException;
-import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.List;
 
 /**
  */
-public class FetchSourceContext implements Streamable, ToXContent {
-
-    public static final ParseField INCLUDES_FIELD = new ParseField("includes", "include");
-    public static final ParseField EXCLUDES_FIELD = new ParseField("excludes", "exclude");
+public class FetchSourceContext implements Streamable {
 
     public static final FetchSourceContext FETCH_SOURCE = new FetchSourceContext(true);
     public static final FetchSourceContext DO_NOT_FETCH_SOURCE = new FetchSourceContext(false);
@@ -51,11 +41,6 @@ public class FetchSourceContext implements Streamable, ToXContent {
     private String[] includes;
     private String[] excludes;
 
-    public static FetchSourceContext parse(XContentParser parser, QueryParseContext context) throws IOException {
-        FetchSourceContext fetchSourceContext = new FetchSourceContext();
-        fetchSourceContext.fromXContent(parser, context);
-        return fetchSourceContext;
-    }
 
     FetchSourceContext() {
 
@@ -187,86 +172,6 @@ public class FetchSourceContext implements Streamable, ToXContent {
         return null;
     }
 
-    public void fromXContent(XContentParser parser, QueryParseContext context) throws IOException {
-        XContentParser.Token token = parser.currentToken();
-        boolean fetchSource = true;
-        String[] includes = Strings.EMPTY_ARRAY;
-        String[] excludes = Strings.EMPTY_ARRAY;
-        if (token == XContentParser.Token.VALUE_BOOLEAN) {
-            fetchSource = parser.booleanValue();
-        } else if (token == XContentParser.Token.VALUE_STRING) {
-            includes = new String[]{parser.text()};
-        } else if (token == XContentParser.Token.START_ARRAY) {
-            ArrayList<String> list = new ArrayList<>();
-            while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                list.add(parser.text());
-            }
-            includes = list.toArray(new String[list.size()]);
-        } else if (token == XContentParser.Token.START_OBJECT) {
-            String currentFieldName = null;
-            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                if (token == XContentParser.Token.FIELD_NAME) {
-                    currentFieldName = parser.currentName();
-                } else if (token == XContentParser.Token.START_ARRAY) {
-                    if (context.parseFieldMatcher().match(currentFieldName, INCLUDES_FIELD)) {
-                        List<String> includesList = new ArrayList<>();
-                        while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                            if (token == XContentParser.Token.VALUE_STRING) {
-                                includesList.add(parser.text());
-                            } else {
-                                throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                                        parser.getTokenLocation());
-                            }
-                        }
-                        includes = includesList.toArray(new String[includesList.size()]);
-                    } else if (context.parseFieldMatcher().match(currentFieldName, EXCLUDES_FIELD)) {
-                        List<String> excludesList = new ArrayList<>();
-                        while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                            if (token == XContentParser.Token.VALUE_STRING) {
-                                excludesList.add(parser.text());
-                            } else {
-                                throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                                        parser.getTokenLocation());
-                            }
-                        }
-                        excludes = excludesList.toArray(new String[excludesList.size()]);
-                    } else {
-                        throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                                parser.getTokenLocation());
-                    }
-                } else if (token == XContentParser.Token.VALUE_STRING) {
-                    if (context.parseFieldMatcher().match(currentFieldName, INCLUDES_FIELD)) {
-                        includes = new String[] {parser.text()};
-                    } else if (context.parseFieldMatcher().match(currentFieldName, EXCLUDES_FIELD)) {
-                        excludes = new String[] {parser.text()};
-                    }
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                            parser.getTokenLocation());
-                }
-            }
-        } else {
-            throw new ParsingException(parser.getTokenLocation(), "Expected one of [" + XContentParser.Token.VALUE_BOOLEAN + ", "
-                    + XContentParser.Token.START_OBJECT + "] but found [" + token + "]", parser.getTokenLocation());
-        }
-        this.fetchSource = fetchSource;
-        this.includes = includes;
-        this.excludes = excludes;
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        if (fetchSource) {
-            builder.startObject();
-            builder.array(INCLUDES_FIELD.getPreferredName(), includes);
-            builder.array(EXCLUDES_FIELD.getPreferredName(), excludes);
-            builder.endObject();
-        } else {
-            builder.value(false);
-        }
-        return builder;
-    }
-
     @Override
     public void readFrom(StreamInput in) throws IOException {
         fetchSource = in.readBoolean();
diff --git a/core/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java b/core/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java
index 7f1e19b..695598e 100644
--- a/core/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java
@@ -227,9 +227,9 @@ public class HighlightBuilder implements ToXContent {
     }
 
     /**
-     * Set to true to cause a field to be highlighted only if a query matches that field.
-     * Default is false meaning that terms are highlighted on all requested fields regardless
-     * if the query matches specifically on them.
+     * Set to true to cause a field to be highlighted only if a query matches that field. 
+     * Default is false meaning that terms are highlighted on all requested fields regardless 
+     * if the query matches specifically on them. 
      */
     public HighlightBuilder requireFieldMatch(boolean requireFieldMatch) {
         this.requireFieldMatch = requireFieldMatch;
@@ -237,7 +237,7 @@ public class HighlightBuilder implements ToXContent {
     }
 
     /**
-     * When using the highlighterType <tt>fast-vector-highlighter</tt> this setting
+     * When using the highlighterType <tt>fast-vector-highlighter</tt> this setting 
      * controls how far to look for boundary characters, and defaults to 20.
      */
     public HighlightBuilder boundaryMaxScan(Integer boundaryMaxScan) {
@@ -246,8 +246,8 @@ public class HighlightBuilder implements ToXContent {
     }
 
     /**
-     * When using the highlighterType <tt>fast-vector-highlighter</tt> this setting
-     * defines what constitutes a boundary for highlighting. Its a single string with
+     * When using the highlighterType <tt>fast-vector-highlighter</tt> this setting 
+     * defines what constitutes a boundary for highlighting. Its a single string with 
      * each boundary character defined in it. It defaults to .,!? \t\n
      */
     public HighlightBuilder boundaryChars(char[] boundaryChars) {
@@ -258,7 +258,7 @@ public class HighlightBuilder implements ToXContent {
     /**
      * Set type of highlighter to use. Supported types
      * are <tt>highlighter</tt>, <tt>fast-vector-highlighter</tt> and <tt>postings-highlighter</tt>.
-     * The default option selected is dependent on the mappings defined for your index.
+     * The default option selected is dependent on the mappings defined for your index. 
      * Details of the different highlighter types are covered in the reference guide.
      */
     public HighlightBuilder highlighterType(String highlighterType) {
@@ -334,13 +334,6 @@ public class HighlightBuilder implements ToXContent {
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
         builder.startObject("highlight");
-        innerXContent(builder, params);
-        builder.endObject();
-        return builder;
-    }
-
-
-    public void innerXContent(XContentBuilder builder, Params params) throws IOException {
         if (tagsSchema != null) {
             builder.field("tags_schema", tagsSchema);
         }
@@ -472,6 +465,8 @@ public class HighlightBuilder implements ToXContent {
                 builder.endObject();
             }
         }
+        builder.endObject();
+        return builder;
     }
 
     public static class Field {
diff --git a/core/src/main/java/org/elasticsearch/search/internal/InternalSearchHits.java b/core/src/main/java/org/elasticsearch/search/internal/InternalSearchHits.java
index 393b7b6..9e787cf 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/InternalSearchHits.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/InternalSearchHits.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.search.internal;
 
 import com.carrotsearch.hppc.IntObjectHashMap;
-import com.google.common.collect.Iterators;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
@@ -30,6 +29,7 @@ import org.elasticsearch.search.SearchHits;
 import org.elasticsearch.search.SearchShardTarget;
 
 import java.io.IOException;
+import java.util.Arrays;
 import java.util.IdentityHashMap;
 import java.util.Iterator;
 import java.util.Map;
@@ -156,7 +156,7 @@ public class InternalSearchHits implements SearchHits {
 
     @Override
     public Iterator<SearchHit> iterator() {
-        return Iterators.forArray(hits());
+        return Arrays.stream(hits()).iterator();
     }
 
     public InternalSearchHit[] internalHits() {
diff --git a/core/src/main/java/org/elasticsearch/search/internal/ShardSearchLocalRequest.java b/core/src/main/java/org/elasticsearch/search/internal/ShardSearchLocalRequest.java
index 8f0cd98..ca8c074 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/ShardSearchLocalRequest.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/ShardSearchLocalRequest.java
@@ -31,7 +31,6 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.script.Template;
 import org.elasticsearch.search.Scroll;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 
 import java.io.IOException;
 
@@ -66,7 +65,9 @@ public class ShardSearchLocalRequest extends ContextAndHeaderHolder implements S
     private Scroll scroll;
     private String[] types = Strings.EMPTY_ARRAY;
     private String[] filteringAliases;
-    private SearchSourceBuilder source;
+    private BytesReference source;
+    private BytesReference extraSource;
+    private BytesReference templateSource;
     private Template template;
     private Boolean requestCache;
     private long nowInMillis;
@@ -78,6 +79,8 @@ public class ShardSearchLocalRequest extends ContextAndHeaderHolder implements S
                             String[] filteringAliases, long nowInMillis) {
         this(shardRouting.shardId(), numberOfShards, searchRequest.searchType(),
                 searchRequest.source(), searchRequest.types(), searchRequest.requestCache());
+        this.extraSource = searchRequest.extraSource();
+        this.templateSource = searchRequest.templateSource();
         this.template = searchRequest.template();
         this.scroll = searchRequest.scroll();
         this.filteringAliases = filteringAliases;
@@ -95,8 +98,8 @@ public class ShardSearchLocalRequest extends ContextAndHeaderHolder implements S
         this.filteringAliases = filteringAliases;
     }
 
-    public ShardSearchLocalRequest(ShardId shardId, int numberOfShards, SearchType searchType, SearchSourceBuilder source, String[] types,
-            Boolean requestCache) {
+    public ShardSearchLocalRequest(ShardId shardId, int numberOfShards, SearchType searchType,
+                                   BytesReference source, String[] types, Boolean requestCache) {
         this.index = shardId.getIndex();
         this.shardId = shardId.id();
         this.numberOfShards = numberOfShards;
@@ -122,16 +125,21 @@ public class ShardSearchLocalRequest extends ContextAndHeaderHolder implements S
     }
 
     @Override
-    public SearchSourceBuilder source() {
+    public BytesReference source() {
         return source;
     }
 
     @Override
-    public void source(SearchSourceBuilder source) {
+    public void source(BytesReference source) {
         this.source = source;
     }
 
     @Override
+    public BytesReference extraSource() {
+        return extraSource;
+    }
+
+    @Override
     public int numberOfShards() {
         return numberOfShards;
     }
@@ -150,12 +158,18 @@ public class ShardSearchLocalRequest extends ContextAndHeaderHolder implements S
     public long nowInMillis() {
         return nowInMillis;
     }
+
     @Override
     public Template template() {
         return template;
     }
 
     @Override
+    public BytesReference templateSource() {
+        return templateSource;
+    }
+
+    @Override
     public Boolean requestCache() {
         return requestCache;
     }
@@ -174,13 +188,18 @@ public class ShardSearchLocalRequest extends ContextAndHeaderHolder implements S
         if (in.readBoolean()) {
             scroll = readScroll(in);
         }
-        if (in.readBoolean()) {
-            source = SearchSourceBuilder.readSearchSourceFrom(in);
-        }
+
+        source = in.readBytesReference();
+        extraSource = in.readBytesReference();
+
         types = in.readStringArray();
         filteringAliases = in.readStringArray();
         nowInMillis = in.readVLong();
-        template = in.readOptionalStreamable(new Template());
+
+        templateSource = in.readBytesReference();
+        if (in.readBoolean()) {
+            template = Template.readTemplate(in);
+        }
         requestCache = in.readOptionalBoolean();
     }
 
@@ -197,20 +216,20 @@ public class ShardSearchLocalRequest extends ContextAndHeaderHolder implements S
             out.writeBoolean(true);
             scroll.writeTo(out);
         }
-        if (source == null) {
-            out.writeBoolean(false);
-        } else {
-            out.writeBoolean(true);
-            source.writeTo(out);
-
-        }
+        out.writeBytesReference(source);
+        out.writeBytesReference(extraSource);
         out.writeStringArray(types);
         out.writeStringArrayNullable(filteringAliases);
         if (!asKey) {
             out.writeVLong(nowInMillis);
         }
 
-        out.writeOptionalStreamable(template);
+        out.writeBytesReference(templateSource);
+        boolean hasTemplate = template != null;
+        out.writeBoolean(hasTemplate);
+        if (hasTemplate) {
+            template.writeTo(out);
+        }
         out.writeOptionalBoolean(requestCache);
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/internal/ShardSearchRequest.java b/core/src/main/java/org/elasticsearch/search/internal/ShardSearchRequest.java
index fb631b0..6d9734f 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/ShardSearchRequest.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/ShardSearchRequest.java
@@ -20,11 +20,12 @@
 package org.elasticsearch.search.internal;
 
 import org.elasticsearch.action.search.SearchType;
+import org.elasticsearch.common.HasContext;
 import org.elasticsearch.common.HasContextAndHeaders;
+import org.elasticsearch.common.HasHeaders;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.script.Template;
 import org.elasticsearch.search.Scroll;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 
 import java.io.IOException;
 
@@ -41,9 +42,11 @@ public interface ShardSearchRequest extends HasContextAndHeaders {
 
     String[] types();
 
-    SearchSourceBuilder source();
+    BytesReference source();
 
-    void source(SearchSourceBuilder source);
+    void source(BytesReference source);
+
+    BytesReference extraSource();
 
     int numberOfShards();
 
@@ -55,6 +58,8 @@ public interface ShardSearchRequest extends HasContextAndHeaders {
 
     Template template();
 
+    BytesReference templateSource();
+
     Boolean requestCache();
 
     Scroll scroll();
diff --git a/core/src/main/java/org/elasticsearch/search/internal/ShardSearchTransportRequest.java b/core/src/main/java/org/elasticsearch/search/internal/ShardSearchTransportRequest.java
index 279d9d6..e7b1e2f 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/ShardSearchTransportRequest.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/ShardSearchTransportRequest.java
@@ -30,7 +30,6 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.script.Template;
 import org.elasticsearch.search.Scroll;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.transport.TransportRequest;
 
 import java.io.IOException;
@@ -88,16 +87,21 @@ public class ShardSearchTransportRequest extends TransportRequest implements Sha
     }
 
     @Override
-    public SearchSourceBuilder source() {
+    public BytesReference source() {
         return shardSearchLocalRequest.source();
     }
 
     @Override
-    public void source(SearchSourceBuilder source) {
+    public void source(BytesReference source) {
         shardSearchLocalRequest.source(source);
     }
 
     @Override
+    public BytesReference extraSource() {
+        return shardSearchLocalRequest.extraSource();
+    }
+
+    @Override
     public int numberOfShards() {
         return shardSearchLocalRequest.numberOfShards();
     }
@@ -116,12 +120,18 @@ public class ShardSearchTransportRequest extends TransportRequest implements Sha
     public long nowInMillis() {
         return shardSearchLocalRequest.nowInMillis();
     }
+
     @Override
     public Template template() {
         return shardSearchLocalRequest.template();
     }
 
     @Override
+    public BytesReference templateSource() {
+        return shardSearchLocalRequest.templateSource();
+    }
+
+    @Override
     public Boolean requestCache() {
         return shardSearchLocalRequest.requestCache();
     }
diff --git a/core/src/main/java/org/elasticsearch/search/warmer/IndexWarmersMetaData.java b/core/src/main/java/org/elasticsearch/search/warmer/IndexWarmersMetaData.java
index 1ce27f9..6e881cb 100644
--- a/core/src/main/java/org/elasticsearch/search/warmer/IndexWarmersMetaData.java
+++ b/core/src/main/java/org/elasticsearch/search/warmer/IndexWarmersMetaData.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.search.warmer;
 
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.action.support.ToXContentToBytes;
 import org.elasticsearch.cluster.AbstractDiffable;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.Nullable;
@@ -29,17 +27,12 @@ import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentGenerator;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.XContentType;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 
-import java.io.ByteArrayOutputStream;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Arrays;
@@ -74,10 +67,10 @@ public class IndexWarmersMetaData extends AbstractDiffable<IndexMetaData.Custom>
     public static class Entry {
         private final String name;
         private final String[] types;
-        private final SearchSource source;
+        private final BytesReference source;
         private final Boolean requestCache;
 
-        public Entry(String name, String[] types, Boolean requestCache, SearchSource source) {
+        public Entry(String name, String[] types, Boolean requestCache, BytesReference source) {
             this.name = name;
             this.types = types == null ? Strings.EMPTY_ARRAY : types;
             this.source = source;
@@ -93,7 +86,7 @@ public class IndexWarmersMetaData extends AbstractDiffable<IndexMetaData.Custom>
         }
 
         @Nullable
-        public SearchSource source() {
+        public BytesReference source() {
             return this.source;
         }
 
@@ -148,9 +141,9 @@ public class IndexWarmersMetaData extends AbstractDiffable<IndexMetaData.Custom>
         for (int i = 0; i < entries.length; i++) {
             String name = in.readString();
             String[] types = in.readStringArray();
-            SearchSource source = null;
+            BytesReference source = null;
             if (in.readBoolean()) {
-                source = new SearchSource(in);
+                source = in.readBytesReference();
             }
             Boolean queryCache;
             queryCache = in.readOptionalBoolean();
@@ -169,7 +162,7 @@ public class IndexWarmersMetaData extends AbstractDiffable<IndexMetaData.Custom>
                 out.writeBoolean(false);
             } else {
                 out.writeBoolean(true);
-                entry.source.writeTo(out);
+                out.writeBytesReference(entry.source());
             }
             out.writeOptionalBoolean(entry.requestCache());
         }
@@ -201,7 +194,7 @@ public class IndexWarmersMetaData extends AbstractDiffable<IndexMetaData.Custom>
             } else if (token == XContentParser.Token.START_OBJECT) {
                 String name = currentFieldName;
                 List<String> types = new ArrayList<>(2);
-                SearchSource source = null;
+                BytesReference source = null;
                 Boolean queryCache = null;
                 while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                     if (token == XContentParser.Token.FIELD_NAME) {
@@ -214,15 +207,12 @@ public class IndexWarmersMetaData extends AbstractDiffable<IndexMetaData.Custom>
                         }
                     } else if (token == XContentParser.Token.START_OBJECT) {
                         if ("source".equals(currentFieldName)) {
-                            ByteArrayOutputStream out = new ByteArrayOutputStream();
-                            try (XContentGenerator generator = XContentType.JSON.xContent().createGenerator(out)) {
-                                generator.copyCurrentStructure(parser);
-                            }
-                            source = new SearchSource(new BytesArray(out.toByteArray()));
+                            XContentBuilder builder = XContentFactory.jsonBuilder().map(parser.mapOrdered());
+                            source = builder.bytes();
                         }
                     } else if (token == XContentParser.Token.VALUE_EMBEDDED_OBJECT) {
                         if ("source".equals(currentFieldName)) {
-                            source = new SearchSource(new BytesArray(parser.binaryValue()));
+                            source = new BytesArray(parser.binaryValue());
                         }
                     } else if (token.isValue()) {
                         if ("requestCache".equals(currentFieldName) || "request_cache".equals(currentFieldName)) {
@@ -249,12 +239,22 @@ public class IndexWarmersMetaData extends AbstractDiffable<IndexMetaData.Custom>
     }
 
     public static void toXContent(Entry entry, XContentBuilder builder, ToXContent.Params params) throws IOException {
+        boolean binary = params.paramAsBoolean("binary", false);
         builder.startObject(entry.name(), XContentBuilder.FieldCaseConversion.NONE);
         builder.field("types", entry.types());
         if (entry.requestCache() != null) {
             builder.field("requestCache", entry.requestCache());
         }
-        builder.field("source", entry.source());
+        builder.field("source");
+        if (binary) {
+            builder.value(entry.source());
+        } else {
+            Map<String, Object> mapping;
+            try (XContentParser parser = XContentFactory.xContent(entry.source()).createParser(entry.source())) {
+                mapping = parser.mapOrdered();
+            }
+            builder.map(mapping);
+        }
         builder.endObject();
     }
 
@@ -277,78 +277,4 @@ public class IndexWarmersMetaData extends AbstractDiffable<IndexMetaData.Custom>
         }
         return new IndexWarmersMetaData(entries.toArray(new Entry[entries.size()]));
     }
-
-    public static class SearchSource extends ToXContentToBytes implements Writeable<SearchSource> {
-        private final BytesReference binary;
-        private SearchSourceBuilder cached;
-
-        public SearchSource(BytesReference bytesArray) {
-            if (bytesArray == null) {
-                throw new IllegalArgumentException("bytesArray must not be null");
-            }
-            this.binary = bytesArray;
-        }
-
-        public SearchSource(StreamInput input) throws IOException {
-            this(input.readBytesReference());
-        }
-
-        public SearchSource(SearchSourceBuilder source) {
-            try (XContentBuilder builder = XContentBuilder.builder(XContentType.JSON.xContent())) {
-                source.toXContent(builder, ToXContent.EMPTY_PARAMS);
-                binary = builder.bytes();
-            } catch (IOException ex) {
-                throw new ElasticsearchException("failed to generate XContent", ex);
-            }
-        }
-
-        public SearchSourceBuilder build(QueryParseContext ctx) throws IOException {
-            if (cached == null) {
-                try (XContentParser parser = XContentFactory.xContent(binary).createParser(binary)) {
-                    ctx.reset(parser);
-                    cached = SearchSourceBuilder.parseSearchSource(parser, ctx);
-                }
-            }
-            return cached;
-        }
-
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            if (binary == null) {
-                cached.toXContent(builder, params);
-            } else {
-                try (XContentParser parser = XContentFactory.xContent(binary).createParser(binary)) {
-                    builder.copyCurrentStructure(parser);
-                }
-            }
-            return builder;
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeBytesReference(binary);
-        }
-
-        @Override
-        public SearchSource readFrom(StreamInput in) throws IOException {
-            return new SearchSource(in);
-        }
-
-        @Override
-        public boolean equals(Object o) {
-            if (this == o) return true;
-            if (o == null || getClass() != o.getClass()) return false;
-
-            SearchSource that = (SearchSource) o;
-
-            return binary.equals(that.binary);
-
-        }
-
-        @Override
-        public int hashCode() {
-            return binary.hashCode();
-        }
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/transport/local/LocalTransport.java b/core/src/main/java/org/elasticsearch/transport/local/LocalTransport.java
index 2dca60c..8d2eb15 100644
--- a/core/src/main/java/org/elasticsearch/transport/local/LocalTransport.java
+++ b/core/src/main/java/org/elasticsearch/transport/local/LocalTransport.java
@@ -37,27 +37,11 @@ import org.elasticsearch.common.transport.TransportAddress;
 import org.elasticsearch.common.util.concurrent.AbstractRunnable;
 import org.elasticsearch.common.util.concurrent.EsExecutors;
 import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.ActionNotFoundTransportException;
-import org.elasticsearch.transport.ConnectTransportException;
-import org.elasticsearch.transport.NodeNotConnectedException;
-import org.elasticsearch.transport.RemoteTransportException;
-import org.elasticsearch.transport.RequestHandlerRegistry;
-import org.elasticsearch.transport.ResponseHandlerFailureTransportException;
-import org.elasticsearch.transport.Transport;
-import org.elasticsearch.transport.TransportException;
-import org.elasticsearch.transport.TransportRequest;
-import org.elasticsearch.transport.TransportRequestOptions;
-import org.elasticsearch.transport.TransportResponse;
-import org.elasticsearch.transport.TransportResponseHandler;
-import org.elasticsearch.transport.TransportSerializationException;
-import org.elasticsearch.transport.TransportServiceAdapter;
-import org.elasticsearch.transport.Transports;
+import org.elasticsearch.transport.*;
 import org.elasticsearch.transport.support.TransportStatus;
 
 import java.io.IOException;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map;
+import java.util.*;
 import java.util.concurrent.ConcurrentMap;
 import java.util.concurrent.ThreadFactory;
 import java.util.concurrent.ThreadPoolExecutor;
@@ -81,7 +65,7 @@ public class LocalTransport extends AbstractLifecycleComponent<Transport> implem
     private final static ConcurrentMap<LocalTransportAddress, LocalTransport> transports = newConcurrentMap();
     private static final AtomicLong transportAddressIdGenerator = new AtomicLong();
     private final ConcurrentMap<DiscoveryNode, LocalTransport> connectedNodes = newConcurrentMap();
-    protected final NamedWriteableRegistry namedWriteableRegistry;
+    private final NamedWriteableRegistry namedWriteableRegistry;
 
     public static final String TRANSPORT_LOCAL_ADDRESS = "transport.local.address";
     public static final String TRANSPORT_LOCAL_WORKERS = "transport.local.workers";
diff --git a/core/src/main/resources/org/elasticsearch/bootstrap/security.policy b/core/src/main/resources/org/elasticsearch/bootstrap/security.policy
index 1126824..76b5a58 100644
--- a/core/src/main/resources/org/elasticsearch/bootstrap/security.policy
+++ b/core/src/main/resources/org/elasticsearch/bootstrap/security.policy
@@ -52,8 +52,8 @@ grant codeBase "${es.security.plugin.discovery-ec2}" {
   permission java.lang.reflect.ReflectPermission "suppressAccessChecks";
 };
 
-grant codeBase "${es.security.plugin.cloud-gce}" {
-  // needed because of problems in cloud-gce
+grant codeBase "${es.security.plugin.discovery-gce}" {
+  // needed because of problems in discovery-gce
   permission java.lang.reflect.ReflectPermission "suppressAccessChecks";
 };
 
diff --git a/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help b/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
index 0811ec7..3577215 100644
--- a/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
+++ b/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
@@ -38,10 +38,10 @@ OFFICIAL PLUGINS
     - analysis-phonetic
     - analysis-smartcn
     - analysis-stempel
-    - cloud-gce
     - delete-by-query
     - discovery-azure
     - discovery-ec2
+    - discovery-gce
     - discovery-multicast
     - lang-expression
     - lang-groovy
diff --git a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
index 9a260f0..9297c6b 100644
--- a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
+++ b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
@@ -20,7 +20,6 @@ package org.elasticsearch;
 
 import com.fasterxml.jackson.core.JsonLocation;
 import com.fasterxml.jackson.core.JsonParseException;
-
 import org.apache.lucene.util.Constants;
 import org.elasticsearch.action.FailedNodeException;
 import org.elasticsearch.action.RoutingMissingException;
@@ -31,12 +30,7 @@ import org.elasticsearch.client.AbstractClientHeadersTestCase;
 import org.elasticsearch.cluster.block.ClusterBlockException;
 import org.elasticsearch.cluster.metadata.SnapshotId;
 import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.cluster.routing.IllegalShardRoutingStateException;
-import org.elasticsearch.cluster.routing.RoutingTableValidation;
-import org.elasticsearch.cluster.routing.RoutingValidationException;
-import org.elasticsearch.cluster.routing.ShardRouting;
-import org.elasticsearch.cluster.routing.ShardRoutingState;
-import org.elasticsearch.cluster.routing.TestShardRouting;
+import org.elasticsearch.cluster.routing.*;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.breaker.CircuitBreakingException;
 import org.elasticsearch.common.io.PathUtils;
@@ -55,7 +49,6 @@ import org.elasticsearch.common.xcontent.XContentLocation;
 import org.elasticsearch.discovery.DiscoverySettings;
 import org.elasticsearch.index.AlreadyExpiredException;
 import org.elasticsearch.index.Index;
-import org.elasticsearch.index.engine.CreateFailedEngineException;
 import org.elasticsearch.index.engine.IndexFailedEngineException;
 import org.elasticsearch.index.engine.RecoveryEngineException;
 import org.elasticsearch.index.mapper.MergeMappingException;
@@ -139,9 +132,9 @@ public class ExceptionSerializationTests extends ESTestCase {
                         Class<?> clazz = loadClass(filename);
                         if (ignore.contains(clazz) == false) {
                             if (Modifier.isAbstract(clazz.getModifiers()) == false && Modifier.isInterface(clazz.getModifiers()) == false && isEsException(clazz)) {
-                                if (ElasticsearchException.isRegistered((Class<? extends Throwable>)clazz) == false && ElasticsearchException.class.equals(clazz.getEnclosingClass()) == false) {
+                                if (ElasticsearchException.isRegistered((Class<? extends Throwable>) clazz) == false && ElasticsearchException.class.equals(clazz.getEnclosingClass()) == false) {
                                     notRegistered.add(clazz);
-                                } else if (ElasticsearchException.isRegistered((Class<? extends Throwable>)clazz)) {
+                                } else if (ElasticsearchException.isRegistered((Class<? extends Throwable>) clazz)) {
                                     registered.add(clazz);
                                     try {
                                         if (clazz.getDeclaredMethod("writeTo", StreamOutput.class) != null) {
@@ -199,7 +192,7 @@ public class ExceptionSerializationTests extends ESTestCase {
     }
 
     public static final class TestException extends ElasticsearchException {
-        public TestException(StreamInput in) throws IOException{
+        public TestException(StreamInput in) throws IOException {
             super(in);
         }
     }
@@ -247,7 +240,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         assertEquals(ex.getIndex(), "foo");
         assertEquals(ex.getMessage(), "fobar");
 
-        ex = serialize(new QueryShardException((Index)null, null, null));
+        ex = serialize(new QueryShardException((Index) null, null, null));
         assertNull(ex.getIndex());
         assertNull(ex.getMessage());
     }
@@ -282,22 +275,8 @@ public class ExceptionSerializationTests extends ESTestCase {
         assertEquals(-3, alreadyExpiredException.now());
     }
 
-    public void testCreateFailedEngineException() throws IOException {
-        CreateFailedEngineException ex = serialize(new CreateFailedEngineException(new ShardId("idx", 2), "type", "id", null));
-        assertEquals(ex.getShardId(), new ShardId("idx", 2));
-        assertEquals("type", ex.type());
-        assertEquals("id", ex.id());
-        assertNull(ex.getCause());
-
-        ex = serialize(new CreateFailedEngineException(null, "type", "id", new NullPointerException()));
-        assertNull(ex.getShardId());
-        assertEquals("type", ex.type());
-        assertEquals("id", ex.id());
-        assertTrue(ex.getCause() instanceof NullPointerException);
-    }
-
     public void testMergeMappingException() throws IOException {
-        MergeMappingException ex = serialize(new MergeMappingException(new String[] {"one", "two"}));
+        MergeMappingException ex = serialize(new MergeMappingException(new String[]{"one", "two"}));
         assertArrayEquals(ex.failures(), new String[]{"one", "two"});
     }
 
@@ -342,7 +321,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         assertEquals("the dude abides!", ex.name());
         assertEquals("index_template [the dude abides!] already exists", ex.getMessage());
 
-        ex = serialize(new IndexTemplateAlreadyExistsException((String)null));
+        ex = serialize(new IndexTemplateAlreadyExistsException((String) null));
         assertNull(ex.name());
         assertEquals("index_template [null] already exists", ex.getMessage());
     }
@@ -449,7 +428,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         assertEquals(ctx.shardTarget(), ex.shard());
     }
 
-    public void testIllegalIndexShardStateException()throws IOException {
+    public void testIllegalIndexShardStateException() throws IOException {
         ShardId id = new ShardId("foo", 1);
         IndexShardState state = randomFrom(IndexShardState.values());
         IllegalIndexShardStateException ex = serialize(new IllegalIndexShardStateException(id, state, "come back later buddy"));
@@ -480,7 +459,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         assertEquals("baam", ex.getMessage());
         assertTrue(ex.getCause() instanceof NullPointerException);
         assertEquals(empty.length, ex.shardFailures().length);
-        ShardSearchFailure[] one = new ShardSearchFailure[] {
+        ShardSearchFailure[] one = new ShardSearchFailure[]{
                 new ShardSearchFailure(new IllegalArgumentException("nono!"))
         };
 
@@ -521,7 +500,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         assertEquals("index_template [name] missing", ex.getMessage());
         assertEquals("name", ex.name());
 
-        ex = serialize(new IndexTemplateMissingException((String)null));
+        ex = serialize(new IndexTemplateMissingException((String) null));
         assertEquals("index_template [null] missing", ex.getMessage());
         assertNull(ex.name());
     }
@@ -570,8 +549,8 @@ public class ExceptionSerializationTests extends ESTestCase {
         ex = serialize(new NotSerializableExceptionWrapper(new IllegalArgumentException("nono!")));
         assertEquals("{\"type\":\"illegal_argument_exception\",\"reason\":\"nono!\"}", toXContent(ex));
 
-        Throwable[] unknowns = new Throwable[] {
-                new JsonParseException("foobar", new JsonLocation(new Object(), 1,2,3,4)),
+        Throwable[] unknowns = new Throwable[]{
+                new JsonParseException("foobar", new JsonLocation(new Object(), 1, 2, 3, 4)),
                 new ClassCastException("boom boom boom"),
                 new IOException("booom")
         };
@@ -609,7 +588,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         UnknownHeaderException uhe = new UnknownHeaderException("msg", status);
         uhe.addHeader("foo", "foo", "bar");
 
-        ElasticsearchException serialize = serialize((ElasticsearchException)uhe);
+        ElasticsearchException serialize = serialize((ElasticsearchException) uhe);
         assertTrue(serialize instanceof NotSerializableExceptionWrapper);
         NotSerializableExceptionWrapper e = (NotSerializableExceptionWrapper) serialize;
         assertEquals("msg", e.getMessage());
@@ -684,7 +663,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         ids.put(19, org.elasticsearch.ResourceNotFoundException.class);
         ids.put(20, org.elasticsearch.transport.ActionTransportException.class);
         ids.put(21, org.elasticsearch.ElasticsearchGenerationException.class);
-        ids.put(22, org.elasticsearch.index.engine.CreateFailedEngineException.class);
+        ids.put(22, null); // was CreateFailedEngineException
         ids.put(23, org.elasticsearch.index.shard.IndexShardStartedException.class);
         ids.put(24, org.elasticsearch.search.SearchContextMissingException.class);
         ids.put(25, org.elasticsearch.script.ScriptException.class);
@@ -716,7 +695,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         ids.put(51, org.elasticsearch.index.IndexShardAlreadyExistsException.class);
         ids.put(52, org.elasticsearch.index.engine.VersionConflictEngineException.class);
         ids.put(53, org.elasticsearch.index.engine.EngineException.class);
-        ids.put(54, org.elasticsearch.index.engine.DocumentAlreadyExistsException.class);
+        ids.put(54, null); // was DocumentAlreadyExistsException, which is superseded with VersionConflictEngineException
         ids.put(55, org.elasticsearch.action.NoSuchNodeException.class);
         ids.put(56, org.elasticsearch.common.settings.SettingsException.class);
         ids.put(57, org.elasticsearch.indices.IndexTemplateMissingException.class);
@@ -813,7 +792,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         }
 
         for (ElasticsearchException.ElasticsearchExceptionHandle handle : ElasticsearchException.ElasticsearchExceptionHandle.values()) {
-            assertEquals((int)reverse.get(handle.exceptionClass), handle.id);
+            assertEquals((int) reverse.get(handle.exceptionClass), handle.id);
         }
 
         for (Map.Entry<Integer, Class<? extends ElasticsearchException>> entry : ids.entrySet()) {
diff --git a/core/src/test/java/org/elasticsearch/action/count/CountRequestBuilderTests.java b/core/src/test/java/org/elasticsearch/action/count/CountRequestBuilderTests.java
index f3058c0..5d77247 100644
--- a/core/src/test/java/org/elasticsearch/action/count/CountRequestBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/action/count/CountRequestBuilderTests.java
@@ -19,16 +19,23 @@
 
 package org.elasticsearch.action.count;
 
+import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.transport.TransportClient;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.query.MatchAllQueryBuilder;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentHelper;
+import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.test.ESTestCase;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.Test;
 
+import java.io.IOException;
+
 import static org.hamcrest.CoreMatchers.containsString;
 import static org.hamcrest.CoreMatchers.equalTo;
 
@@ -55,29 +62,71 @@ public class CountRequestBuilderTests extends ESTestCase {
     @Test
     public void testEmptySourceToString() {
         CountRequestBuilder countRequestBuilder = client.prepareCount();
-        assertThat(countRequestBuilder.toString(), equalTo(new CountRequest().toString()));
+        assertThat(countRequestBuilder.toString(), equalTo(new QuerySourceBuilder().toString()));
     }
 
     @Test
     public void testQueryBuilderQueryToString() {
         CountRequestBuilder countRequestBuilder = client.prepareCount();
         countRequestBuilder.setQuery(QueryBuilders.matchAllQuery());
-        assertThat(countRequestBuilder.toString(), equalTo(new CountRequest().query(QueryBuilders.matchAllQuery()).toString()));
+        assertThat(countRequestBuilder.toString(), equalTo(new QuerySourceBuilder().setQuery(QueryBuilders.matchAllQuery()).toString()));
     }
 
     @Test
     public void testStringQueryToString() {
         CountRequestBuilder countRequestBuilder = client.prepareCount();
-        countRequestBuilder.setQuery(new MatchAllQueryBuilder());
-        assertThat(countRequestBuilder.toString(), containsString("match_all"));
+        String query = "{ \"match_all\" : {} }";
+        countRequestBuilder.setQuery(new BytesArray(query));
+        assertThat(countRequestBuilder.toString(), containsString("\"query\":{ \"match_all\" : {} }"));
+    }
+
+    @Test
+    public void testXContentBuilderQueryToString() throws IOException {
+        CountRequestBuilder countRequestBuilder = client.prepareCount();
+        XContentBuilder xContentBuilder = XContentFactory.contentBuilder(randomFrom(XContentType.values()));
+        xContentBuilder.startObject();
+        xContentBuilder.startObject("match_all");
+        xContentBuilder.endObject();
+        xContentBuilder.endObject();
+        countRequestBuilder.setQuery(xContentBuilder);
+        assertThat(countRequestBuilder.toString(), equalTo(new QuerySourceBuilder().setQuery(xContentBuilder.bytes()).toString()));
+    }
+
+    @Test
+    public void testStringSourceToString() {
+        CountRequestBuilder countRequestBuilder = client.prepareCount();
+        String query = "{ \"query\": { \"match_all\" : {} } }";
+        countRequestBuilder.setSource(new BytesArray(query));
+        assertThat(countRequestBuilder.toString(), equalTo("{ \"query\": { \"match_all\" : {} } }"));
+    }
+
+    @Test
+    public void testXContentBuilderSourceToString() throws IOException {
+        CountRequestBuilder countRequestBuilder = client.prepareCount();
+        XContentBuilder xContentBuilder = XContentFactory.contentBuilder(randomFrom(XContentType.values()));
+        xContentBuilder.startObject();
+        xContentBuilder.startObject("match_all");
+        xContentBuilder.endObject();
+        xContentBuilder.endObject();
+        countRequestBuilder.setSource(xContentBuilder.bytes());
+        assertThat(countRequestBuilder.toString(), equalTo(XContentHelper.convertToJson(xContentBuilder.bytes(), false, true)));
     }
 
     @Test
     public void testThatToStringDoesntWipeSource() {
-        CountRequestBuilder countRequestBuilder = client.prepareCount().setQuery(QueryBuilders.termQuery("field", "value"));
-        String preToString = countRequestBuilder.request().toString();
-        assertThat(countRequestBuilder.toString(), equalTo(new CountRequest().query(QueryBuilders.termQuery("field", "value")).toString()));
-        String postToString = countRequestBuilder.request().toString();
+        String source = "{\n" +
+                "            \"query\" : {\n" +
+                "            \"match\" : {\n" +
+                "                \"field\" : {\n" +
+                "                    \"query\" : \"value\"" +
+                "                }\n" +
+                "            }\n" +
+                "        }\n" +
+                "        }";
+        CountRequestBuilder countRequestBuilder = client.prepareCount().setSource(new BytesArray(source));
+        String preToString = countRequestBuilder.request().source().toUtf8();
+        assertThat(countRequestBuilder.toString(), equalTo(source));
+        String postToString = countRequestBuilder.request().source().toUtf8();
         assertThat(preToString, equalTo(postToString));
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/action/count/CountRequestTests.java b/core/src/test/java/org/elasticsearch/action/count/CountRequestTests.java
index ca7d0c8..407cfba 100644
--- a/core/src/test/java/org/elasticsearch/action/count/CountRequestTests.java
+++ b/core/src/test/java/org/elasticsearch/action/count/CountRequestTests.java
@@ -21,13 +21,18 @@ package org.elasticsearch.action.count;
 
 import org.elasticsearch.action.search.SearchRequest;
 import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.action.support.QuerySourceBuilder;
+import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
+import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.test.ESTestCase;
 import org.junit.Test;
 
+import java.util.Map;
+
 import static org.hamcrest.CoreMatchers.equalTo;
 import static org.hamcrest.CoreMatchers.notNullValue;
+import static org.hamcrest.CoreMatchers.nullValue;
 
 public class CountRequestTests extends ESTestCase {
 
@@ -51,9 +56,8 @@ public class CountRequestTests extends ESTestCase {
         if (randomBoolean()) {
             countRequest.preference(randomAsciiOfLengthBetween(1, 10));
         }
-        final boolean querySet = randomBoolean();
-        if (querySet) {
-            countRequest.query(QueryBuilders.termQuery("field", "value"));
+        if (randomBoolean()) {
+            countRequest.source(new QuerySourceBuilder().setQuery(QueryBuilders.termQuery("field", "value")));
         }
         if (randomBoolean()) {
             countRequest.minScore(randomFloat());
@@ -68,15 +72,31 @@ public class CountRequestTests extends ESTestCase {
         assertThat(searchRequest.types(), equalTo(countRequest.types()));
         assertThat(searchRequest.routing(), equalTo(countRequest.routing()));
         assertThat(searchRequest.preference(), equalTo(countRequest.preference()));
-        SearchSourceBuilder source = searchRequest.source();
-        assertThat(source.size(), equalTo(0));
-        if (querySet) {
-            assertThat(source.query(), notNullValue());
+
+        if (countRequest.source() == null) {
+            assertThat(searchRequest.source(), nullValue());
+        } else {
+            Map<String, Object> sourceMap = XContentHelper.convertToMap(searchRequest.source(), false).v2();
+            assertThat(sourceMap.size(), equalTo(1));
+            assertThat(sourceMap.get("query"), notNullValue());
+        }
+
+        Map<String, Object> extraSourceMap = XContentHelper.convertToMap(searchRequest.extraSource(), false).v2();
+        int count = 1;
+        assertThat((Integer)extraSourceMap.get("size"), equalTo(0));
+        if (countRequest.minScore() == CountRequest.DEFAULT_MIN_SCORE) {
+            assertThat(extraSourceMap.get("min_score"), nullValue());
+        } else {
+            assertThat(((Number)extraSourceMap.get("min_score")).floatValue(), equalTo(countRequest.minScore()));
+            count++;
+        }
+        if (countRequest.terminateAfter() == SearchContext.DEFAULT_TERMINATE_AFTER) {
+            assertThat(extraSourceMap.get("terminate_after"), nullValue());
         } else {
-            assertNull(source.query());
+            assertThat((Integer)extraSourceMap.get("terminate_after"), equalTo(countRequest.terminateAfter()));
+            count++;
         }
-        assertThat(source.minScore(), equalTo(countRequest.minScore()));
-        assertThat(source.terminateAfter(), equalTo(countRequest.terminateAfter()));
+        assertThat(extraSourceMap.size(), equalTo(count));
     }
 
     private static String[] randomStringArray() {
diff --git a/core/src/test/java/org/elasticsearch/action/index/IndexRequestTests.java b/core/src/test/java/org/elasticsearch/action/index/IndexRequestTests.java
index 1cdea96..7c08a0d 100644
--- a/core/src/test/java/org/elasticsearch/action/index/IndexRequestTests.java
+++ b/core/src/test/java/org/elasticsearch/action/index/IndexRequestTests.java
@@ -18,12 +18,18 @@
  */
 package org.elasticsearch.action.index;
 
+import org.elasticsearch.index.VersionType;
 import org.elasticsearch.test.ESTestCase;
 import org.junit.Test;
-import static org.hamcrest.Matchers.equalTo;
+
+import java.util.Arrays;
+import java.util.HashSet;
+import java.util.Set;
+
+import static org.hamcrest.Matchers.*;
 
 /**
-  */
+ */
 public class IndexRequestTests extends ESTestCase {
 
     @Test
@@ -39,9 +45,23 @@ public class IndexRequestTests extends ESTestCase {
         assertThat(IndexRequest.OpType.fromString(indexUpper), equalTo(IndexRequest.OpType.INDEX));
     }
 
-    @Test(expected= IllegalArgumentException.class)
-    public void testReadBogusString(){
+    @Test(expected = IllegalArgumentException.class)
+    public void testReadBogusString() {
         String foobar = "foobar";
         IndexRequest.OpType.fromString(foobar);
     }
+
+    public void testCreateOperationRejectsVersions() {
+        Set<VersionType> allButInternalSet = new HashSet<>(Arrays.asList(VersionType.values()));
+        allButInternalSet.remove(VersionType.INTERNAL);
+        VersionType[] allButInternal = allButInternalSet.toArray(new VersionType[]{});
+        IndexRequest request = new IndexRequest("index", "type", "1");
+        request.opType(IndexRequest.OpType.CREATE);
+        request.versionType(randomFrom(allButInternal));
+        assertThat(request.validate().validationErrors(), not(empty()));
+
+        request.versionType(VersionType.INTERNAL);
+        request.version(randomIntBetween(0, Integer.MAX_VALUE));
+        assertThat(request.validate().validationErrors(), not(empty()));
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java b/core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java
index b07ba2f..5fd9bae 100644
--- a/core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java
+++ b/core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java
@@ -20,13 +20,6 @@
 package org.elasticsearch.action.search;
 
 import org.elasticsearch.action.support.IndicesOptions;
-import org.elasticsearch.common.bytes.BytesArray;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.query.MatchAllQueryParser;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.rest.action.search.RestMultiSearchAction;
-import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.test.StreamsUtils;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
@@ -35,7 +28,6 @@ import org.elasticsearch.test.ESTestCase;
 import org.junit.Test;
 
 import java.io.IOException;
-import java.util.Collections;
 
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.nullValue;
@@ -44,9 +36,8 @@ public class MultiSearchRequestTests extends ESTestCase {
 
     @Test
     public void simpleAdd() throws Exception {
-        IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, Collections.singleton(new MatchAllQueryParser()), new NamedWriteableRegistry());
         byte[] data = StreamsUtils.copyToBytesFromClasspath("/org/elasticsearch/action/search/simple-msearch1.json");
-        MultiSearchRequest request = RestMultiSearchAction.parseRequest(new MultiSearchRequest(), new BytesArray(data), false, null, null, null, null, IndicesOptions.strictExpandOpenAndForbidClosed(),true, registry);
+        MultiSearchRequest request = new MultiSearchRequest().add(data, 0, data.length, false, null, null, null);
         assertThat(request.requests().size(), equalTo(8));
         assertThat(request.requests().get(0).indices()[0], equalTo("test"));
         assertThat(request.requests().get(0).indicesOptions(), equalTo(IndicesOptions.fromOptions(true, true, true, true, IndicesOptions.strictExpandOpenAndForbidClosed())));
@@ -71,9 +62,8 @@ public class MultiSearchRequestTests extends ESTestCase {
 
     @Test
     public void simpleAdd2() throws Exception {
-        IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, Collections.singleton(new MatchAllQueryParser()), new NamedWriteableRegistry());
         byte[] data = StreamsUtils.copyToBytesFromClasspath("/org/elasticsearch/action/search/simple-msearch2.json");
-        MultiSearchRequest request =RestMultiSearchAction.parseRequest(new MultiSearchRequest(), new BytesArray(data), false, null, null, null, null, IndicesOptions.strictExpandOpenAndForbidClosed(), true, registry);
+        MultiSearchRequest request = new MultiSearchRequest().add(data, 0, data.length, false, null, null, null);
         assertThat(request.requests().size(), equalTo(5));
         assertThat(request.requests().get(0).indices()[0], equalTo("test"));
         assertThat(request.requests().get(0).types().length, equalTo(0));
@@ -90,9 +80,8 @@ public class MultiSearchRequestTests extends ESTestCase {
 
     @Test
     public void simpleAdd3() throws Exception {
-        IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, Collections.singleton(new MatchAllQueryParser()), new NamedWriteableRegistry());
         byte[] data = StreamsUtils.copyToBytesFromClasspath("/org/elasticsearch/action/search/simple-msearch3.json");
-        MultiSearchRequest request =RestMultiSearchAction.parseRequest(new MultiSearchRequest(), new BytesArray(data), false, null, null, null, null, IndicesOptions.strictExpandOpenAndForbidClosed(), true, registry);
+        MultiSearchRequest request = new MultiSearchRequest().add(data, 0, data.length, false, null, null, null);
         assertThat(request.requests().size(), equalTo(4));
         assertThat(request.requests().get(0).indices()[0], equalTo("test0"));
         assertThat(request.requests().get(0).indices()[1], equalTo("test1"));
@@ -110,9 +99,8 @@ public class MultiSearchRequestTests extends ESTestCase {
 
     @Test
     public void simpleAdd4() throws Exception {
-        IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, Collections.singleton(new MatchAllQueryParser()), new NamedWriteableRegistry());
         byte[] data = StreamsUtils.copyToBytesFromClasspath("/org/elasticsearch/action/search/simple-msearch4.json");
-        MultiSearchRequest request = RestMultiSearchAction.parseRequest(new MultiSearchRequest(), new BytesArray(data), false, null, null, null, null, IndicesOptions.strictExpandOpenAndForbidClosed(), true, registry);
+        MultiSearchRequest request = new MultiSearchRequest().add(data, 0, data.length, false, null, null, null);
         assertThat(request.requests().size(), equalTo(3));
         assertThat(request.requests().get(0).indices()[0], equalTo("test0"));
         assertThat(request.requests().get(0).indices()[1], equalTo("test1"));
@@ -132,9 +120,8 @@ public class MultiSearchRequestTests extends ESTestCase {
 
     @Test
     public void simpleAdd5() throws Exception {
-        IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, Collections.singleton(new MatchAllQueryParser()), new NamedWriteableRegistry());
         byte[] data = StreamsUtils.copyToBytesFromClasspath("/org/elasticsearch/action/search/simple-msearch5.json");
-        MultiSearchRequest request = RestMultiSearchAction.parseRequest(new MultiSearchRequest(), new BytesArray(data), true, null, null, null, null, IndicesOptions.strictExpandOpenAndForbidClosed(), true, registry);
+        MultiSearchRequest request = new MultiSearchRequest().add(data, 0, data.length, true, null, null, null);
         assertThat(request.requests().size(), equalTo(3));
         assertThat(request.requests().get(0).indices()[0], equalTo("test0"));
         assertThat(request.requests().get(0).indices()[1], equalTo("test1"));
@@ -150,18 +137,6 @@ public class MultiSearchRequestTests extends ESTestCase {
         assertThat(request.requests().get(2).types()[0], equalTo("type2"));
         assertThat(request.requests().get(2).types()[1], equalTo("type1"));
         assertThat(request.requests().get(2).routing(), equalTo("123"));
-        assertNotNull(request.requests().get(0).template());
-        assertNotNull(request.requests().get(1).template());
-        assertNotNull(request.requests().get(2).template());
-        assertEquals(ScriptService.ScriptType.INLINE, request.requests().get(0).template().getType());
-        assertEquals(ScriptService.ScriptType.INLINE, request.requests().get(1).template().getType());
-        assertEquals(ScriptService.ScriptType.INLINE, request.requests().get(2).template().getType());
-        assertEquals("{\"query\":{\"match_{{template}}\":{}}}", request.requests().get(0).template().getScript());
-        assertEquals("{\"query\":{\"match_{{template}}\":{}}}", request.requests().get(1).template().getScript());
-        assertEquals("{\"query\":{\"match_{{template}}\":{}}}", request.requests().get(2).template().getScript());
-        assertEquals(1, request.requests().get(0).template().getParams().size());
-        assertEquals(1, request.requests().get(1).template().getParams().size());
-        assertEquals(1, request.requests().get(2).template().getParams().size());
     }
 
     public void testResponseErrorToXContent() throws IOException {
diff --git a/core/src/test/java/org/elasticsearch/action/search/SearchRequestBuilderTests.java b/core/src/test/java/org/elasticsearch/action/search/SearchRequestBuilderTests.java
index 34bea06..1a05794 100644
--- a/core/src/test/java/org/elasticsearch/action/search/SearchRequestBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/action/search/SearchRequestBuilderTests.java
@@ -21,7 +21,12 @@ package org.elasticsearch.action.search;
 
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.transport.TransportClient;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentHelper;
+import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.test.ESTestCase;
@@ -29,6 +34,9 @@ import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.Test;
 
+import java.io.IOException;
+
+import static org.hamcrest.CoreMatchers.containsString;
 import static org.hamcrest.CoreMatchers.equalTo;
 
 public class SearchRequestBuilderTests extends ESTestCase {
@@ -65,18 +73,62 @@ public class SearchRequestBuilderTests extends ESTestCase {
     }
 
     @Test
-    public void testSearchSourceBuilderToString() {
+    public void testXContentBuilderQueryToString() throws IOException {
+        SearchRequestBuilder searchRequestBuilder = client.prepareSearch();
+        XContentBuilder xContentBuilder = XContentFactory.contentBuilder(randomFrom(XContentType.values()));
+        xContentBuilder.startObject();
+        xContentBuilder.startObject("match_all");
+        xContentBuilder.endObject();
+        xContentBuilder.endObject();
+        searchRequestBuilder.setQuery(xContentBuilder);
+        assertThat(searchRequestBuilder.toString(), equalTo(new SearchSourceBuilder().query(xContentBuilder).toString()));
+    }
+
+    @Test
+    public void testStringQueryToString() {
+        SearchRequestBuilder searchRequestBuilder = client.prepareSearch();
+        String query = "{ \"match_all\" : {} }";
+        searchRequestBuilder.setQuery(query);
+        assertThat(searchRequestBuilder.toString(), containsString("\"query\":{ \"match_all\" : {} }"));
+    }
+
+    @Test
+    public void testStringSourceToString() {
+        SearchRequestBuilder searchRequestBuilder = client.prepareSearch();
+        String source = "{ \"query\" : { \"match_all\" : {} } }";
+        searchRequestBuilder.setSource(new BytesArray(source));
+        assertThat(searchRequestBuilder.toString(), equalTo(source));
+    }
+
+    @Test
+    public void testXContentBuilderSourceToString() throws IOException {
         SearchRequestBuilder searchRequestBuilder = client.prepareSearch();
-        searchRequestBuilder.setSource(new SearchSourceBuilder().query(QueryBuilders.termQuery("field", "value")));
-        assertThat(searchRequestBuilder.toString(), equalTo(new SearchSourceBuilder().query(QueryBuilders.termQuery("field", "value")).toString()));
+        XContentBuilder xContentBuilder = XContentFactory.contentBuilder(randomFrom(XContentType.values()));
+        xContentBuilder.startObject();
+        xContentBuilder.startObject("query");
+        xContentBuilder.startObject("match_all");
+        xContentBuilder.endObject();
+        xContentBuilder.endObject();
+        xContentBuilder.endObject();
+        searchRequestBuilder.setSource(xContentBuilder.bytes());
+        assertThat(searchRequestBuilder.toString(), equalTo(XContentHelper.convertToJson(xContentBuilder.bytes(), false, true)));
     }
 
     @Test
     public void testThatToStringDoesntWipeRequestSource() {
-        SearchRequestBuilder searchRequestBuilder = client.prepareSearch().setSource(new SearchSourceBuilder().query(QueryBuilders.termQuery("field", "value")));
-        String preToString = searchRequestBuilder.request().toString();
-        assertThat(searchRequestBuilder.toString(), equalTo(new SearchSourceBuilder().query(QueryBuilders.termQuery("field", "value")).toString()));
-        String postToString = searchRequestBuilder.request().toString();
+        String source = "{\n" +
+                "            \"query\" : {\n" +
+                "            \"match\" : {\n" +
+                "                \"field\" : {\n" +
+                "                    \"query\" : \"value\"" +
+                "                }\n" +
+                "            }\n" +
+                "        }\n" +
+                "        }";
+        SearchRequestBuilder searchRequestBuilder = client.prepareSearch().setSource(new BytesArray(source));
+        String preToString = searchRequestBuilder.request().source().toUtf8();
+        assertThat(searchRequestBuilder.toString(), equalTo(source));
+        String postToString = searchRequestBuilder.request().source().toUtf8();
         assertThat(preToString, equalTo(postToString));
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationActionTests.java b/core/src/test/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationActionTests.java
new file mode 100644
index 0000000..fce4312
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationActionTests.java
@@ -0,0 +1,316 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.support.single.instance;
+
+import org.elasticsearch.ExceptionsHelper;
+import org.elasticsearch.action.ActionListener;
+import org.elasticsearch.action.ActionResponse;
+import org.elasticsearch.action.IndicesRequest;
+import org.elasticsearch.action.support.ActionFilter;
+import org.elasticsearch.action.support.ActionFilters;
+import org.elasticsearch.action.support.PlainActionFuture;
+import org.elasticsearch.action.support.replication.ClusterStateCreationUtils;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.block.ClusterBlock;
+import org.elasticsearch.cluster.block.ClusterBlockException;
+import org.elasticsearch.cluster.block.ClusterBlockLevel;
+import org.elasticsearch.cluster.block.ClusterBlocks;
+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
+import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.cluster.routing.ShardIterator;
+import org.elasticsearch.cluster.routing.ShardRoutingState;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.rest.RestStatus;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.cluster.TestClusterService;
+import org.elasticsearch.test.transport.CapturingTransport;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.elasticsearch.transport.ConnectTransportException;
+import org.elasticsearch.transport.TransportException;
+import org.elasticsearch.transport.TransportService;
+import org.junit.AfterClass;
+import org.junit.Before;
+import org.junit.BeforeClass;
+
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+import java.util.function.Supplier;
+
+import static org.hamcrest.core.IsEqual.equalTo;
+
+public class TransportInstanceSingleOperationActionTests extends ESTestCase {
+
+    private static ThreadPool THREAD_POOL;
+
+    private TestClusterService clusterService;
+    private CapturingTransport transport;
+    private TransportService transportService;
+
+    private TestTransportInstanceSingleOperationAction action;
+
+    public static class Request extends InstanceShardOperationRequest<Request> {
+        public Request() {
+        }
+    }
+
+    public static class Response extends ActionResponse {
+        public Response() {
+        }
+    }
+
+    class TestTransportInstanceSingleOperationAction extends TransportInstanceSingleOperationAction<Request, Response> {
+        private final Map<ShardId, Object> shards = new HashMap<>();
+
+        public TestTransportInstanceSingleOperationAction(Settings settings, String actionName, TransportService transportService, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver, Supplier<Request> request) {
+            super(settings, actionName, THREAD_POOL, TransportInstanceSingleOperationActionTests.this.clusterService, transportService, actionFilters, indexNameExpressionResolver, request);
+        }
+
+        public Map<ShardId, Object> getResults() {
+            return shards;
+        }
+
+        @Override
+        protected String executor() {
+            return ThreadPool.Names.SAME;
+        }
+
+        @Override
+        protected void shardOperation(Request request, ActionListener<Response> listener) {
+            throw new UnsupportedOperationException("Not implemented in test class");
+        }
+
+        @Override
+        protected Response newResponse() {
+            return new Response();
+        }
+
+        @Override
+        protected boolean resolveRequest(ClusterState state, Request request, ActionListener<Response> listener) {
+            return true;
+        }
+
+        @Override
+        protected ShardIterator shards(ClusterState clusterState, Request request) {
+            return clusterState.routingTable().index(request.concreteIndex()).shard(request.shardId).primaryShardIt();
+        }
+    }
+
+    class MyResolver extends IndexNameExpressionResolver {
+        public MyResolver() {
+            super(Settings.EMPTY);
+        }
+
+        @Override
+        public String[] concreteIndices(ClusterState state, IndicesRequest request) {
+            return request.indices();
+        }
+    }
+
+    @BeforeClass
+    public static void startThreadPool() {
+        THREAD_POOL = new ThreadPool(TransportInstanceSingleOperationActionTests.class.getSimpleName());
+    }
+
+    @Before
+    public void setUp() throws Exception {
+        super.setUp();
+        transport = new CapturingTransport();
+        clusterService = new TestClusterService(THREAD_POOL);
+        transportService = new TransportService(transport, THREAD_POOL);
+        transportService.start();
+        action = new TestTransportInstanceSingleOperationAction(
+                Settings.EMPTY,
+                "indices:admin/test",
+                transportService,
+                new ActionFilters(new HashSet<ActionFilter>()),
+                new MyResolver(),
+                Request::new
+        );
+    }
+
+    @AfterClass
+    public static void destroyThreadPool() {
+        ThreadPool.terminate(THREAD_POOL, 30, TimeUnit.SECONDS);
+        // since static must set to null to be eligible for collection
+        THREAD_POOL = null;
+    }
+
+    public void testGlobalBlock() {
+        Request request = new Request();
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        ClusterBlocks.Builder block = ClusterBlocks.builder()
+                .addGlobalBlock(new ClusterBlock(1, "", false, true, RestStatus.SERVICE_UNAVAILABLE, ClusterBlockLevel.ALL));
+        clusterService.setState(ClusterState.builder(clusterService.state()).blocks(block));
+        try {
+            action.new AsyncSingleAction(request, listener).start();
+            listener.get();
+            fail("expected ClusterBlockException");
+        } catch (Throwable t) {
+            if (ExceptionsHelper.unwrap(t, ClusterBlockException.class) == null) {
+                logger.info("expected ClusterBlockException  but got ", t);
+                fail("expected ClusterBlockException");
+            }
+        }
+    }
+
+    public void testBasicRequestWorks() throws InterruptedException, ExecutionException, TimeoutException {
+        Request request = new Request().index("test");
+        request.shardId = 0;
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        clusterService.setState(ClusterStateCreationUtils.state("test", randomBoolean(), ShardRoutingState.STARTED));
+        action.new AsyncSingleAction(request, listener).start();
+        assertThat(transport.capturedRequests().length, equalTo(1));
+        transport.handleResponse(transport.capturedRequests()[0].requestId, new Response());
+        listener.get();
+    }
+
+    public void testFailureWithoutRetry() throws Exception {
+        Request request = new Request().index("test");
+        request.shardId = 0;
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        clusterService.setState(ClusterStateCreationUtils.state("test", randomBoolean(), ShardRoutingState.STARTED));
+
+        action.new AsyncSingleAction(request, listener).start();
+        assertThat(transport.capturedRequests().length, equalTo(1));
+        long requestId = transport.capturedRequests()[0].requestId;
+        transport.clear();
+        // this should not trigger retry or anything and the listener should report exception immediately
+        transport.handleResponse(requestId, new TransportException("a generic transport exception", new Exception("generic test exception")));
+
+        try {
+            // result should return immediately
+            assertTrue(listener.isDone());
+            listener.get();
+            fail("this should fail with a transport exception");
+        } catch (ExecutionException t) {
+            if (ExceptionsHelper.unwrap(t, TransportException.class) == null) {
+                logger.info("expected TransportException  but got ", t);
+                fail("expected and TransportException");
+            }
+        }
+    }
+
+    public void testSuccessAfterRetryWithClusterStateUpdate() throws Exception {
+        Request request = new Request().index("test");
+        request.shardId = 0;
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        boolean local = randomBoolean();
+        clusterService.setState(ClusterStateCreationUtils.state("test", local, ShardRoutingState.INITIALIZING));
+        action.new AsyncSingleAction(request, listener).start();
+        // this should fail because primary not initialized
+        assertThat(transport.capturedRequests().length, equalTo(0));
+        clusterService.setState(ClusterStateCreationUtils.state("test", local, ShardRoutingState.STARTED));
+        // this time it should work
+        assertThat(transport.capturedRequests().length, equalTo(1));
+        transport.handleResponse(transport.capturedRequests()[0].requestId, new Response());
+        listener.get();
+    }
+
+    public void testSuccessAfterRetryWithExcpetionFromTransport() throws Exception {
+        Request request = new Request().index("test");
+        request.shardId = 0;
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        boolean local = randomBoolean();
+        clusterService.setState(ClusterStateCreationUtils.state("test", local, ShardRoutingState.STARTED));
+        action.new AsyncSingleAction(request, listener).start();
+        assertThat(transport.capturedRequests().length, equalTo(1));
+        long requestId = transport.capturedRequests()[0].requestId;
+        transport.clear();
+        DiscoveryNode node = clusterService.state().getNodes().getLocalNode();
+        transport.handleResponse(requestId, new ConnectTransportException(node, "test exception"));
+        // trigger cluster state observer
+        clusterService.setState(ClusterStateCreationUtils.state("test", local, ShardRoutingState.STARTED));
+        assertThat(transport.capturedRequests().length, equalTo(1));
+        transport.handleResponse(transport.capturedRequests()[0].requestId, new Response());
+        listener.get();
+    }
+
+    public void testRetryOfAnAlreadyTimedOutRequest() throws Exception {
+        Request request = new Request().index("test").timeout(new TimeValue(0, TimeUnit.MILLISECONDS));
+        request.shardId = 0;
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        clusterService.setState(ClusterStateCreationUtils.state("test", randomBoolean(), ShardRoutingState.STARTED));
+        action.new AsyncSingleAction(request, listener).start();
+        assertThat(transport.capturedRequests().length, equalTo(1));
+        long requestId = transport.capturedRequests()[0].requestId;
+        transport.clear();
+        DiscoveryNode node = clusterService.state().getNodes().getLocalNode();
+        transport.handleResponse(requestId, new ConnectTransportException(node, "test exception"));
+
+        // wait until the timeout was triggered and we actually tried to send for the second time
+        assertBusy(new Runnable() {
+            @Override
+            public void run() {
+                assertThat(transport.capturedRequests().length, equalTo(1));
+            }
+        });
+
+        // let it fail the second time too
+        requestId = transport.capturedRequests()[0].requestId;
+        transport.handleResponse(requestId, new ConnectTransportException(node, "test exception"));
+        try {
+            // result should return immediately
+            assertTrue(listener.isDone());
+            listener.get();
+            fail("this should fail with a transport exception");
+        } catch (ExecutionException t) {
+            if (ExceptionsHelper.unwrap(t, ConnectTransportException.class) == null) {
+                logger.info("expected ConnectTransportException  but got ", t);
+                fail("expected and ConnectTransportException");
+            }
+        }
+    }
+
+    public void testUnresolvableRequestDoesNotHang() throws InterruptedException, ExecutionException, TimeoutException {
+        action = new TestTransportInstanceSingleOperationAction(
+                Settings.EMPTY,
+                "indices:admin/test_unresolvable",
+                transportService,
+                new ActionFilters(new HashSet<ActionFilter>()),
+                new MyResolver(),
+                Request::new
+        ) {
+            @Override
+            protected boolean resolveRequest(ClusterState state, Request request, ActionListener<Response> listener) {
+                return false;
+            }
+        };
+        Request request = new Request().index("test");
+        request.shardId = 0;
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        clusterService.setState(ClusterStateCreationUtils.state("test", randomBoolean(), ShardRoutingState.STARTED));
+        action.new AsyncSingleAction(request, listener).start();
+        assertThat(transport.capturedRequests().length, equalTo(0));
+        try {
+            listener.get();
+        } catch (Throwable t) {
+            if (ExceptionsHelper.unwrap(t, IllegalStateException.class) == null) {
+                logger.info("expected IllegalStateException  but got ", t);
+                fail("expected and IllegalStateException");
+            }
+        }
+    }
+}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/SuggestSearchBenchMark.java b/core/src/test/java/org/elasticsearch/benchmark/search/SuggestSearchBenchMark.java
index 89e176f..213a522 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/search/SuggestSearchBenchMark.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/search/SuggestSearchBenchMark.java
@@ -32,7 +32,6 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.node.Node;
 import org.elasticsearch.search.suggest.Suggest.Suggestion.Entry.Option;
-import org.elasticsearch.search.suggest.SuggestBuilder;
 import org.elasticsearch.search.suggest.SuggestBuilders;
 
 import java.io.IOException;
@@ -42,9 +41,7 @@ import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.prefixQuery;
+import static org.elasticsearch.index.query.QueryBuilders.*;
 import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
 
 /**
@@ -121,9 +118,7 @@ public class SuggestSearchBenchMark {
             String term = "prefix" + startChar;
             SearchResponse response = client.prepareSearch()
                     .setQuery(prefixQuery("field", term))
-                    .suggest(
-                            new SuggestBuilder().addSuggestion(SuggestBuilders.termSuggestion("field").field("field").text(term)
-                                    .suggestMode("always")))
+                    .addSuggestion(SuggestBuilders.termSuggestion("field").field("field").text(term).suggestMode("always"))
                     .execute().actionGet();
             if (response.getHits().totalHits() == 0) {
                 System.err.println("No hits");
@@ -140,9 +135,7 @@ public class SuggestSearchBenchMark {
             String term = "prefix" + startChar;
             SearchResponse response = client.prepareSearch()
                     .setQuery(matchQuery("field", term))
-                    .suggest(
-                            new SuggestBuilder().addSuggestion(SuggestBuilders.termSuggestion("field").text(term).field("field")
-                                    .suggestMode("always")))
+                    .addSuggestion(SuggestBuilders.termSuggestion("field").text(term).field("field").suggestMode("always"))
                     .execute().actionGet();
             timeTaken += response.getTookInMillis();
             if (response.getSuggest() == null) {
diff --git a/core/src/test/java/org/elasticsearch/broadcast/BroadcastActionsIT.java b/core/src/test/java/org/elasticsearch/broadcast/BroadcastActionsIT.java
index 78ca44b..e2da702 100644
--- a/core/src/test/java/org/elasticsearch/broadcast/BroadcastActionsIT.java
+++ b/core/src/test/java/org/elasticsearch/broadcast/BroadcastActionsIT.java
@@ -68,6 +68,15 @@ public class BroadcastActionsIT extends ESIntegTestCase {
             assertThat(countResponse.getSuccessfulShards(), equalTo(numShards.numPrimaries));
             assertThat(countResponse.getFailedShards(), equalTo(0));
         }
+
+        for (int i = 0; i < 5; i++) {
+            // test failed (simply query that can't be parsed)
+            try {
+                client().count(countRequest("test").source("{ term : { _type : \"type1 } }".getBytes(StandardCharsets.UTF_8))).actionGet();
+            } catch(SearchPhaseExecutionException e) {
+                assertThat(e.shardFailures().length, equalTo(numShards.numPrimaries));
+            }
+        }
     }
 
     private XContentBuilder source(String id, String nameValue) throws IOException {
diff --git a/core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java b/core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java
index fb60e50..1aa1602 100644
--- a/core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java
@@ -21,7 +21,6 @@ package org.elasticsearch.cluster;
 
 import com.carrotsearch.hppc.cursors.ObjectCursor;
 import com.google.common.collect.ImmutableMap;
-
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.block.ClusterBlock;
 import org.elasticsearch.cluster.block.ClusterBlocks;
@@ -40,7 +39,6 @@ import org.elasticsearch.discovery.DiscoverySettings;
 import org.elasticsearch.gateway.GatewayService;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.warmer.IndexWarmersMetaData;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
@@ -533,7 +531,7 @@ public class ClusterStateDiffIT extends ESIntegTestCase {
                             randomName("warm"),
                             new String[]{randomName("type")},
                             randomBoolean(),
-                            new IndexWarmersMetaData.SearchSource(new BytesArray(randomAsciiOfLength(1000))))
+                            new BytesArray(randomAsciiOfLength(1000)))
             );
         } else {
             return new IndexWarmersMetaData();
diff --git a/core/src/test/java/org/elasticsearch/common/collect/EvictingQueueTests.java b/core/src/test/java/org/elasticsearch/common/collect/EvictingQueueTests.java
new file mode 100644
index 0000000..de822b8
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/collect/EvictingQueueTests.java
@@ -0,0 +1,150 @@
+/*
+ * Copyright (C) 2012 The Guava Authors
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.elasticsearch.common.collect;
+
+import org.elasticsearch.common.util.CollectionUtils;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.Collections;
+import java.util.NoSuchElementException;
+
+public class EvictingQueueTests extends ESTestCase {
+    public void testCreateWithNegativeSize() throws Exception {
+        try {
+            new EvictingQueue<>(-1);
+            fail();
+        } catch (IllegalArgumentException expected) {
+        }
+    }
+
+    public void testCreateWithZeroSize() throws Exception {
+        EvictingQueue<String> queue = new EvictingQueue<>(0);
+        assertEquals(0, queue.size());
+
+        assertTrue(queue.add("hi"));
+        assertEquals(0, queue.size());
+
+        assertTrue(queue.offer("hi"));
+        assertEquals(0, queue.size());
+
+        assertFalse(queue.remove("hi"));
+        assertEquals(0, queue.size());
+
+        try {
+            queue.element();
+            fail();
+        } catch (NoSuchElementException expected) {}
+
+        assertNull(queue.peek());
+        assertNull(queue.poll());
+        try {
+            queue.remove();
+            fail();
+        } catch (NoSuchElementException expected) {}
+    }
+
+    public void testRemainingCapacityMaximumSizeZero() {
+        EvictingQueue<String> queue = new EvictingQueue<>(0);
+        assertEquals(0, queue.remainingCapacity());
+    }
+
+    public void testRemainingCapacityMaximumSizeOne() {
+        EvictingQueue<String> queue = new EvictingQueue<>(1);
+        assertEquals(1, queue.remainingCapacity());
+        queue.add("hi");
+        assertEquals(0, queue.remainingCapacity());
+    }
+
+    public void testRemainingCapacityMaximumSizeThree() {
+        EvictingQueue<String> queue = new EvictingQueue<>(3);
+        assertEquals(3, queue.remainingCapacity());
+        queue.add("hi");
+        assertEquals(2, queue.remainingCapacity());
+        queue.add("hi");
+        assertEquals(1, queue.remainingCapacity());
+        queue.add("hi");
+        assertEquals(0, queue.remainingCapacity());
+    }
+
+    public void testEvictingAfterOne() throws Exception {
+        EvictingQueue<String> queue = new EvictingQueue<>(1);
+        assertEquals(0, queue.size());
+        assertEquals(1, queue.remainingCapacity());
+
+        assertTrue(queue.add("hi"));
+        assertEquals("hi", queue.element());
+        assertEquals("hi", queue.peek());
+        assertEquals(1, queue.size());
+        assertEquals(0, queue.remainingCapacity());
+
+        assertTrue(queue.add("there"));
+        assertEquals("there", queue.element());
+        assertEquals("there", queue.peek());
+        assertEquals(1, queue.size());
+        assertEquals(0, queue.remainingCapacity());
+
+        assertEquals("there", queue.remove());
+        assertEquals(0, queue.size());
+        assertEquals(1, queue.remainingCapacity());
+    }
+
+    public void testEvictingAfterThree() throws Exception {
+        EvictingQueue<String> queue = new EvictingQueue<>(3);
+        assertEquals(0, queue.size());
+        assertEquals(3, queue.remainingCapacity());
+
+        assertTrue(queue.add("one"));
+        assertTrue(queue.add("two"));
+        assertTrue(queue.add("three"));
+        assertEquals("one", queue.element());
+        assertEquals("one", queue.peek());
+        assertEquals(3, queue.size());
+        assertEquals(0, queue.remainingCapacity());
+
+        assertTrue(queue.add("four"));
+        assertEquals("two", queue.element());
+        assertEquals("two", queue.peek());
+        assertEquals(3, queue.size());
+        assertEquals(0, queue.remainingCapacity());
+
+        assertEquals("two", queue.remove());
+        assertEquals(2, queue.size());
+        assertEquals(1, queue.remainingCapacity());
+    }
+
+    public void testAddAll() throws Exception {
+        EvictingQueue<String> queue = new EvictingQueue<>(3);
+        assertEquals(0, queue.size());
+        assertEquals(3, queue.remainingCapacity());
+
+        assertTrue(queue.addAll(CollectionUtils.arrayAsArrayList("one", "two", "three")));
+        assertEquals("one", queue.element());
+        assertEquals("one", queue.peek());
+        assertEquals(3, queue.size());
+        assertEquals(0, queue.remainingCapacity());
+
+        assertTrue(queue.addAll(Collections.singletonList("four")));
+        assertEquals("two", queue.element());
+        assertEquals("two", queue.peek());
+        assertEquals(3, queue.size());
+        assertEquals(0, queue.remainingCapacity());
+
+        assertEquals("two", queue.remove());
+        assertEquals(2, queue.size());
+        assertEquals(1, queue.remainingCapacity());
+    }
+}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/common/collect/IteratorsTests.java b/core/src/test/java/org/elasticsearch/common/collect/IteratorsTests.java
new file mode 100644
index 0000000..9097218
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/collect/IteratorsTests.java
@@ -0,0 +1,162 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.collect;
+
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.*;
+
+public class IteratorsTests extends ESTestCase {
+    public void testConcatentation() {
+        List<Integer> threeTwoOne = Arrays.asList(3, 2, 1);
+        List<Integer> fourFiveSix = Arrays.asList(4, 5, 6);
+        Iterator<Integer> concat = Iterators.concat(threeTwoOne.iterator(), fourFiveSix.iterator());
+        assertContainsInOrder(concat, 3, 2, 1, 4, 5, 6);
+    }
+
+    public void testNoConcatenation() {
+        Iterator<Integer> iterator = Iterators.<Integer>concat();
+        assertEmptyIterator(iterator);
+    }
+
+    public void testEmptyConcatenation() {
+        Iterator<Integer> iterator = Iterators.<Integer>concat(empty());
+        assertEmptyIterator(iterator);
+    }
+
+    public void testMultipleEmptyConcatenation() {
+        Iterator<Integer> iterator = Iterators.concat(empty(), empty());
+        assertEmptyIterator(iterator);
+    }
+
+    public void testSingleton() {
+        int value = randomInt();
+        assertSingleton(value, singletonIterator(value));
+    }
+
+    public void testEmptyBeforeSingleton() {
+        int value = randomInt();
+        assertSingleton(value, empty(), singletonIterator(value));
+    }
+
+
+    public void testEmptyAfterSingleton() {
+        int value = randomInt();
+        assertSingleton(value, singletonIterator(value), empty());
+    }
+
+    public void testRandomSingleton() {
+        int numberOfIterators = randomIntBetween(1, 1000);
+        int singletonIndex = randomIntBetween(0, numberOfIterators - 1);
+        int value = randomInt();
+        Iterator<Integer>[] iterators = new Iterator[numberOfIterators];
+        for (int i = 0; i < numberOfIterators; i++) {
+            iterators[i] = i != singletonIndex ? empty() : singletonIterator(value);
+        }
+        assertSingleton(value, iterators);
+    }
+
+    public void testRandomIterators() {
+        int numberOfIterators = randomIntBetween(1, 1000);
+        Iterator<Integer>[] iterators = new Iterator[numberOfIterators];
+        List<Integer> values = new ArrayList<>();
+        for (int i = 0; i < numberOfIterators; i++) {
+            int numberOfValues = randomIntBetween(0, 256);
+            List<Integer> theseValues = new ArrayList<>();
+            for (int j = 0; j < numberOfValues; j++) {
+                int value = randomInt();
+                values.add(value);
+                theseValues.add(value);
+            }
+            iterators[i] = theseValues.iterator();
+        }
+        assertContainsInOrder(Iterators.concat(iterators), values.toArray(new Integer[values.size()]));
+    }
+
+    public void testTwoEntries() {
+        int first = randomInt();
+        int second = randomInt();
+        Iterator<Integer> concat = Iterators.concat(singletonIterator(first), empty(), empty(), singletonIterator(second));
+        assertContainsInOrder(concat, first, second);
+    }
+
+    public void testNull() {
+        try {
+            Iterators.concat((Iterator<?>)null);
+            fail("expected " + NullPointerException.class.getSimpleName());
+        } catch (NullPointerException e) {
+
+        }
+    }
+
+    public void testNullIterator() {
+        try {
+            Iterators.concat(singletonIterator(1), empty(), null, empty(), singletonIterator(2));
+            fail("expected " + NullPointerException.class.getSimpleName());
+        } catch (NullPointerException e) {
+
+        }
+    }
+
+    private <T> Iterator<T> singletonIterator(T value) {
+        return Collections.singleton(value).iterator();
+    }
+
+    private <T> void assertSingleton(T value, Iterator<T>... iterators) {
+        Iterator<T> concat = Iterators.concat(iterators);
+        assertContainsInOrder(concat, value);
+    }
+
+    private <T> Iterator<T> empty() {
+        return new Iterator<T>() {
+            @Override
+            public boolean hasNext() {
+                return false;
+            }
+
+            @Override
+            public T next() {
+                throw new NoSuchElementException();
+            }
+        };
+    }
+
+    private <T> void assertContainsInOrder(Iterator<T> iterator, T... values) {
+        for (T value : values) {
+            assertTrue(iterator.hasNext());
+            assertEquals(value, iterator.next());
+        }
+        assertNoSuchElementException(iterator);
+    }
+
+    private <T> void assertEmptyIterator(Iterator<T> iterator) {
+        assertFalse(iterator.hasNext());
+        assertNoSuchElementException(iterator);
+    }
+
+    private <T> void assertNoSuchElementException(Iterator<T> iterator) {
+        try {
+            iterator.next();
+            fail("expected " + NoSuchElementException.class.getSimpleName());
+        } catch (NoSuchElementException e) {
+
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/common/inject/ModuleTestCase.java b/core/src/test/java/org/elasticsearch/common/inject/ModuleTestCase.java
index 255def7..9b327fb 100644
--- a/core/src/test/java/org/elasticsearch/common/inject/ModuleTestCase.java
+++ b/core/src/test/java/org/elasticsearch/common/inject/ModuleTestCase.java
@@ -60,22 +60,6 @@ public abstract class ModuleTestCase extends ESTestCase {
         fail("Did not find any binding to " + to.getName() + ". Found these bindings:\n" + s);
     }
 
-//    /** Configures the module and asserts "instance" is bound to "to". */
-//    public void assertInstanceBinding(Module module, Class to, Object instance) {
-//        List<Element> elements = Elements.getElements(module);
-//        for (Element element : elements) {
-//            if (element instanceof ProviderInstanceBinding) {
-//                assertEquals(instance, ((ProviderInstanceBinding) element).getProviderInstance().get());
-//                return;
-//            }
-//        }
-//        StringBuilder s = new StringBuilder();
-//        for (Element element : elements) {
-//            s.append(element + "\n");
-//        }
-//        fail("Did not find any binding to " + to.getName() + ". Found these bindings:\n" + s);
-//    }
-
     /**
      * Attempts to configure the module, and asserts an {@link IllegalArgumentException} is
      * caught, containing the given messages
diff --git a/core/src/test/java/org/elasticsearch/common/network/InetAddressesTests.java b/core/src/test/java/org/elasticsearch/common/network/InetAddressesTests.java
new file mode 100644
index 0000000..2aa284d
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/network/InetAddressesTests.java
@@ -0,0 +1,217 @@
+/*
+ * Copyright (C) 2008 The Guava Authors
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.elasticsearch.common.network;
+
+import org.elasticsearch.test.ESTestCase;
+
+import java.net.InetAddress;
+import java.net.UnknownHostException;
+
+public class InetAddressesTests extends ESTestCase {
+    public void testForStringBogusInput() {
+        String[] bogusInputs = {
+                "",
+                "016.016.016.016",
+                "016.016.016",
+                "016.016",
+                "016",
+                "000.000.000.000",
+                "000",
+                "0x0a.0x0a.0x0a.0x0a",
+                "0x0a.0x0a.0x0a",
+                "0x0a.0x0a",
+                "0x0a",
+                "42.42.42.42.42",
+                "42.42.42",
+                "42.42",
+                "42",
+                "42..42.42",
+                "42..42.42.42",
+                "42.42.42.42.",
+                "42.42.42.42...",
+                ".42.42.42.42",
+                "...42.42.42.42",
+                "42.42.42.-0",
+                "42.42.42.+0",
+                ".",
+                "...",
+                "bogus",
+                "bogus.com",
+                "192.168.0.1.com",
+                "12345.67899.-54321.-98765",
+                "257.0.0.0",
+                "42.42.42.-42",
+                "3ffe::1.net",
+                "3ffe::1::1",
+                "1::2::3::4:5",
+                "::7:6:5:4:3:2:",  // should end with ":0"
+                ":6:5:4:3:2:1::",  // should begin with "0:"
+                "2001::db:::1",
+                "FEDC:9878",
+                "+1.+2.+3.4",
+                "1.2.3.4e0",
+                "::7:6:5:4:3:2:1:0",  // too many parts
+                "7:6:5:4:3:2:1:0::",  // too many parts
+                "9:8:7:6:5:4:3::2:1",  // too many parts
+                "0:1:2:3::4:5:6:7",  // :: must remove at least one 0.
+                "3ffe:0:0:0:0:0:0:0:1",  // too many parts (9 instead of 8)
+                "3ffe::10000",  // hextet exceeds 16 bits
+                "3ffe::goog",
+                "3ffe::-0",
+                "3ffe::+0",
+                "3ffe::-1",
+                ":",
+                ":::",
+                "::1.2.3",
+                "::1.2.3.4.5",
+                "::1.2.3.4:",
+                "1.2.3.4::",
+                "2001:db8::1:",
+                ":2001:db8::1",
+                ":1:2:3:4:5:6:7",
+                "1:2:3:4:5:6:7:",
+                ":1:2:3:4:5:6:"
+        };
+
+        for (int i = 0; i < bogusInputs.length; i++) {
+            try {
+                InetAddresses.forString(bogusInputs[i]);
+                fail("IllegalArgumentException expected for '" + bogusInputs[i] + "'");
+            } catch (IllegalArgumentException expected) {
+                // expected behavior
+            }
+            assertFalse(InetAddresses.isInetAddress(bogusInputs[i]));
+        }
+    }
+
+    public void test3ff31() {
+        try {
+            InetAddresses.forString("3ffe:::1");
+            fail("IllegalArgumentException expected");
+        } catch (IllegalArgumentException expected) {
+            // expected behavior
+        }
+        assertFalse(InetAddresses.isInetAddress("016.016.016.016"));
+    }
+
+    public void testForStringIPv4Input() throws UnknownHostException {
+        String ipStr = "192.168.0.1";
+        InetAddress ipv4Addr = null;
+        // Shouldn't hit DNS, because it's an IP string literal.
+        ipv4Addr = InetAddress.getByName(ipStr);
+        assertEquals(ipv4Addr, InetAddresses.forString(ipStr));
+        assertTrue(InetAddresses.isInetAddress(ipStr));
+    }
+
+    public void testForStringIPv6Input() throws UnknownHostException {
+        String ipStr = "3ffe::1";
+        InetAddress ipv6Addr = null;
+        // Shouldn't hit DNS, because it's an IP string literal.
+        ipv6Addr = InetAddress.getByName(ipStr);
+        assertEquals(ipv6Addr, InetAddresses.forString(ipStr));
+        assertTrue(InetAddresses.isInetAddress(ipStr));
+    }
+
+    public void testForStringIPv6EightColons() throws UnknownHostException {
+        String[] eightColons = {
+                "::7:6:5:4:3:2:1",
+                "::7:6:5:4:3:2:0",
+                "7:6:5:4:3:2:1::",
+                "0:6:5:4:3:2:1::",
+        };
+
+        for (int i = 0; i < eightColons.length; i++) {
+            InetAddress ipv6Addr = null;
+            // Shouldn't hit DNS, because it's an IP string literal.
+            ipv6Addr = InetAddress.getByName(eightColons[i]);
+            assertEquals(ipv6Addr, InetAddresses.forString(eightColons[i]));
+            assertTrue(InetAddresses.isInetAddress(eightColons[i]));
+        }
+    }
+
+    public void testConvertDottedQuadToHex() throws UnknownHostException {
+        String[] ipStrings = {"7::0.128.0.127", "7::0.128.0.128",
+                "7::128.128.0.127", "7::0.128.128.127"};
+
+        for (String ipString : ipStrings) {
+            // Shouldn't hit DNS, because it's an IP string literal.
+            InetAddress ipv6Addr = InetAddress.getByName(ipString);
+            assertEquals(ipv6Addr, InetAddresses.forString(ipString));
+            assertTrue(InetAddresses.isInetAddress(ipString));
+        }
+    }
+
+    public void testToAddrStringIPv4() {
+        // Don't need to test IPv4 much; it just calls getHostAddress().
+        assertEquals("1.2.3.4",
+                InetAddresses.toAddrString(
+                        InetAddresses.forString("1.2.3.4")));
+    }
+
+    public void testToAddrStringIPv6() {
+        assertEquals("1:2:3:4:5:6:7:8",
+                InetAddresses.toAddrString(
+                        InetAddresses.forString("1:2:3:4:5:6:7:8")));
+        assertEquals("2001:0:0:4::8",
+                InetAddresses.toAddrString(
+                        InetAddresses.forString("2001:0:0:4:0:0:0:8")));
+        assertEquals("2001::4:5:6:7:8",
+                InetAddresses.toAddrString(
+                        InetAddresses.forString("2001:0:0:4:5:6:7:8")));
+        assertEquals("2001:0:3:4:5:6:7:8",
+                InetAddresses.toAddrString(
+                        InetAddresses.forString("2001:0:3:4:5:6:7:8")));
+        assertEquals("0:0:3::ffff",
+                InetAddresses.toAddrString(
+                        InetAddresses.forString("0:0:3:0:0:0:0:ffff")));
+        assertEquals("::4:0:0:0:ffff",
+                InetAddresses.toAddrString(
+                        InetAddresses.forString("0:0:0:4:0:0:0:ffff")));
+        assertEquals("::5:0:0:ffff",
+                InetAddresses.toAddrString(
+                        InetAddresses.forString("0:0:0:0:5:0:0:ffff")));
+        assertEquals("1::4:0:0:7:8",
+                InetAddresses.toAddrString(
+                        InetAddresses.forString("1:0:0:4:0:0:7:8")));
+        assertEquals("::",
+                InetAddresses.toAddrString(
+                        InetAddresses.forString("0:0:0:0:0:0:0:0")));
+        assertEquals("::1",
+                InetAddresses.toAddrString(
+                        InetAddresses.forString("0:0:0:0:0:0:0:1")));
+        assertEquals("2001:658:22a:cafe::",
+                InetAddresses.toAddrString(
+                        InetAddresses.forString("2001:0658:022a:cafe::")));
+        assertEquals("::102:304",
+                InetAddresses.toAddrString(
+                        InetAddresses.forString("::1.2.3.4")));
+    }
+
+    public void testToUriStringIPv4() {
+        String ipStr = "1.2.3.4";
+        InetAddress ip = InetAddresses.forString(ipStr);
+        assertEquals("1.2.3.4", InetAddresses.toUriString(ip));
+    }
+
+    public void testToUriStringIPv6() {
+        // Unfortunately the InetAddress.toString() method for IPv6 addresses
+        // does not collapse contiguous shorts of zeroes with the :: abbreviation.
+        String ipStr = "3ffe::1";
+        InetAddress ip = InetAddresses.forString(ipStr);
+        assertEquals("[3ffe::1]", InetAddresses.toUriString(ip));
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/common/network/NetworkAddressTests.java b/core/src/test/java/org/elasticsearch/common/network/NetworkAddressTests.java
index 5847bb7..b53a56a 100644
--- a/core/src/test/java/org/elasticsearch/common/network/NetworkAddressTests.java
+++ b/core/src/test/java/org/elasticsearch/common/network/NetworkAddressTests.java
@@ -22,9 +22,11 @@ package org.elasticsearch.common.network;
 import org.elasticsearch.test.ESTestCase;
 
 import java.io.IOException;
+import java.net.Inet4Address;
 import java.net.Inet6Address;
 import java.net.InetAddress;
 import java.net.InetSocketAddress;
+import java.util.Random;
 
 /**
  * Tests for network address formatting. Please avoid using any methods that cause DNS lookups!
@@ -84,6 +86,32 @@ public class NetworkAddressTests extends ESTestCase {
         assertEquals("[::1]:1234", NetworkAddress.formatAddress(new InetSocketAddress(forgeScoped(null, "::1", 5), 1234)));
         assertEquals("[::1]:1234", NetworkAddress.formatAddress(new InetSocketAddress(forgeScoped("localhost", "::1", 5), 1234)));
     }
+
+    /** Test that ipv4 address formatting round trips */
+    public void testRoundTripV4() throws Exception {
+        byte bytes[] = new byte[4];
+        Random random = random();
+        for (int i = 0; i < 10000; i++) {
+            random.nextBytes(bytes);
+            InetAddress expected = Inet4Address.getByAddress(bytes);
+            String formatted = NetworkAddress.formatAddress(expected);
+            InetAddress actual = InetAddress.getByName(formatted);
+            assertEquals(expected, actual);
+        }
+    }
+
+    /** Test that ipv6 address formatting round trips */
+    public void testRoundTripV6() throws Exception {
+        byte bytes[] = new byte[16];
+        Random random = random();
+        for (int i = 0; i < 10000; i++) {
+            random.nextBytes(bytes);
+            InetAddress expected = Inet6Address.getByAddress(bytes);
+            String formatted = NetworkAddress.formatAddress(expected);
+            InetAddress actual = InetAddress.getByName(formatted);
+            assertEquals(expected, actual);
+        }
+    }
     
     /** creates address without any lookups. hostname can be null, for missing */
     private InetAddress forge(String hostname, String address) throws IOException {
diff --git a/core/src/test/java/org/elasticsearch/document/DocumentActionsIT.java b/core/src/test/java/org/elasticsearch/document/DocumentActionsIT.java
index da7f440..6cf8ba7 100644
--- a/core/src/test/java/org/elasticsearch/document/DocumentActionsIT.java
+++ b/core/src/test/java/org/elasticsearch/document/DocumentActionsIT.java
@@ -32,8 +32,6 @@ import org.elasticsearch.action.index.IndexResponse;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.search.MultiMatchQuery;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
@@ -165,6 +163,13 @@ public class DocumentActionsIT extends ESIntegTestCase {
             assertThat(countResponse.getSuccessfulShards(), equalTo(numShards.numPrimaries));
             assertThat(countResponse.getFailedShards(), equalTo(0));
 
+            // test failed (simply query that can't be parsed)
+            try {
+                client().count(countRequest("test").source("{ term : { _type : \"type1 } }")).actionGet();
+            } catch(SearchPhaseExecutionException e) {
+                assertThat(e.shardFailures().length, equalTo(numShards.numPrimaries));
+            }
+
             // count with no query is a match all one
             countResponse = client().prepareCount("test").execute().actionGet();
             assertThat("Failures " + countResponse.getShardFailures(), countResponse.getShardFailures() == null ? 0 : countResponse.getShardFailures().length, equalTo(0));
diff --git a/core/src/test/java/org/elasticsearch/gateway/MetaDataStateFormatTests.java b/core/src/test/java/org/elasticsearch/gateway/MetaDataStateFormatTests.java
index 98630ed..1d51c30 100644
--- a/core/src/test/java/org/elasticsearch/gateway/MetaDataStateFormatTests.java
+++ b/core/src/test/java/org/elasticsearch/gateway/MetaDataStateFormatTests.java
@@ -18,8 +18,6 @@
  */
 package org.elasticsearch.gateway;
 
-import com.google.common.collect.Iterators;
-
 import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.store.ChecksumIndexInput;
 import org.apache.lucene.store.Directory;
@@ -33,6 +31,7 @@ import org.elasticsearch.Version;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
+import org.elasticsearch.common.collect.Iterators;
 import org.elasticsearch.common.io.FileSystemUtils;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.xcontent.ToXContent;
@@ -59,6 +58,7 @@ import java.util.Collections;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Set;
+import java.util.stream.StreamSupport;
 
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.instanceOf;
@@ -535,7 +535,7 @@ public class MetaDataStateFormatTests extends ESTestCase {
 
     public Path[] content(String glob, Path dir) throws IOException {
         try (DirectoryStream<Path> stream = Files.newDirectoryStream(dir, glob)) {
-            return Iterators.toArray(stream.iterator(), Path.class);
+            return StreamSupport.stream(stream.spliterator(), false).toArray(length -> new Path[length]);
         }
     }
 
diff --git a/core/src/test/java/org/elasticsearch/get/GetActionIT.java b/core/src/test/java/org/elasticsearch/get/GetActionIT.java
index 55b104d..b26e3ec 100644
--- a/core/src/test/java/org/elasticsearch/get/GetActionIT.java
+++ b/core/src/test/java/org/elasticsearch/get/GetActionIT.java
@@ -25,11 +25,7 @@ import org.elasticsearch.action.ShardOperationFailedException;
 import org.elasticsearch.action.admin.indices.alias.Alias;
 import org.elasticsearch.action.admin.indices.flush.FlushResponse;
 import org.elasticsearch.action.delete.DeleteResponse;
-import org.elasticsearch.action.get.GetRequestBuilder;
-import org.elasticsearch.action.get.GetResponse;
-import org.elasticsearch.action.get.MultiGetRequest;
-import org.elasticsearch.action.get.MultiGetRequestBuilder;
-import org.elasticsearch.action.get.MultiGetResponse;
+import org.elasticsearch.action.get.*;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Strings;
@@ -53,14 +49,7 @@ import java.util.Set;
 import static java.util.Collections.singleton;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.hasKey;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.not;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
-import static org.hamcrest.Matchers.startsWith;
+import static org.hamcrest.Matchers.*;
 
 public class GetActionIT extends ESIntegTestCase {
 
@@ -600,7 +589,7 @@ public class GetActionIT extends ESIntegTestCase {
         assertThat(response.getResponses()[1].getResponse().getSourceAsMap().get("field").toString(), equalTo("value1"));
         assertThat(response.getResponses()[2].getFailure(), notNullValue());
         assertThat(response.getResponses()[2].getFailure().getId(), equalTo("1"));
-        assertThat(response.getResponses()[2].getFailure().getMessage(), startsWith("[type1][1]: version conflict, current [1], provided [2]"));
+        assertThat(response.getResponses()[2].getFailure().getMessage(), startsWith("[type1][1]: version conflict"));
         assertThat(response.getResponses()[2].getFailure().getFailure(), instanceOf(VersionConflictEngineException.class));
 
         //Version from Lucene index
@@ -623,7 +612,7 @@ public class GetActionIT extends ESIntegTestCase {
         assertThat(response.getResponses()[1].getResponse().getSourceAsMap().get("field").toString(), equalTo("value1"));
         assertThat(response.getResponses()[2].getFailure(), notNullValue());
         assertThat(response.getResponses()[2].getFailure().getId(), equalTo("1"));
-        assertThat(response.getResponses()[2].getFailure().getMessage(), startsWith("[type1][1]: version conflict, current [1], provided [2]"));
+        assertThat(response.getResponses()[2].getFailure().getMessage(), startsWith("[type1][1]: version conflict"));
         assertThat(response.getResponses()[2].getFailure().getFailure(), instanceOf(VersionConflictEngineException.class));
 
 
@@ -648,7 +637,7 @@ public class GetActionIT extends ESIntegTestCase {
         assertThat(response.getResponses()[1].getFailure(), notNullValue());
         assertThat(response.getResponses()[1].getFailure().getId(), equalTo("2"));
         assertThat(response.getResponses()[1].getIndex(), equalTo("test"));
-        assertThat(response.getResponses()[1].getFailure().getMessage(), startsWith("[type1][2]: version conflict, current [2], provided [1]"));
+        assertThat(response.getResponses()[1].getFailure().getMessage(), startsWith("[type1][2]: version conflict"));
         assertThat(response.getResponses()[2].getId(), equalTo("2"));
         assertThat(response.getResponses()[2].getIndex(), equalTo("test"));
         assertThat(response.getResponses()[2].getFailure(), nullValue());
@@ -674,7 +663,7 @@ public class GetActionIT extends ESIntegTestCase {
         assertThat(response.getResponses()[1].getFailure(), notNullValue());
         assertThat(response.getResponses()[1].getFailure().getId(), equalTo("2"));
         assertThat(response.getResponses()[1].getIndex(), equalTo("test"));
-        assertThat(response.getResponses()[1].getFailure().getMessage(), startsWith("[type1][2]: version conflict, current [2], provided [1]"));
+        assertThat(response.getResponses()[1].getFailure().getMessage(), startsWith("[type1][2]: version conflict"));
         assertThat(response.getResponses()[2].getId(), equalTo("2"));
         assertThat(response.getResponses()[2].getIndex(), equalTo("test"));
         assertThat(response.getResponses()[2].getFailure(), nullValue());
diff --git a/core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java b/core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java
index a54be17..dd73e41 100644
--- a/core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java
+++ b/core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java
@@ -24,6 +24,8 @@ import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;
 import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse;
 import org.elasticsearch.action.admin.cluster.state.ClusterStateResponse;
+import org.elasticsearch.action.admin.indices.stats.IndicesStatsResponse;
+import org.elasticsearch.action.admin.indices.stats.ShardStats;
 import org.elasticsearch.action.get.GetResponse;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.index.IndexResponse;
@@ -36,6 +38,7 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.discovery.Discovery;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.shard.ShadowIndexShard;
+import org.elasticsearch.index.translog.TranslogStats;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.indices.recovery.RecoveryTarget;
 import org.elasticsearch.plugins.Plugin;
@@ -175,6 +178,7 @@ public class IndexWithShadowReplicasIT extends ESIntegTestCase {
         Settings idxSettings = Settings.builder()
                 .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
                 .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 2)
+                .put(IndexShard.INDEX_TRANSLOG_DISABLE_FLUSH, true)
                 .put(IndexMetaData.SETTING_DATA_PATH, dataPath.toAbsolutePath().toString())
                 .put(IndexMetaData.SETTING_SHADOW_REPLICAS, true)
                 .put(IndexMetaData.SETTING_SHARED_FILESYSTEM, true)
@@ -188,6 +192,21 @@ public class IndexWithShadowReplicasIT extends ESIntegTestCase {
         client().prepareIndex(IDX, "doc", "1").setSource("foo", "bar").get();
         client().prepareIndex(IDX, "doc", "2").setSource("foo", "bar").get();
 
+        IndicesStatsResponse indicesStatsResponse = client().admin().indices().prepareStats(IDX).clear().setTranslog(true).get();
+        assertEquals(2, indicesStatsResponse.getIndex(IDX).getPrimaries().getTranslog().estimatedNumberOfOperations());
+        assertEquals(2, indicesStatsResponse.getIndex(IDX).getTotal().getTranslog().estimatedNumberOfOperations());
+        for (IndicesService service : internalCluster().getInstances(IndicesService.class)) {
+            IndexService indexService = service.indexService(IDX);
+            if (indexService != null) {
+                IndexShard shard = indexService.getShard(0);
+                TranslogStats translogStats = shard.translogStats();
+                assertTrue(translogStats != null || shard instanceof ShadowIndexShard);
+                if (translogStats != null) {
+                    assertEquals(2, translogStats.estimatedNumberOfOperations());
+                }
+            }
+        }
+
         // Check that we can get doc 1 and 2, because we are doing realtime
         // gets and getting from the primary
         GetResponse gResp1 = client().prepareGet(IDX, "doc", "1").setRealtime(true).setFields("foo").get();
diff --git a/core/src/test/java/org/elasticsearch/index/VersionTypeTests.java b/core/src/test/java/org/elasticsearch/index/VersionTypeTests.java
index e4a97d2..3f7ea54 100644
--- a/core/src/test/java/org/elasticsearch/index/VersionTypeTests.java
+++ b/core/src/test/java/org/elasticsearch/index/VersionTypeTests.java
@@ -29,26 +29,31 @@ public class VersionTypeTests extends ESTestCase {
     @Test
     public void testInternalVersionConflict() throws Exception {
 
-        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(10, Versions.MATCH_ANY));
+        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(10, Versions.MATCH_ANY, randomBoolean()));
         assertFalse(VersionType.INTERNAL.isVersionConflictForReads(10, Versions.MATCH_ANY));
         // if we don't have a version in the index we accept everything
-        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_SET, 10));
+        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_SET, 10, randomBoolean()));
         assertFalse(VersionType.INTERNAL.isVersionConflictForReads(Versions.NOT_SET, 10));
-        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_SET, Versions.MATCH_ANY));
+        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_SET, Versions.MATCH_ANY, randomBoolean()));
         assertFalse(VersionType.INTERNAL.isVersionConflictForReads(Versions.NOT_SET, Versions.MATCH_ANY));
 
         // if we didn't find a version (but the index does support it), we don't like it unless MATCH_ANY
-        assertTrue(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, 10));
+        assertTrue(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, 10, randomBoolean()));
         assertTrue(VersionType.INTERNAL.isVersionConflictForReads(Versions.NOT_FOUND, 10));
-        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.MATCH_ANY));
+        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.MATCH_ANY, randomBoolean()));
         assertFalse(VersionType.INTERNAL.isVersionConflictForReads(Versions.NOT_FOUND, Versions.MATCH_ANY));
 
+        // deletes
+        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.MATCH_DELETED, true));
+        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(10, Versions.MATCH_DELETED, true));
+
+
         // and the stupid usual case
-        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(10, 10));
+        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(10, 10, randomBoolean()));
         assertFalse(VersionType.INTERNAL.isVersionConflictForReads(10, 10));
-        assertTrue(VersionType.INTERNAL.isVersionConflictForWrites(9, 10));
+        assertTrue(VersionType.INTERNAL.isVersionConflictForWrites(9, 10, randomBoolean()));
         assertTrue(VersionType.INTERNAL.isVersionConflictForReads(9, 10));
-        assertTrue(VersionType.INTERNAL.isVersionConflictForWrites(10, 9));
+        assertTrue(VersionType.INTERNAL.isVersionConflictForWrites(10, 9, randomBoolean()));
         assertTrue(VersionType.INTERNAL.isVersionConflictForReads(10, 9));
 
 // Old indexing code, dictating behavior
@@ -99,23 +104,23 @@ public class VersionTypeTests extends ESTestCase {
     @Test
     public void testExternalVersionConflict() throws Exception {
 
-        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, 10));
-        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(Versions.NOT_SET, 10));
+        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, 10, randomBoolean()));
+        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(Versions.NOT_SET, 10, randomBoolean()));
         // MATCH_ANY must throw an exception in the case of external version, as the version must be set! it used as the new value
-        assertTrue(VersionType.EXTERNAL.isVersionConflictForWrites(10, Versions.MATCH_ANY));
+        assertTrue(VersionType.EXTERNAL.isVersionConflictForWrites(10, Versions.MATCH_ANY, randomBoolean()));
 
         // if we didn't find a version (but the index does support it), we always accept
-        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.NOT_FOUND));
-        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, 10));
+        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.NOT_FOUND, randomBoolean()));
+        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, 10, randomBoolean()));
 
         assertTrue(VersionType.EXTERNAL.isVersionConflictForReads(Versions.NOT_FOUND, Versions.NOT_FOUND));
         assertTrue(VersionType.EXTERNAL.isVersionConflictForReads(Versions.NOT_FOUND, 10));
         assertFalse(VersionType.EXTERNAL.isVersionConflictForReads(Versions.NOT_FOUND, Versions.MATCH_ANY));
 
         // and the standard behavior
-        assertTrue(VersionType.EXTERNAL.isVersionConflictForWrites(10, 10));
-        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(9, 10));
-        assertTrue(VersionType.EXTERNAL.isVersionConflictForWrites(10, 9));
+        assertTrue(VersionType.EXTERNAL.isVersionConflictForWrites(10, 10, randomBoolean()));
+        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(9, 10, randomBoolean()));
+        assertTrue(VersionType.EXTERNAL.isVersionConflictForWrites(10, 9, randomBoolean()));
 
         assertFalse(VersionType.EXTERNAL.isVersionConflictForReads(10, 10));
         assertTrue(VersionType.EXTERNAL.isVersionConflictForReads(9, 10));
@@ -137,14 +142,14 @@ public class VersionTypeTests extends ESTestCase {
     @Test
     public void testExternalGTEVersionConflict() throws Exception {
 
-        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(Versions.NOT_FOUND, 10));
-        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(Versions.NOT_SET, 10));
+        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(Versions.NOT_FOUND, 10, randomBoolean()));
+        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(Versions.NOT_SET, 10, randomBoolean()));
         // MATCH_ANY must throw an exception in the case of external version, as the version must be set! it used as the new value
-        assertTrue(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(10, Versions.MATCH_ANY));
+        assertTrue(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(10, Versions.MATCH_ANY, randomBoolean()));
 
         // if we didn't find a version (but the index does support it), we always accept
-        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.NOT_FOUND));
-        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(Versions.NOT_FOUND, 10));
+        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.NOT_FOUND, randomBoolean()));
+        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(Versions.NOT_FOUND, 10, randomBoolean()));
 
         assertTrue(VersionType.EXTERNAL_GTE.isVersionConflictForReads(Versions.NOT_FOUND, Versions.NOT_FOUND));
         assertTrue(VersionType.EXTERNAL_GTE.isVersionConflictForReads(Versions.NOT_FOUND, 10));
@@ -152,9 +157,9 @@ public class VersionTypeTests extends ESTestCase {
 
 
         // and the standard behavior
-        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(10, 10));
-        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(9, 10));
-        assertTrue(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(10, 9));
+        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(10, 10, randomBoolean()));
+        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(9, 10, randomBoolean()));
+        assertTrue(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(10, 9, randomBoolean()));
 
         assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForReads(10, 10));
         assertTrue(VersionType.EXTERNAL_GTE.isVersionConflictForReads(9, 10));
@@ -166,14 +171,20 @@ public class VersionTypeTests extends ESTestCase {
     @Test
     public void testForceVersionConflict() throws Exception {
 
-        assertFalse(VersionType.FORCE.isVersionConflictForWrites(Versions.NOT_FOUND, 10));
-        assertFalse(VersionType.FORCE.isVersionConflictForWrites(Versions.NOT_SET, 10));
-        // MATCH_ANY must throw an exception in the case of external version, as the version must be set! it used as the new value
-        assertTrue(VersionType.FORCE.isVersionConflictForWrites(10, Versions.MATCH_ANY));
+        assertFalse(VersionType.FORCE.isVersionConflictForWrites(Versions.NOT_FOUND, 10, randomBoolean()));
+        assertFalse(VersionType.FORCE.isVersionConflictForWrites(Versions.NOT_SET, 10, randomBoolean()));
+
+        // MATCH_ANY must throw an exception in the case of force version, as the version must be set! it used as the new value
+        try {
+            VersionType.FORCE.isVersionConflictForWrites(10, Versions.MATCH_ANY, randomBoolean());
+            fail();
+        } catch (IllegalStateException e) {
+            //yes!!
+        }
 
         // if we didn't find a version (but the index does support it), we always accept
-        assertFalse(VersionType.FORCE.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.NOT_FOUND));
-        assertFalse(VersionType.FORCE.isVersionConflictForWrites(Versions.NOT_FOUND, 10));
+        assertFalse(VersionType.FORCE.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.NOT_FOUND, randomBoolean()));
+        assertFalse(VersionType.FORCE.isVersionConflictForWrites(Versions.NOT_FOUND, 10, randomBoolean()));
 
         assertFalse(VersionType.FORCE.isVersionConflictForReads(Versions.NOT_FOUND, Versions.NOT_FOUND));
         assertFalse(VersionType.FORCE.isVersionConflictForReads(Versions.NOT_FOUND, 10));
@@ -181,9 +192,9 @@ public class VersionTypeTests extends ESTestCase {
 
 
         // and the standard behavior
-        assertFalse(VersionType.FORCE.isVersionConflictForWrites(10, 10));
-        assertFalse(VersionType.FORCE.isVersionConflictForWrites(9, 10));
-        assertFalse(VersionType.FORCE.isVersionConflictForWrites(10, 9));
+        assertFalse(VersionType.FORCE.isVersionConflictForWrites(10, 10, randomBoolean()));
+        assertFalse(VersionType.FORCE.isVersionConflictForWrites(9, 10, randomBoolean()));
+        assertFalse(VersionType.FORCE.isVersionConflictForWrites(10, 9, randomBoolean()));
         assertFalse(VersionType.FORCE.isVersionConflictForReads(10, 10));
         assertFalse(VersionType.FORCE.isVersionConflictForReads(9, 10));
         assertFalse(VersionType.FORCE.isVersionConflictForReads(10, 9));
diff --git a/core/src/test/java/org/elasticsearch/index/codec/postingformat/Elasticsearch090RWPostingsFormat.java b/core/src/test/java/org/elasticsearch/index/codec/postingformat/Elasticsearch090RWPostingsFormat.java
index a4285e6..984e151 100644
--- a/core/src/test/java/org/elasticsearch/index/codec/postingformat/Elasticsearch090RWPostingsFormat.java
+++ b/core/src/test/java/org/elasticsearch/index/codec/postingformat/Elasticsearch090RWPostingsFormat.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.index.codec.postingformat;
 
-import com.google.common.collect.Iterators;
 import org.apache.lucene.codecs.FieldsConsumer;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.index.Fields;
@@ -32,6 +31,7 @@ import org.elasticsearch.index.codec.postingsformat.Elasticsearch090PostingsForm
 import org.elasticsearch.index.mapper.internal.UidFieldMapper;
 
 import java.io.IOException;
+import java.util.Collections;
 import java.util.Iterator;
 import java.util.stream.StreamSupport;
 
@@ -61,7 +61,7 @@ public class Elasticsearch090RWPostingsFormat extends Elasticsearch090PostingsFo
                 maskedFields = new FilterLeafReader.FilterFields(fields) {
                     @Override
                     public Iterator<String> iterator() {
-                        return Iterators.singletonIterator(UidFieldMapper.NAME);
+                        return Collections.singleton(UidFieldMapper.NAME).iterator();
                     }
                 };
                 // only go through bloom for the UID field
diff --git a/core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java b/core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java
index 4b7de9b..3dcb548 100644
--- a/core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java
+++ b/core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java
@@ -68,7 +68,7 @@ import org.elasticsearch.index.mapper.internal.SourceFieldMapper;
 import org.elasticsearch.index.mapper.internal.UidFieldMapper;
 import org.elasticsearch.index.mapper.object.RootObjectMapper;
 import org.elasticsearch.index.shard.*;
-import org.elasticsearch.index.similarity.SimilarityLookupService;
+import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.index.store.DirectoryService;
 import org.elasticsearch.index.store.DirectoryUtils;
 import org.elasticsearch.index.store.Store;
@@ -96,7 +96,6 @@ import java.util.concurrent.CyclicBarrier;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicReference;
-import java.util.regex.Pattern;
 
 import static org.elasticsearch.common.settings.Settings.Builder.EMPTY_SETTINGS;
 import static org.elasticsearch.index.engine.Engine.Operation.Origin.PRIMARY;
@@ -105,8 +104,6 @@ import static org.hamcrest.Matchers.*;
 
 public class InternalEngineTests extends ESTestCase {
 
-    private static final Pattern PARSE_LEGACY_ID_PATTERN = Pattern.compile("^" + Translog.TRANSLOG_FILE_PREFIX + "(\\d+)((\\.recovering))?$");
-
     protected final ShardId shardId = new ShardId(new Index("index"), 1);
 
     protected ThreadPool threadPool;
@@ -273,10 +270,10 @@ public class InternalEngineTests extends ESTestCase {
 
             // create a doc and refresh
             ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-            engine.create(new Engine.Create(newUid("1"), doc));
+            engine.index(new Engine.Index(newUid("1"), doc));
 
             ParsedDocument doc2 = testParsedDocument("2", "2", "test", null, -1, -1, testDocumentWithTextField(), B_2, null);
-            engine.create(new Engine.Create(newUid("2"), doc2));
+            engine.index(new Engine.Index(newUid("2"), doc2));
             engine.refresh("test");
 
             segments = engine.segments(false);
@@ -310,7 +307,7 @@ public class InternalEngineTests extends ESTestCase {
             engine.onSettingsChanged();
 
             ParsedDocument doc3 = testParsedDocument("3", "3", "test", null, -1, -1, testDocumentWithTextField(), B_3, null);
-            engine.create(new Engine.Create(newUid("3"), doc3));
+            engine.index(new Engine.Index(newUid("3"), doc3));
             engine.refresh("test");
 
             segments = engine.segments(false);
@@ -358,7 +355,7 @@ public class InternalEngineTests extends ESTestCase {
             engine.config().setCompoundOnFlush(true);
             engine.onSettingsChanged();
             ParsedDocument doc4 = testParsedDocument("4", "4", "test", null, -1, -1, testDocumentWithTextField(), B_3, null);
-            engine.create(new Engine.Create(newUid("4"), doc4));
+            engine.index(new Engine.Index(newUid("4"), doc4));
             engine.refresh("test");
 
             segments = engine.segments(false);
@@ -392,7 +389,7 @@ public class InternalEngineTests extends ESTestCase {
             assertThat(segments.isEmpty(), equalTo(true));
 
             ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-            engine.create(new Engine.Create(newUid("1"), doc));
+            engine.index(new Engine.Index(newUid("1"), doc));
             engine.refresh("test");
 
             segments = engine.segments(true);
@@ -400,10 +397,10 @@ public class InternalEngineTests extends ESTestCase {
             assertThat(segments.get(0).ramTree, notNullValue());
 
             ParsedDocument doc2 = testParsedDocument("2", "2", "test", null, -1, -1, testDocumentWithTextField(), B_2, null);
-            engine.create(new Engine.Create(newUid("2"), doc2));
+            engine.index(new Engine.Index(newUid("2"), doc2));
             engine.refresh("test");
             ParsedDocument doc3 = testParsedDocument("3", "3", "test", null, -1, -1, testDocumentWithTextField(), B_3, null);
-            engine.create(new Engine.Create(newUid("3"), doc3));
+            engine.index(new Engine.Index(newUid("3"), doc3));
             engine.refresh("test");
 
             segments = engine.segments(true);
@@ -473,7 +470,7 @@ public class InternalEngineTests extends ESTestCase {
         Document document = testDocumentWithTextField();
         document.add(new Field(SourceFieldMapper.NAME, B_1.toBytes(), SourceFieldMapper.Defaults.FIELD_TYPE));
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, document, B_1, null);
-        engine.create(new Engine.Create(newUid("1"), doc));
+        engine.index(new Engine.Index(newUid("1"), doc));
 
         CommitStats stats1 = engine.commitStats();
         assertThat(stats1.getGeneration(), greaterThan(0l));
@@ -524,7 +521,7 @@ public class InternalEngineTests extends ESTestCase {
     /* */
     public void testConcurrentGetAndFlush() throws Exception {
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        engine.create(new Engine.Create(newUid("1"), doc));
+        engine.index(new Engine.Index(newUid("1"), doc));
 
         final AtomicReference<Engine.GetResult> latestGetResult = new AtomicReference<>();
         latestGetResult.set(engine.get(new Engine.Get(true, newUid("1"))));
@@ -569,7 +566,7 @@ public class InternalEngineTests extends ESTestCase {
         Document document = testDocumentWithTextField();
         document.add(new Field(SourceFieldMapper.NAME, B_1.toBytes(), SourceFieldMapper.Defaults.FIELD_TYPE));
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, document, B_1, null);
-        engine.create(new Engine.Create(newUid("1"), doc));
+        engine.index(new Engine.Index(newUid("1"), doc));
 
         // its not there...
         searchResult = engine.acquireSearcher("test");
@@ -661,7 +658,7 @@ public class InternalEngineTests extends ESTestCase {
         document = testDocumentWithTextField();
         document.add(new Field(SourceFieldMapper.NAME, B_1.toBytes(), SourceFieldMapper.Defaults.FIELD_TYPE));
         doc = testParsedDocument("1", "1", "test", null, -1, -1, document, B_1, null);
-        engine.create(new Engine.Create(newUid("1"), doc));
+        engine.index(new Engine.Index(newUid("1"), doc, Versions.MATCH_DELETED));
 
         // its not there...
         searchResult = engine.acquireSearcher("test");
@@ -722,7 +719,7 @@ public class InternalEngineTests extends ESTestCase {
 
         // create a document
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        engine.create(new Engine.Create(newUid("1"), doc));
+        engine.index(new Engine.Index(newUid("1"), doc));
 
         // its not there...
         searchResult = engine.acquireSearcher("test");
@@ -758,7 +755,7 @@ public class InternalEngineTests extends ESTestCase {
                      new LogByteSizeMergePolicy()), false)) {
             final String syncId = randomUnicodeOfCodepointLengthBetween(10, 20);
             ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-            engine.create(new Engine.Create(newUid("1"), doc));
+            engine.index(new Engine.Index(newUid("1"), doc));
             Engine.CommitId commitID = engine.flush();
             assertThat(commitID, equalTo(new Engine.CommitId(store.readLastCommittedSegmentsInfo().getId())));
             byte[] wrongBytes = Base64.decode(commitID.toString());
@@ -766,7 +763,7 @@ public class InternalEngineTests extends ESTestCase {
             Engine.CommitId wrongId = new Engine.CommitId(wrongBytes);
             assertEquals("should fail to sync flush with wrong id (but no docs)", engine.syncFlush(syncId + "1", wrongId),
                     Engine.SyncedFlushResult.COMMIT_MISMATCH);
-            engine.create(new Engine.Create(newUid("2"), doc));
+            engine.index(new Engine.Index(newUid("2"), doc));
             assertEquals("should fail to sync flush with right id but pending doc", engine.syncFlush(syncId + "2", commitID),
                     Engine.SyncedFlushResult.PENDING_OPERATIONS);
             commitID = engine.flush();
@@ -780,7 +777,7 @@ public class InternalEngineTests extends ESTestCase {
     public void testSycnedFlushSurvivesEngineRestart() throws IOException {
         final String syncId = randomUnicodeOfCodepointLengthBetween(10, 20);
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        engine.create(new Engine.Create(newUid("1"), doc));
+        engine.index(new Engine.Index(newUid("1"), doc));
         final Engine.CommitId commitID = engine.flush();
         assertEquals("should succeed to flush commit with right id and no pending doc", engine.syncFlush(syncId, commitID),
                 Engine.SyncedFlushResult.SUCCESS);
@@ -799,14 +796,14 @@ public class InternalEngineTests extends ESTestCase {
     public void testSycnedFlushVanishesOnReplay() throws IOException {
         final String syncId = randomUnicodeOfCodepointLengthBetween(10, 20);
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        engine.create(new Engine.Create(newUid("1"), doc));
+        engine.index(new Engine.Index(newUid("1"), doc));
         final Engine.CommitId commitID = engine.flush();
         assertEquals("should succeed to flush commit with right id and no pending doc", engine.syncFlush(syncId, commitID),
                 Engine.SyncedFlushResult.SUCCESS);
         assertEquals(store.readLastCommittedSegmentsInfo().getUserData().get(Engine.SYNC_COMMIT_ID), syncId);
         assertEquals(engine.getLastCommittedSegmentInfos().getUserData().get(Engine.SYNC_COMMIT_ID), syncId);
         doc = testParsedDocument("2", "2", "test", null, -1, -1, testDocumentWithTextField(), new BytesArray("{}"), null);
-        engine.create(new Engine.Create(newUid("2"), doc));
+        engine.index(new Engine.Index(newUid("2"), doc));
         EngineConfig config = engine.config();
         engine.close();
         final MockDirectoryWrapper directory = DirectoryUtils.getLeaf(store.directory(), MockDirectoryWrapper.class);
@@ -823,28 +820,16 @@ public class InternalEngineTests extends ESTestCase {
     @Test
     public void testVersioningNewCreate() {
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocument(), B_1, null);
-        Engine.Create create = new Engine.Create(newUid("1"), doc);
-        engine.create(create);
+        Engine.Index create = new Engine.Index(newUid("1"), doc, Versions.MATCH_DELETED);
+        engine.index(create);
         assertThat(create.version(), equalTo(1l));
 
-        create = new Engine.Create(newUid("1"), doc, create.version(), create.versionType().versionTypeForReplicationAndRecovery(), REPLICA, 0);
-        replicaEngine.create(create);
+        create = new Engine.Index(newUid("1"), doc, create.version(), create.versionType().versionTypeForReplicationAndRecovery(), REPLICA, 0);
+        replicaEngine.index(create);
         assertThat(create.version(), equalTo(1l));
     }
 
     @Test
-    public void testExternalVersioningNewCreate() {
-        ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocument(), B_1, null);
-        Engine.Create create = new Engine.Create(newUid("1"), doc, 12, VersionType.EXTERNAL, Engine.Operation.Origin.PRIMARY, 0);
-        engine.create(create);
-        assertThat(create.version(), equalTo(12l));
-
-        create = new Engine.Create(newUid("1"), doc, create.version(), create.versionType().versionTypeForReplicationAndRecovery(), REPLICA, 0);
-        replicaEngine.create(create);
-        assertThat(create.version(), equalTo(12l));
-    }
-
-    @Test
     public void testVersioningNewIndex() {
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocument(), B_1, null);
         Engine.Index index = new Engine.Index(newUid("1"), doc);
@@ -1107,9 +1092,9 @@ public class InternalEngineTests extends ESTestCase {
         }
 
         // we shouldn't be able to create as well
-        Engine.Create create = new Engine.Create(newUid("1"), doc, 2l, VersionType.INTERNAL, PRIMARY, 0);
+        Engine.Index create = new Engine.Index(newUid("1"), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, 0);
         try {
-            engine.create(create);
+            engine.index(create);
         } catch (VersionConflictEngineException e) {
             // all is well
         }
@@ -1164,9 +1149,9 @@ public class InternalEngineTests extends ESTestCase {
         }
 
         // we shouldn't be able to create as well
-        Engine.Create create = new Engine.Create(newUid("1"), doc, 2l, VersionType.INTERNAL, PRIMARY, 0);
+        Engine.Index create = new Engine.Index(newUid("1"), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, 0);
         try {
-            engine.create(create);
+            engine.index(create);
         } catch (VersionConflictEngineException e) {
             // all is well
         }
@@ -1175,15 +1160,15 @@ public class InternalEngineTests extends ESTestCase {
     @Test
     public void testVersioningCreateExistsException() {
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocument(), B_1, null);
-        Engine.Create create = new Engine.Create(newUid("1"), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, 0);
-        engine.create(create);
+        Engine.Index create = new Engine.Index(newUid("1"), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, 0);
+        engine.index(create);
         assertThat(create.version(), equalTo(1l));
 
-        create = new Engine.Create(newUid("1"), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, 0);
+        create = new Engine.Index(newUid("1"), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, 0);
         try {
-            engine.create(create);
+            engine.index(create);
             fail();
-        } catch (DocumentAlreadyExistsException e) {
+        } catch (VersionConflictEngineException e) {
             // all is well
         }
     }
@@ -1191,17 +1176,17 @@ public class InternalEngineTests extends ESTestCase {
     @Test
     public void testVersioningCreateExistsExceptionWithFlush() {
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocument(), B_1, null);
-        Engine.Create create = new Engine.Create(newUid("1"), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, 0);
-        engine.create(create);
+        Engine.Index create = new Engine.Index(newUid("1"), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, 0);
+        engine.index(create);
         assertThat(create.version(), equalTo(1l));
 
         engine.flush();
 
-        create = new Engine.Create(newUid("1"), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, 0);
+        create = new Engine.Index(newUid("1"), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, 0);
         try {
-            engine.create(create);
+            engine.index(create);
             fail();
-        } catch (DocumentAlreadyExistsException e) {
+        } catch (VersionConflictEngineException e) {
             // all is well
         }
     }
@@ -1365,13 +1350,13 @@ public class InternalEngineTests extends ESTestCase {
         try {
             // First, with DEBUG, which should NOT log IndexWriter output:
             ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-            engine.create(new Engine.Create(newUid("1"), doc));
+            engine.index(new Engine.Index(newUid("1"), doc));
             engine.flush();
             assertFalse(mockAppender.sawIndexWriterMessage);
 
             // Again, with TRACE, which should log IndexWriter output:
             rootLogger.setLevel(Level.TRACE);
-            engine.create(new Engine.Create(newUid("2"), doc));
+            engine.index(new Engine.Index(newUid("2"), doc));
             engine.flush();
             assertTrue(mockAppender.sawIndexWriterMessage);
 
@@ -1400,14 +1385,14 @@ public class InternalEngineTests extends ESTestCase {
         try {
             // First, with DEBUG, which should NOT log IndexWriter output:
             ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-            engine.create(new Engine.Create(newUid("1"), doc));
+            engine.index(new Engine.Index(newUid("1"), doc));
             engine.flush();
             assertFalse(mockAppender.sawIndexWriterMessage);
             assertFalse(mockAppender.sawIndexWriterIFDMessage);
 
             // Again, with TRACE, which should only log IndexWriter IFD output:
             iwIFDLogger.setLevel(Level.TRACE);
-            engine.create(new Engine.Create(newUid("2"), doc));
+            engine.index(new Engine.Index(newUid("2"), doc));
             engine.flush();
             assertFalse(mockAppender.sawIndexWriterMessage);
             assertTrue(mockAppender.sawIndexWriterIFDMessage);
@@ -1607,8 +1592,8 @@ public class InternalEngineTests extends ESTestCase {
         final int numDocs = randomIntBetween(1, 10);
         for (int i = 0; i < numDocs; i++) {
             ParsedDocument doc = testParsedDocument(Integer.toString(i), Integer.toString(i), "test", null, -1, -1, testDocument(), new BytesArray("{}"), null);
-            Engine.Create firstIndexRequest = new Engine.Create(newUid(Integer.toString(i)), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, System.nanoTime());
-            engine.create(firstIndexRequest);
+            Engine.Index firstIndexRequest = new Engine.Index(newUid(Integer.toString(i)), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, System.nanoTime());
+            engine.index(firstIndexRequest);
             assertThat(firstIndexRequest.version(), equalTo(1l));
         }
         engine.refresh("test");
@@ -1660,8 +1645,8 @@ public class InternalEngineTests extends ESTestCase {
         final int numDocs = randomIntBetween(1, 10);
         for (int i = 0; i < numDocs; i++) {
             ParsedDocument doc = testParsedDocument(Integer.toString(i), Integer.toString(i), "test", null, -1, -1, testDocument(), new BytesArray("{}"), null);
-            Engine.Create firstIndexRequest = new Engine.Create(newUid(Integer.toString(i)), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, System.nanoTime());
-            engine.create(firstIndexRequest);
+            Engine.Index firstIndexRequest = new Engine.Index(newUid(Integer.toString(i)), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, System.nanoTime());
+            engine.index(firstIndexRequest);
             assertThat(firstIndexRequest.version(), equalTo(1l));
         }
         engine.refresh("test");
@@ -1761,8 +1746,8 @@ public class InternalEngineTests extends ESTestCase {
                 final int numExtraDocs = randomIntBetween(1, 10);
                 for (int i = 0; i < numExtraDocs; i++) {
                     ParsedDocument doc = testParsedDocument("extra" + Integer.toString(i), "extra" + Integer.toString(i), "test", null, -1, -1, testDocument(), new BytesArray("{}"), null);
-                    Engine.Create firstIndexRequest = new Engine.Create(newUid(Integer.toString(i)), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, System.nanoTime());
-                    engine.create(firstIndexRequest);
+                    Engine.Index firstIndexRequest = new Engine.Index(newUid(Integer.toString(i)), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, System.nanoTime());
+                    engine.index(firstIndexRequest);
                     assertThat(firstIndexRequest.version(), equalTo(1l));
                 }
                 engine.refresh("test");
@@ -1790,8 +1775,8 @@ public class InternalEngineTests extends ESTestCase {
         final int numDocs = randomIntBetween(1, 10);
         for (int i = 0; i < numDocs; i++) {
             ParsedDocument doc = testParsedDocument(Integer.toString(i), Integer.toString(i), "test", null, -1, -1, testDocument(), new BytesArray("{}"), null);
-            Engine.Create firstIndexRequest = new Engine.Create(newUid(Integer.toString(i)), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, System.nanoTime());
-            engine.create(firstIndexRequest);
+            Engine.Index firstIndexRequest = new Engine.Index(newUid(Integer.toString(i)), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, System.nanoTime());
+            engine.index(firstIndexRequest);
             assertThat(firstIndexRequest.version(), equalTo(1l));
         }
         engine.refresh("test");
@@ -1839,8 +1824,8 @@ public class InternalEngineTests extends ESTestCase {
         int randomId = randomIntBetween(numDocs + 1, numDocs + 10);
         String uuidValue = "test#" + Integer.toString(randomId);
         ParsedDocument doc = testParsedDocument(uuidValue, Integer.toString(randomId), "test", null, -1, -1, testDocument(), new BytesArray("{}"), null);
-        Engine.Create firstIndexRequest = new Engine.Create(newUid(uuidValue), doc, 1, VersionType.EXTERNAL, PRIMARY, System.nanoTime());
-        engine.create(firstIndexRequest);
+        Engine.Index firstIndexRequest = new Engine.Index(newUid(uuidValue), doc, 1, VersionType.EXTERNAL, PRIMARY, System.nanoTime());
+        engine.index(firstIndexRequest);
         assertThat(firstIndexRequest.version(), equalTo(1l));
         if (flush) {
             engine.flush();
@@ -1890,10 +1875,10 @@ public class InternalEngineTests extends ESTestCase {
             RootObjectMapper.Builder rootBuilder = new RootObjectMapper.Builder("test");
             Index index = new Index(indexName);
             AnalysisService analysisService = new AnalysisService(index, settings);
-            SimilarityLookupService similarityLookupService = new SimilarityLookupService(index, settings);
-            MapperService mapperService = new MapperService(index, settings, analysisService, similarityLookupService, null);
+            SimilarityService similarityService = new SimilarityService(index, settings);
+            MapperService mapperService = new MapperService(index, settings, analysisService, similarityService, null);
             DocumentMapper.Builder b = new DocumentMapper.Builder(settings, rootBuilder, mapperService);
-            DocumentMapperParser parser = new DocumentMapperParser(settings, mapperService, analysisService, similarityLookupService, null);
+            DocumentMapperParser parser = new DocumentMapperParser(settings, mapperService, analysisService, similarityService, null);
             this.docMapper = b.build(mapperService, parser);
 
         }
@@ -1920,8 +1905,8 @@ public class InternalEngineTests extends ESTestCase {
         final int numDocs = randomIntBetween(1, 10);
         for (int i = 0; i < numDocs; i++) {
             ParsedDocument doc = testParsedDocument(Integer.toString(i), Integer.toString(i), "test", null, -1, -1, testDocument(), new BytesArray("{}"), null);
-            Engine.Create firstIndexRequest = new Engine.Create(newUid(Integer.toString(i)), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, System.nanoTime());
-            engine.create(firstIndexRequest);
+            Engine.Index firstIndexRequest = new Engine.Index(newUid(Integer.toString(i)), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, System.nanoTime());
+            engine.index(firstIndexRequest);
             assertThat(firstIndexRequest.version(), equalTo(1l));
         }
         engine.refresh("test");
@@ -1939,7 +1924,7 @@ public class InternalEngineTests extends ESTestCase {
         engine.close();
 
         Translog translog = new Translog(new TranslogConfig(shardId, createTempDir(), Settings.EMPTY, Translog.Durabilty.REQUEST, BigArrays.NON_RECYCLING_INSTANCE, threadPool));
-        translog.add(new Translog.Create("test", "SomeBogusId", "{}".getBytes(Charset.forName("UTF-8"))));
+        translog.add(new Translog.Index("test", "SomeBogusId", "{}".getBytes(Charset.forName("UTF-8"))));
         assertEquals(generation.translogFileGeneration, translog.currentFileGeneration());
         translog.close();
 
diff --git a/core/src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java b/core/src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java
index b5987a9..2c6ee40 100644
--- a/core/src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java
+++ b/core/src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java
@@ -236,7 +236,7 @@ public class ShadowEngineTests extends ESTestCase {
     public void testCommitStats() {
         // create a doc and refresh
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
 
         CommitStats stats1 = replicaEngine.commitStats();
         assertThat(stats1.getGeneration(), greaterThan(0l));
@@ -271,10 +271,10 @@ public class ShadowEngineTests extends ESTestCase {
 
         // create a doc and refresh
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
 
         ParsedDocument doc2 = testParsedDocument("2", "2", "test", null, -1, -1, testDocumentWithTextField(), B_2, null);
-        primaryEngine.create(new Engine.Create(newUid("2"), doc2));
+        primaryEngine.index(new Engine.Index(newUid("2"), doc2));
         primaryEngine.refresh("test");
 
         segments = primaryEngine.segments(false);
@@ -334,7 +334,7 @@ public class ShadowEngineTests extends ESTestCase {
         primaryEngine.onSettingsChanged();
 
         ParsedDocument doc3 = testParsedDocument("3", "3", "test", null, -1, -1, testDocumentWithTextField(), B_3, null);
-        primaryEngine.create(new Engine.Create(newUid("3"), doc3));
+        primaryEngine.index(new Engine.Index(newUid("3"), doc3));
         primaryEngine.refresh("test");
 
         segments = primaryEngine.segments(false);
@@ -407,7 +407,7 @@ public class ShadowEngineTests extends ESTestCase {
         primaryEngine.onSettingsChanged();
 
         ParsedDocument doc4 = testParsedDocument("4", "4", "test", null, -1, -1, testDocumentWithTextField(), B_3, null);
-        primaryEngine.create(new Engine.Create(newUid("4"), doc4));
+        primaryEngine.index(new Engine.Index(newUid("4"), doc4));
         primaryEngine.refresh("test");
 
         segments = primaryEngine.segments(false);
@@ -441,7 +441,7 @@ public class ShadowEngineTests extends ESTestCase {
         assertThat(segments.isEmpty(), equalTo(true));
 
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
         primaryEngine.refresh("test");
 
         segments = primaryEngine.segments(true);
@@ -449,10 +449,10 @@ public class ShadowEngineTests extends ESTestCase {
         assertThat(segments.get(0).ramTree, notNullValue());
 
         ParsedDocument doc2 = testParsedDocument("2", "2", "test", null, -1, -1, testDocumentWithTextField(), B_2, null);
-        primaryEngine.create(new Engine.Create(newUid("2"), doc2));
+        primaryEngine.index(new Engine.Index(newUid("2"), doc2));
         primaryEngine.refresh("test");
         ParsedDocument doc3 = testParsedDocument("3", "3", "test", null, -1, -1, testDocumentWithTextField(), B_3, null);
-        primaryEngine.create(new Engine.Create(newUid("3"), doc3));
+        primaryEngine.index(new Engine.Index(newUid("3"), doc3));
         primaryEngine.refresh("test");
 
         segments = primaryEngine.segments(true);
@@ -480,7 +480,7 @@ public class ShadowEngineTests extends ESTestCase {
         document.add(new Field(SourceFieldMapper.NAME, B_1.toBytes(), SourceFieldMapper.Defaults.FIELD_TYPE));
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, document, B_1, null);
         try {
-            replicaEngine.create(new Engine.Create(newUid("1"), doc));
+            replicaEngine.index(new Engine.Index(newUid("1"), doc));
             fail("should have thrown an exception");
         } catch (UnsupportedOperationException e) {}
         replicaEngine.refresh("test");
@@ -517,7 +517,7 @@ public class ShadowEngineTests extends ESTestCase {
         document = testDocumentWithTextField();
         document.add(new Field(SourceFieldMapper.NAME, B_1.toBytes(), SourceFieldMapper.Defaults.FIELD_TYPE));
         doc = testParsedDocument("1", "1", "test", null, -1, -1, document, B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
         primaryEngine.flush();
         replicaEngine.refresh("test");
 
@@ -573,7 +573,7 @@ public class ShadowEngineTests extends ESTestCase {
         ParseContext.Document document = testDocumentWithTextField();
         document.add(new Field(SourceFieldMapper.NAME, B_1.toBytes(), SourceFieldMapper.Defaults.FIELD_TYPE));
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, document, B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
 
         // its not there...
         searchResult = primaryEngine.acquireSearcher("test");
@@ -700,7 +700,7 @@ public class ShadowEngineTests extends ESTestCase {
         document = testDocumentWithTextField();
         document.add(new Field(SourceFieldMapper.NAME, B_1.toBytes(), SourceFieldMapper.Defaults.FIELD_TYPE));
         doc = testParsedDocument("1", "1", "test", null, -1, -1, document, B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
 
         // its not there...
         searchResult = primaryEngine.acquireSearcher("test");
@@ -784,7 +784,7 @@ public class ShadowEngineTests extends ESTestCase {
 
         // create a document
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
 
         // its not there...
         searchResult = primaryEngine.acquireSearcher("test");
@@ -830,7 +830,7 @@ public class ShadowEngineTests extends ESTestCase {
     @Test
     public void testFailEngineOnCorruption() {
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
         primaryEngine.flush();
         MockDirectoryWrapper leaf = DirectoryUtils.getLeaf(replicaEngine.config().getStore().directory(), MockDirectoryWrapper.class);
         leaf.setRandomIOExceptionRate(1.0);
@@ -869,7 +869,7 @@ public class ShadowEngineTests extends ESTestCase {
     public void testFailStart() throws IOException {
         // Need a commit point for this
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
         primaryEngine.flush();
 
         // this test fails if any reader, searcher or directory is not closed - MDW FTW
@@ -957,7 +957,7 @@ public class ShadowEngineTests extends ESTestCase {
         ParseContext.Document document = testDocumentWithTextField();
         document.add(new Field(SourceFieldMapper.NAME, B_1.toBytes(), SourceFieldMapper.Defaults.FIELD_TYPE));
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, document, B_1, null);
-        pEngine.create(new Engine.Create(newUid("1"), doc));
+        pEngine.index(new Engine.Index(newUid("1"), doc));
         pEngine.flush(true, true);
 
         t.join();
@@ -965,4 +965,13 @@ public class ShadowEngineTests extends ESTestCase {
         // (shadow engine is already shut down in the try-with-resources)
         IOUtils.close(srStore, pEngine, pStore);
     }
+
+    public void testNoTranslog() {
+        try {
+            replicaEngine.getTranslog();
+            fail("shadow engine has no translog");
+        } catch (UnsupportedOperationException ex) {
+            // all good
+        }
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java
index f989467..ce71635 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java
@@ -197,7 +197,7 @@ public class DynamicMappingTests extends ESSingleNodeTestCase {
         ctx.reset(XContentHelper.createParser(source.source()), new ParseContext.Document(), source);
         assertEquals(XContentParser.Token.START_OBJECT, ctx.parser().nextToken());
         ctx.parser().nextToken();
-        return DocumentParser.parseObject(ctx, mapper.root());
+        return DocumentParser.parseObject(ctx, mapper.root(), true);
     }
 
     public void testDynamicMappingsNotNeeded() throws Exception {
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/FieldTypeLookupTests.java b/core/src/test/java/org/elasticsearch/index/mapper/FieldTypeLookupTests.java
index 5e3b61a..6ab4ca3 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/FieldTypeLookupTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/FieldTypeLookupTests.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.index.mapper;
 
-import com.google.common.collect.Iterators;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.settings.Settings;
@@ -30,7 +29,6 @@ import java.util.Arrays;
 import java.util.Collection;
 import java.util.Iterator;
 import java.util.List;
-import java.util.stream.StreamSupport;
 
 public class FieldTypeLookupTests extends ESTestCase {
 
@@ -61,7 +59,7 @@ public class FieldTypeLookupTests extends ESTestCase {
         assertNull(lookup.get("bar"));
         assertEquals(f.fieldType(), lookup2.getByIndexName("bar"));
         assertNull(lookup.getByIndexName("foo"));
-        assertEquals(1, Iterators.size(lookup2.iterator()));
+        assertEquals(1, size(lookup2.iterator()));
     }
 
     public void testAddExistingField() {
@@ -76,7 +74,7 @@ public class FieldTypeLookupTests extends ESTestCase {
         assertSame(f.fieldType(), f2.fieldType());
         assertSame(f.fieldType(), lookup2.get("foo"));
         assertSame(f.fieldType(), lookup2.getByIndexName("foo"));
-        assertEquals(1, Iterators.size(lookup2.iterator()));
+        assertEquals(1, size(lookup2.iterator()));
     }
 
     public void testAddExistingIndexName() {
@@ -92,7 +90,7 @@ public class FieldTypeLookupTests extends ESTestCase {
         assertSame(f.fieldType(), lookup2.get("foo"));
         assertSame(f.fieldType(), lookup2.get("bar"));
         assertSame(f.fieldType(), lookup2.getByIndexName("foo"));
-        assertEquals(2, Iterators.size(lookup2.iterator()));
+        assertEquals(2, size(lookup2.iterator()));
     }
 
     public void testAddExistingFullName() {
@@ -108,7 +106,7 @@ public class FieldTypeLookupTests extends ESTestCase {
         assertSame(f.fieldType(), lookup2.get("foo"));
         assertSame(f.fieldType(), lookup2.getByIndexName("foo"));
         assertSame(f.fieldType(), lookup2.getByIndexName("bar"));
-        assertEquals(1, Iterators.size(lookup2.iterator()));
+        assertEquals(1, size(lookup2.iterator()));
     }
 
     public void testAddExistingBridgeName() {
@@ -286,4 +284,16 @@ public class FieldTypeLookupTests extends ESTestCase {
         @Override
         protected void parseCreateField(ParseContext context, List list) throws IOException {}
     }
+
+    private int size(Iterator<MappedFieldType> iterator) {
+        if (iterator == null) {
+            throw new NullPointerException("iterator");
+        }
+        int count = 0;
+        while (iterator.hasNext()) {
+            count++;
+            iterator.next();
+        }
+        return count;
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java b/core/src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java
index a7314c2..0e3a04a 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java
@@ -43,6 +43,7 @@ import org.elasticsearch.index.mapper.DocumentMapperParser;
 import org.elasticsearch.index.mapper.MapperParsingException;
 import org.elasticsearch.index.mapper.ParseContext.Document;
 import org.elasticsearch.index.mapper.ParsedDocument;
+import org.elasticsearch.index.mapper.SourceToParse;
 import org.elasticsearch.index.mapper.internal.AllFieldMapper;
 import org.elasticsearch.test.ESSingleNodeTestCase;
 import org.elasticsearch.index.mapper.internal.TimestampFieldMapper;
@@ -453,4 +454,17 @@ public class SimpleAllMapperTests extends ESSingleNodeTestCase {
         // the backcompat behavior is actually ignoring directly specifying _all
         assertFalse(field.getAllEntries().fields().iterator().hasNext());
     }
+
+    public void testIncludeInObjectNotAllowed() throws Exception {
+        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type").endObject().endObject().string();
+        DocumentMapper docMapper = createIndex("test").mapperService().documentMapperParser().parse(mapping);
+
+        try {
+            docMapper.parse("test", "type", "1", XContentFactory.jsonBuilder()
+                .startObject().field("_all", "foo").endObject().bytes());
+            fail("Expected failure to parse metadata field");
+        } catch (MapperParsingException e) {
+            assertTrue(e.getMessage(), e.getMessage().contains("Field [_all] is a metadata field and cannot be added inside a document"));
+        }
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapper.java b/core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapper.java
index e0c7a30..3d2134f 100755
--- a/core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapper.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapper.java
@@ -19,10 +19,10 @@
 
 package org.elasticsearch.index.mapper.externalvalues;
 
-import com.google.common.collect.Iterators;
 import com.spatial4j.core.shape.Point;
 import org.apache.lucene.document.Field;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.collect.Iterators;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.geo.builders.ShapeBuilder;
 import org.elasticsearch.common.settings.Settings;
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/id/IdMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/id/IdMappingTests.java
index 2688674..679b49e 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/id/IdMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/id/IdMappingTests.java
@@ -114,4 +114,17 @@ public class IdMappingTests extends ESSingleNodeTestCase {
         // _id is not indexed so we need to check _uid
         assertEquals(Uid.createUid("type", "1"), doc.rootDoc().get(UidFieldMapper.NAME));
     }
+
+    public void testIncludeInObjectNotAllowed() throws Exception {
+        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type").endObject().endObject().string();
+        DocumentMapper docMapper = createIndex("test").mapperService().documentMapperParser().parse(mapping);
+
+        try {
+            docMapper.parse(SourceToParse.source(XContentFactory.jsonBuilder()
+                .startObject().field("_id", "1").endObject().bytes()).type("type"));
+            fail("Expected failure to parse metadata field");
+        } catch (MapperParsingException e) {
+            assertTrue(e.getMessage(), e.getMessage().contains("Field [_id] is a metadata field and cannot be added inside a document"));
+        }
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/parent/ParentMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/parent/ParentMappingTests.java
index bdfb0e4..3719500 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/parent/ParentMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/parent/ParentMappingTests.java
@@ -23,6 +23,7 @@ import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.mapper.DocumentMapper;
+import org.elasticsearch.index.mapper.MapperParsingException;
 import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.SourceToParse;
 import org.elasticsearch.index.mapper.Uid;
@@ -32,21 +33,18 @@ import static org.hamcrest.Matchers.nullValue;
 
 public class ParentMappingTests extends ESSingleNodeTestCase {
 
-    public void testParentNotSet() throws Exception {
+    public void testParentSetInDocNotAllowed() throws Exception {
         String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
                 .endObject().endObject().string();
         DocumentMapper docMapper = createIndex("test").mapperService().documentMapperParser().parse(mapping);
 
-        ParsedDocument doc = docMapper.parse(SourceToParse.source(XContentFactory.jsonBuilder()
-                .startObject()
-                .field("_parent", "1122")
-                .field("x_field", "x_value")
-                .endObject()
-                .bytes()).type("type").id("1"));
-
-        // no _parent mapping, dynamically used as a string field
-        assertNull(doc.parent());
-        assertNotNull(doc.rootDoc().get("_parent"));
+        try {
+            docMapper.parse(SourceToParse.source(XContentFactory.jsonBuilder()
+                .startObject().field("_parent", "1122").endObject().bytes()).type("type").id("1"));
+            fail("Expected failure to parse metadata field");
+        } catch (MapperParsingException e) {
+            assertTrue(e.getMessage(), e.getMessage().contains("Field [_parent] is a metadata field and cannot be added inside a document"));
+        }
     }
 
     public void testParentSetInDocBackcompat() throws Exception {
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/routing/RoutingTypeMapperTests.java b/core/src/test/java/org/elasticsearch/index/mapper/routing/RoutingTypeMapperTests.java
index 30fcb5f..7d0afdb 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/routing/RoutingTypeMapperTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/routing/RoutingTypeMapperTests.java
@@ -32,6 +32,7 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.json.JsonXContent;
 import org.elasticsearch.index.mapper.DocumentMapper;
+import org.elasticsearch.index.mapper.MapperParsingException;
 import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.SourceToParse;
 import org.elasticsearch.test.ESSingleNodeTestCase;
@@ -113,7 +114,7 @@ public class RoutingTypeMapperTests extends ESSingleNodeTestCase {
         Settings settings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.V_1_4_2.id).build();
         DocumentMapper docMapper = createIndex("test", settings).mapperService().documentMapperParser().parse(mapping);
 
-        XContentBuilder doc = XContentFactory.jsonBuilder().startObject().field("_timestamp", 2000000).endObject();
+        XContentBuilder doc = XContentFactory.jsonBuilder().startObject().field("_routing", "foo").endObject();
         MappingMetaData mappingMetaData = new MappingMetaData(docMapper);
         IndexRequest request = new IndexRequest("test", "type", "1").source(doc);
         request.process(MetaData.builder().build(), mappingMetaData, true, "test");
@@ -122,4 +123,17 @@ public class RoutingTypeMapperTests extends ESSingleNodeTestCase {
         assertNull(request.routing());
         assertNull(docMapper.parse("test", "type", "1", doc.bytes()).rootDoc().get("_routing"));
     }
+
+    public void testIncludeInObjectNotAllowed() throws Exception {
+        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type").endObject().endObject().string();
+        DocumentMapper docMapper = createIndex("test").mapperService().documentMapperParser().parse(mapping);
+
+        try {
+            docMapper.parse("test", "type", "1", XContentFactory.jsonBuilder()
+                .startObject().field("_routing", "foo").endObject().bytes());
+            fail("Expected failure to parse metadata field");
+        } catch (MapperParsingException e) {
+            assertTrue(e.getMessage(), e.getMessage().contains("Field [_routing] is a metadata field and cannot be added inside a document"));
+        }
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java
index 057dc41..e51b6a6 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java
@@ -769,6 +769,21 @@ public class TimestampMappingTests extends ESSingleNodeTestCase {
         assertNull(docMapper.parse("test", "type", "1", doc.bytes()).rootDoc().get("_timestamp"));
     }
 
+    public void testIncludeInObjectNotAllowed() throws Exception {
+        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
+            .startObject("_timestamp").field("enabled", true).field("default", "1970").field("format", "YYYY").endObject()
+            .endObject().endObject().string();
+        DocumentMapper docMapper = createIndex("test").mapperService().documentMapperParser().parse(mapping);
+
+        try {
+            docMapper.parse("test", "type", "1", XContentFactory.jsonBuilder()
+                .startObject().field("_timestamp", 2000000).endObject().bytes());
+            fail("Expected failure to parse metadata field");
+        } catch (MapperParsingException e) {
+            assertTrue(e.getMessage(), e.getMessage().contains("Field [_timestamp] is a metadata field and cannot be added inside a document"));
+        }
+    }
+
     public void testThatEpochCanBeIgnoredWithCustomFormat() throws Exception {
         String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
                 .startObject("_timestamp").field("enabled", true).field("format", "yyyyMMddHH").endObject()
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/ttl/TTLMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/ttl/TTLMappingTests.java
index c9b6131..b9f7a98 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/ttl/TTLMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/ttl/TTLMappingTests.java
@@ -310,6 +310,21 @@ public class TTLMappingTests extends ESSingleNodeTestCase {
         assertNull(docMapper.parse("test", "type", "1", doc.bytes()).rootDoc().get("_ttl"));
     }
 
+    public void testIncludeInObjectNotAllowed() throws Exception {
+        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
+            .startObject("_ttl").field("enabled", true).endObject()
+            .endObject().endObject().string();
+        DocumentMapper docMapper = createIndex("test").mapperService().documentMapperParser().parse(mapping);
+
+        try {
+            docMapper.parse("test", "type", "1", XContentFactory.jsonBuilder()
+                .startObject().field("_ttl", "2d").endObject().bytes());
+            fail("Expected failure to parse metadata field");
+        } catch (MapperParsingException e) {
+            assertTrue(e.getMessage(), e.getMessage().contains("Field [_ttl] is a metadata field and cannot be added inside a document"));
+        }
+    }
+
     private org.elasticsearch.common.xcontent.XContentBuilder getMappingWithTtlEnabled() throws IOException {
         return getMappingWithTtlEnabled(null);
     }
diff --git a/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java b/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
index 5837d5b..fce0a94 100644
--- a/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
+++ b/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
@@ -215,7 +215,7 @@ public abstract class AbstractQueryTestCase<QB extends AbstractQueryBuilder<QB>>
                 new IndexSettingsModule(index, indexSettings),
                 new IndexCacheModule(indexSettings),
                 new AnalysisModule(indexSettings, new IndicesAnalysisService(indexSettings)),
-                new SimilarityModule(indexSettings),
+                new SimilarityModule(index, indexSettings),
                 new IndexNameModule(index),
         new AbstractModule() {
                     @Override
diff --git a/core/src/test/java/org/elasticsearch/index/query/GeoDistanceRangeQueryTests.java b/core/src/test/java/org/elasticsearch/index/query/GeoDistanceRangeQueryTests.java
index 9dd1a55..19e48aa 100644
--- a/core/src/test/java/org/elasticsearch/index/query/GeoDistanceRangeQueryTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/GeoDistanceRangeQueryTests.java
@@ -22,6 +22,7 @@ package org.elasticsearch.index.query;
 import org.apache.lucene.search.Query;
 import org.elasticsearch.common.geo.GeoDistance;
 import org.elasticsearch.common.geo.GeoPoint;
+import org.elasticsearch.common.geo.GeoUtils;
 import org.elasticsearch.common.unit.DistanceUnit;
 import org.elasticsearch.index.search.geo.GeoDistanceRangeQuery;
 import org.junit.Test;
@@ -108,8 +109,12 @@ public class GeoDistanceRangeQueryTests extends AbstractQueryTestCase<GeoDistanc
         GeoDistanceRangeQuery geoQuery = (GeoDistanceRangeQuery) query;
         assertThat(geoQuery.fieldName(), equalTo(queryBuilder.fieldName()));
         if (queryBuilder.point() != null) {
-            assertThat(geoQuery.lat(), equalTo(queryBuilder.point().lat()));
-            assertThat(geoQuery.lon(), equalTo(queryBuilder.point().lon()));
+            GeoPoint expectedPoint = new GeoPoint(queryBuilder.point());
+            if (GeoValidationMethod.isCoerce(queryBuilder.getValidationMethod())) {
+                GeoUtils.normalizePoint(expectedPoint, true, true);
+            }
+            assertThat(geoQuery.lat(), equalTo(expectedPoint.lat()));
+            assertThat(geoQuery.lon(), equalTo(expectedPoint.lon()));
         }
         assertThat(geoQuery.geoDistance(), equalTo(queryBuilder.geoDistance()));
         if (queryBuilder.from() != null && queryBuilder.from() instanceof Number) {
diff --git a/core/src/test/java/org/elasticsearch/index/query/GeohashCellQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/GeohashCellQueryBuilderTests.java
index 022c2e1..0db757f 100644
--- a/core/src/test/java/org/elasticsearch/index/query/GeohashCellQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/GeohashCellQueryBuilderTests.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.index.query;
 
-import com.spatial4j.core.shape.Point;
-
 import org.apache.lucene.index.Term;
 import org.apache.lucene.queries.TermsQuery;
 import org.apache.lucene.search.Query;
@@ -29,7 +27,6 @@ import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.unit.DistanceUnit;
 import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
 import org.elasticsearch.index.query.GeohashCellQuery.Builder;
-import org.elasticsearch.test.geo.RandomShapeGenerator;
 import org.junit.Test;
 
 import java.io.IOException;
@@ -108,15 +105,4 @@ public class GeohashCellQueryBuilderTests extends AbstractQueryTestCase<Builder>
         builder.precision(-1);
     }
 
-    @Test
-    public void testLocationParsing() throws IOException {
-        Point point = RandomShapeGenerator.xRandomPoint(getRandom());
-        Builder pointTestBuilder = new GeohashCellQuery.Builder("pin", new GeoPoint(point.getY(), point.getX()));
-        String pointTest1 = "{\"geohash_cell\": {\"pin\": {\"lat\": " + point.getY() + ",\"lon\": " + point.getX() + "}}}";
-        assertParsedQuery(pointTest1, pointTestBuilder);
-        String pointTest2 = "{\"geohash_cell\": {\"pin\": \"" + point.getY() + "," + point.getX() + "\"}}";
-        assertParsedQuery(pointTest2, pointTestBuilder);
-        String pointTest3 = "{\"geohash_cell\": {\"pin\": [" + point.getX() + "," + point.getY() + "]}}";
-        assertParsedQuery(pointTest3, pointTestBuilder);
-    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTests.java
index 665b4b0..177caf0 100644
--- a/core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTests.java
@@ -137,10 +137,4 @@ public class IdsQueryBuilderTests extends AbstractQueryTestCase<IdsQueryBuilder>
             //all good
         }
     }
-
-    @Test(expected= ParsingException.class) // see #7686.
-    public void testIdsQueryWithInvalidValues() throws Exception {
-        String query = "{ \"ids\": { \"values\": [[1]] } }";
-        parseQuery(query);
-    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTests.java
index 2ca9441..5ae54d4 100644
--- a/core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTests.java
@@ -20,11 +20,7 @@
 package org.elasticsearch.index.query;
 
 import org.apache.lucene.index.Term;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.MatchNoDocsQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.*;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.common.xcontent.XContentFactory;
@@ -32,18 +28,9 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import org.junit.Test;
 
 import java.io.IOException;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.Locale;
-import java.util.Map;
-import java.util.Set;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.notNullValue;
+import java.util.*;
+
+import static org.hamcrest.Matchers.*;
 
 public class SimpleQueryStringBuilderTests extends AbstractQueryTestCase<SimpleQueryStringBuilder> {
 
@@ -340,14 +327,4 @@ public class SimpleQueryStringBuilderTests extends AbstractQueryTestCase<SimpleQ
         assertThat(query, instanceOf(TermQuery.class));
         assertThat(query.getBoost(), equalTo(10f));
     }
-
-    public void testNegativeFlags() throws IOException {
-        String query = "{\"simple_query_string\": {\"query\": \"foo bar\", \"flags\": -1}}";
-        SimpleQueryStringBuilder builder = new SimpleQueryStringBuilder("foo bar");
-        builder.flags(SimpleQueryStringFlag.ALL);
-        assertParsedQuery(query, builder);
-        SimpleQueryStringBuilder otherBuilder = new SimpleQueryStringBuilder("foo bar");
-        otherBuilder.flags(-1);
-        assertThat(builder, equalTo(otherBuilder));
-    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java
index 0c8151b..3d89633 100644
--- a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java
@@ -84,38 +84,7 @@ public class TemplateQueryBuilderTests extends AbstractQueryTestCase<TemplateQue
         builder.doXContent(content, null);
         content.endObject();
         content.close();
-        assertEquals("{\"template\":{\"inline\":\"I am a $template string\",\"lang\":\"mustache\",\"params\":{\"template\":\"filled\"}}}",
-                content.string());
-    }
-
-    @Test
-    public void testRawEscapedTemplate() throws IOException {
-        String expectedTemplateString = "{\"match_{{template}}\": {}}\"";
-        String query = "{\"template\": {\"query\": \"{\\\"match_{{template}}\\\": {}}\\\"\",\"params\" : {\"template\" : \"all\"}}}";
-        Map<String, Object> params = new HashMap<>();
-        params.put("template", "all");
-        QueryBuilder<?> expectedBuilder = new TemplateQueryBuilder(new Template(expectedTemplateString, ScriptType.INLINE, null, null,
-                params));
-        assertParsedQuery(query, expectedBuilder);
-    }
-
-    // NORELEASE Can we actually test raw templates in either unit or
-    // integration tests now?
-    @Test
-    @AwaitsFix(bugUrl = "Can we actually test raw templates in either unit or integration tests now?")
-    public void testRawTemplate() throws IOException {
-        XContentBuilder builder = XContentFactory.jsonBuilder();
-        builder.startObject();
-        builder.startObject("match_{{template}}");
-        builder.endObject();
-        builder.endObject();
-        String expectedTemplateString = "{\"match_{{template}}\": {}}";
-        String query = "{\"template\": {\"query\": {\"match_{{template}}\": {}},\"params\" : {\"template\" : \"all\"}}}";
-        Map<String, Object> params = new HashMap<>();
-        params.put("template", "all");
-        QueryBuilder<?> expectedBuilder = new TemplateQueryBuilder(new Template(expectedTemplateString, ScriptType.INLINE, null, null,
-                params));
-        assertParsedQuery(query, expectedBuilder);
+        assertEquals("{\"template\":{\"inline\":\"I am a $template string\",\"params\":{\"template\":\"filled\"}}}", content.string());
     }
 
 }
diff --git a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryIT.java b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryIT.java
index 71dd323..0c9fc74 100644
--- a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryIT.java
+++ b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryIT.java
@@ -27,15 +27,13 @@ import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptResponse;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchRequest;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.common.ParseFieldMatcher;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.Template;
 import org.elasticsearch.script.mustache.MustacheScriptEngineService;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Before;
 import org.junit.Test;
@@ -101,14 +99,24 @@ public class TemplateQueryIT extends ESIntegTestCase {
                 "        }\n" +
                 "    }\n" +
                 "}";
-        Map<String, Object> params = new HashMap<>();
-        params.put("template", "all");
-        SearchResponse sr = client().prepareSearch()
-                .setSource(
-                        new SearchSourceBuilder().size(0).query(
-                                QueryBuilders.templateQuery(new Template("{ \"query\": { \"match_{{template}}\": {} } }",
-                                        ScriptType.INLINE, null, null, params)))).execute()
-                .actionGet();
+        SearchResponse sr = client().prepareSearch().setSource(new BytesArray(request))
+                .execute().actionGet();
+        assertNoFailures(sr);
+        assertThat(sr.getHits().hits().length, equalTo(0));
+        request = "{\n" +
+                "    \"query\": {\n" +
+                "        \"template\": {\n" +
+                "            \"query\": {\"match_{{template}}\": {}},\n" +
+                "            \"params\" : {\n" +
+                "                \"template\" : \"all\"\n" +
+                "            }\n" +
+                "        }\n" +
+                "    },\n" +
+                "    \"size\":0" +
+                "}";
+
+        sr = client().prepareSearch().setSource(new BytesArray(request))
+                .execute().actionGet();
         assertNoFailures(sr);
         assertThat(sr.getHits().hits().length, equalTo(0));
     }
@@ -137,11 +145,25 @@ public class TemplateQueryIT extends ESIntegTestCase {
     }
 
     @Test
+    public void testRawEscapedTemplate() throws IOException {
+        String query = "{\"template\": {\"query\": \"{\\\"match_{{template}}\\\": {}}\\\"\",\"params\" : {\"template\" : \"all\"}}}";
+
+        SearchResponse sr = client().prepareSearch().setQuery(query).get();
+        assertHitCount(sr, 2);
+    }
+
+    @Test
+    public void testRawTemplate() throws IOException {
+        String query = "{\"template\": {\"query\": {\"match_{{template}}\": {}},\"params\" : {\"template\" : \"all\"}}}";
+        SearchResponse sr = client().prepareSearch().setQuery(query).get();
+        assertHitCount(sr, 2);
+    }
+
+    @Test
     public void testRawFSTemplate() throws IOException {
-        Map<String, Object> params = new HashMap<>();
-        params.put("template", "all");
-        TemplateQueryBuilder builder = new TemplateQueryBuilder(new Template("storedTemplate", ScriptType.FILE, null, null, params));
-        SearchResponse sr = client().prepareSearch().setQuery(builder).get();
+        String query = "{\"template\": {\"file\": \"storedTemplate\",\"params\" : {\"template\" : \"all\"}}}";
+
+        SearchResponse sr = client().prepareSearch().setQuery(query).get();
         assertHitCount(sr, 2);
     }
 
@@ -151,18 +173,13 @@ public class TemplateQueryIT extends ESIntegTestCase {
         searchRequest.indices("_all");
 
         String query = "{ \"template\" : { \"query\": {\"match_{{template}}\": {} } }, \"params\" : { \"template\":\"all\" } }";
-        searchRequest.template(parseTemplate(query));
+        BytesReference bytesRef = new BytesArray(query);
+        searchRequest.templateSource(bytesRef);
 
         SearchResponse searchResponse = client().search(searchRequest).get();
         assertHitCount(searchResponse, 2);
     }
 
-    private Template parseTemplate(String template) throws IOException {
-        try (XContentParser parser = XContentFactory.xContent(template).createParser(template)) {
-            return TemplateQueryParser.parse(parser, ParseFieldMatcher.EMPTY, "params", "template");
-        }
-    }
-
     @Test
     // Releates to #6318
     public void testSearchRequestFail() throws Exception {
@@ -170,14 +187,16 @@ public class TemplateQueryIT extends ESIntegTestCase {
         searchRequest.indices("_all");
         try {
             String query = "{ \"template\" : { \"query\": {\"match_all\": {}}, \"size\" : \"{{my_size}}\"  } }";
-            searchRequest.template(parseTemplate(query));
+            BytesReference bytesRef = new BytesArray(query);
+            searchRequest.templateSource(bytesRef);
             client().search(searchRequest).get();
             fail("expected exception");
         } catch (Exception ex) {
             // expected - no params
         }
         String query = "{ \"template\" : { \"query\": {\"match_all\": {}}, \"size\" : \"{{my_size}}\"  }, \"params\" : { \"my_size\": 1 } }";
-        searchRequest.template(parseTemplate(query));
+        BytesReference bytesRef = new BytesArray(query);
+        searchRequest.templateSource(bytesRef);
 
         SearchResponse searchResponse = client().search(searchRequest).get();
         assertThat(searchResponse.getHits().hits().length, equalTo(1));
@@ -215,9 +234,10 @@ public class TemplateQueryIT extends ESIntegTestCase {
     public void testSearchTemplateQueryFromFile() throws Exception {
         SearchRequest searchRequest = new SearchRequest();
         searchRequest.indices("_all");
-        String query = "{" + "  \"file\": \"full-query-template\"," + "  \"params\":{" + "    \"mySize\": 2,"
+        String templateString = "{" + "  \"file\": \"full-query-template\"," + "  \"params\":{" + "    \"mySize\": 2,"
                 + "    \"myField\": \"text\"," + "    \"myValue\": \"value1\"" + "  }" + "}";
-        searchRequest.template(parseTemplate(query));
+        BytesReference bytesRef = new BytesArray(templateString);
+        searchRequest.templateSource(bytesRef);
         SearchResponse searchResponse = client().search(searchRequest).get();
         assertThat(searchResponse.getHits().hits().length, equalTo(1));
     }
@@ -229,9 +249,10 @@ public class TemplateQueryIT extends ESIntegTestCase {
     public void testTemplateQueryAsEscapedString() throws Exception {
         SearchRequest searchRequest = new SearchRequest();
         searchRequest.indices("_all");
-        String query = "{" + "  \"template\" : \"{ \\\"size\\\": \\\"{{size}}\\\", \\\"query\\\":{\\\"match_all\\\":{}}}\","
+        String templateString = "{" + "  \"template\" : \"{ \\\"size\\\": \\\"{{size}}\\\", \\\"query\\\":{\\\"match_all\\\":{}}}\","
                 + "  \"params\":{" + "    \"size\": 1" + "  }" + "}";
-        searchRequest.template(parseTemplate(query));
+        BytesReference bytesRef = new BytesArray(templateString);
+        searchRequest.templateSource(bytesRef);
         SearchResponse searchResponse = client().search(searchRequest).get();
         assertThat(searchResponse.getHits().hits().length, equalTo(1));
     }
@@ -247,7 +268,8 @@ public class TemplateQueryIT extends ESIntegTestCase {
         String templateString = "{"
                 + "  \"template\" : \"{ {{#use_size}} \\\"size\\\": \\\"{{size}}\\\", {{/use_size}} \\\"query\\\":{\\\"match_all\\\":{}}}\","
                 + "  \"params\":{" + "    \"size\": 1," + "    \"use_size\": true" + "  }" + "}";
-        searchRequest.template(parseTemplate(templateString));
+        BytesReference bytesRef = new BytesArray(templateString);
+        searchRequest.templateSource(bytesRef);
         SearchResponse searchResponse = client().search(searchRequest).get();
         assertThat(searchResponse.getHits().hits().length, equalTo(1));
     }
@@ -263,7 +285,8 @@ public class TemplateQueryIT extends ESIntegTestCase {
         String templateString = "{"
                 + "  \"inline\" : \"{ \\\"query\\\":{\\\"match_all\\\":{}} {{#use_size}}, \\\"size\\\": \\\"{{size}}\\\" {{/use_size}} }\","
                 + "  \"params\":{" + "    \"size\": 1," + "    \"use_size\": true" + "  }" + "}";
-        searchRequest.template(parseTemplate(templateString));
+        BytesReference bytesRef = new BytesArray(templateString);
+        searchRequest.templateSource(bytesRef);
         SearchResponse searchResponse = client().search(searchRequest).get();
         assertThat(searchResponse.getHits().hits().length, equalTo(1));
     }
@@ -428,15 +451,12 @@ public class TemplateQueryIT extends ESIntegTestCase {
                 .execute().actionGet();
         assertHitCount(sr, 1);
 
-        // "{\"template\": {\"id\": \"3\",\"params\" : {\"fieldParam\" : \"foo\"}}}";
-        Map<String, Object> params = new HashMap<>();
-        params.put("fieldParam", "foo");
-        TemplateQueryBuilder templateQuery = new TemplateQueryBuilder(new Template("3", ScriptType.INDEXED, null, null, params));
-        sr = client().prepareSearch().setQuery(templateQuery).get();
+        String query = "{\"template\": {\"id\": \"3\",\"params\" : {\"fieldParam\" : \"foo\"}}}";
+        sr = client().prepareSearch().setQuery(query).get();
         assertHitCount(sr, 4);
 
-        templateQuery = new TemplateQueryBuilder(new Template("/mustache/3", ScriptType.INDEXED, null, null, params));
-        sr = client().prepareSearch().setQuery(templateQuery).get();
+        query = "{\"template\": {\"id\": \"/mustache/3\",\"params\" : {\"fieldParam\" : \"foo\"}}}";
+        sr = client().prepareSearch().setQuery(query).get();
         assertHitCount(sr, 4);
     }
 
@@ -451,7 +471,7 @@ public class TemplateQueryIT extends ESIntegTestCase {
 
         int iterations = randomIntBetween(2, 11);
         for (int i = 1; i < iterations; i++) {
-            PutIndexedScriptResponse scriptResponse = client().preparePutIndexedScript(MustacheScriptEngineService.NAME, "git01",
+            PutIndexedScriptResponse scriptResponse = client().preparePutIndexedScript(MustacheScriptEngineService.NAME, "git01", 
                     "{\"query\": {\"match\": {\"searchtext\": {\"query\": \"{{P_Keyword1}}\",\"type\": \"ooophrase_prefix\"}}}}").get();
             assertEquals(i * 2 - 1, scriptResponse.getVersion());
 
@@ -487,7 +507,7 @@ public class TemplateQueryIT extends ESIntegTestCase {
         }
     }
 
-
+    
     @Test
     public void testIndexedTemplateWithArray() throws Exception {
       createIndex(ScriptService.SCRIPT_INDEX);
diff --git a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTests.java b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTests.java
index 40a002d..985fbfd 100644
--- a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTests.java
@@ -96,7 +96,7 @@ public class TemplateQueryParserTests extends ESTestCase {
                 new IndexSettingsModule(index, settings),
                 new IndexCacheModule(settings),
                 new AnalysisModule(settings, new IndicesAnalysisService(settings)),
-                new SimilarityModule(settings),
+                new SimilarityModule(index, settings),
                 new IndexNameModule(index),
                 new AbstractModule() {
                     @Override
diff --git a/core/src/test/java/org/elasticsearch/index/query/functionscore/FunctionScoreTests.java b/core/src/test/java/org/elasticsearch/index/query/functionscore/FunctionScoreTests.java
new file mode 100644
index 0000000..bc2272e
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/query/functionscore/FunctionScoreTests.java
@@ -0,0 +1,534 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.query.functionscore;
+
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.*;
+import org.apache.lucene.search.*;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Accountable;
+import org.apache.lucene.util.BytesRef;
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.lucene.search.function.*;
+import org.elasticsearch.index.Index;
+import org.elasticsearch.index.fielddata.*;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.query.functionscore.exp.ExponentialDecayFunctionBuilder;
+import org.elasticsearch.index.query.functionscore.gauss.GaussDecayFunctionBuilder;
+import org.elasticsearch.index.query.functionscore.lin.LinearDecayFunctionBuilder;
+import org.elasticsearch.search.MultiValueMode;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.concurrent.ExecutionException;
+
+import static org.hamcrest.Matchers.closeTo;
+import static org.hamcrest.core.IsEqual.equalTo;
+
+public class FunctionScoreTests extends ESTestCase {
+
+    private static final String UNSUPPORTED = "Method not implemented. This is just a stub for testing.";
+
+
+    /**
+     * Stub for IndexFieldData. Needed by some score functions. Returns 1 as count always.
+     */
+    private static class IndexFieldDataStub implements IndexFieldData<AtomicFieldData> {
+        @Override
+        public MappedFieldType.Names getFieldNames() {
+            return new MappedFieldType.Names("test");
+        }
+
+        @Override
+        public FieldDataType getFieldDataType() {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public AtomicFieldData load(LeafReaderContext context) {
+            return new AtomicFieldData() {
+
+                @Override
+                public ScriptDocValues getScriptValues() {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+
+                @Override
+                public SortedBinaryDocValues getBytesValues() {
+                    return new SortedBinaryDocValues() {
+                        @Override
+                        public void setDocument(int docId) {
+                        }
+
+                        @Override
+                        public int count() {
+                            return 1;
+                        }
+
+                        @Override
+                        public BytesRef valueAt(int index) {
+                            return new BytesRef("0");
+                        }
+                    };
+                }
+
+                @Override
+                public long ramBytesUsed() {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+
+                @Override
+                public Collection<Accountable> getChildResources() {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+
+                @Override
+                public void close() {
+                }
+            };
+        }
+
+        @Override
+        public AtomicFieldData loadDirect(LeafReaderContext context) throws Exception {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public IndexFieldData.XFieldComparatorSource comparatorSource(@Nullable Object missingValue, MultiValueMode sortMode, IndexFieldData.XFieldComparatorSource.Nested nested) {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public void clear() {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public void clear(IndexReader reader) {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public Index index() {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+    }
+
+    /**
+     * Stub for IndexNumericFieldData needed by some score functions. Returns 1 as value always.
+     */
+    private static class IndexNumericFieldDataStub implements IndexNumericFieldData {
+
+        @Override
+        public NumericType getNumericType() {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public MappedFieldType.Names getFieldNames() {
+            return new MappedFieldType.Names("test");
+        }
+
+        @Override
+        public FieldDataType getFieldDataType() {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public AtomicNumericFieldData load(LeafReaderContext context) {
+            return new AtomicNumericFieldData() {
+                @Override
+                public SortedNumericDocValues getLongValues() {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+
+                @Override
+                public SortedNumericDoubleValues getDoubleValues() {
+                    return new SortedNumericDoubleValues() {
+                        @Override
+                        public void setDocument(int doc) {
+                        }
+
+                        @Override
+                        public double valueAt(int index) {
+                            return 1;
+                        }
+
+                        @Override
+                        public int count() {
+                            return 1;
+                        }
+                    };
+                }
+
+                @Override
+                public ScriptDocValues getScriptValues() {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+
+                @Override
+                public SortedBinaryDocValues getBytesValues() {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+
+                @Override
+                public long ramBytesUsed() {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+
+                @Override
+                public Collection<Accountable> getChildResources() {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+
+                @Override
+                public void close() {
+                }
+            };
+        }
+
+        @Override
+        public AtomicNumericFieldData loadDirect(LeafReaderContext context) throws Exception {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public XFieldComparatorSource comparatorSource(@Nullable Object missingValue, MultiValueMode sortMode, XFieldComparatorSource.Nested nested) {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public void clear() {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public void clear(IndexReader reader) {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public Index index() {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+    }
+
+    private static final ScoreFunction RANDOM_SCORE_FUNCTION = new RandomScoreFunction(0, 0, new IndexFieldDataStub());
+    private static final ScoreFunction FIELD_VALUE_FACTOR_FUNCTION = new FieldValueFactorFunction("test", 1, FieldValueFactorFunction.Modifier.LN, new Double(1), null);
+    private static final ScoreFunction GAUSS_DECAY_FUNCTION = new DecayFunctionBuilder.NumericFieldDataScoreFunction(0, 1, 0.1, 0, GaussDecayFunctionBuilder.GAUSS_DECAY_FUNCTION, new IndexNumericFieldDataStub(), MultiValueMode.MAX);
+    private static final ScoreFunction EXP_DECAY_FUNCTION = new DecayFunctionBuilder.NumericFieldDataScoreFunction(0, 1, 0.1, 0, ExponentialDecayFunctionBuilder.EXP_DECAY_FUNCTION, new IndexNumericFieldDataStub(), MultiValueMode.MAX);
+    private static final ScoreFunction LIN_DECAY_FUNCTION = new DecayFunctionBuilder.NumericFieldDataScoreFunction(0, 1, 0.1, 0, LinearDecayFunctionBuilder.LINEAR_DECAY_FUNCTION, new IndexNumericFieldDataStub(), MultiValueMode.MAX);
+    private static final ScoreFunction WEIGHT_FACTOR_FUNCTION = new WeightFactorFunction(4);
+    private static final String TEXT = "The way out is through.";
+    private static final String FIELD = "test";
+    private static final Term TERM = new Term(FIELD, "through");
+    private Directory dir;
+    private IndexWriter w;
+    private DirectoryReader reader;
+    private IndexSearcher searcher;
+
+    @Before
+    public void initSearcher() throws IOException {
+        dir = newDirectory();
+        w = new IndexWriter(dir, newIndexWriterConfig(new StandardAnalyzer()));
+        Document d = new Document();
+        d.add(new TextField(FIELD, TEXT, Field.Store.YES));
+        d.add(new TextField("_uid", "1", Field.Store.YES));
+        w.addDocument(d);
+        w.commit();
+        reader = DirectoryReader.open(w, true);
+        searcher = newSearcher(reader);
+    }
+
+    @After
+    public void closeAllTheReaders() throws IOException {
+        reader.close();
+        w.close();
+        dir.close();
+    }
+
+    @Test
+    public void testExplainFunctionScoreQuery() throws IOException {
+
+        Explanation functionExplanation = getFunctionScoreExplanation(searcher, RANDOM_SCORE_FUNCTION);
+        checkFunctionScoreExplanation(functionExplanation, "random score function (seed: 0)");
+        assertThat(functionExplanation.getDetails()[0].getDetails().length, equalTo(0));
+
+        functionExplanation = getFunctionScoreExplanation(searcher, FIELD_VALUE_FACTOR_FUNCTION);
+        checkFunctionScoreExplanation(functionExplanation, "field value function: ln(doc['test'].value?:1.0 * factor=1.0)");
+        assertThat(functionExplanation.getDetails()[0].getDetails().length, equalTo(0));
+
+        functionExplanation = getFunctionScoreExplanation(searcher, GAUSS_DECAY_FUNCTION);
+        checkFunctionScoreExplanation(functionExplanation, "Function for field test:");
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].toString(), equalTo("0.1 = exp(-0.5*pow(MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)],2.0)/0.21714724095162594)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails().length, equalTo(0));
+
+        functionExplanation = getFunctionScoreExplanation(searcher, EXP_DECAY_FUNCTION);
+        checkFunctionScoreExplanation(functionExplanation, "Function for field test:");
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].toString(), equalTo("0.1 = exp(- MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)] * 2.3025850929940455)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails().length, equalTo(0));
+
+        functionExplanation = getFunctionScoreExplanation(searcher, LIN_DECAY_FUNCTION);
+        checkFunctionScoreExplanation(functionExplanation, "Function for field test:");
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].toString(), equalTo("0.1 = max(0.0, ((1.1111111111111112 - MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)])/1.1111111111111112)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails().length, equalTo(0));
+
+        functionExplanation = getFunctionScoreExplanation(searcher, WEIGHT_FACTOR_FUNCTION);
+        checkFunctionScoreExplanation(functionExplanation, "product of:");
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].toString(), equalTo("1.0 = constant score 1.0 - no function provided\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[1].toString(), equalTo("4.0 = weight\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails().length, equalTo(0));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[1].getDetails().length, equalTo(0));
+    }
+
+    public Explanation getFunctionScoreExplanation(IndexSearcher searcher, ScoreFunction scoreFunction) throws IOException {
+        FunctionScoreQuery functionScoreQuery = new FunctionScoreQuery(new TermQuery(TERM), scoreFunction, 0.0f, CombineFunction.AVG, 100);
+        Weight weight = searcher.createNormalizedWeight(functionScoreQuery, true);
+        Explanation explanation = weight.explain(searcher.getIndexReader().leaves().get(0), 0);
+        return explanation.getDetails()[1];
+    }
+
+    public void checkFunctionScoreExplanation(Explanation randomExplanation, String functionExpl) {
+        assertThat(randomExplanation.getDescription(), equalTo("min of:"));
+        assertThat(randomExplanation.getDetails()[0].getDescription(), equalTo(functionExpl));
+    }
+
+    @Test
+    public void testExplainFiltersFunctionScoreQuery() throws IOException {
+        Explanation functionExplanation = getFiltersFunctionScoreExplanation(searcher, RANDOM_SCORE_FUNCTION);
+        checkFiltersFunctionScoreExplanation(functionExplanation, "random score function (seed: 0)", 0);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails().length, equalTo(0));
+
+        functionExplanation = getFiltersFunctionScoreExplanation(searcher, FIELD_VALUE_FACTOR_FUNCTION);
+        checkFiltersFunctionScoreExplanation(functionExplanation, "field value function: ln(doc['test'].value?:1.0 * factor=1.0)", 0);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails().length, equalTo(0));
+
+        functionExplanation = getFiltersFunctionScoreExplanation(searcher, GAUSS_DECAY_FUNCTION);
+        checkFiltersFunctionScoreExplanation(functionExplanation, "Function for field test:", 0);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails()[0].toString(), equalTo("0.1 = exp(-0.5*pow(MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)],2.0)/0.21714724095162594)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails()[0].getDetails().length, equalTo(0));
+
+        functionExplanation = getFiltersFunctionScoreExplanation(searcher, EXP_DECAY_FUNCTION);
+        checkFiltersFunctionScoreExplanation(functionExplanation, "Function for field test:", 0);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails()[0].toString(), equalTo("0.1 = exp(- MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)] * 2.3025850929940455)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails()[0].getDetails().length, equalTo(0));
+
+        functionExplanation = getFiltersFunctionScoreExplanation(searcher, LIN_DECAY_FUNCTION);
+        checkFiltersFunctionScoreExplanation(functionExplanation, "Function for field test:", 0);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails()[0].toString(), equalTo("0.1 = max(0.0, ((1.1111111111111112 - MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)])/1.1111111111111112)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails()[0].getDetails().length, equalTo(0));
+
+        // now test all together
+        functionExplanation = getFiltersFunctionScoreExplanation(searcher
+                , RANDOM_SCORE_FUNCTION
+                , FIELD_VALUE_FACTOR_FUNCTION
+                , GAUSS_DECAY_FUNCTION
+                , EXP_DECAY_FUNCTION
+                , LIN_DECAY_FUNCTION
+        );
+
+        checkFiltersFunctionScoreExplanation(functionExplanation, "random score function (seed: 0)", 0);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails().length, equalTo(0));
+
+        checkFiltersFunctionScoreExplanation(functionExplanation, "field value function: ln(doc['test'].value?:1.0 * factor=1.0)", 1);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[1].getDetails()[1].getDetails().length, equalTo(0));
+
+        checkFiltersFunctionScoreExplanation(functionExplanation, "Function for field test:", 2);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[2].getDetails()[1].getDetails()[0].toString(), equalTo("0.1 = exp(-0.5*pow(MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)],2.0)/0.21714724095162594)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[2].getDetails()[1].getDetails()[0].getDetails().length, equalTo(0));
+
+        checkFiltersFunctionScoreExplanation(functionExplanation, "Function for field test:", 3);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[3].getDetails()[1].getDetails()[0].toString(), equalTo("0.1 = exp(- MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)] * 2.3025850929940455)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[3].getDetails()[1].getDetails()[0].getDetails().length, equalTo(0));
+
+        checkFiltersFunctionScoreExplanation(functionExplanation, "Function for field test:", 4);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[4].getDetails()[1].getDetails()[0].toString(), equalTo("0.1 = max(0.0, ((1.1111111111111112 - MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)])/1.1111111111111112)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[4].getDetails()[1].getDetails()[0].getDetails().length, equalTo(0));
+    }
+
+    public Explanation getFiltersFunctionScoreExplanation(IndexSearcher searcher, ScoreFunction... scoreFunctions) throws IOException {
+        FiltersFunctionScoreQuery filtersFunctionScoreQuery = getFiltersFunctionScoreQuery(FiltersFunctionScoreQuery.ScoreMode.AVG, CombineFunction.AVG, scoreFunctions);
+        Weight weight = searcher.createNormalizedWeight(filtersFunctionScoreQuery, true);
+        Explanation explanation = weight.explain(searcher.getIndexReader().leaves().get(0), 0);
+        return explanation.getDetails()[1];
+    }
+
+    public FiltersFunctionScoreQuery getFiltersFunctionScoreQuery(FiltersFunctionScoreQuery.ScoreMode scoreMode, CombineFunction combineFunction, ScoreFunction... scoreFunctions) {
+        FiltersFunctionScoreQuery.FilterFunction[] filterFunctions = new FiltersFunctionScoreQuery.FilterFunction[scoreFunctions.length];
+        for (int i = 0; i < scoreFunctions.length; i++) {
+            filterFunctions[i] = new FiltersFunctionScoreQuery.FilterFunction(
+                    new TermQuery(TERM), scoreFunctions[i]);
+        }
+        return new FiltersFunctionScoreQuery(new TermQuery(TERM), scoreMode, filterFunctions, Float.MAX_VALUE, Float.MAX_VALUE * -1, combineFunction);
+    }
+
+    public void checkFiltersFunctionScoreExplanation(Explanation randomExplanation, String functionExpl, int whichFunction) {
+        assertThat(randomExplanation.getDescription(), equalTo("min of:"));
+        assertThat(randomExplanation.getDetails()[0].getDescription(), equalTo("function score, score mode [avg]"));
+        assertThat(randomExplanation.getDetails()[0].getDetails()[whichFunction].getDescription(), equalTo("function score, product of:"));
+        assertThat(randomExplanation.getDetails()[0].getDetails()[whichFunction].getDetails()[0].getDescription(), equalTo("match filter: " + FIELD + ":" + TERM.text()));
+        assertThat(randomExplanation.getDetails()[0].getDetails()[whichFunction].getDetails()[1].getDescription(), equalTo(functionExpl));
+    }
+
+    private static float[] randomFloats(int size) {
+        float[] weights = new float[size];
+        for (int i = 0; i < weights.length; i++) {
+            weights[i] = randomFloat() * (randomBoolean() ? 1.0f : -1.0f) * randomInt(100) + 1.e-5f;
+        }
+        return weights;
+    }
+
+    private static class ScoreFunctionStub extends ScoreFunction {
+        private float score;
+
+        ScoreFunctionStub(float score) {
+            super(CombineFunction.REPLACE);
+            this.score = score;
+        }
+
+        @Override
+        public LeafScoreFunction getLeafScoreFunction(LeafReaderContext ctx) throws IOException {
+            return new LeafScoreFunction() {
+                @Override
+                public double score(int docId, float subQueryScore) {
+                    return score;
+                }
+
+                @Override
+                public Explanation explainScore(int docId, Explanation subQueryScore) throws IOException {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+            };
+        }
+
+        @Override
+        public boolean needsScores() {
+            return false;
+        }
+
+        @Override
+        protected boolean doEquals(ScoreFunction other) {
+            return false;
+        }
+    }
+
+    @Test
+    public void simpleWeightedFunctionsTest() throws IOException, ExecutionException, InterruptedException {
+        int numFunctions = randomIntBetween(1, 3);
+        float[] weights = randomFloats(numFunctions);
+        float[] scores = randomFloats(numFunctions);
+        ScoreFunctionStub[] scoreFunctionStubs = new ScoreFunctionStub[numFunctions];
+        for (int i = 0; i < numFunctions; i++) {
+            scoreFunctionStubs[i] = new ScoreFunctionStub(scores[i]);
+        }
+        WeightFactorFunction[] weightFunctionStubs = new WeightFactorFunction[numFunctions];
+        for (int i = 0; i < numFunctions; i++) {
+            weightFunctionStubs[i] = new WeightFactorFunction(weights[i], scoreFunctionStubs[i]);
+        }
+
+        FiltersFunctionScoreQuery filtersFunctionScoreQueryWithWeights = getFiltersFunctionScoreQuery(
+                FiltersFunctionScoreQuery.ScoreMode.MULTIPLY
+                , CombineFunction.REPLACE
+                , weightFunctionStubs
+        );
+
+        TopDocs topDocsWithWeights = searcher.search(filtersFunctionScoreQueryWithWeights, 1);
+        float scoreWithWeight = topDocsWithWeights.scoreDocs[0].score;
+        double score = 1;
+        for (int i = 0; i < weights.length; i++) {
+            score *= weights[i] * scores[i];
+        }
+        assertThat(scoreWithWeight / score, closeTo(1, 1.e-5d));
+
+        filtersFunctionScoreQueryWithWeights = getFiltersFunctionScoreQuery(
+                FiltersFunctionScoreQuery.ScoreMode.SUM
+                , CombineFunction.REPLACE
+                , weightFunctionStubs
+        );
+
+        topDocsWithWeights = searcher.search(filtersFunctionScoreQueryWithWeights, 1);
+        scoreWithWeight = topDocsWithWeights.scoreDocs[0].score;
+        double sum = 0;
+        for (int i = 0; i < weights.length; i++) {
+            sum += weights[i] * scores[i];
+        }
+        assertThat(scoreWithWeight / sum, closeTo(1, 1.e-5d));
+
+        filtersFunctionScoreQueryWithWeights = getFiltersFunctionScoreQuery(
+                FiltersFunctionScoreQuery.ScoreMode.AVG
+                , CombineFunction.REPLACE
+                , weightFunctionStubs
+        );
+
+        topDocsWithWeights = searcher.search(filtersFunctionScoreQueryWithWeights, 1);
+        scoreWithWeight = topDocsWithWeights.scoreDocs[0].score;
+        double norm = 0;
+        sum = 0;
+        for (int i = 0; i < weights.length; i++) {
+            norm += weights[i];
+            sum += weights[i] * scores[i];
+        }
+        assertThat(scoreWithWeight * norm / sum, closeTo(1, 1.e-5d));
+
+        filtersFunctionScoreQueryWithWeights = getFiltersFunctionScoreQuery(
+                FiltersFunctionScoreQuery.ScoreMode.MIN
+                , CombineFunction.REPLACE
+                , weightFunctionStubs
+        );
+
+        topDocsWithWeights = searcher.search(filtersFunctionScoreQueryWithWeights, 1);
+        scoreWithWeight = topDocsWithWeights.scoreDocs[0].score;
+        double min = Double.POSITIVE_INFINITY;
+        for (int i = 0; i < weights.length; i++) {
+            min = Math.min(min, weights[i] * scores[i]);
+        }
+        assertThat(scoreWithWeight / min, closeTo(1, 1.e-5d));
+
+        filtersFunctionScoreQueryWithWeights = getFiltersFunctionScoreQuery(
+                FiltersFunctionScoreQuery.ScoreMode.MAX
+                , CombineFunction.REPLACE
+                , weightFunctionStubs
+        );
+
+        topDocsWithWeights = searcher.search(filtersFunctionScoreQueryWithWeights, 1);
+        scoreWithWeight = topDocsWithWeights.scoreDocs[0].score;
+        double max = Double.NEGATIVE_INFINITY;
+        for (int i = 0; i < weights.length; i++) {
+            max = Math.max(max, weights[i] * scores[i]);
+        }
+        assertThat(scoreWithWeight / max, closeTo(1, 1.e-5d));
+    }
+
+    @Test
+    public void checkWeightOnlyCreatesBoostFunction() throws IOException {
+        FunctionScoreQuery filtersFunctionScoreQueryWithWeights = new FunctionScoreQuery(new MatchAllDocsQuery(), new WeightFactorFunction(2), 0.0f, CombineFunction.MULTIPLY, 100);
+        TopDocs topDocsWithWeights = searcher.search(filtersFunctionScoreQueryWithWeights, 1);
+        float score = topDocsWithWeights.scoreDocs[0].score;
+        assertThat(score, equalTo(2.0f));
+    }
+}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java b/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java
index c72470c..432c833 100644
--- a/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java
+++ b/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java
@@ -27,10 +27,7 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.AbstractQueryBuilder;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.index.query.QueryParser;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.*;
 import org.elasticsearch.indices.IndicesModule;
 import org.elasticsearch.plugins.Plugin;
 
@@ -67,22 +64,22 @@ public class DummyQueryParserPlugin extends Plugin {
 
         @Override
         protected DummyQueryBuilder doReadFrom(StreamInput in) throws IOException {
-            return new DummyQueryBuilder();
+            return null;
         }
 
         @Override
         protected void doWriteTo(StreamOutput out) throws IOException {
-            // Do Nothing
+
         }
 
         @Override
-        protected int doHashCode() {
-            return 0;
+        protected boolean doEquals(DummyQueryBuilder other) {
+            return false;
         }
 
         @Override
-        protected boolean doEquals(DummyQueryBuilder other) {
-            return true;
+        protected int doHashCode() {
+            return 0;
         }
 
         @Override
diff --git a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
index c1bdd9d..89d2701 100644
--- a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
+++ b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
@@ -41,6 +41,7 @@ import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.SnapshotId;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.routing.*;
+import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
@@ -69,7 +70,6 @@ import org.elasticsearch.index.mapper.ParseContext;
 import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.Uid;
 import org.elasticsearch.index.mapper.internal.UidFieldMapper;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.index.settings.IndexSettingsService;
 import org.elasticsearch.index.snapshots.IndexShardRepository;
 import org.elasticsearch.index.snapshots.IndexShardSnapshotStatus;
@@ -95,10 +95,7 @@ import java.util.concurrent.CyclicBarrier;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.atomic.AtomicBoolean;
 
-import static org.elasticsearch.cluster.metadata.IndexMetaData.EMPTY_PARAMS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_VERSION_CREATED;
+import static org.elasticsearch.cluster.metadata.IndexMetaData.*;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
@@ -342,7 +339,8 @@ public class IndexShardTests extends ESSingleNodeTestCase {
         client().prepareIndex("test", "test").setSource("{}").get();
         ensureGreen("test");
         IndicesService indicesService = getInstanceFromNode(IndicesService.class);
-        indicesService.indexService("test").getShardOrNull(0).markAsInactive();
+        Boolean result = indicesService.indexService("test").getShardOrNull(0).checkIdle(0);
+        assertEquals(Boolean.TRUE, result);
         assertBusy(new Runnable() { // should be very very quick
             @Override
             public void run() {
@@ -628,9 +626,9 @@ public class IndexShardTests extends ESSingleNodeTestCase {
 
         shardIndexingService.addListener(new IndexingOperationListener() {
             @Override
-            public Engine.Index preIndex(Engine.Index index) {
+            public Engine.Index preIndex(Engine.Index operation) {
                 preIndexCalled.set(true);
-                return super.preIndex(index);
+                return super.preIndex(operation);
             }
         });
 
@@ -957,7 +955,7 @@ public class IndexShardTests extends ESSingleNodeTestCase {
             }
         };
 
-        IndexServicesProvider newProvider = new IndexServicesProvider(indexServices.getIndicesLifecycle(), indexServices.getThreadPool(), indexServices.getMapperService(), indexServices.getQueryParserService(), indexServices.getIndexCache(), indexServices.getIndexAliasesService(), indexServices.getIndicesQueryCache(), indexServices.getCodecService(), indexServices.getTermVectorsService(), indexServices.getIndexFieldDataService(), indexServices.getWarmer(), indexServices.getSimilarityService(), indexServices.getFactory(), indexServices.getBigArrays(), wrapper);
+        IndexServicesProvider newProvider = new IndexServicesProvider(indexServices.getIndicesLifecycle(), indexServices.getThreadPool(), indexServices.getMapperService(), indexServices.getQueryParserService(), indexServices.getIndexCache(), indexServices.getIndexAliasesService(), indexServices.getIndicesQueryCache(), indexServices.getCodecService(), indexServices.getTermVectorsService(), indexServices.getIndexFieldDataService(), indexServices.getWarmer(), indexServices.getSimilarityService(), indexServices.getFactory(), indexServices.getBigArrays(), wrapper, indexServices.getIndexingMemoryController());
         IndexShard newShard = new IndexShard(shard.shardId(), shard.indexSettings, shard.shardPath(), shard.store(), newProvider);
 
         ShardRoutingHelper.reinit(routing);
diff --git a/core/src/test/java/org/elasticsearch/index/similarity/SimilarityModuleTests.java b/core/src/test/java/org/elasticsearch/index/similarity/SimilarityModuleTests.java
new file mode 100644
index 0000000..a73d2a5
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/similarity/SimilarityModuleTests.java
@@ -0,0 +1,117 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.index.similarity;
+
+import org.apache.lucene.index.FieldInvertState;
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.search.CollectionStatistics;
+import org.apache.lucene.search.TermStatistics;
+import org.apache.lucene.search.similarities.BM25Similarity;
+import org.apache.lucene.search.similarities.Similarity;
+import org.elasticsearch.common.inject.ModuleTestCase;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.Index;
+
+import java.io.IOException;
+
+public class SimilarityModuleTests extends ModuleTestCase {
+
+    public void testAddSimilarity() {
+        Settings indexSettings = Settings.settingsBuilder()
+                .put("index.similarity.my_similarity.type", "test_similarity")
+                .put("index.similarity.my_similarity.key", "there is a key")
+                .build();
+        SimilarityModule module = new SimilarityModule(new Index("foo"), indexSettings);
+        module.addSimilarity("test_similarity", (string, settings) -> new SimilarityProvider() {
+            @Override
+            public String name() {
+                return string;
+            }
+
+            @Override
+            public Similarity get() {
+                return new TestSimilarity(settings.get("key"));
+            }
+        });
+        assertInstanceBinding(module, SimilarityService.class, (inst) -> {
+            if (inst instanceof SimilarityService) {
+                assertNotNull(inst.getSimilarity("my_similarity"));
+                assertTrue(inst.getSimilarity("my_similarity").get() instanceof TestSimilarity);
+                assertEquals("my_similarity", inst.getSimilarity("my_similarity").name());
+                assertEquals("there is a key" , ((TestSimilarity)inst.getSimilarity("my_similarity").get()).key);
+                return true;
+            }
+            return false;
+        });
+    }
+
+    public void testSetupUnknownSimilarity() {
+        Settings indexSettings = Settings.settingsBuilder()
+                .put("index.similarity.my_similarity.type", "test_similarity")
+                .build();
+        SimilarityModule module = new SimilarityModule(new Index("foo"), indexSettings);
+        try {
+            assertInstanceBinding(module, SimilarityService.class, (inst) -> inst instanceof SimilarityService);
+        } catch (IllegalArgumentException ex) {
+            assertEquals("Unknown Similarity type [test_similarity] for [my_similarity]", ex.getMessage());
+        }
+    }
+
+
+    public void testSetupWithoutType() {
+        Settings indexSettings = Settings.settingsBuilder()
+                .put("index.similarity.my_similarity.foo", "bar")
+                .build();
+        SimilarityModule module = new SimilarityModule(new Index("foo"), indexSettings);
+        try {
+            assertInstanceBinding(module, SimilarityService.class, (inst) -> inst instanceof SimilarityService);
+        } catch (IllegalArgumentException ex) {
+            assertEquals("Similarity [my_similarity] must have an associated type", ex.getMessage());
+        }
+    }
+
+
+    private static class TestSimilarity extends Similarity {
+        private final Similarity delegate = new BM25Similarity();
+        private final String key;
+
+
+        public TestSimilarity(String key) {
+            if (key == null) {
+                throw new AssertionError("key is null");
+            }
+            this.key = key;
+        }
+
+        @Override
+        public long computeNorm(FieldInvertState state) {
+            return delegate.computeNorm(state);
+        }
+
+        @Override
+        public SimWeight computeWeight(CollectionStatistics collectionStats, TermStatistics... termStats) {
+            return delegate.computeWeight(collectionStats, termStats);
+        }
+
+        @Override
+        public SimScorer simScorer(SimWeight weight, LeafReaderContext context) throws IOException {
+            return delegate.simScorer(weight, context);
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/index/similarity/SimilarityTests.java b/core/src/test/java/org/elasticsearch/index/similarity/SimilarityTests.java
index 6a42ba7..28f5e5c 100644
--- a/core/src/test/java/org/elasticsearch/index/similarity/SimilarityTests.java
+++ b/core/src/test/java/org/elasticsearch/index/similarity/SimilarityTests.java
@@ -22,6 +22,7 @@ package org.elasticsearch.index.similarity;
 import org.apache.lucene.search.similarities.*;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.test.ESSingleNodeTestCase;
 import org.junit.Test;
@@ -35,11 +36,9 @@ public class SimilarityTests extends ESSingleNodeTestCase {
 
     @Test
     public void testResolveDefaultSimilarities() {
-        SimilarityLookupService similarityLookupService = createIndex("foo").similarityService().similarityLookupService();
-        assertThat(similarityLookupService.similarity("default"), instanceOf(PreBuiltSimilarityProvider.class));
-        assertThat(similarityLookupService.similarity("default").get(), instanceOf(DefaultSimilarity.class));
-        assertThat(similarityLookupService.similarity("BM25"), instanceOf(PreBuiltSimilarityProvider.class));
-        assertThat(similarityLookupService.similarity("BM25").get(), instanceOf(BM25Similarity.class));
+        SimilarityService similarityService = createIndex("foo").similarityService();
+        assertThat(similarityService.getSimilarity("default").get(), instanceOf(DefaultSimilarity.class));
+        assertThat(similarityService.getSimilarity("BM25").get(), instanceOf(BM25Similarity.class));
     }
 
     @Test
@@ -54,8 +53,8 @@ public class SimilarityTests extends ESSingleNodeTestCase {
                 .put("index.similarity.my_similarity.type", "default")
                 .put("index.similarity.my_similarity.discount_overlaps", false)
                 .build();
-        SimilarityService similarityService = createIndex("foo", indexSettings).similarityService();
-        DocumentMapper documentMapper = similarityService.mapperService().documentMapperParser().parse(mapping);
+        IndexService indexService = createIndex("foo", indexSettings);
+        DocumentMapper documentMapper = indexService.mapperService().documentMapperParser().parse(mapping);
         assertThat(documentMapper.mappers().getMapper("field1").fieldType().similarity(), instanceOf(DefaultSimilarityProvider.class));
 
         DefaultSimilarity similarity = (DefaultSimilarity) documentMapper.mappers().getMapper("field1").fieldType().similarity().get();
@@ -76,8 +75,8 @@ public class SimilarityTests extends ESSingleNodeTestCase {
                 .put("index.similarity.my_similarity.b", 1.5f)
                 .put("index.similarity.my_similarity.discount_overlaps", false)
                 .build();
-        SimilarityService similarityService = createIndex("foo", indexSettings).similarityService();
-        DocumentMapper documentMapper = similarityService.mapperService().documentMapperParser().parse(mapping);
+        IndexService indexService = createIndex("foo", indexSettings);
+        DocumentMapper documentMapper = indexService.mapperService().documentMapperParser().parse(mapping);
         assertThat(documentMapper.mappers().getMapper("field1").fieldType().similarity(), instanceOf(BM25SimilarityProvider.class));
 
         BM25Similarity similarity = (BM25Similarity) documentMapper.mappers().getMapper("field1").fieldType().similarity().get();
@@ -101,8 +100,8 @@ public class SimilarityTests extends ESSingleNodeTestCase {
                 .put("index.similarity.my_similarity.normalization", "h2")
                 .put("index.similarity.my_similarity.normalization.h2.c", 3f)
                 .build();
-        SimilarityService similarityService = createIndex("foo", indexSettings).similarityService();
-        DocumentMapper documentMapper = similarityService.mapperService().documentMapperParser().parse(mapping);
+        IndexService indexService = createIndex("foo", indexSettings);
+        DocumentMapper documentMapper = indexService.mapperService().documentMapperParser().parse(mapping);
         assertThat(documentMapper.mappers().getMapper("field1").fieldType().similarity(), instanceOf(DFRSimilarityProvider.class));
 
         DFRSimilarity similarity = (DFRSimilarity) documentMapper.mappers().getMapper("field1").fieldType().similarity().get();
@@ -127,8 +126,8 @@ public class SimilarityTests extends ESSingleNodeTestCase {
                 .put("index.similarity.my_similarity.normalization", "h2")
                 .put("index.similarity.my_similarity.normalization.h2.c", 3f)
                 .build();
-        SimilarityService similarityService = createIndex("foo", indexSettings).similarityService();
-        DocumentMapper documentMapper = similarityService.mapperService().documentMapperParser().parse(mapping);
+        IndexService indexService = createIndex("foo", indexSettings);
+        DocumentMapper documentMapper = indexService.mapperService().documentMapperParser().parse(mapping);
         assertThat(documentMapper.mappers().getMapper("field1").fieldType().similarity(), instanceOf(IBSimilarityProvider.class));
 
         IBSimilarity similarity = (IBSimilarity) documentMapper.mappers().getMapper("field1").fieldType().similarity().get();
@@ -150,8 +149,8 @@ public class SimilarityTests extends ESSingleNodeTestCase {
                 .put("index.similarity.my_similarity.type", "LMDirichlet")
                 .put("index.similarity.my_similarity.mu", 3000f)
                 .build();
-        SimilarityService similarityService = createIndex("foo", indexSettings).similarityService();
-        DocumentMapper documentMapper = similarityService.mapperService().documentMapperParser().parse(mapping);
+        IndexService indexService = createIndex("foo", indexSettings);
+        DocumentMapper documentMapper = indexService.mapperService().documentMapperParser().parse(mapping);
         assertThat(documentMapper.mappers().getMapper("field1").fieldType().similarity(), instanceOf(LMDirichletSimilarityProvider.class));
 
         LMDirichletSimilarity similarity = (LMDirichletSimilarity) documentMapper.mappers().getMapper("field1").fieldType().similarity().get();
@@ -170,8 +169,8 @@ public class SimilarityTests extends ESSingleNodeTestCase {
                 .put("index.similarity.my_similarity.type", "LMJelinekMercer")
                 .put("index.similarity.my_similarity.lambda", 0.7f)
                 .build();
-        SimilarityService similarityService = createIndex("foo", indexSettings).similarityService();
-        DocumentMapper documentMapper = similarityService.mapperService().documentMapperParser().parse(mapping);
+        IndexService indexService = createIndex("foo", indexSettings);
+        DocumentMapper documentMapper = indexService.mapperService().documentMapperParser().parse(mapping);
         assertThat(documentMapper.mappers().getMapper("field1").fieldType().similarity(), instanceOf(LMJelinekMercerSimilarityProvider.class));
 
         LMJelinekMercerSimilarity similarity = (LMJelinekMercerSimilarity) documentMapper.mappers().getMapper("field1").fieldType().similarity().get();
diff --git a/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java b/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java
index 8764d1a..5a25f65 100644
--- a/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java
+++ b/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java
@@ -34,6 +34,7 @@ import org.elasticsearch.common.io.FileSystemUtils;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.concurrent.AbstractRunnable;
 import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
@@ -183,14 +184,14 @@ public class TranslogTests extends ESTestCase {
 
     @Test
     public void testRead() throws IOException {
-        Translog.Location loc1 = translog.add(new Translog.Create("test", "1", new byte[]{1}));
-        Translog.Location loc2 = translog.add(new Translog.Create("test", "2", new byte[]{2}));
+        Translog.Location loc1 = translog.add(new Translog.Index("test", "1", new byte[]{1}));
+        Translog.Location loc2 = translog.add(new Translog.Index("test", "2", new byte[]{2}));
         assertThat(translog.read(loc1).getSource().source.toBytesArray(), equalTo(new BytesArray(new byte[]{1})));
         assertThat(translog.read(loc2).getSource().source.toBytesArray(), equalTo(new BytesArray(new byte[]{2})));
         translog.sync();
         assertThat(translog.read(loc1).getSource().source.toBytesArray(), equalTo(new BytesArray(new byte[]{1})));
         assertThat(translog.read(loc2).getSource().source.toBytesArray(), equalTo(new BytesArray(new byte[]{2})));
-        Translog.Location loc3 = translog.add(new Translog.Create("test", "2", new byte[]{3}));
+        Translog.Location loc3 = translog.add(new Translog.Index("test", "2", new byte[]{3}));
         assertThat(translog.read(loc3).getSource().source.toBytesArray(), equalTo(new BytesArray(new byte[]{3})));
         translog.sync();
         assertThat(translog.read(loc3).getSource().source.toBytesArray(), equalTo(new BytesArray(new byte[]{3})));
@@ -215,19 +216,13 @@ public class TranslogTests extends ESTestCase {
         assertThat(snapshot, SnapshotMatchers.size(0));
         snapshot.close();
 
-        addToTranslogAndList(translog, ops, new Translog.Create("test", "1", new byte[]{1}));
-        snapshot = translog.newSnapshot();
-        assertThat(snapshot, SnapshotMatchers.equalsTo(ops));
-        assertThat(snapshot.estimatedTotalOperations(), equalTo(1));
-        snapshot.close();
-
-        addToTranslogAndList(translog, ops, new Translog.Index("test", "2", new byte[]{2}));
+        addToTranslogAndList(translog, ops, new Translog.Index("test", "1", new byte[]{1}));
         snapshot = translog.newSnapshot();
         assertThat(snapshot, SnapshotMatchers.equalsTo(ops));
         assertThat(snapshot.estimatedTotalOperations(), equalTo(ops.size()));
         snapshot.close();
 
-        addToTranslogAndList(translog, ops, new Translog.Delete(newUid("3")));
+        addToTranslogAndList(translog, ops, new Translog.Delete(newUid("2")));
         snapshot = translog.newSnapshot();
         assertThat(snapshot, SnapshotMatchers.equalsTo(ops));
         assertThat(snapshot.estimatedTotalOperations(), equalTo(ops.size()));
@@ -235,17 +230,13 @@ public class TranslogTests extends ESTestCase {
 
         snapshot = translog.newSnapshot();
 
-        Translog.Create create = (Translog.Create) snapshot.next();
-        assertThat(create != null, equalTo(true));
-        assertThat(create.source().toBytes(), equalTo(new byte[]{1}));
-
         Translog.Index index = (Translog.Index) snapshot.next();
         assertThat(index != null, equalTo(true));
-        assertThat(index.source().toBytes(), equalTo(new byte[]{2}));
+        assertThat(index.source().toBytes(), equalTo(new byte[]{1}));
 
         Translog.Delete delete = (Translog.Delete) snapshot.next();
         assertThat(delete != null, equalTo(true));
-        assertThat(delete.uid(), equalTo(newUid("3")));
+        assertThat(delete.uid(), equalTo(newUid("2")));
 
         assertThat(snapshot.next(), equalTo(null));
 
@@ -286,38 +277,63 @@ public class TranslogTests extends ESTestCase {
         final long firstOperationPosition = translog.getFirstOperationPosition();
         TranslogStats stats = stats();
         assertThat(stats.estimatedNumberOfOperations(), equalTo(0l));
-        long lastSize = stats.translogSizeInBytes().bytes();
+        long lastSize = stats.getTranslogSizeInBytes();
         assertThat((int) firstOperationPosition, greaterThan(CodecUtil.headerLength(TranslogWriter.TRANSLOG_CODEC)));
         assertThat(lastSize, equalTo(firstOperationPosition));
-
-        translog.add(new Translog.Create("test", "1", new byte[]{1}));
+        TranslogStats total = new TranslogStats();
+        translog.add(new Translog.Index("test", "1", new byte[]{1}));
         stats = stats();
+        total.add(stats);
         assertThat(stats.estimatedNumberOfOperations(), equalTo(1l));
-        assertThat(stats.translogSizeInBytes().bytes(), greaterThan(lastSize));
-        lastSize = stats.translogSizeInBytes().bytes();
+        assertThat(stats.getTranslogSizeInBytes(), greaterThan(lastSize));
+        lastSize = stats.getTranslogSizeInBytes();
 
-        translog.add(new Translog.Index("test", "2", new byte[]{2}));
+        translog.add(new Translog.Delete(newUid("2")));
         stats = stats();
+        total.add(stats);
         assertThat(stats.estimatedNumberOfOperations(), equalTo(2l));
-        assertThat(stats.translogSizeInBytes().bytes(), greaterThan(lastSize));
-        lastSize = stats.translogSizeInBytes().bytes();
+        assertThat(stats.getTranslogSizeInBytes(), greaterThan(lastSize));
+        lastSize = stats.getTranslogSizeInBytes();
 
         translog.add(new Translog.Delete(newUid("3")));
-        stats = stats();
-        assertThat(stats.estimatedNumberOfOperations(), equalTo(3l));
-        assertThat(stats.translogSizeInBytes().bytes(), greaterThan(lastSize));
-        lastSize = stats.translogSizeInBytes().bytes();
-
-        translog.add(new Translog.Delete(newUid("4")));
         translog.prepareCommit();
         stats = stats();
-        assertThat(stats.estimatedNumberOfOperations(), equalTo(4l));
-        assertThat(stats.translogSizeInBytes().bytes(), greaterThan(lastSize));
+        total.add(stats);
+        assertThat(stats.estimatedNumberOfOperations(), equalTo(3l));
+        assertThat(stats.getTranslogSizeInBytes(), greaterThan(lastSize));
 
         translog.commit();
         stats = stats();
+        total.add(stats);
         assertThat(stats.estimatedNumberOfOperations(), equalTo(0l));
-        assertThat(stats.translogSizeInBytes().bytes(), equalTo(firstOperationPosition));
+        assertThat(stats.getTranslogSizeInBytes(), equalTo(firstOperationPosition));
+        assertEquals(6, total.estimatedNumberOfOperations());
+        assertEquals(431, total.getTranslogSizeInBytes());
+
+        BytesStreamOutput out = new BytesStreamOutput();
+        total.writeTo(out);
+        TranslogStats copy = new TranslogStats();
+        copy.readFrom(StreamInput.wrap(out.bytes()));
+
+        assertEquals(6, copy.estimatedNumberOfOperations());
+        assertEquals(431, copy.getTranslogSizeInBytes());
+        assertEquals("\"translog\"{\n" +
+                "  \"operations\" : 6,\n" +
+                "  \"size_in_bytes\" : 431\n" +
+                "}", copy.toString().trim());
+
+        try {
+            new TranslogStats(1, -1);
+            fail("must be positive");
+        } catch (IllegalArgumentException ex) {
+            //all well
+        }
+        try {
+            new TranslogStats(-1, 1);
+            fail("must be positive");
+        } catch (IllegalArgumentException ex) {
+            //all well
+        }
     }
 
     @Test
@@ -327,7 +343,7 @@ public class TranslogTests extends ESTestCase {
         assertThat(snapshot, SnapshotMatchers.size(0));
         snapshot.close();
 
-        addToTranslogAndList(translog, ops, new Translog.Create("test", "1", new byte[]{1}));
+        addToTranslogAndList(translog, ops, new Translog.Index("test", "1", new byte[]{1}));
 
         snapshot = translog.newSnapshot();
         assertThat(snapshot, SnapshotMatchers.equalsTo(ops));
@@ -354,7 +370,7 @@ public class TranslogTests extends ESTestCase {
         assertThat(snapshot, SnapshotMatchers.size(0));
         snapshot.close();
 
-        addToTranslogAndList(translog, ops, new Translog.Create("test", "1", new byte[]{1}));
+        addToTranslogAndList(translog, ops, new Translog.Index("test", "1", new byte[]{1}));
         Translog.Snapshot snapshot1 = translog.newSnapshot();
 
         addToTranslogAndList(translog, ops, new Translog.Index("test", "2", new byte[]{2}));
@@ -375,7 +391,7 @@ public class TranslogTests extends ESTestCase {
 
     public void testSnapshotOnClosedTranslog() throws IOException {
         assertTrue(Files.exists(translogDir.resolve(Translog.getFilename(1))));
-        translog.add(new Translog.Create("test", "1", new byte[]{1}));
+        translog.add(new Translog.Index("test", "1", new byte[]{1}));
         translog.close();
         try {
             Translog.Snapshot snapshot = translog.newSnapshot();
@@ -388,7 +404,7 @@ public class TranslogTests extends ESTestCase {
     @Test
     public void deleteOnSnapshotRelease() throws Exception {
         ArrayList<Translog.Operation> firstOps = new ArrayList<>();
-        addToTranslogAndList(translog, firstOps, new Translog.Create("test", "1", new byte[]{1}));
+        addToTranslogAndList(translog, firstOps, new Translog.Index("test", "1", new byte[]{1}));
 
         Translog.Snapshot firstSnapshot = translog.newSnapshot();
         assertThat(firstSnapshot.estimatedTotalOperations(), equalTo(1));
@@ -463,10 +479,7 @@ public class TranslogTests extends ESTestCase {
                             Translog.Operation op;
                             switch (randomFrom(Translog.Operation.Type.values())) {
                                 case CREATE:
-                                    op = new Translog.Create("test", threadId + "_" + opCount,
-                                            randomUnicodeOfLengthBetween(1, 20 * 1024).getBytes("UTF-8"));
-                                    break;
-                                case SAVE:
+                                case INDEX:
                                     op = new Translog.Index("test", threadId + "_" + opCount,
                                             randomUnicodeOfLengthBetween(1, 20 * 1024).getBytes("UTF-8"));
                                     break;
@@ -508,7 +521,7 @@ public class TranslogTests extends ESTestCase {
             Translog.Operation expectedOp = locationOperation.operation;
             assertEquals(expectedOp.opType(), op.opType());
             switch (op.opType()) {
-                case SAVE:
+                case INDEX:
                     Translog.Index indexOp = (Translog.Index) op;
                     Translog.Index expIndexOp = (Translog.Index) expectedOp;
                     assertEquals(expIndexOp.id(), indexOp.id());
@@ -518,16 +531,6 @@ public class TranslogTests extends ESTestCase {
                     assertEquals(expIndexOp.version(), indexOp.version());
                     assertEquals(expIndexOp.versionType(), indexOp.versionType());
                     break;
-                case CREATE:
-                    Translog.Create createOp = (Translog.Create) op;
-                    Translog.Create expCreateOp = (Translog.Create) expectedOp;
-                    assertEquals(expCreateOp.id(), createOp.id());
-                    assertEquals(expCreateOp.routing(), createOp.routing());
-                    assertEquals(expCreateOp.type(), createOp.type());
-                    assertEquals(expCreateOp.source(), createOp.source());
-                    assertEquals(expCreateOp.version(), createOp.version());
-                    assertEquals(expCreateOp.versionType(), createOp.versionType());
-                    break;
                 case DELETE:
                     Translog.Delete delOp = (Translog.Delete) op;
                     Translog.Delete expDelOp = (Translog.Delete) expectedOp;
@@ -550,7 +553,7 @@ public class TranslogTests extends ESTestCase {
         int translogOperations = randomIntBetween(10, 100);
         for (int op = 0; op < translogOperations; op++) {
             String ascii = randomAsciiOfLengthBetween(1, 50);
-            locations.add(translog.add(new Translog.Create("test", "" + op, ascii.getBytes("UTF-8"))));
+            locations.add(translog.add(new Translog.Index("test", "" + op, ascii.getBytes("UTF-8"))));
         }
         translog.sync();
 
@@ -574,7 +577,7 @@ public class TranslogTests extends ESTestCase {
         int translogOperations = randomIntBetween(10, 100);
         for (int op = 0; op < translogOperations; op++) {
             String ascii = randomAsciiOfLengthBetween(1, 50);
-            locations.add(translog.add(new Translog.Create("test", "" + op, ascii.getBytes("UTF-8"))));
+            locations.add(translog.add(new Translog.Index("test", "" + op, ascii.getBytes("UTF-8"))));
         }
         translog.sync();
 
@@ -638,7 +641,7 @@ public class TranslogTests extends ESTestCase {
     @Test
     public void testVerifyTranslogIsNotDeleted() throws IOException {
         assertFileIsPresent(translog, 1);
-        translog.add(new Translog.Create("test", "1", new byte[]{1}));
+        translog.add(new Translog.Index("test", "1", new byte[]{1}));
         Translog.Snapshot snapshot = translog.newSnapshot();
         assertThat(snapshot, SnapshotMatchers.size(1));
         assertFileIsPresent(translog, 1);
@@ -686,9 +689,7 @@ public class TranslogTests extends ESTestCase {
                         final Translog.Operation op;
                         switch (Translog.Operation.Type.values()[((int) (id % Translog.Operation.Type.values().length))]) {
                             case CREATE:
-                                op = new Translog.Create("type", "" + id, new byte[]{(byte) id});
-                                break;
-                            case SAVE:
+                            case INDEX:
                                 op = new Translog.Index("type", "" + id, new byte[]{(byte) id});
                                 break;
                             case DELETE:
@@ -830,12 +831,12 @@ public class TranslogTests extends ESTestCase {
         int translogOperations = randomIntBetween(10, 100);
         int count = 0;
         for (int op = 0; op < translogOperations; op++) {
-            final Translog.Location location = translog.add(new Translog.Create("test", "" + op, Integer.toString(++count).getBytes(Charset.forName("UTF-8"))));
+            final Translog.Location location = translog.add(new Translog.Index("test", "" + op, Integer.toString(++count).getBytes(Charset.forName("UTF-8"))));
             if (randomBoolean()) {
                 assertTrue("at least one operation pending", translog.syncNeeded());
                 assertTrue("this operation has not been synced", translog.ensureSynced(location));
                 assertFalse("the last call to ensureSycned synced all previous ops", translog.syncNeeded()); // we are the last location so everything should be synced
-                translog.add(new Translog.Create("test", "" + op, Integer.toString(++count).getBytes(Charset.forName("UTF-8"))));
+                translog.add(new Translog.Index("test", "" + op, Integer.toString(++count).getBytes(Charset.forName("UTF-8"))));
                 assertTrue("one pending operation", translog.syncNeeded());
                 assertFalse("this op has been synced before", translog.ensureSynced(location)); // not syncing now
                 assertTrue("we only synced a previous operation yet", translog.syncNeeded());
@@ -858,7 +859,7 @@ public class TranslogTests extends ESTestCase {
         int translogOperations = randomIntBetween(10, 100);
         int count = 0;
         for (int op = 0; op < translogOperations; op++) {
-            locations.add(translog.add(new Translog.Create("test", "" + op, Integer.toString(++count).getBytes(Charset.forName("UTF-8")))));
+            locations.add(translog.add(new Translog.Index("test", "" + op, Integer.toString(++count).getBytes(Charset.forName("UTF-8")))));
             if (rarely() && translogOperations > op+1) {
                 translog.commit();
             }
@@ -887,14 +888,14 @@ public class TranslogTests extends ESTestCase {
         int translogOperations = randomIntBetween(10, 100);
         int lastSynced = -1;
         for (int op = 0; op < translogOperations; op++) {
-            locations.add(translog.add(new Translog.Create("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
+            locations.add(translog.add(new Translog.Index("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
             if (frequently()) {
                 translog.sync();
                 lastSynced = op;
             }
         }
         assertEquals(translogOperations, translog.totalOperations());
-        final Translog.Location lastLocation = translog.add(new Translog.Create("test", "" + translogOperations, Integer.toString(translogOperations).getBytes(Charset.forName("UTF-8"))));
+        final Translog.Location lastLocation = translog.add(new Translog.Index("test", "" + translogOperations, Integer.toString(translogOperations).getBytes(Charset.forName("UTF-8"))));
 
         final Checkpoint checkpoint = Checkpoint.read(translog.location().resolve(Translog.CHECKPOINT_FILE_NAME));
         try (final ImmutableTranslogReader reader = translog.openReader(translog.location().resolve(Translog.getFilename(translog.currentFileGeneration())), checkpoint)) {
@@ -975,7 +976,7 @@ public class TranslogTests extends ESTestCase {
         int minUncommittedOp = -1;
         final boolean commitOften = randomBoolean();
         for (int op = 0; op < translogOperations; op++) {
-            locations.add(translog.add(new Translog.Create("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
+            locations.add(translog.add(new Translog.Index("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
             final boolean commit = commitOften ? frequently() : rarely();
             if (commit && op < translogOperations-1) {
                 translog.commit();
@@ -1017,7 +1018,7 @@ public class TranslogTests extends ESTestCase {
         Translog.TranslogGeneration translogGeneration = null;
         final boolean sync = randomBoolean();
         for (int op = 0; op < translogOperations; op++) {
-            locations.add(translog.add(new Translog.Create("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
+            locations.add(translog.add(new Translog.Index("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
             if (op == prepareOp) {
                 translogGeneration = translog.getGeneration();
                 translog.prepareCommit();
@@ -1068,7 +1069,7 @@ public class TranslogTests extends ESTestCase {
         List<Translog.Operation> ops = new ArrayList<>();
         int translogOperations = randomIntBetween(10, 100);
         for (int op = 0; op < translogOperations; op++) {
-            Translog.Create test = new Translog.Create("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")));
+            Translog.Index test = new Translog.Index("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")));
             ops.add(test);
         }
         Translog.writeOperations(out, ops);
@@ -1083,8 +1084,8 @@ public class TranslogTests extends ESTestCase {
         int translogOperations = randomIntBetween(10, 100);
         try(Translog translog2 = create(createTempDir())) {
             for (int op = 0; op < translogOperations; op++) {
-                locations.add(translog.add(new Translog.Create("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
-                locations2.add(translog2.add(new Translog.Create("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
+                locations.add(translog.add(new Translog.Index("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
+                locations2.add(translog2.add(new Translog.Index("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
             }
             int iters = randomIntBetween(10, 100);
             for (int i = 0; i < iters; i++) {
@@ -1110,7 +1111,7 @@ public class TranslogTests extends ESTestCase {
         int translogOperations = randomIntBetween(1, 10);
         int firstUncommitted = 0;
         for (int op = 0; op < translogOperations; op++) {
-            locations.add(translog.add(new Translog.Create("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
+            locations.add(translog.add(new Translog.Index("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
             if (randomBoolean()) {
                 translog.commit();
                 firstUncommitted = op + 1;
diff --git a/core/src/test/java/org/elasticsearch/index/translog/TranslogVersionTests.java b/core/src/test/java/org/elasticsearch/index/translog/TranslogVersionTests.java
index 451fdf3..283124d 100644
--- a/core/src/test/java/org/elasticsearch/index/translog/TranslogVersionTests.java
+++ b/core/src/test/java/org/elasticsearch/index/translog/TranslogVersionTests.java
@@ -45,7 +45,7 @@ public class TranslogVersionTests extends ESTestCase {
             assertThat("a version0 stream is returned", reader instanceof LegacyTranslogReader, equalTo(true));
             try (final Translog.Snapshot snapshot = reader.newSnapshot()) {
                 final Translog.Operation operation = snapshot.next();
-                assertThat("operation is the correct type correctly", operation.opType() == Translog.Operation.Type.SAVE, equalTo(true));
+                assertThat("operation is the correct type correctly", operation.opType() == Translog.Operation.Type.INDEX, equalTo(true));
                 Translog.Index op = (Translog.Index) operation;
                 assertThat(op.id(), equalTo("1"));
                 assertThat(op.type(), equalTo("doc"));
@@ -73,8 +73,8 @@ public class TranslogVersionTests extends ESTestCase {
 
                 Translog.Operation operation = snapshot.next();
 
-                assertThat("operation is the correct type correctly", operation.opType() == Translog.Operation.Type.CREATE, equalTo(true));
-                Translog.Create op = (Translog.Create) operation;
+                assertThat("operation is the correct type correctly", operation.opType() == Translog.Operation.Type.INDEX, equalTo(true));
+                Translog.Index op = (Translog.Index) operation;
                 assertThat(op.id(), equalTo("Bwiq98KFSb6YjJQGeSpeiw"));
                 assertThat(op.type(), equalTo("doc"));
                 assertThat(op.source().toUtf8(), equalTo("{\"body\": \"foo\"}"));
diff --git a/core/src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationIT.java b/core/src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationIT.java
index 348d5bc..9cf2034 100644
--- a/core/src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationIT.java
@@ -48,10 +48,10 @@ import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.suggest.SuggestRequestBuilder;
 import org.elasticsearch.action.support.IndicesOptions;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.IndexNotFoundException;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.suggest.SuggestBuilders;
 import org.elasticsearch.search.warmer.IndexWarmersMetaData;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -61,9 +61,7 @@ import static org.elasticsearch.action.percolate.PercolateSourceBuilder.docBuild
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
+import static org.hamcrest.Matchers.*;
 
 public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
 
@@ -510,7 +508,7 @@ public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
                 .setIndicesOptions(IndicesOptions.lenientExpandOpen())
                 .execute().actionGet();
         assertHitCount(response, 0l);
-
+        
         //you should still be able to run empty searches without things blowing up
         response  = client().prepareSearch()
                 .setIndicesOptions(IndicesOptions.lenientExpandOpen())
@@ -615,7 +613,7 @@ public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
         assertThat(client().admin().indices().prepareExists("bar").get().isExists(), equalTo(false));
         assertThat(client().admin().indices().prepareExists("barbaz").get().isExists(), equalTo(false));
     }
-
+    
     @Test
     public void testPutWarmer() throws Exception {
         createIndex("foobar");
@@ -624,26 +622,26 @@ public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
         assertThat(client().admin().indices().prepareGetWarmers("foobar").setWarmers("warmer1").get().getWarmers().size(), equalTo(1));
 
     }
-
+    
     @Test
     public void testPutWarmer_wildcard() throws Exception {
         createIndex("foo", "foobar", "bar", "barbaz");
         ensureYellow();
 
         verify(client().admin().indices().preparePutWarmer("warmer1").setSearchRequest(client().prepareSearch().setIndices("foo*").setQuery(QueryBuilders.matchAllQuery())), false);
-
+        
         assertThat(client().admin().indices().prepareGetWarmers("foo").setWarmers("warmer1").get().getWarmers().size(), equalTo(1));
         assertThat(client().admin().indices().prepareGetWarmers("foobar").setWarmers("warmer1").get().getWarmers().size(), equalTo(1));
         assertThat(client().admin().indices().prepareGetWarmers("bar").setWarmers("warmer1").get().getWarmers().size(), equalTo(0));
         assertThat(client().admin().indices().prepareGetWarmers("barbaz").setWarmers("warmer1").get().getWarmers().size(), equalTo(0));
 
         verify(client().admin().indices().preparePutWarmer("warmer2").setSearchRequest(client().prepareSearch().setIndices().setQuery(QueryBuilders.matchAllQuery())), false);
-
+        
         assertThat(client().admin().indices().prepareGetWarmers("foo").setWarmers("warmer2").get().getWarmers().size(), equalTo(1));
         assertThat(client().admin().indices().prepareGetWarmers("foobar").setWarmers("warmer2").get().getWarmers().size(), equalTo(1));
         assertThat(client().admin().indices().prepareGetWarmers("bar").setWarmers("warmer2").get().getWarmers().size(), equalTo(1));
         assertThat(client().admin().indices().prepareGetWarmers("barbaz").setWarmers("warmer2").get().getWarmers().size(), equalTo(1));
-
+        
     }
 
     @Test
@@ -654,7 +652,7 @@ public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
         assertThat(client().admin().indices().prepareAliasesExist("foobar_alias").setIndices("foobar").get().exists(), equalTo(true));
 
     }
-
+    
     @Test
     public void testPutAlias_wildcard() throws Exception {
         createIndex("foo", "foobar", "bar", "barbaz");
@@ -671,14 +669,14 @@ public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
         assertThat(client().admin().indices().prepareAliasesExist("foobar_alias").setIndices("foobar").get().exists(), equalTo(true));
         assertThat(client().admin().indices().prepareAliasesExist("foobar_alias").setIndices("bar").get().exists(), equalTo(true));
         assertThat(client().admin().indices().prepareAliasesExist("foobar_alias").setIndices("barbaz").get().exists(), equalTo(true));
-
+        
     }
-
+    
     @Test
     public void testDeleteWarmer() throws Exception {
-        SearchSourceBuilder source = new SearchSourceBuilder();
-        source.query(QueryBuilders.matchAllQuery());
-        IndexWarmersMetaData.Entry entry = new IndexWarmersMetaData.Entry("test1", new String[] { "typ1" }, false, new IndexWarmersMetaData.SearchSource(source));
+        IndexWarmersMetaData.Entry entry = new IndexWarmersMetaData.Entry(
+                "test1", new String[]{"typ1"}, false, new BytesArray("{\"query\" : { \"match_all\" : {}}}")
+        );
         assertAcked(prepareCreate("foobar").addCustom(new IndexWarmersMetaData(entry)));
         ensureYellow();
 
@@ -692,9 +690,9 @@ public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
     public void testDeleteWarmer_wildcard() throws Exception {
         verify(client().admin().indices().prepareDeleteWarmer().setIndices("_all").setNames("test1"), true);
 
-        SearchSourceBuilder source = new SearchSourceBuilder();
-        source.query(QueryBuilders.matchAllQuery());
-        IndexWarmersMetaData.Entry entry = new IndexWarmersMetaData.Entry("test1", new String[] { "type1" }, false, new IndexWarmersMetaData.SearchSource(source));
+        IndexWarmersMetaData.Entry entry = new IndexWarmersMetaData.Entry(
+                "test1", new String[]{"type1"}, false, new BytesArray("{\"query\" : { \"match_all\" : {}}}")
+        );
         assertAcked(prepareCreate("foo").addCustom(new IndexWarmersMetaData(entry)));
         assertAcked(prepareCreate("foobar").addCustom(new IndexWarmersMetaData(entry)));
         assertAcked(prepareCreate("bar").addCustom(new IndexWarmersMetaData(entry)));
@@ -739,7 +737,7 @@ public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
         assertThat(client().admin().indices().prepareGetMappings("foobar").get().mappings().get("foobar").get("type3"), notNullValue());
         assertThat(client().admin().indices().prepareGetMappings("bar").get().mappings().get("bar").get("type3"), notNullValue());
         assertThat(client().admin().indices().prepareGetMappings("barbaz").get().mappings().get("barbaz").get("type3"), notNullValue());
-
+        
 
         verify(client().admin().indices().preparePutMapping("c*").setType("type1").setSource("field", "type=string"), true);
 
@@ -885,7 +883,7 @@ public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
     private static void verify(ActionRequestBuilder requestBuilder, boolean fail) {
         verify(requestBuilder, fail, 0);
     }
-
+    
     private static void verify(ActionRequestBuilder requestBuilder, boolean fail, long expectedCount) {
         if (fail) {
             if (requestBuilder instanceof MultiSearchRequestBuilder) {
diff --git a/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerIT.java b/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerIT.java
index aaca771..e14cc22 100644
--- a/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerIT.java
@@ -82,7 +82,7 @@ public class IndexingMemoryControllerIT extends ESIntegTestCase {
         index("test1", "type", "1", "f", 1);
 
         // make shard the shard buffer was set to inactive size
-        final ByteSizeValue inactiveBuffer = EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER;
+        final ByteSizeValue inactiveBuffer = IndexingMemoryController.INACTIVE_SHARD_INDEXING_BUFFER;
         if (awaitBusy(() -> getIWBufferSize("test1") == inactiveBuffer.bytes()) == false) {
             fail("failed to update shard indexing buffer size for test1 index to [" + inactiveBuffer + "]; got: " + getIWBufferSize("test1"));
         }
diff --git a/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerTests.java b/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerTests.java
index f6e21db..d3d9e96 100644
--- a/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerTests.java
+++ b/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerTests.java
@@ -22,13 +22,17 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.ByteSizeUnit;
 import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.index.engine.EngineConfig;
 import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.index.translog.TranslogConfig;
 import org.elasticsearch.test.ESTestCase;
 
 import java.util.ArrayList;
 import java.util.HashMap;
+import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
+import java.util.Set;
 
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.not;
@@ -39,44 +43,28 @@ public class IndexingMemoryControllerTests extends ESTestCase {
 
         final static ByteSizeValue INACTIVE = new ByteSizeValue(-1);
 
-        final Map<ShardId, Long> translogIds = new HashMap<>();
-        final Map<ShardId, Long> translogOps = new HashMap<>();
-
         final Map<ShardId, ByteSizeValue> indexingBuffers = new HashMap<>();
         final Map<ShardId, ByteSizeValue> translogBuffers = new HashMap<>();
 
+        final Map<ShardId, Long> lastIndexTimeNanos = new HashMap<>();
+        final Set<ShardId> activeShards = new HashSet<>();
+
         long currentTimeSec = TimeValue.timeValueNanos(System.nanoTime()).seconds();
 
         public MockController(Settings settings) {
             super(Settings.builder()
                             .put(SHARD_INACTIVE_INTERVAL_TIME_SETTING, "200h") // disable it
-                            .put(SHARD_INACTIVE_TIME_SETTING, "0s") // immediate
+                            .put(SHARD_INACTIVE_TIME_SETTING, "1ms") // nearly immediate
                             .put(settings)
                             .build(),
                     null, null, 100 * 1024 * 1024); // fix jvm mem size to 100mb
         }
 
-        public void incTranslog(ShardId shard1, int id, int ops) {
-            setTranslog(shard1, translogIds.get(shard1) + id, translogOps.get(shard1) + ops);
-        }
-
-        public void setTranslog(ShardId id, long translogId, long ops) {
-            translogIds.put(id, translogId);
-            translogOps.put(id, ops);
-        }
-
         public void deleteShard(ShardId id) {
-            translogIds.remove(id);
-            translogOps.remove(id);
             indexingBuffers.remove(id);
             translogBuffers.remove(id);
         }
 
-        public void assertActive(ShardId id) {
-            assertThat(indexingBuffers.get(id), not(equalTo(INACTIVE)));
-            assertThat(translogBuffers.get(id), not(equalTo(INACTIVE)));
-        }
-
         public void assertBuffers(ShardId id, ByteSizeValue indexing, ByteSizeValue translog) {
             assertThat(indexingBuffers.get(id), equalTo(indexing));
             assertThat(translogBuffers.get(id), equalTo(translog));
@@ -94,29 +82,17 @@ public class IndexingMemoryControllerTests extends ESTestCase {
 
         @Override
         protected List<ShardId> availableShards() {
-            return new ArrayList<>(translogIds.keySet());
+            return new ArrayList<>(indexingBuffers.keySet());
         }
 
         @Override
         protected boolean shardAvailable(ShardId shardId) {
-            return translogIds.containsKey(shardId);
+            return indexingBuffers.containsKey(shardId);
         }
 
         @Override
-        protected void markShardAsInactive(ShardId shardId) {
-            indexingBuffers.put(shardId, INACTIVE);
-            translogBuffers.put(shardId, INACTIVE);
-        }
-
-        @Override
-        protected ShardIndexingStatus getTranslogStatus(ShardId shardId) {
-            if (!shardAvailable(shardId)) {
-                return null;
-            }
-            ShardIndexingStatus status = new ShardIndexingStatus();
-            status.translogId = translogIds.get(shardId);
-            status.translogNumberOfOperations = translogOps.get(shardId);
-            return status;
+        protected Boolean getShardActive(ShardId shardId) {
+            return activeShards.contains(shardId);
         }
 
         @Override
@@ -125,12 +101,34 @@ public class IndexingMemoryControllerTests extends ESTestCase {
             translogBuffers.put(shardId, shardTranslogBufferSize);
         }
 
+        @Override
+        protected Boolean checkIdle(ShardId shardId, long inactiveTimeNS) {
+            Long ns = lastIndexTimeNanos.get(shardId);
+            if (ns == null) {
+                return null;
+            } else if (currentTimeInNanos() - ns >= inactiveTimeNS) {
+                indexingBuffers.put(shardId, INACTIVE);
+                translogBuffers.put(shardId, INACTIVE);
+                activeShards.remove(shardId);
+                return true;
+            } else {
+                return false;
+            }
+        }
+
         public void incrementTimeSec(int sec) {
             currentTimeSec += sec;
         }
 
-        public void simulateFlush(ShardId shard) {
-            setTranslog(shard, translogIds.get(shard) + 1, 0);
+        public void simulateIndexing(ShardId shardId) {
+            lastIndexTimeNanos.put(shardId, currentTimeInNanos());
+            if (indexingBuffers.containsKey(shardId) == false) {
+                // First time we are seeing this shard; start it off with inactive buffers as IndexShard does:
+                indexingBuffers.put(shardId, IndexingMemoryController.INACTIVE_SHARD_INDEXING_BUFFER);
+                translogBuffers.put(shardId, IndexingMemoryController.INACTIVE_SHARD_TRANSLOG_BUFFER);
+            }
+            activeShards.add(shardId);
+            forceCheck();
         }
     }
 
@@ -139,14 +137,12 @@ public class IndexingMemoryControllerTests extends ESTestCase {
                 .put(IndexingMemoryController.INDEX_BUFFER_SIZE_SETTING, "10mb")
                 .put(IndexingMemoryController.TRANSLOG_BUFFER_SIZE_SETTING, "100kb").build());
         final ShardId shard1 = new ShardId("test", 1);
-        controller.setTranslog(shard1, randomInt(10), randomInt(10));
-        controller.forceCheck();
+        controller.simulateIndexing(shard1);
         controller.assertBuffers(shard1, new ByteSizeValue(10, ByteSizeUnit.MB), new ByteSizeValue(64, ByteSizeUnit.KB)); // translog is maxed at 64K
 
         // add another shard
         final ShardId shard2 = new ShardId("test", 2);
-        controller.setTranslog(shard2, randomInt(10), randomInt(10));
-        controller.forceCheck();
+        controller.simulateIndexing(shard2);
         controller.assertBuffers(shard1, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
         controller.assertBuffers(shard2, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
 
@@ -161,8 +157,7 @@ public class IndexingMemoryControllerTests extends ESTestCase {
 
         // add a new one
         final ShardId shard3 = new ShardId("test", 3);
-        controller.setTranslog(shard3, randomInt(10), randomInt(10));
-        controller.forceCheck();
+        controller.simulateIndexing(shard3);
         controller.assertBuffers(shard3, new ByteSizeValue(10, ByteSizeUnit.MB), new ByteSizeValue(64, ByteSizeUnit.KB)); // translog is maxed at 64K
     }
 
@@ -174,48 +169,42 @@ public class IndexingMemoryControllerTests extends ESTestCase {
                 .build());
 
         final ShardId shard1 = new ShardId("test", 1);
-        controller.setTranslog(shard1, 0, 0);
+        controller.simulateIndexing(shard1);
         final ShardId shard2 = new ShardId("test", 2);
-        controller.setTranslog(shard2, 0, 0);
-        controller.forceCheck();
+        controller.simulateIndexing(shard2);
         controller.assertBuffers(shard1, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
         controller.assertBuffers(shard2, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
 
         // index into both shards, move the clock and see that they are still active
-        controller.setTranslog(shard1, randomInt(2), randomInt(2) + 1);
-        controller.setTranslog(shard2, randomInt(2) + 1, randomInt(2));
-        // the controller doesn't know when the ops happened, so even if this is more
-        // than the inactive time the shard is still marked as active
+        controller.simulateIndexing(shard1);
+        controller.simulateIndexing(shard2);
+
         controller.incrementTimeSec(10);
         controller.forceCheck();
-        controller.assertBuffers(shard1, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
-        controller.assertBuffers(shard2, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
 
-        // index into one shard only, see other shard is made inactive correctly
-        controller.incTranslog(shard1, randomInt(2), randomInt(2) + 1);
-        controller.forceCheck(); // register what happened with the controller (shard is still active)
-        controller.incrementTimeSec(3); // increment but not enough
-        controller.forceCheck();
-        controller.assertBuffers(shard1, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
-        controller.assertBuffers(shard2, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
+        // both shards now inactive
+        controller.assertInActive(shard1);
+        controller.assertInActive(shard2);
 
-        controller.incrementTimeSec(3); // increment some more
+        // index into one shard only, see it becomes active
+        controller.simulateIndexing(shard1);
+        controller.assertBuffers(shard1, new ByteSizeValue(10, ByteSizeUnit.MB), new ByteSizeValue(64, ByteSizeUnit.KB));
+        controller.assertInActive(shard2);
+
+        controller.incrementTimeSec(3); // increment but not enough to become inactive
         controller.forceCheck();
         controller.assertBuffers(shard1, new ByteSizeValue(10, ByteSizeUnit.MB), new ByteSizeValue(64, ByteSizeUnit.KB));
         controller.assertInActive(shard2);
 
-        if (randomBoolean()) {
-            // once a shard gets inactive it will be synced flushed and a new translog generation will be made
-            controller.simulateFlush(shard2);
-            controller.forceCheck();
-            controller.assertInActive(shard2);
-        }
+        controller.incrementTimeSec(3); // increment some more
+        controller.forceCheck();
+        controller.assertInActive(shard1);
+        controller.assertInActive(shard2);
 
         // index some and shard becomes immediately active
-        controller.incTranslog(shard2, randomInt(2), 1 + randomInt(2)); // we must make sure translog ops is never 0
-        controller.forceCheck();
-        controller.assertBuffers(shard1, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
-        controller.assertBuffers(shard2, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
+        controller.simulateIndexing(shard2);
+        controller.assertInActive(shard1);
+        controller.assertBuffers(shard2, new ByteSizeValue(10, ByteSizeUnit.MB), new ByteSizeValue(64, ByteSizeUnit.KB));
     }
 
     public void testMinShardBufferSizes() {
@@ -273,10 +262,9 @@ public class IndexingMemoryControllerTests extends ESTestCase {
 
     protected void assertTwoActiveShards(MockController controller, ByteSizeValue indexBufferSize, ByteSizeValue translogBufferSize) {
         final ShardId shard1 = new ShardId("test", 1);
-        controller.setTranslog(shard1, 0, 0);
+        controller.simulateIndexing(shard1);
         final ShardId shard2 = new ShardId("test", 2);
-        controller.setTranslog(shard2, 0, 0);
-        controller.forceCheck();
+        controller.simulateIndexing(shard2);
         controller.assertBuffers(shard1, indexBufferSize, translogBufferSize);
         controller.assertBuffers(shard2, indexBufferSize, translogBufferSize);
 
diff --git a/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java b/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java
index 1ffce8d..f624700 100644
--- a/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java
+++ b/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.plugins;
 
+import com.google.common.hash.Hashing;
 import org.apache.http.impl.client.HttpClients;
 import org.apache.lucene.util.LuceneTestCase;
 import org.elasticsearch.Version;
@@ -587,7 +588,6 @@ public class PluginManagerIT extends ESIntegTestCase {
         PluginManager.checkForOfficialPlugins("analysis-phonetic");
         PluginManager.checkForOfficialPlugins("analysis-smartcn");
         PluginManager.checkForOfficialPlugins("analysis-stempel");
-        PluginManager.checkForOfficialPlugins("cloud-gce");
         PluginManager.checkForOfficialPlugins("delete-by-query");
         PluginManager.checkForOfficialPlugins("lang-expression");
         PluginManager.checkForOfficialPlugins("lang-groovy");
@@ -598,6 +598,7 @@ public class PluginManagerIT extends ESIntegTestCase {
         PluginManager.checkForOfficialPlugins("discovery-multicast");
         PluginManager.checkForOfficialPlugins("discovery-azure");
         PluginManager.checkForOfficialPlugins("discovery-ec2");
+        PluginManager.checkForOfficialPlugins("discovery-gce");
         PluginManager.checkForOfficialPlugins("repository-azure");
         PluginManager.checkForOfficialPlugins("repository-s3");
         PluginManager.checkForOfficialPlugins("store-smb");
diff --git a/core/src/test/java/org/elasticsearch/plugins/PluginManagerPermissionTests.java b/core/src/test/java/org/elasticsearch/plugins/PluginManagerPermissionTests.java
new file mode 100644
index 0000000..374c776
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/plugins/PluginManagerPermissionTests.java
@@ -0,0 +1,300 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plugins;
+
+import org.apache.lucene.util.LuceneTestCase;
+import org.elasticsearch.Version;
+import org.elasticsearch.common.cli.CliToolTestCase.CaptureOutputTerminal;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.env.Environment;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Before;
+
+import java.io.IOException;
+import java.net.URL;
+import java.nio.charset.Charset;
+import java.nio.file.*;
+import java.nio.file.attribute.BasicFileAttributes;
+import java.nio.file.attribute.PosixFilePermissions;
+import java.util.zip.ZipEntry;
+import java.util.zip.ZipOutputStream;
+
+import static org.elasticsearch.common.settings.Settings.settingsBuilder;
+import static org.elasticsearch.plugins.PluginInfoTests.writeProperties;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
+
+// there are some lucene file systems that seem to cause problems (deleted files, dirs instead of files)
+@LuceneTestCase.SuppressFileSystems("*")
+public class PluginManagerPermissionTests extends ESTestCase {
+
+    private String pluginName = "my-plugin";
+    private CaptureOutputTerminal terminal = new CaptureOutputTerminal();
+    private Environment environment;
+    private boolean supportsPermissions;
+
+    @Before
+    public void setup() {
+        Path tempDir = createTempDir();
+        Settings.Builder settingsBuilder = settingsBuilder().put("path.home", tempDir);
+        if (randomBoolean()) {
+            settingsBuilder.put("path.plugins", createTempDir());
+        }
+
+        if (randomBoolean()) {
+            settingsBuilder.put("path.conf", createTempDir());
+        }
+
+        environment = new Environment(settingsBuilder.build());
+
+        supportsPermissions = tempDir.getFileSystem().supportedFileAttributeViews().contains("posix");
+    }
+
+    public void testThatUnaccessibleBinDirectoryAbortsPluginInstallation() throws Exception {
+        assumeTrue("File system does not support permissions, skipping", supportsPermissions);
+
+        URL pluginUrl = createPlugin(true, randomBoolean());
+
+        Path binPath = environment.binFile().resolve(pluginName);
+        Files.createDirectories(binPath);
+        try {
+            Files.setPosixFilePermissions(binPath, PosixFilePermissions.fromString("---------"));
+
+            PluginManager pluginManager = new PluginManager(environment, pluginUrl, PluginManager.OutputMode.VERBOSE, TimeValue.timeValueSeconds(10));
+            pluginManager.downloadAndExtract(pluginName, terminal);
+
+            fail("Expected IOException but did not happen");
+        } catch (IOException e) {
+            assertFileNotExists(environment.pluginsFile().resolve(pluginName));
+            assertFileNotExists(environment.configFile().resolve(pluginName));
+            // exists, because of our weird permissions above
+            assertDirectoryExists(environment.binFile().resolve(pluginName));
+
+            assertThat(terminal.getTerminalOutput(), hasItem(containsString("Error copying bin directory ")));
+        } finally {
+            Files.setPosixFilePermissions(binPath, PosixFilePermissions.fromString("rwxrwxrwx"));
+        }
+    }
+
+    public void testThatUnaccessiblePluginConfigDirectoryAbortsPluginInstallation() throws Exception {
+        assumeTrue("File system does not support permissions, skipping", supportsPermissions);
+
+        URL pluginUrl = createPlugin(randomBoolean(), true);
+
+        Path path = environment.configFile().resolve(pluginName);
+        Files.createDirectories(path);
+        Files.createFile(path.resolve("my-custom-config.yaml"));
+        Path binPath = environment.binFile().resolve(pluginName);
+        Files.createDirectories(binPath);
+
+        try {
+            Files.setPosixFilePermissions(path.resolve("my-custom-config.yaml"), PosixFilePermissions.fromString("---------"));
+            Files.setPosixFilePermissions(path, PosixFilePermissions.fromString("---------"));
+
+            PluginManager pluginManager = new PluginManager(environment, pluginUrl, PluginManager.OutputMode.VERBOSE, TimeValue.timeValueSeconds(10));
+            pluginManager.downloadAndExtract(pluginName, terminal);
+
+            fail("Expected IOException but did not happen, terminal output was " + terminal.getTerminalOutput());
+        } catch (IOException e) {
+            assertFileNotExists(environment.pluginsFile().resolve(pluginName));
+            assertFileNotExists(environment.binFile().resolve(pluginName));
+            // exists, because of our weird permissions above
+            assertDirectoryExists(environment.configFile().resolve(pluginName));
+
+            assertThat(terminal.getTerminalOutput(), hasItem(containsString("Error copying config directory ")));
+        } finally {
+            Files.setPosixFilePermissions(path, PosixFilePermissions.fromString("rwxrwxrwx"));
+            Files.setPosixFilePermissions(path.resolve("my-custom-config.yaml"), PosixFilePermissions.fromString("rwxrwxrwx"));
+        }
+    }
+
+    // config/bin are not writable, but the plugin does not need to put anything into it
+    public void testThatPluginWithoutBinAndConfigWorksEvenIfPermissionsAreWrong() throws Exception {
+        assumeTrue("File system does not support permissions, skipping", supportsPermissions);
+
+        URL pluginUrl = createPlugin(false, false);
+        Path path = environment.configFile().resolve(pluginName);
+        Files.createDirectories(path);
+        Files.createFile(path.resolve("my-custom-config.yaml"));
+        Path binPath = environment.binFile().resolve(pluginName);
+        Files.createDirectories(binPath);
+
+        try {
+            Files.setPosixFilePermissions(path.resolve("my-custom-config.yaml"), PosixFilePermissions.fromString("---------"));
+            Files.setPosixFilePermissions(path, PosixFilePermissions.fromString("---------"));
+            Files.setPosixFilePermissions(binPath, PosixFilePermissions.fromString("---------"));
+
+            PluginManager pluginManager = new PluginManager(environment, pluginUrl, PluginManager.OutputMode.VERBOSE, TimeValue.timeValueSeconds(10));
+            pluginManager.downloadAndExtract(pluginName, terminal);
+        } finally {
+            Files.setPosixFilePermissions(binPath, PosixFilePermissions.fromString("rwxrwxrwx"));
+            Files.setPosixFilePermissions(path, PosixFilePermissions.fromString("rwxrwxrwx"));
+            Files.setPosixFilePermissions(path.resolve("my-custom-config.yaml"), PosixFilePermissions.fromString("rwxrwxrwx"));
+        }
+
+    }
+
+    // plugins directory no accessible, should leave no other left over directories
+    public void testThatNonWritablePluginsDirectoryLeavesNoLeftOver() throws Exception {
+        assumeTrue("File system does not support permissions, skipping", supportsPermissions);
+
+        URL pluginUrl = createPlugin(true, true);
+        Files.createDirectories(environment.pluginsFile());
+
+        try {
+            Files.setPosixFilePermissions(environment.pluginsFile(), PosixFilePermissions.fromString("---------"));
+            PluginManager pluginManager = new PluginManager(environment, pluginUrl, PluginManager.OutputMode.VERBOSE, TimeValue.timeValueSeconds(10));
+            try {
+                pluginManager.downloadAndExtract(pluginName, terminal);
+                fail("Expected IOException due to read-only plugins/ directory");
+            } catch (IOException e) {
+                assertFileNotExists(environment.binFile().resolve(pluginName));
+                assertFileNotExists(environment.configFile().resolve(pluginName));
+
+                Files.setPosixFilePermissions(environment.pluginsFile(), PosixFilePermissions.fromString("rwxrwxrwx"));
+                assertDirectoryExists(environment.pluginsFile());
+                assertFileNotExists(environment.pluginsFile().resolve(pluginName));
+            }
+        } finally {
+            Files.setPosixFilePermissions(environment.pluginsFile(), PosixFilePermissions.fromString("rwxrwxrwx"));
+        }
+    }
+
+    public void testThatUnwriteableBackupFilesInConfigurationDirectoryAreReplaced() throws Exception {
+        assumeTrue("File system does not support permissions, skipping", supportsPermissions);
+
+        boolean pluginContainsExecutables = randomBoolean();
+        URL pluginUrl = createPlugin(pluginContainsExecutables, true);
+        Files.createDirectories(environment.configFile().resolve(pluginName));
+
+        Path configFile = environment.configFile().resolve(pluginName).resolve("my-custom-config.yaml");
+        Files.createFile(configFile);
+        Path backupConfigFile = environment.configFile().resolve(pluginName).resolve("my-custom-config.yaml.new");
+        Files.createFile(backupConfigFile);
+        Files.write(backupConfigFile, "foo".getBytes(Charset.forName("UTF-8")));
+
+        PluginManager pluginManager = new PluginManager(environment, pluginUrl, PluginManager.OutputMode.VERBOSE, TimeValue.timeValueSeconds(10));
+        try {
+            Files.setPosixFilePermissions(backupConfigFile, PosixFilePermissions.fromString("---------"));
+
+            pluginManager.downloadAndExtract(pluginName, terminal);
+
+            if (pluginContainsExecutables) {
+                assertDirectoryExists(environment.binFile().resolve(pluginName));
+            }
+            assertDirectoryExists(environment.pluginsFile().resolve(pluginName));
+            assertDirectoryExists(environment.configFile().resolve(pluginName));
+
+            assertFileExists(backupConfigFile);
+            Files.setPosixFilePermissions(backupConfigFile, PosixFilePermissions.fromString("rw-rw-rw-"));
+            String content = new String(Files.readAllBytes(backupConfigFile), Charset.forName("UTF-8"));
+            assertThat(content, is(not("foo")));
+        } finally {
+            Files.setPosixFilePermissions(backupConfigFile, PosixFilePermissions.fromString("rw-rw-rw-"));
+        }
+    }
+
+    public void testThatConfigDirectoryBeingAFileAbortsInstallationAndDoesNotAccidentallyDeleteThisFile() throws Exception {
+        assumeTrue("File system does not support permissions, skipping", supportsPermissions);
+
+        Files.createDirectories(environment.configFile());
+        Files.createFile(environment.configFile().resolve(pluginName));
+        URL pluginUrl = createPlugin(randomBoolean(), true);
+
+        PluginManager pluginManager = new PluginManager(environment, pluginUrl, PluginManager.OutputMode.VERBOSE, TimeValue.timeValueSeconds(10));
+
+        try {
+            pluginManager.downloadAndExtract(pluginName, terminal);
+            fail("Expected plugin installation to fail, but didnt");
+        } catch (IOException e) {
+            assertFileExists(environment.configFile().resolve(pluginName));
+            assertFileNotExists(environment.binFile().resolve(pluginName));
+            assertFileNotExists(environment.pluginsFile().resolve(pluginName));
+        }
+    }
+
+    public void testThatBinDirectoryBeingAFileAbortsInstallationAndDoesNotAccidentallyDeleteThisFile() throws Exception {
+        assumeTrue("File system does not support permissions, skipping", supportsPermissions);
+
+        Files.createDirectories(environment.binFile());
+        Files.createFile(environment.binFile().resolve(pluginName));
+        URL pluginUrl = createPlugin(true, randomBoolean());
+
+        PluginManager pluginManager = new PluginManager(environment, pluginUrl, PluginManager.OutputMode.VERBOSE, TimeValue.timeValueSeconds(10));
+
+        try {
+            pluginManager.downloadAndExtract(pluginName, terminal);
+            fail("Expected plugin installation to fail, but didnt");
+        } catch (IOException e) {
+            assertFileExists(environment.binFile().resolve(pluginName));
+            assertFileNotExists(environment.configFile().resolve(pluginName));
+            assertFileNotExists(environment.pluginsFile().resolve(pluginName));
+        }
+    }
+
+
+    private URL createPlugin(boolean withBinDir, boolean withConfigDir) throws IOException {
+        final Path structure = createTempDir().resolve("fake-plugin");
+        writeProperties(structure, "description", "fake desc",
+                "version", "1.0",
+                "elasticsearch.version", Version.CURRENT.toString(),
+                "jvm", "true",
+                "java.version", "1.7",
+                "name", pluginName,
+                "classname", pluginName);
+        if (withBinDir) {
+            // create bin dir
+            Path binDir = structure.resolve("bin");
+            Files.createDirectory(binDir);
+            Files.setPosixFilePermissions(binDir, PosixFilePermissions.fromString("rwxr-xr-x"));
+
+            // create executable
+            Path executable = binDir.resolve("my-binary");
+            Files.createFile(executable);
+            Files.setPosixFilePermissions(executable, PosixFilePermissions.fromString("rwxr-xr-x"));
+        }
+        if (withConfigDir) {
+            // create bin dir
+            Path configDir = structure.resolve("config");
+            Files.createDirectory(configDir);
+            Files.setPosixFilePermissions(configDir, PosixFilePermissions.fromString("rwxr-xr-x"));
+
+            // create config file
+            Path configFile = configDir.resolve("my-custom-config.yaml");
+            Files.createFile(configFile);
+            Files.write(configFile, "my custom config content".getBytes(Charset.forName("UTF-8")));
+            Files.setPosixFilePermissions(configFile, PosixFilePermissions.fromString("rw-r--r--"));
+        }
+
+        Path zip = createTempDir().resolve(structure.getFileName() + ".zip");
+        try (ZipOutputStream stream = new ZipOutputStream(Files.newOutputStream(zip))) {
+            Files.walkFileTree(structure, new SimpleFileVisitor<Path>() {
+                @Override
+                public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException {
+                    stream.putNextEntry(new ZipEntry(structure.relativize(file).toString()));
+                    Files.copy(file, stream);
+                    return FileVisitResult.CONTINUE;
+                }
+            });
+        }
+        return zip.toUri().toURL();
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/AggregationsBinaryIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/AggregationsBinaryIT.java
index e5634fe..631f705 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/AggregationsBinaryIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/AggregationsBinaryIT.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.search.aggregations;
 
-import org.apache.lucene.util.LuceneTestCase.AwaitsFix;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.client.Requests;
@@ -42,8 +41,6 @@ import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.core.IsNull.notNullValue;
 
 @ESIntegTestCase.SuiteScopeTestCase
-@AwaitsFix(bugUrl = "needs fixing after the search request refactor. Do we need agg binary?")
-// NO RELEASE
 public class AggregationsBinaryIT extends ESIntegTestCase {
 
     private static final String STRING_FIELD_NAME = "s_value";
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/ParsingIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/ParsingIT.java
index 87307c0..64f80d6 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/ParsingIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/ParsingIT.java
@@ -31,151 +31,150 @@ import java.util.regex.Pattern;
 
 public class ParsingIT extends ESIntegTestCase {
 
-    // NORELEASE move these tests to unit tests when aggs refactoring is done
-//    @Test(expected=SearchPhaseExecutionException.class)
-//    public void testTwoTypes() throws Exception {
-//        createIndex("idx");
-//        ensureGreen();
-//        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
-//            .startObject()
-//                .startObject("in_stock")
-//                    .startObject("filter")
-//                        .startObject("range")
-//                            .startObject("stock")
-//                                .field("gt", 0)
-//                            .endObject()
-//                        .endObject()
-//                    .endObject()
-//                    .startObject("terms")
-//                        .field("field", "stock")
-//                    .endObject()
-//                .endObject()
-//            .endObject()).execute().actionGet();
-//    }
-//
-//    @Test(expected=SearchPhaseExecutionException.class)
-//    public void testTwoAggs() throws Exception {
-//        createIndex("idx");
-//        ensureGreen();
-//        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
-//            .startObject()
-//                .startObject("by_date")
-//                    .startObject("date_histogram")
-//                        .field("field", "timestamp")
-//                        .field("interval", "month")
-//                    .endObject()
-//                    .startObject("aggs")
-//                        .startObject("tag_count")
-//                            .startObject("cardinality")
-//                                .field("field", "tag")
-//                            .endObject()
-//                        .endObject()
-//                    .endObject()
-//                    .startObject("aggs") // 2nd "aggs": illegal
-//                        .startObject("tag_count2")
-//                            .startObject("cardinality")
-//                                .field("field", "tag")
-//                            .endObject()
-//                        .endObject()
-//                    .endObject()
-//            .endObject()).execute().actionGet();
-//    }
-//
-//    @Test(expected=SearchPhaseExecutionException.class)
-//    public void testInvalidAggregationName() throws Exception {
-//
-//        Matcher matcher = Pattern.compile("[^\\[\\]>]+").matcher("");
-//        String name;
-//        SecureRandom rand = new SecureRandom();
-//        int len = randomIntBetween(1, 5);
-//        char[] word = new char[len];
-//        while(true) {
-//            for (int i = 0; i < word.length; i++) {
-//                word[i] = (char) rand.nextInt(127);
-//            }
-//            name = String.valueOf(word);
-//            if (!matcher.reset(name).matches()) {
-//                break;
-//            }
-//        }
-//
-//        createIndex("idx");
-//        ensureGreen();
-//        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
-//            .startObject()
-//                .startObject(name)
-//                    .startObject("filter")
-//                        .startObject("range")
-//                            .startObject("stock")
-//                                .field("gt", 0)
-//                            .endObject()
-//                        .endObject()
-//                    .endObject()
-//            .endObject()).execute().actionGet();
-//    }
-//
-//    @Test(expected=SearchPhaseExecutionException.class)
-//    public void testSameAggregationName() throws Exception {
-//        createIndex("idx");
-//        ensureGreen();
-//        final String name = RandomStrings.randomAsciiOfLength(getRandom(), 10);
-//        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
-//            .startObject()
-//                .startObject(name)
-//                    .startObject("terms")
-//                        .field("field", "a")
-//                    .endObject()
-//                .endObject()
-//                .startObject(name)
-//                    .startObject("terms")
-//                        .field("field", "b")
-//                    .endObject()
-//                .endObject()
-//            .endObject()).execute().actionGet();
-//    }
-//
-//    @Test(expected=SearchPhaseExecutionException.class)
-//    public void testMissingName() throws Exception {
-//        createIndex("idx");
-//        ensureGreen();
-//        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
-//            .startObject()
-//                .startObject("by_date")
-//                    .startObject("date_histogram")
-//                        .field("field", "timestamp")
-//                        .field("interval", "month")
-//                    .endObject()
-//                    .startObject("aggs")
-//                        // the aggregation name is missing
-//                        //.startObject("tag_count")
-//                            .startObject("cardinality")
-//                                .field("field", "tag")
-//                            .endObject()
-//                        //.endObject()
-//                    .endObject()
-//            .endObject()).execute().actionGet();
-//    }
-//
-//    @Test(expected=SearchPhaseExecutionException.class)
-//    public void testMissingType() throws Exception {
-//        createIndex("idx");
-//        ensureGreen();
-//        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
-//            .startObject()
-//                .startObject("by_date")
-//                    .startObject("date_histogram")
-//                        .field("field", "timestamp")
-//                        .field("interval", "month")
-//                    .endObject()
-//                    .startObject("aggs")
-//                        .startObject("tag_count")
-//                            // the aggregation type is missing
-//                            //.startObject("cardinality")
-//                                .field("field", "tag")
-//                            //.endObject()
-//                        .endObject()
-//                    .endObject()
-//            .endObject()).execute().actionGet();
-//    }
+    @Test(expected=SearchPhaseExecutionException.class)
+    public void testTwoTypes() throws Exception {
+        createIndex("idx");
+        ensureGreen();
+        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
+            .startObject()
+                .startObject("in_stock")
+                    .startObject("filter")
+                        .startObject("range")
+                            .startObject("stock")
+                                .field("gt", 0)
+                            .endObject()
+                        .endObject()
+                    .endObject()
+                    .startObject("terms")
+                        .field("field", "stock")
+                    .endObject()
+                .endObject()
+            .endObject()).execute().actionGet();
+    }
+
+    @Test(expected=SearchPhaseExecutionException.class)
+    public void testTwoAggs() throws Exception {
+        createIndex("idx");
+        ensureGreen();
+        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
+            .startObject()
+                .startObject("by_date")
+                    .startObject("date_histogram")
+                        .field("field", "timestamp")
+                        .field("interval", "month")
+                    .endObject()
+                    .startObject("aggs")
+                        .startObject("tag_count")
+                            .startObject("cardinality")
+                                .field("field", "tag")
+                            .endObject()
+                        .endObject()
+                    .endObject()
+                    .startObject("aggs") // 2nd "aggs": illegal
+                        .startObject("tag_count2")
+                            .startObject("cardinality")
+                                .field("field", "tag")
+                            .endObject()
+                        .endObject()
+                    .endObject()
+            .endObject()).execute().actionGet();
+    }
+
+    @Test(expected=SearchPhaseExecutionException.class)
+    public void testInvalidAggregationName() throws Exception {
+
+        Matcher matcher = Pattern.compile("[^\\[\\]>]+").matcher("");
+        String name;
+        SecureRandom rand = new SecureRandom();
+        int len = randomIntBetween(1, 5);
+        char[] word = new char[len];
+        while(true) {
+            for (int i = 0; i < word.length; i++) {
+                word[i] = (char) rand.nextInt(127);
+            }
+            name = String.valueOf(word);
+            if (!matcher.reset(name).matches()) {
+                break;
+            }
+        }
+
+        createIndex("idx");
+        ensureGreen();
+        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
+            .startObject()
+                .startObject(name)
+                    .startObject("filter")
+                        .startObject("range")
+                            .startObject("stock")
+                                .field("gt", 0)
+                            .endObject()
+                        .endObject()
+                    .endObject()
+            .endObject()).execute().actionGet();
+    }
+
+    @Test(expected=SearchPhaseExecutionException.class)
+    public void testSameAggregationName() throws Exception {
+        createIndex("idx");
+        ensureGreen();
+        final String name = RandomStrings.randomAsciiOfLength(getRandom(), 10);
+        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
+            .startObject()
+                .startObject(name)
+                    .startObject("terms")
+                        .field("field", "a")
+                    .endObject()
+                .endObject()
+                .startObject(name)
+                    .startObject("terms")
+                        .field("field", "b")
+                    .endObject()
+                .endObject()
+            .endObject()).execute().actionGet();
+    }
+
+    @Test(expected=SearchPhaseExecutionException.class)
+    public void testMissingName() throws Exception {
+        createIndex("idx");
+        ensureGreen();
+        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
+            .startObject()
+                .startObject("by_date")
+                    .startObject("date_histogram")
+                        .field("field", "timestamp")
+                        .field("interval", "month")
+                    .endObject()
+                    .startObject("aggs")
+                        // the aggregation name is missing
+                        //.startObject("tag_count")
+                            .startObject("cardinality")
+                                .field("field", "tag")
+                            .endObject()
+                        //.endObject()
+                    .endObject()
+            .endObject()).execute().actionGet();
+    }
+
+    @Test(expected=SearchPhaseExecutionException.class)
+    public void testMissingType() throws Exception {
+        createIndex("idx");
+        ensureGreen();
+        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
+            .startObject()
+                .startObject("by_date")
+                    .startObject("date_histogram")
+                        .field("field", "timestamp")
+                        .field("interval", "month")
+                    .endObject()
+                    .startObject("aggs")
+                        .startObject("tag_count")
+                            // the aggregation type is missing
+                            //.startObject("cardinality")
+                                .field("field", "tag")
+                            //.endObject()
+                        .endObject()
+                    .endObject()
+            .endObject()).execute().actionGet();
+    }
 
 }
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java
index 71c7ccd..ae01ea1 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java
@@ -24,6 +24,7 @@ import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.search.SearchType;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.plugins.Plugin;
@@ -539,7 +540,7 @@ public class TopHitsIT extends ESIntegTestCase {
                                 .field(TERMS_AGGS_FIELD)
                                 .subAggregation(
                                         topHits("hits").setSize(1)
-                                            .highlighter(new HighlightBuilder().field("text"))
+                                            .addHighlightedField("text")
                                             .setExplain(true)
                                             .addFieldDataField("field1")
                                             .addScriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap()))
@@ -602,39 +603,38 @@ public class TopHitsIT extends ESIntegTestCase {
         }
     }
 
-    // @Test
-    // public void testFailWithSubAgg() throws Exception {
-    // String source = "{\n" +
-    // "  \"aggs\": {\n" +
-    // "    \"top-tags\": {\n" +
-    // "      \"terms\": {\n" +
-    // "        \"field\": \"tags\"\n" +
-    // "      },\n" +
-    // "      \"aggs\": {\n" +
-    // "        \"top_tags_hits\": {\n" +
-    // "          \"top_hits\": {},\n" +
-    // "          \"aggs\": {\n" +
-    // "            \"max\": {\n" +
-    // "              \"max\": {\n" +
-    // "                \"field\": \"age\"\n" +
-    // "              }\n" +
-    // "            }\n" +
-    // "          }\n" +
-    // "        }\n" +
-    // "      }\n" +
-    // "    }\n" +
-    // "  }\n" +
-    // "}";
-    // try {
-    // client().prepareSearch("idx").setTypes("type")
-    // .setSource(new BytesArray(source))
-    // .get();
-    // fail();
-    // } catch (SearchPhaseExecutionException e) {
-    // assertThat(e.toString(),
-    // containsString("Aggregator [top_tags_hits] of type [top_hits] cannot accept sub-aggregations"));
-    // }
-    // } NORELEASE this needs to be tested in a top_hits aggregations unit test
+    @Test
+    public void testFailWithSubAgg() throws Exception {
+        String source = "{\n" +
+                "  \"aggs\": {\n" +
+                "    \"top-tags\": {\n" +
+                "      \"terms\": {\n" +
+                "        \"field\": \"tags\"\n" +
+                "      },\n" +
+                "      \"aggs\": {\n" +
+                "        \"top_tags_hits\": {\n" +
+                "          \"top_hits\": {},\n" +
+                "          \"aggs\": {\n" +
+                "            \"max\": {\n" +
+                "              \"max\": {\n" +
+                "                \"field\": \"age\"\n" +
+                "              }\n" +
+                "            }\n" +
+                "          }\n" +
+                "        }\n" +
+                "      }\n" +
+                "    }\n" +
+                "  }\n" +
+                "}";
+        try {
+            client().prepareSearch("idx").setTypes("type")
+                    .setSource(new BytesArray(source))
+                            .get();
+            fail();
+        } catch (SearchPhaseExecutionException e) {
+            assertThat(e.toString(), containsString("Aggregator [top_tags_hits] of type [top_hits] cannot accept sub-aggregations"));
+        }
+    }
 
     @Test
     public void testEmptyIndex() throws Exception {
@@ -862,7 +862,7 @@ public class TopHitsIT extends ESIntegTestCase {
                 .setQuery(nestedQuery("comments", matchQuery("comments.message", "comment").queryName("test")))
                 .addAggregation(
                         nested("to-comments").path("comments").subAggregation(
-                                topHits("top-comments").setSize(1).highlighter(new HighlightBuilder().field(hlField)).setExplain(true)
+                                topHits("top-comments").setSize(1).addHighlightedField(hlField).setExplain(true)
                                                 .addFieldDataField("comments.user")
                                         .addScriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap())).setFetchSource("message", null)
                                         .setVersion(true).addSort("comments.date", SortOrder.ASC))).get();
@@ -914,7 +914,7 @@ public class TopHitsIT extends ESIntegTestCase {
                                         nested("to-comments")
                                                 .path("comments")
                                                 .subAggregation(topHits("comments")
-                                                        .highlighter(new HighlightBuilder().field(new HighlightBuilder.Field("comments.message").highlightQuery(matchQuery("comments.message", "text"))))
+                                                        .addHighlightedField(new HighlightBuilder.Field("comments.message").highlightQuery(matchQuery("comments.message", "text")))
                                                         .addSort("comments.id", SortOrder.ASC))
                                 )
                 )
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java
index fe942dc..ac4fcf8 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java
@@ -19,12 +19,10 @@
 
 package org.elasticsearch.search.aggregations.pipeline.moving.avg;
 
-
-import com.google.common.collect.EvictingQueue;
-
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.common.collect.EvictingQueue;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.Bucket;
@@ -42,11 +40,7 @@ import org.junit.Test;
 import java.util.*;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.avg;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.histogram;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.max;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.min;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.range;
+import static org.elasticsearch.search.aggregations.AggregationBuilders.*;
 import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilders.derivative;
 import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilders.movingAvg;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
@@ -170,7 +164,7 @@ public class MovAvgIT extends ESIntegTestCase {
      */
     private void setupExpected(MovAvgType type, MetricTarget target, int windowSize) {
         ArrayList<Double> values = new ArrayList<>(numBuckets);
-        EvictingQueue<Double> window = EvictingQueue.create(windowSize);
+        EvictingQueue<Double> window = new EvictingQueue<>(windowSize);
 
         for (PipelineAggregationHelperTests.MockBucket mockBucket : mockHisto) {
             double metricValue;
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgUnitTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgUnitTests.java
index 65e44b9..11c5e40 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgUnitTests.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgUnitTests.java
@@ -19,8 +19,8 @@
 
 package org.elasticsearch.search.aggregations.pipeline.moving.avg;
 
-import com.google.common.collect.EvictingQueue;
 import org.elasticsearch.common.ParseFieldMatcher;
+import org.elasticsearch.common.collect.EvictingQueue;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.*;
 import org.elasticsearch.test.ESTestCase;
 import org.junit.Test;
@@ -39,7 +39,7 @@ public class MovAvgUnitTests extends ESTestCase {
         int numValues = randomIntBetween(1, 100);
         int windowSize = randomIntBetween(1, 50);
 
-        EvictingQueue<Double> window = EvictingQueue.create(windowSize);
+        EvictingQueue<Double> window = new EvictingQueue<>(windowSize);
         for (int i = 0; i < numValues; i++) {
 
             double randValue = randomDouble();
@@ -68,7 +68,7 @@ public class MovAvgUnitTests extends ESTestCase {
         int windowSize = randomIntBetween(1, 50);
         int numPredictions = randomIntBetween(1, 50);
 
-        EvictingQueue<Double> window = EvictingQueue.create(windowSize);
+        EvictingQueue<Double> window = new EvictingQueue<>(windowSize);
         for (int i = 0; i < windowSize; i++) {
             window.offer(randomDouble());
         }
@@ -94,7 +94,7 @@ public class MovAvgUnitTests extends ESTestCase {
         int numValues = randomIntBetween(1, 100);
         int windowSize = randomIntBetween(1, 50);
 
-        EvictingQueue<Double> window = EvictingQueue.create(windowSize);
+        EvictingQueue<Double> window = new EvictingQueue<>(windowSize);
         for (int i = 0; i < numValues; i++) {
             double randValue = randomDouble();
 
@@ -126,7 +126,7 @@ public class MovAvgUnitTests extends ESTestCase {
         int windowSize = randomIntBetween(1, 50);
         int numPredictions = randomIntBetween(1,50);
 
-        EvictingQueue<Double> window = EvictingQueue.create(windowSize);
+        EvictingQueue<Double> window = new EvictingQueue<>(windowSize);
         for (int i = 0; i < windowSize; i++) {
             window.offer(randomDouble());
         }
@@ -158,7 +158,7 @@ public class MovAvgUnitTests extends ESTestCase {
         int numValues = randomIntBetween(1, 100);
         int windowSize = randomIntBetween(1, 50);
 
-        EvictingQueue<Double> window = EvictingQueue.create(windowSize);
+        EvictingQueue<Double> window = new EvictingQueue<>(windowSize);
         for (int i = 0; i < numValues; i++) {
             double randValue = randomDouble();
 
@@ -193,7 +193,7 @@ public class MovAvgUnitTests extends ESTestCase {
         int windowSize = randomIntBetween(1, 50);
         int numPredictions = randomIntBetween(1,50);
 
-        EvictingQueue<Double> window = EvictingQueue.create(windowSize);
+        EvictingQueue<Double> window = new EvictingQueue<>(windowSize);
         for (int i = 0; i < windowSize; i++) {
             window.offer(randomDouble());
         }
@@ -227,7 +227,7 @@ public class MovAvgUnitTests extends ESTestCase {
         int numValues = randomIntBetween(1, 100);
         int windowSize = randomIntBetween(1, 50);
 
-        EvictingQueue<Double> window = EvictingQueue.create(windowSize);
+        EvictingQueue<Double> window = new EvictingQueue<>(windowSize);
         for (int i = 0; i < numValues; i++) {
             double randValue = randomDouble();
 
@@ -276,7 +276,7 @@ public class MovAvgUnitTests extends ESTestCase {
         int windowSize = randomIntBetween(1, 50);
         int numPredictions = randomIntBetween(1, 50);
 
-        EvictingQueue<Double> window = EvictingQueue.create(windowSize);
+        EvictingQueue<Double> window = new EvictingQueue<>(windowSize);
         for (int i = 0; i < windowSize; i++) {
             window.offer(randomDouble());
         }
@@ -323,7 +323,7 @@ public class MovAvgUnitTests extends ESTestCase {
 
         int windowSize = randomIntBetween(period * 2, 50); // HW requires at least two periods of data
 
-        EvictingQueue<Double> window = EvictingQueue.create(windowSize);
+        EvictingQueue<Double> window = new EvictingQueue<>(windowSize);
         for (int i = 0; i < windowSize; i++) {
             window.offer(randomDouble());
         }
@@ -392,7 +392,7 @@ public class MovAvgUnitTests extends ESTestCase {
         int windowSize = randomIntBetween(period * 2, 50); // HW requires at least two periods of data
         int numPredictions = randomIntBetween(1, 50);
 
-        EvictingQueue<Double> window = EvictingQueue.create(windowSize);
+        EvictingQueue<Double> window = new EvictingQueue<>(windowSize);
         for (int i = 0; i < windowSize; i++) {
             window.offer(randomDouble());
         }
@@ -465,7 +465,7 @@ public class MovAvgUnitTests extends ESTestCase {
 
         int windowSize = randomIntBetween(period * 2, 50); // HW requires at least two periods of data
 
-        EvictingQueue<Double> window = EvictingQueue.create(windowSize);
+        EvictingQueue<Double> window = new EvictingQueue<>(windowSize);
         for (int i = 0; i < windowSize; i++) {
             window.offer(randomDouble());
         }
@@ -533,7 +533,7 @@ public class MovAvgUnitTests extends ESTestCase {
         int windowSize = randomIntBetween(period * 2, 50); // HW requires at least two periods of data
         int numPredictions = randomIntBetween(1, 50);
 
-        EvictingQueue<Double> window = EvictingQueue.create(windowSize);
+        EvictingQueue<Double> window = new EvictingQueue<>(windowSize);
         for (int i = 0; i < windowSize; i++) {
             window.offer(randomDouble());
         }
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java
index af68667..ccd4dcb 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java
@@ -19,10 +19,10 @@
 
 package org.elasticsearch.search.aggregations.pipeline.serialdiff;
 
-import com.google.common.collect.EvictingQueue;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.common.collect.EvictingQueue;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
 import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
@@ -160,7 +160,7 @@ public class SerialDiffIT extends ESIntegTestCase {
      */
     private void setupExpected(MetricTarget target) {
         ArrayList<Double> values = new ArrayList<>(numBuckets);
-        EvictingQueue<Double> lagWindow = EvictingQueue.create(lag);
+        EvictingQueue<Double> lagWindow = new EvictingQueue<>(lag);
 
         int counter = 0;
         for (PipelineAggregationHelperTests.MockBucket mockBucket : mockHisto) {
diff --git a/core/src/test/java/org/elasticsearch/search/basic/TransportSearchFailuresIT.java b/core/src/test/java/org/elasticsearch/search/basic/TransportSearchFailuresIT.java
index db0b5c2..e5cc0ee 100644
--- a/core/src/test/java/org/elasticsearch/search/basic/TransportSearchFailuresIT.java
+++ b/core/src/test/java/org/elasticsearch/search/basic/TransportSearchFailuresIT.java
@@ -19,15 +19,28 @@
 
 package org.elasticsearch.search.basic;
 
+import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.WriteConsistencyLevel;
+import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
+import org.elasticsearch.action.admin.cluster.health.ClusterHealthStatus;
+import org.elasticsearch.action.admin.indices.refresh.RefreshResponse;
+import org.elasticsearch.action.search.SearchPhaseExecutionException;
+import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.Requests;
+import org.elasticsearch.common.Priority;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
+import org.junit.Test;
 
 import java.io.IOException;
 
+import static org.elasticsearch.client.Requests.*;
+import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
+import static org.hamcrest.Matchers.*;
 
 public class TransportSearchFailuresIT extends ESIntegTestCase {
 
@@ -36,66 +49,65 @@ public class TransportSearchFailuresIT extends ESIntegTestCase {
         return 1;
     }
 
-    // NORELEASE can this still be tested? if so, how?
-//    @Test
-//    public void testFailedSearchWithWrongQuery() throws Exception {
-//        logger.info("Start Testing failed search with wrong query");
-//        assertAcked(prepareCreate("test", 1, settingsBuilder().put("routing.hash.type", "simple")));
-//        ensureYellow();
-//
-//        NumShards test = getNumShards("test");
-//
-//        for (int i = 0; i < 100; i++) {
-//            index(client(), Integer.toString(i), "test", i);
-//        }
-//        RefreshResponse refreshResponse = client().admin().indices().refresh(refreshRequest("test")).actionGet();
-//        assertThat(refreshResponse.getTotalShards(), equalTo(test.totalNumShards));
-//        assertThat(refreshResponse.getSuccessfulShards(), equalTo(test.numPrimaries));
-//        assertThat(refreshResponse.getFailedShards(), equalTo(0));
-//        for (int i = 0; i < 5; i++) {
-//            try {
-//                SearchResponse searchResponse = client().search(searchRequest("test").source(new BytesArray("{ xxx }"))).actionGet();
-//                assertThat(searchResponse.getTotalShards(), equalTo(test.numPrimaries));
-//                assertThat(searchResponse.getSuccessfulShards(), equalTo(0));
-//                assertThat(searchResponse.getFailedShards(), equalTo(test.numPrimaries));
-//                fail("search should fail");
-//            } catch (ElasticsearchException e) {
-//                assertThat(e.unwrapCause(), instanceOf(SearchPhaseExecutionException.class));
-//                // all is well
-//            }
-//        }
-//
-//        allowNodes("test", 2);
-//        assertThat(client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForNodes(">=2").execute().actionGet().isTimedOut(), equalTo(false));
-//
-//        logger.info("Running Cluster Health");
-//        ClusterHealthResponse clusterHealth = client().admin().cluster().health(clusterHealthRequest("test")
-//                .waitForYellowStatus().waitForRelocatingShards(0).waitForActiveShards(test.totalNumShards)).actionGet();
-//        logger.info("Done Cluster Health, status " + clusterHealth.getStatus());
-//        assertThat(clusterHealth.isTimedOut(), equalTo(false));
-//        assertThat(clusterHealth.getStatus(), anyOf(equalTo(ClusterHealthStatus.YELLOW), equalTo(ClusterHealthStatus.GREEN)));
-//        assertThat(clusterHealth.getActiveShards(), equalTo(test.totalNumShards));
-//
-//        refreshResponse = client().admin().indices().refresh(refreshRequest("test")).actionGet();
-//        assertThat(refreshResponse.getTotalShards(), equalTo(test.totalNumShards));
-//        assertThat(refreshResponse.getSuccessfulShards(), equalTo(test.totalNumShards));
-//        assertThat(refreshResponse.getFailedShards(), equalTo(0));
-//
-//        for (int i = 0; i < 5; i++) {
-//            try {
-//                SearchResponse searchResponse = client().search(searchRequest("test").source(new BytesArray("{ xxx }"))).actionGet();
-//                assertThat(searchResponse.getTotalShards(), equalTo(test.numPrimaries));
-//                assertThat(searchResponse.getSuccessfulShards(), equalTo(0));
-//                assertThat(searchResponse.getFailedShards(), equalTo(test.numPrimaries));
-//                fail("search should fail");
-//            } catch (ElasticsearchException e) {
-//                assertThat(e.unwrapCause(), instanceOf(SearchPhaseExecutionException.class));
-//                // all is well
-//            }
-//        }
-//
-//        logger.info("Done Testing failed search");
-//    }
+    @Test
+    public void testFailedSearchWithWrongQuery() throws Exception {
+        logger.info("Start Testing failed search with wrong query");
+        assertAcked(prepareCreate("test", 1, settingsBuilder().put("routing.hash.type", "simple")));
+        ensureYellow();
+
+        NumShards test = getNumShards("test");
+
+        for (int i = 0; i < 100; i++) {
+            index(client(), Integer.toString(i), "test", i);
+        }
+        RefreshResponse refreshResponse = client().admin().indices().refresh(refreshRequest("test")).actionGet();
+        assertThat(refreshResponse.getTotalShards(), equalTo(test.totalNumShards));
+        assertThat(refreshResponse.getSuccessfulShards(), equalTo(test.numPrimaries));
+        assertThat(refreshResponse.getFailedShards(), equalTo(0));
+        for (int i = 0; i < 5; i++) {
+            try {
+                SearchResponse searchResponse = client().search(searchRequest("test").source(new BytesArray("{ xxx }"))).actionGet();
+                assertThat(searchResponse.getTotalShards(), equalTo(test.numPrimaries));
+                assertThat(searchResponse.getSuccessfulShards(), equalTo(0));
+                assertThat(searchResponse.getFailedShards(), equalTo(test.numPrimaries));
+                fail("search should fail");
+            } catch (ElasticsearchException e) {
+                assertThat(e.unwrapCause(), instanceOf(SearchPhaseExecutionException.class));
+                // all is well
+            }
+        }
+
+        allowNodes("test", 2);
+        assertThat(client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForNodes(">=2").execute().actionGet().isTimedOut(), equalTo(false));
+
+        logger.info("Running Cluster Health");
+        ClusterHealthResponse clusterHealth = client().admin().cluster().health(clusterHealthRequest("test")
+                .waitForYellowStatus().waitForRelocatingShards(0).waitForActiveShards(test.totalNumShards)).actionGet();
+        logger.info("Done Cluster Health, status " + clusterHealth.getStatus());
+        assertThat(clusterHealth.isTimedOut(), equalTo(false));
+        assertThat(clusterHealth.getStatus(), anyOf(equalTo(ClusterHealthStatus.YELLOW), equalTo(ClusterHealthStatus.GREEN)));
+        assertThat(clusterHealth.getActiveShards(), equalTo(test.totalNumShards));
+
+        refreshResponse = client().admin().indices().refresh(refreshRequest("test")).actionGet();
+        assertThat(refreshResponse.getTotalShards(), equalTo(test.totalNumShards));
+        assertThat(refreshResponse.getSuccessfulShards(), equalTo(test.totalNumShards));
+        assertThat(refreshResponse.getFailedShards(), equalTo(0));
+
+        for (int i = 0; i < 5; i++) {
+            try {
+                SearchResponse searchResponse = client().search(searchRequest("test").source(new BytesArray("{ xxx }"))).actionGet();
+                assertThat(searchResponse.getTotalShards(), equalTo(test.numPrimaries));
+                assertThat(searchResponse.getSuccessfulShards(), equalTo(0));
+                assertThat(searchResponse.getFailedShards(), equalTo(test.numPrimaries));
+                fail("search should fail");
+            } catch (ElasticsearchException e) {
+                assertThat(e.unwrapCause(), instanceOf(SearchPhaseExecutionException.class));
+                // all is well
+            }
+        }
+
+        logger.info("Done Testing failed search");
+    }
 
     private void index(Client client, String id, String nameValue, int age) throws IOException {
         client.index(Requests.indexRequest("test").type("type1").id(id).source(source(id, nameValue, age)).consistencyLevel(WriteConsistencyLevel.ONE)).actionGet();
diff --git a/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java b/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java
index 5e6c7bc..a9b41ce 100644
--- a/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java
@@ -20,9 +20,12 @@
 package org.elasticsearch.search.basic;
 
 
+import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.search.MultiSearchResponse;
+import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.client.Requests;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.xcontent.XContentBuilder;
@@ -49,6 +52,7 @@ import static org.elasticsearch.action.search.SearchType.DFS_QUERY_AND_FETCH;
 import static org.elasticsearch.action.search.SearchType.DFS_QUERY_THEN_FETCH;
 import static org.elasticsearch.action.search.SearchType.QUERY_AND_FETCH;
 import static org.elasticsearch.action.search.SearchType.QUERY_THEN_FETCH;
+
 import static org.elasticsearch.client.Requests.createIndexRequest;
 import static org.elasticsearch.client.Requests.searchRequest;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
@@ -60,6 +64,7 @@ import static org.elasticsearch.index.query.QueryBuilders.termQuery;
 import static org.elasticsearch.search.builder.SearchSourceBuilder.searchSource;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
 import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.instanceOf;
 import static org.hamcrest.Matchers.lessThanOrEqualTo;
 import static org.hamcrest.Matchers.notNullValue;
 import static org.hamcrest.Matchers.nullValue;
@@ -126,7 +131,7 @@ public class TransportTwoNodesSearchIT extends ESIntegTestCase {
             .settings(settingsBuilder))
             .actionGet();
         ensureGreen();
-
+        
         // we need to have age (ie number of repeats of "test" term) high enough
         // to produce the same 8-bit norm for all docs here, so that
         // the tf is basically the entire score (assuming idf is fixed, which
@@ -330,7 +335,7 @@ public class TransportTwoNodesSearchIT extends ESIntegTestCase {
 
         do {
             searchResponse = client().prepareSearchScroll(searchResponse.getScrollId()).setScroll("10m").get();
-
+    
             assertThat(searchResponse.getHits().totalHits(), equalTo(100l));
             assertThat(searchResponse.getHits().hits().length, lessThanOrEqualTo(40));
             for (int i = 0; i < searchResponse.getHits().hits().length; i++) {
@@ -365,25 +370,25 @@ public class TransportTwoNodesSearchIT extends ESIntegTestCase {
         assertThat(all.getDocCount(), equalTo(100l));
     }
 
-//    @Test
-//    public void testFailedSearchWithWrongQuery() throws Exception {
-//        prepareData();
-//
-//        NumShards test = getNumShards("test");
-//
-//        logger.info("Start Testing failed search with wrong query");
-//        try {
-//            SearchResponse searchResponse = client().search(searchRequest("test").source(new BytesArray("{ xxx }"))).actionGet();
-//            assertThat(searchResponse.getTotalShards(), equalTo(test.numPrimaries));
-//            assertThat(searchResponse.getSuccessfulShards(), equalTo(0));
-//            assertThat(searchResponse.getFailedShards(), equalTo(test.numPrimaries));
-//            fail("search should fail");
-//        } catch (ElasticsearchException e) {
-//            assertThat(e.unwrapCause(), instanceOf(SearchPhaseExecutionException.class));
-//            // all is well
-//        }
-//        logger.info("Done Testing failed search");
-    // } NORELEASE can this still be tested? if so, how?
+    @Test
+    public void testFailedSearchWithWrongQuery() throws Exception {
+        prepareData();
+
+        NumShards test = getNumShards("test");
+
+        logger.info("Start Testing failed search with wrong query");
+        try {
+            SearchResponse searchResponse = client().search(searchRequest("test").source(new BytesArray("{ xxx }"))).actionGet();
+            assertThat(searchResponse.getTotalShards(), equalTo(test.numPrimaries));
+            assertThat(searchResponse.getSuccessfulShards(), equalTo(0));
+            assertThat(searchResponse.getFailedShards(), equalTo(test.numPrimaries));
+            fail("search should fail");
+        } catch (ElasticsearchException e) {
+            assertThat(e.unwrapCause(), instanceOf(SearchPhaseExecutionException.class));
+            // all is well
+        }
+        logger.info("Done Testing failed search");
+    }
 
     @Test
     public void testFailedSearchWithWrongFrom() throws Exception {
diff --git a/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java b/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java
index 89a9af4..80a683c 100644
--- a/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java
@@ -16,410 +16,75 @@
  * specific language governing permissions and limitations
  * under the License.
  */
-
 package org.elasticsearch.search.builder;
 
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Injector;
-import org.elasticsearch.common.inject.ModulesBuilder;
-import org.elasticsearch.common.inject.multibindings.Multibinder;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.NamedWriteableAwareStreamInput;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsModule;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.AbstractQueryTestCase;
-import org.elasticsearch.index.query.EmptyQueryBuilder;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
-import org.elasticsearch.indices.IndicesModule;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.AggregationBuilders;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder.InnerHit;
-import org.elasticsearch.search.fetch.source.FetchSourceContext;
-import org.elasticsearch.search.highlight.HighlightBuilder;
-import org.elasticsearch.search.rescore.RescoreBuilder;
-import org.elasticsearch.search.sort.SortBuilders;
-import org.elasticsearch.search.sort.SortOrder;
-import org.elasticsearch.search.suggest.SuggestBuilder;
-import org.elasticsearch.search.suggest.SuggestBuilders;
+import org.elasticsearch.common.xcontent.json.JsonXContent;
 import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.threadpool.ThreadPoolModule;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
 import org.junit.Test;
 
 import java.io.IOException;
-import java.util.ArrayList;
 import java.util.List;
-import java.util.concurrent.TimeUnit;
+import java.util.Map;
 
-import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.*;
 
 public class SearchSourceBuilderTests extends ESTestCase {
 
-    private static Injector injector;
+    SearchSourceBuilder builder = new SearchSourceBuilder();
 
-    private static NamedWriteableRegistry namedWriteableRegistry;
+    @Test // issue #6632
+    public void testThatSearchSourceBuilderIncludesExcludesAreAppliedCorrectly() throws Exception {
+        builder.fetchSource("foo", null);
+        assertIncludes(builder, "foo");
+        assertExcludes(builder);
 
-    private static IndicesQueriesRegistry indicesQueriesRegistry;
+        builder.fetchSource(null, "foo");
+        assertIncludes(builder);
+        assertExcludes(builder, "foo");
 
-    @BeforeClass
-    public static void init() throws IOException {
-        Settings settings = Settings.settingsBuilder()
-                .put("name", SearchSourceBuilderTests.class.toString())
-                .put("path.home", createTempDir())
-                .build();
-        injector = new ModulesBuilder().add(
-                new SettingsModule(settings),
-                new ThreadPoolModule(new ThreadPool(settings)),
-                new IndicesModule(settings) {
-                    @Override
-                    public void configure() {
-                        // skip services
-                        bindQueryParsersExtension();
-                    }
-                },
-                new AbstractModule() {
-                    @Override
-                    protected void configure() {
-                        Multibinder.newSetBinder(binder(), ScoreFunctionParser.class);
-                        bind(NamedWriteableRegistry.class).asEagerSingleton();
-                    }
-                }
-        ).createInjector();
-        indicesQueriesRegistry = injector.getInstance(IndicesQueriesRegistry.class);
-        namedWriteableRegistry = injector.getInstance(NamedWriteableRegistry.class);
-    }
+        builder.fetchSource(null, new String[]{"foo"});
+        assertIncludes(builder);
+        assertExcludes(builder, "foo");
 
-    @AfterClass
-    public static void afterClass() throws Exception {
-        terminate(injector.getInstance(ThreadPool.class));
-        injector = null;
-        namedWriteableRegistry = null;
-        indicesQueriesRegistry = null;
-    }
+        builder.fetchSource(new String[]{"foo"}, null);
+        assertIncludes(builder, "foo");
+        assertExcludes(builder);
 
-    protected final SearchSourceBuilder createSearchSourceBuilder() throws IOException {
-        SearchSourceBuilder builder = new SearchSourceBuilder();
-        if (randomBoolean()) {
-            builder.from(randomIntBetween(0, 10000));
-        }
-        if (randomBoolean()) {
-            builder.size(randomIntBetween(0, 10000));
-        }
-        if (randomBoolean()) {
-            builder.explain(randomBoolean());
-        }
-        if (randomBoolean()) {
-            builder.version(randomBoolean());
-        }
-        if (randomBoolean()) {
-            builder.trackScores(randomBoolean());
-        }
-        if (randomBoolean()) {
-            builder.minScore(randomFloat() * 1000);
-        }
-        if (randomBoolean()) {
-            builder.timeout(new TimeValue(randomIntBetween(1, 100), randomFrom(TimeUnit.values())));
-        }
-        if (randomBoolean()) {
-            builder.terminateAfter(randomIntBetween(1, 100000));
-        }
-        // if (randomBoolean()) {
-        // builder.defaultRescoreWindowSize(randomIntBetween(1, 100));
-        // }
-        if (randomBoolean()) {
-            int fieldsSize = randomInt(25);
-            List<String> fields = new ArrayList<>(fieldsSize);
-            for (int i = 0; i < fieldsSize; i++) {
-                fields.add(randomAsciiOfLengthBetween(5, 50));
-            }
-            builder.fields(fields);
-        }
-        if (randomBoolean()) {
-            int fieldDataFieldsSize = randomInt(25);
-            for (int i = 0; i < fieldDataFieldsSize; i++) {
-                builder.fieldDataField(randomAsciiOfLengthBetween(5, 50));
-            }
-        }
-        if (randomBoolean()) {
-            int scriptFieldsSize = randomInt(25);
-            for (int i = 0; i < scriptFieldsSize; i++) {
-                if (randomBoolean()) {
-                    builder.scriptField(randomAsciiOfLengthBetween(5, 50), new Script("foo"), randomBoolean());
-                } else {
-                    builder.scriptField(randomAsciiOfLengthBetween(5, 50), new Script("foo"));
-                }
-            }
-        }
-        if (randomBoolean()) {
-            FetchSourceContext fetchSourceContext;
-            int branch = randomInt(5);
-            String[] includes = new String[randomIntBetween(0, 20)];
-            for (int i = 0; i < includes.length; i++) {
-                includes[i] = randomAsciiOfLengthBetween(5, 20);
-            }
-            String[] excludes = new String[randomIntBetween(0, 20)];
-            for (int i = 0; i < excludes.length; i++) {
-                excludes[i] = randomAsciiOfLengthBetween(5, 20);
-            }
-            switch (branch) {
-            case 0:
-                fetchSourceContext = new FetchSourceContext(randomBoolean());
-                break;
-            case 1:
-                fetchSourceContext = new FetchSourceContext(includes, excludes);
-                break;
-            case 2:
-                fetchSourceContext = new FetchSourceContext(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20));
-                break;
-            case 3:
-                fetchSourceContext = new FetchSourceContext(true, includes, excludes, randomBoolean());
-                break;
-            case 4:
-                fetchSourceContext = new FetchSourceContext(includes);
-                break;
-            case 5:
-                fetchSourceContext = new FetchSourceContext(randomAsciiOfLengthBetween(5, 20));
-                break;
-            default:
-                throw new IllegalStateException();
-            }
-            builder.fetchSource(fetchSourceContext);
-        }
-        if (randomBoolean()) {
-            int size = randomIntBetween(0, 20);
-            List<String> statsGroups = new ArrayList<>(size);
-            for (int i = 0; i < size; i++) {
-                statsGroups.add(randomAsciiOfLengthBetween(5, 20));
-            }
-            builder.stats(statsGroups);
-        }
-        if (randomBoolean()) {
-            int indexBoostSize = randomIntBetween(1, 10);
-            for (int i = 0; i < indexBoostSize; i++) {
-                builder.indexBoost(randomAsciiOfLengthBetween(5, 20), randomFloat() * 10);
-            }
-        }
-        if (randomBoolean()) {
-            // NORELEASE make RandomQueryBuilder work outside of the
-            // AbstractQueryTestCase
-            // builder.query(RandomQueryBuilder.createQuery(getRandom()));
-            builder.query(QueryBuilders.termQuery(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20)));
-        }
-        if (randomBoolean()) {
-            // NORELEASE make RandomQueryBuilder work outside of the
-            // AbstractQueryTestCase
-            // builder.postFilter(RandomQueryBuilder.createQuery(getRandom()));
-            builder.postFilter(QueryBuilders.termQuery(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20)));
-        }
-        if (randomBoolean()) {
-            int numSorts = randomIntBetween(1, 5);
-            for (int i = 0; i < numSorts; i++) {
-                int branch = randomInt(5);
-                switch (branch) {
-                case 0:
-                    builder.sort(SortBuilders.fieldSort(randomAsciiOfLengthBetween(5, 20)).order(randomFrom(SortOrder.values())));
-                    break;
-                case 1:
-                    builder.sort(SortBuilders.geoDistanceSort(randomAsciiOfLengthBetween(5, 20))
-                            .geohashes(AbstractQueryTestCase.randomGeohash(1, 12)).order(randomFrom(SortOrder.values())));
-                    break;
-                case 2:
-                    builder.sort(SortBuilders.scoreSort().order(randomFrom(SortOrder.values())));
-                    break;
-                case 3:
-                    builder.sort(SortBuilders.scriptSort(new Script("foo"), "number").order(randomFrom(SortOrder.values())));
-                    break;
-                case 4:
-                    builder.sort(randomAsciiOfLengthBetween(5, 20));
-                    break;
-                case 5:
-                    builder.sort(randomAsciiOfLengthBetween(5, 20), randomFrom(SortOrder.values()));
-                    break;
-                }
-            }
-        }
-        if (randomBoolean()) {
-            // NORELEASE need a random highlight builder method
-            builder.highlighter(new HighlightBuilder().field(randomAsciiOfLengthBetween(5, 20)));
-        }
-        if (randomBoolean()) {
-            // NORELEASE need a random suggest builder method
-            builder.suggest(new SuggestBuilder().setText(randomAsciiOfLengthBetween(1, 5)).addSuggestion(
-                    SuggestBuilders.termSuggestion(randomAsciiOfLengthBetween(1, 5))));
-        }
-        if (randomBoolean()) {
-            // NORELEASE need a random inner hits builder method
-            InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-            InnerHit innerHit = new InnerHit();
-            innerHit.field(randomAsciiOfLengthBetween(5, 20));
-            innerHitsBuilder.addNestedInnerHits(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20), innerHit);
-            builder.innerHits(innerHitsBuilder);
-        }
-        if (randomBoolean()) {
-            int numRescores = randomIntBetween(1, 5);
-            for (int i = 0; i < numRescores; i++) {
-                // NORELEASE need a random rescore builder method
-                RescoreBuilder rescoreBuilder = new RescoreBuilder();
-                rescoreBuilder.rescorer(RescoreBuilder.queryRescorer(QueryBuilders.termQuery(randomAsciiOfLengthBetween(5, 20),
-                        randomAsciiOfLengthBetween(5, 20))));
-                builder.addRescorer(rescoreBuilder);
-            }
-        }
-        if (randomBoolean()) {
-            // NORELEASE need a random aggregation builder method
-            builder.aggregation(AggregationBuilders.avg(randomAsciiOfLengthBetween(5, 20)));
-        }
-        if (true) {
-            // NORELEASE need a method to randomly build content for ext
-            XContentBuilder xContentBuilder = XContentFactory.jsonBuilder();
-            xContentBuilder.startObject();
-            xContentBuilder.field("term_vectors_fetch", randomAsciiOfLengthBetween(5, 20));
-            xContentBuilder.endObject();
-            builder.ext(xContentBuilder);
-        }
-        return builder;
-    }
-
-    @Test
-    public void testFromXContent() throws IOException {
-        SearchSourceBuilder testBuilder = createSearchSourceBuilder();
-        String builderAsString = testBuilder.toString();
-        assertParseSearchSource(testBuilder, builderAsString);
-    }
-
-    private void assertParseSearchSource(SearchSourceBuilder testBuilder, String builderAsString) throws IOException {
-        XContentParser parser = XContentFactory.xContent(builderAsString).createParser(builderAsString);
-        SearchSourceBuilder newBuilder = SearchSourceBuilder.parseSearchSource(parser, createParseContext(parser));
-        assertNotSame(testBuilder, newBuilder);
-        assertEquals(testBuilder, newBuilder);
-        assertEquals(testBuilder.hashCode(), newBuilder.hashCode());
-    }
+        builder.fetchSource("foo", "bar");
+        assertIncludes(builder, "foo");
+        assertExcludes(builder, "bar");
 
-    private static QueryParseContext createParseContext(XContentParser parser) {
-        QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
-        context.reset(parser);
-        context.parseFieldMatcher(ParseFieldMatcher.STRICT);
-        return context;
+        builder.fetchSource(new String[]{"foo"}, new String[]{"bar", "baz"});
+        assertIncludes(builder, "foo");
+        assertExcludes(builder, "bar", "baz");
     }
 
-    @Test
-    public void testSerialization() throws IOException {
-        SearchSourceBuilder testBuilder = createSearchSourceBuilder();
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            testBuilder.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                SearchSourceBuilder deserializedBuilder = SearchSourceBuilder.readSearchSourceFrom(in);
-                assertEquals(deserializedBuilder, testBuilder);
-                assertEquals(deserializedBuilder.hashCode(), testBuilder.hashCode());
-                assertNotSame(deserializedBuilder, testBuilder);
-            }
-        }
+    private void assertIncludes(SearchSourceBuilder builder, String... elems) throws IOException {
+        assertFieldValues(builder, "includes", elems);
     }
 
-    @Test
-    public void testEqualsAndHashcode() throws IOException {
-        SearchSourceBuilder firstBuilder = createSearchSourceBuilder();
-        assertFalse("source builder is equal to null", firstBuilder.equals(null));
-        assertFalse("source builder is equal to incompatible type", firstBuilder.equals(""));
-        assertTrue("source builder is not equal to self", firstBuilder.equals(firstBuilder));
-        assertThat("same source builder's hashcode returns different values if called multiple times", firstBuilder.hashCode(),
-                equalTo(firstBuilder.hashCode()));
-
-        SearchSourceBuilder secondBuilder = copyBuilder(firstBuilder);
-        assertTrue("source builder is not equal to self", secondBuilder.equals(secondBuilder));
-        assertTrue("source builder is not equal to its copy", firstBuilder.equals(secondBuilder));
-        assertTrue("source builder is not symmetric", secondBuilder.equals(firstBuilder));
-        assertThat("source builder copy's hashcode is different from original hashcode", secondBuilder.hashCode(), equalTo(firstBuilder.hashCode()));
-
-        SearchSourceBuilder thirdBuilder = copyBuilder(secondBuilder);
-        assertTrue("source builder is not equal to self", thirdBuilder.equals(thirdBuilder));
-        assertTrue("source builder is not equal to its copy", secondBuilder.equals(thirdBuilder));
-        assertThat("source builder copy's hashcode is different from original hashcode", secondBuilder.hashCode(), equalTo(thirdBuilder.hashCode()));
-        assertTrue("equals is not transitive", firstBuilder.equals(thirdBuilder));
-        assertThat("source builder copy's hashcode is different from original hashcode", firstBuilder.hashCode(), equalTo(thirdBuilder.hashCode()));
-        assertTrue("equals is not symmetric", thirdBuilder.equals(secondBuilder));
-        assertTrue("equals is not symmetric", thirdBuilder.equals(firstBuilder));
+    private void assertExcludes(SearchSourceBuilder builder, String... elems) throws IOException {
+        assertFieldValues(builder, "excludes", elems);
     }
 
-    //we use the streaming infra to create a copy of the query provided as argument
-    protected SearchSourceBuilder copyBuilder(SearchSourceBuilder builder) throws IOException {
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            builder.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                return SearchSourceBuilder.readSearchSourceFrom(in);
-            }
-        }
-    }
+    private void assertFieldValues(SearchSourceBuilder builder, String fieldName, String... elems) throws IOException {
+        Map<String, Object> map = getSourceMap(builder);
 
-    public void testParseIncludeExclude() throws IOException {
-        {
-            String restContent = " { \"_source\": { \"includes\": \"include\", \"excludes\": \"*.field2\"}}";
-            try (XContentParser parser = XContentFactory.xContent(restContent).createParser(restContent)) {
-                SearchSourceBuilder searchSourceBuilder = SearchSourceBuilder.parseSearchSource(parser, createParseContext(parser));
-                assertArrayEquals(new String[]{"*.field2" }, searchSourceBuilder.fetchSource().excludes());
-                assertArrayEquals(new String[]{"include" }, searchSourceBuilder.fetchSource().includes());
-            }
-        }
-        {
-            String restContent = " { \"_source\": false}";
-            try (XContentParser parser = XContentFactory.xContent(restContent).createParser(restContent)) {
-                SearchSourceBuilder searchSourceBuilder = SearchSourceBuilder.parseSearchSource(parser, createParseContext(parser));
-                assertArrayEquals(new String[]{}, searchSourceBuilder.fetchSource().excludes());
-                assertArrayEquals(new String[]{}, searchSourceBuilder.fetchSource().includes());
-                assertFalse(searchSourceBuilder.fetchSource().fetchSource());
-            }
-        }
+        assertThat(map, hasKey(fieldName));
+        assertThat(map.get(fieldName), is(instanceOf(List.class)));
+        List<String> castedList = (List<String>) map.get(fieldName);
+        assertThat(castedList, hasSize(elems.length));
+        assertThat(castedList, hasItems(elems));
     }
 
-    @Test
-    public void testParseSort() throws IOException {
-        {
-            String restContent = " { \"sort\": \"foo\"}";
-            try (XContentParser parser = XContentFactory.xContent(restContent).createParser(restContent)) {
-                SearchSourceBuilder searchSourceBuilder = SearchSourceBuilder.parseSearchSource(parser, createParseContext(parser));
-                assertEquals(1, searchSourceBuilder.sorts().size());
-                assertEquals("{\"foo\":{}}", searchSourceBuilder.sorts().get(0).toUtf8());
-            }
-        }
-
-        {
-            String restContent = "{\"sort\" : [\n" +
-                    "        { \"post_date\" : {\"order\" : \"asc\"}},\n" +
-                    "        \"user\",\n" +
-                    "        { \"name\" : \"desc\" },\n" +
-                    "        { \"age\" : \"desc\" },\n" +
-                    "        \"_score\"\n" +
-                    "    ]}";
-            try (XContentParser parser = XContentFactory.xContent(restContent).createParser(restContent)) {
-                SearchSourceBuilder searchSourceBuilder = SearchSourceBuilder.parseSearchSource(parser, createParseContext(parser));
-                assertEquals(5, searchSourceBuilder.sorts().size());
-                assertEquals("{\"post_date\":{\"order\":\"asc\"}}", searchSourceBuilder.sorts().get(0).toUtf8());
-                assertEquals("\"user\"", searchSourceBuilder.sorts().get(1).toUtf8());
-                assertEquals("{\"name\":\"desc\"}", searchSourceBuilder.sorts().get(2).toUtf8());
-                assertEquals("{\"age\":\"desc\"}", searchSourceBuilder.sorts().get(3).toUtf8());
-                assertEquals("\"_score\"", searchSourceBuilder.sorts().get(4).toUtf8());
-            }
+    private Map<String, Object> getSourceMap(SearchSourceBuilder builder) throws IOException {
+        Map<String, Object> data;
+        try (XContentParser parser = JsonXContent.jsonXContent.createParser(builder.toString())) {
+            data = parser.map();
         }
+        assertThat(data, hasKey("_source"));
+        return (Map<String, Object>) data.get("_source");
     }
 
-    @Test
-    public void testEmptyPostFilter() throws IOException {
-        SearchSourceBuilder builder = new SearchSourceBuilder();
-        builder.postFilter(EmptyQueryBuilder.PROTOTYPE);
-        String query = "{ \"post_filter\": {} }";
-        assertParseSearchSource(builder, query);
-    }
 }
diff --git a/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java b/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java
index dadb0f6..09f33f6 100644
--- a/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java
@@ -27,6 +27,7 @@ import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.search.SearchType;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.lucene.search.function.CombineFunction;
 import org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery;
 import org.elasticsearch.common.settings.Settings;
@@ -43,7 +44,6 @@ import org.elasticsearch.search.aggregations.AggregationBuilders;
 import org.elasticsearch.search.aggregations.bucket.filter.Filter;
 import org.elasticsearch.search.aggregations.bucket.global.Global;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.sort.SortBuilders;
 import org.elasticsearch.search.sort.SortOrder;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -53,43 +53,15 @@ import org.hamcrest.Matchers;
 import org.junit.Test;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-import java.util.Set;
+import java.util.*;
 
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
-import static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.hasParentQuery;
-import static org.elasticsearch.index.query.QueryBuilders.idsQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.multiMatchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.notQuery;
-import static org.elasticsearch.index.query.QueryBuilders.prefixQuery;
-import static org.elasticsearch.index.query.QueryBuilders.queryStringQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termsQuery;
+import static org.elasticsearch.index.query.QueryBuilders.*;
 import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.fieldValueFactorFunction;
 import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.weightFactorFunction;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHits;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasId;
-import static org.hamcrest.Matchers.anyOf;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThanOrEqualTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.notNullValue;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
 
 /**
  *
@@ -1481,9 +1453,14 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
 
         SearchResponse resp;
         resp = client().prepareSearch("test")
-                .setSource(new SearchSourceBuilder().query(QueryBuilders.hasChildQuery("posts", QueryBuilders.matchQuery("field", "bar"))))
-                .get();
+                .setSource(new BytesArray("{\"query\": {\"has_child\": {\"type\": \"posts\", \"query\": {\"match\": {\"field\": \"bar\"}}}}}")).get();
         assertHitCount(resp, 1L);
+
+        // Now reverse the order for the type after the query
+        resp = client().prepareSearch("test")
+                .setSource(new BytesArray("{\"query\": {\"has_child\": {\"query\": {\"match\": {\"field\": \"bar\"}}, \"type\": \"posts\"}}}")).get();
+        assertHitCount(resp, 1L);
+
     }
 
     @Test
diff --git a/core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginIT.java b/core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginIT.java
index 66ba1fe..f3aff00 100644
--- a/core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginIT.java
+++ b/core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginIT.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.search.fetch;
 
 import com.google.common.collect.ImmutableMap;
-
 import org.apache.lucene.index.PostingsEnum;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.util.BytesRef;
@@ -28,14 +27,15 @@ import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.termvectors.TermVectorsRequest;
 import org.elasticsearch.action.termvectors.TermVectorsResponse;
 import org.elasticsearch.common.Priority;
-import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.termvectors.TermVectorsService;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.search.SearchHitField;
 import org.elasticsearch.search.SearchModule;
 import org.elasticsearch.search.SearchParseElement;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.internal.InternalSearchHit;
 import org.elasticsearch.search.internal.InternalSearchHitField;
 import org.elasticsearch.search.internal.SearchContext;
@@ -51,9 +51,10 @@ import java.util.HashMap;
 import java.util.Map;
 
 import static org.elasticsearch.client.Requests.indexRequest;
+import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.Matchers.equalTo;
 
 /**
  *
@@ -89,16 +90,14 @@ public class FetchSubPhasePluginIT extends ESIntegTestCase {
 
         client().admin().indices().prepareRefresh().execute().actionGet();
 
-        XContentBuilder extSource = jsonBuilder().startObject()
+        String searchSource = jsonBuilder().startObject()
                 .field("term_vectors_fetch", "test")
-                .endObject();
-         SearchResponse response = client().prepareSearch().setSource(new SearchSourceBuilder().ext(extSource)).get();
+                .endObject().string();
+        SearchResponse response = client().prepareSearch().setSource(new BytesArray(searchSource)).get();
         assertSearchResponse(response);
         assertThat(((Map<String, Integer>) response.getHits().getAt(0).field("term_vectors_fetch").getValues().get(0)).get("i"), equalTo(2));
-        assertThat(((Map<String, Integer>) response.getHits().getAt(0).field("term_vectors_fetch").getValues().get(0)).get("am"),
-                equalTo(2));
-        assertThat(((Map<String, Integer>) response.getHits().getAt(0).field("term_vectors_fetch").getValues().get(0)).get("sam"),
-                equalTo(1));
+        assertThat(((Map<String, Integer>) response.getHits().getAt(0).field("term_vectors_fetch").getValues().get(0)).get("am"), equalTo(2));
+        assertThat(((Map<String, Integer>) response.getHits().getAt(0).field("term_vectors_fetch").getValues().get(0)).get("sam"), equalTo(1));
     }
 
     public static class FetchTermVectorsPlugin extends Plugin {
diff --git a/core/src/test/java/org/elasticsearch/search/fetch/FieldDataFieldsTests.java b/core/src/test/java/org/elasticsearch/search/fetch/FieldDataFieldsTests.java
deleted file mode 100644
index 7fce514..0000000
--- a/core/src/test/java/org/elasticsearch/search/fetch/FieldDataFieldsTests.java
+++ /dev/null
@@ -1,75 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.fetch;
-
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.bytes.BytesArray;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsParseElement;
-import org.elasticsearch.search.internal.SearchContext;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.TestSearchContext;
-import org.junit.Test;
-
-public class FieldDataFieldsTests extends ESTestCase {
-
-    public void testValidFieldDataFieldString() throws Exception {
-        FieldDataFieldsParseElement parseElement = new FieldDataFieldsParseElement();
-
-        BytesArray data = new BytesArray(new BytesRef("{\"fielddata_fields\": \"foobar\"}"));
-        XContentParser parser = XContentFactory.xContent(data).createParser(data);
-        parser.nextToken();
-        parser.nextToken();
-        parser.nextToken();
-        SearchContext context = new TestSearchContext();
-        parseElement.parse(parser, context);
-    }
-
-    public void testValidFieldDataFieldArray() throws Exception {
-        FieldDataFieldsParseElement parseElement = new FieldDataFieldsParseElement();
-
-        BytesArray data = new BytesArray(new BytesRef("{\"fielddata_fields\": [ \"foo\", \"bar\", \"baz\"]}}"));
-        XContentParser parser = XContentFactory.xContent(data).createParser(data);
-        parser.nextToken();
-        parser.nextToken();
-        parser.nextToken();
-        SearchContext context = new TestSearchContext();
-        parseElement.parse(parser, context);
-    }
-
-    @Test(expected = IllegalStateException.class)
-    public void testInvalidFieldDataField() throws Exception {
-        FieldDataFieldsParseElement parseElement = new FieldDataFieldsParseElement();
-
-        BytesArray data;
-        if (randomBoolean()) {
-            data = new BytesArray(new BytesRef("{\"fielddata_fields\": {}}}"));
-        } else {
-            data = new BytesArray(new BytesRef("{\"fielddata_fields\": 1.0}}"));
-        }
-        XContentParser parser = XContentFactory.xContent(data).createParser(data);
-        parser.nextToken();
-        parser.nextToken();
-        parser.nextToken();
-        SearchContext context = new TestSearchContext();
-        parseElement.parse(parser, context);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java b/core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java
index fe53317..983fb52 100644
--- a/core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java
+++ b/core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java
@@ -27,8 +27,11 @@ import org.elasticsearch.action.search.SearchType;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.lucene.search.function.CombineFunction;
 import org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.functionscore.FunctionScoreQueryBuilder;
+import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilder;
 import org.elasticsearch.search.MultiValueMode;
 import org.elasticsearch.search.SearchHits;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -45,23 +48,11 @@ import java.util.concurrent.ExecutionException;
 import static org.elasticsearch.client.Requests.indexRequest;
 import static org.elasticsearch.client.Requests.searchRequest;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.functionScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.exponentialDecayFunction;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.gaussDecayFunction;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.linearDecayFunction;
+import static org.elasticsearch.index.query.QueryBuilders.*;
+import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.*;
 import static org.elasticsearch.search.builder.SearchSourceBuilder.searchSource;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertOrderedSearchHits;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHits;
-import static org.hamcrest.Matchers.anyOf;
-import static org.hamcrest.Matchers.closeTo;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.isOneOf;
-import static org.hamcrest.Matchers.lessThan;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
 
 
 public class DecayFunctionScoreIT extends ESIntegTestCase {
@@ -442,10 +433,10 @@ public class DecayFunctionScoreIT extends ESIntegTestCase {
         SearchResponse sr = response.actionGet();
         assertOrderedSearchHits(sr, "2", "1");
     }
-
+    
     @Test
     public void testParseDateMath() throws Exception {
-
+        
         assertAcked(prepareCreate("test").addMapping(
                 "type1",
                 jsonBuilder().startObject().startObject("type1").startObject("properties").startObject("test").field("type", "string")
@@ -466,7 +457,7 @@ public class DecayFunctionScoreIT extends ESIntegTestCase {
 
         assertNoFailures(sr);
         assertOrderedSearchHits(sr, "1", "2");
-
+        
         sr = client().search(
                 searchRequest().source(
                         searchSource().query(
@@ -591,9 +582,9 @@ public class DecayFunctionScoreIT extends ESIntegTestCase {
         List<IndexRequestBuilder> indexBuilders = new ArrayList<>();
 
         for (int i = 0; i < numDocs; i++) {
-            double lat = 100 + (int) (10.0 * (i) / (numDocs));
+            double lat = 100 + (int) (10.0 * (float) (i) / (float) (numDocs));
             double lon = 100;
-            int day = (int) (29.0 * (i) / (numDocs)) + 1;
+            int day = (int) (29.0 * (float) (i) / (float) (numDocs)) + 1;
             String dayString = day < 10 ? "0" + Integer.toString(day) : Integer.toString(day);
             String date = "2013-05-" + dayString;
 
@@ -781,7 +772,7 @@ public class DecayFunctionScoreIT extends ESIntegTestCase {
 
         assertThat(sh.getAt(0).getId(), equalTo("2"));
         assertThat(sh.getAt(1).getId(), equalTo("1"));
-        assertThat(1.0 - sh.getAt(0).getScore(), closeTo((1.0 - sh.getAt(1).getScore())/3.0, 1.e-6d));
+        assertThat((double)(1.0 - sh.getAt(0).getScore()), closeTo((double)((1.0 - sh.getAt(1).getScore())/3.0), 1.e-6d));
         response = client().search(
                 searchRequest().source(
                         searchSource().query(
@@ -789,35 +780,47 @@ public class DecayFunctionScoreIT extends ESIntegTestCase {
         sr = response.actionGet();
         assertSearchHits(sr, "1", "2");
         sh = sr.getHits();
-        assertThat((double) (sh.getAt(0).getScore()), closeTo((sh.getAt(1).getScore()), 1.e-6d));
+        assertThat((double) (sh.getAt(0).getScore()), closeTo((double) (sh.getAt(1).getScore()), 1.e-6d));
     }
 
     @Test
-    public void testExplainString() throws IOException, ExecutionException, InterruptedException {
+    public void errorMessageForFaultyFunctionScoreBody() throws Exception {
         assertAcked(prepareCreate("test").addMapping(
-                "type1",
-                jsonBuilder().startObject().startObject("type1").startObject("properties").startObject("test").field("type", "string")
+                "type",
+                jsonBuilder().startObject().startObject("type").startObject("properties").startObject("test").field("type", "string")
                         .endObject().startObject("num").field("type", "double").endObject().endObject().endObject().endObject()));
         ensureYellow();
-
-
-        client().prepareIndex().setType("type1").setId("1").setIndex("test")
-                .setSource(jsonBuilder().startObject().field("test", "value").array("num", 0.5, 0.7).endObject()).get();
-
+        client().index(
+                indexRequest("test").type("type").source(jsonBuilder().startObject().field("test", "value").field("num", 1.0).endObject()))
+                .actionGet();
         refresh();
 
-        SearchResponse response = client().search(
-                searchRequest().searchType(SearchType.QUERY_THEN_FETCH).source(
-                        searchSource().explain(true)
-                                .query(functionScoreQuery(termQuery("test", "value"), new FunctionScoreQueryBuilder.FilterFunctionBuilder[]{
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(gaussDecayFunction("num", 1.0, 5.0, 1.0)),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(linearDecayFunction("num", 1.0, 5.0, 1.0)),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(exponentialDecayFunction("num", 1.0, 5.0, 1.0))
-                                }).boostMode(CombineFunction.REPLACE)))).get();
-        String explanation = response.getHits().getAt(0).getExplanation().toString();
-        assertThat(explanation, containsString(" 1.0 = exp(-0.5*pow(MIN[Math.max(Math.abs(0.5(=doc value) - 1.0(=origin))) - 1.0(=offset), 0), Math.max(Math.abs(0.7(=doc value) - 1.0(=origin))) - 1.0(=offset), 0)],2.0)/18.033688011112044)"));
-        assertThat(explanation, containsString("1.0 = max(0.0, ((10.0 - MIN[Math.max(Math.abs(0.5(=doc value) - 1.0(=origin))) - 1.0(=offset), 0), Math.max(Math.abs(0.7(=doc value) - 1.0(=origin))) - 1.0(=offset), 0)])/10.0)"));
-        assertThat(explanation, containsString("1.0 = exp(- MIN[Math.max(Math.abs(0.5(=doc value) - 1.0(=origin))) - 1.0(=offset), 0), Math.max(Math.abs(0.7(=doc value) - 1.0(=origin))) - 1.0(=offset), 0)] * 0.13862943611198905)"));
+        XContentBuilder query = XContentFactory.jsonBuilder();
+        // query that contains a single function and a functions[] array
+        query.startObject().startObject("function_score").field("weight", "1").startArray("functions").startObject().startObject("script_score").field("script", "3").endObject().endObject().endArray().endObject().endObject();
+        try {
+            client().search(
+                    searchRequest().source(
+                            searchSource().query(query))).actionGet();
+            fail("Search should result in SearchPhaseExecutionException");
+        } catch (SearchPhaseExecutionException e) {
+            logger.info(e.shardFailures()[0].reason());
+            assertThat(e.shardFailures()[0].reason(), containsString("already found [weight], now encountering [functions]."));
+        }
 
+        query = XContentFactory.jsonBuilder();
+        // query that contains a single function (but not boost factor) and a functions[] array
+        query.startObject().startObject("function_score").startObject("random_score").field("seed", 3).endObject().startArray("functions").startObject().startObject("random_score").field("seed", 3).endObject().endObject().endArray().endObject().endObject();
+        try {
+            client().search(
+                    searchRequest().source(
+                            searchSource().query(query))).actionGet();
+            fail("Search should result in SearchPhaseExecutionException");
+        } catch (SearchPhaseExecutionException e) {
+            logger.info(e.shardFailures()[0].reason());
+            assertThat(e.shardFailures()[0].reason(), containsString("already found [random_score], now encountering [functions]"));
+            assertThat(e.shardFailures()[0].reason(), not(containsString("did you mean [boost] instead?")));
+
+        }
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/search/functionscore/QueryRescorerIT.java b/core/src/test/java/org/elasticsearch/search/functionscore/QueryRescorerIT.java
index dce6ef3..e906ac6 100644
--- a/core/src/test/java/org/elasticsearch/search/functionscore/QueryRescorerIT.java
+++ b/core/src/test/java/org/elasticsearch/search/functionscore/QueryRescorerIT.java
@@ -48,19 +48,8 @@ import java.util.Comparator;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFirstHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFourthHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSecondHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThirdHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasId;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasScore;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.lessThanOrEqualTo;
-import static org.hamcrest.Matchers.notNullValue;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
 
 /**
  *
@@ -84,9 +73,8 @@ public class QueryRescorerIT extends ESIntegTestCase {
                     .setQuery(QueryBuilders.matchAllQuery())
                     .setRescorer(RescoreBuilder.queryRescorer(
                             QueryBuilders.functionScoreQuery(QueryBuilders.matchAllQuery(),
-                                                    ScoreFunctionBuilders.weightFactorFunction(100)).boostMode(CombineFunction.REPLACE))
-                                    .setQueryWeight(0.0f).setRescoreQueryWeight(1.0f), 1).setSize(randomIntBetween(2, 10)).execute()
-                    .actionGet();
+                                    ScoreFunctionBuilders.weightFactorFunction(100)).boostMode(CombineFunction.REPLACE)).setQueryWeight(0.0f).setRescoreQueryWeight(1.0f))
+                    .setRescoreWindow(1).setSize(randomIntBetween(2, 10)).execute().actionGet();
             assertSearchResponse(searchResponse);
             assertFirstHit(searchResponse, hasScore(100.f));
             int numDocsWith100AsAScore = 0;
@@ -118,9 +106,8 @@ public class QueryRescorerIT extends ESIntegTestCase {
         refresh();
         SearchResponse searchResponse = client().prepareSearch()
                 .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
-                .setRescorer(
-                        RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "quick brown").slop(2).boost(4.0f))
-                                .setRescoreQueryWeight(2), 5).execute().actionGet();
+                .setRescorer(RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "quick brown").slop(2).boost(4.0f)).setRescoreQueryWeight(2))
+                .setRescoreWindow(5).execute().actionGet();
 
         assertThat(searchResponse.getHits().totalHits(), equalTo(3l));
         assertThat(searchResponse.getHits().getHits()[0].getId(), equalTo("1"));
@@ -129,8 +116,8 @@ public class QueryRescorerIT extends ESIntegTestCase {
 
         searchResponse = client().prepareSearch()
                 .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
-                .setRescorer(RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "the quick brown").slop(3)), 5)
-                .execute().actionGet();
+                .setRescorer(RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "the quick brown").slop(3)))
+                .setRescoreWindow(5).execute().actionGet();
 
         assertHitCount(searchResponse, 3);
         assertFirstHit(searchResponse, hasId("1"));
@@ -139,8 +126,8 @@ public class QueryRescorerIT extends ESIntegTestCase {
 
         searchResponse = client().prepareSearch()
                 .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
-                .setRescorer(RescoreBuilder.queryRescorer((QueryBuilders.matchPhraseQuery("field1", "the quick brown"))), 5).execute()
-                .actionGet();
+                .setRescorer(RescoreBuilder.queryRescorer((QueryBuilders.matchPhraseQuery("field1", "the quick brown"))))
+                .setRescoreWindow(5).execute().actionGet();
 
         assertHitCount(searchResponse, 3);
         assertFirstHit(searchResponse, hasId("1"));
@@ -186,7 +173,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                 .setSize(5)
                 .setRescorer(
                         RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "lexington avenue massachusetts").slop(3))
-                                .setQueryWeight(0.6f).setRescoreQueryWeight(2.0f), 20).execute().actionGet();
+                                .setQueryWeight(0.6f).setRescoreQueryWeight(2.0f)).setRescoreWindow(20).execute().actionGet();
 
         assertThat(searchResponse.getHits().hits().length, equalTo(5));
         assertHitCount(searchResponse, 9);
@@ -202,7 +189,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                 .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
                 .setRescorer(
                         RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "lexington avenue massachusetts").slop(3))
-                                .setQueryWeight(0.6f).setRescoreQueryWeight(2.0f), 20).execute().actionGet();
+                                .setQueryWeight(0.6f).setRescoreQueryWeight(2.0f)).setRescoreWindow(20).execute().actionGet();
 
         assertThat(searchResponse.getHits().hits().length, equalTo(5));
         assertHitCount(searchResponse, 9);
@@ -219,7 +206,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                 .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
                 .setRescorer(
                         RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "lexington avenue massachusetts").slop(3))
-                                .setQueryWeight(0.6f).setRescoreQueryWeight(2.0f), 20).execute().actionGet();
+                                .setQueryWeight(0.6f).setRescoreQueryWeight(2.0f)).setRescoreWindow(20).execute().actionGet();
 
         assertThat(searchResponse.getHits().hits().length, equalTo(5));
         assertHitCount(searchResponse, 9);
@@ -269,7 +256,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                 .setSize(5)
                 .setRescorer(
                         RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "lexington avenue massachusetts").slop(3))
-                                .setQueryWeight(0.6f).setRescoreQueryWeight(2.0f), 2).execute().actionGet();
+                                .setQueryWeight(0.6f).setRescoreQueryWeight(2.0f)).setRescoreWindow(2).execute().actionGet();
         // Only top 2 hits were re-ordered:
         assertThat(searchResponse.getHits().hits().length, equalTo(4));
         assertHitCount(searchResponse, 4);
@@ -286,7 +273,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                 .setSize(5)
                 .setRescorer(
                         RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "lexington avenue massachusetts").slop(3))
-                                .setQueryWeight(0.6f).setRescoreQueryWeight(2.0f), 3).execute().actionGet();
+                                .setQueryWeight(0.6f).setRescoreQueryWeight(2.0f)).setRescoreWindow(3).execute().actionGet();
 
         // Only top 3 hits were re-ordered:
         assertThat(searchResponse.getHits().hits().length, equalTo(4));
@@ -340,7 +327,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                 .setSize(5)
                 .setRescorer(
                         RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "lexington avenue massachusetts").slop(3))
-                                .setQueryWeight(1.0f).setRescoreQueryWeight(-1f), 3).execute().actionGet();
+                                .setQueryWeight(1.0f).setRescoreQueryWeight(-1f)).setRescoreWindow(3).execute().actionGet();
 
         // 6 and 1 got worse, and then the hit (2) outside the rescore window were sorted ahead:
         assertFirstHit(searchResponse, hasId("3"));
@@ -436,28 +423,15 @@ public class QueryRescorerIT extends ESIntegTestCase {
                                             QueryBuilders
                                                     .constantScoreQuery(QueryBuilders.matchPhraseQuery("field1", intToEnglish).slop(3)))
                                     .setQueryWeight(1.0f)
-.setRescoreQueryWeight(0.0f), rescoreWindow) // no
-                                                                                                      // weight
-                                                                                                      // -
-                                                                                                      // so
-                                                                                                      // we
-                                                                                                      // basically
-                                                                                                      // use
-                                                                                                      // the
-                                                                                                      // same
-                                                                                                      // score
-                                                                                                      // as
-                                                                                                      // the
-                                                                                                      // actual
-                                                                                                      // query
-                    .execute().actionGet();
+                                    .setRescoreQueryWeight(0.0f)) // no weight - so we basically use the same score as the actual query
+                    .setRescoreWindow(rescoreWindow).execute().actionGet();
 
             SearchResponse plain = client().prepareSearch()
                     .setSearchType(SearchType.QUERY_THEN_FETCH)
                     .setPreference("test") // ensure we hit the same shards for tie-breaking
                     .setQuery(QueryBuilders.matchQuery("field1", query).operator(Operator.OR)).setFrom(0).setSize(resultSize)
                     .execute().actionGet();
-
+            
             // check equivalence
             assertEquivalent(query, plain, rescored);
 
@@ -474,8 +448,8 @@ public class QueryRescorerIT extends ESIntegTestCase {
                                             QueryBuilders
                                                     .constantScoreQuery(QueryBuilders.matchPhraseQuery("field1", "not in the index").slop(3)))
                                     .setQueryWeight(1.0f)
-.setRescoreQueryWeight(1.0f), rescoreWindow).execute()
-                    .actionGet();
+                                    .setRescoreQueryWeight(1.0f))
+                    .setRescoreWindow(rescoreWindow).execute().actionGet();
             // check equivalence
             assertEquivalent(query, plain, rescored);
 
@@ -490,7 +464,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                             RescoreBuilder
                                     .queryRescorer(
                                             QueryBuilders.matchPhraseQuery("field1", intToEnglish).slop(0))
-                                    .setQueryWeight(1.0f).setRescoreQueryWeight(1.0f), 2 * rescoreWindow).execute().actionGet();
+                                    .setQueryWeight(1.0f).setRescoreQueryWeight(1.0f)).setRescoreWindow(2 * rescoreWindow).execute().actionGet();
             // check equivalence or if the first match differs we check if the phrase is a substring of the top doc
             assertEquivalentOrSubstringMatch(intToEnglish, plain, rescored);
         }
@@ -521,7 +495,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                     .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
                     .setRescorer(
                             RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "the quick brown").slop(2).boost(4.0f))
-                                    .setQueryWeight(0.5f).setRescoreQueryWeight(0.4f), 5).setExplain(true).execute()
+                                    .setQueryWeight(0.5f).setRescoreQueryWeight(0.4f)).setRescoreWindow(5).setExplain(true).execute()
                     .actionGet();
             assertHitCount(searchResponse, 3);
             assertFirstHit(searchResponse, hasId("1"));
@@ -557,7 +531,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                     .prepareSearch()
                     .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
                     .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
-                    .setRescorer(innerRescoreQuery, 5).setExplain(true).execute()
+                    .setRescorer(innerRescoreQuery).setRescoreWindow(5).setExplain(true).execute()
                     .actionGet();
             assertHitCount(searchResponse, 3);
             assertFirstHit(searchResponse, hasId("1"));
@@ -580,7 +554,8 @@ public class QueryRescorerIT extends ESIntegTestCase {
                         .prepareSearch()
                         .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
                         .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
-                        .addRescorer(innerRescoreQuery, 5).addRescorer(outerRescoreQuery, 10)
+                        .addRescorer(innerRescoreQuery).setRescoreWindow(5)
+                        .addRescorer(outerRescoreQuery).setRescoreWindow(10)
                         .setExplain(true).get();
                 assertHitCount(searchResponse, 3);
                 assertFirstHit(searchResponse, hasId("1"));
@@ -640,7 +615,8 @@ public class QueryRescorerIT extends ESIntegTestCase {
                                         ScoreFunctionBuilders.weightFactorFunction(0.2f)).boostMode(CombineFunction.REPLACE)))
                                 .setFrom(0)
                                 .setSize(10)
-.setRescorer(rescoreQuery, 50).execute().actionGet();
+                                .setRescorer(rescoreQuery)
+                                .setRescoreWindow(50).execute().actionGet();
 
                 assertHitCount(rescored, 4);
 
@@ -701,14 +677,14 @@ public class QueryRescorerIT extends ESIntegTestCase {
                 .setScoreMode("total");
 
         // First set the rescore window large enough that both rescores take effect
-        SearchRequestBuilder request = client().prepareSearch();
-        request.addRescorer(eightIsGreat, numDocs).addRescorer(sevenIsBetter, numDocs);
+        SearchRequestBuilder request = client().prepareSearch().setRescoreWindow(numDocs);
+        request.addRescorer(eightIsGreat).addRescorer(sevenIsBetter);
         SearchResponse response = request.get();
         assertFirstHit(response, hasId("7"));
         assertSecondHit(response, hasId("8"));
 
         // Now squash the second rescore window so it never gets to see a seven
-        response = request.setSize(1).clearRescorers().addRescorer(eightIsGreat, numDocs).addRescorer(sevenIsBetter, 1).get();
+        response = request.setSize(1).clearRescorers().addRescorer(eightIsGreat).addRescorer(sevenIsBetter, 1).get();
         assertFirstHit(response, hasId("8"));
         // We have no idea what the second hit will be because we didn't get a chance to look for seven
 
@@ -719,7 +695,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         QueryRescorer oneToo = RescoreBuilder.queryRescorer(
                 QueryBuilders.functionScoreQuery(QueryBuilders.queryStringQuery("*one*"), ScoreFunctionBuilders.weightFactorFunction(1000.0f))
                         .boostMode(CombineFunction.REPLACE)).setScoreMode("total");
-        request.clearRescorers().addRescorer(ninetyIsGood, numDocs).addRescorer(oneToo, 10);
+        request.clearRescorers().addRescorer(ninetyIsGood).addRescorer(oneToo, 10);
         response = request.setSize(2).get();
         assertFirstHit(response, hasId("91"));
         assertFirstHit(response, hasScore(2001.0f));
@@ -769,7 +745,8 @@ public class QueryRescorerIT extends ESIntegTestCase {
         request.setQuery(QueryBuilders.termQuery("text", "hello"));
         request.setFrom(1);
         request.setSize(4);
-        request.addRescorer(RescoreBuilder.queryRescorer(QueryBuilders.matchAllQuery()), 50);
+        request.addRescorer(RescoreBuilder.queryRescorer(QueryBuilders.matchAllQuery()));
+        request.setRescoreWindow(50);
 
         assertEquals(4, request.get().getHits().hits().length);
     }
diff --git a/core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java b/core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java
index b2b1b6f..3cb7025 100644
--- a/core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java
+++ b/core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java
@@ -57,6 +57,7 @@ import java.io.IOException;
 import java.io.InputStream;
 import java.util.ArrayList;
 import java.util.Collection;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.Iterator;
 import java.util.List;
@@ -516,30 +517,52 @@ public class GeoFilterIT extends ESIntegTestCase {
 
         expectedCounts.put(geoHashCellQuery("pin", point).neighbors(true).precision(precision), 1L + neighbors.size());
 
+        logger.info("random testing of setting");
 
         List<GeohashCellQuery.Builder> filterBuilders = new ArrayList<>(expectedCounts.keySet());
-        for (GeohashCellQuery.Builder builder : filterBuilders) {
-            try {
-                long expectedCount = expectedCounts.get(builder);
-                SearchResponse response = client().prepareSearch("locations").setQuery(QueryBuilders.matchAllQuery())
-                        .setPostFilter(builder).setSize((int) expectedCount).get();
-                assertHitCount(response, expectedCount);
-                String[] expectedIds = expectedResults.get(builder);
-                if (expectedIds == null) {
-                    ArrayList<String> ids = new ArrayList<>();
-                    for (SearchHit hit : response.getHits()) {
-                        ids.add(hit.id());
+        for (int j = filterBuilders.size() * 2 * randomIntBetween(1, 5); j > 0; j--) {
+            Collections.shuffle(filterBuilders, getRandom());
+            for (GeohashCellQuery.Builder builder : filterBuilders) {
+                try {
+                    long expectedCount = expectedCounts.get(builder);
+                    SearchResponse response = client().prepareSearch("locations").setQuery(QueryBuilders.matchAllQuery())
+                            .setPostFilter(builder).setSize((int) expectedCount).get();
+                    assertHitCount(response, expectedCount);
+                    String[] expectedIds = expectedResults.get(builder);
+                    if (expectedIds == null) {
+                        ArrayList<String> ids = new ArrayList<>();
+                        for (SearchHit hit : response.getHits()) {
+                            ids.add(hit.id());
+                        }
+                        expectedResults.put(builder, ids.toArray(Strings.EMPTY_ARRAY));
+                        continue;
                     }
-                    expectedResults.put(builder, ids.toArray(Strings.EMPTY_ARRAY));
-                    continue;
+
+                    assertSearchHits(response, expectedIds);
+
+                } catch (AssertionError error) {
+                    throw new AssertionError(error.getMessage() + "\n geohash_cell filter:" + builder, error);
                 }
 
-                assertSearchHits(response, expectedIds);
 
-            } catch (AssertionError error) {
-                throw new AssertionError(error.getMessage() + "\n geohash_cell filter:" + builder, error);
             }
         }
+
+        logger.info("Testing lat/lon format");
+        String pointTest1 = "{\"geohash_cell\": {\"pin\": {\"lat\": " + point.lat() + ",\"lon\": " + point.lon() + "},\"precision\": " + precision + ",\"neighbors\": true}}";
+        SearchResponse results3 = client().prepareSearch("locations").setQuery(QueryBuilders.matchAllQuery()).setPostFilter(pointTest1).execute().actionGet();
+        assertHitCount(results3, neighbors.size() + 1);
+
+
+        logger.info("Testing String format");
+        String pointTest2 = "{\"geohash_cell\": {\"pin\": \"" + point.lat() + "," + point.lon() + "\",\"precision\": " + precision + ",\"neighbors\": true}}";
+        SearchResponse results4 = client().prepareSearch("locations").setQuery(QueryBuilders.matchAllQuery()).setPostFilter(pointTest2).execute().actionGet();
+        assertHitCount(results4, neighbors.size() + 1);
+
+        logger.info("Testing Array format");
+        String pointTest3 = "{\"geohash_cell\": {\"pin\": [" + point.lon() + "," + point.lat() + "],\"precision\": " + precision + ",\"neighbors\": true}}";
+        SearchResponse results5 = client().prepareSearch("locations").setQuery(QueryBuilders.matchAllQuery()).setPostFilter(pointTest3).execute().actionGet();
+        assertHitCount(results5, neighbors.size() + 1);
     }
 
     @Test
diff --git a/core/src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationIT.java b/core/src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationIT.java
index 1f057af..670d317 100644
--- a/core/src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationIT.java
+++ b/core/src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationIT.java
@@ -226,6 +226,44 @@ public class GeoShapeIntegrationIT extends ESIntegTestCase {
     }
 
     @Test
+    public void testParsingMultipleShapes() throws Exception {
+        String mapping = XContentFactory.jsonBuilder()
+                .startObject()
+                .startObject("type1")
+                .startObject("properties")
+                .startObject("location1")
+                .field("type", "geo_shape")
+                .endObject()
+                .startObject("location2")
+                .field("type", "geo_shape")
+                .endObject()
+                .endObject()
+                .endObject()
+                .endObject()
+                .string();
+
+        assertAcked(prepareCreate("test").addMapping("type1", mapping));
+        ensureYellow();
+
+        String p1 = "\"location1\" : {\"type\":\"polygon\", \"coordinates\":[[[-10,-10],[10,-10],[10,10],[-10,10],[-10,-10]]]}";
+        String p2 = "\"location2\" : {\"type\":\"polygon\", \"coordinates\":[[[-20,-20],[20,-20],[20,20],[-20,20],[-20,-20]]]}";
+        String o1 = "{" + p1 + ", " + p2 + "}";
+
+        indexRandom(true, client().prepareIndex("test", "type1", "1").setSource(o1));
+
+        String filter = "{\"geo_shape\": {\"location2\": {\"indexed_shape\": {"
+                + "\"id\": \"1\","
+                + "\"type\": \"type1\","
+                + "\"index\": \"test\","
+                + "\"path\": \"location2\""
+                + "}}}}";
+
+        SearchResponse result = client().prepareSearch("test").setQuery(QueryBuilders.matchAllQuery()).setPostFilter(filter).execute().actionGet();
+        assertSearchResponse(result);
+        assertHitCount(result, 1);
+    }
+
+    @Test
     public void testShapeFetchingPath() throws Exception {
         createIndex("shapes");
         assertAcked(prepareCreate("test").addMapping("type", "location", "type=geo_shape"));
diff --git a/core/src/test/java/org/elasticsearch/search/highlight/CustomHighlighterSearchIT.java b/core/src/test/java/org/elasticsearch/search/highlight/CustomHighlighterSearchIT.java
index 5f5ecfc..7c1f163 100644
--- a/core/src/test/java/org/elasticsearch/search/highlight/CustomHighlighterSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/highlight/CustomHighlighterSearchIT.java
@@ -60,7 +60,7 @@ public class CustomHighlighterSearchIT extends ESIntegTestCase {
     public void testThatCustomHighlightersAreSupported() throws IOException {
         SearchResponse searchResponse = client().prepareSearch("test").setTypes("test")
                 .setQuery(QueryBuilders.matchAllQuery())
-                .highlighter(new HighlightBuilder().field("name").highlighterType("test-custom"))
+                .addHighlightedField("name").setHighlighterType("test-custom")
                 .execute().actionGet();
         assertHighlight(searchResponse, 0, "name", 0, equalTo("standard response for name at position 1"));
     }
@@ -75,7 +75,7 @@ public class CustomHighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse searchResponse = client().prepareSearch("test").setTypes("test")
                 .setQuery(QueryBuilders.matchAllQuery())
-                .highlighter(new HighlightBuilder().field(highlightConfig))
+                .addHighlightedField(highlightConfig)
                 .execute().actionGet();
 
         assertHighlight(searchResponse, 0, "name", 0, equalTo("standard response for name at position 1"));
@@ -87,8 +87,11 @@ public class CustomHighlighterSearchIT extends ESIntegTestCase {
         Map<String, Object> options = new HashMap<>();
         options.put("myGlobalOption", "someValue");
 
-        SearchResponse searchResponse = client().prepareSearch("test").setTypes("test").setQuery(QueryBuilders.matchAllQuery())
-                .highlighter(new HighlightBuilder().field("name").highlighterType("test-custom").options(options))
+        SearchResponse searchResponse = client().prepareSearch("test").setTypes("test")
+                .setQuery(QueryBuilders.matchAllQuery())
+                .setHighlighterOptions(options)
+                .setHighlighterType("test-custom")
+                .addHighlightedField("name")
                 .execute().actionGet();
 
         assertHighlight(searchResponse, 0, "name", 0, equalTo("standard response for name at position 1"));
@@ -100,9 +103,11 @@ public class CustomHighlighterSearchIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("test").setTypes("test")
                 .setQuery(QueryBuilders.boolQuery().must(QueryBuilders.matchAllQuery()).should(QueryBuilders
                         .termQuery("name", "arbitrary")))
-                .highlighter(
-                        new HighlightBuilder().highlighterType("test-custom").field("name").field("other_name").field("other_other_name")
-                                .useExplicitFieldOrder(true))
+                .setHighlighterType("test-custom")
+                .addHighlightedField("name")
+                .addHighlightedField("other_name")
+                .addHighlightedField("other_other_name")
+                .setHighlighterExplicitFieldOrder(true)
                 .get();
 
         assertHighlight(searchResponse, 0, "name", 0, equalTo("standard response for name at position 1"));
diff --git a/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java b/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
index 93fd7eb..4134c4f 100644
--- a/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
@@ -19,22 +19,20 @@
 package org.elasticsearch.search.highlight;
 
 import com.carrotsearch.randomizedtesting.generators.RandomPicks;
-
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.settings.Settings.Builder;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.query.AbstractQueryBuilder;
+import org.elasticsearch.index.query.*;
 import org.elasticsearch.index.query.IdsQueryBuilder;
 import org.elasticsearch.index.query.MatchQueryBuilder;
+import org.elasticsearch.index.search.MatchQuery.Type;
+import org.elasticsearch.index.search.MatchQuery;
 import org.elasticsearch.index.query.MultiMatchQueryBuilder;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.search.MatchQuery;
-import org.elasticsearch.index.search.MatchQuery.Type;
 import org.elasticsearch.rest.RestStatus;
 import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
@@ -51,38 +49,12 @@ import java.util.Map;
 import static org.elasticsearch.client.Requests.searchRequest;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
-import static org.elasticsearch.index.query.QueryBuilders.boostingQuery;
-import static org.elasticsearch.index.query.QueryBuilders.commonTermsQuery;
-import static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.fuzzyQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchPhrasePrefixQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchPhraseQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.missingQuery;
-import static org.elasticsearch.index.query.QueryBuilders.multiMatchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.prefixQuery;
-import static org.elasticsearch.index.query.QueryBuilders.queryStringQuery;
-import static org.elasticsearch.index.query.QueryBuilders.rangeQuery;
-import static org.elasticsearch.index.query.QueryBuilders.regexpQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.index.query.QueryBuilders.typeQuery;
-import static org.elasticsearch.index.query.QueryBuilders.wildcardQuery;
+import static org.elasticsearch.index.query.QueryBuilders.*;
 import static org.elasticsearch.search.builder.SearchSourceBuilder.highlight;
 import static org.elasticsearch.search.builder.SearchSourceBuilder.searchSource;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHighlight;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNotHighlighted;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
 import static org.elasticsearch.test.hamcrest.RegexMatcher.matches;
-import static org.hamcrest.Matchers.anyOf;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.hasKey;
-import static org.hamcrest.Matchers.not;
-import static org.hamcrest.Matchers.startsWith;
+import static org.hamcrest.Matchers.*;
 
 public class HighlighterSearchIT extends ESIntegTestCase {
 
@@ -110,8 +82,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .get();
         refresh();
         String highlighter = randomFrom(new String[]{"plain", "postings", "fvh"});
-        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text")))
-                .highlighter(new HighlightBuilder().field(new Field("*").highlighterType(highlighter))).get();
+        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text"))).addHighlightedField(new Field("*").highlighterType(highlighter)).get();
         assertHighlight(search, 0, "text", 0, equalTo("<em>text</em>"));
     }
 
@@ -150,17 +121,14 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .setSource(jsonBuilder().startObject().field("long_text", builder.toString()).field("text", "text").endObject())
                 .get();
         refresh();
-        String highlighter = randomFrom(new String[] { "plain", "postings", "fvh" });
-        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text")))
-                .highlighter(new HighlightBuilder().field(new Field("*").highlighterType(highlighter))).get();
+        String highlighter = randomFrom(new String[]{"plain", "postings", "fvh"});
+        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text"))).addHighlightedField(new Field("*").highlighterType(highlighter)).get();
         assertHighlight(search, 0, "text", 0, equalTo("<em>text</em>"));
-        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text")))
-                .highlighter(new HighlightBuilder().field(new Field("long_text").highlighterType(highlighter))).get();
+        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text"))).addHighlightedField(new Field("long_text").highlighterType(highlighter)).get();
         assertNoFailures(search);
         assertThat(search.getHits().getAt(0).getHighlightFields().size(), equalTo(0));
 
-        search = client().prepareSearch().setQuery(prefixQuery("text", "te"))
-                .highlighter(new HighlightBuilder().field(new Field("long_text").highlighterType(highlighter))).get();
+        search = client().prepareSearch().setQuery(prefixQuery("text", "te")).addHighlightedField(new Field("long_text").highlighterType(highlighter)).get();
         assertNoFailures(search);
         assertThat(search.getHits().getAt(0).getHighlightFields().size(), equalTo(0));
     }
@@ -196,12 +164,10 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .setSource(jsonBuilder().startObject().field("unstored_text", "text").field("text", "text").endObject())
                 .get();
         refresh();
-        String highlighter = randomFrom(new String[] { "plain", "postings", "fvh" });
-        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text")))
-                .highlighter(new HighlightBuilder().field(new Field("*").highlighterType(highlighter))).get();
+        String highlighter = randomFrom(new String[]{"plain", "postings", "fvh"});
+        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text"))).addHighlightedField(new Field("*").highlighterType(highlighter)).get();
         assertHighlight(search, 0, "text", 0, equalTo("<em>text</em>"));
-        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text")))
-                .highlighter(new HighlightBuilder().field(new Field("unstored_text"))).get();
+        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text"))).addHighlightedField(new Field("unstored_text")).get();
         assertNoFailures(search);
         assertThat(search.getHits().getAt(0).getHighlightFields().size(), equalTo(0));
     }
@@ -221,8 +187,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
             .setSource("name", builder.toString())
             .get();
         refresh();
-        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name", "abc")))
-                .highlighter(new HighlightBuilder().field("name")).get();
+        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name", "abc"))).addHighlightedField("name").get();
         assertHighlight(search, 0, "name", 0, startsWith("<em>abc</em> <em>abc</em> <em>abc</em> <em>abc</em>"));
     }
 
@@ -278,9 +243,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         client().prepareIndex("test", "test", "1")
             .setSource("name", "ARCOTEL Hotels Deutschland").get();
         refresh();
-        SearchResponse search = client().prepareSearch("test").setTypes("test")
-                .setQuery(matchQuery("name.autocomplete", "deut tel").operator(Operator.OR))
-                .highlighter(new HighlightBuilder().field("name.autocomplete")).execute().actionGet();
+        SearchResponse search = client().prepareSearch("test").setTypes("test").setQuery(matchQuery("name.autocomplete", "deut tel").operator(Operator.OR)).addHighlightedField("name.autocomplete").execute().actionGet();
         assertHighlight(search, 0, "name.autocomplete", 0, equalTo("ARCO<em>TEL</em> Ho<em>tel</em>s <em>Deut</em>schland"));
     }
 
@@ -310,22 +273,10 @@ public class HighlighterSearchIT extends ESIntegTestCase {
             .setSource("body", "Test: http://www.facebook.com http://elasticsearch.org http://xing.com http://cnn.com http://quora.com http://twitter.com this is a test for highlighting feature Test: http://www.facebook.com http://elasticsearch.org http://xing.com http://cnn.com http://quora.com http://twitter.com this is a test for highlighting feature")
             .get();
         refresh();
-        SearchResponse search = client().prepareSearch().setQuery(matchQuery("body", "Test: http://www.facebook.com ").type(Type.PHRASE))
-                .highlighter(new HighlightBuilder().field("body")).execute().actionGet();
+        SearchResponse search = client().prepareSearch().setQuery(matchQuery("body", "Test: http://www.facebook.com ").type(Type.PHRASE)).addHighlightedField("body").execute().actionGet();
         assertHighlight(search, 0, "body", 0, startsWith("<em>Test: http://www.facebook.com</em>"));
-        search = client()
-                .prepareSearch()
-                .setQuery(
-                        matchQuery(
-                                "body",
-                                "Test: http://www.facebook.com http://elasticsearch.org http://xing.com http://cnn.com http://quora.com http://twitter.com this is a test for highlighting feature Test: http://www.facebook.com http://elasticsearch.org http://xing.com http://cnn.com http://quora.com http://twitter.com this is a test for highlighting feature")
-                                .type(Type.PHRASE)).highlighter(new HighlightBuilder().field("body")).execute().actionGet();
-        assertHighlight(
-                search,
-                0,
-                "body",
-                0,
-                equalTo("<em>Test</em>: <em>http://www.facebook.com</em> <em>http://elasticsearch.org</em> <em>http://xing.com</em> <em>http://cnn.com</em> http://quora.com"));
+        search = client().prepareSearch().setQuery(matchQuery("body", "Test: http://www.facebook.com http://elasticsearch.org http://xing.com http://cnn.com http://quora.com http://twitter.com this is a test for highlighting feature Test: http://www.facebook.com http://elasticsearch.org http://xing.com http://cnn.com http://quora.com http://twitter.com this is a test for highlighting feature").type(Type.PHRASE)).addHighlightedField("body").execute().actionGet();
+        assertHighlight(search, 0, "body", 0, equalTo("<em>Test</em>: <em>http://www.facebook.com</em> <em>http://elasticsearch.org</em> <em>http://xing.com</em> <em>http://cnn.com</em> http://quora.com"));
     }
 
     @Test
@@ -359,43 +310,37 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                     "name2", "avinci, unilog avinci, logicacmg, logica").get();
         refresh();
 
-        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name", "logica m")))
-                .highlighter(new HighlightBuilder().field("name")).get();
+        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name", "logica m"))).addHighlightedField("name").get();
         assertHighlight(search, 0, "name", 0, anyOf(equalTo("<em>logica</em>c<em>m</em>g ehe<em>m</em>als avinci - the know how co<em>m</em>pany"),
                 equalTo("avinci, unilog avinci, <em>logica</em>c<em>m</em>g, <em>logica</em>")));
         assertHighlight(search, 1, "name", 0, anyOf(equalTo("<em>logica</em>c<em>m</em>g ehe<em>m</em>als avinci - the know how co<em>m</em>pany"),
                 equalTo("avinci, unilog avinci, <em>logica</em>c<em>m</em>g, <em>logica</em>")));
 
-        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name", "logica ma")))
-                .highlighter(new HighlightBuilder().field("name")).get();
+        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name", "logica ma"))).addHighlightedField("name").get();
         assertHighlight(search, 0, "name", 0, anyOf(equalTo("<em>logica</em>cmg ehe<em>ma</em>ls avinci - the know how company"),
                 equalTo("avinci, unilog avinci, <em>logica</em>cmg, <em>logica</em>")));
         assertHighlight(search, 1, "name", 0, anyOf(equalTo("<em>logica</em>cmg ehe<em>ma</em>ls avinci - the know how company"),
                 equalTo("avinci, unilog avinci, <em>logica</em>cmg, <em>logica</em>")));
 
-        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name", "logica")))
-                .highlighter(new HighlightBuilder().field("name")).get();
+        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name", "logica"))).addHighlightedField("name").get();
         assertHighlight(search, 0, "name", 0, anyOf(equalTo("<em>logica</em>cmg ehemals avinci - the know how company"),
                 equalTo("avinci, unilog avinci, <em>logica</em>cmg, <em>logica</em>")));
         assertHighlight(search, 0, "name", 0, anyOf(equalTo("<em>logica</em>cmg ehemals avinci - the know how company"),
                 equalTo("avinci, unilog avinci, <em>logica</em>cmg, <em>logica</em>")));
 
-        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name2", "logica m")))
-                .highlighter(new HighlightBuilder().field("name2")).get();
+        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name2", "logica m"))).addHighlightedField("name2").get();
         assertHighlight(search, 0, "name2", 0, anyOf(equalTo("<em>logica</em>c<em>m</em>g ehe<em>m</em>als avinci - the know how co<em>m</em>pany"),
                 equalTo("avinci, unilog avinci, <em>logica</em>c<em>m</em>g, <em>logica</em>")));
         assertHighlight(search, 1, "name2", 0, anyOf(equalTo("<em>logica</em>c<em>m</em>g ehe<em>m</em>als avinci - the know how co<em>m</em>pany"),
                 equalTo("avinci, unilog avinci, <em>logica</em>c<em>m</em>g, <em>logica</em>")));
 
-        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name2", "logica ma")))
-                .highlighter(new HighlightBuilder().field("name2")).get();
+        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name2", "logica ma"))).addHighlightedField("name2").get();
         assertHighlight(search, 0, "name2", 0, anyOf(equalTo("<em>logica</em>cmg ehe<em>ma</em>ls avinci - the know how company"),
                 equalTo("avinci, unilog avinci, <em>logica</em>cmg, <em>logica</em>")));
         assertHighlight(search, 1, "name2", 0, anyOf(equalTo("<em>logica</em>cmg ehe<em>ma</em>ls avinci - the know how company"),
                 equalTo("avinci, unilog avinci, <em>logica</em>cmg, <em>logica</em>")));
 
-        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name2", "logica")))
-                .highlighter(new HighlightBuilder().field("name2")).get();
+        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name2", "logica"))).addHighlightedField("name2").get();
         assertHighlight(search, 0, "name2", 0, anyOf(equalTo("<em>logica</em>cmg ehemals avinci - the know how company"),
                 equalTo("avinci, unilog avinci, <em>logica</em>cmg, <em>logica</em>")));
         assertHighlight(search, 1, "name2", 0, anyOf(equalTo("<em>logica</em>cmg ehemals avinci - the know how company"),
@@ -426,25 +371,22 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                        "name2", "logicacmg ehemals avinci - the know how company").get();
         refresh();
         ensureGreen();
-        SearchResponse search = client().prepareSearch().setQuery(matchQuery("name", "logica m"))
-                .highlighter(new HighlightBuilder().field("name")).get();
+        SearchResponse search = client().prepareSearch().setQuery(matchQuery("name", "logica m")).addHighlightedField("name").get();
         assertHighlight(search, 0, "name", 0, equalTo("<em>logica</em>c<em>m</em>g ehe<em>m</em>als avinci - the know how co<em>m</em>pany"));
 
-        search = client().prepareSearch().setQuery(matchQuery("name", "logica ma")).highlighter(new HighlightBuilder().field("name")).get();
+        search = client().prepareSearch().setQuery(matchQuery("name", "logica ma")).addHighlightedField("name").get();
         assertHighlight(search, 0, "name", 0, equalTo("<em>logica</em>cmg ehe<em>ma</em>ls avinci - the know how company"));
 
-        search = client().prepareSearch().setQuery(matchQuery("name", "logica")).highlighter(new HighlightBuilder().field("name")).get();
+        search = client().prepareSearch().setQuery(matchQuery("name", "logica")).addHighlightedField("name").get();
         assertHighlight(search, 0, "name", 0, equalTo("<em>logica</em>cmg ehemals avinci - the know how company"));
 
-        search = client().prepareSearch().setQuery(matchQuery("name2", "logica m")).highlighter(new HighlightBuilder().field("name2"))
-                .get();
+        search = client().prepareSearch().setQuery(matchQuery("name2", "logica m")).addHighlightedField("name2").get();
         assertHighlight(search, 0, "name2", 0, equalTo("<em>logicacmg</em> <em>ehemals</em> avinci - the know how <em>company</em>"));
 
-        search = client().prepareSearch().setQuery(matchQuery("name2", "logica ma")).highlighter(new HighlightBuilder().field("name2"))
-                .get();
+        search = client().prepareSearch().setQuery(matchQuery("name2", "logica ma")).addHighlightedField("name2").get();
         assertHighlight(search, 0, "name2", 0, equalTo("<em>logicacmg</em> <em>ehemals</em> avinci - the know how company"));
 
-        search = client().prepareSearch().setQuery(matchQuery("name2", "logica")).highlighter(new HighlightBuilder().field("name2")).get();
+        search = client().prepareSearch().setQuery(matchQuery("name2", "logica")).addHighlightedField("name2").get();
         assertHighlight(search, 0, "name2", 0, equalTo("<em>logicacmg</em> ehemals avinci - the know how company"));
     }
 
@@ -464,19 +406,19 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("long_term", "thisisaverylongwordandmakessurethisfails foo highlighed"))
-                .highlighter(new HighlightBuilder().field("long_term", 18, 1))
+                .addHighlightedField("long_term", 18, 1)
                 .get();
         assertHighlight(search, 0, "long_term", 0, 1, equalTo("<em>thisisaverylongwordandmakessurethisfails</em>"));
 
         search = client().prepareSearch()
                 .setQuery(matchQuery("no_long_term", "test foo highlighed").type(Type.PHRASE).slop(3))
-                .highlighter(new HighlightBuilder().field("no_long_term", 18, 1).postTags("</b>").preTags("<b>"))
+                .addHighlightedField("no_long_term", 18, 1).setHighlighterPostTags("</b>").setHighlighterPreTags("<b>")
                 .get();
         assertNotHighlighted(search, 0, "no_long_term");
 
         search = client().prepareSearch()
                 .setQuery(matchQuery("no_long_term", "test foo highlighed").type(Type.PHRASE).slop(3))
-                .highlighter(new HighlightBuilder().field("no_long_term", 30, 1).postTags("</b>").preTags("<b>"))
+                .addHighlightedField("no_long_term", 30, 1).setHighlighterPostTags("</b>").setHighlighterPreTags("<b>")
                 .get();
 
         assertHighlight(search, 0, "no_long_term", 0, 1, equalTo("a <b>test</b> where <b>foo</b> is <b>highlighed</b> and"));
@@ -504,7 +446,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "bug"))
-                .highlighter(new HighlightBuilder().field("title", -1, 0))
+                .addHighlightedField("title", -1, 0)
                 .get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
@@ -513,7 +455,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         search = client().prepareSearch()
                 .setQuery(matchQuery("attachments.body", "attachment"))
-                .highlighter(new HighlightBuilder().field("attachments.body", -1, 0))
+                .addHighlightedField("attachments.body", -1, 0)
                 .get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
@@ -544,7 +486,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "bug"))
-                .highlighter(new HighlightBuilder().field("title", -1, 0))
+                .addHighlightedField("title", -1, 0)
                 .get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
@@ -553,7 +495,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         search = client().prepareSearch()
                 .setQuery(matchQuery("attachments.body", "attachment"))
-                .highlighter(new HighlightBuilder().field("attachments.body", -1, 2))
+                .addHighlightedField("attachments.body", -1, 2)
                 .execute().get();
 
         for (int i = 0; i < 5; i++) {
@@ -586,7 +528,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "bug"))
                 //asking for the whole field to be highlighted
-                .highlighter(new HighlightBuilder().field("title", -1, 0)).get();
+                .addHighlightedField("title", -1, 0).get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
             assertHighlight(search, i, "title", 0, equalTo("This is a test on the highlighting <em>bug</em> present in elasticsearch. Hopefully it works."));
@@ -596,7 +538,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         search = client().prepareSearch()
                 .setQuery(matchQuery("title", "bug"))
                 //sentences will be generated out of each value
-                .highlighter(new HighlightBuilder().field("title")).get();
+                .addHighlightedField("title").get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
             assertHighlight(search, i, "title", 0, equalTo("This is a test on the highlighting <em>bug</em> present in elasticsearch."));
@@ -605,7 +547,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         search = client().prepareSearch()
                 .setQuery(matchQuery("attachments.body", "attachment"))
-                .highlighter(new HighlightBuilder().field("attachments.body", -1, 2))
+                .addHighlightedField("attachments.body", -1, 2)
                 .get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
@@ -629,7 +571,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "bug"))
-                .highlighter(new HighlightBuilder().field("title", -1, 2).field("titleTV", -1, 2).requireFieldMatch(false))
+                .addHighlightedField("title", -1, 2)
+                .addHighlightedField("titleTV", -1, 2).setHighlighterRequireFieldMatch(false)
                 .get();
 
         assertHighlight(search, 0, "title", 0, equalTo("This is a test on the highlighting <em>bug</em> present in elasticsearch"));
@@ -639,7 +582,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         search = client().prepareSearch()
                 .setQuery(matchQuery("titleTV", "highlight"))
-                .highlighter(new HighlightBuilder().field("titleTV", -1, 2))
+                .addHighlightedField("titleTV", -1, 2)
                 .get();
 
         assertHighlight(search, 0, "titleTV", 0, equalTo("some text to <em>highlight</em>"));
@@ -659,7 +602,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1 and field2 produces different tags");
         SearchSourceBuilder source = searchSource()
                 .query(termQuery("field1", "test"))
-                .highlighter(highlight().order("score").preTags("<global>").postTags("</global>").fragmentSize(1).numOfFragments(1)
+                .highlight(highlight().order("score").preTags("<global>").postTags("</global>").fragmentSize(1).numOfFragments(1)
                         .field(new HighlightBuilder.Field("field1").numOfFragments(2))
                         .field(new HighlightBuilder.Field("field2").preTags("<field2>").postTags("</field2>").fragmentSize(50).requireFieldMatch(false)));
 
@@ -689,7 +632,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         SearchSourceBuilder source = searchSource()
                 //postings hl doesn't support require_field_match, its field needs to be queried directly
                 .query(termQuery("field-postings", "test"))
-                .highlighter(highlight().field("field*").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field*").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -718,42 +661,36 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         //works using stored field
         SearchResponse searchResponse = client().prepareSearch("test")
                 .setQuery(termQuery("field1", "quick"))
-                .highlighter(new HighlightBuilder().field(new Field("field1").preTags("<xxx>").postTags("</xxx>")))
+                .addHighlightedField(new Field("field1").preTags("<xxx>").postTags("</xxx>"))
                 .get();
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("The <xxx>quick</xxx> brown fox jumps over the lazy dog"));
 
         assertFailures(client().prepareSearch("test")
                         .setQuery(termQuery("field1", "quick"))
-                        .highlighter(
-                                new HighlightBuilder().field(new Field("field1").preTags("<xxx>").postTags("</xxx>")
-                                        .highlighterType("plain").forceSource(true))),
+                        .addHighlightedField(new Field("field1").preTags("<xxx>").postTags("</xxx>").highlighterType("plain").forceSource(true)),
                 RestStatus.BAD_REQUEST,
                 containsString("source is forced for fields [field1] but type [type1] has disabled _source"));
 
         assertFailures(client().prepareSearch("test")
                         .setQuery(termQuery("field1", "quick"))
-                        .highlighter(
-                                new HighlightBuilder().field(new Field("field1").preTags("<xxx>").postTags("</xxx>").highlighterType("fvh")
-                                        .forceSource(true))),
+                        .addHighlightedField(new Field("field1").preTags("<xxx>").postTags("</xxx>").highlighterType("fvh").forceSource(true)),
                 RestStatus.BAD_REQUEST,
                 containsString("source is forced for fields [field1] but type [type1] has disabled _source"));
 
         assertFailures(client().prepareSearch("test")
                 .setQuery(termQuery("field1", "quick"))
-                        .highlighter(
-                                new HighlightBuilder().field(new Field("field1").preTags("<xxx>").postTags("</xxx>")
-                                        .highlighterType("postings").forceSource(true))),
+                .addHighlightedField(new Field("field1").preTags("<xxx>").postTags("</xxx>").highlighterType("postings").forceSource(true)),
                 RestStatus.BAD_REQUEST,
                 containsString("source is forced for fields [field1] but type [type1] has disabled _source"));
 
         SearchSourceBuilder searchSource = SearchSourceBuilder.searchSource().query(termQuery("field1", "quick"))
-                .highlighter(highlight().forceSource(true).field("field1"));
+                .highlight(highlight().forceSource(true).field("field1"));
         assertFailures(client().prepareSearch("test").setSource(searchSource),
                 RestStatus.BAD_REQUEST,
                 containsString("source is forced for fields [field1] but type [type1] has disabled _source"));
 
         searchSource = SearchSourceBuilder.searchSource().query(termQuery("field1", "quick"))
-                .highlighter(highlight().forceSource(true).field("field*"));
+                .highlight(highlight().forceSource(true).field("field*"));
         assertFailures(client().prepareSearch("test").setSource(searchSource),
                 RestStatus.BAD_REQUEST,
                 matches("source is forced for fields \\[field\\d, field\\d\\] but type \\[type1\\] has disabled _source"));
@@ -771,7 +708,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(termQuery("field1", "test"))
-                .highlighter(highlight().field("field1").order("score").preTags("<xxx>").postTags("</xxx>"));
+                .highlight(highlight().field("field1").order("score").preTags("<xxx>").postTags("</xxx>"));
 
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -780,7 +717,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on _all, highlighting on field1");
         source = searchSource()
                 .query(termQuery("_all", "test"))
-                .highlighter(highlight().field("field1").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field1").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -789,7 +726,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on _all, highlighting on field2");
         source = searchSource()
                 .query(termQuery("_all", "quick"))
-                .highlighter(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -798,7 +735,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on _all, highlighting on field2");
         source = searchSource()
                 .query(prefixQuery("_all", "qui"))
-                .highlighter(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -807,7 +744,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on _all with constant score, highlighting on field2");
         source = searchSource()
                 .query(constantScoreQuery(prefixQuery("_all", "qui")))
-                .highlighter(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -816,7 +753,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on _all with constant score, highlighting on field2");
         source = searchSource()
                 .query(boolQuery().should(constantScoreQuery(prefixQuery("_all", "qui"))))
-                .highlighter(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <xxx>quick</xxx> brown fox jumps over the lazy dog"));
@@ -834,7 +771,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(termQuery("field1", "test"))
-                .highlighter(highlight().field("field1", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>"));
+                .highlight(highlight().field("field1", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>"));
 
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -843,7 +780,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on _all, highlighting on field1");
         source = searchSource()
                 .query(termQuery("_all", "test"))
-                .highlighter(highlight().field("field1", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field1", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -853,7 +790,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on _all, highlighting on field2");
         source = searchSource()
                 .query(termQuery("_all", "quick"))
-                .highlighter(highlight().field("field2", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field2", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -863,7 +800,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on _all, highlighting on field2");
         source = searchSource()
                 .query(prefixQuery("_all", "qui"))
-                .highlighter(highlight().field("field2", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field2", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -889,7 +826,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(termQuery("field1", "t"))
-                .highlighter(highlight().highlighterType("fvh").field("field1", 20, 1).order("score").preTags("<xxx>").postTags("</xxx>"));
+                .highlight(highlight().highlighterType("fvh").field("field1", 20, 1).order("score").preTags("<xxx>").postTags("</xxx>"));
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
         assertHighlight(searchResponse, 0, "field1", 0, 1, containsString("<xxx>t</xxx>"));
         logger.info("--> done");
@@ -957,7 +894,9 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         Field fooField = new Field("foo").numOfFragments(1).order("score").fragmentSize(25)
                 .highlighterType("fvh").requireFieldMatch(requireFieldMatch);
-        SearchRequestBuilder req = client().prepareSearch("test").highlighter(new HighlightBuilder().field(fooField));
+        Field barField = new Field("bar").numOfFragments(1).order("score").fragmentSize(25)
+                .highlighterType("fvh").requireFieldMatch(requireFieldMatch);
+        SearchRequestBuilder req = client().prepareSearch("test").addHighlightedField(fooField);
 
         // First check highlighting without any matched fields set
         SearchResponse resp = req.setQuery(queryStringQuery("running scissors").field("foo")).get();
@@ -969,31 +908,21 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         // Add the subfield to the list of matched fields but don't match it.  Everything should still work
         // like before we added it.
-        fooField = new Field("foo").numOfFragments(1).order("score").fragmentSize(25).highlighterType("fvh")
-                .requireFieldMatch(requireFieldMatch);
         fooField.matchedFields("foo", "foo.plain");
-        req = client().prepareSearch("test").highlighter(new HighlightBuilder().field(fooField));
         resp = req.setQuery(queryStringQuery("running scissors").field("foo")).get();
         assertHighlight(resp, 0, "foo", 0, equalTo("<em>running</em> with <em>scissors</em>"));
 
-
         // Now make half the matches come from the stored field and half from just a matched field.
         resp = req.setQuery(queryStringQuery("foo.plain:running scissors").field("foo")).get();
         assertHighlight(resp, 0, "foo", 0, equalTo("<em>running</em> with <em>scissors</em>"));
 
         // Now remove the stored field from the matched field list.  That should work too.
-        fooField = new Field("foo").numOfFragments(1).order("score").fragmentSize(25).highlighterType("fvh")
-                .requireFieldMatch(requireFieldMatch);
         fooField.matchedFields("foo.plain");
-        req = client().prepareSearch("test").highlighter(new HighlightBuilder().field(fooField));
         resp = req.setQuery(queryStringQuery("foo.plain:running scissors").field("foo")).get();
         assertHighlight(resp, 0, "foo", 0, equalTo("<em>running</em> with scissors"));
 
         // Now make sure boosted fields don't blow up when matched fields is both the subfield and stored field.
-        fooField = new Field("foo").numOfFragments(1).order("score").fragmentSize(25).highlighterType("fvh")
-                .requireFieldMatch(requireFieldMatch);
         fooField.matchedFields("foo", "foo.plain");
-        req = client().prepareSearch("test").highlighter(new HighlightBuilder().field(fooField));
         resp = req.setQuery(queryStringQuery("foo.plain:running^5 scissors").field("foo")).get();
         assertHighlight(resp, 0, "foo", 0, equalTo("<em>running</em> with <em>scissors</em>"));
 
@@ -1020,46 +949,41 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // Speaking of two fields, you can have two fields, only one of which has matchedFields enabled
         QueryBuilder twoFieldsQuery = queryStringQuery("cats").field("foo").field("foo.plain", 5)
                 .field("bar").field("bar.plain", 5);
-        Field barField = new Field("bar").numOfFragments(1).order("score").fragmentSize(25).highlighterType("fvh")
-                .requireFieldMatch(requireFieldMatch);
-        resp = req.setQuery(twoFieldsQuery).highlighter(new HighlightBuilder().field(fooField).field(barField)).get();
+        resp = req.setQuery(twoFieldsQuery).addHighlightedField(barField).get();
         assertHighlight(resp, 0, "foo", 0, equalTo("junk junk <em>cats</em> junk junk"));
         assertHighlight(resp, 0, "bar", 0, equalTo("<em>cat</em> <em>cat</em> junk junk junk junk"));
+
         // And you can enable matchedField highlighting on both
         barField.matchedFields("bar", "bar.plain");
-        resp = req.setQuery(twoFieldsQuery).highlighter(new HighlightBuilder().field(fooField).field(barField)).get();
+        resp = req.get();
         assertHighlight(resp, 0, "foo", 0, equalTo("junk junk <em>cats</em> junk junk"));
         assertHighlight(resp, 0, "bar", 0, equalTo("junk junk <em>cats</em> junk junk"));
 
         // Setting a matchedField that isn't searched/doesn't exist is simply ignored.
         barField.matchedFields("bar", "candy");
-        resp = req.setQuery(twoFieldsQuery).highlighter(new HighlightBuilder().field(fooField).field(barField)).get();
+        resp = req.get();
         assertHighlight(resp, 0, "foo", 0, equalTo("junk junk <em>cats</em> junk junk"));
         assertHighlight(resp, 0, "bar", 0, equalTo("<em>cat</em> <em>cat</em> junk junk junk junk"));
 
         // If the stored field doesn't have a value it doesn't matter what you match, you get nothing.
         barField.matchedFields("bar", "foo.plain");
-        resp = req.setQuery(queryStringQuery("running scissors").field("foo.plain").field("bar"))
-                .highlighter(new HighlightBuilder().field(fooField).field(barField)).get();
+        resp = req.setQuery(queryStringQuery("running scissors").field("foo.plain").field("bar")).get();
         assertHighlight(resp, 0, "foo", 0, equalTo("<em>running</em> with <em>scissors</em>"));
         assertThat(resp.getHits().getAt(0).getHighlightFields(), not(hasKey("bar")));
 
         // If the stored field is found but the matched field isn't then you don't get a result either.
         fooField.matchedFields("bar.plain");
-        resp = req.setQuery(queryStringQuery("running scissors").field("foo").field("foo.plain").field("bar").field("bar.plain"))
-                .highlighter(new HighlightBuilder().field(fooField).field(barField)).get();
+        resp = req.setQuery(queryStringQuery("running scissors").field("foo").field("foo.plain").field("bar").field("bar.plain")).get();
         assertThat(resp.getHits().getAt(0).getHighlightFields(), not(hasKey("foo")));
 
         // But if you add the stored field to the list of matched fields then you'll get a result again
         fooField.matchedFields("foo", "bar.plain");
-        resp = req.setQuery(queryStringQuery("running scissors").field("foo").field("foo.plain").field("bar").field("bar.plain"))
-                .highlighter(new HighlightBuilder().field(fooField).field(barField)).get();
+        resp = req.setQuery(queryStringQuery("running scissors").field("foo").field("foo.plain").field("bar").field("bar.plain")).get();
         assertHighlight(resp, 0, "foo", 0, equalTo("<em>running</em> with <em>scissors</em>"));
         assertThat(resp.getHits().getAt(0).getHighlightFields(), not(hasKey("bar")));
 
         // You _can_ highlight fields that aren't subfields of one another.
-        resp = req.setQuery(queryStringQuery("weird").field("foo").field("foo.plain").field("bar").field("bar.plain"))
-                .highlighter(new HighlightBuilder().field(fooField).field(barField)).get();
+        resp = req.setQuery(queryStringQuery("weird").field("foo").field("foo.plain").field("bar").field("bar.plain")).get();
         assertHighlight(resp, 0, "foo", 0, equalTo("<em>weird</em>"));
         assertHighlight(resp, 0, "bar", 0, equalTo("<em>resul</em>t"));
 
@@ -1084,7 +1008,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch()
                 .setSize(COUNT)
                 .setQuery(termQuery("field1", "test"))
-                .highlighter(new HighlightBuilder().field("field1", 100, 0))
+                .addHighlightedField("field1", 100, 0)
                 .get();
         for (int i = 0; i < COUNT; i++) {
             SearchHit hit = searchResponse.getHits().getHits()[i];
@@ -1096,7 +1020,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         searchResponse = client().prepareSearch()
                 .setSize(COUNT)
                 .setQuery(termQuery("_all", "test"))
-                .highlighter(new HighlightBuilder().field("_all", 100, 0))
+                .addHighlightedField("_all", 100, 0)
                 .get();
         for (int i = 0; i < COUNT; i++) {
             SearchHit hit = searchResponse.getHits().getHits()[i];
@@ -1129,7 +1053,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "bug"))
-                .highlighter(new HighlightBuilder().field("title", -1, 0))
+                .addHighlightedField("title", -1, 0)
                 .get();
 
         for (int i = 0; i < 5; i++) {
@@ -1152,7 +1076,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "bug"))
-                .highlighter(new HighlightBuilder().field("title", 30, 1, 10))
+                .addHighlightedField("title", 30, 1, 10)
                 .get();
 
         for (int i = 0; i < 5; i++) {
@@ -1176,7 +1100,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title", 50, 1, 10))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title", 50, 1, 10)
                 .get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
@@ -1199,7 +1124,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title", 30, 1, 10))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title", 30, 1, 10)
                 .get();
 
         for (int i = 0; i < 5; i++) {
@@ -1222,7 +1148,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // simple search on body with standard analyzer with a simple field query
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title", 50, 1))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title", 50, 1)
                 .get();
 
         assertHighlight(search, 0, "title", 0, 1, equalTo("this is a <em>test</em>"));
@@ -1230,7 +1157,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // search on title.key and highlight on title
         search = client().prepareSearch()
                 .setQuery(matchQuery("title.key", "this is a test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title.key", 50, 1))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title.key", 50, 1)
                 .get();
 
         assertHighlight(search, 0, "title.key", 0, 1, equalTo("<em>this</em> <em>is</em> <em>a</em> <em>test</em>"));
@@ -1253,7 +1181,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // simple search on body with standard analyzer with a simple field query
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title", 50, 1))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title", 50, 1)
                 .get();
 
         assertHighlight(search, 0, "title", 0, 1, equalTo("this is a <em>test</em>"));
@@ -1261,7 +1190,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // search on title.key and highlight on title.key
         search = client().prepareSearch()
                 .setQuery(matchQuery("title.key", "this is a test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title.key", 50, 1))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title.key", 50, 1)
                 .get();
 
         assertHighlight(search, 0, "title.key", 0, 1, equalTo("<em>this</em> <em>is</em> <em>a</em> <em>test</em>"));
@@ -1284,7 +1214,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // simple search on body with standard analyzer with a simple field query
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title", 50, 1))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title", 50, 1)
                 .get();
 
         assertHighlight(search, 0, "title", 0, 1, equalTo("this is a <em>test</em>"));
@@ -1292,7 +1223,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // search on title.key and highlight on title
         search = client().prepareSearch()
                 .setQuery(matchQuery("title.key", "this is a test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title.key", 50, 1))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title.key", 50, 1)
                 .get();
 
         assertHighlight(search, 0, "title.key", 0, 1, equalTo("<em>this</em> <em>is</em> <em>a</em> <em>test</em>"));
@@ -1314,7 +1246,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // simple search on body with standard analyzer with a simple field query
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title", 50, 1))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title", 50, 1)
                 .get();
 
         assertHighlight(search, 0, "title", 0, 1, equalTo("this is a <em>test</em>"));
@@ -1322,7 +1255,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // search on title.key and highlight on title.key
         search = client().prepareSearch()
                 .setQuery(matchQuery("title.key", "this is a test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title.key", 50, 1))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title.key", 50, 1)
                 .get();
 
         assertHighlight(search, 0, "title.key", 0, 1, equalTo("<em>this</em> <em>is</em> <em>a</em> <em>test</em>"));
@@ -1343,20 +1277,22 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchPhraseQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().field("title", 50, 1, 10))
+                .addHighlightedField("title", 50, 1, 10)
                 .get();
         assertNoFailures(search);
 
         assertFailures(client().prepareSearch()
                 .setQuery(matchPhraseQuery("title", "this is a test"))
-                        .highlighter(new HighlightBuilder().field("title", 50, 1, 10).highlighterType("fast-vector-highlighter")),
+                .addHighlightedField("title", 50, 1, 10)
+                .setHighlighterType("fast-vector-highlighter"),
                 RestStatus.BAD_REQUEST,
                 containsString("the field [title] should be indexed with term vector with position offsets to be used with fast vector highlighter"));
 
         //should not fail if there is a wildcard
         assertNoFailures(client().prepareSearch()
                 .setQuery(matchPhraseQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().field("tit*", 50, 1, 10).highlighterType("fast-vector-highlighter")).get());
+                .addHighlightedField("tit*", 50, 1, 10)
+                .setHighlighterType("fast-vector-highlighter").get());
     }
 
     @Test
@@ -1374,7 +1310,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchPhraseQuery("title", "test for the workaround"))
-                .highlighter(new HighlightBuilder().field("title", 50, 1, 10))
+                .addHighlightedField("title", 50, 1, 10)
                 .get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
@@ -1385,7 +1321,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // Using plain highlighter instead of FVH
         search = client().prepareSearch()
                 .setQuery(matchPhraseQuery("title", "test for the workaround"))
-                .highlighter(new HighlightBuilder().field("title", 50, 1, 10).highlighterType("highlighter"))
+                .addHighlightedField("title", 50, 1, 10)
+                .setHighlighterType("highlighter")
                 .get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
@@ -1395,9 +1332,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // Using plain highlighter instead of FVH on the field level
         search = client().prepareSearch()
                 .setQuery(matchPhraseQuery("title", "test for the workaround"))
-                .highlighter(
-                        new HighlightBuilder().field(new HighlightBuilder.Field("title").highlighterType("highlighter")).highlighterType(
-                                "highlighter"))
+                .addHighlightedField(new HighlightBuilder.Field("title").highlighterType("highlighter"))
+                .setHighlighterType("highlighter")
                 .get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
@@ -1418,7 +1354,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("tags", "tag"))
-                .highlighter(new HighlightBuilder().field("tags", -1, 0)).get();
+                .addHighlightedField("tags", -1, 0).get();
 
         assertHighlight(response, 0, "tags", 0, equalTo("this is a really long <em>tag</em> i would like to highlight"));
         assertHighlight(response, 0, "tags", 1, 2, equalTo("here is another one that is very long and has the <em>tag</em> token near the end"));
@@ -1435,7 +1371,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(boostingQuery(termQuery("field2", "brown"), termQuery("field2", "foobar")).negativeBoost(0.5f))
-                .highlighter(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
 
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -1454,7 +1390,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(boostingQuery(termQuery("field2", "brown"), termQuery("field2", "foobar")).negativeBoost(0.5f))
-                .highlighter(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
 
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -1474,7 +1410,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(commonTermsQuery("field2", "quick brown").cutoffFrequency(100))
-                .highlighter(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
 
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <x>quick</x> <x>brown</x> fox jumps over the lazy dog"));
@@ -1489,7 +1425,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource().query(commonTermsQuery("field2", "quick brown").cutoffFrequency(100))
-                .highlighter(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
 
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -1519,7 +1455,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field0");
         SearchSourceBuilder source = searchSource()
                 .query(matchPhrasePrefixQuery("field0", "quick bro"))
-                .highlighter(highlight().field("field0").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field0").order("score").preTags("<x>").postTags("</x>"));
 
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -1528,7 +1464,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         source = searchSource()
                 .query(matchPhrasePrefixQuery("field1", "quick bro"))
-                .highlighter(highlight().field("field1").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field1").order("score").preTags("<x>").postTags("</x>"));
 
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -1545,7 +1481,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
 
         source = searchSource().postFilter(typeQuery("type2")).query(matchPhrasePrefixQuery("field3", "fast bro"))
-                .highlighter(highlight().field("field3").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field3").order("score").preTags("<x>").postTags("</x>"));
 
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -1553,7 +1489,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field4");
         source = searchSource().postFilter(typeQuery("type2")).query(matchPhrasePrefixQuery("field4", "the fast bro"))
-                .highlighter(highlight().field("field4").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field4").order("score").preTags("<x>").postTags("</x>"));
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
         assertHighlight(searchResponse, 0, "field4", 0, 1, anyOf(equalTo("<x>The quick browse</x> button is a fancy thing, right bro?"), equalTo("<x>The quick brown</x> fox jumps over the lazy dog")));
@@ -1561,7 +1497,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field4");
         source = searchSource().postFilter(typeQuery("type2")).query(matchPhrasePrefixQuery("field4", "a fast quick blue ca"))
-                .highlighter(highlight().field("field4").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field4").order("score").preTags("<x>").postTags("</x>"));
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
         assertHighlight(searchResponse, 0, "field4", 0, 1, equalTo("<x>a quick fast blue car</x>"));
@@ -1580,27 +1516,24 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("tags", "long tag").type(MatchQuery.Type.PHRASE))
-                .highlighter(
-                        new HighlightBuilder().field(new HighlightBuilder.Field("tags").fragmentSize(-1).numOfFragments(2)
-                                .fragmenter("simple"))).get();
+                .addHighlightedField(new HighlightBuilder.Field("tags")
+                        .fragmentSize(-1).numOfFragments(2).fragmenter("simple")).get();
 
         assertHighlight(response, 0, "tags", 0, equalTo("this is a really <em>long</em> <em>tag</em> i would like to highlight"));
         assertHighlight(response, 0, "tags", 1, 2, equalTo("here is another one that is very <em>long</em> <em>tag</em> and has the <em>tag</em> token near the end"));
 
         response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("tags", "long tag").type(MatchQuery.Type.PHRASE))
-                .highlighter(
-                        new HighlightBuilder().field(new HighlightBuilder.Field("tags").fragmentSize(-1).numOfFragments(2)
-                                .fragmenter("span"))).get();
+                .addHighlightedField(new HighlightBuilder.Field("tags")
+                        .fragmentSize(-1).numOfFragments(2).fragmenter("span")).get();
 
         assertHighlight(response, 0, "tags", 0, equalTo("this is a really <em>long</em> <em>tag</em> i would like to highlight"));
         assertHighlight(response, 0, "tags", 1, 2, equalTo("here is another one that is very <em>long</em> <em>tag</em> and has the <em>tag</em> token near the end"));
 
         assertFailures(client().prepareSearch("test")
                         .setQuery(QueryBuilders.matchQuery("tags", "long tag").type(MatchQuery.Type.PHRASE))
-                        .highlighter(
-                                new HighlightBuilder().field(new HighlightBuilder.Field("tags").fragmentSize(-1).numOfFragments(2)
-                                        .fragmenter("invalid"))),
+                        .addHighlightedField(new HighlightBuilder.Field("tags")
+                                .fragmentSize(-1).numOfFragments(2).fragmenter("invalid")),
                 RestStatus.BAD_REQUEST,
                 containsString("unknown fragmenter option [invalid] for the field [tags]"));
     }
@@ -1615,10 +1548,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("field1", "fox"))
-                .highlighter(
-                        new HighlightBuilder().field(
-                                new HighlightBuilder.Field("field1").preTags("<1>").postTags("</1>").requireFieldMatch(true)).field(
-                                new HighlightBuilder.Field("field2").preTags("<2>").postTags("</2>").requireFieldMatch(false)))
+                .addHighlightedField(new HighlightBuilder.Field("field1").preTags("<1>").postTags("</1>").requireFieldMatch(true))
+                .addHighlightedField(new HighlightBuilder.Field("field2").preTags("<2>").postTags("</2>").requireFieldMatch(false))
                 .get();
         assertHighlight(response, 0, "field1", 0, 1, equalTo("The <b>quick<b> brown <1>fox</1>"));
         assertHighlight(response, 0, "field2", 0, 1, equalTo("The <b>slow<b> brown <2>fox</2>"));
@@ -1635,10 +1566,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("field1", "fox"))
-                .highlighter(
-                        new HighlightBuilder().field(
-                                new HighlightBuilder.Field("field1").preTags("<1>").postTags("</1>").requireFieldMatch(true)).field(
-                                new HighlightBuilder.Field("field2").preTags("<2>").postTags("</2>").requireFieldMatch(false)))
+                .addHighlightedField(new HighlightBuilder.Field("field1").preTags("<1>").postTags("</1>").requireFieldMatch(true))
+                .addHighlightedField(new HighlightBuilder.Field("field2").preTags("<2>").postTags("</2>").requireFieldMatch(false))
                 .get();
         assertHighlight(response, 0, "field1", 0, 1, equalTo("The <b>quick<b> brown <1>fox</1>"));
         assertHighlight(response, 0, "field2", 0, 1, equalTo("The <b>slow<b> brown <2>fox</2>"));
@@ -1658,9 +1587,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // This query used to fail when the field to highlight was absent
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("field", "highlight").type(MatchQuery.Type.BOOLEAN))
-                .highlighter(
-                        new HighlightBuilder().field(new HighlightBuilder.Field("highlight_field").fragmentSize(-1).numOfFragments(1)
-                                .fragmenter("simple"))).get();
+                .addHighlightedField(new HighlightBuilder.Field("highlight_field")
+                        .fragmentSize(-1).numOfFragments(1).fragmenter("simple")).get();
         assertThat(response.getHits().hits()[0].highlightFields().isEmpty(), equalTo(true));
     }
 
@@ -1679,9 +1607,13 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("text", "test").type(MatchQuery.Type.BOOLEAN))
-                .highlighter(
-                        new HighlightBuilder().field("text").field("byte").field("short").field("int").field("long").field("float")
-                                .field("double"))
+                .addHighlightedField("text")
+                .addHighlightedField("byte")
+                .addHighlightedField("short")
+                .addHighlightedField("int")
+                .addHighlightedField("long")
+                .addHighlightedField("float")
+                .addHighlightedField("double")
                 .get();
         // Highlighting of numeric fields is not supported, but it should not raise errors
         // (this behavior is consistent with version 0.20)
@@ -1705,7 +1637,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("text", "test").type(MatchQuery.Type.BOOLEAN))
-                .highlighter(new HighlightBuilder().field("text")).execute().actionGet();
+                .addHighlightedField("text").execute().actionGet();
         // PatternAnalyzer will throw an exception if it is resetted twice
         assertHitCount(response, 1l);
     }
@@ -1721,9 +1653,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         HighlightBuilder.Field field = new HighlightBuilder.Field("text");
 
-        HighlightBuilder highlightBuilder = new HighlightBuilder().field(field);
         SearchRequestBuilder search = client().prepareSearch("test").setQuery(QueryBuilders.matchQuery("text", "testing"))
-                .highlighter(highlightBuilder);
+                .addHighlightedField(field);
         Matcher<String> searchQueryMatcher = equalTo("<em>Testing</em> the highlight query feature");
 
         field.highlighterType("plain");
@@ -1736,12 +1667,9 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         response = search.get();
         assertHighlight(response, 0, "text", 0, searchQueryMatcher);
 
-        field = new HighlightBuilder.Field("text");
 
         Matcher<String> hlQueryMatcher = equalTo("Testing the highlight <em>query</em> feature");
         field.highlightQuery(matchQuery("text", "query"));
-        highlightBuilder = new HighlightBuilder().field(field);
-        search = client().prepareSearch("test").setQuery(QueryBuilders.matchQuery("text", "testing")).highlighter(highlightBuilder);
 
         field.highlighterType("fvh");
         response = search.get();
@@ -1756,7 +1684,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         assertHighlight(response, 0, "text", 0, hlQueryMatcher);
 
         // Make sure the the highlightQuery is taken into account when it is set on the highlight context instead of the field
-        highlightBuilder.highlightQuery(matchQuery("text", "query"));
+        search.setHighlighterQuery(matchQuery("text", "query"));
         field.highlighterType("fvh").highlightQuery(null);
         response = search.get();
         assertHighlight(response, 0, "text", 0, hlQueryMatcher);
@@ -1792,97 +1720,97 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .fragmentSize(21)
                 .numOfFragments(1)
                 .highlighterType("plain");
-        SearchResponse response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        SearchResponse response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         // When noMatchSize is set to 0 you also shouldn't get any
         field.highlighterType("plain").noMatchSize(0);
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         // When noMatchSize is between 0 and the size of the string
         field.highlighterType("plain").noMatchSize(21);
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so"));
 
         // The FVH also works but the fragment is longer than the plain highlighter because of boundary_max_scan
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so some"));
 
         // Postings hl also works but the fragment is the whole first sentence (size ignored)
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so some of me should get cut off."));
 
         // We can also ask for a fragment longer than the input string and get the whole string
         field.highlighterType("plain").noMatchSize(text.length() * 2);
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo(text));
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo(text));
 
         //no difference using postings hl as the noMatchSize is ignored (just needs to be greater than 0)
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so some of me should get cut off."));
 
         // We can also ask for a fragment exactly the size of the input field and get the whole field
         field.highlighterType("plain").noMatchSize(text.length());
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo(text));
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo(text));
 
         //no difference using postings hl as the noMatchSize is ignored (just needs to be greater than 0)
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so some of me should get cut off."));
 
         // You can set noMatchSize globally in the highlighter as well
         field.highlighterType("plain").noMatchSize(null);
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field).noMatchSize(21)).get();
+        response = client().prepareSearch("test").setHighlighterNoMatchSize(21).addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so"));
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field).noMatchSize(21)).get();
+        response = client().prepareSearch("test").setHighlighterNoMatchSize(21).addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so some"));
 
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field).noMatchSize(21)).get();
+        response = client().prepareSearch("test").setHighlighterNoMatchSize(21).addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so some of me should get cut off."));
 
         // We don't break if noMatchSize is less than zero though
         field.highlighterType("plain").noMatchSize(randomIntBetween(Integer.MIN_VALUE, -1));
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
     }
 
@@ -1903,16 +1831,16 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .numOfFragments(1)
                 .highlighterType("plain")
                 .noMatchSize(21);
-        SearchResponse response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        SearchResponse response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so"));
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so some"));
 
         // Postings hl also works but the fragment is the whole first sentence (size ignored)
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so some of me should get cut off."));
 
         // And noMatchSize returns nothing when the first entry is empty string!
@@ -1923,19 +1851,19 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         field.highlighterType("plain");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("fvh");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("postings");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         // But if the field was actually empty then you should get no highlighting field
@@ -1945,19 +1873,19 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         field.highlighterType("plain");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("fvh");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("postings");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         // Same for if the field doesn't even exist on the document
@@ -1968,34 +1896,34 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         field.highlighterType("plain");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("fvh");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("fvh");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "postings");
 
         // Again same if the field isn't mapped
         field = new HighlightBuilder.Field("unmapped")
                 .highlighterType("plain")
                 .noMatchSize(21);
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
     }
 
@@ -2017,32 +1945,32 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .numOfFragments(0)
                 .highlighterType("plain")
                 .noMatchSize(20);
-        SearchResponse response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        SearchResponse response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("This is the first"));
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("This is the first sentence"));
 
         // Postings hl also works but the fragment is the whole first sentence (size ignored)
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("This is the first sentence."));
 
         //if there's a match we only return the values with matches (whole value as number_of_fragments == 0)
         MatchQueryBuilder queryBuilder = QueryBuilders.matchQuery("text", "third fifth");
         field.highlighterType("plain");
-        response = client().prepareSearch("test").setQuery(queryBuilder).highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").setQuery(queryBuilder).addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 2, equalTo("This is the <em>third</em> sentence. This is the fourth sentence."));
         assertHighlight(response, 0, "text", 1, 2, equalTo("This is the <em>fifth</em> sentence"));
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").setQuery(queryBuilder).highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").setQuery(queryBuilder).addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 2, equalTo("This is the <em>third</em> sentence. This is the fourth sentence."));
         assertHighlight(response, 0, "text", 1, 2, equalTo("This is the <em>fifth</em> sentence"));
 
         field.highlighterType("postings");
-        response = client().prepareSearch("test").setQuery(queryBuilder).highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").setQuery(queryBuilder).addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 2, equalTo("This is the <em>third</em> sentence. This is the fourth sentence."));
         assertHighlight(response, 0, "text", 1, 2, equalTo("This is the <em>fifth</em> sentence"));
     }
@@ -2059,7 +1987,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(termQuery("field1", "test"))
-                .highlighter(highlight().field("field1").preTags("<xxx>").postTags("</xxx>"));
+                .highlight(highlight().field("field1").preTags("<xxx>").postTags("</xxx>"));
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("this is a <xxx>test</xxx>"));
@@ -2067,7 +1995,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on field1, highlighting on field1");
         source = searchSource()
                 .query(termQuery("field1", "test"))
-                .highlighter(highlight().field("field1").preTags("<xxx>").postTags("</xxx>"));
+                .highlight(highlight().field("field1").preTags("<xxx>").postTags("</xxx>"));
 
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -2076,7 +2004,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on field2, highlighting on field2");
         source = searchSource()
                 .query(termQuery("field2", "quick"))
-                .highlighter(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>"));
+                .highlight(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>"));
 
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -2085,7 +2013,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on field2, highlighting on field2");
         source = searchSource()
                 .query(matchPhraseQuery("field2", "quick brown"))
-                .highlighter(highlight().field("field2").preTags("<xxx>").postTags("</xxx>"));
+                .highlight(highlight().field("field2").preTags("<xxx>").postTags("</xxx>"));
 
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -2096,7 +2024,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on field2, highlighting on field2, falling back to the plain highlighter");
         source = searchSource()
                 .query(matchPhraseQuery("_all", "quick brown"))
-                .highlighter(highlight().field("field2").preTags("<xxx>").postTags("</xxx>").highlighterType("highlighter").requireFieldMatch(false));
+                .highlight(highlight().field("field2").preTags("<xxx>").postTags("</xxx>").highlighterType("highlighter").requireFieldMatch(false));
 
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -2113,9 +2041,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("field1", "fox"))
-                .highlighter(
-                        new HighlightBuilder().field(new HighlightBuilder.Field("field1").preTags("<1>").postTags("</1>")
-                                .requireFieldMatch(true)))
+                .addHighlightedField(new HighlightBuilder.Field("field1").preTags("<1>").postTags("</1>").requireFieldMatch(true))
                 .get();
         assertHighlight(response, 0, "field1", 0, 1, equalTo("The <b>quick<b> brown <1>fox</1>."));
     }
@@ -2133,7 +2059,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(termQuery("field1", "fox"))
-                .highlighter(highlight()
+                .highlight(highlight()
                         .field(new HighlightBuilder.Field("field1").numOfFragments(5).preTags("<field1>").postTags("</field1>")));
 
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
@@ -2148,7 +2074,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         source = searchSource()
                 .query(termQuery("field1", "fox"))
-                .highlighter(highlight()
+                .highlight(highlight()
                         .field(new HighlightBuilder.Field("field1").numOfFragments(0).preTags("<field1>").postTags("</field1>")));
 
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
@@ -2198,7 +2124,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
             SearchSourceBuilder source = searchSource()
                     .query(multiMatchQueryBuilder)
-                    .highlighter(highlight().highlightQuery(randomBoolean() ? multiMatchQueryBuilder : null).highlighterType(highlighterType)
+                    .highlight(highlight().highlightQuery(randomBoolean() ? multiMatchQueryBuilder : null).highlighterType(highlighterType)
                             .field(new Field("field1").requireFieldMatch(true).preTags("<field1>").postTags("</field1>")));
             logger.info("Running multi-match type: [" + matchQueryType + "] highlight with type: [" + highlighterType + "]");
             SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
@@ -2222,7 +2148,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(termQuery("field1", "sentence"))
-                .highlighter(highlight().field("field1").order("score"));
+                .highlight(highlight().field("field1").order("score"));
 
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -2252,7 +2178,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse searchResponse = client().prepareSearch()
                 .setQuery(matchQuery("title", "test"))
-                .highlighter(new HighlightBuilder().field("title").encoder("html")).get();
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title").get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
             assertHighlight(searchResponse, i, "title", 0, 1, equalTo("This is a html escaping highlighting <em>test</em> for *&amp;?"));
@@ -2276,7 +2203,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch()
                 //lets make sure we analyze the query and we highlight the resulting terms
                 .setQuery(matchQuery("title", "This is a Test"))
-.highlighter(new HighlightBuilder().field("title")).get();
+                .addHighlightedField("title").get();
 
         assertHitCount(searchResponse, 1l);
         SearchHit hit = searchResponse.getHits().getAt(0);
@@ -2286,7 +2213,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // search on title.key and highlight on title
         searchResponse = client().prepareSearch()
                 .setQuery(matchQuery("title.key", "this is a test"))
-                .highlighter(new HighlightBuilder().field("title.key")).get();
+                .addHighlightedField("title.key").get();
         assertHitCount(searchResponse, 1l);
 
         //stopwords are now highlighted since we used only whitespace analyzer here
@@ -2310,7 +2237,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // simple search on body with standard analyzer with a simple field query
         SearchResponse searchResponse = client().prepareSearch()
                 .setQuery(matchQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().field("title"))
+                .addHighlightedField("title")
                 .get();
 
         assertHighlight(searchResponse, 0, "title", 0, 1, equalTo("this is a <em>test</em>"));
@@ -2318,7 +2245,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // search on title.key and highlight on title.key
         searchResponse = client().prepareSearch()
                 .setQuery(matchQuery("title.key", "this is a test"))
-                .highlighter(new HighlightBuilder().field("title.key")).get();
+                .addHighlightedField("title.key").get();
 
         assertHighlight(searchResponse, 0, "title.key", 0, 1, equalTo("<em>this</em> <em>is</em> <em>a</em> <em>test</em>"));
     }
@@ -2340,27 +2267,30 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().field("title"))
+                .addHighlightedField("title")
                 .get();
         assertNoFailures(search);
 
         assertFailures(client().prepareSearch()
                         .setQuery(matchQuery("title", "this is a test"))
-                        .highlighter(new HighlightBuilder().field("title").highlighterType("postings-highlighter")),
+                        .addHighlightedField("title")
+                        .setHighlighterType("postings-highlighter"),
                 RestStatus.BAD_REQUEST,
                 containsString("the field [title] should be indexed with positions and offsets in the postings list to be used with postings highlighter"));
 
 
         assertFailures(client().prepareSearch()
                         .setQuery(matchQuery("title", "this is a test"))
-                        .highlighter(new HighlightBuilder().field("title").highlighterType("postings")),
+                        .addHighlightedField("title")
+                        .setHighlighterType("postings"),
                 RestStatus.BAD_REQUEST,
                 containsString("the field [title] should be indexed with positions and offsets in the postings list to be used with postings highlighter"));
 
         //should not fail if there is a wildcard
         assertNoFailures(client().prepareSearch()
                         .setQuery(matchQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().field("tit*").highlighterType("postings")).get());
+                        .addHighlightedField("tit*")
+                        .setHighlighterType("postings").get());
     }
 
     @Test
@@ -2374,7 +2304,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(boostingQuery(termQuery("field2", "brown"), termQuery("field2", "foobar")).negativeBoost(0.5f))
-                .highlighter(highlight().field("field2").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field2").preTags("<x>").postTags("</x>"));
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The quick <x>brown</x> fox jumps over the lazy dog!"));
@@ -2389,7 +2319,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource().query(commonTermsQuery("field2", "quick brown").cutoffFrequency(100))
-                .highlighter(highlight().field("field2").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field2").preTags("<x>").postTags("</x>"));
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
         assertHitCount(searchResponse, 1l);
 
@@ -2415,7 +2345,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field2");
 
         SearchSourceBuilder source = searchSource().query(prefixQuery("field2", "qui"))
-                .highlighter(highlight().field("field2"));
+                .highlight(highlight().field("field2"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <em>quick</em> brown fox jumps over the lazy dog!"));
 
@@ -2430,7 +2360,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
         logger.info("--> highlighting and searching on field2");
         SearchSourceBuilder source = searchSource().query(fuzzyQuery("field2", "quck"))
-                .highlighter(highlight().field("field2"));
+                .highlight(highlight().field("field2"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <em>quick</em> brown fox jumps over the lazy dog!"));
@@ -2445,7 +2375,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
         logger.info("--> highlighting and searching on field2");
         SearchSourceBuilder source = searchSource().query(regexpQuery("field2", "qu[a-l]+k"))
-                .highlighter(highlight().field("field2"));
+                .highlight(highlight().field("field2"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <em>quick</em> brown fox jumps over the lazy dog!"));
@@ -2460,13 +2390,13 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
         logger.info("--> highlighting and searching on field2");
         SearchSourceBuilder source = searchSource().query(wildcardQuery("field2", "qui*"))
-                .highlighter(highlight().field("field2"));
+                .highlight(highlight().field("field2"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <em>quick</em> brown fox jumps over the lazy dog!"));
 
         source = searchSource().query(wildcardQuery("field2", "qu*k"))
-                .highlighter(highlight().field("field2"));
+                .highlight(highlight().field("field2"));
         searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHitCount(searchResponse, 1l);
 
@@ -2482,7 +2412,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
         logger.info("--> highlighting and searching on field2");
         SearchSourceBuilder source = searchSource().query(rangeQuery("field2").gte("aaaa").lt("zzzz"))
-                .highlighter(highlight().field("field2"));
+                .highlight(highlight().field("field2"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("<em>aaab</em>"));
@@ -2497,7 +2427,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
         logger.info("--> highlighting and searching on field2");
         SearchSourceBuilder source = searchSource().query(queryStringQuery("qui*").defaultField("field2"))
-                .highlighter(highlight().field("field2"));
+                .highlight(highlight().field("field2"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <em>quick</em> brown fox jumps over the lazy dog!"));
     }
@@ -2513,7 +2443,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource().query(constantScoreQuery(regexpQuery("field1", "pho[a-z]+")))
-                .highlighter(highlight().field("field1"));
+                .highlight(highlight().field("field1"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("The <em>photography</em> word will get highlighted"));
     }
@@ -2532,7 +2462,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .should(constantScoreQuery(QueryBuilders.missingQuery("field1")))
                 .should(matchQuery("field1", "test"))
                 .should(constantScoreQuery(queryStringQuery("field1:photo*"))))
-                .highlighter(highlight().field("field1"));
+                .highlight(highlight().field("field1"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("The <em>photography</em> word will get highlighted"));
     }
@@ -2548,7 +2478,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource().query(boolQuery().must(prefixQuery("field1", "photo")).should(matchQuery("field1", "test").minimumShouldMatch("0")))
-                .highlighter(highlight().field("field1"));
+                .highlight(highlight().field("field1"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("The <em>photography</em> word will get highlighted"));
     }
@@ -2564,7 +2494,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource().query(boolQuery().must(queryStringQuery("field1:photo*")).filter(missingQuery("field_null")))
-                .highlighter(highlight().field("field1"));
+                .highlight(highlight().field("field1"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("The <em>photography</em> word will get highlighted"));
     }
@@ -2593,10 +2523,10 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         SearchRequestBuilder searchRequestBuilder = client().prepareSearch()
                 .setSize(COUNT)
                 .setQuery(termQuery("field1", "test"))
-                .highlighter(new HighlightBuilder().field("field1"));
+                .addHighlightedField("field1");
         SearchResponse searchResponse =
                 searchRequestBuilder.get();
-        assertHitCount(searchResponse, COUNT);
+        assertHitCount(searchResponse, (long)COUNT);
         assertThat(searchResponse.getHits().hits().length, equalTo(COUNT));
         for (SearchHit hit : searchResponse.getHits()) {
             String prefix = prefixes.get(hit.id());
@@ -2666,8 +2596,9 @@ public class HighlighterSearchIT extends ESIntegTestCase {
             phraseBoostTestCaseForClauses(String highlighterType, float boost, QueryBuilder terms, P phrase) {
         Matcher<String> highlightedMatcher = Matchers.either(containsString("<em>highlight words together</em>")).or(
                 containsString("<em>highlight</em> <em>words</em> <em>together</em>"));
-        SearchRequestBuilder search = client().prepareSearch("test").highlighter(
-                new HighlightBuilder().field("field1", 100, 1).order("score").highlighterType(highlighterType).requireFieldMatch(true));
+        SearchRequestBuilder search = client().prepareSearch("test").setHighlighterRequireFieldMatch(true)
+                .setHighlighterOrder("score").setHighlighterType(highlighterType)
+                .addHighlightedField("field1", 100, 1);
 
         // Try with a bool query
         phrase.boost(boost);
diff --git a/core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java b/core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java
index 4c8243c..84a315c 100644
--- a/core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java
+++ b/core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java
@@ -36,7 +36,6 @@ import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.search.SearchHits;
 import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
-import org.elasticsearch.search.highlight.HighlightBuilder;
 import org.elasticsearch.search.sort.SortBuilders;
 import org.elasticsearch.search.sort.SortOrder;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -49,24 +48,9 @@ import java.util.List;
 import java.util.Locale;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
-import static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.hasChildQuery;
-import static org.elasticsearch.index.query.QueryBuilders.hasParentQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.nestedQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAllSuccessful;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasId;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
+import static org.elasticsearch.index.query.QueryBuilders.*;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
 
 /**
  */
@@ -112,14 +96,11 @@ public class InnerHitsIT extends ESIntegTestCase {
                 .endObject()));
         indexRandom(true, requests);
 
-        InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addNestedInnerHits("comment", "comments",
-                new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.message", "fox")));
         // Inner hits can be defined in two ways: 1) with the query 2) as seperate inner_hit definition
         SearchRequest[] searchRequests = new SearchRequest[]{
                 client().prepareSearch("articles").setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHits("comment", null))).request(),
                 client().prepareSearch("articles").setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")))
-                        .innerHits(innerHitsBuilder).request()
+                        .addNestedInnerHits("comment", "comments", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.message", "fox"))).request()
         };
         for (SearchRequest searchRequest : searchRequests) {
             SearchResponse response = client().search(searchRequest).actionGet();
@@ -138,15 +119,10 @@ public class InnerHitsIT extends ESIntegTestCase {
             assertThat(innerHits.getAt(1).getNestedIdentity().getOffset(), equalTo(1));
         }
 
-        innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addNestedInnerHits("comment", "comments",
-                new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.message", "elephant")));
-        // Inner hits can be defined in two ways: 1) with the query 2) as
-        // seperate inner_hit definition
         searchRequests = new SearchRequest[] {
                 client().prepareSearch("articles")
                         .setQuery(nestedQuery("comments", matchQuery("comments.message", "elephant")))
-                        .innerHits(innerHitsBuilder).request(),
+                        .addNestedInnerHits("comment", "comments", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.message", "elephant"))).request(),
                 client().prepareSearch("articles")
                         .setQuery(nestedQuery("comments", matchQuery("comments.message", "elephant")).innerHit(new QueryInnerHits("comment", null))).request(),
                 client().prepareSearch("articles")
@@ -173,23 +149,21 @@ public class InnerHitsIT extends ESIntegTestCase {
             assertThat(innerHits.getAt(2).getNestedIdentity().getOffset(), equalTo(2));
         }
         InnerHitsBuilder.InnerHit innerHit = new InnerHitsBuilder.InnerHit();
-        innerHit.highlighter(new HighlightBuilder().field("comments.message"));
+        innerHit.highlightBuilder().field("comments.message");
         innerHit.setExplain(true);
         innerHit.addFieldDataField("comments.message");
         innerHit.addScriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap()));
         innerHit.setSize(1);
-        innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addNestedInnerHits("comments", "comments", new InnerHitsBuilder.InnerHit()
+        searchRequests = new SearchRequest[] {
+                client().prepareSearch("articles")
+                        .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")))
+                        .addNestedInnerHits("comments", "comments", new InnerHitsBuilder.InnerHit()
                                 .setQuery(matchQuery("comments.message", "fox"))
-                            .highlighter(new HighlightBuilder().field("comments.message"))
+                                .addHighlightedField("comments.message")
                                 .setExplain(true)
                                 .addFieldDataField("comments.message")
                                 .addScriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap()))
-                            .setSize(1));
-        searchRequests = new SearchRequest[] {
-                client().prepareSearch("articles")
-                        .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")))
-                        .innerHits(innerHitsBuilder).request(),
+                                .setSize(1)).request(),
                 client().prepareSearch("articles")
                         .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHits(null, innerHit))).request()
         };
@@ -234,13 +208,11 @@ public class InnerHitsIT extends ESIntegTestCase {
         int size = randomIntBetween(0, numDocs);
         SearchResponse searchResponse;
         if (randomBoolean()) {
-            InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-            innerHitsBuilder.addNestedInnerHits("a", "field1", new InnerHitsBuilder.InnerHit().addSort("_doc", SortOrder.DESC).setSize(size)); // Sort order is DESC, because we reverse the inner objects during indexing!
-            innerHitsBuilder.addNestedInnerHits("b", "field2", new InnerHitsBuilder.InnerHit().addSort("_doc", SortOrder.DESC).setSize(size));
             searchResponse = client().prepareSearch("idx")
                     .setSize(numDocs)
                     .addSort("_uid", SortOrder.ASC)
-                    .innerHits(innerHitsBuilder)
+                    .addNestedInnerHits("a", "field1", new InnerHitsBuilder.InnerHit().addSort("_doc", SortOrder.DESC).setSize(size)) // Sort order is DESC, because we reverse the inner objects during indexing!
+                    .addNestedInnerHits("b", "field2", new InnerHitsBuilder.InnerHit().addSort("_doc", SortOrder.DESC).setSize(size))
                     .get();
         } else {
             BoolQueryBuilder boolQuery = new BoolQueryBuilder();
@@ -302,12 +274,10 @@ public class InnerHitsIT extends ESIntegTestCase {
         requests.add(client().prepareIndex("articles", "comment", "6").setParent("2").setSource("message", "elephant scared by mice x y"));
         indexRandom(true, requests);
 
-        InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "fox")));
         SearchRequest[] searchRequests = new SearchRequest[]{
                 client().prepareSearch("articles")
                         .setQuery(hasChildQuery("comment", matchQuery("message", "fox")))
-                        .innerHits(innerHitsBuilder)
+                        .addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "fox")))
                         .request(),
                 client().prepareSearch("articles")
                         .setQuery(hasChildQuery("comment", matchQuery("message", "fox")).innerHit(new QueryInnerHits("comment", null)))
@@ -330,12 +300,10 @@ public class InnerHitsIT extends ESIntegTestCase {
             assertThat(innerHits.getAt(1).type(), equalTo("comment"));
         }
 
-        innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "elephant")));
         searchRequests = new SearchRequest[] {
                 client().prepareSearch("articles")
                         .setQuery(hasChildQuery("comment", matchQuery("message", "elephant")))
-                        .innerHits(innerHitsBuilder)
+                        .addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "elephant")))
                         .request(),
                 client().prepareSearch("articles")
                         .setQuery(hasChildQuery("comment", matchQuery("message", "elephant")).innerHit(new QueryInnerHits()))
@@ -359,24 +327,22 @@ public class InnerHitsIT extends ESIntegTestCase {
             assertThat(innerHits.getAt(2).type(), equalTo("comment"));
         }
         InnerHitsBuilder.InnerHit innerHit = new InnerHitsBuilder.InnerHit();
-        innerHit.highlighter(new HighlightBuilder().field("message"));
+        innerHit.highlightBuilder().field("message");
         innerHit.setExplain(true);
         innerHit.addFieldDataField("message");
         innerHit.addScriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap()));
         innerHit.setSize(1);
-        innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit()
+        searchRequests = new SearchRequest[] {
+                client().prepareSearch("articles")
+                        .setQuery(hasChildQuery("comment", matchQuery("message", "fox")))
+                        .addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit()
                                         .setQuery(matchQuery("message", "fox"))
-                            .highlighter(new HighlightBuilder().field("message"))
+                                        .addHighlightedField("message")
                                         .setExplain(true)
                                         .addFieldDataField("message")
                                         .addScriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap()))
-                            .setSize(1));
-        searchRequests = new SearchRequest[] {
-                client().prepareSearch("articles")
-                        .setQuery(hasChildQuery("comment", matchQuery("message", "fox")))
-                        .innerHits(innerHitsBuilder)
-                        .request(),
+                                        .setSize(1)
+                        ).request(),
 
                 client().prepareSearch("articles")
                         .setQuery(
@@ -427,16 +393,14 @@ public class InnerHitsIT extends ESIntegTestCase {
         indexRandom(true, requestBuilders);
 
         int size = randomIntBetween(0, numDocs);
-        InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addParentChildInnerHits("a", "child1", new InnerHitsBuilder.InnerHit().addSort("_uid", SortOrder.ASC).setSize(size));
-        innerHitsBuilder.addParentChildInnerHits("b", "child2", new InnerHitsBuilder.InnerHit().addSort("_uid", SortOrder.ASC).setSize(size));
         SearchResponse searchResponse;
         if (randomBoolean()) {
             searchResponse = client().prepareSearch("idx")
                     .setSize(numDocs)
                     .setTypes("parent")
                     .addSort("_uid", SortOrder.ASC)
-                    .innerHits(innerHitsBuilder)
+                    .addParentChildInnerHits("a", "child1", new InnerHitsBuilder.InnerHit().addSort("_uid", SortOrder.ASC).setSize(size))
+                    .addParentChildInnerHits("b", "child2", new InnerHitsBuilder.InnerHit().addSort("_uid", SortOrder.ASC).setSize(size))
                     .get();
         } else {
             BoolQueryBuilder boolQuery = new BoolQueryBuilder();
@@ -492,15 +456,12 @@ public class InnerHitsIT extends ESIntegTestCase {
     }
 
     @Test
-    @AwaitsFix(bugUrl = "need validation of type or path defined in InnerHitsBuilder")
     public void testPathOrTypeMustBeDefined() {
         createIndex("articles");
         ensureGreen("articles");
         try {
-            InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-            innerHitsBuilder.addParentChildInnerHits("comment", null, new InnerHitsBuilder.InnerHit());
             client().prepareSearch("articles")
-                    .innerHits(innerHitsBuilder)
+                    .addParentChildInnerHits("comment", null, new InnerHitsBuilder.InnerHit())
                     .get();
         } catch (Exception e) {
             assertThat(e.getMessage(), containsString("Failed to build"));
@@ -564,15 +525,13 @@ public class InnerHitsIT extends ESIntegTestCase {
         requests.add(client().prepareIndex("articles", "remark", "2").setParent("2").setRouting("2").setSource("message", "bad"));
         indexRandom(true, requests);
 
-        InnerHitsBuilder innerInnerHitsBuilder = new InnerHitsBuilder();
-        innerInnerHitsBuilder.addParentChildInnerHits("remark", "remark", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "good")));
-        InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit()
-                            .setQuery(hasChildQuery("remark", matchQuery("message", "good")))
-                            .innerHits(innerInnerHitsBuilder));
         SearchResponse response = client().prepareSearch("articles")
                 .setQuery(hasChildQuery("comment", hasChildQuery("remark", matchQuery("message", "good"))))
-                .innerHits(innerHitsBuilder)
+                .addParentChildInnerHits("comment", "comment",
+                        new InnerHitsBuilder.InnerHit()
+                                .setQuery(hasChildQuery("remark", matchQuery("message", "good")))
+                                .addParentChildInnerHits("remark", "remark", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "good")))
+                )
                 .get();
 
         assertNoFailures(response);
@@ -590,15 +549,13 @@ public class InnerHitsIT extends ESIntegTestCase {
         assertThat(innerHits.getAt(0).getId(), equalTo("1"));
         assertThat(innerHits.getAt(0).type(), equalTo("remark"));
 
-        innerInnerHitsBuilder = new InnerHitsBuilder();
-        innerInnerHitsBuilder.addParentChildInnerHits("remark", "remark", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "bad")));
-        innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit()
-                .setQuery(hasChildQuery("remark", matchQuery("message", "bad")))
-                .innerHits(innerInnerHitsBuilder));
         response = client().prepareSearch("articles")
                 .setQuery(hasChildQuery("comment", hasChildQuery("remark", matchQuery("message", "bad"))))
-                .innerHits(innerHitsBuilder)
+                .addParentChildInnerHits("comment", "comment",
+                        new InnerHitsBuilder.InnerHit()
+                                .setQuery(hasChildQuery("remark", matchQuery("message", "bad")))
+                                .addParentChildInnerHits("remark", "remark", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "bad")))
+                )
                 .get();
 
         assertNoFailures(response);
@@ -660,16 +617,12 @@ public class InnerHitsIT extends ESIntegTestCase {
                 .endObject()));
         indexRandom(true, requests);
 
-        InnerHitsBuilder innerInnerHitsBuilder = new InnerHitsBuilder();
-        innerInnerHitsBuilder.addNestedInnerHits("remark", "comments.remarks", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.remarks.message", "good")));
-        InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addNestedInnerHits("comment", "comments", new InnerHitsBuilder.InnerHit()
-                .setQuery(nestedQuery("comments.remarks", matchQuery("comments.remarks.message", "good")))
-                .innerHits(innerInnerHitsBuilder)
-        );
         SearchResponse response = client().prepareSearch("articles")
                 .setQuery(nestedQuery("comments", nestedQuery("comments.remarks", matchQuery("comments.remarks.message", "good"))))
-                .innerHits(innerHitsBuilder).get();
+                .addNestedInnerHits("comment", "comments", new InnerHitsBuilder.InnerHit()
+                                .setQuery(nestedQuery("comments.remarks", matchQuery("comments.remarks.message", "good")))
+                                .addNestedInnerHits("remark", "comments.remarks", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.remarks.message", "good")))
+                ).get();
         assertNoFailures(response);
         assertHitCount(response, 1);
         assertSearchHit(response, 1, hasId("1"));
@@ -706,15 +659,11 @@ public class InnerHitsIT extends ESIntegTestCase {
         assertThat(innerHits.getAt(0).getNestedIdentity().getChild().getField().string(), equalTo("remarks"));
         assertThat(innerHits.getAt(0).getNestedIdentity().getChild().getOffset(), equalTo(0));
 
-        innerInnerHitsBuilder = new InnerHitsBuilder();
-        innerInnerHitsBuilder.addNestedInnerHits("remark", "comments.remarks", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.remarks.message", "bad")));
-        innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addNestedInnerHits("comment", "comments", new InnerHitsBuilder.InnerHit()
-                            .setQuery(nestedQuery("comments.remarks", matchQuery("comments.remarks.message", "bad")))
-                            .innerHits(innerInnerHitsBuilder));
         response = client().prepareSearch("articles")
                 .setQuery(nestedQuery("comments", nestedQuery("comments.remarks", matchQuery("comments.remarks.message", "bad"))))
-                .innerHits(innerHitsBuilder)
+                .addNestedInnerHits("comment", "comments", new InnerHitsBuilder.InnerHit()
+                        .setQuery(nestedQuery("comments.remarks", matchQuery("comments.remarks.message", "bad")))
+                        .addNestedInnerHits("remark", "comments.remarks", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.remarks.message", "bad"))))
                 .get();
         assertNoFailures(response);
         assertHitCount(response, 1);
@@ -797,7 +746,7 @@ public class InnerHitsIT extends ESIntegTestCase {
         assertThat(response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).getNestedIdentity().getField().string(), equalTo("comments"));
         assertThat(response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).getNestedIdentity().getOffset(), equalTo(0));
         assertThat(response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).getNestedIdentity().getChild(), nullValue());
-        assertThat(String.valueOf(response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).fields().get("comments.message").<Object>getValue()), equalTo("fox eat quick"));
+        assertThat(String.valueOf((Object)response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).fields().get("comments.message").getValue()), equalTo("fox eat quick"));
     }
 
     @Test
@@ -825,7 +774,7 @@ public class InnerHitsIT extends ESIntegTestCase {
                 .endObject()));
         indexRandom(true, requests);
         InnerHitsBuilder.InnerHit builder = new InnerHitsBuilder.InnerHit();
-        builder.highlighter(new HighlightBuilder().field("comments.message"));
+        builder.highlightBuilder().field("comments.message");
         SearchResponse response = client().prepareSearch("articles")
                 .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHits(null, builder)))
                 .get();
@@ -877,7 +826,7 @@ public class InnerHitsIT extends ESIntegTestCase {
         assertThat(response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).getNestedIdentity().getField().string(), equalTo("comments"));
         assertThat(response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).getNestedIdentity().getOffset(), equalTo(0));
         assertThat(response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).getNestedIdentity().getChild(), nullValue());
-        assertThat(String.valueOf(response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).fields().get("comments.message").<Object>getValue()), equalTo("fox eat quick"));
+        assertThat(String.valueOf((Object)response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).fields().get("comments.message").getValue()), equalTo("fox eat quick"));
     }
 
     @Test
@@ -904,7 +853,7 @@ public class InnerHitsIT extends ESIntegTestCase {
                 .endObject()));
         indexRandom(true, requests);
         InnerHitsBuilder.InnerHit builder = new InnerHitsBuilder.InnerHit();
-        builder.highlighter(new HighlightBuilder().field("comments.message"));
+        builder.highlightBuilder().field("comments.message");
         SearchResponse response = client().prepareSearch("articles")
                 .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHits(null, builder)))
                         .get();
@@ -1016,23 +965,17 @@ public class InnerHitsIT extends ESIntegTestCase {
         requests.add(client().prepareIndex("royals", "baron", "baron4").setParent("earl4").setRouting("king").setSource("{}"));
         indexRandom(true, requests);
 
-        InnerHitsBuilder innerInnerHitsBuilder = new InnerHitsBuilder();
-        innerInnerHitsBuilder.addParentChildInnerHits("barons", "baron", new InnerHitsBuilder.InnerHit());
-        InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addParentChildInnerHits("earls", "earl", new InnerHitsBuilder.InnerHit()
+        SearchResponse response = client().prepareSearch("royals")
+                .setTypes("duke")
+                .addParentChildInnerHits("earls", "earl", new InnerHitsBuilder.InnerHit()
                                 .addSort(SortBuilders.fieldSort("_uid").order(SortOrder.ASC))
                                 .setSize(4)
-                .innerHits(innerInnerHitsBuilder)
-        );
-        innerInnerHitsBuilder = new InnerHitsBuilder();
-        innerInnerHitsBuilder.addParentChildInnerHits("kings", "king", new InnerHitsBuilder.InnerHit());
-        innerHitsBuilder.addParentChildInnerHits("princes", "prince",
+                                .addParentChildInnerHits("barons", "baron", new InnerHitsBuilder.InnerHit())
+                )
+                .addParentChildInnerHits("princes", "prince",
                         new InnerHitsBuilder.InnerHit()
-            .innerHits(innerInnerHitsBuilder)
-        );
-        SearchResponse response = client().prepareSearch("royals")
-                .setTypes("duke")
-                .innerHits(innerHitsBuilder)
+                        .addParentChildInnerHits("kings", "king", new InnerHitsBuilder.InnerHit())
+                )
                 .get();
         assertHitCount(response, 1);
         assertThat(response.getHits().getAt(0).getId(), equalTo("duke"));
diff --git a/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java b/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java
index b394d10..bf3e458 100644
--- a/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java
+++ b/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java
@@ -21,12 +21,11 @@ package org.elasticsearch.search.query;
 
 import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.BoolQueryBuilder;
 import org.elasticsearch.index.query.Operator;
-import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.SimpleQueryStringFlag;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
@@ -35,17 +34,8 @@ import java.util.Locale;
 import java.util.concurrent.ExecutionException;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
-import static org.elasticsearch.index.query.QueryBuilders.queryStringQuery;
-import static org.elasticsearch.index.query.QueryBuilders.simpleQueryStringQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFirstHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHits;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasId;
+import static org.elasticsearch.index.query.QueryBuilders.*;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
 import static org.hamcrest.Matchers.equalTo;
 
 /**
@@ -252,6 +242,11 @@ public class SimpleQueryStringIT extends ESIntegTestCase {
         assertHitCount(searchResponse, 3l);
         assertSearchHits(searchResponse, "1", "2", "3");
 
+        // Sending a negative 'flags' value is the same as SimpleQueryStringFlag.ALL
+        searchResponse = client().prepareSearch().setQuery("{\"simple_query_string\": {\"query\": \"foo bar\", \"flags\": -1}}").get();
+        assertHitCount(searchResponse, 3l);
+        assertSearchHits(searchResponse, "1", "2", "3");
+
         searchResponse = client().prepareSearch().setQuery(
                 simpleQueryStringQuery("foo | bar")
                         .defaultOperator(Operator.AND)
@@ -272,18 +267,21 @@ public class SimpleQueryStringIT extends ESIntegTestCase {
                         .flags(SimpleQueryStringFlag.NONE)).get();
         assertHitCount(searchResponse, 0l);
 
-        searchResponse = client()
-                .prepareSearch()
-                .setSource(
-                        new SearchSourceBuilder().query(QueryBuilders.simpleQueryStringQuery("foo|bar").defaultOperator(Operator.AND)
-                                .flags(SimpleQueryStringFlag.NONE))).get();
+        searchResponse = client().prepareSearch().setSource(new BytesArray("{\n" +
+                "  \"query\": {\n" +
+                "    \"simple_query_string\": {\n" +
+                "      \"query\": \"foo|bar\",\n" +
+                "      \"default_operator\": \"AND\"," +
+                "      \"flags\": \"NONE\"\n" +
+                "    }\n" +
+                "  }\n" +
+                "}")).get();
         assertHitCount(searchResponse, 1l);
 
-        searchResponse = client()
-                .prepareSearch()
-                .setQuery(
-                        simpleQueryStringQuery("baz | egg*").defaultOperator(Operator.AND).flags(SimpleQueryStringFlag.WHITESPACE,
-                                SimpleQueryStringFlag.PREFIX)).get();
+        searchResponse = client().prepareSearch().setQuery(
+                simpleQueryStringQuery("baz | egg*")
+                        .defaultOperator(Operator.AND)
+                        .flags(SimpleQueryStringFlag.WHITESPACE, SimpleQueryStringFlag.PREFIX)).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("4"));
     }
diff --git a/core/src/test/java/org/elasticsearch/search/suggest/CustomSuggesterSearchIT.java b/core/src/test/java/org/elasticsearch/search/suggest/CustomSuggesterSearchIT.java
index df04d43..9b97afc 100644
--- a/core/src/test/java/org/elasticsearch/search/suggest/CustomSuggesterSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/suggest/CustomSuggesterSearchIT.java
@@ -20,12 +20,13 @@ package org.elasticsearch.search.suggest;
 
 import org.elasticsearch.action.search.SearchRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.util.CollectionUtils;
+import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
-import org.elasticsearch.test.ESIntegTestCase.Scope;
 import org.junit.Test;
 
 import java.io.IOException;
@@ -34,6 +35,7 @@ import java.util.List;
 import java.util.Locale;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
+import static org.elasticsearch.test.ESIntegTestCase.Scope;
 import static org.hamcrest.Matchers.hasSize;
 import static org.hamcrest.Matchers.is;
 
@@ -57,12 +59,11 @@ public class CustomSuggesterSearchIT extends ESIntegTestCase {
                 .endObject())
                 .setRefresh(true).execute().actionGet();
         ensureYellow();
-
+        
         String randomText = randomAsciiOfLength(10);
         String randomField = randomAsciiOfLength(10);
         String randomSuffix = randomAsciiOfLength(10);
-        SuggestBuilder suggestBuilder = new SuggestBuilder();
-        suggestBuilder.addSuggestion(
+        SearchRequestBuilder searchRequestBuilder = client().prepareSearch("test").setTypes("test").setFrom(0).setSize(1).addSuggestion(
                 new SuggestBuilder.SuggestionBuilder<SuggestBuilder.SuggestionBuilder>("someName", "custom") {
                     @Override
                     protected XContentBuilder innerToXContent(XContentBuilder builder, Params params) throws IOException {
@@ -72,8 +73,6 @@ public class CustomSuggesterSearchIT extends ESIntegTestCase {
                     }
                 }.text(randomText)
         );
-        SearchRequestBuilder searchRequestBuilder = client().prepareSearch("test").setTypes("test").setFrom(0).setSize(1)
-                .suggest(suggestBuilder);
 
         SearchResponse searchResponse = searchRequestBuilder.execute().actionGet();
 
diff --git a/core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java b/core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java
index c5e0912..85993fd 100644
--- a/core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java
@@ -19,15 +19,10 @@
 
 package org.elasticsearch.search.suggest;
 
-
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
 import org.elasticsearch.action.index.IndexRequestBuilder;
-import org.elasticsearch.action.search.ReduceSearchPhaseException;
-import org.elasticsearch.action.search.SearchPhaseExecutionException;
-import org.elasticsearch.action.search.SearchRequestBuilder;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.action.search.ShardSearchFailure;
+import org.elasticsearch.action.search.*;
 import org.elasticsearch.action.suggest.SuggestRequestBuilder;
 import org.elasticsearch.action.suggest.SuggestResponse;
 import org.elasticsearch.common.io.PathUtils;
@@ -55,17 +50,8 @@ import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
 import static org.elasticsearch.search.suggest.SuggestBuilders.phraseSuggestion;
 import static org.elasticsearch.search.suggest.SuggestBuilders.termSuggestion;
 import static org.elasticsearch.search.suggest.phrase.PhraseSuggestionBuilder.candidateGenerator;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSuggestion;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSuggestionPhraseCollateMatchExists;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSuggestionSize;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThrows;
-import static org.hamcrest.Matchers.anyOf;
-import static org.hamcrest.Matchers.endsWith;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.nullValue;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
 
 /**
  * Integration tests for term and phrase suggestions.  Many of these tests many requests that vary only slightly from one another.  Where
@@ -284,14 +270,16 @@ public class SuggestSearchIT extends ESIntegTestCase {
 
         phraseSuggestion.field("nosuchField");
         {
-            SearchRequestBuilder searchBuilder = client().prepareSearch().setSize(0);
-            searchBuilder.suggest(new SuggestBuilder().setText("tetsting sugestion").addSuggestion(phraseSuggestion));
-            assertThrows(searchBuilder, SearchPhaseExecutionException.class);
+            SearchRequestBuilder suggestBuilder = client().prepareSearch().setSize(0);
+            suggestBuilder.setSuggestText("tetsting sugestion");
+            suggestBuilder.addSuggestion(phraseSuggestion);
+            assertThrows(suggestBuilder, SearchPhaseExecutionException.class);
         }
         {
-            SearchRequestBuilder searchBuilder = client().prepareSearch().setSize(0);
-            searchBuilder.suggest(new SuggestBuilder().setText("tetsting sugestion").addSuggestion(phraseSuggestion));
-            assertThrows(searchBuilder, SearchPhaseExecutionException.class);
+            SearchRequestBuilder suggestBuilder = client().prepareSearch().setSize(0);
+            suggestBuilder.setSuggestText("tetsting sugestion");
+            suggestBuilder.addSuggestion(phraseSuggestion);
+            assertThrows(suggestBuilder, SearchPhaseExecutionException.class);
         }
     }
 
@@ -611,7 +599,7 @@ public class SuggestSearchIT extends ESIntegTestCase {
         // Check the name this time because we're repeating it which is funky
         assertThat(searchSuggest.getSuggestion("simple_phrase").getEntries().get(0).getText().string(), equalTo("Xor the Got-Jewel Xor the Got-Jewel Xor the Got-Jewel"));
     }
-    
+
     private List<String> readMarvelHeroNames() throws IOException, URISyntaxException {
         return Files.readAllLines(PathUtils.get(SuggestSearchIT.class.getResource("/config/names.txt").toURI()), StandardCharsets.UTF_8);
     }
@@ -829,16 +817,14 @@ public class SuggestSearchIT extends ESIntegTestCase {
 
         // When searching on a shard with a non existing mapping, we should fail
         SearchRequestBuilder request = client().prepareSearch().setSize(0)
-                .suggest(
-                        new SuggestBuilder().setText("tetsting sugestion").addSuggestion(
-                                phraseSuggestion("did_you_mean").field("fielddoesnotexist").maxErrors(5.0f)));
+            .setSuggestText("tetsting sugestion")
+            .addSuggestion(phraseSuggestion("did_you_mean").field("fielddoesnotexist").maxErrors(5.0f));
         assertThrows(request, SearchPhaseExecutionException.class);
 
         // When searching on a shard which does not hold yet any document of an existing type, we should not fail
         SearchResponse searchResponse = client().prepareSearch().setSize(0)
-                .suggest(
-                        new SuggestBuilder().setText("tetsting sugestion").addSuggestion(
-                                phraseSuggestion("did_you_mean").field("name").maxErrors(5.0f)))
+            .setSuggestText("tetsting sugestion")
+            .addSuggestion(phraseSuggestion("did_you_mean").field("name").maxErrors(5.0f))
             .get();
         ElasticsearchAssertions.assertNoFailures(searchResponse);
         ElasticsearchAssertions.assertSuggestion(searchResponse.getSuggest(), 0, 0, "did_you_mean", "testing suggestions");
@@ -880,9 +866,8 @@ public class SuggestSearchIT extends ESIntegTestCase {
 
         SearchResponse searchResponse = client().prepareSearch()
                 .setSize(0)
-                .suggest(
-                        new SuggestBuilder().setText("tetsting sugestion").addSuggestion(
-                                phraseSuggestion("did_you_mean").field("name").maxErrors(5.0f)))
+                .setSuggestText("tetsting sugestion")
+                .addSuggestion(phraseSuggestion("did_you_mean").field("name").maxErrors(5.0f))
                 .get();
 
         assertNoFailures(searchResponse);
@@ -1273,14 +1258,12 @@ public class SuggestSearchIT extends ESIntegTestCase {
     protected Suggest searchSuggest(String suggestText, int expectShardsFailed, SuggestionBuilder<?>... suggestions) {
         if (randomBoolean()) {
             SearchRequestBuilder builder = client().prepareSearch().setSize(0);
-            SuggestBuilder suggestBuilder = new SuggestBuilder();
             if (suggestText != null) {
-                suggestBuilder.setText(suggestText);
+                builder.setSuggestText(suggestText);
             }
             for (SuggestionBuilder<?> suggestion : suggestions) {
-                suggestBuilder.addSuggestion(suggestion);
+                builder.addSuggestion(suggestion);
             }
-            builder.suggest(suggestBuilder);
             SearchResponse actionGet = builder.execute().actionGet();
             assertThat(Arrays.toString(actionGet.getShardFailures()), actionGet.getFailedShards(), equalTo(expectShardsFailed));
             return actionGet.getSuggest();
diff --git a/core/src/test/java/org/elasticsearch/similarity/SimilarityIT.java b/core/src/test/java/org/elasticsearch/similarity/SimilarityIT.java
index 229f2a8..d486cdb 100644
--- a/core/src/test/java/org/elasticsearch/similarity/SimilarityIT.java
+++ b/core/src/test/java/org/elasticsearch/similarity/SimilarityIT.java
@@ -30,7 +30,6 @@ import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.not;
 
 public class SimilarityIT  extends ESIntegTestCase {
-    
 
     @Test
     public void testCustomBM25Similarity() throws Exception {
diff --git a/core/src/test/java/org/elasticsearch/test/CompositeTestCluster.java b/core/src/test/java/org/elasticsearch/test/CompositeTestCluster.java
index 843fc2e..8b14ef0 100644
--- a/core/src/test/java/org/elasticsearch/test/CompositeTestCluster.java
+++ b/core/src/test/java/org/elasticsearch/test/CompositeTestCluster.java
@@ -19,7 +19,6 @@
 package org.elasticsearch.test;
 
 import com.carrotsearch.randomizedtesting.generators.RandomPicks;
-import com.google.common.collect.Iterators;
 import org.apache.lucene.util.IOUtils;
 import org.elasticsearch.action.admin.cluster.node.stats.NodeStats;
 import org.elasticsearch.action.admin.cluster.node.stats.NodesStatsResponse;
@@ -31,11 +30,7 @@ import org.elasticsearch.common.transport.TransportAddress;
 
 import java.io.IOException;
 import java.net.InetSocketAddress;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.Iterator;
-import java.util.Random;
+import java.util.*;
 import java.util.stream.Collectors;
 
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoTimeout;
@@ -247,7 +242,7 @@ public class CompositeTestCluster extends TestCluster {
 
     @Override
     public synchronized Iterator<Client> iterator() {
-        return Iterators.singletonIterator(client());
+        return Collections.singleton(client()).iterator();
     }
 
     /**
diff --git a/core/src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java b/core/src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
index 9c24a11..5772543 100644
--- a/core/src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
+++ b/core/src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
@@ -53,8 +53,6 @@ import org.elasticsearch.cluster.metadata.IndexTemplateMetaData;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.NamedWriteableAwareStreamInput;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
@@ -84,7 +82,13 @@ import java.util.function.Function;
 import java.util.function.Predicate;
 import java.util.stream.Collectors;
 
-import static org.apache.lucene.util.LuceneTestCase.random;
+import static org.elasticsearch.test.ESTestCase.assertArrayEquals;
+import static org.elasticsearch.test.ESTestCase.assertEquals;
+import static org.elasticsearch.test.ESTestCase.assertFalse;
+import static org.elasticsearch.test.ESTestCase.assertNotNull;
+import static org.elasticsearch.test.ESTestCase.assertTrue;
+import static org.elasticsearch.test.ESTestCase.fail;
+import static org.elasticsearch.test.ESTestCase.random;
 import static org.elasticsearch.test.VersionUtils.randomVersion;
 import static org.hamcrest.CoreMatchers.equalTo;
 import static org.hamcrest.CoreMatchers.is;
@@ -98,12 +102,6 @@ import static org.hamcrest.Matchers.instanceOf;
 import static org.hamcrest.Matchers.not;
 import static org.hamcrest.Matchers.notNullValue;
 import static org.hamcrest.Matchers.nullValue;
-import static org.junit.Assert.assertArrayEquals;
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertNotNull;
-import static org.junit.Assert.assertTrue;
-import static org.junit.Assert.fail;
 
 /**
  *
@@ -665,10 +663,6 @@ public class ElasticsearchAssertions {
     }
 
     public static void assertVersionSerializable(Version version, Streamable streamable) {
-        assertVersionSerializable(version, streamable, null);
-    }
-
-    public static void assertVersionSerializable(Version version, Streamable streamable, NamedWriteableRegistry namedWriteableRegistry) {
         try {
             Streamable newInstance = tryCreateNewInstance(streamable);
             if (newInstance == null) {
@@ -680,15 +674,10 @@ public class ElasticsearchAssertions {
             }
             BytesReference orig = serialize(version, streamable);
             StreamInput input = StreamInput.wrap(orig);
-            if (namedWriteableRegistry != null) {
-                input = new NamedWriteableAwareStreamInput(input, namedWriteableRegistry);
-            }
             input.setVersion(version);
             newInstance.readFrom(input);
-            assertThat("Stream should be fully read with version [" + version + "] for streamable [" + streamable + "]", input.available(),
-                    equalTo(0));
-            assertThat("Serialization failed with version [" + version + "] bytes should be equal for streamable [" + streamable + "]",
-                    serialize(version, streamable), equalTo(orig));
+            assertThat("Stream should be fully read with version [" + version + "] for streamable [" + streamable + "]", input.available(), equalTo(0));
+            assertThat("Serialization failed with version [" + version + "] bytes should be equal for streamable [" + streamable + "]", serialize(version, streamable), equalTo(orig));
         } catch (Throwable ex) {
             throw new RuntimeException("failed to check serialization - version [" + version + "] for streamable [" + streamable + "]", ex);
         }
diff --git a/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java b/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
index 64cc401..c253a75 100644
--- a/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
+++ b/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
@@ -77,15 +77,13 @@ public class AssertingLocalTransport extends LocalTransport {
 
     @Override
     protected void handleParsedResponse(final TransportResponse response, final TransportResponseHandler handler) {
-        ElasticsearchAssertions.assertVersionSerializable(VersionUtils.randomVersionBetween(random, minVersion, maxVersion), response,
-                namedWriteableRegistry);
+        ElasticsearchAssertions.assertVersionSerializable(VersionUtils.randomVersionBetween(random, minVersion, maxVersion), response);
         super.handleParsedResponse(response, handler);
     }
 
     @Override
     public void sendRequest(final DiscoveryNode node, final long requestId, final String action, final TransportRequest request, TransportRequestOptions options) throws IOException, TransportException {
-        ElasticsearchAssertions.assertVersionSerializable(VersionUtils.randomVersionBetween(random, minVersion, maxVersion), request,
-                namedWriteableRegistry);
+        ElasticsearchAssertions.assertVersionSerializable(VersionUtils.randomVersionBetween(random, minVersion, maxVersion), request);
         super.sendRequest(node, requestId, action, request, options);
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/versioning/SimpleVersioningIT.java b/core/src/test/java/org/elasticsearch/versioning/SimpleVersioningIT.java
index 5296e76..93c29e0 100644
--- a/core/src/test/java/org/elasticsearch/versioning/SimpleVersioningIT.java
+++ b/core/src/test/java/org/elasticsearch/versioning/SimpleVersioningIT.java
@@ -18,15 +18,6 @@
  */
 package org.elasticsearch.versioning;
 
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Locale;
-import java.util.Map;
-import java.util.Random;
-import java.util.Set;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.atomic.AtomicInteger;
-
 import org.apache.lucene.util.TestUtil;
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.action.bulk.BulkResponse;
@@ -37,12 +28,15 @@ import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.lucene.uid.Versions;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.VersionType;
-import org.elasticsearch.index.engine.DocumentAlreadyExistsException;
 import org.elasticsearch.index.engine.FlushNotAllowedEngineException;
 import org.elasticsearch.index.engine.VersionConflictEngineException;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
+import java.util.*;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.atomic.AtomicInteger;
+
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThrows;
@@ -100,7 +94,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         }
 
         // deleting with a lower version works.
-        long v= randomIntBetween(12,14);
+        long v = randomIntBetween(12, 14);
         DeleteResponse deleteResponse = client().prepareDelete("test", "type", "1").setVersion(v).setVersionType(VersionType.FORCE).get();
         assertThat(deleteResponse.isFound(), equalTo(true));
         assertThat(deleteResponse.getVersion(), equalTo(v));
@@ -136,7 +130,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
                 VersionConflictEngineException.class);
 
         // Delete with a higher or equal version deletes all versions up to the given one.
-        long v= randomIntBetween(14,17);
+        long v = randomIntBetween(14, 17);
         DeleteResponse deleteResponse = client().prepareDelete("test", "type", "1").setVersion(v).setVersionType(VersionType.EXTERNAL_GTE).execute().actionGet();
         assertThat(deleteResponse.isFound(), equalTo(true));
         assertThat(deleteResponse.getVersion(), equalTo(v));
@@ -165,7 +159,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         assertThat(indexResponse.getVersion(), equalTo(14l));
 
         assertThrows(client().prepareIndex("test", "type", "1").setSource("field1", "value1_1").setVersion(13).setVersionType(VersionType.EXTERNAL).execute(),
-                     VersionConflictEngineException.class);
+                VersionConflictEngineException.class);
 
         if (randomBoolean()) {
             refresh();
@@ -176,8 +170,8 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         // deleting with a lower version fails.
         assertThrows(
-            client().prepareDelete("test", "type", "1").setVersion(2).setVersionType(VersionType.EXTERNAL).execute(),
-            VersionConflictEngineException.class);
+                client().prepareDelete("test", "type", "1").setVersion(2).setVersionType(VersionType.EXTERNAL).execute(),
+                VersionConflictEngineException.class);
 
         // Delete with a higher version deletes all versions up to the given one.
         DeleteResponse deleteResponse = client().prepareDelete("test", "type", "1").setVersion(17).setVersionType(VersionType.EXTERNAL).execute().actionGet();
@@ -186,8 +180,8 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         // Deleting with a lower version keeps on failing after a delete.
         assertThrows(
-            client().prepareDelete("test", "type", "1").setVersion(2).setVersionType(VersionType.EXTERNAL).execute(),
-            VersionConflictEngineException.class);
+                client().prepareDelete("test", "type", "1").setVersion(2).setVersionType(VersionType.EXTERNAL).execute(),
+                VersionConflictEngineException.class);
 
 
         // But delete with a higher version is OK.
@@ -206,8 +200,8 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         assertThat(deleteResponse.getVersion(), equalTo(20l));
 
         // Make sure that the next delete will be GC. Note we do it on the index settings so it will be cleaned up
-        HashMap<String,Object> newSettings = new HashMap<>();
-        newSettings.put("index.gc_deletes",-1);
+        HashMap<String, Object> newSettings = new HashMap<>();
+        newSettings.put("index.gc_deletes", -1);
         client().admin().indices().prepareUpdateSettings("test").setSettings(newSettings).execute().actionGet();
 
         Thread.sleep(300); // gc works based on estimated sampled time. Give it a chance...
@@ -221,7 +215,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
     public void testRequireUnitsOnUpdateSettings() throws Exception {
         createIndex("test");
         ensureGreen();
-        HashMap<String,Object> newSettings = new HashMap<>();
+        HashMap<String, Object> newSettings = new HashMap<>();
         newSettings.put("index.gc_deletes", "42");
         try {
             client().admin().indices().prepareUpdateSettings("test").setSettings(newSettings).execute().actionGet();
@@ -262,22 +256,12 @@ public class SimpleVersioningIT extends ESIntegTestCase {
                 VersionConflictEngineException.class);
 
         assertThrows(
-            client().prepareIndex("test", "type", "1").setSource("field1", "value1_1").setVersion(1).execute(),
-            VersionConflictEngineException.class);
-
-        assertThrows(
-            client().prepareIndex("test", "type", "1").setCreate(true).setSource("field1", "value1_1").setVersion(1).execute(),
-                VersionConflictEngineException.class);
-        assertThrows(
-            client().prepareIndex("test", "type", "1").setCreate(true).setSource("field1", "value1_1").setVersion(1).execute(),
+                client().prepareIndex("test", "type", "1").setSource("field1", "value1_1").setVersion(1).execute(),
                 VersionConflictEngineException.class);
 
         assertThrows(
-                client().prepareIndex("test", "type", "1").setCreate(true).setSource("field1", "value1_1").setVersion(2).execute(),
-                DocumentAlreadyExistsException.class);
-        assertThrows(
-                client().prepareIndex("test", "type", "1").setCreate(true).setSource("field1", "value1_1").setVersion(2).execute(),
-                DocumentAlreadyExistsException.class);
+                client().prepareIndex("test", "type", "1").setCreate(true).setSource("field1", "value1_1").execute(),
+                VersionConflictEngineException.class);
 
 
         assertThrows(client().prepareDelete("test", "type", "1").setVersion(1).execute(), VersionConflictEngineException.class);
@@ -334,10 +318,8 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         assertThrows(client().prepareIndex("test", "type", "1").setSource("field1", "value1_1").setVersion(1).execute(),
                 VersionConflictEngineException.class);
-        assertThrows(client().prepareIndex("test", "type", "1").setCreate(true).setSource("field1", "value1_1").setVersion(1).execute(),
-                VersionConflictEngineException.class);
 
-        assertThrows(client().prepareIndex("test", "type", "1").setCreate(true).setSource("field1", "value1_1").setVersion(1).execute(),
+        assertThrows(client().prepareIndex("test", "type", "1").setCreate(true).setSource("field1", "value1_1").execute(),
                 VersionConflictEngineException.class);
 
         assertThrows(client().prepareDelete("test", "type", "1").setVersion(1).execute(), VersionConflictEngineException.class);
@@ -377,90 +359,94 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         IDSource ids;
         final Random random = getRandom();
         switch (random.nextInt(6)) {
-        case 0:
-            // random simple
-            if (VERBOSE) {
-                System.out.println("TEST: use random simple ids");
-            }
-            ids = new IDSource() {
+            case 0:
+                // random simple
+                if (VERBOSE) {
+                    System.out.println("TEST: use random simple ids");
+                }
+                ids = new IDSource() {
                     @Override
                     public String next() {
                         return TestUtil.randomSimpleString(random);
                     }
                 };
-            break;
-        case 1:
-            // random realistic unicode
-            if (VERBOSE) {
-                System.out.println("TEST: use random realistic unicode ids");
-            }
-            ids = new IDSource() {
+                break;
+            case 1:
+                // random realistic unicode
+                if (VERBOSE) {
+                    System.out.println("TEST: use random realistic unicode ids");
+                }
+                ids = new IDSource() {
                     @Override
                     public String next() {
                         return TestUtil.randomRealisticUnicodeString(random);
                     }
                 };
-            break;
-        case 2:
-            // sequential
-            if (VERBOSE) {
-                System.out.println("TEST: use seuquential ids");
-            }
-            ids = new IDSource() {
+                break;
+            case 2:
+                // sequential
+                if (VERBOSE) {
+                    System.out.println("TEST: use seuquential ids");
+                }
+                ids = new IDSource() {
                     int upto;
+
                     @Override
                     public String next() {
                         return Integer.toString(upto++);
                     }
                 };
-            break;
-        case 3:
-            // zero-pad sequential
-            if (VERBOSE) {
-                System.out.println("TEST: use zero-pad seuquential ids");
-            }
-            ids = new IDSource() {
+                break;
+            case 3:
+                // zero-pad sequential
+                if (VERBOSE) {
+                    System.out.println("TEST: use zero-pad seuquential ids");
+                }
+                ids = new IDSource() {
                     final int radix = TestUtil.nextInt(random, Character.MIN_RADIX, Character.MAX_RADIX);
                     final String zeroPad = String.format(Locale.ROOT, "%0" + TestUtil.nextInt(random, 4, 20) + "d", 0);
                     int upto;
+
                     @Override
                     public String next() {
                         String s = Integer.toString(upto++);
                         return zeroPad.substring(zeroPad.length() - s.length()) + s;
                     }
                 };
-            break;
-        case 4:
-            // random long
-            if (VERBOSE) {
-                System.out.println("TEST: use random long ids");
-            }
-            ids = new IDSource() {
+                break;
+            case 4:
+                // random long
+                if (VERBOSE) {
+                    System.out.println("TEST: use random long ids");
+                }
+                ids = new IDSource() {
                     final int radix = TestUtil.nextInt(random, Character.MIN_RADIX, Character.MAX_RADIX);
                     int upto;
+
                     @Override
                     public String next() {
                         return Long.toString(random.nextLong() & 0x3ffffffffffffffL, radix);
                     }
                 };
-            break;
-        case 5:
-            // zero-pad random long
-            if (VERBOSE) {
-                System.out.println("TEST: use zero-pad random long ids");
-            }
-            ids = new IDSource() {
+                break;
+            case 5:
+                // zero-pad random long
+                if (VERBOSE) {
+                    System.out.println("TEST: use zero-pad random long ids");
+                }
+                ids = new IDSource() {
                     final int radix = TestUtil.nextInt(random, Character.MIN_RADIX, Character.MAX_RADIX);
                     final String zeroPad = String.format(Locale.ROOT, "%015d", 0);
                     int upto;
+
                     @Override
                     public String next() {
                         return Long.toString(random.nextLong() & 0x3ffffffffffffffL, radix);
                     }
                 };
-            break;
-        default:
-            throw new AssertionError();
+                break;
+            default:
+                throw new AssertionError();
         }
 
         return ids;
@@ -530,7 +516,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
             } else {
                 sb.append("  response: null");
             }
-            
+
             return sb.toString();
         }
     }
@@ -547,7 +533,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         // TODO: not great we don't test deletes GC here:
 
         // We test deletes, but can't rely on wall-clock delete GC:
-        HashMap<String,Object> newSettings = new HashMap<>();
+        HashMap<String, Object> newSettings = new HashMap<>();
         newSettings.put("index.gc_deletes", "1000000h");
         assertAcked(client().admin().indices().prepareUpdateSettings("test").setSettings(newSettings).execute().actionGet());
 
@@ -584,14 +570,14 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         // Attach random versions to them:
         long version = 0;
-        final IDAndVersion[] idVersions = new IDAndVersion[TestUtil.nextInt(random, numIDs/2, numIDs*(TEST_NIGHTLY ? 8 : 2))];
-        final Map<String,IDAndVersion> truth = new HashMap<>();
+        final IDAndVersion[] idVersions = new IDAndVersion[TestUtil.nextInt(random, numIDs / 2, numIDs * (TEST_NIGHTLY ? 8 : 2))];
+        final Map<String, IDAndVersion> truth = new HashMap<>();
 
         if (VERBOSE) {
             System.out.println("TEST: use " + numIDs + " ids; " + idVersions.length + " operations");
         }
 
-        for(int i=0;i<idVersions.length;i++) {
+        for (int i = 0; i < idVersions.length; i++) {
 
             if (useMonotonicVersion) {
                 version += TestUtil.nextInt(random, 1, 10);
@@ -612,7 +598,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         }
 
         // Shuffle
-        for(int i = idVersions.length - 1; i > 0; i--) {
+        for (int i = idVersions.length - 1; i > 0; i--) {
             int index = random.nextInt(i + 1);
             IDAndVersion x = idVersions[index];
             idVersions[index] = idVersions[i];
@@ -620,7 +606,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         }
 
         if (VERBOSE) {
-            for(IDAndVersion idVersion : idVersions) {
+            for (IDAndVersion idVersion : idVersions) {
                 System.out.println("id=" + idVersion.id + " version=" + idVersion.version + " delete?=" + idVersion.delete + " truth?=" + (truth.get(idVersion.id) == idVersion));
             }
         }
@@ -629,109 +615,87 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         final CountDownLatch startingGun = new CountDownLatch(1);
         Thread[] threads = new Thread[TestUtil.nextInt(random, 1, TEST_NIGHTLY ? 20 : 5)];
         final long startTime = System.nanoTime();
-        for(int i=0;i<threads.length;i++) {
+        for (int i = 0; i < threads.length; i++) {
             final int threadID = i;
             threads[i] = new Thread() {
-                    @Override
-                    public void run() {
-                        try {
-                            //final Random threadRandom = RandomizedContext.current().getRandom();
-                            final Random threadRandom = getRandom();
-                            startingGun.await();
-                            while (true) {
-
-                                // TODO: sometimes use bulk:
-
-                                int index = upto.getAndIncrement();
-                                if (index >= idVersions.length) {
-                                    break;
-                                }
-                                if (VERBOSE && index % 100 == 0) {
-                                    System.out.println(Thread.currentThread().getName() + ": index=" + index);
-                                }
-                                IDAndVersion idVersion = idVersions[index];
-
-                                String id = idVersion.id;
-                                idVersion.threadID = threadID;
-                                idVersion.indexStartTime = System.nanoTime()-startTime;
-                                long version = idVersion.version;
-                                if (idVersion.delete) {
-                                    try {
-                                        idVersion.response = client().prepareDelete("test", "type", id)
+                @Override
+                public void run() {
+                    try {
+                        //final Random threadRandom = RandomizedContext.current().getRandom();
+                        final Random threadRandom = getRandom();
+                        startingGun.await();
+                        while (true) {
+
+                            // TODO: sometimes use bulk:
+
+                            int index = upto.getAndIncrement();
+                            if (index >= idVersions.length) {
+                                break;
+                            }
+                            if (VERBOSE && index % 100 == 0) {
+                                System.out.println(Thread.currentThread().getName() + ": index=" + index);
+                            }
+                            IDAndVersion idVersion = idVersions[index];
+
+                            String id = idVersion.id;
+                            idVersion.threadID = threadID;
+                            idVersion.indexStartTime = System.nanoTime() - startTime;
+                            long version = idVersion.version;
+                            if (idVersion.delete) {
+                                try {
+                                    idVersion.response = client().prepareDelete("test", "type", id)
                                             .setVersion(version)
                                             .setVersionType(VersionType.EXTERNAL).execute().actionGet();
-                                    } catch (VersionConflictEngineException vcee) {
-                                        // OK: our version is too old
-                                        assertThat(version, lessThanOrEqualTo(truth.get(id).version));
-                                        idVersion.versionConflict = true;
-                                    }
-                                } else {
-                                    for (int x=0;x<2;x++) {
-                                        // Try create first:
-                                    
-                                        IndexRequest.OpType op;
-                                        if (x == 0) {
-                                            op = IndexRequest.OpType.CREATE;
-                                        } else {
-                                            op = IndexRequest.OpType.INDEX;
-                                        }
-                                    
-                                        // index document
-                                        try {
-                                            idVersion.response = client().prepareIndex("test", "type", id)
-                                                .setSource("foo", "bar")
-                                                .setOpType(op)
-                                                .setVersion(version)
-                                                .setVersionType(VersionType.EXTERNAL).execute().actionGet();
-                                            break;
-                                        } catch (DocumentAlreadyExistsException daee) {
-                                            if (x == 0) {
-                                                // OK: id was already indexed by another thread, now use index:
-                                                idVersion.alreadyExists = true;
-                                            } else {
-                                                // Should not happen with op=INDEX:
-                                                throw daee;
-                                            }
-                                        } catch (VersionConflictEngineException vcee) {
-                                            // OK: our version is too old
-                                            assertThat(version, lessThanOrEqualTo(truth.get(id).version));
-                                            idVersion.versionConflict = true;
-                                        }
-                                    }
+                                } catch (VersionConflictEngineException vcee) {
+                                    // OK: our version is too old
+                                    assertThat(version, lessThanOrEqualTo(truth.get(id).version));
+                                    idVersion.versionConflict = true;
                                 }
-                                idVersion.indexFinishTime = System.nanoTime()-startTime;
-
-                                if (threadRandom.nextInt(100) == 7) {
-                                    System.out.println(threadID + ": TEST: now refresh at " + (System.nanoTime()-startTime));
-                                    refresh();
-                                    System.out.println(threadID + ": TEST: refresh done at " + (System.nanoTime()-startTime));
+                            } else {
+                                try {
+                                    idVersion.response = client().prepareIndex("test", "type", id)
+                                            .setSource("foo", "bar")
+                                            .setVersion(version).setVersionType(VersionType.EXTERNAL).get();
+
+                                } catch (VersionConflictEngineException vcee) {
+                                    // OK: our version is too old
+                                    assertThat(version, lessThanOrEqualTo(truth.get(id).version));
+                                    idVersion.versionConflict = true;
                                 }
-                                if (threadRandom.nextInt(100) == 7) {
-                                    System.out.println(threadID + ": TEST: now flush at " + (System.nanoTime()-startTime));
-                                    try {
-                                        flush();
-                                    } catch (FlushNotAllowedEngineException fnaee) {
-                                        // OK
-                                    }
-                                    System.out.println(threadID + ": TEST: flush done at " + (System.nanoTime()-startTime));
+                            }
+                            idVersion.indexFinishTime = System.nanoTime() - startTime;
+
+                            if (threadRandom.nextInt(100) == 7) {
+                                System.out.println(threadID + ": TEST: now refresh at " + (System.nanoTime() - startTime));
+                                refresh();
+                                System.out.println(threadID + ": TEST: refresh done at " + (System.nanoTime() - startTime));
+                            }
+                            if (threadRandom.nextInt(100) == 7) {
+                                System.out.println(threadID + ": TEST: now flush at " + (System.nanoTime() - startTime));
+                                try {
+                                    flush();
+                                } catch (FlushNotAllowedEngineException fnaee) {
+                                    // OK
                                 }
+                                System.out.println(threadID + ": TEST: flush done at " + (System.nanoTime() - startTime));
                             }
-                        } catch (Exception e) {
-                            throw new RuntimeException(e);
                         }
+                    } catch (Exception e) {
+                        throw new RuntimeException(e);
                     }
-                };
+                }
+            };
             threads[i].start();
         }
 
         startingGun.countDown();
-        for(Thread thread : threads) {
+        for (Thread thread : threads) {
             thread.join();
         }
 
         // Verify against truth:
         boolean failed = false;
-        for(String id : ids) {
+        for (String id : ids) {
             long expected;
             IDAndVersion idVersion = truth.get(id);
             if (idVersion != null && idVersion.delete == false) {
@@ -748,7 +712,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         if (failed) {
             System.out.println("All versions:");
-            for(int i=0;i<idVersions.length;i++) {
+            for (int i = 0; i < idVersions.length; i++) {
                 System.out.println("i=" + i + " " + idVersions[i]);
             }
             fail("wrong versions for some IDs");
@@ -760,36 +724,36 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         // We require only one shard for this test, so that the 2nd delete provokes pruning the deletes map:
         client()
-            .admin()
-            .indices()
-            .prepareCreate("test")
-            .setSettings(Settings.settingsBuilder()
-                         .put("index.number_of_shards", 1))
-            .execute().
-            actionGet();
+                .admin()
+                .indices()
+                .prepareCreate("test")
+                .setSettings(Settings.settingsBuilder()
+                        .put("index.number_of_shards", 1))
+                .execute().
+                actionGet();
 
         ensureGreen();
 
-        HashMap<String,Object> newSettings = new HashMap<>();
+        HashMap<String, Object> newSettings = new HashMap<>();
         newSettings.put("index.gc_deletes", "10ms");
         newSettings.put("index.refresh_interval", "-1");
         client()
-            .admin()
-            .indices()
-            .prepareUpdateSettings("test")
-            .setSettings(newSettings)
-            .execute()
-            .actionGet();
+                .admin()
+                .indices()
+                .prepareUpdateSettings("test")
+                .setSettings(newSettings)
+                .execute()
+                .actionGet();
 
         // Index a doc:
         client()
-            .prepareIndex("test", "type", "id")
-            .setSource("foo", "bar")
-            .setOpType(IndexRequest.OpType.INDEX)
-            .setVersion(10)
-            .setVersionType(VersionType.EXTERNAL)
-            .execute()
-            .actionGet();
+                .prepareIndex("test", "type", "id")
+                .setSource("foo", "bar")
+                .setOpType(IndexRequest.OpType.INDEX)
+                .setVersion(10)
+                .setVersionType(VersionType.EXTERNAL)
+                .execute()
+                .actionGet();
 
         if (randomBoolean()) {
             // Force refresh so the add is sometimes visible in the searcher:
@@ -798,20 +762,20 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         // Delete it
         client()
-            .prepareDelete("test", "type", "id")
-            .setVersion(11)
-            .setVersionType(VersionType.EXTERNAL)
-            .execute()
-            .actionGet();
+                .prepareDelete("test", "type", "id")
+                .setVersion(11)
+                .setVersionType(VersionType.EXTERNAL)
+                .execute()
+                .actionGet();
 
         // Real-time get should reflect delete:
         assertThat("doc should have been deleted",
-                   client()
-                   .prepareGet("test", "type", "id")
-                   .execute()
-                   .actionGet()
-                   .getVersion(),
-                   equalTo(-1L));
+                client()
+                        .prepareGet("test", "type", "id")
+                        .execute()
+                        .actionGet()
+                        .getVersion(),
+                equalTo(-1L));
 
         // ThreadPool.estimatedTimeInMillis has default granularity of 200 msec, so we must sleep at least that long; sleep much longer in
         // case system is busy:
@@ -819,20 +783,20 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         // Delete an unrelated doc (provokes pruning deletes from versionMap)
         client()
-            .prepareDelete("test", "type", "id2")
-            .setVersion(11)
-            .setVersionType(VersionType.EXTERNAL)
-            .execute()
-            .actionGet();
+                .prepareDelete("test", "type", "id2")
+                .setVersion(11)
+                .setVersionType(VersionType.EXTERNAL)
+                .execute()
+                .actionGet();
 
         // Real-time get should still reflect delete:
         assertThat("doc should have been deleted",
-                   client()
-                   .prepareGet("test", "type", "id")
-                   .execute()
-                   .actionGet()
-                   .getVersion(),
-                   equalTo(-1L));
+                client()
+                        .prepareGet("test", "type", "id")
+                        .execute()
+                        .actionGet()
+                        .getVersion(),
+                equalTo(-1L));
     }
 
     @Test
@@ -842,25 +806,25 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         ensureGreen();
 
         // We test deletes, but can't rely on wall-clock delete GC:
-        HashMap<String,Object> newSettings = new HashMap<>();
+        HashMap<String, Object> newSettings = new HashMap<>();
         newSettings.put("index.gc_deletes", "0ms");
         client()
-            .admin()
-            .indices()
-            .prepareUpdateSettings("test")
-            .setSettings(newSettings)
-            .execute()
-            .actionGet();
+                .admin()
+                .indices()
+                .prepareUpdateSettings("test")
+                .setSettings(newSettings)
+                .execute()
+                .actionGet();
 
         // Index a doc:
         client()
-            .prepareIndex("test", "type", "id")
-            .setSource("foo", "bar")
-            .setOpType(IndexRequest.OpType.INDEX)
-            .setVersion(10)
-            .setVersionType(VersionType.EXTERNAL)
-            .execute()
-            .actionGet();
+                .prepareIndex("test", "type", "id")
+                .setSource("foo", "bar")
+                .setOpType(IndexRequest.OpType.INDEX)
+                .setVersion(10)
+                .setVersionType(VersionType.EXTERNAL)
+                .execute()
+                .actionGet();
 
         if (randomBoolean()) {
             // Force refresh so the add is sometimes visible in the searcher:
@@ -869,19 +833,19 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         // Delete it
         client()
-            .prepareDelete("test", "type", "id")
-            .setVersion(11)
-            .setVersionType(VersionType.EXTERNAL)
-            .execute()
-            .actionGet();
+                .prepareDelete("test", "type", "id")
+                .setVersion(11)
+                .setVersionType(VersionType.EXTERNAL)
+                .execute()
+                .actionGet();
 
         // Real-time get should reflect delete even though index.gc_deletes is 0:
         assertThat("doc should have been deleted",
-                   client()
-                   .prepareGet("test", "type", "id")
-                   .execute()
-                   .actionGet()
-                   .getVersion(),
-                   equalTo(-1L));
+                client()
+                        .prepareGet("test", "type", "id")
+                        .execute()
+                        .actionGet()
+                        .getVersion(),
+                equalTo(-1L));
     }
 }
diff --git a/core/src/test/resources/indices/bwc/index-2.0.0-rc1.zip b/core/src/test/resources/indices/bwc/index-2.0.0-rc1.zip
new file mode 100644
index 0000000..44e78b4
Binary files /dev/null and b/core/src/test/resources/indices/bwc/index-2.0.0-rc1.zip differ
diff --git a/core/src/test/resources/indices/bwc/repo-2.0.0-rc1.zip b/core/src/test/resources/indices/bwc/repo-2.0.0-rc1.zip
new file mode 100644
index 0000000..b15cedf
Binary files /dev/null and b/core/src/test/resources/indices/bwc/repo-2.0.0-rc1.zip differ
diff --git a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch1.json b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch1.json
index eefec53..3d98f37 100644
--- a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch1.json
+++ b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch1.json
@@ -1,16 +1,16 @@
 {"index":"test", "ignore_unavailable" : true, "expand_wildcards" : "open,closed"}}
-{"query" : {"match_all" :{}}}
+{"query" : {"match_all" {}}}
 {"index" : "test", "type" : "type1", "expand_wildcards" : ["open", "closed"]}
-{"query" : {"match_all" :{}}}
+{"query" : {"match_all" {}}}
 {"index":"test", "ignore_unavailable" : false, "expand_wildcards" : ["open"]}}
-{"query" : {"match_all" :{}}}
+{"query" : {"match_all" {}}}
 {"index":"test", "ignore_unavailable" : true, "allow_no_indices": true, "expand_wildcards" : ["open", "closed"]}}
-{"query" : {"match_all" :{}}}
+{"query" : {"match_all" {}}}
 {"index":"test", "ignore_unavailable" : true, "allow_no_indices": false, "expand_wildcards" : ["closed"]}}
-{"query" : {"match_all" :{}}}
+{"query" : {"match_all" {}}}
 {}
-{"query" : {"match_all" :{}}}
+{"query" : {"match_all" {}}}
 {"search_type" : "dfs_query_then_fetch"}
-{"query" : {"match_all" :{}}}
+{"query" : {"match_all" {}}}
 
-{"query" : {"match_all" :{}}}
+{"query" : {"match_all" {}}}
diff --git a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch2.json b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch2.json
index 79330d8..e2e06d9 100644
--- a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch2.json
+++ b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch2.json
@@ -1,10 +1,10 @@
 {"index":"test"}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 {"index" : "test", "type" : "type1"}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 {}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 {"search_type" : "dfs_query_then_fetch"}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
diff --git a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch3.json b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch3.json
index a6b52fd..6416720 100644
--- a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch3.json
+++ b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch3.json
@@ -1,8 +1,8 @@
 {"index":["test0", "test1"]}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 {"index" : "test2,test3", "type" : "type1"}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 {"index" : ["test4", "test1"], "type" :  [ "type2", "type1" ]}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 {"search_type" : "dfs_query_then_fetch"}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
diff --git a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch4.json b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch4.json
index 844d8be..b98e24b 100644
--- a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch4.json
+++ b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch4.json
@@ -1,6 +1,6 @@
 {"index":["test0", "test1"], "request_cache": true}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 {"index" : "test2,test3", "type" : "type1", "preference": "_local"}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 {"index" : ["test4", "test1"], "type" :  [ "type2", "type1" ], "routing": "123"}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
diff --git a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch5.json b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch5.json
index b337eae..5f08919 100644
--- a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch5.json
+++ b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch5.json
@@ -1,6 +1,6 @@
 {"index":["test0", "test1"], "request_cache": true}
-{"template": {"query" : {"match_{{template}}" :{}}}, "params": {"template": "all" } } }
+{"template": {"query" : {"match_{{template}}" {}}}, "params": {"template": "all" } } }
 {"index" : "test2,test3", "type" : "type1", "preference": "_local"}
-{"template": {"query" : {"match_{{template}}" :{}}}, "params": {"template": "all" } } }
+{"template": {"query" : {"match_{{template}}" {}}}, "params": {"template": "all" } } }
 {"index" : ["test4", "test1"], "type" :  [ "type2", "type1" ], "routing": "123"}
-{"template": {"query" : {"match_{{template}}" :{}}}, "params": {"template": "all" } } }
+{"template": {"query" : {"match_{{template}}" {}}}, "params": {"template": "all" } } }
diff --git a/dev-tools/prepare_release_candidate.py b/dev-tools/prepare_release_candidate.py
index 5f83893..24450a6 100644
--- a/dev-tools/prepare_release_candidate.py
+++ b/dev-tools/prepare_release_candidate.py
@@ -63,7 +63,7 @@ To install the deb from an APT repo:
 
 APT line sources.list line:
 
-deb http://%(bucket)s/elasticsearch/staging/%(version)s-%(hash)s/repos/%(major_minor_version)s/debian/ stable main
+deb http://%(bucket)s/elasticsearch/staging/%(version)s-%(hash)s/repos/%(package_repo_version)s/debian/ stable main
 
 To install the RPM, create a YUM file like:
 
@@ -73,7 +73,7 @@ containing:
 
 [elasticsearch-2.0]
 name=Elasticsearch repository for packages
-baseurl=http://%(bucket)s/elasticsearch/staging/%(version)s-%(hash)s/repos/%(major_minor_version)s/centos
+baseurl=http://%(bucket)s/elasticsearch/staging/%(version)s-%(hash)s/repos/%(package_repo_version)s/centos
 gpgcheck=1
 gpgkey=http://packages.elastic.co/GPG-KEY-elasticsearch
 enabled=1
@@ -300,7 +300,7 @@ if __name__ == "__main__":
   ensure_checkout_is_clean()
   if not re.match('(\d+\.\d+)\.*',release_version):
     raise RuntimeError('illegal release version format: %s' % (release_version))
-  major_minor_version = re.match('(\d+\.\d+)\.*',release_version).group(1)
+  package_repo_version = '%s.x' % re.match('(\d+)\.*', release_version).group(1)
 
   print('*** Preparing release version: [%s]' % release_version)
 
@@ -348,13 +348,13 @@ if __name__ == "__main__":
   # repository push commands
   s3cmd_sync_to_staging_bucket_cmd = 's3cmd sync -P %s s3://%s/elasticsearch/staging/%s-%s/org/' % (localRepoElasticsearch, bucket, release_version, shortHash)
   s3_bucket_sync_to = '%s/elasticsearch/staging/%s-%s/repos/' % (bucket, release_version, shortHash)
-  s3cmd_sync_official_repo_cmd = 's3cmd sync s3://packages.elasticsearch.org/elasticsearch/%s s3://%s' % (major_minor_version, s3_bucket_sync_to)
+  s3cmd_sync_official_repo_cmd = 's3cmd sync s3://packages.elasticsearch.org/elasticsearch/%s s3://%s' % (package_repo_version, s3_bucket_sync_to)
 
-  debs3_prefix = 'elasticsearch/staging/%s-%s/repos/%s/debian' % (release_version, shortHash, major_minor_version)
+  debs3_prefix = 'elasticsearch/staging/%s-%s/repos/%s/debian' % (release_version, shortHash, package_repo_version)
   debs3_upload_cmd = 'deb-s3 upload --preserve-versions %s/distribution/deb/elasticsearch/%s/elasticsearch-%s.deb -b %s --prefix %s --sign %s --arch amd64' % (localRepoElasticsearch, release_version, release_version, bucket, debs3_prefix, gpg_key)
   debs3_list_cmd = 'deb-s3 list -b %s --prefix %s' % (bucket, debs3_prefix)
   debs3_verify_cmd = 'deb-s3 verify -b %s --prefix %s' % (bucket, debs3_prefix)
-  rpms3_prefix = 'elasticsearch/staging/%s-%s/repos/%s/centos' % (release_version, shortHash, major_minor_version)
+  rpms3_prefix = 'elasticsearch/staging/%s-%s/repos/%s/centos' % (release_version, shortHash, package_repo_version)
   rpms3_upload_cmd = 'rpm-s3 -v -b %s -p %s --sign --visibility public-read -k 0 %s' % (bucket, rpms3_prefix, rpm)
 
   if deploy_s3:
@@ -397,7 +397,7 @@ if __name__ == "__main__":
     print('NOTE: Running s3cmd might require you to create a config file with your credentials, if the s3cmd does not support suppliying them via the command line!')
 
   print('*** Once the release is deployed and published send out the following mail to dev@elastic.co:')
-  string_format_dict = {'version' : release_version, 'hash': shortHash, 'major_minor_version' : major_minor_version, 'bucket': bucket}
+  string_format_dict = {'version' : release_version, 'hash': shortHash, 'package_repo_version' : package_repo_version, 'bucket': bucket}
   print(MAIL_TEMPLATE % string_format_dict)
 
   print('')
@@ -406,7 +406,7 @@ if __name__ == "__main__":
 
   print('')
   print('To publish the release and the repo on S3 execute the following commands:')
-  print('   s3cmd cp --recursive s3://%(bucket)s/elasticsearch/staging/%(version)s-%(hash)s/repos/%(major_minor_version)s/ s3://packages.elasticsearch.org/elasticsearch/%(major_minor_version)s'  % string_format_dict)
+  print('   s3cmd cp --recursive s3://%(bucket)s/elasticsearch/staging/%(version)s-%(hash)s/repos/%(package_repo_version)s/ s3://packages.elasticsearch.org/elasticsearch/%(package_repo_version)s'  % string_format_dict)
   print('   s3cmd cp --recursive s3://%(bucket)s/elasticsearch/staging/%(version)s-%(hash)s/org/ s3://%(bucket)s/elasticsearch/release/org'  % string_format_dict)
   print('Now go ahead and tag the release:')
   print('   git tag -a v%(version)s %(hash)s'  % string_format_dict)
diff --git a/dev-tools/smoke_test_rc.py b/dev-tools/smoke_test_rc.py
index 3bb11ca..b7bc00d 100644
--- a/dev-tools/smoke_test_rc.py
+++ b/dev-tools/smoke_test_rc.py
@@ -62,10 +62,10 @@ DEFAULT_PLUGINS = ["analysis-icu",
                    "analysis-phonetic",
                    "analysis-smartcn",
                    "analysis-stempel",
-                   "cloud-gce",
                    "delete-by-query",
                    "discovery-azure",
                    "discovery-ec2",
+                   "discovery-gce",
                    "discovery-multicast",
                    "lang-expression",
                    "lang-groovy",
diff --git a/dev-tools/src/main/resources/forbidden/all-signatures.txt b/dev-tools/src/main/resources/forbidden/all-signatures.txt
index e57a862..ee534b4 100644
--- a/dev-tools/src/main/resources/forbidden/all-signatures.txt
+++ b/dev-tools/src/main/resources/forbidden/all-signatures.txt
@@ -133,6 +133,8 @@ com.google.common.io.Resources
 com.google.common.hash.HashCode
 com.google.common.hash.HashFunction
 com.google.common.hash.Hashing
+com.google.common.collect.Iterators
+com.google.common.net.InetAddresses
 
 @defaultMessage Do not violate java's access system
 java.lang.reflect.AccessibleObject#setAccessible(boolean)
diff --git a/dev-tools/src/main/resources/plugin-metadata/plugin-descriptor.properties b/dev-tools/src/main/resources/plugin-metadata/plugin-descriptor.properties
index 67d139e..1588e11 100644
--- a/dev-tools/src/main/resources/plugin-metadata/plugin-descriptor.properties
+++ b/dev-tools/src/main/resources/plugin-metadata/plugin-descriptor.properties
@@ -24,7 +24,7 @@
 # jvm=true
 # classname=foo.bar.BazPlugin
 # description=My cool plugin
-# version=2.0
+# version=2.0.0-rc1
 # elasticsearch.version=2.0
 # java.version=1.7
 #
@@ -64,6 +64,10 @@ classname=${elasticsearch.plugin.classname}
 java.version=${maven.compiler.target}
 #
 # 'elasticsearch.version' version of elasticsearch compiled against
+# You will have to release a new version of the plugin for each new
+# elasticsearch release. This version is checked when the plugin
+# is loaded so Elasticsearch will refuse to start in the presence of
+# plugins with the incorrect elasticsearch.version.
 elasticsearch.version=${elasticsearch.version}
 #
 ### deprecated elements for jvm plugins :
diff --git a/distribution/deb/pom.xml b/distribution/deb/pom.xml
index 7237674..182398d 100644
--- a/distribution/deb/pom.xml
+++ b/distribution/deb/pom.xml
@@ -76,6 +76,7 @@
                                         <include>bin/elasticsearch</include>
                                         <include>bin/elasticsearch.in.sh</include>
                                         <include>bin/plugin</include>
+                                        <include>bin/elasticsearch-systemd-pre-exec</include>
                                     </includes>
                                 </resource>
                             </resources>
@@ -110,7 +111,7 @@
                                 <data>
                                     <src>${project.build.directory}/generated-packaging/deb/bin</src>
                                     <type>directory</type>
-                                    <includes>elasticsearch,elasticsearch.in.sh,plugin</includes>
+                                    <includes>elasticsearch,elasticsearch.in.sh,plugin,elasticsearch-systemd-pre-exec</includes>
                                     <mapper>
                                         <type>perm</type>
                                         <prefix>${packaging.elasticsearch.bin.dir}</prefix>
diff --git a/distribution/deb/src/main/packaging/init.d/elasticsearch b/distribution/deb/src/main/packaging/init.d/elasticsearch
index 9ea2beb..3a82bbe 100755
--- a/distribution/deb/src/main/packaging/init.d/elasticsearch
+++ b/distribution/deb/src/main/packaging/init.d/elasticsearch
@@ -74,9 +74,6 @@ DATA_DIR=/var/lib/$NAME
 # Elasticsearch configuration directory
 CONF_DIR=/etc/$NAME
 
-# Elasticsearch configuration file (elasticsearch.yml)
-CONF_FILE=$CONF_DIR/elasticsearch.yml
-
 # Maximum number of VMA (Virtual Memory Areas) a process can own
 MAX_MAP_COUNT=262144
 
@@ -93,10 +90,16 @@ if [ -f "$DEFAULT" ]; then
 	. "$DEFAULT"
 fi
 
+# CONF_FILE setting was removed
+if [ ! -z "$CONF_FILE" ]; then
+    echo "CONF_FILE setting is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed."
+    exit 1
+fi
+
 # Define other required variables
 PID_FILE="$PID_DIR/$NAME.pid"
 DAEMON=$ES_HOME/bin/elasticsearch
-DAEMON_OPTS="-d -p $PID_FILE --default.config=$CONF_FILE --default.path.home=$ES_HOME --default.path.logs=$LOG_DIR --default.path.data=$DATA_DIR --default.path.conf=$CONF_DIR"
+DAEMON_OPTS="-d -p $PID_FILE --default.path.home=$ES_HOME --default.path.logs=$LOG_DIR --default.path.data=$DATA_DIR --default.path.conf=$CONF_DIR"
 
 export ES_HEAP_SIZE
 export ES_HEAP_NEWSIZE
diff --git a/distribution/deb/src/main/packaging/packaging.properties b/distribution/deb/src/main/packaging/packaging.properties
index f268cde..3635928 100644
--- a/distribution/deb/src/main/packaging/packaging.properties
+++ b/distribution/deb/src/main/packaging/packaging.properties
@@ -6,7 +6,6 @@ packaging.env.file=/etc/default/elasticsearch
 
 # Default configuration directory and file to use in bin/plugin script
 packaging.plugin.default.config.dir=${packaging.elasticsearch.conf.dir}
-packaging.plugin.default.config.file=${packaging.elasticsearch.conf.dir}/elasticsearch.yml
 
 # Simple marker to check that properties are correctly overridden
 packaging.type=deb
diff --git a/distribution/rpm/pom.xml b/distribution/rpm/pom.xml
index 37f7203..1e3004c 100644
--- a/distribution/rpm/pom.xml
+++ b/distribution/rpm/pom.xml
@@ -79,6 +79,7 @@
                                         <include>bin/elasticsearch</include>
                                         <include>bin/elasticsearch.in.sh</include>
                                         <include>bin/plugin</include>
+                                        <include>bin/elasticsearch-systemd-pre-exec</include>
                                     </includes>
                                 </resource>
                             </resources>
@@ -127,6 +128,7 @@
                                         <include>elasticsearch</include>
                                         <include>elasticsearch.in.sh</include>
                                         <include>plugin</include>
+                                        <include>elasticsearch-systemd-pre-exec</include>
                                     </includes>
                                 </source>
                             </sources>
diff --git a/distribution/rpm/src/main/packaging/init.d/elasticsearch b/distribution/rpm/src/main/packaging/init.d/elasticsearch
index 9626dfc..924c678 100644
--- a/distribution/rpm/src/main/packaging/init.d/elasticsearch
+++ b/distribution/rpm/src/main/packaging/init.d/elasticsearch
@@ -40,7 +40,7 @@ MAX_MAP_COUNT=${packaging.os.max.map.count}
 LOG_DIR="${packaging.elasticsearch.log.dir}"
 DATA_DIR="${packaging.elasticsearch.data.dir}"
 CONF_DIR="${packaging.elasticsearch.conf.dir}"
-CONF_FILE="${packaging.elasticsearch.conf.dir}/elasticsearch.yml"
+
 PID_DIR="${packaging.elasticsearch.pid.dir}"
 
 # Source the default env file
@@ -49,6 +49,12 @@ if [ -f "$ES_ENV_FILE" ]; then
     . "$ES_ENV_FILE"
 fi
 
+# CONF_FILE setting was removed
+if [ ! -z "$CONF_FILE" ]; then
+    echo "CONF_FILE setting is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed."
+    exit 1
+fi
+
 exec="$ES_HOME/bin/elasticsearch"
 prog="elasticsearch"
 pidfile="$PID_DIR/${prog}.pid"
@@ -83,7 +89,6 @@ checkJava() {
 start() {
     checkJava
     [ -x $exec ] || exit 5
-    [ -f $CONF_FILE ] || exit 6
     if [ -n "$MAX_LOCKED_MEMORY" -a -z "$ES_HEAP_SIZE" ]; then
         echo "MAX_LOCKED_MEMORY is set - ES_HEAP_SIZE must also be set"
         return 7
diff --git a/distribution/rpm/src/main/packaging/packaging.properties b/distribution/rpm/src/main/packaging/packaging.properties
index b5bf28a..bc4af5f 100644
--- a/distribution/rpm/src/main/packaging/packaging.properties
+++ b/distribution/rpm/src/main/packaging/packaging.properties
@@ -6,7 +6,6 @@ packaging.env.file=/etc/sysconfig/elasticsearch
 
 # Default configuration directory and file to use in bin/plugin script
 packaging.plugin.default.config.dir=${packaging.elasticsearch.conf.dir}
-packaging.plugin.default.config.file=${packaging.elasticsearch.conf.dir}/elasticsearch.yml
 
 # Simple marker to check that properties are correctly overridden
 packaging.type=rpm
diff --git a/distribution/src/main/packaging/env/elasticsearch b/distribution/src/main/packaging/env/elasticsearch
index cdf05bb..0c01d4f 100644
--- a/distribution/src/main/packaging/env/elasticsearch
+++ b/distribution/src/main/packaging/env/elasticsearch
@@ -8,9 +8,6 @@
 # Elasticsearch configuration directory
 #CONF_DIR=${packaging.elasticsearch.conf.dir}
 
-# Elasticsearch configuration file
-#CONF_FILE=$CONF_DIR/elasticsearch.yml
-
 # Elasticsearch data directory
 #DATA_DIR=${packaging.elasticsearch.data.dir}
 
diff --git a/distribution/src/main/packaging/packaging.properties b/distribution/src/main/packaging/packaging.properties
index ff95c9d..be5b604 100644
--- a/distribution/src/main/packaging/packaging.properties
+++ b/distribution/src/main/packaging/packaging.properties
@@ -8,7 +8,6 @@ packaging.env.file=
 
 # Default configuration directory and file to use in bin/plugin script
 packaging.plugin.default.config.dir=$ES_HOME/config
-packaging.plugin.default.config.file=$ES_HOME/config/elasticsearch.yml
 
 # Default values for min/max heap memory allocated to elasticsearch java process
 packaging.elasticsearch.heap.min=256m
diff --git a/distribution/src/main/packaging/systemd/elasticsearch.service b/distribution/src/main/packaging/systemd/elasticsearch.service
index cdcad9d..d8f56f7 100644
--- a/distribution/src/main/packaging/systemd/elasticsearch.service
+++ b/distribution/src/main/packaging/systemd/elasticsearch.service
@@ -7,7 +7,6 @@ After=network-online.target
 [Service]
 Environment=ES_HOME=${packaging.elasticsearch.home.dir}
 Environment=CONF_DIR=${packaging.elasticsearch.conf.dir}
-Environment=CONF_FILE=${packaging.elasticsearch.conf.dir}/elasticsearch.yml
 Environment=DATA_DIR=${packaging.elasticsearch.data.dir}
 Environment=LOG_DIR=${packaging.elasticsearch.log.dir}
 Environment=PID_DIR=${packaging.elasticsearch.pid.dir}
@@ -18,12 +17,13 @@ WorkingDirectory=${packaging.elasticsearch.home.dir}
 User=${packaging.elasticsearch.user}
 Group=${packaging.elasticsearch.group}
 
+ExecStartPre=${packaging.elasticsearch.bin.dir}/elasticsearch-systemd-pre-exec
+
 ExecStart=${packaging.elasticsearch.bin.dir}/elasticsearch \
                                                 -Des.pidfile=${PID_DIR}/elasticsearch.pid \
                                                 -Des.default.path.home=${ES_HOME} \
                                                 -Des.default.path.logs=${LOG_DIR} \
                                                 -Des.default.path.data=${DATA_DIR} \
-                                                -Des.default.config=${CONF_FILE} \
                                                 -Des.default.path.conf=${CONF_DIR}
 
 # Connects standard output to /dev/null
diff --git a/distribution/src/main/resources/bin/elasticsearch b/distribution/src/main/resources/bin/elasticsearch
index 878fcff..66f4657 100755
--- a/distribution/src/main/resources/bin/elasticsearch
+++ b/distribution/src/main/resources/bin/elasticsearch
@@ -42,10 +42,10 @@
 # Be aware that you will be entirely responsible for populating the needed
 # environment variables.
 
-
 # Maven will replace the project.name with elasticsearch below. If that
 # hasn't been done, we assume that this is not a packaged version and the
 # user has forgotten to run Maven to create a package.
+
 IS_PACKAGED_VERSION='${project.parent.artifactId}'
 if [ "$IS_PACKAGED_VERSION" != "distributions" ]; then
     cat >&2 << EOF
diff --git a/distribution/src/main/resources/bin/elasticsearch-systemd-pre-exec b/distribution/src/main/resources/bin/elasticsearch-systemd-pre-exec
new file mode 100755
index 0000000..a51d639
--- /dev/null
+++ b/distribution/src/main/resources/bin/elasticsearch-systemd-pre-exec
@@ -0,0 +1,7 @@
+#!/bin/sh
+
+# CONF_FILE setting was removed
+if [ ! -z "$CONF_FILE" ]; then
+    echo "CONF_FILE setting is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed."
+    exit 1
+fi
diff --git a/distribution/src/main/resources/bin/plugin b/distribution/src/main/resources/bin/plugin
index c466d48..35dbe3a 100755
--- a/distribution/src/main/resources/bin/plugin
+++ b/distribution/src/main/resources/bin/plugin
@@ -1,5 +1,6 @@
 #!/bin/sh
 
+
 CDPATH=""
 SCRIPT="$0"
 
@@ -21,17 +22,10 @@ ES_HOME=`dirname "$SCRIPT"`/..
 # make ELASTICSEARCH_HOME absolute
 ES_HOME=`cd "$ES_HOME"; pwd`
 
+
 # Sets the default values for elasticsearch variables used in this script
 if [ -z "$CONF_DIR" ]; then
   CONF_DIR="${packaging.plugin.default.config.dir}"
-
-  if [ -z "$CONF_FILE" ]; then
-    CONF_FILE="$CONF_DIR/elasticsearch.yml"
-  fi
-fi
-
-if [ -z "$CONF_FILE" ]; then
-  CONF_FILE="${packaging.plugin.default.config.file}"
 fi
 
 # The default env file is defined at building/packaging time.
@@ -66,6 +60,12 @@ if [ "x$JAVA_TOOL_OPTIONS" != "x" ]; then
     unset JAVA_TOOL_OPTIONS
 fi
 
+# CONF_FILE setting was removed
+if [ ! -z "$CONF_FILE" ]; then
+    echo "CONF_FILE setting is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed."
+    exit 1
+fi
+
 if [ -x "$JAVA_HOME/bin/java" ]; then
     JAVA=$JAVA_HOME/bin/java
 else
@@ -105,16 +105,6 @@ if [ -e "$CONF_DIR" ]; then
   esac
 fi
 
-if [ -e "$CONF_FILE" ]; then
-  case "$properties" in
-    *-Des.default.config=*|*-Des.config=*)
-    ;;
-    *)
-      properties="$properties -Des.default.config=\"$CONF_FILE\""
-    ;;
-  esac
-fi
-
 # full hostname passed through cut for portability on systems that do not support hostname -s
 # export on separate line for shells that do not support combining definition and export
 HOSTNAME=`hostname | cut -d. -f1`
diff --git a/distribution/src/main/resources/bin/service.bat b/distribution/src/main/resources/bin/service.bat
index 06c9c64..9822e6b 100644
--- a/distribution/src/main/resources/bin/service.bat
+++ b/distribution/src/main/resources/bin/service.bat
@@ -5,6 +5,8 @@ TITLE Elasticsearch Service ${project.version}
 
 if NOT DEFINED JAVA_HOME goto err
 
+if not "%CONF_FILE%" == "" goto conffileset
+
 set SCRIPT_DIR=%~dp0
 for %%I in ("%SCRIPT_DIR%..") do set ES_HOME=%%~dpfI
 
@@ -147,9 +149,7 @@ if "%DATA_DIR%" == "" set DATA_DIR=%ES_HOME%\data
 
 if "%CONF_DIR%" == "" set CONF_DIR=%ES_HOME%\config
 
-if "%CONF_FILE%" == "" set CONF_FILE=%ES_HOME%\config\elasticsearch.yml
-
-set ES_PARAMS=-Delasticsearch;-Des.path.home="%ES_HOME%";-Des.default.config="%CONF_FILE%";-Des.default.path.home="%ES_HOME%";-Des.default.path.logs="%LOG_DIR%";-Des.default.path.data="%DATA_DIR%";-Des.default.path.conf="%CONF_DIR%"
+set ES_PARAMS=-Delasticsearch;-Des.path.home="%ES_HOME%";-Des.default.path.home="%ES_HOME%";-Des.default.path.logs="%LOG_DIR%";-Des.default.path.data="%DATA_DIR%";-Des.default.path.conf="%CONF_DIR%"
 
 set JVM_OPTS=%JAVA_OPTS: =;%
 
@@ -207,4 +207,8 @@ set /a conv=%conv% * 1024
 set "%~2=%conv%"
 goto:eof
 
+:conffileset
+echo CONF_FILE setting is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed.
+goto:eof
+
 ENDLOCAL
diff --git a/docs/plugins/authors.asciidoc b/docs/plugins/authors.asciidoc
index 2ce05fd..e0db081 100644
--- a/docs/plugins/authors.asciidoc
+++ b/docs/plugins/authors.asciidoc
@@ -43,6 +43,72 @@ instance, see
 https://github.com/elastic/elasticsearch/blob/master/plugins/site-example/pom.xml[`plugins/site-example/pom.xml`].
 
 [float]
+==== Mandatory elements for all plugins
+
+
+[cols="<,<,<",options="header",]
+|=======================================================================
+|Element                    | Type   | Description
+
+|`description`              |String  | simple summary of the plugin
+
+|`version`                  |String  | plugin's version
+
+|`name`                     |String  | the plugin name
+
+|=======================================================================
+
+
+
+[float]
+==== Mandatory elements for Java plugins
+
+
+[cols="<,<,<",options="header",]
+|=======================================================================
+|Element                    | Type   | Description
+
+|`jvm`                      |Boolean | true if the `classname` class should be loaded
+from jar files in the root directory of the plugin.
+Note that only jar files in the root directory are added to the classpath for the plugin!
+If you need other resources, package them into a resources jar.
+
+|`classname`                |String  | the name of the class to load, fully-qualified.
+
+|`java.version`             |String  | version of java the code is built against.
+Use the system property `java.specification.version`. Version string must be a sequence
+of nonnegative decimal integers separated by "."'s and may have leading zeros.
+
+|`elasticsearch.version`    |String  | version of elasticsearch compiled against.
+
+|=======================================================================
+
+[IMPORTANT]
+.Plugin release lifecycle
+==============================================
+
+You will have to release a new version of the plugin for each new elasticsearch release.
+This version is checked when the plugin is loaded so Elasticsearch will refuse to start
+in the presence of plugins with the incorrect `elasticsearch.version`.
+
+==============================================
+
+
+[float]
+==== Mandatory elements for Site plugins
+
+
+[cols="<,<,<",options="header",]
+|=======================================================================
+|Element                    | Type   | Description
+
+|`site`                     |Boolean | true to indicate contents of the `_site/`
+directory in the root of the plugin should be served.
+
+|=======================================================================
+
+
+[float]
 === Testing your plugin
 
 When testing a Java plugin, it will only be auto-loaded if it is in the
diff --git a/docs/plugins/cloud-gce.asciidoc b/docs/plugins/cloud-gce.asciidoc
deleted file mode 100644
index 6d712ad..0000000
--- a/docs/plugins/cloud-gce.asciidoc
+++ /dev/null
@@ -1,454 +0,0 @@
-[[cloud-gce]]
-=== GCE Cloud Plugin
-
-The Google Compute Engine Cloud plugin uses the GCE API for unicast discovery.
-
-[[cloud-gce-install]]
-[float]
-==== Installation
-
-This plugin can be installed using the plugin manager:
-
-[source,sh]
-----------------------------------------------------------------
-sudo bin/plugin install cloud-gce
-----------------------------------------------------------------
-
-The plugin must be installed on every node in the cluster, and each node must
-be restarted after installation.
-
-[[cloud-gce-remove]]
-[float]
-==== Removal
-
-The plugin can be removed with the following command:
-
-[source,sh]
-----------------------------------------------------------------
-sudo bin/plugin remove cloud-gce
-----------------------------------------------------------------
-
-The node must be stopped before removing the plugin.
-
-[[cloud-gce-usage-discovery]]
-==== GCE Virtual Machine Discovery
-
-Google Compute Engine VM discovery allows to use the google APIs to perform automatic discovery (similar to multicast
-in non hostile multicast environments). Here is a simple sample configuration:
-
-[source,yaml]
---------------------------------------------------
-cloud:
-  gce:
-      project_id: <your-google-project-id>
-      zone: <your-zone>
-discovery:
-      type: gce
---------------------------------------------------
-
-[[cloud-gce-usage-discovery-short]]
-===== How to start (short story)
-
-* Create Google Compute Engine instance (with compute rw permissions)
-* Install Elasticsearch
-* Install Google Compute Engine Cloud plugin
-* Modify `elasticsearch.yml` file
-* Start Elasticsearch
-
-[[cloud-gce-usage-discovery-long]]
-==== Setting up GCE Discovery
-
-
-[[cloud-gce-usage-discovery-long-prerequisites]]
-===== Prerequisites
-
-Before starting, you need:
-
-* Your project ID, e.g. `es-cloud`. Get it from https://code.google.com/apis/console/[Google API Console].
-* To install https://developers.google.com/cloud/sdk/[Google Cloud SDK]
-
-If you did not set it yet, you can define your default project you will work on:
-
-[source,sh]
---------------------------------------------------
-gcloud config set project es-cloud
---------------------------------------------------
-
-[[cloud-gce-usage-discovery-long-login]]
-
-If you haven't already, login to Google Cloud
-
-[source,sh]
---------------------------------------------------
-gcloud auth login
---------------------------------------------------
-
-This will open your browser. You will be asked to sign-in to a Google account and
-authorize access to the Google Cloud SDK.
-
-[[cloud-gce-usage-discovery-long-first-instance]]
-===== Creating your first instance
-
-
-[source,sh]
---------------------------------------------------
-gcloud compute instances create myesnode1 \
-       --zone <your-zone> \
-       --scopes compute-rw
---------------------------------------------------
-
-When done, a report like this one should appears:
-
-[source,text]
---------------------------------------------------
-Created [https://www.googleapis.com/compute/v1/projects/es-cloud-1070/zones/us-central1-f/instances/myesnode1].
-NAME      ZONE          MACHINE_TYPE  PREEMPTIBLE INTERNAL_IP   EXTERNAL_IP   STATUS
-myesnode1 us-central1-f n1-standard-1             10.240.133.54 104.197.94.25 RUNNING
---------------------------------------------------
-
-You can now connect to your instance:
-
-[source,sh]
---------------------------------------------------
-# Connect using google cloud SDK
-gcloud compute ssh myesnode1 --zone europe-west1-a
-
-# Or using SSH with external IP address
-ssh -i ~/.ssh/google_compute_engine 192.158.29.199
---------------------------------------------------
-
-[IMPORTANT]
-.Service Account Permissions
-==============================================
-
-It's important when creating an instance that the correct permissions are set. At a minimum, you must ensure you have:
-
-[source,text]
---------------------------------------------------
-scopes=compute-rw
---------------------------------------------------
-
-Failing to set this will result in unauthorized messages when starting Elasticsearch.
-See [Machine Permissions](#machine-permissions).
-==============================================
-
-
-Once connected, install Elasticsearch:
-
-[source,sh]
---------------------------------------------------
-sudo apt-get update
-
-# Download Elasticsearch
-wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-2.0.0.deb
-
-# Prepare Java installation
-sudo apt-get install java8-runtime-headless
-
-# Prepare Elasticsearch installation
-sudo dpkg -i elasticsearch-2.0.0.deb
---------------------------------------------------
-
-[[cloud-gce-usage-discovery-long-install-plugin]]
-===== Install elasticsearch cloud gce plugin
-
-Install the plugin:
-
-[source,sh]
---------------------------------------------------
-# Use Plugin Manager to install it
-sudo bin/plugin install cloud-gce
---------------------------------------------------
-
-Open the `elasticsearch.yml` file:
-
-[source,sh]
---------------------------------------------------
-sudo vi /etc/elasticsearch/elasticsearch.yml
---------------------------------------------------
-
-And add the following lines:
-
-[source,yaml]
---------------------------------------------------
-cloud:
-  gce:
-      project_id: es-cloud
-      zone: europe-west1-a
-discovery:
-      type: gce
---------------------------------------------------
-
-
-Start elasticsearch:
-
-[source,sh]
---------------------------------------------------
-sudo /etc/init.d/elasticsearch start
---------------------------------------------------
-
-If anything goes wrong, you should check logs:
-
-[source,sh]
---------------------------------------------------
-tail -f /var/log/elasticsearch/elasticsearch.log
---------------------------------------------------
-
-If needed, you can change log level to `TRACE` by opening `logging.yml`:
-
-[source,sh]
---------------------------------------------------
-sudo vi /etc/elasticsearch/logging.yml
---------------------------------------------------
-
-and adding the following line:
-
-[source,yaml]
---------------------------------------------------
-# discovery
-discovery.gce: TRACE
---------------------------------------------------
-
-
-
-[[cloud-gce-usage-discovery-cloning]]
-==== Cloning your existing machine
-
-In order to build a cluster on many nodes, you can clone your configured instance to new nodes.
-You won't have to reinstall everything!
-
-First create an image of your running instance and upload it to Google Cloud Storage:
-
-[source,sh]
---------------------------------------------------
-# Create an image of yur current instance
-sudo /usr/bin/gcimagebundle -d /dev/sda -o /tmp/
-
-# An image has been created in `/tmp` directory:
-ls /tmp
-e4686d7f5bf904a924ae0cfeb58d0827c6d5b966.image.tar.gz
-
-# Upload your image to Google Cloud Storage:
-# Create a bucket to hold your image, let's say `esimage`:
-gsutil mb gs://esimage
-
-# Copy your image to this bucket:
-gsutil cp /tmp/e4686d7f5bf904a924ae0cfeb58d0827c6d5b966.image.tar.gz gs://esimage
-
-# Then add your image to images collection:
-gcloud compute images create elasticsearch-2-0-0 --source-uri gs://esimage/e4686d7f5bf904a924ae0cfeb58d0827c6d5b966.image.tar.gz
-
-# If the previous command did not work for you, logout from your instance
-# and launch the same command from your local machine.
---------------------------------------------------
-
-[[cloud-gce-usage-discovery-start-new-instances]]
-===== Start new instances
-
-As you have now an image, you can create as many instances as you need:
-
-[source,sh]
---------------------------------------------------
-# Just change node name (here myesnode2)
-gcloud compute instances create myesnode2 --image elasticsearch-2-0-0 --zone europe-west1-a
-
-# If you want to provide all details directly, you can use:
-gcloud compute instances create myesnode2 --image=elasticsearch-2-0-0 \
-       --zone europe-west1-a --machine-type f1-micro --scopes=compute-rw
---------------------------------------------------
-
-[[cloud-gce-usage-discovery-remove-instance]]
-===== Remove an instance (aka shut it down)
-
-You can use https://cloud.google.com/console[Google Cloud Console] or CLI to manage your instances:
-
-[source,sh]
---------------------------------------------------
-# Stopping and removing instances
-gcloud compute instances delete myesnode1 myesnode2 \
-       --zone=europe-west1-a
-
-# Consider removing disk as well if you don't need them anymore
-gcloud compute disks deleted boot-myesnode1 boot-myesnode2  \
-       --zone=europe-west1-a
---------------------------------------------------
-
-[[cloud-gce-usage-discovery-zones]]
-==== Using GCE zones
-
-`cloud.gce.zone` helps to retrieve instances running in a given zone. It should be one of the
-https://developers.google.com/compute/docs/zones#available[GCE supported zones].
-
-The GCE discovery can support multi zones although you need to be aware of network latency between zones.
-To enable discovery across more than one zone, just enter add your zone list to `cloud.gce.zone` setting:
-
-[source,yaml]
---------------------------------------------------
-cloud:
-  gce:
-      project_id: <your-google-project-id>
-      zone: ["<your-zone1>", "<your-zone2>"]
-discovery:
-      type: gce
---------------------------------------------------
-
-
-
-[[cloud-gce-usage-discovery-tags]]
-==== Filtering by tags
-
-The GCE discovery can also filter machines to include in the cluster based on tags using `discovery.gce.tags` settings.
-For example, setting `discovery.gce.tags` to `dev` will only filter instances having a tag set to `dev`. Several tags
-set will require all of those tags to be set for the instance to be included.
-
-One practical use for tag filtering is when an GCE cluster contains many nodes that are not running
-elasticsearch. In this case (particularly with high ping_timeout values) there is a risk that a new node's discovery
-phase will end before it has found the cluster (which will result in it declaring itself master of a new cluster
-with the same name - highly undesirable). Adding tag on elasticsearch GCE nodes and then filtering by that
-tag will resolve this issue.
-
-Add your tag when building the new instance:
-
-[source,sh]
---------------------------------------------------
-gcloud compute instances create myesnode1 --project=es-cloud \
-       --scopes=compute-rw \
-       --tags=elasticsearch,dev
---------------------------------------------------
-
-Then, define it in `elasticsearch.yml`:
-
-[source,yaml]
---------------------------------------------------
-cloud:
-  gce:
-      project_id: es-cloud
-      zone: europe-west1-a
-discovery:
-      type: gce
-      gce:
-            tags: elasticsearch, dev
---------------------------------------------------
-
-[[cloud-gce-usage-discovery-port]]
-==== Changing default transport port
-
-By default, elasticsearch GCE plugin assumes that you run elasticsearch on 9300 default port.
-But you can specify the port value elasticsearch is meant to use using google compute engine metadata `es_port`:
-
-[[cloud-gce-usage-discovery-port-create]]
-===== When creating instance
-
-Add `--metadata es_port=9301` option:
-
-[source,sh]
---------------------------------------------------
-# when creating first instance
-gcloud compute instances create myesnode1 \
-       --scopes=compute-rw,storage-full \
-       --metadata es_port=9301
-
-# when creating an instance from an image
-gcloud compute instances create myesnode2 --image=elasticsearch-1-0-0-RC1 \
-       --zone europe-west1-a --machine-type f1-micro --scopes=compute-rw \
-       --metadata es_port=9301
---------------------------------------------------
-
-[[cloud-gce-usage-discovery-port-run]]
-===== On a running instance
-
-[source,sh]
---------------------------------------------------
-gcloud compute instances add-metadata myesnode1 \
-       --zone europe-west1-a \
-       --metadata es_port=9301
---------------------------------------------------
-
-
-[[cloud-gce-usage-discovery-tips]]
-==== GCE Tips
-
-[[cloud-gce-usage-discovery-tips-projectid]]
-===== Store project id locally
-
-If you don't want to repeat the project id each time, you can save it in the local gcloud config
-
-[source,sh]
---------------------------------------------------
-gcloud config set project es-cloud
---------------------------------------------------
-
-[[cloud-gce-usage-discovery-tips-permissions]]
-===== Machine Permissions
-
-If you have created a machine without the correct permissions, you will see `403 unauthorized` error messages. The only
-way to alter these permissions is to delete the instance (NOT THE DISK). Then create another with the correct permissions.
-
-Creating machines with gcloud::
-+
---
-Ensure the following flags are set:
-
-[source,text]
---------------------------------------------------
---scopes=compute-rw
---------------------------------------------------
---
-
-Creating with console (web)::
-+
---
-When creating an instance using the web portal, click _Show advanced options_.
-
-At the bottom of the page, under `PROJECT ACCESS`, choose `>> Compute >> Read Write`.
---
-
-Creating with knife google::
-+
---
-Set the service account scopes when creating the machine:
-
-[source,sh]
---------------------------------------------------
-knife google server create www1 \
-    -m n1-standard-1 \
-    -I debian-8 \
-    -Z us-central1-a \
-    -i ~/.ssh/id_rsa \
-    -x jdoe \
-    --gce-service-account-scopes https://www.googleapis.com/auth/compute.full_control
---------------------------------------------------
-
-Or, you may use the alias:
-
-[source,sh]
---------------------------------------------------
-    --gce-service-account-scopes compute-rw
---------------------------------------------------
---
-
-[[cloud-gce-usage-discovery-testing]]
-==== Testing GCE
-
-Integrations tests in this plugin require working GCE configuration and
-therefore disabled by default. To enable tests prepare a config file
-elasticsearch.yml with the following content:
-
-[source,yaml]
---------------------------------------------------
-cloud:
-  gce:
-      project_id: es-cloud
-      zone: europe-west1-a
-discovery:
-      type: gce
---------------------------------------------------
-
-Replaces `project_id` and `zone` with your settings.
-
-To run test:
-
-[source,sh]
---------------------------------------------------
-mvn -Dtests.gce=true -Dtests.config=/path/to/config/file/elasticsearch.yml clean test
---------------------------------------------------
diff --git a/docs/plugins/discovery-gce.asciidoc b/docs/plugins/discovery-gce.asciidoc
new file mode 100644
index 0000000..fef8646
--- /dev/null
+++ b/docs/plugins/discovery-gce.asciidoc
@@ -0,0 +1,500 @@
+[[discovery-gce]]
+=== GCE Discovery Plugin
+
+The Google Compute Engine Discovery plugin uses the GCE API for unicast discovery.
+
+[[discovery-gce-install]]
+[float]
+==== Installation
+
+This plugin can be installed using the plugin manager:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin install discovery-gce
+----------------------------------------------------------------
+
+The plugin must be installed on every node in the cluster, and each node must
+be restarted after installation.
+
+[[discovery-gce-remove]]
+[float]
+==== Removal
+
+The plugin can be removed with the following command:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin remove discovery-gce
+----------------------------------------------------------------
+
+The node must be stopped before removing the plugin.
+
+[[discovery-gce-usage]]
+==== GCE Virtual Machine Discovery
+
+Google Compute Engine VM discovery allows to use the google APIs to perform automatic discovery (similar to multicast
+in non hostile multicast environments). Here is a simple sample configuration:
+
+[source,yaml]
+--------------------------------------------------
+cloud:
+  gce:
+      project_id: <your-google-project-id>
+      zone: <your-zone>
+discovery:
+      type: gce
+--------------------------------------------------
+
+
+[IMPORTANT]
+.Binding the network host
+==============================================
+
+It's important to define `network.host` as by default it's bound to `localhost`.
+
+You can use {ref}/modules-network.html[core network host settings] or
+<<discovery-gce-network-host,gce specific host settings>>:
+
+==============================================
+
+[[discovery-gce-network-host]]
+==== GCE Network Host
+
+When the `discovery-gce` plugin is installed, the following are also allowed
+as valid network host settings:
+
+[cols="<,<",options="header",]
+|==================================================================
+|GCE Host Value |Description
+|`_gce:privateIp:X_` |The private IP address of the machine for a given network interface.
+|`_gce:hostname_` |The hostname of the machine.
+|`_gce_` |Same as `_gce:privateIp:0_` (recommended).
+|==================================================================
+
+Examples:
+
+[source,yaml]
+--------------------------------------------------
+# get the IP address from network interface 1
+network.host: _gce:privateIp:1_
+# shortcut for _gce:privateIp:0_
+network.host: _gce_
+# Using GCE internal hostname (recommended)
+network.host: _gce:hostname_
+--------------------------------------------------
+
+[[discovery-gce-usage-short]]
+===== How to start (short story)
+
+* Create Google Compute Engine instance (with compute rw permissions)
+* Install Elasticsearch
+* Install Google Compute Engine Cloud plugin
+* Modify `elasticsearch.yml` file
+* Start Elasticsearch
+
+[[discovery-gce-usage-long]]
+==== Setting up GCE Discovery
+
+
+[[discovery-gce-usage-long-prerequisites]]
+===== Prerequisites
+
+Before starting, you need:
+
+* Your project ID, e.g. `es-cloud`. Get it from https://code.google.com/apis/console/[Google API Console].
+* To install https://developers.google.com/cloud/sdk/[Google Cloud SDK]
+
+If you did not set it yet, you can define your default project you will work on:
+
+[source,sh]
+--------------------------------------------------
+gcloud config set project es-cloud
+--------------------------------------------------
+
+[[discovery-gce-usage-long-login]]
+===== Login to Google Cloud
+
+If you haven't already, login to Google Cloud
+
+[source,sh]
+--------------------------------------------------
+gcloud auth login
+--------------------------------------------------
+
+This will open your browser. You will be asked to sign-in to a Google account and
+authorize access to the Google Cloud SDK.
+
+[[discovery-gce-usage-long-first-instance]]
+===== Creating your first instance
+
+
+[source,sh]
+--------------------------------------------------
+gcloud compute instances create myesnode1 \
+       --zone <your-zone> \
+       --scopes compute-rw
+--------------------------------------------------
+
+When done, a report like this one should appears:
+
+[source,text]
+--------------------------------------------------
+Created [https://www.googleapis.com/compute/v1/projects/es-cloud-1070/zones/us-central1-f/instances/myesnode1].
+NAME      ZONE          MACHINE_TYPE  PREEMPTIBLE INTERNAL_IP   EXTERNAL_IP   STATUS
+myesnode1 us-central1-f n1-standard-1             10.240.133.54 104.197.94.25 RUNNING
+--------------------------------------------------
+
+You can now connect to your instance:
+
+[source,sh]
+--------------------------------------------------
+# Connect using google cloud SDK
+gcloud compute ssh myesnode1 --zone europe-west1-a
+
+# Or using SSH with external IP address
+ssh -i ~/.ssh/google_compute_engine 192.158.29.199
+--------------------------------------------------
+
+[IMPORTANT]
+.Service Account Permissions
+==============================================
+
+It's important when creating an instance that the correct permissions are set. At a minimum, you must ensure you have:
+
+[source,text]
+--------------------------------------------------
+scopes=compute-rw
+--------------------------------------------------
+
+Failing to set this will result in unauthorized messages when starting Elasticsearch.
+See <<discovery-gce-usage-tips-permissions>>.
+==============================================
+
+
+Once connected, install Elasticsearch:
+
+[source,sh]
+--------------------------------------------------
+sudo apt-get update
+
+# Download Elasticsearch
+wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-2.0.0.deb
+
+# Prepare Java installation (Oracle)
+sudo echo "deb http://ppa.launchpad.net/webupd8team/java/ubuntu trusty main" | sudo tee /etc/apt/sources.list.d/webupd8team-java.list
+sudo echo "deb-src http://ppa.launchpad.net/webupd8team/java/ubuntu trusty main" | sudo tee -a /etc/apt/sources.list.d/webupd8team-java.list
+sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys EEA14886
+sudo apt-get update
+sudo apt-get install oracle-java8-installer
+
+# Prepare Java installation (or OpenJDK)
+# sudo apt-get install java8-runtime-headless
+
+# Prepare Elasticsearch installation
+sudo dpkg -i elasticsearch-2.0.0.deb
+--------------------------------------------------
+
+[[discovery-gce-usage-long-install-plugin]]
+===== Install elasticsearch discovery gce plugin
+
+Install the plugin:
+
+[source,sh]
+--------------------------------------------------
+# Use Plugin Manager to install it
+sudo bin/plugin install discovery-gce
+--------------------------------------------------
+
+Open the `elasticsearch.yml` file:
+
+[source,sh]
+--------------------------------------------------
+sudo vi /etc/elasticsearch/elasticsearch.yml
+--------------------------------------------------
+
+And add the following lines:
+
+[source,yaml]
+--------------------------------------------------
+cloud:
+  gce:
+      project_id: es-cloud
+      zone: europe-west1-a
+discovery:
+      type: gce
+--------------------------------------------------
+
+
+Start elasticsearch:
+
+[source,sh]
+--------------------------------------------------
+sudo /etc/init.d/elasticsearch start
+--------------------------------------------------
+
+If anything goes wrong, you should check logs:
+
+[source,sh]
+--------------------------------------------------
+tail -f /var/log/elasticsearch/elasticsearch.log
+--------------------------------------------------
+
+If needed, you can change log level to `TRACE` by opening `logging.yml`:
+
+[source,sh]
+--------------------------------------------------
+sudo vi /etc/elasticsearch/logging.yml
+--------------------------------------------------
+
+and adding the following line:
+
+[source,yaml]
+--------------------------------------------------
+# discovery
+discovery.gce: TRACE
+--------------------------------------------------
+
+
+
+[[discovery-gce-usage-cloning]]
+==== Cloning your existing machine
+
+In order to build a cluster on many nodes, you can clone your configured instance to new nodes.
+You won't have to reinstall everything!
+
+First create an image of your running instance and upload it to Google Cloud Storage:
+
+[source,sh]
+--------------------------------------------------
+# Create an image of your current instance
+sudo /usr/bin/gcimagebundle -d /dev/sda -o /tmp/
+
+# An image has been created in `/tmp` directory:
+ls /tmp
+e4686d7f5bf904a924ae0cfeb58d0827c6d5b966.image.tar.gz
+
+# Upload your image to Google Cloud Storage:
+# Create a bucket to hold your image, let's say `esimage`:
+gsutil mb gs://esimage
+
+# Copy your image to this bucket:
+gsutil cp /tmp/e4686d7f5bf904a924ae0cfeb58d0827c6d5b966.image.tar.gz gs://esimage
+
+# Then add your image to images collection:
+gcloud compute images create elasticsearch-2-0-0 --source-uri gs://esimage/e4686d7f5bf904a924ae0cfeb58d0827c6d5b966.image.tar.gz
+
+# If the previous command did not work for you, logout from your instance
+# and launch the same command from your local machine.
+--------------------------------------------------
+
+[[discovery-gce-usage-start-new-instances]]
+===== Start new instances
+
+As you have now an image, you can create as many instances as you need:
+
+[source,sh]
+--------------------------------------------------
+# Just change node name (here myesnode2)
+gcloud compute instances create myesnode2 --image elasticsearch-2-0-0 --zone europe-west1-a
+
+# If you want to provide all details directly, you can use:
+gcloud compute instances create myesnode2 --image=elasticsearch-2-0-0 \
+       --zone europe-west1-a --machine-type f1-micro --scopes=compute-rw
+--------------------------------------------------
+
+[[discovery-gce-usage-remove-instance]]
+===== Remove an instance (aka shut it down)
+
+You can use https://cloud.google.com/console[Google Cloud Console] or CLI to manage your instances:
+
+[source,sh]
+--------------------------------------------------
+# Stopping and removing instances
+gcloud compute instances delete myesnode1 myesnode2 \
+       --zone=europe-west1-a
+
+# Consider removing disk as well if you don't need them anymore
+gcloud compute disks deleted boot-myesnode1 boot-myesnode2  \
+       --zone=europe-west1-a
+--------------------------------------------------
+
+[[discovery-gce-usage-zones]]
+==== Using GCE zones
+
+`cloud.gce.zone` helps to retrieve instances running in a given zone. It should be one of the
+https://developers.google.com/compute/docs/zones#available[GCE supported zones].
+
+The GCE discovery can support multi zones although you need to be aware of network latency between zones.
+To enable discovery across more than one zone, just enter add your zone list to `cloud.gce.zone` setting:
+
+[source,yaml]
+--------------------------------------------------
+cloud:
+  gce:
+      project_id: <your-google-project-id>
+      zone: ["<your-zone1>", "<your-zone2>"]
+discovery:
+      type: gce
+--------------------------------------------------
+
+
+
+[[discovery-gce-usage-tags]]
+==== Filtering by tags
+
+The GCE discovery can also filter machines to include in the cluster based on tags using `discovery.gce.tags` settings.
+For example, setting `discovery.gce.tags` to `dev` will only filter instances having a tag set to `dev`. Several tags
+set will require all of those tags to be set for the instance to be included.
+
+One practical use for tag filtering is when an GCE cluster contains many nodes that are not running
+elasticsearch. In this case (particularly with high ping_timeout values) there is a risk that a new node's discovery
+phase will end before it has found the cluster (which will result in it declaring itself master of a new cluster
+with the same name - highly undesirable). Adding tag on elasticsearch GCE nodes and then filtering by that
+tag will resolve this issue.
+
+Add your tag when building the new instance:
+
+[source,sh]
+--------------------------------------------------
+gcloud compute instances create myesnode1 --project=es-cloud \
+       --scopes=compute-rw \
+       --tags=elasticsearch,dev
+--------------------------------------------------
+
+Then, define it in `elasticsearch.yml`:
+
+[source,yaml]
+--------------------------------------------------
+cloud:
+  gce:
+      project_id: es-cloud
+      zone: europe-west1-a
+discovery:
+      type: gce
+      gce:
+            tags: elasticsearch, dev
+--------------------------------------------------
+
+[[discovery-gce-usage-port]]
+==== Changing default transport port
+
+By default, elasticsearch GCE plugin assumes that you run elasticsearch on 9300 default port.
+But you can specify the port value elasticsearch is meant to use using google compute engine metadata `es_port`:
+
+[[discovery-gce-usage-port-create]]
+===== When creating instance
+
+Add `--metadata es_port=9301` option:
+
+[source,sh]
+--------------------------------------------------
+# when creating first instance
+gcloud compute instances create myesnode1 \
+       --scopes=compute-rw,storage-full \
+       --metadata es_port=9301
+
+# when creating an instance from an image
+gcloud compute instances create myesnode2 --image=elasticsearch-1-0-0-RC1 \
+       --zone europe-west1-a --machine-type f1-micro --scopes=compute-rw \
+       --metadata es_port=9301
+--------------------------------------------------
+
+[[discovery-gce-usage-port-run]]
+===== On a running instance
+
+[source,sh]
+--------------------------------------------------
+gcloud compute instances add-metadata myesnode1 \
+       --zone europe-west1-a \
+       --metadata es_port=9301
+--------------------------------------------------
+
+
+[[discovery-gce-usage-tips]]
+==== GCE Tips
+
+[[discovery-gce-usage-tips-projectid]]
+===== Store project id locally
+
+If you don't want to repeat the project id each time, you can save it in the local gcloud config
+
+[source,sh]
+--------------------------------------------------
+gcloud config set project es-cloud
+--------------------------------------------------
+
+[[discovery-gce-usage-tips-permissions]]
+===== Machine Permissions
+
+If you have created a machine without the correct permissions, you will see `403 unauthorized` error messages. The only
+way to alter these permissions is to delete the instance (NOT THE DISK). Then create another with the correct permissions.
+
+Creating machines with gcloud::
++
+--
+Ensure the following flags are set:
+
+[source,text]
+--------------------------------------------------
+--scopes=compute-rw
+--------------------------------------------------
+--
+
+Creating with console (web)::
++
+--
+When creating an instance using the web portal, click _Show advanced options_.
+
+At the bottom of the page, under `PROJECT ACCESS`, choose `>> Compute >> Read Write`.
+--
+
+Creating with knife google::
++
+--
+Set the service account scopes when creating the machine:
+
+[source,sh]
+--------------------------------------------------
+knife google server create www1 \
+    -m n1-standard-1 \
+    -I debian-8 \
+    -Z us-central1-a \
+    -i ~/.ssh/id_rsa \
+    -x jdoe \
+    --gce-service-account-scopes https://www.googleapis.com/auth/compute.full_control
+--------------------------------------------------
+
+Or, you may use the alias:
+
+[source,sh]
+--------------------------------------------------
+    --gce-service-account-scopes compute-rw
+--------------------------------------------------
+--
+
+[[discovery-gce-usage-testing]]
+==== Testing GCE
+
+Integrations tests in this plugin require working GCE configuration and
+therefore disabled by default. To enable tests prepare a config file
+elasticsearch.yml with the following content:
+
+[source,yaml]
+--------------------------------------------------
+cloud:
+  gce:
+      project_id: es-cloud
+      zone: europe-west1-a
+discovery:
+      type: gce
+--------------------------------------------------
+
+Replaces `project_id` and `zone` with your settings.
+
+To run test:
+
+[source,sh]
+--------------------------------------------------
+mvn -Dtests.gce=true -Dtests.config=/path/to/config/file/elasticsearch.yml clean test
+--------------------------------------------------
diff --git a/docs/plugins/discovery.asciidoc b/docs/plugins/discovery.asciidoc
index 289a020..1fab942 100644
--- a/docs/plugins/discovery.asciidoc
+++ b/docs/plugins/discovery.asciidoc
@@ -17,9 +17,9 @@ The EC2 discovery plugin uses the https://github.com/aws/aws-sdk-java[AWS API] f
 
 The Azure discovery plugin uses the Azure API for unicast discovery.
 
-<<cloud-gce,GCE Cloud>>::
+<<discovery-gce,GCE discovery>>::
 
-The Google Compute Engine Cloud plugin uses the GCE API for unicast discovery.
+The Google Compute Engine discovery plugin uses the GCE API for unicast discovery.
 
 <<discovery-multicast,Multicast>>::
 
@@ -38,7 +38,7 @@ include::discovery-ec2.asciidoc[]
 
 include::discovery-azure.asciidoc[]
 
-include::cloud-gce.asciidoc[]
+include::discovery-gce.asciidoc[]
 
 include::discovery-multicast.asciidoc[]
 
diff --git a/docs/plugins/plugin-script.asciidoc b/docs/plugins/plugin-script.asciidoc
index bae6613..52ff574 100644
--- a/docs/plugins/plugin-script.asciidoc
+++ b/docs/plugins/plugin-script.asciidoc
@@ -131,6 +131,8 @@ Plugins can be removed manually, by deleting the appropriate directory under
 sudo bin/plugin remove [pluginname]
 -----------------------------------
 
+After a Java plugin has been removed, you will need to restart the node to complete the removal process.
+
 === Other command line parameters
 
 The `plugin` scripts supports a number of other command line parameters:
diff --git a/docs/reference/aggregations/bucket/datehistogram-aggregation.asciidoc b/docs/reference/aggregations/bucket/datehistogram-aggregation.asciidoc
index cdd7601..5afff0c 100644
--- a/docs/reference/aggregations/bucket/datehistogram-aggregation.asciidoc
+++ b/docs/reference/aggregations/bucket/datehistogram-aggregation.asciidoc
@@ -281,7 +281,7 @@ had a value.
 {
     "aggs" : {
         "publish_date" : {
-             "datehistogram" : {
+             "date_histogram" : {
                  "field" : "publish_date",
                  "interval": "year",
                  "missing": "2000-01-01" <1>
diff --git a/docs/reference/aggregations/pipeline.asciidoc b/docs/reference/aggregations/pipeline.asciidoc
index 4410db3..e4cdae5 100644
--- a/docs/reference/aggregations/pipeline.asciidoc
+++ b/docs/reference/aggregations/pipeline.asciidoc
@@ -2,8 +2,6 @@
 
 == Pipeline Aggregations
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 Pipeline aggregations work on the outputs produced from other aggregations rather than from document sets, adding
diff --git a/docs/reference/aggregations/pipeline/avg-bucket-aggregation.asciidoc b/docs/reference/aggregations/pipeline/avg-bucket-aggregation.asciidoc
index b2b9d93f..541ffec 100644
--- a/docs/reference/aggregations/pipeline/avg-bucket-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/avg-bucket-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-avg-bucket-aggregation]]
 === Avg Bucket Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 A sibling pipeline aggregation which calculates the (mean) average value of a specified metric in a sibling aggregation. 
diff --git a/docs/reference/aggregations/pipeline/bucket-script-aggregation.asciidoc b/docs/reference/aggregations/pipeline/bucket-script-aggregation.asciidoc
index 81372c1..b1bbfcd 100644
--- a/docs/reference/aggregations/pipeline/bucket-script-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/bucket-script-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-bucket-script-aggregation]]
 === Bucket Script Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 A parent pipeline aggregation which executes a script which can perform per bucket computations on specified metrics 
diff --git a/docs/reference/aggregations/pipeline/bucket-selector-aggregation.asciidoc b/docs/reference/aggregations/pipeline/bucket-selector-aggregation.asciidoc
index cef1e67..2d80abc 100644
--- a/docs/reference/aggregations/pipeline/bucket-selector-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/bucket-selector-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-bucket-selector-aggregation]]
 === Bucket Selector Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 A parent pipeline aggregation which executes a script which determines whether the current bucket will be retained 
diff --git a/docs/reference/aggregations/pipeline/cumulative-sum-aggregation.asciidoc b/docs/reference/aggregations/pipeline/cumulative-sum-aggregation.asciidoc
index 823c5c8..e29dbbe 100644
--- a/docs/reference/aggregations/pipeline/cumulative-sum-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/cumulative-sum-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-cumulative-sum-aggregation]]
 === Cumulative Sum Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 A parent pipeline aggregation which calculates the cumulative sum of a specified metric in a parent histogram (or date_histogram) 
diff --git a/docs/reference/aggregations/pipeline/derivative-aggregation.asciidoc b/docs/reference/aggregations/pipeline/derivative-aggregation.asciidoc
index 48296ca..f68a811 100644
--- a/docs/reference/aggregations/pipeline/derivative-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/derivative-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-derivative-aggregation]]
 === Derivative Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 A parent pipeline aggregation which calculates the derivative of a specified metric in a parent histogram (or date_histogram) 
diff --git a/docs/reference/aggregations/pipeline/extended-stats-bucket-aggregation.asciidoc b/docs/reference/aggregations/pipeline/extended-stats-bucket-aggregation.asciidoc
index bbf610a..0a44685 100644
--- a/docs/reference/aggregations/pipeline/extended-stats-bucket-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/extended-stats-bucket-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-extended-stats-bucket-aggregation]]
 === Extended Stats Bucket Aggregation
 
-coming[2.1.0]
-
 experimental[]
 
 A sibling pipeline aggregation which calculates a variety of stats across all bucket of a specified metric in a sibling aggregation.
diff --git a/docs/reference/aggregations/pipeline/max-bucket-aggregation.asciidoc b/docs/reference/aggregations/pipeline/max-bucket-aggregation.asciidoc
index 310a643..96094d0 100644
--- a/docs/reference/aggregations/pipeline/max-bucket-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/max-bucket-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-max-bucket-aggregation]]
 === Max Bucket Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 A sibling pipeline aggregation which identifies the bucket(s) with the maximum value of a specified metric in a sibling aggregation
diff --git a/docs/reference/aggregations/pipeline/min-bucket-aggregation.asciidoc b/docs/reference/aggregations/pipeline/min-bucket-aggregation.asciidoc
index 11d3d55..c970384 100644
--- a/docs/reference/aggregations/pipeline/min-bucket-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/min-bucket-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-min-bucket-aggregation]]
 === Min Bucket Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 A sibling pipeline aggregation which identifies the bucket(s) with the minimum value of a specified metric in a sibling aggregation 
diff --git a/docs/reference/aggregations/pipeline/movavg-aggregation.asciidoc b/docs/reference/aggregations/pipeline/movavg-aggregation.asciidoc
index 6fe91cb..968c596 100644
--- a/docs/reference/aggregations/pipeline/movavg-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/movavg-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-movavg-aggregation]]
 === Moving Average Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 Given an ordered series of data, the Moving Average aggregation will slide a window across the data and emit the average
diff --git a/docs/reference/aggregations/pipeline/percentiles-bucket-aggregation.asciidoc b/docs/reference/aggregations/pipeline/percentiles-bucket-aggregation.asciidoc
index 2476969..4e6423f 100644
--- a/docs/reference/aggregations/pipeline/percentiles-bucket-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/percentiles-bucket-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-percentiles-bucket-aggregation]]
 === Percentiles Bucket Aggregation
 
-coming[2.1.0]
-
 experimental[]
 
 A sibling pipeline aggregation which calculates percentiles across all bucket of a specified metric in a sibling aggregation.
diff --git a/docs/reference/aggregations/pipeline/serial-diff-aggregation.asciidoc b/docs/reference/aggregations/pipeline/serial-diff-aggregation.asciidoc
index 7193510..17cfea9 100644
--- a/docs/reference/aggregations/pipeline/serial-diff-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/serial-diff-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-serialdiff-aggregation]]
 === Serial Differencing Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 Serial differencing is a technique where values in a time series are subtracted from itself at
diff --git a/docs/reference/aggregations/pipeline/stats-bucket-aggregation.asciidoc b/docs/reference/aggregations/pipeline/stats-bucket-aggregation.asciidoc
index 7d6d24d..f524032 100644
--- a/docs/reference/aggregations/pipeline/stats-bucket-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/stats-bucket-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-stats-bucket-aggregation]]
 === Stats Bucket Aggregation
 
-coming[2.1.0]
-
 experimental[]
 
 A sibling pipeline aggregation which calculates a variety of stats across all bucket of a specified metric in a sibling aggregation.
diff --git a/docs/reference/aggregations/pipeline/sum-bucket-aggregation.asciidoc b/docs/reference/aggregations/pipeline/sum-bucket-aggregation.asciidoc
index 56d786f..52022b3 100644
--- a/docs/reference/aggregations/pipeline/sum-bucket-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/sum-bucket-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-sum-bucket-aggregation]]
 === Sum Bucket Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 A sibling pipeline aggregation which calculates the sum across all bucket of a specified metric in a sibling aggregation. 
diff --git a/docs/reference/docs/bulk.asciidoc b/docs/reference/docs/bulk.asciidoc
index b9b7d47..ef066eb 100644
--- a/docs/reference/docs/bulk.asciidoc
+++ b/docs/reference/docs/bulk.asciidoc
@@ -131,8 +131,6 @@ operation based on the `_parent` / `_routing` mapping.
 [[bulk-timestamp]]
 === Timestamp
 
-deprecated[2.0.0,The `_timestamp` field is deprecated.  Instead, use a normal <<date,`date`>> field and set its value explicitly]
-
 Each bulk item can include the timestamp value using the
 `_timestamp`/`timestamp` field. It automatically follows the behavior of
 the index operation based on the `_timestamp` mapping.
@@ -141,8 +139,6 @@ the index operation based on the `_timestamp` mapping.
 [[bulk-ttl]]
 === TTL
 
-deprecated[2.0.0,The current `_ttl` implementation is deprecated and will be replaced with a different implementation in a future version]
-
 Each bulk item can include the ttl value using the `_ttl`/`ttl` field.
 It automatically follows the behavior of the index operation based on
 the `_ttl` mapping.
diff --git a/docs/reference/docs/index_.asciidoc b/docs/reference/docs/index_.asciidoc
index ab542c5..089af3e 100644
--- a/docs/reference/docs/index_.asciidoc
+++ b/docs/reference/docs/index_.asciidoc
@@ -257,8 +257,6 @@ specified using the `routing` parameter.
 [[index-timestamp]]
 === Timestamp
 
-deprecated[2.0.0,The `_timestamp` field is deprecated.  Instead, use a normal <<date,`date`>> field and set its value explicitly]
-
 A document can be indexed with a `timestamp` associated with it. The
 `timestamp` value of a document can be set using the `timestamp`
 parameter. For example:
@@ -281,8 +279,6 @@ page>>.
 [[index-ttl]]
 === TTL
 
-deprecated[2.0.0,The current `_ttl` implementation is deprecated and will be replaced with a different implementation in a future version]
-
 
 A document can be indexed with a `ttl` (time to live) associated with
 it. Expired documents will be expunged automatically. The expiration
diff --git a/docs/reference/docs/termvectors.asciidoc b/docs/reference/docs/termvectors.asciidoc
index 7530ff7..0e10843 100644
--- a/docs/reference/docs/termvectors.asciidoc
+++ b/docs/reference/docs/termvectors.asciidoc
@@ -81,8 +81,6 @@ omit :
 [float]
 ==== Distributed frequencies
 
-coming[2.0.0-beta1]
-
 Setting `dfs` to `true` (default is `false`) will return the term statistics
 or the field statistics of the entire index, and not just at the shard. Use it
 with caution as distributed frequencies can have a serious performance impact.
@@ -90,8 +88,6 @@ with caution as distributed frequencies can have a serious performance impact.
 [float]
 ==== Terms Filtering
 
-coming[2.0.0-beta1]
-
 With the parameter `filter`, the terms returned could also be filtered based
 on their tf-idf scores. This could be useful in order find out a good
 characteristic vector of a document. This feature works in a similar manner to
diff --git a/docs/reference/getting-started.asciidoc b/docs/reference/getting-started.asciidoc
index 2731fca..a30046e 100755
--- a/docs/reference/getting-started.asciidoc
+++ b/docs/reference/getting-started.asciidoc
@@ -862,7 +862,7 @@ In the previous section, we skipped over a little detail called the document sco
 
 But queries do not always need to produce scores, in particular when they are only used for "filtering" the document set. Elasticsearch detects these situations and automatically optimizes query execution in order not to compute useless scores.
 
-The <<query-dsl-range-query,`range` query>> that we introduced in the previous section also supports `filter` clauses which allow to use a query to restrict the documents that will be matched by other clauses, without changing how scores are computed. As an example, let's introduce the <<query-dsl-range-query,`range` query>>, which allows us to filter documents by a range of values. This is generally used for numeric or date filtering.
+The <<query-dsl-bool-query,`bool` query>> that we introduced in the previous section also supports `filter` clauses which allow to use a query to restrict the documents that will be matched by other clauses, without changing how scores are computed. As an example, let's introduce the <<query-dsl-range-query,`range` query>>, which allows us to filter documents by a range of values. This is generally used for numeric or date filtering.
 
 This example uses a bool query to return all accounts with balances between 20000 and 30000, inclusive. In other words, we want to find accounts with a balance that is greater than or equal to 20000 and less than or equal to 30000.
 
diff --git a/docs/reference/index.asciidoc b/docs/reference/index.asciidoc
index e36a66a..8e34747 100644
--- a/docs/reference/index.asciidoc
+++ b/docs/reference/index.asciidoc
@@ -1,11 +1,12 @@
 [[elasticsearch-reference]]
 = Elasticsearch Reference
 
-:version:   2.0.0-beta1
-:branch:    2.0
-:jdk:       1.8.0_25
-:defguide:  https://www.elastic.co/guide/en/elasticsearch/guide/current
-:plugins:   https://www.elastic.co/guide/en/elasticsearch/plugins/master
+:version:       3.0.0-beta1
+:major-version: 3.x
+:branch:        3.0
+:jdk:           1.8.0_25
+:defguide:      https://www.elastic.co/guide/en/elasticsearch/guide/current
+:plugins:       https://www.elastic.co/guide/en/elasticsearch/plugins/master
 
 include::getting-started.asciidoc[]
 
diff --git a/docs/reference/indices/analyze.asciidoc b/docs/reference/indices/analyze.asciidoc
index d17f409..1a256a6 100644
--- a/docs/reference/indices/analyze.asciidoc
+++ b/docs/reference/indices/analyze.asciidoc
@@ -16,8 +16,6 @@ curl -XGET 'localhost:9200/_analyze' -d '
 }'
 --------------------------------------------------
 
-coming[2.0.0-beta1, body based parameters were added in 2.0.0]
-
 If text parameter is provided as array of strings, it is analyzed as a multi-valued field.
 
 [source,js]
@@ -29,8 +27,6 @@ curl -XGET 'localhost:9200/_analyze' -d '
 }'
 --------------------------------------------------
 
-coming[2.0.0-beta1, body based parameters were added in 2.0.0]
-
 Or by building a custom transient analyzer out of tokenizers,
 token filters and char filters. Token filters can use the shorter 'filters'
 parameter name:
@@ -53,8 +49,6 @@ curl -XGET 'localhost:9200/_analyze' -d '
 }'
 --------------------------------------------------
 
-coming[2.0.0-beta1, body based parameters were added in 2.0.0]
-
 It can also run against a specific index:
 
 [source,js]
@@ -78,8 +72,6 @@ curl -XGET 'localhost:9200/test/_analyze' -d '
 }'
 --------------------------------------------------
 
-coming[2.0.0-beta1, body based parameters were added in 2.0.0]
-
 Also, the analyzer can be derived based on a field mapping, for example:
 
 [source,js]
@@ -91,8 +83,6 @@ curl -XGET 'localhost:9200/test/_analyze' -d '
 }'
 --------------------------------------------------
 
-coming[2.0.0-beta1, body based parameters were added in 2.0.0]
-
 Will cause the analysis to happen based on the analyzer configured in the
 mapping for `obj1.field1` (and if not, the default index analyzer).
 
diff --git a/docs/reference/mapping/fields/parent-field.asciidoc b/docs/reference/mapping/fields/parent-field.asciidoc
index 22e46c4..64f4a99 100644
--- a/docs/reference/mapping/fields/parent-field.asciidoc
+++ b/docs/reference/mapping/fields/parent-field.asciidoc
@@ -1,8 +1,6 @@
 [[mapping-parent-field]]
 === `_parent` field
 
-added[2.0.0-beta1,The parent-child implementation has been completely rewritten. It is advisable to reindex any 1.x indices which use parent-child to take advantage of the new optimizations]
-
 A parent-child relationship can be established between documents in the same
 index by making one mapping type the parent of another:
 
diff --git a/docs/reference/mapping/fields/timestamp-field.asciidoc b/docs/reference/mapping/fields/timestamp-field.asciidoc
index 3f4bf8a..5971a02 100644
--- a/docs/reference/mapping/fields/timestamp-field.asciidoc
+++ b/docs/reference/mapping/fields/timestamp-field.asciidoc
@@ -1,8 +1,6 @@
 [[mapping-timestamp-field]]
 === `_timestamp` field
 
-deprecated[2.0.0,The `_timestamp` field is deprecated.  Instead, use a normal <<date,`date`>> field and set its value explicitly]
-
 The `_timestamp` field, when enabled, allows a timestamp to be indexed and
 stored with a document. The timestamp may be specified manually, generated
 automatically, or set to a default value:
diff --git a/docs/reference/mapping/fields/ttl-field.asciidoc b/docs/reference/mapping/fields/ttl-field.asciidoc
index 5394d28..07ce8a8 100644
--- a/docs/reference/mapping/fields/ttl-field.asciidoc
+++ b/docs/reference/mapping/fields/ttl-field.asciidoc
@@ -1,8 +1,6 @@
 [[mapping-ttl-field]]
 === `_ttl` field
 
-deprecated[2.0.0,The current `_ttl` implementation is deprecated and will be replaced with a different implementation in a future version]
-
 Some types of documents, such as session data or special offers, come with an
 expiration date. The `_ttl` field allows you to specify the minimum time a
 document should live, after which time the document is deleted automatically.
diff --git a/docs/reference/migration/migrate_2_0/java.asciidoc b/docs/reference/migration/migrate_2_0/java.asciidoc
index 9871df4..65bfaef 100644
--- a/docs/reference/migration/migrate_2_0/java.asciidoc
+++ b/docs/reference/migration/migrate_2_0/java.asciidoc
@@ -32,6 +32,13 @@ Settings settings = Settings.settingsBuilder()
 Client client = TransportClient.builder().settings(settings).build();
 --------------------------------------------------
 
+==== Exception are only thrown on total failure
+
+Previously, many APIs would throw an exception if any shard failed to execute
+the request. Now the exception is only thrown if all shards fail the request.
+The responses for these APIs will always have a `getShardFailures` method that
+you can and should check for failures.
+
 ==== Automatically thread client listeners
 
 Previously, the user had to set request listener threads to `true` when on the
@@ -109,7 +116,7 @@ new InetSocketTransportAddress(new InetSocketAddress("127.0.0.1", 0));
 Elasticsearch used to shade its dependencies and to relocate packages. We no longer use shading or relocation.
 You might need to change your imports to the original package names:
 
-* `com.google.common` was `org.elasticsearch.common` 
+* `com.google.common` was `org.elasticsearch.common`
 * `com.carrotsearch.hppc` was `org.elasticsearch.common.hppc`
 * `jsr166e` was `org.elasticsearch.common.util.concurrent.jsr166e`
 * `com.fasterxml.jackson` was `org.elasticsearch.common.jackson`
@@ -121,4 +128,3 @@ You might need to change your imports to the original package names:
 * `com.tdunning.math.stats` was `org.elasticsearch.common.stats`
 * `org.apache.commons.lang` was `org.elasticsearch.common.lang`
 * `org.apache.commons.cli` was `org.elasticsearch.common.cli.commons`
- 
diff --git a/docs/reference/migration/migrate_2_0/settings.asciidoc b/docs/reference/migration/migrate_2_0/settings.asciidoc
index 17d36c5..0be16cb 100644
--- a/docs/reference/migration/migrate_2_0/settings.asciidoc
+++ b/docs/reference/migration/migrate_2_0/settings.asciidoc
@@ -164,3 +164,13 @@ the `logging.yml` configuration file with the `file.layout.conversionPattern`
 setting.
 
 Remove mapping.date.round_ceil setting for date math parsing #8889 (issues: #8556, #8598)
+
+==== Custom config file
+
+It is no longer possible to specify a custom config file with the `CONF_FILE`
+environment variable, or the `-Des.config`, `-Des.default.config`, or
+`-Delasticsearch.config` parameters.
+
+Instead, the config file must be named `elasticsearch.yml` and must be located
+in the default `config/` directory, or in the directory specified in the
+`CONF_DIR` environment variable.
diff --git a/docs/reference/migration/migrate_3_0.asciidoc b/docs/reference/migration/migrate_3_0.asciidoc
index 897098f..c1ba60c 100644
--- a/docs/reference/migration/migrate_3_0.asciidoc
+++ b/docs/reference/migration/migrate_3_0.asciidoc
@@ -121,6 +121,25 @@ function that it supports and it's able to parse. The function object can then t
 function through the new `toFunction(QueryShardContext)` method, which returns a lucene function to be executed
 on the data node.
 
+==== Cloud AWS plugin
+
+Cloud AWS plugin has been split in two plugins:
+
+* {plugins}/discovery-ec2.html[Discovery EC2 plugin]
+* {plugins}/repository-s3.html[Repository S3 plugin]
+
+==== Cloud Azure plugin
+
+Cloud Azure plugin has been split in three plugins:
+
+* {plugins}/discovery-azure.html[Discovery Azure plugin]
+* {plugins}/repository-azure.html[Repository Azure plugin]
+* {plugins}/store-smb.html[Store SMB plugin]
+
+==== Cloud GCE plugin
+
+Cloud GCE plugin has been renamed to {plugins}/discovery-gce.html[Discovery GCE plugin].
+
 === Java-API
 
 ==== BoostingQueryBuilder
@@ -266,3 +285,8 @@ of string values: see `FilterFunctionScoreQuery.ScoreMode` and `CombineFunction`
 
 For simplicity, only one way of adding the ids to the existing list (empty by default)  is left: `addIds(String...)`
 
+==== DocumentAlreadyExistsException removed
+
+`DocumentAlreadyExistsException` is removed and a `VersionConflictException` is thrown instead (with a better
+error description). This will influence code that use the `IndexRequest.opType()` or `IndexRequest.create()`
+to index a document only if it doesn't already exist.
diff --git a/docs/reference/modules/discovery/gce.asciidoc b/docs/reference/modules/discovery/gce.asciidoc
index bb9c89f..ea367d5 100644
--- a/docs/reference/modules/discovery/gce.asciidoc
+++ b/docs/reference/modules/discovery/gce.asciidoc
@@ -2,5 +2,5 @@
 === Google Compute Engine Discovery
 
 Google Compute Engine (GCE) discovery allows to use the GCE APIs to perform automatic discovery (similar to multicast).
-Please check the https://github.com/elasticsearch/elasticsearch-cloud-gce[plugin website]
-to find the full documentation.
+It is available as a plugin. See {plugins}/discovery-gce.html[discovery-gce] for more information.
+
diff --git a/docs/reference/modules/network.asciidoc b/docs/reference/modules/network.asciidoc
index 70b4d8c..7105d2d 100644
--- a/docs/reference/modules/network.asciidoc
+++ b/docs/reference/modules/network.asciidoc
@@ -54,6 +54,10 @@ provided network interface. For example `_en0:ipv6_`.
 When the `discovery-ec2` plugin is installed, you can use
 {plugins}/discovery-ec2-discovery.html#discovery-ec2-network-host[ec2 specific host settings].
 
+When the `discovery-gce` plugin is installed, you can use
+{plugins}/discovery-gce-network-host.html[gce specific host settings].
+
+
 [float]
 [[tcp-settings]]
 === TCP Settings
diff --git a/docs/reference/modules/scripting.asciidoc b/docs/reference/modules/scripting.asciidoc
index 7729ce2..50be5fd 100644
--- a/docs/reference/modules/scripting.asciidoc
+++ b/docs/reference/modules/scripting.asciidoc
@@ -351,28 +351,86 @@ to `false`.
 [float]
 === Native (Java) Scripts
 
-Even though `groovy` is pretty fast, this allows to register native Java based
-scripts for faster execution.
+Sometimes `groovy` and `expressions` aren't enough. For those times you can
+implement a native script.
 
-In order to allow for scripts, the `NativeScriptFactory` needs to be
-implemented that constructs the script that will be executed. There are
-two main types, one that extends `AbstractExecutableScript` and one that
-extends `AbstractSearchScript` (probably the one most users will extend,
-with additional helper classes in `AbstractLongSearchScript`,
-`AbstractDoubleSearchScript`, and `AbstractFloatSearchScript`).
+The best way to implement a native script is to write a plugin and install it.
+The plugin {plugins}/plugin-authors.html[documentation] has more information on
+how to write a plugin so that Elasticsearch will properly load it.
 
-Registering them can either be done by settings, for example:
-`script.native.my.type` set to `sample.MyNativeScriptFactory` will
-register a script named `my`. Another option is in a plugin, access
-`ScriptModule` and call `registerScript` on it.
+To register the actual script you'll need to implement `NativeScriptFactory`
+to construct the script. The actual script will extend either
+`AbstractExecutableScript` or `AbstractSearchScript`. The second one is likely
+the most useful and has several helpful subclasses you can extend like
+`AbstractLongSearchScript`, `AbstractDoubleSearchScript`, and
+`AbstractFloatSearchScript`. Finally, your plugin should register the native
+script by declaring the `onModule(ScriptModule)` method.
 
-Executing the script is done by specifying the `lang` as `native`, and
-the name of the script as the `script`.
+If you squashed the whole thing into one class it'd look like:
+
+[source,java]
+--------------------------------------------------
+public class MyNativeScriptPlugin extends Plugin {
+    @Override
+    public String name() {
+        return "my-native-script";
+    }
+    @Override
+    public String description() {
+        return "my native script that does something great";
+    }
+    public void onModule(ScriptModule scriptModule) {
+        scriptModule.registerScript("my_script", MyNativeScriptFactory.class);
+    }
+
+    public static class MyNativeScriptFactory implements NativeScriptFactory {
+        @Override
+        public ExecutableScript newScript(@Nullable Map<String, Object> params) {
+            return new MyNativeScript();
+        }
+        @Override
+        public boolean needsScores() {
+            return false;
+        }
+    }
+
+    public static class MyNativeScript extends AbstractFloatSearchScript {
+        @Override
+        public float runAsFloat() {
+            float a = (float) source().get("a");
+            float b = (float) source().get("b");
+            return a * b;
+        }
+    }
+}
+--------------------------------------------------
+
+You can execute the script by specifying its `lang` as `native`, and the name
+of the script as the `id`:
+
+[source,js]
+--------------------------------------------------
+curl -XPOST localhost:9200/_search -d '{
+  "query": {
+    "function_score": {
+      "query": {
+        "match": {
+          "body": "foo"
+        }
+      },
+      "functions": [
+        {
+          "script_score": {
+            "id": "my_script",
+            "lang" : "native"
+          }
+        }
+      ]
+    }
+  }
+}'
+--------------------------------------------------
 
-Note, the scripts need to be in the classpath of elasticsearch. One
-simple way to do it is to create a directory under plugins (choose a
-descriptive name), and place the jar / classes files there. They will be
-automatically loaded.
 
 [float]
 === Lucene Expressions Scripts
@@ -624,16 +682,3 @@ power of the second argument.
 |`hypot(x, y)` |Returns sqrt(_x2_ + _y2_) without intermediate overflow
 or underflow.
 |=======================================================================
-
-[float]
-=== Arithmetic precision in MVEL
-
-When dividing two numbers using MVEL based scripts, the engine tries to
-be smart and adheres to the default behaviour of java. This means if you
-divide two integers (you might have configured the fields as integer in
-the mapping), the result will also be an integer. This means, if a
-calculation like `1/num` is happening in your scripts and `num` is an
-integer with the value of `8`, the result is `0` even though you were
-expecting it to be `0.125`. You may need to enforce precision by
-explicitly using a double like `1.0/num` in order to get the expected
-result.
diff --git a/docs/reference/modules/snapshots.asciidoc b/docs/reference/modules/snapshots.asciidoc
index 4830e43..50ee4df 100644
--- a/docs/reference/modules/snapshots.asciidoc
+++ b/docs/reference/modules/snapshots.asciidoc
@@ -121,7 +121,7 @@ The following settings are supported:
  using size value notation, i.e. 1g, 10m, 5k. Defaults to `null` (unlimited chunk size).
 `max_restore_bytes_per_sec`:: Throttles per node restore rate. Defaults to `40mb` per second.
 `max_snapshot_bytes_per_sec`:: Throttles per node snapshot rate. Defaults to `40mb` per second.
-`readonly`:: Makes repository read-only. coming[2.1.0]  Defaults to `false`.
+`readonly`:: Makes repository read-only.  Defaults to `false`.
 
 [float]
 ===== Read-only URL Repository
@@ -259,7 +259,7 @@ GET /_snapshot/my_backup/_all
 -----------------------------------
 // AUTOSENSE
 
-coming[2.0.0-beta1] A currently running snapshot can be retrieved using the following command:
+A currently running snapshot can be retrieved using the following command:
 
 [source,sh]
 -----------------------------------
diff --git a/docs/reference/query-dsl/mlt-query.asciidoc b/docs/reference/query-dsl/mlt-query.asciidoc
index 9c42881..ee4b695 100644
--- a/docs/reference/query-dsl/mlt-query.asciidoc
+++ b/docs/reference/query-dsl/mlt-query.asciidoc
@@ -149,7 +149,7 @@ input, the other one for term selection and for query formation.
 ==== Document Input Parameters
 
 [horizontal]
-`like`:: coming[2.0.0-beta1]
+`like`::
 The only *required* parameter of the MLT query is `like` and follows a
 versatile syntax, in which the user can specify free form text and/or a single
 or multiple documents (see examples above). The syntax to specify documents is
@@ -162,7 +162,7 @@ follows a similar syntax to the `per_field_analyzer` parameter of the
 Additionally, to provide documents not necessarily present in the index,
 <<docs-termvectors-artificial-doc,artificial documents>> are also supported.
 
-`unlike`:: coming[2.0.0-beta1] 
+`unlike`:: 
 The `unlike` parameter is used in conjunction with `like` in order not to
 select terms found in a chosen set of documents. In other words, we could ask
 for documents `like: "Apple"`, but `unlike: "cake crumble tree"`. The syntax
@@ -172,10 +172,10 @@ is the same as `like`.
 A list of fields to fetch and analyze the text from. Defaults to the `_all`
 field for free text and to all possible fields for document inputs.
 
-`like_text`:: deprecated[2.0.0-beta1,Replaced by `like`]
+`like_text`::
 The text to find documents like it.
 
-`ids` or `docs`:: deprecated[2.0.0-beta1,Replaced by `like`]
+`ids` or `docs`::
 A list of documents following the same syntax as the <<docs-multi-get,Multi GET API>>.
 
 [float]
diff --git a/docs/reference/search/request/scroll.asciidoc b/docs/reference/search/request/scroll.asciidoc
index 2921441..825564d 100644
--- a/docs/reference/search/request/scroll.asciidoc
+++ b/docs/reference/search/request/scroll.asciidoc
@@ -63,8 +63,6 @@ curl -XGET <1> 'localhost:9200/_search/scroll' <2> -d'
 '
 --------------------------------------------------
 
-coming[2.0.0-beta1, body based parameters were added in 2.0.0]
-
 <1> `GET` or `POST` can be used.
 <2> The URL should not include the `index` or `type` name -- these
     are specified in the original `search` request instead.
@@ -151,8 +149,6 @@ curl -XDELETE localhost:9200/_search/scroll -d '
 }'
 ---------------------------------------
 
-coming[2.0.0-beta1, Body based parameters were added in 2.0.0]
-
 Multiple scroll IDs can be passed as array:
 
 [source,js]
@@ -163,8 +159,6 @@ curl -XDELETE localhost:9200/_search/scroll -d '
 }'
 ---------------------------------------
 
-coming[2.0.0-beta1, Body based parameters were added in 2.0.0]
-
 All search contexts can be cleared with the `_all` parameter:
 
 [source,js]
diff --git a/docs/reference/setup/as-a-service.asciidoc b/docs/reference/setup/as-a-service.asciidoc
index 01bbd2d..50454ca 100644
--- a/docs/reference/setup/as-a-service.asciidoc
+++ b/docs/reference/setup/as-a-service.asciidoc
@@ -22,7 +22,6 @@ Each package features a configuration file, which allows you to set the followin
 `LOG_DIR`::               Log directory, defaults to `/var/log/elasticsearch`
 `DATA_DIR`::              Data directory, defaults to `/var/lib/elasticsearch`
 `CONF_DIR`::              Configuration file directory (which needs to include `elasticsearch.yml` and `logging.yml` files), defaults to `/etc/elasticsearch`
-`CONF_FILE`::             Path to configuration file, defaults to `/etc/elasticsearch/elasticsearch.yml`
 `ES_JAVA_OPTS`::          Any additional java options you may want to apply. This may be useful, if you need to set the `node.name` property, but do not want to change the `elasticsearch.yml` configuration file, because it is distributed via a provisioning system like puppet or chef. Example: `ES_JAVA_OPTS="-Des.node.name=search-01"`
 `RESTART_ON_UPGRADE`::    Configure restart on package upgrade, defaults to `false`. This means you will have to restart your elasticsearch instance after installing a package manually. The reason for this is to ensure, that upgrades in a cluster do not result in a continuous shard reallocation resulting in high network traffic and reducing the response times of your cluster.
 `ES_GC_LOG_FILE` ::       The absolute log file path for creating a garbage collection logfile, which is done by the JVM. Note that this logfile can grow pretty quick and thus is disabled by default.
@@ -72,9 +71,9 @@ sudo service elasticsearch start
 
 
 [float]
-===== Using systemd
+==== Using systemd
 
-Distributions like SUSE do not use the `chkconfig` tool to register services, but rather `systemd` and its command `/bin/systemctl` to start and stop services (at least in newer versions, otherwise use the `chkconfig` commands above). The configuration file is also placed at `/etc/sysconfig/elasticsearch`. After installing the RPM, you have to change the systemd configuration and then start up elasticsearch
+Distributions like SUSE do not use the `chkconfig` tool to register services, but rather `systemd` and its command `/bin/systemctl` to start and stop services (at least in newer versions, otherwise use the `chkconfig` commands above). The configuration file is also placed at `/etc/sysconfig/elasticsearch` if the system is rpm based and `/etc/default/elasticsearch` if it is deb. After installing the RPM, you have to change the systemd configuration and then start up elasticsearch
 
 [source,sh]
 --------------------------------------------------
diff --git a/docs/reference/setup/configuration.asciidoc b/docs/reference/setup/configuration.asciidoc
index 45c384b..0768891 100644
--- a/docs/reference/setup/configuration.asciidoc
+++ b/docs/reference/setup/configuration.asciidoc
@@ -298,14 +298,6 @@ Enter value for [node.name]:
 NOTE: Elasticsearch will not start if `${prompt.text}` or `${prompt.secret}`
 is used in the settings and the process is run as a service or in the background.
 
-The location of the configuration file can be set externally using a
-system property:
-
-[source,sh]
---------------------------------------------------
-$ elasticsearch -Des.config=/path/to/config/file
---------------------------------------------------
-
 [float]
 [[configuration-index-settings]]
 === Index Settings
diff --git a/docs/reference/setup/repositories.asciidoc b/docs/reference/setup/repositories.asciidoc
index 79b8959..70b000e 100644
--- a/docs/reference/setup/repositories.asciidoc
+++ b/docs/reference/setup/repositories.asciidoc
@@ -6,7 +6,7 @@ binary packages, but no source packages, as the packages are created as part of
 build.
 
 We have split the major versions in separate urls to avoid accidental upgrades across major version.
-For all 0.90.x releases use 0.90 as version number, for 1.0.x use 1.0, for 1.1.x use 1.1 etc.
+For all 2.x releases use 2.x as version number, for 3.x.y use 3.x etc...
 
 We use the PGP key https://pgp.mit.edu/pks/lookup?op=vindex&search=0xD27D666CD88E42B4[D88E42B4],
 Elasticsearch Signing Key, with fingerprint
@@ -25,11 +25,11 @@ Download and install the Public Signing Key:
 wget -qO - https://packages.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
 --------------------------------------------------
 
-Save the repository definition to  `/etc/apt/sources.list.d/elasticsearch-{branch}.list`:
+Save the repository definition to  +/etc/apt/sources.list.d/elasticsearch-{major-version}.list+:
 
 ["source","sh",subs="attributes,callouts"]
 --------------------------------------------------
-echo "deb http://packages.elastic.co/elasticsearch/{branch}/debian stable main" | sudo tee -a /etc/apt/sources.list.d/elasticsearch-{branch}.list
+echo "deb http://packages.elastic.co/elasticsearch/{major-version}/debian stable main" | sudo tee -a /etc/apt/sources.list.d/elasticsearch-{major-version}.list
 --------------------------------------------------
 
 [WARNING]
@@ -57,9 +57,9 @@ If two entries exist for the same Elasticsearch repository, you will see an erro
 
 ["literal",subs="attributes,callouts"]
 
-Duplicate sources.list entry http://packages.elastic.co/elasticsearch/{branch}/debian/ ...`
+Duplicate sources.list entry http://packages.elastic.co/elasticsearch/{major-version}/debian/ ...`
 
-Examine +/etc/apt/sources.list.d/elasticsearch-{branch}.list+ for the duplicate entry or locate the duplicate entry amongst the files in `/etc/apt/sources.list.d/` and the `/etc/apt/sources.list` file.
+Examine +/etc/apt/sources.list.d/elasticsearch-{major-version}.list+ for the duplicate entry or locate the duplicate entry amongst the files in `/etc/apt/sources.list.d/` and the `/etc/apt/sources.list` file.
 ==================================================
 
 Configure Elasticsearch to automatically start during bootup. If your
@@ -93,9 +93,9 @@ in a file with a `.repo` suffix, for example `elasticsearch.repo`
 
 ["source","sh",subs="attributes,callouts"]
 --------------------------------------------------
-[elasticsearch-{branch}]
-name=Elasticsearch repository for {branch}.x packages
-baseurl=http://packages.elastic.co/elasticsearch/{branch}/centos
+[elasticsearch-{major-version}]
+name=Elasticsearch repository for {major-version} packages
+baseurl=http://packages.elastic.co/elasticsearch/{major-version}/centos
 gpgcheck=1
 gpgkey=http://packages.elastic.co/GPG-KEY-elasticsearch
 enabled=1
diff --git a/plugins/cloud-gce/licenses/commons-codec-1.6.jar.sha1 b/plugins/cloud-gce/licenses/commons-codec-1.6.jar.sha1
deleted file mode 100644
index bf78aff..0000000
--- a/plugins/cloud-gce/licenses/commons-codec-1.6.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-b7f0fc8f61ecadeb3695f0b9464755eee44374d4
diff --git a/plugins/cloud-gce/licenses/commons-codec-LICENSE.txt b/plugins/cloud-gce/licenses/commons-codec-LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/cloud-gce/licenses/commons-codec-LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/cloud-gce/licenses/commons-codec-NOTICE.txt b/plugins/cloud-gce/licenses/commons-codec-NOTICE.txt
deleted file mode 100644
index 5691644..0000000
--- a/plugins/cloud-gce/licenses/commons-codec-NOTICE.txt
+++ /dev/null
@@ -1,17 +0,0 @@
-Apache Commons Codec
-Copyright 2002-2015 The Apache Software Foundation
-
-This product includes software developed at
-The Apache Software Foundation (http://www.apache.org/).
-
-src/test/org/apache/commons/codec/language/DoubleMetaphoneTest.java
-contains test data from http://aspell.net/test/orig/batch0.tab.
-Copyright (C) 2002 Kevin Atkinson (kevina@gnu.org)
-
-===============================================================================
-
-The content of package org.apache.commons.codec.language.bm has been translated
-from the original php source code available at http://stevemorse.org/phoneticinfo.htm
-with permission from the original authors.
-Original source copyright:
-Copyright (c) 2008 Alexander Beider & Stephen P. Morse.
diff --git a/plugins/cloud-gce/licenses/commons-logging-1.1.3.jar.sha1 b/plugins/cloud-gce/licenses/commons-logging-1.1.3.jar.sha1
deleted file mode 100644
index c8756c4..0000000
--- a/plugins/cloud-gce/licenses/commons-logging-1.1.3.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-f6f66e966c70a83ffbdb6f17a0919eaf7c8aca7f
diff --git a/plugins/cloud-gce/licenses/commons-logging-LICENSE.txt b/plugins/cloud-gce/licenses/commons-logging-LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/cloud-gce/licenses/commons-logging-LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/cloud-gce/licenses/commons-logging-NOTICE.txt b/plugins/cloud-gce/licenses/commons-logging-NOTICE.txt
deleted file mode 100644
index d3d6e14..0000000
--- a/plugins/cloud-gce/licenses/commons-logging-NOTICE.txt
+++ /dev/null
@@ -1,5 +0,0 @@
-Apache Commons Logging
-Copyright 2003-2014 The Apache Software Foundation
-
-This product includes software developed at
-The Apache Software Foundation (http://www.apache.org/).
diff --git a/plugins/cloud-gce/licenses/google-LICENSE.txt b/plugins/cloud-gce/licenses/google-LICENSE.txt
deleted file mode 100644
index 980a15a..0000000
--- a/plugins/cloud-gce/licenses/google-LICENSE.txt
+++ /dev/null
@@ -1,201 +0,0 @@
-                                Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "{}"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright {yyyy} {name of copyright owner}
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/cloud-gce/licenses/google-NOTICE.txt b/plugins/cloud-gce/licenses/google-NOTICE.txt
deleted file mode 100644
index 8d1c8b6..0000000
--- a/plugins/cloud-gce/licenses/google-NOTICE.txt
+++ /dev/null
@@ -1 +0,0 @@
- 
diff --git a/plugins/cloud-gce/licenses/google-api-client-1.20.0.jar.sha1 b/plugins/cloud-gce/licenses/google-api-client-1.20.0.jar.sha1
deleted file mode 100644
index 08c24d1..0000000
--- a/plugins/cloud-gce/licenses/google-api-client-1.20.0.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-d3e66209ae9e749b2d6833761e7885f60f285564
diff --git a/plugins/cloud-gce/licenses/google-api-services-compute-v1-rev71-1.20.0.jar.sha1 b/plugins/cloud-gce/licenses/google-api-services-compute-v1-rev71-1.20.0.jar.sha1
deleted file mode 100644
index c6e6948..0000000
--- a/plugins/cloud-gce/licenses/google-api-services-compute-v1-rev71-1.20.0.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-2fa36fff3b5bf59a63c4f2bbfac1f88251cd7986
diff --git a/plugins/cloud-gce/licenses/google-http-client-1.20.0.jar.sha1 b/plugins/cloud-gce/licenses/google-http-client-1.20.0.jar.sha1
deleted file mode 100644
index 66a2247..0000000
--- a/plugins/cloud-gce/licenses/google-http-client-1.20.0.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-93d82db2bca534960253f43424b2ba9d7638b4d2
diff --git a/plugins/cloud-gce/licenses/google-http-client-jackson2-1.20.0.jar.sha1 b/plugins/cloud-gce/licenses/google-http-client-jackson2-1.20.0.jar.sha1
deleted file mode 100644
index 6d861e6..0000000
--- a/plugins/cloud-gce/licenses/google-http-client-jackson2-1.20.0.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-2408070b2abec043624d35b35e30450f1b663858
diff --git a/plugins/cloud-gce/licenses/google-oauth-client-1.20.0.jar.sha1 b/plugins/cloud-gce/licenses/google-oauth-client-1.20.0.jar.sha1
deleted file mode 100644
index c35c4bc..0000000
--- a/plugins/cloud-gce/licenses/google-oauth-client-1.20.0.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-1d086ac5756475ddf451af2e2df6e288d18608ca
diff --git a/plugins/cloud-gce/licenses/httpclient-4.3.6.jar.sha1 b/plugins/cloud-gce/licenses/httpclient-4.3.6.jar.sha1
deleted file mode 100644
index 3d35ee9..0000000
--- a/plugins/cloud-gce/licenses/httpclient-4.3.6.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-4c47155e3e6c9a41a28db36680b828ced53b8af4
diff --git a/plugins/cloud-gce/licenses/httpclient-LICENSE.txt b/plugins/cloud-gce/licenses/httpclient-LICENSE.txt
deleted file mode 100644
index 32f01ed..0000000
--- a/plugins/cloud-gce/licenses/httpclient-LICENSE.txt
+++ /dev/null
@@ -1,558 +0,0 @@
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-=========================================================================
-
-This project includes Public Suffix List copied from
-<https://publicsuffix.org/list/effective_tld_names.dat>
-licensed under the terms of the Mozilla Public License, v. 2.0
-
-Full license text: <http://mozilla.org/MPL/2.0/>
-
-Mozilla Public License Version 2.0
-==================================
-
-1. Definitions
---------------
-
-1.1. "Contributor"
-    means each individual or legal entity that creates, contributes to
-    the creation of, or owns Covered Software.
-
-1.2. "Contributor Version"
-    means the combination of the Contributions of others (if any) used
-    by a Contributor and that particular Contributor's Contribution.
-
-1.3. "Contribution"
-    means Covered Software of a particular Contributor.
-
-1.4. "Covered Software"
-    means Source Code Form to which the initial Contributor has attached
-    the notice in Exhibit A, the Executable Form of such Source Code
-    Form, and Modifications of such Source Code Form, in each case
-    including portions thereof.
-
-1.5. "Incompatible With Secondary Licenses"
-    means
-
-    (a) that the initial Contributor has attached the notice described
-        in Exhibit B to the Covered Software; or
-
-    (b) that the Covered Software was made available under the terms of
-        version 1.1 or earlier of the License, but not also under the
-        terms of a Secondary License.
-
-1.6. "Executable Form"
-    means any form of the work other than Source Code Form.
-
-1.7. "Larger Work"
-    means a work that combines Covered Software with other material, in
-    a separate file or files, that is not Covered Software.
-
-1.8. "License"
-    means this document.
-
-1.9. "Licensable"
-    means having the right to grant, to the maximum extent possible,
-    whether at the time of the initial grant or subsequently, any and
-    all of the rights conveyed by this License.
-
-1.10. "Modifications"
-    means any of the following:
-
-    (a) any file in Source Code Form that results from an addition to,
-        deletion from, or modification of the contents of Covered
-        Software; or
-
-    (b) any new file in Source Code Form that contains any Covered
-        Software.
-
-1.11. "Patent Claims" of a Contributor
-    means any patent claim(s), including without limitation, method,
-    process, and apparatus claims, in any patent Licensable by such
-    Contributor that would be infringed, but for the grant of the
-    License, by the making, using, selling, offering for sale, having
-    made, import, or transfer of either its Contributions or its
-    Contributor Version.
-
-1.12. "Secondary License"
-    means either the GNU General Public License, Version 2.0, the GNU
-    Lesser General Public License, Version 2.1, the GNU Affero General
-    Public License, Version 3.0, or any later versions of those
-    licenses.
-
-1.13. "Source Code Form"
-    means the form of the work preferred for making modifications.
-
-1.14. "You" (or "Your")
-    means an individual or a legal entity exercising rights under this
-    License. For legal entities, "You" includes any entity that
-    controls, is controlled by, or is under common control with You. For
-    purposes of this definition, "control" means (a) the power, direct
-    or indirect, to cause the direction or management of such entity,
-    whether by contract or otherwise, or (b) ownership of more than
-    fifty percent (50%) of the outstanding shares or beneficial
-    ownership of such entity.
-
-2. License Grants and Conditions
---------------------------------
-
-2.1. Grants
-
-Each Contributor hereby grants You a world-wide, royalty-free,
-non-exclusive license:
-
-(a) under intellectual property rights (other than patent or trademark)
-    Licensable by such Contributor to use, reproduce, make available,
-    modify, display, perform, distribute, and otherwise exploit its
-    Contributions, either on an unmodified basis, with Modifications, or
-    as part of a Larger Work; and
-
-(b) under Patent Claims of such Contributor to make, use, sell, offer
-    for sale, have made, import, and otherwise transfer either its
-    Contributions or its Contributor Version.
-
-2.2. Effective Date
-
-The licenses granted in Section 2.1 with respect to any Contribution
-become effective for each Contribution on the date the Contributor first
-distributes such Contribution.
-
-2.3. Limitations on Grant Scope
-
-The licenses granted in this Section 2 are the only rights granted under
-this License. No additional rights or licenses will be implied from the
-distribution or licensing of Covered Software under this License.
-Notwithstanding Section 2.1(b) above, no patent license is granted by a
-Contributor:
-
-(a) for any code that a Contributor has removed from Covered Software;
-    or
-
-(b) for infringements caused by: (i) Your and any other third party's
-    modifications of Covered Software, or (ii) the combination of its
-    Contributions with other software (except as part of its Contributor
-    Version); or
-
-(c) under Patent Claims infringed by Covered Software in the absence of
-    its Contributions.
-
-This License does not grant any rights in the trademarks, service marks,
-or logos of any Contributor (except as may be necessary to comply with
-the notice requirements in Section 3.4).
-
-2.4. Subsequent Licenses
-
-No Contributor makes additional grants as a result of Your choice to
-distribute the Covered Software under a subsequent version of this
-License (see Section 10.2) or under the terms of a Secondary License (if
-permitted under the terms of Section 3.3).
-
-2.5. Representation
-
-Each Contributor represents that the Contributor believes its
-Contributions are its original creation(s) or it has sufficient rights
-to grant the rights to its Contributions conveyed by this License.
-
-2.6. Fair Use
-
-This License is not intended to limit any rights You have under
-applicable copyright doctrines of fair use, fair dealing, or other
-equivalents.
-
-2.7. Conditions
-
-Sections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted
-in Section 2.1.
-
-3. Responsibilities
--------------------
-
-3.1. Distribution of Source Form
-
-All distribution of Covered Software in Source Code Form, including any
-Modifications that You create or to which You contribute, must be under
-the terms of this License. You must inform recipients that the Source
-Code Form of the Covered Software is governed by the terms of this
-License, and how they can obtain a copy of this License. You may not
-attempt to alter or restrict the recipients' rights in the Source Code
-Form.
-
-3.2. Distribution of Executable Form
-
-If You distribute Covered Software in Executable Form then:
-
-(a) such Covered Software must also be made available in Source Code
-    Form, as described in Section 3.1, and You must inform recipients of
-    the Executable Form how they can obtain a copy of such Source Code
-    Form by reasonable means in a timely manner, at a charge no more
-    than the cost of distribution to the recipient; and
-
-(b) You may distribute such Executable Form under the terms of this
-    License, or sublicense it under different terms, provided that the
-    license for the Executable Form does not attempt to limit or alter
-    the recipients' rights in the Source Code Form under this License.
-
-3.3. Distribution of a Larger Work
-
-You may create and distribute a Larger Work under terms of Your choice,
-provided that You also comply with the requirements of this License for
-the Covered Software. If the Larger Work is a combination of Covered
-Software with a work governed by one or more Secondary Licenses, and the
-Covered Software is not Incompatible With Secondary Licenses, this
-License permits You to additionally distribute such Covered Software
-under the terms of such Secondary License(s), so that the recipient of
-the Larger Work may, at their option, further distribute the Covered
-Software under the terms of either this License or such Secondary
-License(s).
-
-3.4. Notices
-
-You may not remove or alter the substance of any license notices
-(including copyright notices, patent notices, disclaimers of warranty,
-or limitations of liability) contained within the Source Code Form of
-the Covered Software, except that You may alter any license notices to
-the extent required to remedy known factual inaccuracies.
-
-3.5. Application of Additional Terms
-
-You may choose to offer, and to charge a fee for, warranty, support,
-indemnity or liability obligations to one or more recipients of Covered
-Software. However, You may do so only on Your own behalf, and not on
-behalf of any Contributor. You must make it absolutely clear that any
-such warranty, support, indemnity, or liability obligation is offered by
-You alone, and You hereby agree to indemnify every Contributor for any
-liability incurred by such Contributor as a result of warranty, support,
-indemnity or liability terms You offer. You may include additional
-disclaimers of warranty and limitations of liability specific to any
-jurisdiction.
-
-4. Inability to Comply Due to Statute or Regulation
----------------------------------------------------
-
-If it is impossible for You to comply with any of the terms of this
-License with respect to some or all of the Covered Software due to
-statute, judicial order, or regulation then You must: (a) comply with
-the terms of this License to the maximum extent possible; and (b)
-describe the limitations and the code they affect. Such description must
-be placed in a text file included with all distributions of the Covered
-Software under this License. Except to the extent prohibited by statute
-or regulation, such description must be sufficiently detailed for a
-recipient of ordinary skill to be able to understand it.
-
-5. Termination
---------------
-
-5.1. The rights granted under this License will terminate automatically
-if You fail to comply with any of its terms. However, if You become
-compliant, then the rights granted under this License from a particular
-Contributor are reinstated (a) provisionally, unless and until such
-Contributor explicitly and finally terminates Your grants, and (b) on an
-ongoing basis, if such Contributor fails to notify You of the
-non-compliance by some reasonable means prior to 60 days after You have
-come back into compliance. Moreover, Your grants from a particular
-Contributor are reinstated on an ongoing basis if such Contributor
-notifies You of the non-compliance by some reasonable means, this is the
-first time You have received notice of non-compliance with this License
-from such Contributor, and You become compliant prior to 30 days after
-Your receipt of the notice.
-
-5.2. If You initiate litigation against any entity by asserting a patent
-infringement claim (excluding declaratory judgment actions,
-counter-claims, and cross-claims) alleging that a Contributor Version
-directly or indirectly infringes any patent, then the rights granted to
-You by any and all Contributors for the Covered Software under Section
-2.1 of this License shall terminate.
-
-5.3. In the event of termination under Sections 5.1 or 5.2 above, all
-end user license agreements (excluding distributors and resellers) which
-have been validly granted by You or Your distributors under this License
-prior to termination shall survive termination.
-
-************************************************************************
-*                                                                      *
-*  6. Disclaimer of Warranty                                           *
-*  -------------------------                                           *
-*                                                                      *
-*  Covered Software is provided under this License on an "as is"       *
-*  basis, without warranty of any kind, either expressed, implied, or  *
-*  statutory, including, without limitation, warranties that the       *
-*  Covered Software is free of defects, merchantable, fit for a        *
-*  particular purpose or non-infringing. The entire risk as to the     *
-*  quality and performance of the Covered Software is with You.        *
-*  Should any Covered Software prove defective in any respect, You     *
-*  (not any Contributor) assume the cost of any necessary servicing,   *
-*  repair, or correction. This disclaimer of warranty constitutes an   *
-*  essential part of this License. No use of any Covered Software is   *
-*  authorized under this License except under this disclaimer.         *
-*                                                                      *
-************************************************************************
-
-************************************************************************
-*                                                                      *
-*  7. Limitation of Liability                                          *
-*  --------------------------                                          *
-*                                                                      *
-*  Under no circumstances and under no legal theory, whether tort      *
-*  (including negligence), contract, or otherwise, shall any           *
-*  Contributor, or anyone who distributes Covered Software as          *
-*  permitted above, be liable to You for any direct, indirect,         *
-*  special, incidental, or consequential damages of any character      *
-*  including, without limitation, damages for lost profits, loss of    *
-*  goodwill, work stoppage, computer failure or malfunction, or any    *
-*  and all other commercial damages or losses, even if such party      *
-*  shall have been informed of the possibility of such damages. This   *
-*  limitation of liability shall not apply to liability for death or   *
-*  personal injury resulting from such party's negligence to the       *
-*  extent applicable law prohibits such limitation. Some               *
-*  jurisdictions do not allow the exclusion or limitation of           *
-*  incidental or consequential damages, so this exclusion and          *
-*  limitation may not apply to You.                                    *
-*                                                                      *
-************************************************************************
-
-8. Litigation
--------------
-
-Any litigation relating to this License may be brought only in the
-courts of a jurisdiction where the defendant maintains its principal
-place of business and such litigation shall be governed by laws of that
-jurisdiction, without reference to its conflict-of-law provisions.
-Nothing in this Section shall prevent a party's ability to bring
-cross-claims or counter-claims.
-
-9. Miscellaneous
-----------------
-
-This License represents the complete agreement concerning the subject
-matter hereof. If any provision of this License is held to be
-unenforceable, such provision shall be reformed only to the extent
-necessary to make it enforceable. Any law or regulation which provides
-that the language of a contract shall be construed against the drafter
-shall not be used to construe this License against a Contributor.
-
-10. Versions of the License
----------------------------
-
-10.1. New Versions
-
-Mozilla Foundation is the license steward. Except as provided in Section
-10.3, no one other than the license steward has the right to modify or
-publish new versions of this License. Each version will be given a
-distinguishing version number.
-
-10.2. Effect of New Versions
-
-You may distribute the Covered Software under the terms of the version
-of the License under which You originally received the Covered Software,
-or under the terms of any subsequent version published by the license
-steward.
-
-10.3. Modified Versions
-
-If you create software not governed by this License, and you want to
-create a new license for such software, you may create and use a
-modified version of this License if you rename the license and remove
-any references to the name of the license steward (except to note that
-such modified license differs from this License).
-
-10.4. Distributing Source Code Form that is Incompatible With Secondary
-Licenses
-
-If You choose to distribute Source Code Form that is Incompatible With
-Secondary Licenses under the terms of this version of the License, the
-notice described in Exhibit B of this License must be attached.
-
-Exhibit A - Source Code Form License Notice
--------------------------------------------
-
-  This Source Code Form is subject to the terms of the Mozilla Public
-  License, v. 2.0. If a copy of the MPL was not distributed with this
-  file, You can obtain one at http://mozilla.org/MPL/2.0/.
-
-If it is not possible or desirable to put the notice in a particular
-file, then You may include the notice in a location (such as a LICENSE
-file in a relevant directory) where a recipient would be likely to look
-for such a notice.
-
-You may add additional accurate notices of copyright ownership.
-
-Exhibit B - "Incompatible With Secondary Licenses" Notice
----------------------------------------------------------
-
-  This Source Code Form is "Incompatible With Secondary Licenses", as
-  defined by the Mozilla Public License, v. 2.0.
diff --git a/plugins/cloud-gce/licenses/httpclient-NOTICE.txt b/plugins/cloud-gce/licenses/httpclient-NOTICE.txt
deleted file mode 100644
index 4f60581..0000000
--- a/plugins/cloud-gce/licenses/httpclient-NOTICE.txt
+++ /dev/null
@@ -1,5 +0,0 @@
-Apache HttpComponents Client
-Copyright 1999-2015 The Apache Software Foundation
-
-This product includes software developed at
-The Apache Software Foundation (http://www.apache.org/).
diff --git a/plugins/cloud-gce/licenses/httpcore-4.3.3.jar.sha1 b/plugins/cloud-gce/licenses/httpcore-4.3.3.jar.sha1
deleted file mode 100644
index 5d9c0e2..0000000
--- a/plugins/cloud-gce/licenses/httpcore-4.3.3.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-f91b7a4aadc5cf486df6e4634748d7dd7a73f06d
diff --git a/plugins/cloud-gce/licenses/httpcore-LICENSE.txt b/plugins/cloud-gce/licenses/httpcore-LICENSE.txt
deleted file mode 100644
index 72819a9..0000000
--- a/plugins/cloud-gce/licenses/httpcore-LICENSE.txt
+++ /dev/null
@@ -1,241 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-=========================================================================
-
-This project contains annotations in the package org.apache.http.annotation
-which are derived from JCIP-ANNOTATIONS
-Copyright (c) 2005 Brian Goetz and Tim Peierls.
-See http://www.jcip.net and the Creative Commons Attribution License
-(http://creativecommons.org/licenses/by/2.5)
-Full text: http://creativecommons.org/licenses/by/2.5/legalcode
-
-License
-
-THE WORK (AS DEFINED BELOW) IS PROVIDED UNDER THE TERMS OF THIS CREATIVE COMMONS PUBLIC LICENSE ("CCPL" OR "LICENSE"). THE WORK IS PROTECTED BY COPYRIGHT AND/OR OTHER APPLICABLE LAW. ANY USE OF THE WORK OTHER THAN AS AUTHORIZED UNDER THIS LICENSE OR COPYRIGHT LAW IS PROHIBITED.
-
-BY EXERCISING ANY RIGHTS TO THE WORK PROVIDED HERE, YOU ACCEPT AND AGREE TO BE BOUND BY THE TERMS OF THIS LICENSE. THE LICENSOR GRANTS YOU THE RIGHTS CONTAINED HERE IN CONSIDERATION OF YOUR ACCEPTANCE OF SUCH TERMS AND CONDITIONS.
-
-1. Definitions
-
-    "Collective Work" means a work, such as a periodical issue, anthology or encyclopedia, in which the Work in its entirety in unmodified form, along with a number of other contributions, constituting separate and independent works in themselves, are assembled into a collective whole. A work that constitutes a Collective Work will not be considered a Derivative Work (as defined below) for the purposes of this License.
-    "Derivative Work" means a work based upon the Work or upon the Work and other pre-existing works, such as a translation, musical arrangement, dramatization, fictionalization, motion picture version, sound recording, art reproduction, abridgment, condensation, or any other form in which the Work may be recast, transformed, or adapted, except that a work that constitutes a Collective Work will not be considered a Derivative Work for the purpose of this License. For the avoidance of doubt, where the Work is a musical composition or sound recording, the synchronization of the Work in timed-relation with a moving image ("synching") will be considered a Derivative Work for the purpose of this License.
-    "Licensor" means the individual or entity that offers the Work under the terms of this License.
-    "Original Author" means the individual or entity who created the Work.
-    "Work" means the copyrightable work of authorship offered under the terms of this License.
-    "You" means an individual or entity exercising rights under this License who has not previously violated the terms of this License with respect to the Work, or who has received express permission from the Licensor to exercise rights under this License despite a previous violation.
-
-2. Fair Use Rights. Nothing in this license is intended to reduce, limit, or restrict any rights arising from fair use, first sale or other limitations on the exclusive rights of the copyright owner under copyright law or other applicable laws.
-
-3. License Grant. Subject to the terms and conditions of this License, Licensor hereby grants You a worldwide, royalty-free, non-exclusive, perpetual (for the duration of the applicable copyright) license to exercise the rights in the Work as stated below:
-
-    to reproduce the Work, to incorporate the Work into one or more Collective Works, and to reproduce the Work as incorporated in the Collective Works;
-    to create and reproduce Derivative Works;
-    to distribute copies or phonorecords of, display publicly, perform publicly, and perform publicly by means of a digital audio transmission the Work including as incorporated in Collective Works;
-    to distribute copies or phonorecords of, display publicly, perform publicly, and perform publicly by means of a digital audio transmission Derivative Works.
-
-    For the avoidance of doubt, where the work is a musical composition:
-        Performance Royalties Under Blanket Licenses. Licensor waives the exclusive right to collect, whether individually or via a performance rights society (e.g. ASCAP, BMI, SESAC), royalties for the public performance or public digital performance (e.g. webcast) of the Work.
-        Mechanical Rights and Statutory Royalties. Licensor waives the exclusive right to collect, whether individually or via a music rights agency or designated agent (e.g. Harry Fox Agency), royalties for any phonorecord You create from the Work ("cover version") and distribute, subject to the compulsory license created by 17 USC Section 115 of the US Copyright Act (or the equivalent in other jurisdictions).
-    Webcasting Rights and Statutory Royalties. For the avoidance of doubt, where the Work is a sound recording, Licensor waives the exclusive right to collect, whether individually or via a performance-rights society (e.g. SoundExchange), royalties for the public digital performance (e.g. webcast) of the Work, subject to the compulsory license created by 17 USC Section 114 of the US Copyright Act (or the equivalent in other jurisdictions).
-
-The above rights may be exercised in all media and formats whether now known or hereafter devised. The above rights include the right to make such modifications as are technically necessary to exercise the rights in other media and formats. All rights not expressly granted by Licensor are hereby reserved.
-
-4. Restrictions.The license granted in Section 3 above is expressly made subject to and limited by the following restrictions:
-
-    You may distribute, publicly display, publicly perform, or publicly digitally perform the Work only under the terms of this License, and You must include a copy of, or the Uniform Resource Identifier for, this License with every copy or phonorecord of the Work You distribute, publicly display, publicly perform, or publicly digitally perform. You may not offer or impose any terms on the Work that alter or restrict the terms of this License or the recipients' exercise of the rights granted hereunder. You may not sublicense the Work. You must keep intact all notices that refer to this License and to the disclaimer of warranties. You may not distribute, publicly display, publicly perform, or publicly digitally perform the Work with any technological measures that control access or use of the Work in a manner inconsistent with the terms of this License Agreement. The above applies to the Work as incorporated in a Collective Work, but this does not require the Collective Work apart from the Work itself to be made subject to the terms of this License. If You create a Collective Work, upon notice from any Licensor You must, to the extent practicable, remove from the Collective Work any credit as required by clause 4(b), as requested. If You create a Derivative Work, upon notice from any Licensor You must, to the extent practicable, remove from the Derivative Work any credit as required by clause 4(b), as requested.
-    If you distribute, publicly display, publicly perform, or publicly digitally perform the Work or any Derivative Works or Collective Works, You must keep intact all copyright notices for the Work and provide, reasonable to the medium or means You are utilizing: (i) the name of the Original Author (or pseudonym, if applicable) if supplied, and/or (ii) if the Original Author and/or Licensor designate another party or parties (e.g. a sponsor institute, publishing entity, journal) for attribution in Licensor's copyright notice, terms of service or by other reasonable means, the name of such party or parties; the title of the Work if supplied; to the extent reasonably practicable, the Uniform Resource Identifier, if any, that Licensor specifies to be associated with the Work, unless such URI does not refer to the copyright notice or licensing information for the Work; and in the case of a Derivative Work, a credit identifying the use of the Work in the Derivative Work (e.g., "French translation of the Work by Original Author," or "Screenplay based on original Work by Original Author"). Such credit may be implemented in any reasonable manner; provided, however, that in the case of a Derivative Work or Collective Work, at a minimum such credit will appear where any other comparable authorship credit appears and in a manner at least as prominent as such other comparable authorship credit.
-
-5. Representations, Warranties and Disclaimer
-
-UNLESS OTHERWISE MUTUALLY AGREED TO BY THE PARTIES IN WRITING, LICENSOR OFFERS THE WORK AS-IS AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND CONCERNING THE WORK, EXPRESS, IMPLIED, STATUTORY OR OTHERWISE, INCLUDING, WITHOUT LIMITATION, WARRANTIES OF TITLE, MERCHANTIBILITY, FITNESS FOR A PARTICULAR PURPOSE, NONINFRINGEMENT, OR THE ABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OF ABSENCE OF ERRORS, WHETHER OR NOT DISCOVERABLE. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF IMPLIED WARRANTIES, SO SUCH EXCLUSION MAY NOT APPLY TO YOU.
-
-6. Limitation on Liability. EXCEPT TO THE EXTENT REQUIRED BY APPLICABLE LAW, IN NO EVENT WILL LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY FOR ANY SPECIAL, INCIDENTAL, CONSEQUENTIAL, PUNITIVE OR EXEMPLARY DAMAGES ARISING OUT OF THIS LICENSE OR THE USE OF THE WORK, EVEN IF LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
-
-7. Termination
-
-    This License and the rights granted hereunder will terminate automatically upon any breach by You of the terms of this License. Individuals or entities who have received Derivative Works or Collective Works from You under this License, however, will not have their licenses terminated provided such individuals or entities remain in full compliance with those licenses. Sections 1, 2, 5, 6, 7, and 8 will survive any termination of this License.
-    Subject to the above terms and conditions, the license granted here is perpetual (for the duration of the applicable copyright in the Work). Notwithstanding the above, Licensor reserves the right to release the Work under different license terms or to stop distributing the Work at any time; provided, however that any such election will not serve to withdraw this License (or any other license that has been, or is required to be, granted under the terms of this License), and this License will continue in full force and effect unless terminated as stated above.
-
-8. Miscellaneous
-
-    Each time You distribute or publicly digitally perform the Work or a Collective Work, the Licensor offers to the recipient a license to the Work on the same terms and conditions as the license granted to You under this License.
-    Each time You distribute or publicly digitally perform a Derivative Work, Licensor offers to the recipient a license to the original Work on the same terms and conditions as the license granted to You under this License.
-    If any provision of this License is invalid or unenforceable under applicable law, it shall not affect the validity or enforceability of the remainder of the terms of this License, and without further action by the parties to this agreement, such provision shall be reformed to the minimum extent necessary to make such provision valid and enforceable.
-    No term or provision of this License shall be deemed waived and no breach consented to unless such waiver or consent shall be in writing and signed by the party to be charged with such waiver or consent.
-    This License constitutes the entire agreement between the parties with respect to the Work licensed here. There are no understandings, agreements or representations with respect to the Work not specified here. Licensor shall not be bound by any additional provisions that may appear in any communication from You. This License may not be modified without the mutual written agreement of the Licensor and You.
diff --git a/plugins/cloud-gce/licenses/httpcore-NOTICE.txt b/plugins/cloud-gce/licenses/httpcore-NOTICE.txt
deleted file mode 100644
index c0be50a..0000000
--- a/plugins/cloud-gce/licenses/httpcore-NOTICE.txt
+++ /dev/null
@@ -1,8 +0,0 @@
-Apache HttpComponents Core
-Copyright 2005-2014 The Apache Software Foundation
-
-This product includes software developed at
-The Apache Software Foundation (http://www.apache.org/).
-
-This project contains annotations derived from JCIP-ANNOTATIONS
-Copyright (c) 2005 Brian Goetz and Tim Peierls. See http://www.jcip.net
diff --git a/plugins/cloud-gce/licenses/jsr305-1.3.9.jar.sha1 b/plugins/cloud-gce/licenses/jsr305-1.3.9.jar.sha1
deleted file mode 100644
index c04a429..0000000
--- a/plugins/cloud-gce/licenses/jsr305-1.3.9.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-40719ea6961c0cb6afaeb6a921eaa1f6afd4cfdf
diff --git a/plugins/cloud-gce/licenses/jsr305-LICENSE.txt b/plugins/cloud-gce/licenses/jsr305-LICENSE.txt
deleted file mode 100644
index 0cb8710..0000000
--- a/plugins/cloud-gce/licenses/jsr305-LICENSE.txt
+++ /dev/null
@@ -1,29 +0,0 @@
-Copyright (c) 2007-2009, JSR305 expert group
-All rights reserved.
-
-http://www.opensource.org/licenses/bsd-license.php
-
-Redistribution and use in source and binary forms, with or without 
-modification, are permitted provided that the following conditions are met:
-
-    * Redistributions of source code must retain the above copyright notice, 
-      this list of conditions and the following disclaimer.
-    * Redistributions in binary form must reproduce the above copyright notice, 
-      this list of conditions and the following disclaimer in the documentation 
-      and/or other materials provided with the distribution.
-    * Neither the name of the JSR305 expert group nor the names of its 
-      contributors may be used to endorse or promote products derived from 
-      this software without specific prior written permission.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" 
-AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, 
-THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE 
-ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE 
-LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR 
-CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF 
-SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS 
-INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN 
-CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) 
-ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE 
-POSSIBILITY OF SUCH DAMAGE.
-
diff --git a/plugins/cloud-gce/licenses/jsr305-NOTICE.txt b/plugins/cloud-gce/licenses/jsr305-NOTICE.txt
deleted file mode 100644
index 8d1c8b6..0000000
--- a/plugins/cloud-gce/licenses/jsr305-NOTICE.txt
+++ /dev/null
@@ -1 +0,0 @@
- 
diff --git a/plugins/cloud-gce/pom.xml b/plugins/cloud-gce/pom.xml
deleted file mode 100644
index da987f8..0000000
--- a/plugins/cloud-gce/pom.xml
+++ /dev/null
@@ -1,68 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<!-- Licensed to Elasticsearch under one or more contributor
-license agreements. See the NOTICE file distributed with this work for additional
-information regarding copyright ownership. Elasticsearch licenses this file to you
-under the Apache License, Version 2.0 (the "License"); you may not use this
-file except in compliance with the License. You may obtain a copy of the
-License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by
-applicable law or agreed to in writing, software distributed under the License
-is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-KIND, either express or implied. See the License for the specific language
-governing permissions and limitations under the License. -->
-
-<project xmlns="http://maven.apache.org/POM/4.0.0"
-         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
-    <modelVersion>4.0.0</modelVersion>
-
-    <parent>
-        <groupId>org.elasticsearch.plugin</groupId>
-        <artifactId>plugins</artifactId>
-        <version>3.0.0-SNAPSHOT</version>
-    </parent>
-
-    <artifactId>cloud-gce</artifactId>
-    <name>Plugin: Cloud: Google Compute Engine</name>
-    <description>The Google Compute Engine (GCE) Cloud plugin allows to use GCE API for the unicast discovery mechanism.</description>
-
-    <properties>
-        <elasticsearch.plugin.classname>org.elasticsearch.plugin.cloud.gce.CloudGcePlugin</elasticsearch.plugin.classname>
-        <google.gce.version>v1-rev71-1.20.0</google.gce.version>
-        <!-- currently has no unit tests -->
-        <tests.rest.suite>cloud_gce</tests.rest.suite>
-        <tests.rest.load_packaged>false</tests.rest.load_packaged>
-        <xlint.options>-Xlint:-rawtypes,-unchecked</xlint.options>
-    </properties>
-
-    <dependencies>
-        <!-- Google APIs -->
-        <dependency>
-          <groupId>com.google.apis</groupId>
-          <artifactId>google-api-services-compute</artifactId>
-          <version>${google.gce.version}</version>
-          <exclusions>
-              <exclusion>
-                 <groupId>com.google.guava</groupId>
-                 <artifactId>guava-jdk5</artifactId>
-              </exclusion>
-          </exclusions>
-        </dependency>
-        <!-- We need to force here the compile scope as it was defined as test scope in plugins/pom.xml -->
-        <!-- TODO: remove this dependency when we will have a REST Test module -->
-        <dependency>
-            <groupId>org.apache.httpcomponents</groupId>
-            <artifactId>httpclient</artifactId>
-            <scope>compile</scope>
-        </dependency>
-    </dependencies>
-
-    <build>
-        <plugins>
-            <plugin>
-                <groupId>org.apache.maven.plugins</groupId>
-                <artifactId>maven-assembly-plugin</artifactId>
-            </plugin>
-        </plugins>
-    </build>
-
-</project>
diff --git a/plugins/cloud-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeService.java b/plugins/cloud-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeService.java
deleted file mode 100644
index 6ba857d..0000000
--- a/plugins/cloud-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeService.java
+++ /dev/null
@@ -1,40 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.gce;
-
-import com.google.api.services.compute.model.Instance;
-import org.elasticsearch.common.component.LifecycleComponent;
-
-import java.util.Collection;
-
-/**
- *
- */
-public interface GceComputeService extends LifecycleComponent<GceComputeService> {
-    static final public class Fields {
-        public static final String PROJECT = "cloud.gce.project_id";
-        public static final String ZONE = "cloud.gce.zone";
-        public static final String REFRESH = "cloud.gce.refresh_interval";
-        public static final String TAGS = "discovery.gce.tags";
-        public static final String VERSION = "Elasticsearch/GceCloud/1.0";
-    }
-
-    public Collection<Instance> instances();
-}
diff --git a/plugins/cloud-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeServiceImpl.java b/plugins/cloud-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeServiceImpl.java
deleted file mode 100644
index 2a9bf6d..0000000
--- a/plugins/cloud-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeServiceImpl.java
+++ /dev/null
@@ -1,192 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.gce;
-
-import com.google.api.client.googleapis.compute.ComputeCredential;
-import com.google.api.client.googleapis.javanet.GoogleNetHttpTransport;
-import com.google.api.client.http.HttpTransport;
-import com.google.api.client.json.JsonFactory;
-import com.google.api.client.json.jackson2.JacksonFactory;
-import com.google.api.services.compute.Compute;
-import com.google.api.services.compute.model.Instance;
-import com.google.api.services.compute.model.InstanceList;
-
-import org.elasticsearch.SpecialPermission;
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.component.AbstractLifecycleComponent;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
-
-import java.io.IOException;
-import java.security.AccessController;
-import java.security.GeneralSecurityException;
-import java.security.PrivilegedActionException;
-import java.security.PrivilegedExceptionAction;
-import java.util.*;
-
-/**
- *
- */
-public class GceComputeServiceImpl extends AbstractLifecycleComponent<GceComputeService>
-    implements GceComputeService {
-
-    private final String project;
-    private final List<String> zones;
-
-    // Forcing Google Token API URL as set in GCE SDK to
-    //      http://metadata/computeMetadata/v1/instance/service-accounts/default/token
-    // See https://developers.google.com/compute/docs/metadata#metadataserver
-    public static final String TOKEN_SERVER_ENCODED_URL = "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token";
-
-    @Override
-    public Collection<Instance> instances() {
-        logger.debug("get instances for project [{}], zones [{}]", project, zones);
-        final List<Instance> instances = zones.stream().map((zoneId) -> {
-            try {
-                // hack around code messiness in GCE code
-                // TODO: get this fixed
-                SecurityManager sm = System.getSecurityManager();
-                if (sm != null) {
-                    sm.checkPermission(new SpecialPermission());
-                }
-                InstanceList instanceList = AccessController.doPrivileged(new PrivilegedExceptionAction<InstanceList>() {
-                    @Override
-                    public InstanceList run() throws Exception {
-                        Compute.Instances.List list = client().instances().list(project, zoneId);
-                        return list.execute();
-                    }
-                });
-                if (instanceList.isEmpty()) {
-                    return Collections.EMPTY_LIST;
-                }
-                return instanceList.getItems();
-            } catch (PrivilegedActionException e) {
-                logger.warn("Problem fetching instance list for zone {}", zoneId);
-                logger.debug("Full exception:", e);
-                return Collections.EMPTY_LIST;
-            }
-        }).reduce(new ArrayList<>(), (a, b) -> {
-            a.addAll(b);
-            return a;
-        });
-
-        if (instances.isEmpty()) {
-            logger.warn("disabling GCE discovery. Can not get list of nodes");
-        }
-
-        return instances;
-    }
-
-    private Compute client;
-    private TimeValue refreshInterval = null;
-    private long lastRefresh;
-
-    /** Global instance of the HTTP transport. */
-    private HttpTransport gceHttpTransport;
-
-    /** Global instance of the JSON factory. */
-    private JsonFactory gceJsonFactory;
-
-    @Inject
-    public GceComputeServiceImpl(Settings settings) {
-        super(settings);
-        this.project = settings.get(Fields.PROJECT);
-        String[] zoneList = settings.getAsArray(Fields.ZONE);
-        this.zones = Arrays.asList(zoneList);
-    }
-
-    protected synchronized HttpTransport getGceHttpTransport() throws GeneralSecurityException, IOException {
-        if (gceHttpTransport == null) {
-            gceHttpTransport = GoogleNetHttpTransport.newTrustedTransport();
-        }
-        return gceHttpTransport;
-    }
-
-    public synchronized Compute client() {
-        if (refreshInterval != null && refreshInterval.millis() != 0) {
-            if (client != null &&
-                    (refreshInterval.millis() < 0 || (System.currentTimeMillis() - lastRefresh) < refreshInterval.millis())) {
-                if (logger.isTraceEnabled()) logger.trace("using cache to retrieve client");
-                return client;
-            }
-            lastRefresh = System.currentTimeMillis();
-        }
-
-        try {
-            gceJsonFactory = new JacksonFactory();
-
-            logger.info("starting GCE discovery service");
-            ComputeCredential credential = new ComputeCredential.Builder(getGceHttpTransport(), gceJsonFactory)
-                        .setTokenServerEncodedUrl(TOKEN_SERVER_ENCODED_URL)
-                    .build();
-
-            // hack around code messiness in GCE code
-            // TODO: get this fixed
-            SecurityManager sm = System.getSecurityManager();
-            if (sm != null) {
-                sm.checkPermission(new SpecialPermission());
-            }
-            AccessController.doPrivileged(new PrivilegedExceptionAction<Void>() {
-                @Override
-                public Void run() throws IOException {
-                    credential.refreshToken();
-                    return null;
-                }
-            });
-
-            logger.debug("token [{}] will expire in [{}] s", credential.getAccessToken(), credential.getExpiresInSeconds());
-            if (credential.getExpiresInSeconds() != null) {
-                refreshInterval = TimeValue.timeValueSeconds(credential.getExpiresInSeconds()-1);
-            }
-
-            // Once done, let's use this token
-            this.client = new Compute.Builder(getGceHttpTransport(), gceJsonFactory, null)
-                    .setApplicationName(Fields.VERSION)
-                    .setHttpRequestInitializer(credential)
-                    .build();
-        } catch (Exception e) {
-            logger.warn("unable to start GCE discovery service", e);
-            throw new IllegalArgumentException("unable to start GCE discovery service", e);
-        }
-
-        return this.client;
-    }
-
-    @Override
-    protected void doStart() throws ElasticsearchException {
-    }
-
-    @Override
-    protected void doStop() throws ElasticsearchException {
-        if (gceHttpTransport != null) {
-            try {
-                gceHttpTransport.shutdown();
-            } catch (IOException e) {
-                logger.warn("unable to shutdown GCE Http Transport", e);
-            }
-            gceHttpTransport = null;
-        }
-    }
-
-    @Override
-    protected void doClose() throws ElasticsearchException {
-    }
-}
diff --git a/plugins/cloud-gce/src/main/java/org/elasticsearch/cloud/gce/GceModule.java b/plugins/cloud-gce/src/main/java/org/elasticsearch/cloud/gce/GceModule.java
deleted file mode 100644
index 8db0dec..0000000
--- a/plugins/cloud-gce/src/main/java/org/elasticsearch/cloud/gce/GceModule.java
+++ /dev/null
@@ -1,38 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.gce;
-
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-
-public class GceModule extends AbstractModule {
-    // pkg private so tests can override with mock
-    static Class<? extends GceComputeService> computeServiceImpl = GceComputeServiceImpl.class;
-
-    public static Class<? extends GceComputeService> getComputeServiceImpl() {
-        return computeServiceImpl;
-    }
-
-    @Override
-    protected void configure() {
-        bind(GceComputeService.class).to(computeServiceImpl).asEagerSingleton();
-    }
-}
diff --git a/plugins/cloud-gce/src/main/java/org/elasticsearch/discovery/gce/GceDiscovery.java b/plugins/cloud-gce/src/main/java/org/elasticsearch/discovery/gce/GceDiscovery.java
deleted file mode 100755
index f20d1c7..0000000
--- a/plugins/cloud-gce/src/main/java/org/elasticsearch/discovery/gce/GceDiscovery.java
+++ /dev/null
@@ -1,49 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.gce;
-
-import org.elasticsearch.cluster.ClusterName;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.discovery.DiscoverySettings;
-import org.elasticsearch.discovery.zen.ZenDiscovery;
-import org.elasticsearch.discovery.zen.elect.ElectMasterService;
-import org.elasticsearch.discovery.zen.ping.ZenPingService;
-import org.elasticsearch.node.settings.NodeSettingsService;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportService;
-
-/**
- *
- */
-public class GceDiscovery extends ZenDiscovery {
-
-    public static final String GCE = "gce";
-
-    @Inject
-    public GceDiscovery(Settings settings, ClusterName clusterName, ThreadPool threadPool, TransportService transportService,
-                        ClusterService clusterService, NodeSettingsService nodeSettingsService, ZenPingService pingService,
-                        DiscoverySettings discoverySettings,
-                        ElectMasterService electMasterService) {
-        super(settings, clusterName, threadPool, transportService, clusterService, nodeSettingsService,
-                pingService, electMasterService, discoverySettings);
-    }
-}
diff --git a/plugins/cloud-gce/src/main/java/org/elasticsearch/discovery/gce/GceUnicastHostsProvider.java b/plugins/cloud-gce/src/main/java/org/elasticsearch/discovery/gce/GceUnicastHostsProvider.java
deleted file mode 100644
index 8feb9b8..0000000
--- a/plugins/cloud-gce/src/main/java/org/elasticsearch/discovery/gce/GceUnicastHostsProvider.java
+++ /dev/null
@@ -1,249 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.gce;
-
-import com.google.api.services.compute.model.AccessConfig;
-import com.google.api.services.compute.model.Instance;
-import com.google.api.services.compute.model.NetworkInterface;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.cloud.gce.GceComputeService;
-import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.component.AbstractComponent;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.network.NetworkAddress;
-import org.elasticsearch.common.network.NetworkService;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.transport.TransportAddress;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.discovery.zen.ping.unicast.UnicastHostsProvider;
-import org.elasticsearch.transport.TransportService;
-
-import java.io.IOException;
-import java.net.InetAddress;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.List;
-
-import static org.elasticsearch.cloud.gce.GceComputeService.Fields;
-
-/**
- *
- */
-public class GceUnicastHostsProvider extends AbstractComponent implements UnicastHostsProvider {
-
-    static final class Status {
-        private static final String TERMINATED = "TERMINATED";
-    }
-
-    private final GceComputeService gceComputeService;
-    private TransportService transportService;
-    private NetworkService networkService;
-
-    private final Version version;
-    private final String project;
-    private final String[] zones;
-    private final String[] tags;
-
-    private final TimeValue refreshInterval;
-    private long lastRefresh;
-    private List<DiscoveryNode> cachedDiscoNodes;
-
-    @Inject
-    public GceUnicastHostsProvider(Settings settings, GceComputeService gceComputeService,
-            TransportService transportService,
-            NetworkService networkService,
-            Version version) {
-        super(settings);
-        this.gceComputeService = gceComputeService;
-        this.transportService = transportService;
-        this.networkService = networkService;
-        this.version = version;
-
-        this.refreshInterval = settings.getAsTime(Fields.REFRESH, TimeValue.timeValueSeconds(0));
-        this.project = settings.get(Fields.PROJECT);
-        this.zones = settings.getAsArray(Fields.ZONE);
-
-        this.tags = settings.getAsArray(Fields.TAGS);
-        if (logger.isDebugEnabled()) {
-            logger.debug("using tags {}", Arrays.asList(this.tags));
-        }
-    }
-
-    /**
-     * We build the list of Nodes from GCE Management API
-     * Information can be cached using `plugins.refresh_interval` property if needed.
-     * Setting `plugins.refresh_interval` to `-1` will cause infinite caching.
-     * Setting `plugins.refresh_interval` to `0` will disable caching (default).
-     */
-    @Override
-    public List<DiscoveryNode> buildDynamicNodes() {
-        if (refreshInterval.millis() != 0) {
-            if (cachedDiscoNodes != null &&
-                    (refreshInterval.millis() < 0 || (System.currentTimeMillis() - lastRefresh) < refreshInterval.millis())) {
-                if (logger.isTraceEnabled()) logger.trace("using cache to retrieve node list");
-                return cachedDiscoNodes;
-            }
-            lastRefresh = System.currentTimeMillis();
-        }
-        logger.debug("start building nodes list using GCE API");
-
-        cachedDiscoNodes = new ArrayList<>();
-        String ipAddress = null;
-        try {
-            InetAddress inetAddress = networkService.resolvePublishHostAddress(null);
-            if (inetAddress != null) {
-                ipAddress = NetworkAddress.formatAddress(inetAddress);
-            }
-        } catch (IOException e) {
-            // We can't find the publish host address... Hmmm. Too bad :-(
-            // We won't simply filter it
-        }
-
-        try {
-            Collection<Instance> instances = gceComputeService.instances();
-
-            if (instances == null) {
-                logger.trace("no instance found for project [{}], zones [{}].", this.project, this.zones);
-                return cachedDiscoNodes;
-            }
-
-            for (Instance instance : instances) {
-                String name = instance.getName();
-                String type = instance.getMachineType();
-
-                String status = instance.getStatus();
-                logger.trace("gce instance {} with status {} found.", name, status);
-
-                // We don't want to connect to TERMINATED status instances
-                // See https://github.com/elasticsearch/elasticsearch-cloud-gce/issues/3
-                if (Status.TERMINATED.equals(status)) {
-                    logger.debug("node {} is TERMINATED. Ignoring", name);
-                    continue;
-                }
-
-                // see if we need to filter by tag
-                boolean filterByTag = false;
-                if (tags.length > 0) {
-                    logger.trace("start filtering instance {} with tags {}.", name, tags);
-                    if (instance.getTags() == null || instance.getTags().isEmpty()
-                            || instance.getTags().getItems() == null || instance.getTags().getItems().isEmpty()) {
-                        // If this instance have no tag, we filter it
-                        logger.trace("no tags for this instance but we asked for tags. {} won't be part of the cluster.", name);
-                        filterByTag = true;
-                    } else {
-                        // check that all tags listed are there on the instance
-                        logger.trace("comparing instance tags {} with tags filter {}.", instance.getTags().getItems(), tags);
-                        for (String tag : tags) {
-                            boolean found = false;
-                            for (String instancetag : instance.getTags().getItems()) {
-                                if (instancetag.equals(tag)) {
-                                    found = true;
-                                    break;
-                                }
-                            }
-                            if (!found) {
-                                filterByTag = true;
-                                break;
-                            }
-                        }
-                    }
-                }
-                if (filterByTag) {
-                    logger.trace("filtering out instance {} based tags {}, not part of {}", name, tags,
-                            instance.getTags() == null || instance.getTags().getItems() == null ? "" : instance.getTags());
-                    continue;
-                } else {
-                    logger.trace("instance {} with tags {} is added to discovery", name, tags);
-                }
-
-                String ip_public = null;
-                String ip_private = null;
-
-                List<NetworkInterface> interfaces = instance.getNetworkInterfaces();
-
-                for (NetworkInterface networkInterface : interfaces) {
-                    if (ip_public == null) {
-                        // Trying to get Public IP Address (For future use)
-                        if (networkInterface.getAccessConfigs() != null) {
-                            for (AccessConfig accessConfig : networkInterface.getAccessConfigs()) {
-                                if (Strings.hasText(accessConfig.getNatIP())) {
-                                    ip_public = accessConfig.getNatIP();
-                                    break;
-                                }
-                            }
-                        }
-                    }
-
-                    if (ip_private == null) {
-                        ip_private = networkInterface.getNetworkIP();
-                    }
-
-                    // If we have both public and private, we can stop here
-                    if (ip_private != null && ip_public != null) break;
-                }
-
-                try {
-                    if (ip_private.equals(ipAddress)) {
-                        // We found the current node.
-                        // We can ignore it in the list of DiscoveryNode
-                        logger.trace("current node found. Ignoring {} - {}", name, ip_private);
-                    } else {
-                        String address = ip_private;
-                        // Test if we have es_port metadata defined here
-                        if (instance.getMetadata() != null && instance.getMetadata().containsKey("es_port")) {
-                            Object es_port = instance.getMetadata().get("es_port");
-                            logger.trace("es_port is defined with {}", es_port);
-                            if (es_port instanceof String) {
-                                address = address.concat(":").concat((String) es_port);
-                            } else {
-                                // Ignoring other values
-                                logger.trace("es_port is instance of {}. Ignoring...", es_port.getClass().getName());
-                            }
-                        }
-
-                        // ip_private is a single IP Address. We need to build a TransportAddress from it
-                        // If user has set `es_port` metadata, we don't need to ping all ports
-                        // we only limit to 1 addresses, makes no sense to ping 100 ports
-                        TransportAddress[] addresses = transportService.addressesFromString(address, 1);
-
-                        for (TransportAddress transportAddress : addresses) {
-                            logger.trace("adding {}, type {}, address {}, transport_address {}, status {}", name, type,
-                                    ip_private, transportAddress, status);
-                            cachedDiscoNodes.add(new DiscoveryNode("#cloud-" + name + "-" + 0, transportAddress, version.minimumCompatibilityVersion()));
-                        }
-                    }
-                } catch (Exception e) {
-                    logger.warn("failed to add {}, address {}", e, name, ip_private);
-                }
-
-            }
-        } catch (Throwable e) {
-            logger.warn("Exception caught during discovery: {}", e, e.getMessage());
-        }
-
-        logger.debug("{} node(s) added", cachedDiscoNodes.size());
-        logger.debug("using dynamic discovery nodes {}", cachedDiscoNodes);
-
-        return cachedDiscoNodes;
-    }
-}
diff --git a/plugins/cloud-gce/src/main/java/org/elasticsearch/plugin/cloud/gce/CloudGcePlugin.java b/plugins/cloud-gce/src/main/java/org/elasticsearch/plugin/cloud/gce/CloudGcePlugin.java
deleted file mode 100644
index 0ff8d4e..0000000
--- a/plugins/cloud-gce/src/main/java/org/elasticsearch/plugin/cloud/gce/CloudGcePlugin.java
+++ /dev/null
@@ -1,128 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.cloud.gce;
-
-import org.elasticsearch.cloud.gce.GceComputeService;
-import org.elasticsearch.cloud.gce.GceModule;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.component.LifecycleComponent;
-import org.elasticsearch.common.inject.Module;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.Loggers;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.discovery.DiscoveryModule;
-import org.elasticsearch.discovery.gce.GceDiscovery;
-import org.elasticsearch.discovery.gce.GceUnicastHostsProvider;
-import org.elasticsearch.plugins.Plugin;
-
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.List;
-
-/**
- *
- */
-public class CloudGcePlugin extends Plugin {
-
-    private final Settings settings;
-    protected final ESLogger logger = Loggers.getLogger(CloudGcePlugin.class);
-
-    public CloudGcePlugin(Settings settings) {
-        this.settings = settings;
-    }
-
-    @Override
-    public String name() {
-        return "cloud-gce";
-    }
-
-    @Override
-    public String description() {
-        return "Cloud Google Compute Engine Plugin";
-    }
-
-    @Override
-    public Collection<Module> nodeModules() {
-        List<Module> modules = new ArrayList<>();
-        if (isDiscoveryAlive(settings, logger)) {
-            modules.add(new GceModule());
-        }
-        return modules;
-    }
-
-    @Override
-    public Collection<Class<? extends LifecycleComponent>> nodeServices() {
-        Collection<Class<? extends LifecycleComponent>> services = new ArrayList<>();
-        if (isDiscoveryAlive(settings, logger)) {
-            services.add(GceModule.getComputeServiceImpl());
-        }
-        return services;
-    }
-
-    public void onModule(DiscoveryModule discoveryModule) {
-        if (isDiscoveryAlive(settings, logger)) {
-            discoveryModule.addDiscoveryType("gce", GceDiscovery.class);
-            discoveryModule.addUnicastHostProvider(GceUnicastHostsProvider.class);
-        }
-    }
-
-    /**
-     * Check if discovery is meant to start
-     *
-     * @return true if we can start gce discovery features
-     */
-    public static boolean isDiscoveryAlive(Settings settings, ESLogger logger) {
-        // User set discovery.type: gce
-        if (GceDiscovery.GCE.equalsIgnoreCase(settings.get("discovery.type")) == false) {
-            logger.debug("discovery.type not set to {}", GceDiscovery.GCE);
-            return false;
-        }
-
-        if (checkProperty(GceComputeService.Fields.PROJECT, settings.get(GceComputeService.Fields.PROJECT), logger) == false ||
-                checkProperty(GceComputeService.Fields.ZONE, settings.getAsArray(GceComputeService.Fields.ZONE), logger) == false) {
-            logger.debug("one or more gce discovery settings are missing. " +
-                            "Check elasticsearch.yml file. Should have [{}] and [{}].",
-                    GceComputeService.Fields.PROJECT,
-                    GceComputeService.Fields.ZONE);
-            return false;
-        }
-
-        logger.trace("all required properties for gce discovery are set!");
-
-        return true;
-    }
-
-    private static boolean checkProperty(String name, String value, ESLogger logger) {
-        if (!Strings.hasText(value)) {
-            logger.warn("{} is not set.", name);
-            return false;
-        }
-        return true;
-    }
-
-    private static boolean checkProperty(String name, String[] values, ESLogger logger) {
-        if (values == null || values.length == 0) {
-            logger.warn("{} is not set.", name);
-            return false;
-        }
-        return true;
-    }
-
-}
diff --git a/plugins/cloud-gce/src/test/java/org/elasticsearch/cloud/gce/CloudGCERestIT.java b/plugins/cloud-gce/src/test/java/org/elasticsearch/cloud/gce/CloudGCERestIT.java
deleted file mode 100644
index e209655..0000000
--- a/plugins/cloud-gce/src/test/java/org/elasticsearch/cloud/gce/CloudGCERestIT.java
+++ /dev/null
@@ -1,49 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.gce;
-
-import com.carrotsearch.randomizedtesting.annotations.Name;
-import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
-import org.elasticsearch.plugin.cloud.gce.CloudGcePlugin;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.test.rest.ESRestTestCase;
-import org.elasticsearch.test.rest.RestTestCandidate;
-import org.elasticsearch.test.rest.parser.RestTestParseException;
-
-import java.io.IOException;
-import java.util.Collection;
-
-public class CloudGCERestIT extends ESRestTestCase {
-
-    @Override
-    protected Collection<Class<? extends Plugin>> nodePlugins() {
-        return pluginList(CloudGcePlugin.class);
-    }
-
-    public CloudGCERestIT(@Name("yaml") RestTestCandidate testCandidate) {
-        super(testCandidate);
-    }
-
-    @ParametersFactory
-    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
-        return ESRestTestCase.createParameters(0, 1);
-    }
-}
-
diff --git a/plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceComputeServiceMock.java b/plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceComputeServiceMock.java
deleted file mode 100644
index 1892297..0000000
--- a/plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceComputeServiceMock.java
+++ /dev/null
@@ -1,113 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.gce;
-
-import com.google.api.client.http.HttpTransport;
-import com.google.api.client.http.LowLevelHttpRequest;
-import com.google.api.client.http.LowLevelHttpResponse;
-import com.google.api.client.json.Json;
-import com.google.api.client.testing.http.MockHttpTransport;
-import com.google.api.client.testing.http.MockLowLevelHttpRequest;
-import com.google.api.client.testing.http.MockLowLevelHttpResponse;
-import org.elasticsearch.cloud.gce.GceComputeServiceImpl;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.Streams;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.util.Callback;
-
-import java.io.IOException;
-import java.io.InputStream;
-import java.net.URL;
-import java.security.GeneralSecurityException;
-
-/**
- *
- */
-public class GceComputeServiceMock extends GceComputeServiceImpl {
-
-    protected HttpTransport mockHttpTransport;
-
-    public GceComputeServiceMock(Settings settings) {
-        super(settings);
-        this.mockHttpTransport = configureMock();
-    }
-
-    @Override
-    protected HttpTransport getGceHttpTransport() throws GeneralSecurityException, IOException {
-        return this.mockHttpTransport;
-    }
-
-    protected HttpTransport configureMock() {
-        HttpTransport transport = new MockHttpTransport() {
-            @Override
-            public LowLevelHttpRequest buildRequest(String method, final String url) throws IOException {
-                return new MockLowLevelHttpRequest() {
-                    @Override
-                    public LowLevelHttpResponse execute() throws IOException {
-                        MockLowLevelHttpResponse response = new MockLowLevelHttpResponse();
-                        response.setStatusCode(200);
-                        response.setContentType(Json.MEDIA_TYPE);
-                        if (url.equals(TOKEN_SERVER_ENCODED_URL)) {
-                            logger.info("--> Simulate GCE Auth response for [{}]", url);
-                            response.setContent(readGoogleInternalJsonResponse(url));
-                        } else {
-                            logger.info("--> Simulate GCE API response for [{}]", url);
-                            response.setContent(readGoogleApiJsonResponse(url));
-                        }
-
-                        return response;
-                    }
-                };
-            }
-        };
-
-        return transport;
-    }
-
-    private String readGoogleInternalJsonResponse(String url) throws IOException {
-        return readJsonResponse(url, "http://metadata.google.internal/");
-    }
-
-    private String readGoogleApiJsonResponse(String url) throws IOException {
-        return readJsonResponse(url, "https://www.googleapis.com/");
-    }
-
-    private String readJsonResponse(String url, String urlRoot) throws IOException {
-        // We extract from the url the mock file path we want to use
-        String mockFileName = Strings.replace(url, urlRoot, "") + ".json";
-
-        logger.debug("--> read mock file from [{}]", mockFileName);
-        URL resource = GceComputeServiceMock.class.getResource(mockFileName);
-        try (InputStream is = resource.openStream()) {
-            final StringBuilder sb = new StringBuilder();
-            Streams.readAllLines(is, new Callback<String>() {
-                @Override
-                public void handle(String s) {
-                    sb.append(s).append("\n");
-                }
-            });
-            String response = sb.toString();
-            logger.trace("{}", response);
-            return response;
-        } catch (IOException e) {
-            throw e;
-        }
-    }
-}
diff --git a/plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoverySettingsTests.java b/plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoverySettingsTests.java
deleted file mode 100644
index 90b331d..0000000
--- a/plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoverySettingsTests.java
+++ /dev/null
@@ -1,78 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.gce;
-
-import org.elasticsearch.cloud.gce.GceModule;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.plugin.cloud.gce.CloudGcePlugin;
-import org.elasticsearch.test.ESTestCase;
-
-import static org.hamcrest.Matchers.is;
-
-public class GceDiscoverySettingsTests extends ESTestCase {
-    public void testDiscoveryReady() {
-        Settings settings = Settings.builder()
-                .put("discovery.type", "gce")
-                .put("cloud.gce.project_id", "gce_id")
-                .putArray("cloud.gce.zone", "gce_zones_1", "gce_zones_2")
-                .build();
-
-        boolean discoveryReady = CloudGcePlugin.isDiscoveryAlive(settings, logger);
-        assertThat(discoveryReady, is(true));
-    }
-
-    public void testDiscoveryNotReady() {
-        Settings settings = Settings.EMPTY;
-        boolean discoveryReady = CloudGcePlugin.isDiscoveryAlive(settings, logger);
-        assertThat(discoveryReady, is(false));
-
-        settings = Settings.builder()
-                .put("discovery.type", "gce")
-                .build();
-
-        discoveryReady = CloudGcePlugin.isDiscoveryAlive(settings, logger);
-        assertThat(discoveryReady, is(false));
-
-        settings = Settings.builder()
-                .put("discovery.type", "gce")
-                .put("cloud.gce.project_id", "gce_id")
-                .build();
-
-        discoveryReady = CloudGcePlugin.isDiscoveryAlive(settings, logger);
-        assertThat(discoveryReady, is(false));
-
-
-        settings = Settings.builder()
-                .put("discovery.type", "gce")
-                .putArray("cloud.gce.zone", "gce_zones_1", "gce_zones_2")
-                .build();
-
-        discoveryReady = CloudGcePlugin.isDiscoveryAlive(settings, logger);
-        assertThat(discoveryReady, is(false));
-
-        settings = Settings.builder()
-                .put("cloud.gce.project_id", "gce_id")
-                .putArray("cloud.gce.zone", "gce_zones_1", "gce_zones_2")
-                .build();
-
-        discoveryReady = CloudGcePlugin.isDiscoveryAlive(settings, logger);
-        assertThat(discoveryReady, is(false));
-    }
-}
diff --git a/plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoveryTests.java b/plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoveryTests.java
deleted file mode 100644
index b18cca1..0000000
--- a/plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoveryTests.java
+++ /dev/null
@@ -1,217 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.gce;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.cloud.gce.GceComputeService;
-import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.network.NetworkService;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.transport.MockTransportService;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.local.LocalTransport;
-import org.junit.*;
-
-import java.util.List;
-import java.util.Locale;
-
-import static org.hamcrest.Matchers.hasSize;
-import static org.hamcrest.Matchers.is;
-
-/**
- * This test class uses a GCE HTTP Mock system which allows to simulate JSON Responses.
- *
- * To implement a new test you'll need to create an `instances.json` file which contains expected response
- * for a given project-id and zone under the src/test/resources/org/elasticsearch/discovery/gce with dir name:
- *
- * compute/v1/projects/[project-id]/zones/[zone]
- *
- * By default, project-id is the test method name, lowercase.
- *
- * For example, if you create a test `myNewAwesomeTest` with following settings:
- *
- * Settings nodeSettings = Settings.builder()
- *  .put(GceComputeService.Fields.PROJECT, projectName)
- *  .put(GceComputeService.Fields.ZONE, "europe-west1-b")
- *  .build();
- *
- *  You need to create a file under `src/test/resources/org/elasticsearch/discovery/gce/` named:
- *
- *  compute/v1/projects/mynewawesometest/zones/europe-west1-b/instances.json
- *
- */
-public class GceDiscoveryTests extends ESTestCase {
-
-    protected static ThreadPool threadPool;
-    protected MockTransportService transportService;
-    protected GceComputeService mock;
-    protected String projectName;
-
-    @BeforeClass
-    public static void createThreadPool() {
-        threadPool = new ThreadPool(GceDiscoveryTests.class.getName());
-    }
-
-    @AfterClass
-    public static void stopThreadPool() {
-        if (threadPool !=null) {
-            threadPool.shutdownNow();
-            threadPool = null;
-        }
-    }
-
-    @Before
-    public void setProjectName() {
-        projectName = getTestName().toLowerCase(Locale.ROOT);
-    }
-
-    @Before
-    public void createTransportService() {
-        transportService = new MockTransportService(
-                Settings.EMPTY,
-                new LocalTransport(Settings.EMPTY, threadPool, Version.CURRENT, new NamedWriteableRegistry()), threadPool);
-    }
-
-    @After
-    public void stopGceComputeService() {
-        if (mock != null) {
-            mock.stop();
-        }
-    }
-
-    protected List<DiscoveryNode> buildDynamicNodes(GceComputeService gceComputeService, Settings nodeSettings) {
-        GceUnicastHostsProvider provider = new GceUnicastHostsProvider(nodeSettings, gceComputeService,
-                transportService, new NetworkService(Settings.EMPTY), Version.CURRENT);
-
-        List<DiscoveryNode> discoveryNodes = provider.buildDynamicNodes();
-        logger.info("--> nodes found: {}", discoveryNodes);
-        return discoveryNodes;
-    }
-
-    @Test
-    public void nodesWithDifferentTagsAndNoTagSet() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(2));
-    }
-
-    @Test
-    public void nodesWithDifferentTagsAndOneTagSet() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
-                .putArray(GceComputeService.Fields.TAGS, "elasticsearch")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(1));
-        assertThat(discoveryNodes.get(0).getId(), is("#cloud-test2-0"));
-    }
-
-    @Test
-    public void nodesWithDifferentTagsAndTwoTagSet() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
-                .putArray(GceComputeService.Fields.TAGS, "elasticsearch", "dev")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(1));
-        assertThat(discoveryNodes.get(0).getId(), is("#cloud-test2-0"));
-    }
-
-    @Test
-    public void nodesWithSameTagsAndNoTagSet() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(2));
-    }
-
-    @Test
-    public void nodesWithSameTagsAndOneTagSet() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
-                .putArray(GceComputeService.Fields.TAGS, "elasticsearch")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(2));
-    }
-
-    @Test
-    public void nodesWithSameTagsAndTwoTagsSet() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
-                .putArray(GceComputeService.Fields.TAGS, "elasticsearch", "dev")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(2));
-    }
-
-    @Test
-    public void multipleZonesAndTwoNodesInSameZone() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .putArray(GceComputeService.Fields.ZONE, "us-central1-a", "europe-west1-b")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(2));
-    }
-
-    @Test
-    public void multipleZonesAndTwoNodesInDifferentZones() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .putArray(GceComputeService.Fields.ZONE, "us-central1-a", "europe-west1-b")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(2));
-    }
-
-    /**
-     * For issue https://github.com/elastic/elasticsearch-cloud-gce/issues/43
-     */
-    @Test
-    public void zeroNode43() {
-        Settings nodeSettings = Settings.builder()
-                .put(GceComputeService.Fields.PROJECT, projectName)
-                .putArray(GceComputeService.Fields.ZONE, "us-central1-a", "us-central1-b")
-                .build();
-        mock = new GceComputeServiceMock(nodeSettings);
-        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
-        assertThat(discoveryNodes, hasSize(0));
-    }
-}
diff --git a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesindifferentzones/zones/europe-west1-b/instances.json b/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesindifferentzones/zones/europe-west1-b/instances.json
deleted file mode 100644
index 049e0e1..0000000
--- a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesindifferentzones/zones/europe-west1-b/instances.json
+++ /dev/null
@@ -1,36 +0,0 @@
-{
-  "id": "dummy",
-  "items":[
-    {
-      "description": "ES Node 1",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test1",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "elasticsearch",
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    }
-  ]
-}
diff --git a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesindifferentzones/zones/us-central1-a/instances.json b/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesindifferentzones/zones/us-central1-a/instances.json
deleted file mode 100644
index 7e1e5d5..0000000
--- a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesindifferentzones/zones/us-central1-a/instances.json
+++ /dev/null
@@ -1,36 +0,0 @@
-{
-  "id": "dummy",
-  "items":[
-    {
-      "description": "ES Node 2",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test2",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "elasticsearch",
-          "dev"
-        ]
-      },
-      "zone": "us-central1-a"
-    }
-  ]
-}
diff --git a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesinsamezone/zones/europe-west1-b/instances.json b/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesinsamezone/zones/europe-west1-b/instances.json
deleted file mode 100644
index 78de693..0000000
--- a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesinsamezone/zones/europe-west1-b/instances.json
+++ /dev/null
@@ -1,67 +0,0 @@
-{
-  "id": "dummy",
-  "items":[
-    {
-      "description": "ES Node 1",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test1",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "elasticsearch",
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    },
-    {
-      "description": "ES Node 2",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test2",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "elasticsearch",
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    }
-  ]
-}
diff --git a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesinsamezone/zones/us-central1-a/instances.json b/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesinsamezone/zones/us-central1-a/instances.json
deleted file mode 100644
index 54c3836..0000000
--- a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesinsamezone/zones/us-central1-a/instances.json
+++ /dev/null
@@ -1,5 +0,0 @@
-{
-  "id": "dummy",
-  "items":[
-  ]
-}
diff --git a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandnotagset/zones/europe-west1-b/instances.json b/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandnotagset/zones/europe-west1-b/instances.json
deleted file mode 100644
index 1ca810c..0000000
--- a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandnotagset/zones/europe-west1-b/instances.json
+++ /dev/null
@@ -1,66 +0,0 @@
-{
-  "id": "dummy",
-  "items":[
-    {
-      "description": "ES Node 1",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test1",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    },
-    {
-      "description": "ES Node 2",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test2",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "elasticsearch",
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    }
-  ]
-}
diff --git a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandonetagset/zones/europe-west1-b/instances.json b/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandonetagset/zones/europe-west1-b/instances.json
deleted file mode 100644
index 1ca810c..0000000
--- a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandonetagset/zones/europe-west1-b/instances.json
+++ /dev/null
@@ -1,66 +0,0 @@
-{
-  "id": "dummy",
-  "items":[
-    {
-      "description": "ES Node 1",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test1",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    },
-    {
-      "description": "ES Node 2",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test2",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "elasticsearch",
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    }
-  ]
-}
diff --git a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandtwotagset/zones/europe-west1-b/instances.json b/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandtwotagset/zones/europe-west1-b/instances.json
deleted file mode 100644
index 1ca810c..0000000
--- a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandtwotagset/zones/europe-west1-b/instances.json
+++ /dev/null
@@ -1,66 +0,0 @@
-{
-  "id": "dummy",
-  "items":[
-    {
-      "description": "ES Node 1",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test1",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    },
-    {
-      "description": "ES Node 2",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test2",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "elasticsearch",
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    }
-  ]
-}
diff --git a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandnotagset/zones/europe-west1-b/instances.json b/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandnotagset/zones/europe-west1-b/instances.json
deleted file mode 100644
index 78de693..0000000
--- a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandnotagset/zones/europe-west1-b/instances.json
+++ /dev/null
@@ -1,67 +0,0 @@
-{
-  "id": "dummy",
-  "items":[
-    {
-      "description": "ES Node 1",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test1",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "elasticsearch",
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    },
-    {
-      "description": "ES Node 2",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test2",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "elasticsearch",
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    }
-  ]
-}
diff --git a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandonetagset/zones/europe-west1-b/instances.json b/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandonetagset/zones/europe-west1-b/instances.json
deleted file mode 100644
index 78de693..0000000
--- a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandonetagset/zones/europe-west1-b/instances.json
+++ /dev/null
@@ -1,67 +0,0 @@
-{
-  "id": "dummy",
-  "items":[
-    {
-      "description": "ES Node 1",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test1",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "elasticsearch",
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    },
-    {
-      "description": "ES Node 2",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test2",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "elasticsearch",
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    }
-  ]
-}
diff --git a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandtwotagsset/zones/europe-west1-b/instances.json b/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandtwotagsset/zones/europe-west1-b/instances.json
deleted file mode 100644
index 78de693..0000000
--- a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandtwotagsset/zones/europe-west1-b/instances.json
+++ /dev/null
@@ -1,67 +0,0 @@
-{
-  "id": "dummy",
-  "items":[
-    {
-      "description": "ES Node 1",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test1",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "elasticsearch",
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    },
-    {
-      "description": "ES Node 2",
-      "id": "9309873766428965105",
-      "kind": "compute#instance",
-      "machineType": "n1-standard-1",
-      "name": "test2",
-      "networkInterfaces": [
-        {
-          "accessConfigs": [
-            {
-              "kind": "compute#accessConfig",
-              "name": "External NAT",
-              "natIP": "104.155.13.147",
-              "type": "ONE_TO_ONE_NAT"
-            }
-          ],
-          "name": "nic0",
-          "network": "default",
-          "networkIP": "10.240.79.59"
-        }
-      ],
-      "status": "RUNNING",
-      "tags": {
-        "fingerprint": "xA6QJb-rGtg=",
-        "items": [
-          "elasticsearch",
-          "dev"
-        ]
-      },
-      "zone": "europe-west1-b"
-    }
-  ]
-}
diff --git a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/zeronode43/zones/us-central1-a/instances.json b/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/zeronode43/zones/us-central1-a/instances.json
deleted file mode 100644
index 54c3836..0000000
--- a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/zeronode43/zones/us-central1-a/instances.json
+++ /dev/null
@@ -1,5 +0,0 @@
-{
-  "id": "dummy",
-  "items":[
-  ]
-}
diff --git a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/zeronode43/zones/us-central1-b/instances.json b/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/zeronode43/zones/us-central1-b/instances.json
deleted file mode 100644
index 54c3836..0000000
--- a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/zeronode43/zones/us-central1-b/instances.json
+++ /dev/null
@@ -1,5 +0,0 @@
-{
-  "id": "dummy",
-  "items":[
-  ]
-}
diff --git a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/service-accounts/default/token.json b/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/service-accounts/default/token.json
deleted file mode 100644
index b338f61..0000000
--- a/plugins/cloud-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/service-accounts/default/token.json
+++ /dev/null
@@ -1,4 +0,0 @@
-{
-  "access_token":"1/fFAGRNJru1FTz70BzhT3Zg",
-  "token_type":"Bearer"
-}
diff --git a/plugins/cloud-gce/src/test/resources/rest-api-spec/test/cloud_gce/10_basic.yaml b/plugins/cloud-gce/src/test/resources/rest-api-spec/test/cloud_gce/10_basic.yaml
deleted file mode 100644
index 9cff52e..0000000
--- a/plugins/cloud-gce/src/test/resources/rest-api-spec/test/cloud_gce/10_basic.yaml
+++ /dev/null
@@ -1,14 +0,0 @@
-# Integration tests for Cloud GCE components
-#
-"Cloud GCE loaded":
-    - do:
-        cluster.state: {}
-
-    # Get master node id
-    - set: { master_node: master }
-
-    - do:
-        nodes.info: {}
-
-    - match:  { nodes.$master.plugins.0.name: cloud-gce  }
-    - match:  { nodes.$master.plugins.0.jvm: true  }
diff --git a/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequest.java b/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequest.java
index e3faeb1..4c29e7c 100644
--- a/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequest.java
+++ b/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequest.java
@@ -19,19 +19,27 @@
 
 package org.elasticsearch.action.deletebyquery;
 
+import org.elasticsearch.ElasticsearchGenerationException;
 import org.elasticsearch.action.ActionRequest;
 import org.elasticsearch.action.ActionRequestValidationException;
 import org.elasticsearch.action.IndicesRequest;
 import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.action.support.QuerySourceBuilder;
+import org.elasticsearch.client.Requests;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.index.query.QueryBuilder;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.search.Scroll;
 
 import java.io.IOException;
 import java.util.Arrays;
+import java.util.Map;
 
 import static org.elasticsearch.action.ValidateActions.addValidationError;
 import static org.elasticsearch.search.Scroll.readScroll;
@@ -69,7 +77,7 @@ public class DeleteByQueryRequest extends ActionRequest<DeleteByQueryRequest> im
 
     private String[] types = Strings.EMPTY_ARRAY;
 
-    private QueryBuilder<?> query;
+    private BytesReference source;
 
     private String routing;
 
@@ -93,7 +101,7 @@ public class DeleteByQueryRequest extends ActionRequest<DeleteByQueryRequest> im
     @Override
     public ActionRequestValidationException validate() {
         ActionRequestValidationException validationException = null;
-        if (query == null) {
+        if (source == null) {
             validationException = addValidationError("source is missing", validationException);
         }
         return validationException;
@@ -132,12 +140,45 @@ public class DeleteByQueryRequest extends ActionRequest<DeleteByQueryRequest> im
         return this;
     }
 
-    public QueryBuilder<?> query() {
-        return query;
+    public BytesReference source() {
+        return source;
     }
 
-    public DeleteByQueryRequest query(QueryBuilder<?> queryBuilder) {
-        this.query = queryBuilder;
+    public DeleteByQueryRequest source(QuerySourceBuilder sourceBuilder) {
+        this.source = sourceBuilder.buildAsBytes(Requests.CONTENT_TYPE);
+        return this;
+    }
+
+    public DeleteByQueryRequest source(Map<String,?> querySource) {
+        try {
+            XContentBuilder builder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
+            builder.map(querySource);
+            return source(builder);
+        } catch (IOException e) {
+            throw new ElasticsearchGenerationException("Failed to generate [" + querySource + "]", e);
+        }
+    }
+
+    public DeleteByQueryRequest source(XContentBuilder builder) {
+        this.source = builder.bytes();
+        return this;
+    }
+
+    public DeleteByQueryRequest source(String querySource) {
+        this.source = new BytesArray(querySource);
+        return this;
+    }
+
+    public DeleteByQueryRequest source(byte[] querySource) {
+        return source(querySource, 0, querySource.length);
+    }
+
+    public DeleteByQueryRequest source(byte[] querySource, int offset, int length) {
+        return source(new BytesArray(querySource, offset, length));
+    }
+
+    public DeleteByQueryRequest source(BytesReference querySource) {
+        this.source = querySource;
         return this;
     }
 
@@ -208,7 +249,7 @@ public class DeleteByQueryRequest extends ActionRequest<DeleteByQueryRequest> im
         indices = in.readStringArray();
         indicesOptions = IndicesOptions.readIndicesOptions(in);
         types = in.readStringArray();
-        query = in.readQuery();
+        source = in.readBytesReference();
         routing = in.readOptionalString();
         size = in.readVInt();
         if (in.readBoolean()) {
@@ -225,7 +266,7 @@ public class DeleteByQueryRequest extends ActionRequest<DeleteByQueryRequest> im
         out.writeStringArray(indices);
         indicesOptions.writeIndicesOptions(out);
         out.writeStringArray(types);
-        out.writeQuery(query);
+        out.writeBytesReference(source);
         out.writeOptionalString(routing);
         out.writeVInt(size);
         out.writeOptionalStreamable(scroll);
@@ -234,11 +275,12 @@ public class DeleteByQueryRequest extends ActionRequest<DeleteByQueryRequest> im
 
     @Override
     public String toString() {
-        return "delete-by-query indices:" + Arrays.toString(indices) +
-                ", types:" + Arrays.toString(types) +
-                ", size:" + size +
-                ", timeout:" + timeout +
-                ", routing:" + routing +
-                ", query:" + query.toString();
+        String sSource = "_na_";
+        try {
+            sSource = XContentHelper.convertToJson(source, false);
+        } catch (Exception e) {
+            // ignore
+        }
+        return "delete-by-query [" + Arrays.toString(indices) + "][" + Arrays.toString(types) + "], source[" + sSource + "]";
     }
 }
diff --git a/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequestBuilder.java b/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequestBuilder.java
index 7560e1e..d30cfaa 100644
--- a/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequestBuilder.java
+++ b/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequestBuilder.java
@@ -20,17 +20,25 @@
 package org.elasticsearch.action.deletebyquery;
 
 import org.elasticsearch.action.ActionRequestBuilder;
+import org.elasticsearch.action.ListenableActionFuture;
 import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.client.ElasticsearchClient;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.query.QueryBuilder;
 
+import java.util.Map;
+
 /**
  * Creates a new {@link DeleteByQueryRequestBuilder}
  * @see DeleteByQueryRequest
  */
 public class DeleteByQueryRequestBuilder extends ActionRequestBuilder<DeleteByQueryRequest, DeleteByQueryResponse, DeleteByQueryRequestBuilder> {
 
+    private QuerySourceBuilder sourceBuilder;
+
     public DeleteByQueryRequestBuilder(ElasticsearchClient client, DeleteByQueryAction action) {
         super(client, action, new DeleteByQueryRequest());
     }
@@ -56,11 +64,26 @@ public class DeleteByQueryRequestBuilder extends ActionRequestBuilder<DeleteByQu
      * @see org.elasticsearch.index.query.QueryBuilders
      */
     public DeleteByQueryRequestBuilder setQuery(QueryBuilder<?> queryBuilder) {
-        request.query(queryBuilder);
+        sourceBuilder().setQuery(queryBuilder);
+        return this;
+    }
+
+    /**
+     * The query binary used to delete documents.
+     */
+    public DeleteByQueryRequestBuilder setQuery(BytesReference queryBinary) {
+        sourceBuilder().setQuery(queryBinary);
         return this;
     }
 
     /**
+     * Constructs a new builder with a raw search query.
+     */
+    public DeleteByQueryRequestBuilder setQuery(XContentBuilder query) {
+        return setQuery(query.bytes());
+    }
+
+    /**
      * A comma separated list of routing values to control the shards the action will be executed on.
      */
     public DeleteByQueryRequestBuilder setRouting(String routing) {
@@ -77,6 +100,47 @@ public class DeleteByQueryRequestBuilder extends ActionRequestBuilder<DeleteByQu
     }
 
     /**
+     * The source to execute. It is preferable to use either {@link #setSource(byte[])}
+     * or {@link #setQuery(QueryBuilder)}.
+     */
+    public DeleteByQueryRequestBuilder setSource(String source) {
+        request().source(source);
+        return this;
+    }
+
+    /**
+     * The source to execute in the form of a map.
+     */
+    public DeleteByQueryRequestBuilder setSource(Map<String, Object> source) {
+        request().source(source);
+        return this;
+    }
+
+    /**
+     * The source to execute in the form of a builder.
+     */
+    public DeleteByQueryRequestBuilder setSource(XContentBuilder builder) {
+        request().source(builder);
+        return this;
+    }
+
+    /**
+     * The source to execute.
+     */
+    public DeleteByQueryRequestBuilder setSource(byte[] source) {
+        request().source(source);
+        return this;
+    }
+
+    /**
+     * The source to execute.
+     */
+    public DeleteByQueryRequestBuilder setSource(BytesReference source) {
+        request().source(source);
+        return this;
+    }
+
+    /**
      * An optional timeout to control how long the delete by query is allowed to take.
      */
     public DeleteByQueryRequestBuilder setTimeout(TimeValue timeout) {
@@ -100,4 +164,19 @@ public class DeleteByQueryRequestBuilder extends ActionRequestBuilder<DeleteByQu
         return this;
     }
 
+    @Override
+    public ListenableActionFuture<DeleteByQueryResponse> execute() {
+        if (sourceBuilder != null) {
+            request.source(sourceBuilder);
+        }
+        return super.execute();
+    }
+
+    private QuerySourceBuilder sourceBuilder() {
+        if (sourceBuilder == null) {
+            sourceBuilder = new QuerySourceBuilder();
+        }
+        return sourceBuilder;
+    }
+
 }
diff --git a/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java b/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java
index 83a3015..252befd 100644
--- a/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java
+++ b/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java
@@ -27,13 +27,7 @@ import org.elasticsearch.action.bulk.BulkRequest;
 import org.elasticsearch.action.bulk.BulkResponse;
 import org.elasticsearch.action.delete.DeleteRequest;
 import org.elasticsearch.action.delete.DeleteResponse;
-import org.elasticsearch.action.search.ClearScrollResponse;
-import org.elasticsearch.action.search.SearchRequest;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.action.search.SearchScrollRequest;
-import org.elasticsearch.action.search.ShardSearchFailure;
-import org.elasticsearch.action.search.TransportSearchAction;
-import org.elasticsearch.action.search.TransportSearchScrollAction;
+import org.elasticsearch.action.search.*;
 import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.HandledTransportAction;
 import org.elasticsearch.client.Client;
@@ -48,9 +42,7 @@ import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
-import java.util.ArrayList;
 import java.util.HashMap;
-import java.util.List;
 import java.util.Map;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicLong;
@@ -115,11 +107,9 @@ public class TransportDeleteByQueryAction extends HandledTransportAction<DeleteB
                     scanRequest.routing(request.routing());
                 }
 
-                List<String> fields = new ArrayList<>();
-                fields.add("_routing");
-                fields.add("_parent");
                 SearchSourceBuilder source = new SearchSourceBuilder()
-.query(request.query()).fields(fields)
+                        .query(request.source())
+                        .fields("_routing", "_parent")
                         .sort("_doc") // important for performance
                         .fetchSource(false)
                         .version(true);
diff --git a/plugins/delete-by-query/src/main/java/org/elasticsearch/rest/action/deletebyquery/RestDeleteByQueryAction.java b/plugins/delete-by-query/src/main/java/org/elasticsearch/rest/action/deletebyquery/RestDeleteByQueryAction.java
index aa2fbcc..251953d 100644
--- a/plugins/delete-by-query/src/main/java/org/elasticsearch/rest/action/deletebyquery/RestDeleteByQueryAction.java
+++ b/plugins/delete-by-query/src/main/java/org/elasticsearch/rest/action/deletebyquery/RestDeleteByQueryAction.java
@@ -22,15 +22,11 @@ package org.elasticsearch.rest.action.deletebyquery;
 import org.elasticsearch.action.deletebyquery.DeleteByQueryRequest;
 import org.elasticsearch.action.deletebyquery.DeleteByQueryResponse;
 import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.rest.BaseRestHandler;
 import org.elasticsearch.rest.RestChannel;
 import org.elasticsearch.rest.RestController;
@@ -38,8 +34,6 @@ import org.elasticsearch.rest.RestRequest;
 import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestToXContentListener;
 
-import java.io.IOException;
-
 import static org.elasticsearch.action.deletebyquery.DeleteByQueryAction.INSTANCE;
 import static org.elasticsearch.rest.RestRequest.Method.DELETE;
 
@@ -48,19 +42,15 @@ import static org.elasticsearch.rest.RestRequest.Method.DELETE;
  */
 public class RestDeleteByQueryAction extends BaseRestHandler {
 
-    private IndicesQueriesRegistry indicesQueriesRegistry;
-
     @Inject
-    public RestDeleteByQueryAction(Settings settings, RestController controller, Client client,
-            IndicesQueriesRegistry indicesQueriesRegistry) {
+    public RestDeleteByQueryAction(Settings settings, RestController controller, Client client) {
         super(settings, controller, client);
-        this.indicesQueriesRegistry = indicesQueriesRegistry;
         controller.registerHandler(DELETE, "/{index}/_query", this);
         controller.registerHandler(DELETE, "/{index}/{type}/_query", this);
     }
 
     @Override
-    public void handleRequest(final RestRequest request, final RestChannel channel, final Client client) throws IOException {
+    public void handleRequest(final RestRequest request, final RestChannel channel, final Client client) {
         DeleteByQueryRequest delete = new DeleteByQueryRequest(Strings.splitStringByCommaToArray(request.param("index")));
         delete.indicesOptions(IndicesOptions.fromRequest(request, delete.indicesOptions()));
         delete.routing(request.param("routing"));
@@ -68,23 +58,15 @@ public class RestDeleteByQueryAction extends BaseRestHandler {
             delete.timeout(request.paramAsTime("timeout", null));
         }
         if (request.hasContent()) {
-            XContentParser requestParser = XContentFactory.xContent(request.content()).createParser(request.content());
-            QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
-            context.reset(requestParser);
-            final QueryBuilder<?> builder = context.parseInnerQueryBuilder();
-            delete.query(builder);
+            delete.source(request.content());
         } else {
             String source = request.param("source");
             if (source != null) {
-                XContentParser requestParser = XContentFactory.xContent(source).createParser(source);
-                QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
-                context.reset(requestParser);
-                final QueryBuilder<?> builder = context.parseInnerQueryBuilder();
-                delete.query(builder);
+                delete.source(source);
             } else {
-                QueryBuilder<?> queryBuilder = RestActions.urlParamsToQueryBuilder(request);
-                if (queryBuilder != null) {
-                    delete.query(queryBuilder);
+                QuerySourceBuilder querySourceBuilder = RestActions.parseQuerySource(request);
+                if (querySourceBuilder != null) {
+                    delete.source(querySourceBuilder);
                 }
             }
         }
diff --git a/plugins/delete-by-query/src/test/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryActionTests.java b/plugins/delete-by-query/src/test/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryActionTests.java
index 853b6cf..c9d3f44 100644
--- a/plugins/delete-by-query/src/test/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryActionTests.java
+++ b/plugins/delete-by-query/src/test/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryActionTests.java
@@ -42,8 +42,7 @@ import org.junit.Test;
 import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
 import static org.elasticsearch.index.query.QueryBuilders.rangeQuery;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.*;
 
 public class TransportDeleteByQueryActionTests extends ESSingleNodeTestCase {
 
@@ -60,6 +59,20 @@ public class TransportDeleteByQueryActionTests extends ESSingleNodeTestCase {
     }
 
     @Test
+    public void testExecuteScanFailsOnMalformedQuery() {
+        createIndex("test");
+
+        DeleteByQueryRequest delete = new DeleteByQueryRequest().indices(new String[]{"test"}).source("{...}");
+        TestActionListener listener = new TestActionListener();
+
+        newAsyncAction(delete, listener).executeScan();
+        waitForCompletion("scan request should fail on malformed query", listener);
+
+        assertFailure(listener, "all shards failed");
+        assertSearchContextsClosed();
+    }
+
+    @Test
     public void testExecuteScan() {
         createIndex("test");
         final int numDocs = randomIntBetween(1, 200);
@@ -70,7 +83,7 @@ public class TransportDeleteByQueryActionTests extends ESSingleNodeTestCase {
         assertHitCount(client().prepareCount("test").get(), numDocs);
 
         final long limit = randomIntBetween(0, numDocs);
-        DeleteByQueryRequest delete = new DeleteByQueryRequest().indices(new String[]{"test"}).query(boolQuery().must(rangeQuery("num").lte(limit)));
+        DeleteByQueryRequest delete = new DeleteByQueryRequest().indices(new String[]{"test"}).source(boolQuery().must(rangeQuery("num").lte(limit)).buildAsBytes());
         TestActionListener listener = new TestActionListener();
 
         newAsyncAction(delete, listener).executeScan();
@@ -206,7 +219,7 @@ public class TransportDeleteByQueryActionTests extends ESSingleNodeTestCase {
         assertTrue(Strings.hasText(scrollId));
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(limit));
 
-        DeleteByQueryRequest delete = new DeleteByQueryRequest().indices(new String[]{"test"}).size(100).query(boolQuery().must(rangeQuery("num").lte(limit)));
+        DeleteByQueryRequest delete = new DeleteByQueryRequest().indices(new String[]{"test"}).size(100).source(boolQuery().must(rangeQuery("num").lte(limit)).buildAsBytes());
         TestActionListener listener = new TestActionListener();
 
         newAsyncAction(delete, listener).executeScroll(searchResponse.getScrollId());
diff --git a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/network/Ec2NameResolver.java b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/network/Ec2NameResolver.java
index f04d3ec..3788f82 100755
--- a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/network/Ec2NameResolver.java
+++ b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/network/Ec2NameResolver.java
@@ -91,31 +91,25 @@ public class Ec2NameResolver extends AbstractComponent implements CustomNameReso
      * @return the appropriate host resolved from ec2 meta-data, or null if it cannot be obtained.
      * @see CustomNameResolver#resolveIfPossible(String)
      */
-    public InetAddress[] resolve(Ec2HostnameType type, boolean warnOnFailure) {
-        URLConnection urlConnection = null;
+    public InetAddress[] resolve(Ec2HostnameType type) throws IOException {
         InputStream in = null;
+        String metadataUrl = AwsEc2ServiceImpl.EC2_METADATA_URL + type.ec2Name;
         try {
-            URL url = new URL(AwsEc2ServiceImpl.EC2_METADATA_URL + type.ec2Name);
+            URL url = new URL(metadataUrl);
             logger.debug("obtaining ec2 hostname from ec2 meta-data url {}", url);
-            urlConnection = url.openConnection();
+            URLConnection urlConnection = url.openConnection();
             urlConnection.setConnectTimeout(2000);
             in = urlConnection.getInputStream();
             BufferedReader urlReader = new BufferedReader(new InputStreamReader(in, StandardCharsets.UTF_8));
 
             String metadataResult = urlReader.readLine();
             if (metadataResult == null || metadataResult.length() == 0) {
-                logger.error("no ec2 metadata returned from {}", url);
-                return null;
+                throw new IOException("no gce metadata returned from [" + url + "] for [" + type.configName + "]");
             }
             // only one address: because we explicitly ask for only one via the Ec2HostnameType
             return new InetAddress[] { InetAddress.getByName(metadataResult) };
         } catch (IOException e) {
-            if (warnOnFailure) {
-                logger.warn("failed to get metadata for [" + type.configName + "]", e);
-            } else {
-                logger.debug("failed to get metadata for [" + type.configName + "]", e);
-            }
-            return null;
+            throw new IOException("IOException caught when fetching InetAddress from [" + metadataUrl + "]", e);
         } finally {
             IOUtils.closeWhileHandlingException(in);
         }
@@ -128,10 +122,10 @@ public class Ec2NameResolver extends AbstractComponent implements CustomNameReso
     }
 
     @Override
-    public InetAddress[] resolveIfPossible(String value) {
+    public InetAddress[] resolveIfPossible(String value) throws IOException {
         for (Ec2HostnameType type : Ec2HostnameType.values()) {
             if (type.configName.equals(value)) {
-                return resolve(type, true);
+                return resolve(type);
             }
         }
         return null;
diff --git a/plugins/discovery-ec2/src/test/java/org/elasticsearch/discovery/ec2/Ec2NetworkTests.java b/plugins/discovery-ec2/src/test/java/org/elasticsearch/discovery/ec2/Ec2NetworkTests.java
new file mode 100644
index 0000000..8aa9ca5
--- /dev/null
+++ b/plugins/discovery-ec2/src/test/java/org/elasticsearch/discovery/ec2/Ec2NetworkTests.java
@@ -0,0 +1,187 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.ec2;
+
+import org.elasticsearch.cloud.aws.network.Ec2NameResolver;
+import org.elasticsearch.common.network.NetworkService;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.net.InetAddress;
+
+import static org.hamcrest.Matchers.arrayContaining;
+import static org.hamcrest.Matchers.containsString;
+
+/**
+ * Test for EC2 network.host settings.
+ */
+public class Ec2NetworkTests extends ESTestCase {
+
+    /**
+     * Test for network.host: _ec2_
+     */
+    @Test
+    public void networkHostEc2() throws IOException {
+        Settings nodeSettings = Settings.builder()
+                .put("network.host", "_ec2_")
+                .build();
+
+        NetworkService networkService = new NetworkService(nodeSettings);
+        networkService.addCustomNameResolver(new Ec2NameResolver(nodeSettings));
+        // TODO we need to replace that with a mock. For now we check the URL we are supposed to reach.
+        try {
+            networkService.resolveBindHostAddress(null);
+        } catch (IOException e) {
+            assertThat(e.getMessage(), containsString("local-ipv4"));
+        }
+    }
+
+    /**
+     * Test for network.host: _ec2:publicIp_
+     */
+    @Test
+    public void networkHostEc2PublicIp() throws IOException {
+        Settings nodeSettings = Settings.builder()
+                .put("network.host", "_ec2:publicIp_")
+                .build();
+
+        NetworkService networkService = new NetworkService(nodeSettings);
+        networkService.addCustomNameResolver(new Ec2NameResolver(nodeSettings));
+        // TODO we need to replace that with a mock. For now we check the URL we are supposed to reach.
+        try {
+            networkService.resolveBindHostAddress(null);
+        } catch (IOException e) {
+            assertThat(e.getMessage(), containsString("public-ipv4"));
+        }
+    }
+
+    /**
+     * Test for network.host: _ec2:privateIp_
+     */
+    @Test
+    public void networkHostEc2PrivateIp() throws IOException {
+        Settings nodeSettings = Settings.builder()
+                .put("network.host", "_ec2:privateIp_")
+                .build();
+
+        NetworkService networkService = new NetworkService(nodeSettings);
+        networkService.addCustomNameResolver(new Ec2NameResolver(nodeSettings));
+        // TODO we need to replace that with a mock. For now we check the URL we are supposed to reach.
+        try {
+            networkService.resolveBindHostAddress(null);
+        } catch (IOException e) {
+            assertThat(e.getMessage(), containsString("local-ipv4"));
+        }
+    }
+
+    /**
+     * Test for network.host: _ec2:privateIpv4_
+     */
+    @Test
+    public void networkHostEc2PrivateIpv4() throws IOException {
+        Settings nodeSettings = Settings.builder()
+                .put("network.host", "_ec2:privateIpv4_")
+                .build();
+
+        NetworkService networkService = new NetworkService(nodeSettings);
+        networkService.addCustomNameResolver(new Ec2NameResolver(nodeSettings));
+        // TODO we need to replace that with a mock. For now we check the URL we are supposed to reach.
+        try {
+            networkService.resolveBindHostAddress(null);
+        } catch (IOException e) {
+            assertThat(e.getMessage(), containsString("local-ipv4"));
+        }
+    }
+
+    /**
+     * Test for network.host: _ec2:privateDns_
+     */
+    @Test
+    public void networkHostEc2PrivateDns() throws IOException {
+        Settings nodeSettings = Settings.builder()
+                .put("network.host", "_ec2:privateDns_")
+                .build();
+
+        NetworkService networkService = new NetworkService(nodeSettings);
+        networkService.addCustomNameResolver(new Ec2NameResolver(nodeSettings));
+        // TODO we need to replace that with a mock. For now we check the URL we are supposed to reach.
+        try {
+            networkService.resolveBindHostAddress(null);
+        } catch (IOException e) {
+            assertThat(e.getMessage(), containsString("local-hostname"));
+        }
+    }
+
+    /**
+     * Test for network.host: _ec2:publicIpv4_
+     */
+    @Test
+    public void networkHostEc2PublicIpv4() throws IOException {
+        Settings nodeSettings = Settings.builder()
+                .put("network.host", "_ec2:publicIpv4_")
+                .build();
+
+        NetworkService networkService = new NetworkService(nodeSettings);
+        networkService.addCustomNameResolver(new Ec2NameResolver(nodeSettings));
+        // TODO we need to replace that with a mock. For now we check the URL we are supposed to reach.
+        try {
+            networkService.resolveBindHostAddress(null);
+        } catch (IOException e) {
+            assertThat(e.getMessage(), containsString("public-ipv4"));
+        }
+    }
+
+    /**
+     * Test for network.host: _ec2:publicDns_
+     */
+    @Test
+    public void networkHostEc2PublicDns() throws IOException {
+        Settings nodeSettings = Settings.builder()
+                .put("network.host", "_ec2:publicDns_")
+                .build();
+
+        NetworkService networkService = new NetworkService(nodeSettings);
+        networkService.addCustomNameResolver(new Ec2NameResolver(nodeSettings));
+        // TODO we need to replace that with a mock. For now we check the URL we are supposed to reach.
+        try {
+            networkService.resolveBindHostAddress(null);
+        } catch (IOException e) {
+            assertThat(e.getMessage(), containsString("public-hostname"));
+        }
+    }
+
+    /**
+     * Test that we don't have any regression with network host core settings such as
+     * network.host: _local_
+     */
+    @Test
+    public void networkHostCoreLocal() throws IOException {
+        Settings nodeSettings = Settings.builder()
+                .put("network.host", "_local_")
+                .build();
+
+        NetworkService networkService = new NetworkService(nodeSettings);
+        networkService.addCustomNameResolver(new Ec2NameResolver(nodeSettings));
+        InetAddress[] addresses = networkService.resolveBindHostAddress(null);
+        assertThat(addresses, arrayContaining(networkService.resolveBindHostAddress("_local_")));
+    }
+}
diff --git a/plugins/discovery-gce/licenses/commons-codec-1.6.jar.sha1 b/plugins/discovery-gce/licenses/commons-codec-1.6.jar.sha1
new file mode 100644
index 0000000..bf78aff
--- /dev/null
+++ b/plugins/discovery-gce/licenses/commons-codec-1.6.jar.sha1
@@ -0,0 +1 @@
+b7f0fc8f61ecadeb3695f0b9464755eee44374d4
diff --git a/plugins/discovery-gce/licenses/commons-codec-LICENSE.txt b/plugins/discovery-gce/licenses/commons-codec-LICENSE.txt
new file mode 100644
index 0000000..d645695
--- /dev/null
+++ b/plugins/discovery-gce/licenses/commons-codec-LICENSE.txt
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/plugins/discovery-gce/licenses/commons-codec-NOTICE.txt b/plugins/discovery-gce/licenses/commons-codec-NOTICE.txt
new file mode 100644
index 0000000..5691644
--- /dev/null
+++ b/plugins/discovery-gce/licenses/commons-codec-NOTICE.txt
@@ -0,0 +1,17 @@
+Apache Commons Codec
+Copyright 2002-2015 The Apache Software Foundation
+
+This product includes software developed at
+The Apache Software Foundation (http://www.apache.org/).
+
+src/test/org/apache/commons/codec/language/DoubleMetaphoneTest.java
+contains test data from http://aspell.net/test/orig/batch0.tab.
+Copyright (C) 2002 Kevin Atkinson (kevina@gnu.org)
+
+===============================================================================
+
+The content of package org.apache.commons.codec.language.bm has been translated
+from the original php source code available at http://stevemorse.org/phoneticinfo.htm
+with permission from the original authors.
+Original source copyright:
+Copyright (c) 2008 Alexander Beider & Stephen P. Morse.
diff --git a/plugins/discovery-gce/licenses/commons-logging-1.1.3.jar.sha1 b/plugins/discovery-gce/licenses/commons-logging-1.1.3.jar.sha1
new file mode 100644
index 0000000..c8756c4
--- /dev/null
+++ b/plugins/discovery-gce/licenses/commons-logging-1.1.3.jar.sha1
@@ -0,0 +1 @@
+f6f66e966c70a83ffbdb6f17a0919eaf7c8aca7f
diff --git a/plugins/discovery-gce/licenses/commons-logging-LICENSE.txt b/plugins/discovery-gce/licenses/commons-logging-LICENSE.txt
new file mode 100644
index 0000000..d645695
--- /dev/null
+++ b/plugins/discovery-gce/licenses/commons-logging-LICENSE.txt
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/plugins/discovery-gce/licenses/commons-logging-NOTICE.txt b/plugins/discovery-gce/licenses/commons-logging-NOTICE.txt
new file mode 100644
index 0000000..d3d6e14
--- /dev/null
+++ b/plugins/discovery-gce/licenses/commons-logging-NOTICE.txt
@@ -0,0 +1,5 @@
+Apache Commons Logging
+Copyright 2003-2014 The Apache Software Foundation
+
+This product includes software developed at
+The Apache Software Foundation (http://www.apache.org/).
diff --git a/plugins/discovery-gce/licenses/google-LICENSE.txt b/plugins/discovery-gce/licenses/google-LICENSE.txt
new file mode 100644
index 0000000..980a15a
--- /dev/null
+++ b/plugins/discovery-gce/licenses/google-LICENSE.txt
@@ -0,0 +1,201 @@
+                                Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "{}"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright {yyyy} {name of copyright owner}
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/plugins/discovery-gce/licenses/google-NOTICE.txt b/plugins/discovery-gce/licenses/google-NOTICE.txt
new file mode 100644
index 0000000..8d1c8b6
--- /dev/null
+++ b/plugins/discovery-gce/licenses/google-NOTICE.txt
@@ -0,0 +1 @@
+ 
diff --git a/plugins/discovery-gce/licenses/google-api-client-1.20.0.jar.sha1 b/plugins/discovery-gce/licenses/google-api-client-1.20.0.jar.sha1
new file mode 100644
index 0000000..08c24d1
--- /dev/null
+++ b/plugins/discovery-gce/licenses/google-api-client-1.20.0.jar.sha1
@@ -0,0 +1 @@
+d3e66209ae9e749b2d6833761e7885f60f285564
diff --git a/plugins/discovery-gce/licenses/google-api-services-compute-v1-rev71-1.20.0.jar.sha1 b/plugins/discovery-gce/licenses/google-api-services-compute-v1-rev71-1.20.0.jar.sha1
new file mode 100644
index 0000000..c6e6948
--- /dev/null
+++ b/plugins/discovery-gce/licenses/google-api-services-compute-v1-rev71-1.20.0.jar.sha1
@@ -0,0 +1 @@
+2fa36fff3b5bf59a63c4f2bbfac1f88251cd7986
diff --git a/plugins/discovery-gce/licenses/google-http-client-1.20.0.jar.sha1 b/plugins/discovery-gce/licenses/google-http-client-1.20.0.jar.sha1
new file mode 100644
index 0000000..66a2247
--- /dev/null
+++ b/plugins/discovery-gce/licenses/google-http-client-1.20.0.jar.sha1
@@ -0,0 +1 @@
+93d82db2bca534960253f43424b2ba9d7638b4d2
diff --git a/plugins/discovery-gce/licenses/google-http-client-jackson2-1.20.0.jar.sha1 b/plugins/discovery-gce/licenses/google-http-client-jackson2-1.20.0.jar.sha1
new file mode 100644
index 0000000..6d861e6
--- /dev/null
+++ b/plugins/discovery-gce/licenses/google-http-client-jackson2-1.20.0.jar.sha1
@@ -0,0 +1 @@
+2408070b2abec043624d35b35e30450f1b663858
diff --git a/plugins/discovery-gce/licenses/google-oauth-client-1.20.0.jar.sha1 b/plugins/discovery-gce/licenses/google-oauth-client-1.20.0.jar.sha1
new file mode 100644
index 0000000..c35c4bc
--- /dev/null
+++ b/plugins/discovery-gce/licenses/google-oauth-client-1.20.0.jar.sha1
@@ -0,0 +1 @@
+1d086ac5756475ddf451af2e2df6e288d18608ca
diff --git a/plugins/discovery-gce/licenses/httpclient-4.3.6.jar.sha1 b/plugins/discovery-gce/licenses/httpclient-4.3.6.jar.sha1
new file mode 100644
index 0000000..3d35ee9
--- /dev/null
+++ b/plugins/discovery-gce/licenses/httpclient-4.3.6.jar.sha1
@@ -0,0 +1 @@
+4c47155e3e6c9a41a28db36680b828ced53b8af4
diff --git a/plugins/discovery-gce/licenses/httpclient-LICENSE.txt b/plugins/discovery-gce/licenses/httpclient-LICENSE.txt
new file mode 100644
index 0000000..32f01ed
--- /dev/null
+++ b/plugins/discovery-gce/licenses/httpclient-LICENSE.txt
@@ -0,0 +1,558 @@
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+=========================================================================
+
+This project includes Public Suffix List copied from
+<https://publicsuffix.org/list/effective_tld_names.dat>
+licensed under the terms of the Mozilla Public License, v. 2.0
+
+Full license text: <http://mozilla.org/MPL/2.0/>
+
+Mozilla Public License Version 2.0
+==================================
+
+1. Definitions
+--------------
+
+1.1. "Contributor"
+    means each individual or legal entity that creates, contributes to
+    the creation of, or owns Covered Software.
+
+1.2. "Contributor Version"
+    means the combination of the Contributions of others (if any) used
+    by a Contributor and that particular Contributor's Contribution.
+
+1.3. "Contribution"
+    means Covered Software of a particular Contributor.
+
+1.4. "Covered Software"
+    means Source Code Form to which the initial Contributor has attached
+    the notice in Exhibit A, the Executable Form of such Source Code
+    Form, and Modifications of such Source Code Form, in each case
+    including portions thereof.
+
+1.5. "Incompatible With Secondary Licenses"
+    means
+
+    (a) that the initial Contributor has attached the notice described
+        in Exhibit B to the Covered Software; or
+
+    (b) that the Covered Software was made available under the terms of
+        version 1.1 or earlier of the License, but not also under the
+        terms of a Secondary License.
+
+1.6. "Executable Form"
+    means any form of the work other than Source Code Form.
+
+1.7. "Larger Work"
+    means a work that combines Covered Software with other material, in
+    a separate file or files, that is not Covered Software.
+
+1.8. "License"
+    means this document.
+
+1.9. "Licensable"
+    means having the right to grant, to the maximum extent possible,
+    whether at the time of the initial grant or subsequently, any and
+    all of the rights conveyed by this License.
+
+1.10. "Modifications"
+    means any of the following:
+
+    (a) any file in Source Code Form that results from an addition to,
+        deletion from, or modification of the contents of Covered
+        Software; or
+
+    (b) any new file in Source Code Form that contains any Covered
+        Software.
+
+1.11. "Patent Claims" of a Contributor
+    means any patent claim(s), including without limitation, method,
+    process, and apparatus claims, in any patent Licensable by such
+    Contributor that would be infringed, but for the grant of the
+    License, by the making, using, selling, offering for sale, having
+    made, import, or transfer of either its Contributions or its
+    Contributor Version.
+
+1.12. "Secondary License"
+    means either the GNU General Public License, Version 2.0, the GNU
+    Lesser General Public License, Version 2.1, the GNU Affero General
+    Public License, Version 3.0, or any later versions of those
+    licenses.
+
+1.13. "Source Code Form"
+    means the form of the work preferred for making modifications.
+
+1.14. "You" (or "Your")
+    means an individual or a legal entity exercising rights under this
+    License. For legal entities, "You" includes any entity that
+    controls, is controlled by, or is under common control with You. For
+    purposes of this definition, "control" means (a) the power, direct
+    or indirect, to cause the direction or management of such entity,
+    whether by contract or otherwise, or (b) ownership of more than
+    fifty percent (50%) of the outstanding shares or beneficial
+    ownership of such entity.
+
+2. License Grants and Conditions
+--------------------------------
+
+2.1. Grants
+
+Each Contributor hereby grants You a world-wide, royalty-free,
+non-exclusive license:
+
+(a) under intellectual property rights (other than patent or trademark)
+    Licensable by such Contributor to use, reproduce, make available,
+    modify, display, perform, distribute, and otherwise exploit its
+    Contributions, either on an unmodified basis, with Modifications, or
+    as part of a Larger Work; and
+
+(b) under Patent Claims of such Contributor to make, use, sell, offer
+    for sale, have made, import, and otherwise transfer either its
+    Contributions or its Contributor Version.
+
+2.2. Effective Date
+
+The licenses granted in Section 2.1 with respect to any Contribution
+become effective for each Contribution on the date the Contributor first
+distributes such Contribution.
+
+2.3. Limitations on Grant Scope
+
+The licenses granted in this Section 2 are the only rights granted under
+this License. No additional rights or licenses will be implied from the
+distribution or licensing of Covered Software under this License.
+Notwithstanding Section 2.1(b) above, no patent license is granted by a
+Contributor:
+
+(a) for any code that a Contributor has removed from Covered Software;
+    or
+
+(b) for infringements caused by: (i) Your and any other third party's
+    modifications of Covered Software, or (ii) the combination of its
+    Contributions with other software (except as part of its Contributor
+    Version); or
+
+(c) under Patent Claims infringed by Covered Software in the absence of
+    its Contributions.
+
+This License does not grant any rights in the trademarks, service marks,
+or logos of any Contributor (except as may be necessary to comply with
+the notice requirements in Section 3.4).
+
+2.4. Subsequent Licenses
+
+No Contributor makes additional grants as a result of Your choice to
+distribute the Covered Software under a subsequent version of this
+License (see Section 10.2) or under the terms of a Secondary License (if
+permitted under the terms of Section 3.3).
+
+2.5. Representation
+
+Each Contributor represents that the Contributor believes its
+Contributions are its original creation(s) or it has sufficient rights
+to grant the rights to its Contributions conveyed by this License.
+
+2.6. Fair Use
+
+This License is not intended to limit any rights You have under
+applicable copyright doctrines of fair use, fair dealing, or other
+equivalents.
+
+2.7. Conditions
+
+Sections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted
+in Section 2.1.
+
+3. Responsibilities
+-------------------
+
+3.1. Distribution of Source Form
+
+All distribution of Covered Software in Source Code Form, including any
+Modifications that You create or to which You contribute, must be under
+the terms of this License. You must inform recipients that the Source
+Code Form of the Covered Software is governed by the terms of this
+License, and how they can obtain a copy of this License. You may not
+attempt to alter or restrict the recipients' rights in the Source Code
+Form.
+
+3.2. Distribution of Executable Form
+
+If You distribute Covered Software in Executable Form then:
+
+(a) such Covered Software must also be made available in Source Code
+    Form, as described in Section 3.1, and You must inform recipients of
+    the Executable Form how they can obtain a copy of such Source Code
+    Form by reasonable means in a timely manner, at a charge no more
+    than the cost of distribution to the recipient; and
+
+(b) You may distribute such Executable Form under the terms of this
+    License, or sublicense it under different terms, provided that the
+    license for the Executable Form does not attempt to limit or alter
+    the recipients' rights in the Source Code Form under this License.
+
+3.3. Distribution of a Larger Work
+
+You may create and distribute a Larger Work under terms of Your choice,
+provided that You also comply with the requirements of this License for
+the Covered Software. If the Larger Work is a combination of Covered
+Software with a work governed by one or more Secondary Licenses, and the
+Covered Software is not Incompatible With Secondary Licenses, this
+License permits You to additionally distribute such Covered Software
+under the terms of such Secondary License(s), so that the recipient of
+the Larger Work may, at their option, further distribute the Covered
+Software under the terms of either this License or such Secondary
+License(s).
+
+3.4. Notices
+
+You may not remove or alter the substance of any license notices
+(including copyright notices, patent notices, disclaimers of warranty,
+or limitations of liability) contained within the Source Code Form of
+the Covered Software, except that You may alter any license notices to
+the extent required to remedy known factual inaccuracies.
+
+3.5. Application of Additional Terms
+
+You may choose to offer, and to charge a fee for, warranty, support,
+indemnity or liability obligations to one or more recipients of Covered
+Software. However, You may do so only on Your own behalf, and not on
+behalf of any Contributor. You must make it absolutely clear that any
+such warranty, support, indemnity, or liability obligation is offered by
+You alone, and You hereby agree to indemnify every Contributor for any
+liability incurred by such Contributor as a result of warranty, support,
+indemnity or liability terms You offer. You may include additional
+disclaimers of warranty and limitations of liability specific to any
+jurisdiction.
+
+4. Inability to Comply Due to Statute or Regulation
+---------------------------------------------------
+
+If it is impossible for You to comply with any of the terms of this
+License with respect to some or all of the Covered Software due to
+statute, judicial order, or regulation then You must: (a) comply with
+the terms of this License to the maximum extent possible; and (b)
+describe the limitations and the code they affect. Such description must
+be placed in a text file included with all distributions of the Covered
+Software under this License. Except to the extent prohibited by statute
+or regulation, such description must be sufficiently detailed for a
+recipient of ordinary skill to be able to understand it.
+
+5. Termination
+--------------
+
+5.1. The rights granted under this License will terminate automatically
+if You fail to comply with any of its terms. However, if You become
+compliant, then the rights granted under this License from a particular
+Contributor are reinstated (a) provisionally, unless and until such
+Contributor explicitly and finally terminates Your grants, and (b) on an
+ongoing basis, if such Contributor fails to notify You of the
+non-compliance by some reasonable means prior to 60 days after You have
+come back into compliance. Moreover, Your grants from a particular
+Contributor are reinstated on an ongoing basis if such Contributor
+notifies You of the non-compliance by some reasonable means, this is the
+first time You have received notice of non-compliance with this License
+from such Contributor, and You become compliant prior to 30 days after
+Your receipt of the notice.
+
+5.2. If You initiate litigation against any entity by asserting a patent
+infringement claim (excluding declaratory judgment actions,
+counter-claims, and cross-claims) alleging that a Contributor Version
+directly or indirectly infringes any patent, then the rights granted to
+You by any and all Contributors for the Covered Software under Section
+2.1 of this License shall terminate.
+
+5.3. In the event of termination under Sections 5.1 or 5.2 above, all
+end user license agreements (excluding distributors and resellers) which
+have been validly granted by You or Your distributors under this License
+prior to termination shall survive termination.
+
+************************************************************************
+*                                                                      *
+*  6. Disclaimer of Warranty                                           *
+*  -------------------------                                           *
+*                                                                      *
+*  Covered Software is provided under this License on an "as is"       *
+*  basis, without warranty of any kind, either expressed, implied, or  *
+*  statutory, including, without limitation, warranties that the       *
+*  Covered Software is free of defects, merchantable, fit for a        *
+*  particular purpose or non-infringing. The entire risk as to the     *
+*  quality and performance of the Covered Software is with You.        *
+*  Should any Covered Software prove defective in any respect, You     *
+*  (not any Contributor) assume the cost of any necessary servicing,   *
+*  repair, or correction. This disclaimer of warranty constitutes an   *
+*  essential part of this License. No use of any Covered Software is   *
+*  authorized under this License except under this disclaimer.         *
+*                                                                      *
+************************************************************************
+
+************************************************************************
+*                                                                      *
+*  7. Limitation of Liability                                          *
+*  --------------------------                                          *
+*                                                                      *
+*  Under no circumstances and under no legal theory, whether tort      *
+*  (including negligence), contract, or otherwise, shall any           *
+*  Contributor, or anyone who distributes Covered Software as          *
+*  permitted above, be liable to You for any direct, indirect,         *
+*  special, incidental, or consequential damages of any character      *
+*  including, without limitation, damages for lost profits, loss of    *
+*  goodwill, work stoppage, computer failure or malfunction, or any    *
+*  and all other commercial damages or losses, even if such party      *
+*  shall have been informed of the possibility of such damages. This   *
+*  limitation of liability shall not apply to liability for death or   *
+*  personal injury resulting from such party's negligence to the       *
+*  extent applicable law prohibits such limitation. Some               *
+*  jurisdictions do not allow the exclusion or limitation of           *
+*  incidental or consequential damages, so this exclusion and          *
+*  limitation may not apply to You.                                    *
+*                                                                      *
+************************************************************************
+
+8. Litigation
+-------------
+
+Any litigation relating to this License may be brought only in the
+courts of a jurisdiction where the defendant maintains its principal
+place of business and such litigation shall be governed by laws of that
+jurisdiction, without reference to its conflict-of-law provisions.
+Nothing in this Section shall prevent a party's ability to bring
+cross-claims or counter-claims.
+
+9. Miscellaneous
+----------------
+
+This License represents the complete agreement concerning the subject
+matter hereof. If any provision of this License is held to be
+unenforceable, such provision shall be reformed only to the extent
+necessary to make it enforceable. Any law or regulation which provides
+that the language of a contract shall be construed against the drafter
+shall not be used to construe this License against a Contributor.
+
+10. Versions of the License
+---------------------------
+
+10.1. New Versions
+
+Mozilla Foundation is the license steward. Except as provided in Section
+10.3, no one other than the license steward has the right to modify or
+publish new versions of this License. Each version will be given a
+distinguishing version number.
+
+10.2. Effect of New Versions
+
+You may distribute the Covered Software under the terms of the version
+of the License under which You originally received the Covered Software,
+or under the terms of any subsequent version published by the license
+steward.
+
+10.3. Modified Versions
+
+If you create software not governed by this License, and you want to
+create a new license for such software, you may create and use a
+modified version of this License if you rename the license and remove
+any references to the name of the license steward (except to note that
+such modified license differs from this License).
+
+10.4. Distributing Source Code Form that is Incompatible With Secondary
+Licenses
+
+If You choose to distribute Source Code Form that is Incompatible With
+Secondary Licenses under the terms of this version of the License, the
+notice described in Exhibit B of this License must be attached.
+
+Exhibit A - Source Code Form License Notice
+-------------------------------------------
+
+  This Source Code Form is subject to the terms of the Mozilla Public
+  License, v. 2.0. If a copy of the MPL was not distributed with this
+  file, You can obtain one at http://mozilla.org/MPL/2.0/.
+
+If it is not possible or desirable to put the notice in a particular
+file, then You may include the notice in a location (such as a LICENSE
+file in a relevant directory) where a recipient would be likely to look
+for such a notice.
+
+You may add additional accurate notices of copyright ownership.
+
+Exhibit B - "Incompatible With Secondary Licenses" Notice
+---------------------------------------------------------
+
+  This Source Code Form is "Incompatible With Secondary Licenses", as
+  defined by the Mozilla Public License, v. 2.0.
diff --git a/plugins/discovery-gce/licenses/httpclient-NOTICE.txt b/plugins/discovery-gce/licenses/httpclient-NOTICE.txt
new file mode 100644
index 0000000..4f60581
--- /dev/null
+++ b/plugins/discovery-gce/licenses/httpclient-NOTICE.txt
@@ -0,0 +1,5 @@
+Apache HttpComponents Client
+Copyright 1999-2015 The Apache Software Foundation
+
+This product includes software developed at
+The Apache Software Foundation (http://www.apache.org/).
diff --git a/plugins/discovery-gce/licenses/httpcore-4.3.3.jar.sha1 b/plugins/discovery-gce/licenses/httpcore-4.3.3.jar.sha1
new file mode 100644
index 0000000..5d9c0e2
--- /dev/null
+++ b/plugins/discovery-gce/licenses/httpcore-4.3.3.jar.sha1
@@ -0,0 +1 @@
+f91b7a4aadc5cf486df6e4634748d7dd7a73f06d
diff --git a/plugins/discovery-gce/licenses/httpcore-LICENSE.txt b/plugins/discovery-gce/licenses/httpcore-LICENSE.txt
new file mode 100644
index 0000000..72819a9
--- /dev/null
+++ b/plugins/discovery-gce/licenses/httpcore-LICENSE.txt
@@ -0,0 +1,241 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+=========================================================================
+
+This project contains annotations in the package org.apache.http.annotation
+which are derived from JCIP-ANNOTATIONS
+Copyright (c) 2005 Brian Goetz and Tim Peierls.
+See http://www.jcip.net and the Creative Commons Attribution License
+(http://creativecommons.org/licenses/by/2.5)
+Full text: http://creativecommons.org/licenses/by/2.5/legalcode
+
+License
+
+THE WORK (AS DEFINED BELOW) IS PROVIDED UNDER THE TERMS OF THIS CREATIVE COMMONS PUBLIC LICENSE ("CCPL" OR "LICENSE"). THE WORK IS PROTECTED BY COPYRIGHT AND/OR OTHER APPLICABLE LAW. ANY USE OF THE WORK OTHER THAN AS AUTHORIZED UNDER THIS LICENSE OR COPYRIGHT LAW IS PROHIBITED.
+
+BY EXERCISING ANY RIGHTS TO THE WORK PROVIDED HERE, YOU ACCEPT AND AGREE TO BE BOUND BY THE TERMS OF THIS LICENSE. THE LICENSOR GRANTS YOU THE RIGHTS CONTAINED HERE IN CONSIDERATION OF YOUR ACCEPTANCE OF SUCH TERMS AND CONDITIONS.
+
+1. Definitions
+
+    "Collective Work" means a work, such as a periodical issue, anthology or encyclopedia, in which the Work in its entirety in unmodified form, along with a number of other contributions, constituting separate and independent works in themselves, are assembled into a collective whole. A work that constitutes a Collective Work will not be considered a Derivative Work (as defined below) for the purposes of this License.
+    "Derivative Work" means a work based upon the Work or upon the Work and other pre-existing works, such as a translation, musical arrangement, dramatization, fictionalization, motion picture version, sound recording, art reproduction, abridgment, condensation, or any other form in which the Work may be recast, transformed, or adapted, except that a work that constitutes a Collective Work will not be considered a Derivative Work for the purpose of this License. For the avoidance of doubt, where the Work is a musical composition or sound recording, the synchronization of the Work in timed-relation with a moving image ("synching") will be considered a Derivative Work for the purpose of this License.
+    "Licensor" means the individual or entity that offers the Work under the terms of this License.
+    "Original Author" means the individual or entity who created the Work.
+    "Work" means the copyrightable work of authorship offered under the terms of this License.
+    "You" means an individual or entity exercising rights under this License who has not previously violated the terms of this License with respect to the Work, or who has received express permission from the Licensor to exercise rights under this License despite a previous violation.
+
+2. Fair Use Rights. Nothing in this license is intended to reduce, limit, or restrict any rights arising from fair use, first sale or other limitations on the exclusive rights of the copyright owner under copyright law or other applicable laws.
+
+3. License Grant. Subject to the terms and conditions of this License, Licensor hereby grants You a worldwide, royalty-free, non-exclusive, perpetual (for the duration of the applicable copyright) license to exercise the rights in the Work as stated below:
+
+    to reproduce the Work, to incorporate the Work into one or more Collective Works, and to reproduce the Work as incorporated in the Collective Works;
+    to create and reproduce Derivative Works;
+    to distribute copies or phonorecords of, display publicly, perform publicly, and perform publicly by means of a digital audio transmission the Work including as incorporated in Collective Works;
+    to distribute copies or phonorecords of, display publicly, perform publicly, and perform publicly by means of a digital audio transmission Derivative Works.
+
+    For the avoidance of doubt, where the work is a musical composition:
+        Performance Royalties Under Blanket Licenses. Licensor waives the exclusive right to collect, whether individually or via a performance rights society (e.g. ASCAP, BMI, SESAC), royalties for the public performance or public digital performance (e.g. webcast) of the Work.
+        Mechanical Rights and Statutory Royalties. Licensor waives the exclusive right to collect, whether individually or via a music rights agency or designated agent (e.g. Harry Fox Agency), royalties for any phonorecord You create from the Work ("cover version") and distribute, subject to the compulsory license created by 17 USC Section 115 of the US Copyright Act (or the equivalent in other jurisdictions).
+    Webcasting Rights and Statutory Royalties. For the avoidance of doubt, where the Work is a sound recording, Licensor waives the exclusive right to collect, whether individually or via a performance-rights society (e.g. SoundExchange), royalties for the public digital performance (e.g. webcast) of the Work, subject to the compulsory license created by 17 USC Section 114 of the US Copyright Act (or the equivalent in other jurisdictions).
+
+The above rights may be exercised in all media and formats whether now known or hereafter devised. The above rights include the right to make such modifications as are technically necessary to exercise the rights in other media and formats. All rights not expressly granted by Licensor are hereby reserved.
+
+4. Restrictions.The license granted in Section 3 above is expressly made subject to and limited by the following restrictions:
+
+    You may distribute, publicly display, publicly perform, or publicly digitally perform the Work only under the terms of this License, and You must include a copy of, or the Uniform Resource Identifier for, this License with every copy or phonorecord of the Work You distribute, publicly display, publicly perform, or publicly digitally perform. You may not offer or impose any terms on the Work that alter or restrict the terms of this License or the recipients' exercise of the rights granted hereunder. You may not sublicense the Work. You must keep intact all notices that refer to this License and to the disclaimer of warranties. You may not distribute, publicly display, publicly perform, or publicly digitally perform the Work with any technological measures that control access or use of the Work in a manner inconsistent with the terms of this License Agreement. The above applies to the Work as incorporated in a Collective Work, but this does not require the Collective Work apart from the Work itself to be made subject to the terms of this License. If You create a Collective Work, upon notice from any Licensor You must, to the extent practicable, remove from the Collective Work any credit as required by clause 4(b), as requested. If You create a Derivative Work, upon notice from any Licensor You must, to the extent practicable, remove from the Derivative Work any credit as required by clause 4(b), as requested.
+    If you distribute, publicly display, publicly perform, or publicly digitally perform the Work or any Derivative Works or Collective Works, You must keep intact all copyright notices for the Work and provide, reasonable to the medium or means You are utilizing: (i) the name of the Original Author (or pseudonym, if applicable) if supplied, and/or (ii) if the Original Author and/or Licensor designate another party or parties (e.g. a sponsor institute, publishing entity, journal) for attribution in Licensor's copyright notice, terms of service or by other reasonable means, the name of such party or parties; the title of the Work if supplied; to the extent reasonably practicable, the Uniform Resource Identifier, if any, that Licensor specifies to be associated with the Work, unless such URI does not refer to the copyright notice or licensing information for the Work; and in the case of a Derivative Work, a credit identifying the use of the Work in the Derivative Work (e.g., "French translation of the Work by Original Author," or "Screenplay based on original Work by Original Author"). Such credit may be implemented in any reasonable manner; provided, however, that in the case of a Derivative Work or Collective Work, at a minimum such credit will appear where any other comparable authorship credit appears and in a manner at least as prominent as such other comparable authorship credit.
+
+5. Representations, Warranties and Disclaimer
+
+UNLESS OTHERWISE MUTUALLY AGREED TO BY THE PARTIES IN WRITING, LICENSOR OFFERS THE WORK AS-IS AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND CONCERNING THE WORK, EXPRESS, IMPLIED, STATUTORY OR OTHERWISE, INCLUDING, WITHOUT LIMITATION, WARRANTIES OF TITLE, MERCHANTIBILITY, FITNESS FOR A PARTICULAR PURPOSE, NONINFRINGEMENT, OR THE ABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OF ABSENCE OF ERRORS, WHETHER OR NOT DISCOVERABLE. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF IMPLIED WARRANTIES, SO SUCH EXCLUSION MAY NOT APPLY TO YOU.
+
+6. Limitation on Liability. EXCEPT TO THE EXTENT REQUIRED BY APPLICABLE LAW, IN NO EVENT WILL LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY FOR ANY SPECIAL, INCIDENTAL, CONSEQUENTIAL, PUNITIVE OR EXEMPLARY DAMAGES ARISING OUT OF THIS LICENSE OR THE USE OF THE WORK, EVEN IF LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+
+7. Termination
+
+    This License and the rights granted hereunder will terminate automatically upon any breach by You of the terms of this License. Individuals or entities who have received Derivative Works or Collective Works from You under this License, however, will not have their licenses terminated provided such individuals or entities remain in full compliance with those licenses. Sections 1, 2, 5, 6, 7, and 8 will survive any termination of this License.
+    Subject to the above terms and conditions, the license granted here is perpetual (for the duration of the applicable copyright in the Work). Notwithstanding the above, Licensor reserves the right to release the Work under different license terms or to stop distributing the Work at any time; provided, however that any such election will not serve to withdraw this License (or any other license that has been, or is required to be, granted under the terms of this License), and this License will continue in full force and effect unless terminated as stated above.
+
+8. Miscellaneous
+
+    Each time You distribute or publicly digitally perform the Work or a Collective Work, the Licensor offers to the recipient a license to the Work on the same terms and conditions as the license granted to You under this License.
+    Each time You distribute or publicly digitally perform a Derivative Work, Licensor offers to the recipient a license to the original Work on the same terms and conditions as the license granted to You under this License.
+    If any provision of this License is invalid or unenforceable under applicable law, it shall not affect the validity or enforceability of the remainder of the terms of this License, and without further action by the parties to this agreement, such provision shall be reformed to the minimum extent necessary to make such provision valid and enforceable.
+    No term or provision of this License shall be deemed waived and no breach consented to unless such waiver or consent shall be in writing and signed by the party to be charged with such waiver or consent.
+    This License constitutes the entire agreement between the parties with respect to the Work licensed here. There are no understandings, agreements or representations with respect to the Work not specified here. Licensor shall not be bound by any additional provisions that may appear in any communication from You. This License may not be modified without the mutual written agreement of the Licensor and You.
diff --git a/plugins/discovery-gce/licenses/httpcore-NOTICE.txt b/plugins/discovery-gce/licenses/httpcore-NOTICE.txt
new file mode 100644
index 0000000..c0be50a
--- /dev/null
+++ b/plugins/discovery-gce/licenses/httpcore-NOTICE.txt
@@ -0,0 +1,8 @@
+Apache HttpComponents Core
+Copyright 2005-2014 The Apache Software Foundation
+
+This product includes software developed at
+The Apache Software Foundation (http://www.apache.org/).
+
+This project contains annotations derived from JCIP-ANNOTATIONS
+Copyright (c) 2005 Brian Goetz and Tim Peierls. See http://www.jcip.net
diff --git a/plugins/discovery-gce/licenses/jsr305-1.3.9.jar.sha1 b/plugins/discovery-gce/licenses/jsr305-1.3.9.jar.sha1
new file mode 100644
index 0000000..c04a429
--- /dev/null
+++ b/plugins/discovery-gce/licenses/jsr305-1.3.9.jar.sha1
@@ -0,0 +1 @@
+40719ea6961c0cb6afaeb6a921eaa1f6afd4cfdf
diff --git a/plugins/discovery-gce/licenses/jsr305-LICENSE.txt b/plugins/discovery-gce/licenses/jsr305-LICENSE.txt
new file mode 100644
index 0000000..0cb8710
--- /dev/null
+++ b/plugins/discovery-gce/licenses/jsr305-LICENSE.txt
@@ -0,0 +1,29 @@
+Copyright (c) 2007-2009, JSR305 expert group
+All rights reserved.
+
+http://www.opensource.org/licenses/bsd-license.php
+
+Redistribution and use in source and binary forms, with or without 
+modification, are permitted provided that the following conditions are met:
+
+    * Redistributions of source code must retain the above copyright notice, 
+      this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright notice, 
+      this list of conditions and the following disclaimer in the documentation 
+      and/or other materials provided with the distribution.
+    * Neither the name of the JSR305 expert group nor the names of its 
+      contributors may be used to endorse or promote products derived from 
+      this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" 
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, 
+THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE 
+ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE 
+LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR 
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF 
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS 
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN 
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) 
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE 
+POSSIBILITY OF SUCH DAMAGE.
+
diff --git a/plugins/discovery-gce/licenses/jsr305-NOTICE.txt b/plugins/discovery-gce/licenses/jsr305-NOTICE.txt
new file mode 100644
index 0000000..8d1c8b6
--- /dev/null
+++ b/plugins/discovery-gce/licenses/jsr305-NOTICE.txt
@@ -0,0 +1 @@
+ 
diff --git a/plugins/discovery-gce/pom.xml b/plugins/discovery-gce/pom.xml
new file mode 100644
index 0000000..b7c1c0a
--- /dev/null
+++ b/plugins/discovery-gce/pom.xml
@@ -0,0 +1,68 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!-- Licensed to Elasticsearch under one or more contributor
+license agreements. See the NOTICE file distributed with this work for additional
+information regarding copyright ownership. Elasticsearch licenses this file to you
+under the Apache License, Version 2.0 (the "License"); you may not use this
+file except in compliance with the License. You may obtain a copy of the
+License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by
+applicable law or agreed to in writing, software distributed under the License
+is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied. See the License for the specific language
+governing permissions and limitations under the License. -->
+
+<project xmlns="http://maven.apache.org/POM/4.0.0"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+
+    <parent>
+        <groupId>org.elasticsearch.plugin</groupId>
+        <artifactId>plugins</artifactId>
+        <version>3.0.0-SNAPSHOT</version>
+    </parent>
+
+    <artifactId>discovery-gce</artifactId>
+    <name>Plugin: Discovery: Google Compute Engine</name>
+    <description>The Google Compute Engine (GCE) Discovery plugin allows to use GCE API for the unicast discovery mechanism.</description>
+
+    <properties>
+        <elasticsearch.plugin.classname>org.elasticsearch.plugin.discovery.gce.GceDiscoveryPlugin</elasticsearch.plugin.classname>
+        <google.gce.version>v1-rev71-1.20.0</google.gce.version>
+        <!-- currently has no unit tests -->
+        <tests.rest.suite>discovery_gce</tests.rest.suite>
+        <tests.rest.load_packaged>false</tests.rest.load_packaged>
+        <xlint.options>-Xlint:-rawtypes,-unchecked</xlint.options>
+    </properties>
+
+    <dependencies>
+        <!-- Google APIs -->
+        <dependency>
+          <groupId>com.google.apis</groupId>
+          <artifactId>google-api-services-compute</artifactId>
+          <version>${google.gce.version}</version>
+          <exclusions>
+              <exclusion>
+                 <groupId>com.google.guava</groupId>
+                 <artifactId>guava-jdk5</artifactId>
+              </exclusion>
+          </exclusions>
+        </dependency>
+        <!-- We need to force here the compile scope as it was defined as test scope in plugins/pom.xml -->
+        <!-- TODO: remove this dependency when we will have a REST Test module -->
+        <dependency>
+            <groupId>org.apache.httpcomponents</groupId>
+            <artifactId>httpclient</artifactId>
+            <scope>compile</scope>
+        </dependency>
+    </dependencies>
+
+    <build>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
diff --git a/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeService.java b/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeService.java
new file mode 100644
index 0000000..c7f4598
--- /dev/null
+++ b/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeService.java
@@ -0,0 +1,57 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.gce;
+
+import com.google.api.services.compute.model.Instance;
+import org.elasticsearch.common.component.LifecycleComponent;
+
+import java.io.IOException;
+import java.util.Collection;
+
+public interface GceComputeService extends LifecycleComponent<GceComputeService> {
+    final class Fields {
+        public static final String PROJECT = "cloud.gce.project_id";
+        public static final String ZONE = "cloud.gce.zone";
+        public static final String REFRESH = "cloud.gce.refresh_interval";
+        public static final String TAGS = "discovery.gce.tags";
+        public static final String VERSION = "Elasticsearch/GceCloud/1.0";
+    }
+
+    /**
+     * Return a collection of running instances within the same GCE project
+     * @return a collection of running instances within the same GCE project
+     */
+    Collection<Instance> instances();
+
+    /**
+     * <p>Gets metadata on the current running machine (call to
+     * http://metadata.google.internal/computeMetadata/v1/instance/xxx).</p>
+     * <p>For example, you can retrieve network information by replacing xxx with:</p>
+     * <ul>
+     *     <li>`hostname` when we need to resolve the host name</li>
+     *     <li>`network-interfaces/0/ip` when we need to resolve private IP</li>
+     * </ul>
+     * @see org.elasticsearch.cloud.gce.network.GceNameResolver for bindings
+     * @param metadataPath path to metadata information
+     * @return extracted information (for example a hostname or an IP address)
+     * @throws IOException in case metadata URL is not accessible
+     */
+    String metadata(String metadataPath) throws IOException;
+}
diff --git a/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeServiceImpl.java b/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeServiceImpl.java
new file mode 100644
index 0000000..a29c21e
--- /dev/null
+++ b/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeServiceImpl.java
@@ -0,0 +1,228 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.gce;
+
+import com.google.api.client.googleapis.compute.ComputeCredential;
+import com.google.api.client.googleapis.javanet.GoogleNetHttpTransport;
+import com.google.api.client.http.GenericUrl;
+import com.google.api.client.http.HttpHeaders;
+import com.google.api.client.http.HttpResponse;
+import com.google.api.client.http.HttpTransport;
+import com.google.api.client.json.JsonFactory;
+import com.google.api.client.json.jackson2.JacksonFactory;
+import com.google.api.services.compute.Compute;
+import com.google.api.services.compute.model.Instance;
+import com.google.api.services.compute.model.InstanceList;
+
+import org.elasticsearch.SpecialPermission;
+import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.cloud.gce.network.GceNameResolver;
+import org.elasticsearch.common.component.AbstractLifecycleComponent;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.network.NetworkService;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.TimeValue;
+
+import java.io.IOException;
+import java.net.URL;
+import java.security.AccessController;
+import java.security.GeneralSecurityException;
+import java.security.PrivilegedActionException;
+import java.security.PrivilegedExceptionAction;
+import java.util.*;
+
+public class GceComputeServiceImpl extends AbstractLifecycleComponent<GceComputeService>
+    implements GceComputeService {
+
+    private final String project;
+    private final List<String> zones;
+
+    // Forcing Google Token API URL as set in GCE SDK to
+    //      http://metadata/computeMetadata/v1/instance/service-accounts/default/token
+    // See https://developers.google.com/compute/docs/metadata#metadataserver
+    public static final String GCE_METADATA_URL = "http://metadata.google.internal/computeMetadata/v1/instance";
+    public static final String TOKEN_SERVER_ENCODED_URL = GCE_METADATA_URL + "/service-accounts/default/token";
+
+    @Override
+    public Collection<Instance> instances() {
+        logger.debug("get instances for project [{}], zones [{}]", project, zones);
+        final List<Instance> instances = zones.stream().map((zoneId) -> {
+            try {
+                // hack around code messiness in GCE code
+                // TODO: get this fixed
+                SecurityManager sm = System.getSecurityManager();
+                if (sm != null) {
+                    sm.checkPermission(new SpecialPermission());
+                }
+                InstanceList instanceList = AccessController.doPrivileged(new PrivilegedExceptionAction<InstanceList>() {
+                    @Override
+                    public InstanceList run() throws Exception {
+                        Compute.Instances.List list = client().instances().list(project, zoneId);
+                        return list.execute();
+                    }
+                });
+                if (instanceList.isEmpty()) {
+                    return Collections.EMPTY_LIST;
+                }
+                return instanceList.getItems();
+            } catch (PrivilegedActionException e) {
+                logger.warn("Problem fetching instance list for zone {}", zoneId);
+                logger.debug("Full exception:", e);
+                return Collections.EMPTY_LIST;
+            }
+        }).reduce(new ArrayList<>(), (a, b) -> {
+            a.addAll(b);
+            return a;
+        });
+
+        if (instances.isEmpty()) {
+            logger.warn("disabling GCE discovery. Can not get list of nodes");
+        }
+
+        return instances;
+    }
+
+    @Override
+    public String metadata(String metadataPath) throws IOException {
+        String urlMetadataNetwork = GCE_METADATA_URL + "/" + metadataPath;
+        logger.debug("get metadata from [{}]", urlMetadataNetwork);
+        URL url = new URL(urlMetadataNetwork);
+        HttpHeaders headers;
+        try {
+            // hack around code messiness in GCE code
+            // TODO: get this fixed
+            headers = AccessController.doPrivileged(new PrivilegedExceptionAction<HttpHeaders>() {
+                @Override
+                public HttpHeaders run() throws IOException {
+                    return new HttpHeaders();
+                }
+            });
+
+            // This is needed to query meta data: https://cloud.google.com/compute/docs/metadata
+            headers.put("Metadata-Flavor", "Google");
+            HttpResponse response;
+            response = getGceHttpTransport().createRequestFactory()
+                    .buildGetRequest(new GenericUrl(url))
+                    .setHeaders(headers)
+                    .execute();
+            String metadata = response.parseAsString();
+            logger.debug("metadata found [{}]", metadata);
+            return metadata;
+        } catch (Exception e) {
+            throw new IOException("failed to fetch metadata from [" + urlMetadataNetwork + "]", e);
+        }
+    }
+
+    private Compute client;
+    private TimeValue refreshInterval = null;
+    private long lastRefresh;
+
+    /** Global instance of the HTTP transport. */
+    private HttpTransport gceHttpTransport;
+
+    /** Global instance of the JSON factory. */
+    private JsonFactory gceJsonFactory;
+
+    @Inject
+    public GceComputeServiceImpl(Settings settings, NetworkService networkService) {
+        super(settings);
+        this.project = settings.get(Fields.PROJECT);
+        String[] zoneList = settings.getAsArray(Fields.ZONE);
+        this.zones = Arrays.asList(zoneList);
+        networkService.addCustomNameResolver(new GceNameResolver(settings, this));
+    }
+
+    protected synchronized HttpTransport getGceHttpTransport() throws GeneralSecurityException, IOException {
+        if (gceHttpTransport == null) {
+            gceHttpTransport = GoogleNetHttpTransport.newTrustedTransport();
+        }
+        return gceHttpTransport;
+    }
+
+    public synchronized Compute client() {
+        if (refreshInterval != null && refreshInterval.millis() != 0) {
+            if (client != null &&
+                    (refreshInterval.millis() < 0 || (System.currentTimeMillis() - lastRefresh) < refreshInterval.millis())) {
+                if (logger.isTraceEnabled()) logger.trace("using cache to retrieve client");
+                return client;
+            }
+            lastRefresh = System.currentTimeMillis();
+        }
+
+        try {
+            gceJsonFactory = new JacksonFactory();
+
+            logger.info("starting GCE discovery service");
+            ComputeCredential credential = new ComputeCredential.Builder(getGceHttpTransport(), gceJsonFactory)
+                        .setTokenServerEncodedUrl(TOKEN_SERVER_ENCODED_URL)
+                    .build();
+
+            // hack around code messiness in GCE code
+            // TODO: get this fixed
+            SecurityManager sm = System.getSecurityManager();
+            if (sm != null) {
+                sm.checkPermission(new SpecialPermission());
+            }
+            AccessController.doPrivileged(new PrivilegedExceptionAction<Void>() {
+                @Override
+                public Void run() throws IOException {
+                    credential.refreshToken();
+                    return null;
+                }
+            });
+
+            logger.debug("token [{}] will expire in [{}] s", credential.getAccessToken(), credential.getExpiresInSeconds());
+            if (credential.getExpiresInSeconds() != null) {
+                refreshInterval = TimeValue.timeValueSeconds(credential.getExpiresInSeconds()-1);
+            }
+
+            // Once done, let's use this token
+            this.client = new Compute.Builder(getGceHttpTransport(), gceJsonFactory, null)
+                    .setApplicationName(Fields.VERSION)
+                    .setHttpRequestInitializer(credential)
+                    .build();
+        } catch (Exception e) {
+            logger.warn("unable to start GCE discovery service", e);
+            throw new IllegalArgumentException("unable to start GCE discovery service", e);
+        }
+
+        return this.client;
+    }
+
+    @Override
+    protected void doStart() throws ElasticsearchException {
+    }
+
+    @Override
+    protected void doStop() throws ElasticsearchException {
+        if (gceHttpTransport != null) {
+            try {
+                gceHttpTransport.shutdown();
+            } catch (IOException e) {
+                logger.warn("unable to shutdown GCE Http Transport", e);
+            }
+            gceHttpTransport = null;
+        }
+    }
+
+    @Override
+    protected void doClose() throws ElasticsearchException {
+    }
+}
diff --git a/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceModule.java b/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceModule.java
new file mode 100644
index 0000000..e1b8d6c
--- /dev/null
+++ b/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceModule.java
@@ -0,0 +1,36 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.gce;
+
+import org.elasticsearch.common.inject.AbstractModule;
+
+public class GceModule extends AbstractModule {
+    // pkg private so tests can override with mock
+    static Class<? extends GceComputeService> computeServiceImpl = GceComputeServiceImpl.class;
+
+    public static Class<? extends GceComputeService> getComputeServiceImpl() {
+        return computeServiceImpl;
+    }
+
+    @Override
+    protected void configure() {
+        bind(GceComputeService.class).to(computeServiceImpl).asEagerSingleton();
+    }
+}
diff --git a/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/network/GceNameResolver.java b/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/network/GceNameResolver.java
new file mode 100644
index 0000000..22d79fb
--- /dev/null
+++ b/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/network/GceNameResolver.java
@@ -0,0 +1,132 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.gce.network;
+
+import org.elasticsearch.cloud.gce.GceComputeService;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.network.NetworkService.CustomNameResolver;
+import org.elasticsearch.common.settings.Settings;
+
+import java.io.IOException;
+import java.net.InetAddress;
+
+/**
+ * <p>Resolves certain GCE related 'meta' hostnames into an actual hostname
+ * obtained from gce meta-data.</p>
+ * Valid config values for {@link GceAddressResolverType}s are -
+ * <ul>
+ * <li>_gce_ - maps to privateIp</li>
+ * <li>_gce:privateIp_</li>
+ * <li>_gce:hostname_</li>
+ * </ul>
+ */
+public class GceNameResolver extends AbstractComponent implements CustomNameResolver {
+
+    private final GceComputeService gceComputeService;
+
+    /**
+     * enum that can be added to over time with more meta-data types
+     */
+    private enum GceAddressResolverType {
+
+        /**
+         * Using the hostname
+         */
+        PRIVATE_DNS("gce:hostname", "hostname"),
+        /**
+         * Can be gce:privateIp, gce:privateIp:X where X is the network interface
+         */
+        PRIVATE_IP("gce:privateIp", "network-interfaces/{{network}}/ip"),
+        /**
+         * same as "gce:privateIp" or "gce:privateIp:0"
+         */
+        GCE("gce", PRIVATE_IP.gceName);
+
+        final String configName;
+        final String gceName;
+
+        GceAddressResolverType(String configName, String gceName) {
+            this.configName = configName;
+            this.gceName = gceName;
+        }
+    }
+
+    /**
+     * Construct a {@link CustomNameResolver}.
+     */
+    public GceNameResolver(Settings settings, GceComputeService gceComputeService) {
+        super(settings);
+        this.gceComputeService = gceComputeService;
+    }
+
+    /**
+     * @param value the gce hostname type to discover.
+     * @return the appropriate host resolved from gce meta-data.
+     * @see CustomNameResolver#resolveIfPossible(String)
+     */
+    private InetAddress[] resolve(String value) throws IOException {
+        String gceMetadataPath;
+        if (value.equals(GceAddressResolverType.GCE.configName)) {
+            // We replace network placeholder with default network interface value: 0
+            gceMetadataPath = Strings.replace(GceAddressResolverType.GCE.gceName, "{{network}}", "0");
+        } else if (value.equals(GceAddressResolverType.PRIVATE_DNS.configName)) {
+            gceMetadataPath = GceAddressResolverType.PRIVATE_DNS.gceName;
+        } else if (value.startsWith(GceAddressResolverType.PRIVATE_IP.configName)) {
+            // We extract the network interface from gce:privateIp:XX
+            String network = "0";
+            String[] privateIpConfig = Strings.splitStringToArray(value, ':');
+            if (privateIpConfig != null && privateIpConfig.length == 3) {
+                network = privateIpConfig[2];
+            }
+
+            // We replace network placeholder with network interface value
+            gceMetadataPath = Strings.replace(GceAddressResolverType.PRIVATE_IP.gceName, "{{network}}", network);
+        } else {
+            throw new IllegalArgumentException("[" + value + "] is not one of the supported GCE network.host setting. " +
+                    "Expecting _gce_, _gce:privateIp:X_, _gce:hostname_");
+        }
+
+        try {
+            String metadataResult = gceComputeService.metadata(gceMetadataPath);
+            if (metadataResult == null || metadataResult.length() == 0) {
+                throw new IOException("no gce metadata returned from [" + gceMetadataPath + "] for [" + value + "]");
+            }
+            // only one address: because we explicitly ask for only one via the GceHostnameType
+            return new InetAddress[] { InetAddress.getByName(metadataResult) };
+        } catch (IOException e) {
+            throw new IOException("IOException caught when fetching InetAddress from [" + gceMetadataPath + "]", e);
+        }
+    }
+
+    @Override
+    public InetAddress[] resolveDefault() {
+        return null; // using this, one has to explicitly specify _gce_ in network setting
+    }
+
+    @Override
+    public InetAddress[] resolveIfPossible(String value) throws IOException {
+        // We only try to resolve network.host setting when it starts with _gce
+        if (value.startsWith("gce")) {
+            return resolve(value);
+        }
+        return null;
+    }
+}
diff --git a/plugins/discovery-gce/src/main/java/org/elasticsearch/discovery/gce/GceDiscovery.java b/plugins/discovery-gce/src/main/java/org/elasticsearch/discovery/gce/GceDiscovery.java
new file mode 100755
index 0000000..f20d1c7
--- /dev/null
+++ b/plugins/discovery-gce/src/main/java/org/elasticsearch/discovery/gce/GceDiscovery.java
@@ -0,0 +1,49 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.gce;
+
+import org.elasticsearch.cluster.ClusterName;
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.discovery.DiscoverySettings;
+import org.elasticsearch.discovery.zen.ZenDiscovery;
+import org.elasticsearch.discovery.zen.elect.ElectMasterService;
+import org.elasticsearch.discovery.zen.ping.ZenPingService;
+import org.elasticsearch.node.settings.NodeSettingsService;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.elasticsearch.transport.TransportService;
+
+/**
+ *
+ */
+public class GceDiscovery extends ZenDiscovery {
+
+    public static final String GCE = "gce";
+
+    @Inject
+    public GceDiscovery(Settings settings, ClusterName clusterName, ThreadPool threadPool, TransportService transportService,
+                        ClusterService clusterService, NodeSettingsService nodeSettingsService, ZenPingService pingService,
+                        DiscoverySettings discoverySettings,
+                        ElectMasterService electMasterService) {
+        super(settings, clusterName, threadPool, transportService, clusterService, nodeSettingsService,
+                pingService, electMasterService, discoverySettings);
+    }
+}
diff --git a/plugins/discovery-gce/src/main/java/org/elasticsearch/discovery/gce/GceUnicastHostsProvider.java b/plugins/discovery-gce/src/main/java/org/elasticsearch/discovery/gce/GceUnicastHostsProvider.java
new file mode 100644
index 0000000..8feb9b8
--- /dev/null
+++ b/plugins/discovery-gce/src/main/java/org/elasticsearch/discovery/gce/GceUnicastHostsProvider.java
@@ -0,0 +1,249 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.gce;
+
+import com.google.api.services.compute.model.AccessConfig;
+import com.google.api.services.compute.model.Instance;
+import com.google.api.services.compute.model.NetworkInterface;
+
+import org.elasticsearch.Version;
+import org.elasticsearch.cloud.gce.GceComputeService;
+import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.network.NetworkAddress;
+import org.elasticsearch.common.network.NetworkService;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.transport.TransportAddress;
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.discovery.zen.ping.unicast.UnicastHostsProvider;
+import org.elasticsearch.transport.TransportService;
+
+import java.io.IOException;
+import java.net.InetAddress;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.List;
+
+import static org.elasticsearch.cloud.gce.GceComputeService.Fields;
+
+/**
+ *
+ */
+public class GceUnicastHostsProvider extends AbstractComponent implements UnicastHostsProvider {
+
+    static final class Status {
+        private static final String TERMINATED = "TERMINATED";
+    }
+
+    private final GceComputeService gceComputeService;
+    private TransportService transportService;
+    private NetworkService networkService;
+
+    private final Version version;
+    private final String project;
+    private final String[] zones;
+    private final String[] tags;
+
+    private final TimeValue refreshInterval;
+    private long lastRefresh;
+    private List<DiscoveryNode> cachedDiscoNodes;
+
+    @Inject
+    public GceUnicastHostsProvider(Settings settings, GceComputeService gceComputeService,
+            TransportService transportService,
+            NetworkService networkService,
+            Version version) {
+        super(settings);
+        this.gceComputeService = gceComputeService;
+        this.transportService = transportService;
+        this.networkService = networkService;
+        this.version = version;
+
+        this.refreshInterval = settings.getAsTime(Fields.REFRESH, TimeValue.timeValueSeconds(0));
+        this.project = settings.get(Fields.PROJECT);
+        this.zones = settings.getAsArray(Fields.ZONE);
+
+        this.tags = settings.getAsArray(Fields.TAGS);
+        if (logger.isDebugEnabled()) {
+            logger.debug("using tags {}", Arrays.asList(this.tags));
+        }
+    }
+
+    /**
+     * We build the list of Nodes from GCE Management API
+     * Information can be cached using `plugins.refresh_interval` property if needed.
+     * Setting `plugins.refresh_interval` to `-1` will cause infinite caching.
+     * Setting `plugins.refresh_interval` to `0` will disable caching (default).
+     */
+    @Override
+    public List<DiscoveryNode> buildDynamicNodes() {
+        if (refreshInterval.millis() != 0) {
+            if (cachedDiscoNodes != null &&
+                    (refreshInterval.millis() < 0 || (System.currentTimeMillis() - lastRefresh) < refreshInterval.millis())) {
+                if (logger.isTraceEnabled()) logger.trace("using cache to retrieve node list");
+                return cachedDiscoNodes;
+            }
+            lastRefresh = System.currentTimeMillis();
+        }
+        logger.debug("start building nodes list using GCE API");
+
+        cachedDiscoNodes = new ArrayList<>();
+        String ipAddress = null;
+        try {
+            InetAddress inetAddress = networkService.resolvePublishHostAddress(null);
+            if (inetAddress != null) {
+                ipAddress = NetworkAddress.formatAddress(inetAddress);
+            }
+        } catch (IOException e) {
+            // We can't find the publish host address... Hmmm. Too bad :-(
+            // We won't simply filter it
+        }
+
+        try {
+            Collection<Instance> instances = gceComputeService.instances();
+
+            if (instances == null) {
+                logger.trace("no instance found for project [{}], zones [{}].", this.project, this.zones);
+                return cachedDiscoNodes;
+            }
+
+            for (Instance instance : instances) {
+                String name = instance.getName();
+                String type = instance.getMachineType();
+
+                String status = instance.getStatus();
+                logger.trace("gce instance {} with status {} found.", name, status);
+
+                // We don't want to connect to TERMINATED status instances
+                // See https://github.com/elasticsearch/elasticsearch-cloud-gce/issues/3
+                if (Status.TERMINATED.equals(status)) {
+                    logger.debug("node {} is TERMINATED. Ignoring", name);
+                    continue;
+                }
+
+                // see if we need to filter by tag
+                boolean filterByTag = false;
+                if (tags.length > 0) {
+                    logger.trace("start filtering instance {} with tags {}.", name, tags);
+                    if (instance.getTags() == null || instance.getTags().isEmpty()
+                            || instance.getTags().getItems() == null || instance.getTags().getItems().isEmpty()) {
+                        // If this instance have no tag, we filter it
+                        logger.trace("no tags for this instance but we asked for tags. {} won't be part of the cluster.", name);
+                        filterByTag = true;
+                    } else {
+                        // check that all tags listed are there on the instance
+                        logger.trace("comparing instance tags {} with tags filter {}.", instance.getTags().getItems(), tags);
+                        for (String tag : tags) {
+                            boolean found = false;
+                            for (String instancetag : instance.getTags().getItems()) {
+                                if (instancetag.equals(tag)) {
+                                    found = true;
+                                    break;
+                                }
+                            }
+                            if (!found) {
+                                filterByTag = true;
+                                break;
+                            }
+                        }
+                    }
+                }
+                if (filterByTag) {
+                    logger.trace("filtering out instance {} based tags {}, not part of {}", name, tags,
+                            instance.getTags() == null || instance.getTags().getItems() == null ? "" : instance.getTags());
+                    continue;
+                } else {
+                    logger.trace("instance {} with tags {} is added to discovery", name, tags);
+                }
+
+                String ip_public = null;
+                String ip_private = null;
+
+                List<NetworkInterface> interfaces = instance.getNetworkInterfaces();
+
+                for (NetworkInterface networkInterface : interfaces) {
+                    if (ip_public == null) {
+                        // Trying to get Public IP Address (For future use)
+                        if (networkInterface.getAccessConfigs() != null) {
+                            for (AccessConfig accessConfig : networkInterface.getAccessConfigs()) {
+                                if (Strings.hasText(accessConfig.getNatIP())) {
+                                    ip_public = accessConfig.getNatIP();
+                                    break;
+                                }
+                            }
+                        }
+                    }
+
+                    if (ip_private == null) {
+                        ip_private = networkInterface.getNetworkIP();
+                    }
+
+                    // If we have both public and private, we can stop here
+                    if (ip_private != null && ip_public != null) break;
+                }
+
+                try {
+                    if (ip_private.equals(ipAddress)) {
+                        // We found the current node.
+                        // We can ignore it in the list of DiscoveryNode
+                        logger.trace("current node found. Ignoring {} - {}", name, ip_private);
+                    } else {
+                        String address = ip_private;
+                        // Test if we have es_port metadata defined here
+                        if (instance.getMetadata() != null && instance.getMetadata().containsKey("es_port")) {
+                            Object es_port = instance.getMetadata().get("es_port");
+                            logger.trace("es_port is defined with {}", es_port);
+                            if (es_port instanceof String) {
+                                address = address.concat(":").concat((String) es_port);
+                            } else {
+                                // Ignoring other values
+                                logger.trace("es_port is instance of {}. Ignoring...", es_port.getClass().getName());
+                            }
+                        }
+
+                        // ip_private is a single IP Address. We need to build a TransportAddress from it
+                        // If user has set `es_port` metadata, we don't need to ping all ports
+                        // we only limit to 1 addresses, makes no sense to ping 100 ports
+                        TransportAddress[] addresses = transportService.addressesFromString(address, 1);
+
+                        for (TransportAddress transportAddress : addresses) {
+                            logger.trace("adding {}, type {}, address {}, transport_address {}, status {}", name, type,
+                                    ip_private, transportAddress, status);
+                            cachedDiscoNodes.add(new DiscoveryNode("#cloud-" + name + "-" + 0, transportAddress, version.minimumCompatibilityVersion()));
+                        }
+                    }
+                } catch (Exception e) {
+                    logger.warn("failed to add {}, address {}", e, name, ip_private);
+                }
+
+            }
+        } catch (Throwable e) {
+            logger.warn("Exception caught during discovery: {}", e, e.getMessage());
+        }
+
+        logger.debug("{} node(s) added", cachedDiscoNodes.size());
+        logger.debug("using dynamic discovery nodes {}", cachedDiscoNodes);
+
+        return cachedDiscoNodes;
+    }
+}
diff --git a/plugins/discovery-gce/src/main/java/org/elasticsearch/plugin/discovery/gce/GceDiscoveryPlugin.java b/plugins/discovery-gce/src/main/java/org/elasticsearch/plugin/discovery/gce/GceDiscoveryPlugin.java
new file mode 100644
index 0000000..a17c396
--- /dev/null
+++ b/plugins/discovery-gce/src/main/java/org/elasticsearch/plugin/discovery/gce/GceDiscoveryPlugin.java
@@ -0,0 +1,125 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plugin.discovery.gce;
+
+import org.elasticsearch.cloud.gce.GceComputeService;
+import org.elasticsearch.cloud.gce.GceModule;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.component.LifecycleComponent;
+import org.elasticsearch.common.inject.Module;
+import org.elasticsearch.common.logging.ESLogger;
+import org.elasticsearch.common.logging.Loggers;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.discovery.DiscoveryModule;
+import org.elasticsearch.discovery.gce.GceDiscovery;
+import org.elasticsearch.discovery.gce.GceUnicastHostsProvider;
+import org.elasticsearch.plugins.Plugin;
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+
+public class GceDiscoveryPlugin extends Plugin {
+
+    private final Settings settings;
+    protected final ESLogger logger = Loggers.getLogger(GceDiscoveryPlugin.class);
+
+    public GceDiscoveryPlugin(Settings settings) {
+        this.settings = settings;
+    }
+
+    @Override
+    public String name() {
+        return "discovery-gce";
+    }
+
+    @Override
+    public String description() {
+        return "Cloud Google Compute Engine Discovery Plugin";
+    }
+
+    @Override
+    public Collection<Module> nodeModules() {
+        List<Module> modules = new ArrayList<>();
+        if (isDiscoveryAlive(settings, logger)) {
+            modules.add(new GceModule());
+        }
+        return modules;
+    }
+
+    @Override
+    public Collection<Class<? extends LifecycleComponent>> nodeServices() {
+        Collection<Class<? extends LifecycleComponent>> services = new ArrayList<>();
+        if (isDiscoveryAlive(settings, logger)) {
+            services.add(GceModule.getComputeServiceImpl());
+        }
+        return services;
+    }
+
+    public void onModule(DiscoveryModule discoveryModule) {
+        if (isDiscoveryAlive(settings, logger)) {
+            discoveryModule.addDiscoveryType("gce", GceDiscovery.class);
+            discoveryModule.addUnicastHostProvider(GceUnicastHostsProvider.class);
+        }
+    }
+
+    /**
+     * Check if discovery is meant to start
+     *
+     * @return true if we can start gce discovery features
+     */
+    public static boolean isDiscoveryAlive(Settings settings, ESLogger logger) {
+        // User set discovery.type: gce
+        if (GceDiscovery.GCE.equalsIgnoreCase(settings.get("discovery.type")) == false) {
+            logger.debug("discovery.type not set to {}", GceDiscovery.GCE);
+            return false;
+        }
+
+        if (checkProperty(GceComputeService.Fields.PROJECT, settings.get(GceComputeService.Fields.PROJECT), logger) == false ||
+                checkProperty(GceComputeService.Fields.ZONE, settings.getAsArray(GceComputeService.Fields.ZONE), logger) == false) {
+            logger.debug("one or more gce discovery settings are missing. " +
+                            "Check elasticsearch.yml file. Should have [{}] and [{}].",
+                    GceComputeService.Fields.PROJECT,
+                    GceComputeService.Fields.ZONE);
+            return false;
+        }
+
+        logger.trace("all required properties for gce discovery are set!");
+
+        return true;
+    }
+
+    private static boolean checkProperty(String name, String value, ESLogger logger) {
+        if (!Strings.hasText(value)) {
+            logger.warn("{} is not set.", name);
+            return false;
+        }
+        return true;
+    }
+
+    private static boolean checkProperty(String name, String[] values, ESLogger logger) {
+        if (values == null || values.length == 0) {
+            logger.warn("{} is not set.", name);
+            return false;
+        }
+        return true;
+    }
+
+}
diff --git a/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/DiscoveryGCERestIT.java b/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/DiscoveryGCERestIT.java
new file mode 100644
index 0000000..1a21839
--- /dev/null
+++ b/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/DiscoveryGCERestIT.java
@@ -0,0 +1,49 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.gce;
+
+import com.carrotsearch.randomizedtesting.annotations.Name;
+import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
+import org.elasticsearch.plugin.discovery.gce.GceDiscoveryPlugin;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.test.rest.ESRestTestCase;
+import org.elasticsearch.test.rest.RestTestCandidate;
+import org.elasticsearch.test.rest.parser.RestTestParseException;
+
+import java.io.IOException;
+import java.util.Collection;
+
+public class DiscoveryGCERestIT extends ESRestTestCase {
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return pluginList(GceDiscoveryPlugin.class);
+    }
+
+    public DiscoveryGCERestIT(@Name("yaml") RestTestCandidate testCandidate) {
+        super(testCandidate);
+    }
+
+    @ParametersFactory
+    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
+        return ESRestTestCase.createParameters(0, 1);
+    }
+}
+
diff --git a/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceComputeServiceMock.java b/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceComputeServiceMock.java
new file mode 100644
index 0000000..dcbd53f
--- /dev/null
+++ b/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceComputeServiceMock.java
@@ -0,0 +1,113 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.gce;
+
+import com.google.api.client.http.HttpTransport;
+import com.google.api.client.http.LowLevelHttpRequest;
+import com.google.api.client.http.LowLevelHttpResponse;
+import com.google.api.client.json.Json;
+import com.google.api.client.testing.http.MockHttpTransport;
+import com.google.api.client.testing.http.MockLowLevelHttpRequest;
+import com.google.api.client.testing.http.MockLowLevelHttpResponse;
+import org.elasticsearch.cloud.gce.GceComputeServiceImpl;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.io.Streams;
+import org.elasticsearch.common.network.NetworkService;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.util.Callback;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.net.URL;
+import java.security.GeneralSecurityException;
+
+/**
+ *
+ */
+public class GceComputeServiceMock extends GceComputeServiceImpl {
+
+    protected HttpTransport mockHttpTransport;
+
+    public GceComputeServiceMock(Settings settings, NetworkService networkService) {
+        super(settings, networkService);
+        this.mockHttpTransport = configureMock();
+    }
+
+    @Override
+    protected HttpTransport getGceHttpTransport() throws GeneralSecurityException, IOException {
+        return this.mockHttpTransport;
+    }
+
+    protected HttpTransport configureMock() {
+        return new MockHttpTransport() {
+            @Override
+            public LowLevelHttpRequest buildRequest(String method, final String url) throws IOException {
+                return new MockLowLevelHttpRequest() {
+                    @Override
+                    public LowLevelHttpResponse execute() throws IOException {
+                        MockLowLevelHttpResponse response = new MockLowLevelHttpResponse();
+                        response.setStatusCode(200);
+                        response.setContentType(Json.MEDIA_TYPE);
+                        if (url.startsWith(GCE_METADATA_URL)) {
+                            logger.info("--> Simulate GCE Auth/Metadata response for [{}]", url);
+                            response.setContent(readGoogleInternalJsonResponse(url));
+                        } else {
+                            logger.info("--> Simulate GCE API response for [{}]", url);
+                            response.setContent(readGoogleApiJsonResponse(url));
+                        }
+
+                        return response;
+                    }
+                };
+            }
+        };
+    }
+
+    private String readGoogleInternalJsonResponse(String url) throws IOException {
+        return readJsonResponse(url, "http://metadata.google.internal/");
+    }
+
+    private String readGoogleApiJsonResponse(String url) throws IOException {
+        return readJsonResponse(url, "https://www.googleapis.com/");
+    }
+
+    private String readJsonResponse(String url, String urlRoot) throws IOException {
+        // We extract from the url the mock file path we want to use
+        String mockFileName = Strings.replace(url, urlRoot, "");
+
+        logger.debug("--> read mock file from [{}]", mockFileName);
+        URL resource = GceComputeServiceMock.class.getResource(mockFileName);
+        if (resource == null) {
+            throw new IOException("can't read [" + url + "] in src/test/resources/org/elasticsearch/discovery/gce");
+        }
+        try (InputStream is = resource.openStream()) {
+            final StringBuilder sb = new StringBuilder();
+            Streams.readAllLines(is, new Callback<String>() {
+                @Override
+                public void handle(String s) {
+                    sb.append(s);
+                }
+            });
+            String response = sb.toString();
+            logger.trace("{}", response);
+            return response;
+        }
+    }
+}
diff --git a/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoverySettingsTests.java b/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoverySettingsTests.java
new file mode 100644
index 0000000..334c685
--- /dev/null
+++ b/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoverySettingsTests.java
@@ -0,0 +1,77 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.gce;
+
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.plugin.discovery.gce.GceDiscoveryPlugin;
+import org.elasticsearch.test.ESTestCase;
+
+import static org.hamcrest.Matchers.is;
+
+public class GceDiscoverySettingsTests extends ESTestCase {
+    public void testDiscoveryReady() {
+        Settings settings = Settings.builder()
+                .put("discovery.type", "gce")
+                .put("cloud.gce.project_id", "gce_id")
+                .putArray("cloud.gce.zone", "gce_zones_1", "gce_zones_2")
+                .build();
+
+        boolean discoveryReady = GceDiscoveryPlugin.isDiscoveryAlive(settings, logger);
+        assertThat(discoveryReady, is(true));
+    }
+
+    public void testDiscoveryNotReady() {
+        Settings settings = Settings.EMPTY;
+        boolean discoveryReady = GceDiscoveryPlugin.isDiscoveryAlive(settings, logger);
+        assertThat(discoveryReady, is(false));
+
+        settings = Settings.builder()
+                .put("discovery.type", "gce")
+                .build();
+
+        discoveryReady = GceDiscoveryPlugin.isDiscoveryAlive(settings, logger);
+        assertThat(discoveryReady, is(false));
+
+        settings = Settings.builder()
+                .put("discovery.type", "gce")
+                .put("cloud.gce.project_id", "gce_id")
+                .build();
+
+        discoveryReady = GceDiscoveryPlugin.isDiscoveryAlive(settings, logger);
+        assertThat(discoveryReady, is(false));
+
+
+        settings = Settings.builder()
+                .put("discovery.type", "gce")
+                .putArray("cloud.gce.zone", "gce_zones_1", "gce_zones_2")
+                .build();
+
+        discoveryReady = GceDiscoveryPlugin.isDiscoveryAlive(settings, logger);
+        assertThat(discoveryReady, is(false));
+
+        settings = Settings.builder()
+                .put("cloud.gce.project_id", "gce_id")
+                .putArray("cloud.gce.zone", "gce_zones_1", "gce_zones_2")
+                .build();
+
+        discoveryReady = GceDiscoveryPlugin.isDiscoveryAlive(settings, logger);
+        assertThat(discoveryReady, is(false));
+    }
+}
diff --git a/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoveryTests.java b/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoveryTests.java
new file mode 100644
index 0000000..450ff72
--- /dev/null
+++ b/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoveryTests.java
@@ -0,0 +1,223 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.gce;
+
+import org.elasticsearch.Version;
+import org.elasticsearch.cloud.gce.GceComputeService;
+import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
+import org.elasticsearch.common.network.NetworkService;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.transport.MockTransportService;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.elasticsearch.transport.local.LocalTransport;
+import org.junit.*;
+
+import java.util.List;
+import java.util.Locale;
+
+import static org.hamcrest.Matchers.hasSize;
+import static org.hamcrest.Matchers.is;
+
+/**
+ * This test class uses a GCE HTTP Mock system which allows to simulate JSON Responses.
+ *
+ * To implement a new test you'll need to create an `instances.json` file which contains expected response
+ * for a given project-id and zone under the src/test/resources/org/elasticsearch/discovery/gce with dir name:
+ *
+ * compute/v1/projects/[project-id]/zones/[zone]
+ *
+ * By default, project-id is the test method name, lowercase.
+ *
+ * For example, if you create a test `myNewAwesomeTest` with following settings:
+ *
+ * Settings nodeSettings = Settings.builder()
+ *  .put(GceComputeService.Fields.PROJECT, projectName)
+ *  .put(GceComputeService.Fields.ZONE, "europe-west1-b")
+ *  .build();
+ *
+ *  You need to create a file under `src/test/resources/org/elasticsearch/discovery/gce/` named:
+ *
+ *  compute/v1/projects/mynewawesometest/zones/europe-west1-b/instances.json
+ *
+ */
+public class GceDiscoveryTests extends ESTestCase {
+
+    protected static ThreadPool threadPool;
+    protected MockTransportService transportService;
+    protected NetworkService networkService;
+    protected GceComputeService mock;
+    protected String projectName;
+
+    @BeforeClass
+    public static void createThreadPool() {
+        threadPool = new ThreadPool(GceDiscoveryTests.class.getName());
+    }
+
+    @AfterClass
+    public static void stopThreadPool() {
+        if (threadPool !=null) {
+            threadPool.shutdownNow();
+            threadPool = null;
+        }
+    }
+
+    @Before
+    public void setProjectName() {
+        projectName = getTestName().toLowerCase(Locale.ROOT);
+    }
+
+    @Before
+    public void createTransportService() {
+        transportService = new MockTransportService(
+                Settings.EMPTY,
+                new LocalTransport(Settings.EMPTY, threadPool, Version.CURRENT, new NamedWriteableRegistry()), threadPool);
+    }
+
+    @Before
+    public void createNetworkService() {
+        networkService = new NetworkService(Settings.EMPTY);
+    }
+
+    @After
+    public void stopGceComputeService() {
+        if (mock != null) {
+            mock.stop();
+        }
+    }
+
+    protected List<DiscoveryNode> buildDynamicNodes(GceComputeService gceComputeService, Settings nodeSettings) {
+        GceUnicastHostsProvider provider = new GceUnicastHostsProvider(nodeSettings, gceComputeService,
+                transportService, new NetworkService(Settings.EMPTY), Version.CURRENT);
+
+        List<DiscoveryNode> discoveryNodes = provider.buildDynamicNodes();
+        logger.info("--> nodes found: {}", discoveryNodes);
+        return discoveryNodes;
+    }
+
+    @Test
+    public void nodesWithDifferentTagsAndNoTagSet() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings, networkService);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(2));
+    }
+
+    @Test
+    public void nodesWithDifferentTagsAndOneTagSet() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
+                .putArray(GceComputeService.Fields.TAGS, "elasticsearch")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings, networkService);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(1));
+        assertThat(discoveryNodes.get(0).getId(), is("#cloud-test2-0"));
+    }
+
+    @Test
+    public void nodesWithDifferentTagsAndTwoTagSet() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
+                .putArray(GceComputeService.Fields.TAGS, "elasticsearch", "dev")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings, networkService);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(1));
+        assertThat(discoveryNodes.get(0).getId(), is("#cloud-test2-0"));
+    }
+
+    @Test
+    public void nodesWithSameTagsAndNoTagSet() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings, networkService);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(2));
+    }
+
+    @Test
+    public void nodesWithSameTagsAndOneTagSet() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
+                .putArray(GceComputeService.Fields.TAGS, "elasticsearch")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings, networkService);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(2));
+    }
+
+    @Test
+    public void nodesWithSameTagsAndTwoTagsSet() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .put(GceComputeService.Fields.ZONE, "europe-west1-b")
+                .putArray(GceComputeService.Fields.TAGS, "elasticsearch", "dev")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings, networkService);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(2));
+    }
+
+    @Test
+    public void multipleZonesAndTwoNodesInSameZone() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .putArray(GceComputeService.Fields.ZONE, "us-central1-a", "europe-west1-b")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings, networkService);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(2));
+    }
+
+    @Test
+    public void multipleZonesAndTwoNodesInDifferentZones() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .putArray(GceComputeService.Fields.ZONE, "us-central1-a", "europe-west1-b")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings, networkService);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(2));
+    }
+
+    /**
+     * For issue https://github.com/elastic/elasticsearch-cloud-gce/issues/43
+     */
+    @Test
+    public void zeroNode43() {
+        Settings nodeSettings = Settings.builder()
+                .put(GceComputeService.Fields.PROJECT, projectName)
+                .putArray(GceComputeService.Fields.ZONE, "us-central1-a", "us-central1-b")
+                .build();
+        mock = new GceComputeServiceMock(nodeSettings, networkService);
+        List<DiscoveryNode> discoveryNodes = buildDynamicNodes(mock, nodeSettings);
+        assertThat(discoveryNodes, hasSize(0));
+    }
+}
diff --git a/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceNetworkTests.java b/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceNetworkTests.java
new file mode 100644
index 0000000..7550cdc
--- /dev/null
+++ b/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceNetworkTests.java
@@ -0,0 +1,132 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.gce;
+
+import org.elasticsearch.cloud.gce.network.GceNameResolver;
+import org.elasticsearch.common.network.NetworkService;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.net.InetAddress;
+
+import static org.hamcrest.Matchers.arrayContaining;
+import static org.hamcrest.Matchers.containsString;
+
+/**
+ * Test for GCE network.host settings.
+ * Related to https://github.com/elastic/elasticsearch/issues/13605
+ */
+public class GceNetworkTests extends ESTestCase {
+
+    /**
+     * Test for network.host: _gce_
+     */
+    @Test
+    public void networkHostGceDefault() throws IOException {
+        resolveGce("_gce_", InetAddress.getByName("10.240.0.2"));
+    }
+
+    /**
+     * Test for network.host: _gce:privateIp_
+     */
+    @Test
+    public void networkHostPrivateIp() throws IOException {
+        resolveGce("_gce:privateIp_", InetAddress.getByName("10.240.0.2"));
+    }
+
+    /**
+     * Test for network.host: _gce:hostname_
+     */
+    @Test
+    public void networkHostPrivateDns() throws IOException {
+        resolveGce("_gce:hostname_", InetAddress.getByName("localhost"));
+    }
+
+    /**
+     * Test for network.host: _gce:doesnotexist_
+     * This should raise an IllegalArgumentException as this setting does not exist
+     */
+    @Test
+    public void networkHostWrongSetting() throws IOException {
+        resolveGce("_gce:doesnotexist_", (InetAddress) null);
+    }
+
+    /**
+     * Test with multiple network interfaces:
+     * network.host: _gce:privateIp:0_
+     * network.host: _gce:privateIp:1_
+     */
+    @Test
+    public void networkHostPrivateIpInterface() throws IOException {
+        resolveGce("_gce:privateIp:0_", InetAddress.getByName("10.240.0.2"));
+        resolveGce("_gce:privateIp:1_", InetAddress.getByName("10.150.0.1"));
+    }
+
+    /**
+     * Test that we don't have any regression with network host core settings such as
+     * network.host: _local_
+     */
+    @Test
+    public void networkHostCoreLocal() throws IOException {
+        resolveGce("_local_", new NetworkService(Settings.EMPTY).resolveBindHostAddress(NetworkService.DEFAULT_NETWORK_HOST));
+    }
+
+    /**
+     * Utility test method to test different settings
+     * @param gceNetworkSetting tested network.host property
+     * @param expected expected InetAddress, null if we expect an exception
+     * @throws IOException Well... If something goes wrong :)
+     */
+    private void resolveGce(String gceNetworkSetting, InetAddress expected) throws IOException {
+        resolveGce(gceNetworkSetting, expected == null ? null : new InetAddress [] { expected });
+    }
+
+    /**
+     * Utility test method to test different settings
+     * @param gceNetworkSetting tested network.host property
+     * @param expected expected InetAddress, null if we expect an exception
+     * @throws IOException Well... If something goes wrong :)
+     */
+    private void resolveGce(String gceNetworkSetting, InetAddress[] expected) throws IOException {
+        Settings nodeSettings = Settings.builder()
+                .put("network.host", gceNetworkSetting)
+                .build();
+
+        NetworkService networkService = new NetworkService(nodeSettings);
+        GceComputeServiceMock mock = new GceComputeServiceMock(nodeSettings, networkService);
+        networkService.addCustomNameResolver(new GceNameResolver(nodeSettings, mock));
+        try {
+            InetAddress[] addresses = networkService.resolveBindHostAddress(null);
+            if (expected == null) {
+                fail("We should get a IllegalArgumentException when setting network.host: _gce:doesnotexist_");
+            }
+            assertThat(addresses, arrayContaining(expected));
+        } catch (IllegalArgumentException e) {
+            if (expected != null) {
+                // We were expecting something and not an exception
+                throw e;
+            }
+            // We check that we get the expected exception
+            assertThat(e.getMessage(), containsString("is not one of the supported GCE network.host setting"));
+        }
+    }
+}
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesindifferentzones/zones/europe-west1-b/instances b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesindifferentzones/zones/europe-west1-b/instances
new file mode 100644
index 0000000..049e0e1
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesindifferentzones/zones/europe-west1-b/instances
@@ -0,0 +1,36 @@
+{
+  "id": "dummy",
+  "items":[
+    {
+      "description": "ES Node 1",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test1",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "elasticsearch",
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    }
+  ]
+}
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesindifferentzones/zones/us-central1-a/instances b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesindifferentzones/zones/us-central1-a/instances
new file mode 100644
index 0000000..7e1e5d5
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesindifferentzones/zones/us-central1-a/instances
@@ -0,0 +1,36 @@
+{
+  "id": "dummy",
+  "items":[
+    {
+      "description": "ES Node 2",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test2",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "elasticsearch",
+          "dev"
+        ]
+      },
+      "zone": "us-central1-a"
+    }
+  ]
+}
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesinsamezone/zones/europe-west1-b/instances b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesinsamezone/zones/europe-west1-b/instances
new file mode 100644
index 0000000..78de693
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesinsamezone/zones/europe-west1-b/instances
@@ -0,0 +1,67 @@
+{
+  "id": "dummy",
+  "items":[
+    {
+      "description": "ES Node 1",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test1",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "elasticsearch",
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    },
+    {
+      "description": "ES Node 2",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test2",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "elasticsearch",
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    }
+  ]
+}
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesinsamezone/zones/us-central1-a/instances b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesinsamezone/zones/us-central1-a/instances
new file mode 100644
index 0000000..54c3836
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/multiplezonesandtwonodesinsamezone/zones/us-central1-a/instances
@@ -0,0 +1,5 @@
+{
+  "id": "dummy",
+  "items":[
+  ]
+}
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandnotagset/zones/europe-west1-b/instances b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandnotagset/zones/europe-west1-b/instances
new file mode 100644
index 0000000..1ca810c
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandnotagset/zones/europe-west1-b/instances
@@ -0,0 +1,66 @@
+{
+  "id": "dummy",
+  "items":[
+    {
+      "description": "ES Node 1",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test1",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    },
+    {
+      "description": "ES Node 2",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test2",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "elasticsearch",
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    }
+  ]
+}
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandonetagset/zones/europe-west1-b/instances b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandonetagset/zones/europe-west1-b/instances
new file mode 100644
index 0000000..1ca810c
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandonetagset/zones/europe-west1-b/instances
@@ -0,0 +1,66 @@
+{
+  "id": "dummy",
+  "items":[
+    {
+      "description": "ES Node 1",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test1",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    },
+    {
+      "description": "ES Node 2",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test2",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "elasticsearch",
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    }
+  ]
+}
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandtwotagset/zones/europe-west1-b/instances b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandtwotagset/zones/europe-west1-b/instances
new file mode 100644
index 0000000..1ca810c
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithdifferenttagsandtwotagset/zones/europe-west1-b/instances
@@ -0,0 +1,66 @@
+{
+  "id": "dummy",
+  "items":[
+    {
+      "description": "ES Node 1",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test1",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    },
+    {
+      "description": "ES Node 2",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test2",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "elasticsearch",
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    }
+  ]
+}
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandnotagset/zones/europe-west1-b/instances b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandnotagset/zones/europe-west1-b/instances
new file mode 100644
index 0000000..78de693
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandnotagset/zones/europe-west1-b/instances
@@ -0,0 +1,67 @@
+{
+  "id": "dummy",
+  "items":[
+    {
+      "description": "ES Node 1",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test1",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "elasticsearch",
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    },
+    {
+      "description": "ES Node 2",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test2",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "elasticsearch",
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    }
+  ]
+}
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandonetagset/zones/europe-west1-b/instances b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandonetagset/zones/europe-west1-b/instances
new file mode 100644
index 0000000..78de693
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandonetagset/zones/europe-west1-b/instances
@@ -0,0 +1,67 @@
+{
+  "id": "dummy",
+  "items":[
+    {
+      "description": "ES Node 1",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test1",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "elasticsearch",
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    },
+    {
+      "description": "ES Node 2",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test2",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "elasticsearch",
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    }
+  ]
+}
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandtwotagsset/zones/europe-west1-b/instances b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandtwotagsset/zones/europe-west1-b/instances
new file mode 100644
index 0000000..78de693
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/nodeswithsametagsandtwotagsset/zones/europe-west1-b/instances
@@ -0,0 +1,67 @@
+{
+  "id": "dummy",
+  "items":[
+    {
+      "description": "ES Node 1",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test1",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "elasticsearch",
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    },
+    {
+      "description": "ES Node 2",
+      "id": "9309873766428965105",
+      "kind": "compute#instance",
+      "machineType": "n1-standard-1",
+      "name": "test2",
+      "networkInterfaces": [
+        {
+          "accessConfigs": [
+            {
+              "kind": "compute#accessConfig",
+              "name": "External NAT",
+              "natIP": "104.155.13.147",
+              "type": "ONE_TO_ONE_NAT"
+            }
+          ],
+          "name": "nic0",
+          "network": "default",
+          "networkIP": "10.240.79.59"
+        }
+      ],
+      "status": "RUNNING",
+      "tags": {
+        "fingerprint": "xA6QJb-rGtg=",
+        "items": [
+          "elasticsearch",
+          "dev"
+        ]
+      },
+      "zone": "europe-west1-b"
+    }
+  ]
+}
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/zeronode43/zones/us-central1-a/instances b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/zeronode43/zones/us-central1-a/instances
new file mode 100644
index 0000000..54c3836
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/zeronode43/zones/us-central1-a/instances
@@ -0,0 +1,5 @@
+{
+  "id": "dummy",
+  "items":[
+  ]
+}
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/zeronode43/zones/us-central1-b/instances b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/zeronode43/zones/us-central1-b/instances
new file mode 100644
index 0000000..54c3836
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/compute/v1/projects/zeronode43/zones/us-central1-b/instances
@@ -0,0 +1,5 @@
+{
+  "id": "dummy",
+  "items":[
+  ]
+}
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/hostname b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/hostname
new file mode 100644
index 0000000..2fbb50c
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/hostname
@@ -0,0 +1 @@
+localhost
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/network-interfaces/0/ip b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/network-interfaces/0/ip
new file mode 100644
index 0000000..1ac79d6
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/network-interfaces/0/ip
@@ -0,0 +1 @@
+10.240.0.2
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/network-interfaces/1/ip b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/network-interfaces/1/ip
new file mode 100644
index 0000000..e3bb0f8
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/network-interfaces/1/ip
@@ -0,0 +1 @@
+10.150.0.1
diff --git a/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/service-accounts/default/token b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/service-accounts/default/token
new file mode 100644
index 0000000..b338f61
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/org/elasticsearch/discovery/gce/computeMetadata/v1/instance/service-accounts/default/token
@@ -0,0 +1,4 @@
+{
+  "access_token":"1/fFAGRNJru1FTz70BzhT3Zg",
+  "token_type":"Bearer"
+}
diff --git a/plugins/discovery-gce/src/test/resources/rest-api-spec/test/discovery_gce/10_basic.yaml b/plugins/discovery-gce/src/test/resources/rest-api-spec/test/discovery_gce/10_basic.yaml
new file mode 100644
index 0000000..8f5fbdc
--- /dev/null
+++ b/plugins/discovery-gce/src/test/resources/rest-api-spec/test/discovery_gce/10_basic.yaml
@@ -0,0 +1,14 @@
+# Integration tests for Discovery GCE components
+#
+"Discovery GCE loaded":
+    - do:
+        cluster.state: {}
+
+    # Get master node id
+    - set: { master_node: master }
+
+    - do:
+        nodes.info: {}
+
+    - match:  { nodes.$master.plugins.0.name: discovery-gce  }
+    - match:  { nodes.$master.plugins.0.jvm: true  }
diff --git a/plugins/lang-expression/src/test/java/org/elasticsearch/script/expression/IndexedExpressionTests.java b/plugins/lang-expression/src/test/java/org/elasticsearch/script/expression/IndexedExpressionTests.java
index b8d7044..b91450f 100644
--- a/plugins/lang-expression/src/test/java/org/elasticsearch/script/expression/IndexedExpressionTests.java
+++ b/plugins/lang-expression/src/test/java/org/elasticsearch/script/expression/IndexedExpressionTests.java
@@ -19,13 +19,11 @@
 
 package org.elasticsearch.script.expression;
 
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.search.aggregations.AggregationBuilders;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
@@ -37,7 +35,7 @@ import static org.hamcrest.Matchers.containsString;
 
 //TODO: please convert to unit tests!
 public class IndexedExpressionTests extends ESIntegTestCase {
-
+    
     @Override
     protected Settings nodeSettings(int nodeOrdinal) {
         Settings.Builder builder = Settings.builder().put(super.nodeSettings(nodeOrdinal));
@@ -47,7 +45,7 @@ public class IndexedExpressionTests extends ESIntegTestCase {
         builder.put("script.engine.expression.indexed.mapping", "off");
         return builder.build();
     }
-
+    
     @Override
     protected Collection<Class<? extends Plugin>> nodePlugins() {
         return Collections.singleton(ExpressionPlugin.class);
@@ -70,20 +68,16 @@ public class IndexedExpressionTests extends ESIntegTestCase {
             assertThat(e.getCause().getMessage(), containsString("scripts of type [indexed], operation [update] and lang [expression] are disabled"));
         }
         try {
-            client().prepareSearch()
-                    .setSource(
-                            new SearchSourceBuilder().scriptField("test1", new Script("script1", ScriptType.INDEXED, "expression", null)))
-                    .setIndices("test").setTypes("scriptTest").get();
+            String query = "{ \"script_fields\" : { \"test1\" : { \"script_id\" : \"script1\", \"lang\":\"expression\" }}}";
+            client().prepareSearch().setSource(new BytesArray(query)).setIndices("test").setTypes("scriptTest").get();
             fail("search script should have been rejected");
         } catch(Exception e) {
             assertThat(e.toString(), containsString("scripts of type [indexed], operation [search] and lang [expression] are disabled"));
         }
         try {
-            client().prepareSearch("test")
-                    .setSource(
-                            new SearchSourceBuilder().aggregation(AggregationBuilders.terms("test").script(
-                                    new Script("script1", ScriptType.INDEXED, "expression", null)))).get();
-        } catch (Exception e) {
+            String source = "{\"aggs\": {\"test\": { \"terms\" : { \"script_id\":\"script1\", \"script_lang\":\"expression\" } } } }";
+            client().prepareSearch("test").setSource(new BytesArray(source)).get();
+        } catch(Exception e) {
             assertThat(e.toString(), containsString("scripts of type [indexed], operation [aggs] and lang [expression] are disabled"));
         }
     }
diff --git a/plugins/lang-expression/src/test/resources/rest-api-spec/test/lang_expression/20_search.yaml b/plugins/lang-expression/src/test/resources/rest-api-spec/test/lang_expression/20_search.yaml
index 36ff7f5..a0953a25 100644
--- a/plugins/lang-expression/src/test/resources/rest-api-spec/test/lang_expression/20_search.yaml
+++ b/plugins/lang-expression/src/test/resources/rest-api-spec/test/lang_expression/20_search.yaml
@@ -22,6 +22,6 @@ setup:
 ---
 "Expressions scripting test":
 
-  - do: { search: { body: { script_fields : { my_field : { script: { lang: expression, inline: 'doc["age"].value + 19' } } } } } }
+  - do: { search: { body: { script_fields : { my_field : { lang: expression, script: 'doc["age"].value + 19' } } } } }
   - match:  { hits.hits.0.fields.my_field.0: 42.0 }
 
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BulkTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BulkTests.java
index 6cba9ae..fa6e91b 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BulkTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BulkTests.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.messy.tests;
 
-import java.nio.charset.StandardCharsets;
-
 import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.indices.alias.Alias;
 import org.elasticsearch.action.bulk.BulkItemResponse;
@@ -49,21 +47,15 @@ import org.elasticsearch.script.groovy.GroovyPlugin;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
+import java.nio.charset.StandardCharsets;
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Collections;
 import java.util.concurrent.CyclicBarrier;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertExists;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHits;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.nullValue;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
 
 public class BulkTests extends ESIntegTestCase {
 
@@ -190,8 +182,8 @@ public class BulkTests extends ESIntegTestCase {
         assertThat(((UpdateResponse) bulkResponse.getItems()[2].getResponse()).getVersion(), equalTo(3l));
 
         bulkResponse = client().prepareBulk()
-                .add(client().prepareIndex("test", "type", "e1").setCreate(true).setSource("field", "1").setVersion(10).setVersionType(VersionType.EXTERNAL))
-                .add(client().prepareIndex("test", "type", "e2").setCreate(true).setSource("field", "1").setVersion(10).setVersionType(VersionType.EXTERNAL))
+                .add(client().prepareIndex("test", "type", "e1").setSource("field", "1").setVersion(10).setVersionType(VersionType.EXTERNAL))
+                .add(client().prepareIndex("test", "type", "e2").setSource("field", "1").setVersion(10).setVersionType(VersionType.EXTERNAL))
                 .add(client().prepareIndex("test", "type", "e1").setSource("field", "2").setVersion(12).setVersionType(VersionType.EXTERNAL)).get();
 
         assertTrue(((IndexResponse) bulkResponse.getItems()[0].getResponse()).isCreated());
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java
index 81971d6..a1ed4b7 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java
@@ -45,7 +45,6 @@ import org.elasticsearch.client.FilterClient;
 import org.elasticsearch.common.inject.AbstractModule;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.inject.Module;
-import org.elasticsearch.common.lucene.search.function.CombineFunction;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
@@ -56,7 +55,6 @@ import org.elasticsearch.index.query.MoreLikeThisQueryBuilder;
 import org.elasticsearch.index.query.MoreLikeThisQueryBuilder.Item;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.TermsQueryBuilder;
-import org.elasticsearch.index.query.functionscore.script.ScriptScoreFunctionBuilder;
 import org.elasticsearch.indices.cache.query.terms.TermsLookup;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.rest.RestController;
@@ -69,7 +67,6 @@ import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 import org.elasticsearch.search.aggregations.AggregationBuilders;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilders;
 import org.elasticsearch.search.suggest.Suggest;
-import org.elasticsearch.search.suggest.SuggestBuilder;
 import org.elasticsearch.search.suggest.phrase.PhraseSuggestionBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
@@ -277,12 +274,14 @@ public class ContextAndHeaderTransportTests extends ESIntegTestCase {
                 .get();
         transportClient().admin().indices().prepareRefresh(queryIndex).get();
 
+        // custom content, not sure how to specify "script_id" otherwise in the API
+        XContentBuilder builder = jsonBuilder().startObject().startObject("function_score").field("boost_mode", "replace").startArray("functions")
+                .startObject().startObject("script_score").field("script_id", "my_script").field("lang", "groovy").endObject().endObject().endArray().endObject().endObject();
+
         SearchResponse searchResponse = transportClient()
                 .prepareSearch(queryIndex)
-                .setQuery(
-                        QueryBuilders.functionScoreQuery(
-                                new ScriptScoreFunctionBuilder(new Script("my_script", ScriptType.INDEXED, "groovy", null))).boostMode(
-                                CombineFunction.REPLACE)).get();
+                .setQuery(builder)
+                .get();
         assertNoFailures(searchResponse);
         assertHitCount(searchResponse, 1);
         assertThat(searchResponse.getHits().getMaxScore(), is(10.0f));
@@ -444,13 +443,11 @@ public class ContextAndHeaderTransportTests extends ESIntegTestCase {
                 MustacheScriptEngineService.NAME, null, null));
 
         SearchRequestBuilder searchRequestBuilder = transportClient().prepareSearch("test").setSize(0);
-        SuggestBuilder suggestBuilder = new SuggestBuilder();
         String suggestText = "united states house of representatives elections in washington 2006";
         if (suggestText != null) {
-            suggestBuilder.setText(suggestText);
+            searchRequestBuilder.setSuggestText(suggestText);
         }
-        suggestBuilder.addSuggestion(filteredFilterSuggest);
-        searchRequestBuilder.suggest(suggestBuilder);
+        searchRequestBuilder.addSuggestion(filteredFilterSuggest);
         SearchResponse actionGet = searchRequestBuilder.execute().actionGet();
         assertThat(Arrays.toString(actionGet.getShardFailures()), actionGet.getFailedShards(), equalTo(0));
         Suggest searchSuggest = actionGet.getSuggest();
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/FunctionScoreTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/FunctionScoreTests.java
index cadd4b8..51fc5a4 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/FunctionScoreTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/FunctionScoreTests.java
@@ -19,26 +19,16 @@
 
 package org.elasticsearch.messy.tests;
 
-import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.action.search.SearchType;
-import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.lucene.search.function.CombineFunction;
-import org.elasticsearch.common.lucene.search.function.FieldValueFactorFunction;
 import org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.query.MatchAllQueryBuilder;
-import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.functionscore.FunctionScoreQueryBuilder;
-import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilder;
-import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders;
-import org.elasticsearch.index.query.functionscore.weight.WeightBuilder;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.groovy.GroovyPlugin;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
@@ -51,366 +41,26 @@ import java.util.concurrent.ExecutionException;
 
 import static org.elasticsearch.client.Requests.searchRequest;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;
 import static org.elasticsearch.index.query.QueryBuilders.functionScoreQuery;
 import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.exponentialDecayFunction;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.fieldValueFactorFunction;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.gaussDecayFunction;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.linearDecayFunction;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.randomFunction;
 import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.scriptFunction;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.weightFactorFunction;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
 import static org.elasticsearch.search.builder.SearchSourceBuilder.searchSource;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.hamcrest.Matchers.closeTo;
 import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
 import static org.hamcrest.Matchers.is;
 
 public class FunctionScoreTests extends ESIntegTestCase {
 
     static final String TYPE = "type";
     static final String INDEX = "index";
-    static final String TEXT_FIELD = "text_field";
-    static final String DOUBLE_FIELD = "double_field";
-    static final String GEO_POINT_FIELD = "geo_point_field";
-    static final XContentBuilder SIMPLE_DOC;
-    static final XContentBuilder MAPPING_WITH_DOUBLE_AND_GEO_POINT_AND_TEXT_FIELD;
 
     @Override
     protected Collection<Class<? extends Plugin>> nodePlugins() {
         return Collections.singleton(GroovyPlugin.class);
     }
-    
-    @Test
-    public void testExplainQueryOnlyOnce() throws IOException, ExecutionException, InterruptedException {
-        assertAcked(prepareCreate("test").addMapping(
-                "type1",
-                jsonBuilder().startObject().startObject("type1").startObject("properties").startObject("test").field("type", "string")
-                        .endObject().startObject("num").field("type", "float").endObject().endObject().endObject().endObject()));
-        ensureYellow();
-
-        client().prepareIndex()
-                .setType("type1")
-                .setId("1")
-                .setIndex("test")
-                .setSource(
-                        jsonBuilder().startObject().field("test", "value").field("num", 10).endObject()).get();
-        refresh();
-
-        SearchResponse response = client().search(
-                searchRequest().searchType(SearchType.QUERY_THEN_FETCH).source(
-                        searchSource().explain(true).query(
-                                functionScoreQuery(termQuery("test", "value"), new FunctionScoreQueryBuilder.FilterFunctionBuilder[]{
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(gaussDecayFunction("num", 5, 5)),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(exponentialDecayFunction("num", 5, 5)),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(linearDecayFunction("num", 5, 5))
-                                })))).get();
-        String explanation = response.getHits().getAt(0).explanation().toString();
-
-        checkQueryExplanationAppearsOnlyOnce(explanation);
-        response = client().search(
-                searchRequest().searchType(SearchType.QUERY_THEN_FETCH).source(
-                        searchSource().explain(true).query(
-                                functionScoreQuery(termQuery("test", "value"), fieldValueFactorFunction("num"))))).get();
-        explanation = response.getHits().getAt(0).explanation().toString();
-        checkQueryExplanationAppearsOnlyOnce(explanation);
-
-        response = client().search(
-                searchRequest().searchType(SearchType.QUERY_THEN_FETCH).source(
-                        searchSource().explain(true).query(
-                                functionScoreQuery(termQuery("test", "value"), randomFunction(10))))).get();
-        explanation = response.getHits().getAt(0).explanation().toString();
-
-        checkQueryExplanationAppearsOnlyOnce(explanation);
-    }
-
-    private void checkQueryExplanationAppearsOnlyOnce(String explanation) {
-        // use some substring of the query explanation and see if it appears twice
-        String queryExplanation = "idf(docFreq=1, maxDocs=1)";
-        int queryExplanationIndex = explanation.indexOf(queryExplanation, 0);
-        assertThat(queryExplanationIndex, greaterThan(-1));
-        queryExplanationIndex = explanation.indexOf(queryExplanation, queryExplanationIndex + 1);
-        assertThat(queryExplanationIndex, equalTo(-1));
-    }
-
-    static {
-        XContentBuilder simpleDoc;
-        XContentBuilder mappingWithDoubleAndGeoPointAndTestField;
-        try {
-            simpleDoc = jsonBuilder().startObject()
-                    .field(TEXT_FIELD, "value")
-                    .startObject(GEO_POINT_FIELD)
-                    .field("lat", 10)
-                    .field("lon", 20)
-                    .endObject()
-                    .field(DOUBLE_FIELD, Math.E)
-                    .endObject();
-        } catch (IOException e) {
-            throw new ElasticsearchException("Exception while initializing FunctionScoreIT", e);
-        }
-        SIMPLE_DOC = simpleDoc;
-        try {
-
-            mappingWithDoubleAndGeoPointAndTestField = jsonBuilder().startObject()
-                    .startObject(TYPE)
-                    .startObject("properties")
-                    .startObject(TEXT_FIELD)
-                    .field("type", "string")
-                    .endObject()
-                    .startObject(GEO_POINT_FIELD)
-                    .field("type", "geo_point")
-                    .endObject()
-                    .startObject(DOUBLE_FIELD)
-                    .field("type", "double")
-                    .endObject()
-                    .endObject()
-                    .endObject()
-                    .endObject();
-        } catch (IOException e) {
-            throw new ElasticsearchException("Exception while initializing FunctionScoreIT", e);
-        }
-        MAPPING_WITH_DOUBLE_AND_GEO_POINT_AND_TEXT_FIELD = mappingWithDoubleAndGeoPointAndTestField;
-    }
-
-    @Test
-    public void testExplain() throws IOException, ExecutionException, InterruptedException {
-        assertAcked(prepareCreate(INDEX).addMapping(
-                TYPE, MAPPING_WITH_DOUBLE_AND_GEO_POINT_AND_TEXT_FIELD
-        ));
-        ensureYellow();
-
-        index(INDEX, TYPE, "1", SIMPLE_DOC);
-        refresh();
-
-        SearchResponse responseWithWeights = client().search(
-                searchRequest().source(
-                        searchSource().query(
-                                functionScoreQuery(constantScoreQuery(termQuery(TEXT_FIELD, "value")), new FunctionScoreQueryBuilder.FilterFunctionBuilder[]{
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(gaussDecayFunction(GEO_POINT_FIELD, new GeoPoint(10, 20), "1000km")),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(fieldValueFactorFunction(DOUBLE_FIELD).modifier(FieldValueFactorFunction.Modifier.LN).setWeight(2)),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(scriptFunction(new Script("_index['" + TEXT_FIELD + "']['value'].tf()")).setWeight(3))
-                                })).explain(true))).actionGet();
-
-        assertThat(
-                responseWithWeights.getHits().getAt(0).getExplanation().toString(),
-                equalTo("6.0 = function score, product of:\n  1.0 = ConstantScore(text_field:value), product of:\n    1.0 = boost\n    1.0 = queryNorm\n  6.0 = min of:\n    6.0 = function score, score mode [multiply]\n      1.0 = function score, product of:\n        1.0 = match filter: *:*\n        1.0 = Function for field geo_point_field:\n          1.0 = exp(-0.5*pow(MIN of: [Math.max(arcDistance([10.0, 20.0](=doc value),[10.0, 20.0](=origin)) - 0.0(=offset), 0)],2.0)/7.213475204444817E11)\n      2.0 = function score, product of:\n        1.0 = match filter: *:*\n        2.0 = product of:\n          1.0 = field value function: ln(doc['double_field'].value * factor=1.0)\n          2.0 = weight\n      3.0 = function score, product of:\n        1.0 = match filter: *:*\n        3.0 = product of:\n          1.0 = script score function, computed with script:\"[script: _index['text_field']['value'].tf(), type: inline, lang: null, params: null]\n            1.0 = _score: \n              1.0 = ConstantScore(text_field:value), product of:\n                1.0 = boost\n                1.0 = queryNorm\n          3.0 = weight\n    3.4028235E38 = maxBoost\n"));
-        responseWithWeights = client().search(
-                searchRequest().source(
-                        searchSource().query(
-                                functionScoreQuery(constantScoreQuery(termQuery(TEXT_FIELD, "value")), weightFactorFunction(4.0f)))
-                                .explain(true))).actionGet();
-        assertThat(
-                responseWithWeights.getHits().getAt(0).getExplanation().toString(),
-                equalTo("4.0 = function score, product of:\n  1.0 = ConstantScore(text_field:value), product of:\n    1.0 = boost\n    1.0 = queryNorm\n  4.0 = min of:\n    4.0 = product of:\n      1.0 = constant score 1.0 - no function provided\n      4.0 = weight\n    3.4028235E38 = maxBoost\n"));
-
-    }
-
-    @Test
-    public void simpleWeightedFunctionsTest() throws IOException, ExecutionException, InterruptedException {
-        assertAcked(prepareCreate(INDEX).addMapping(
-                TYPE, MAPPING_WITH_DOUBLE_AND_GEO_POINT_AND_TEXT_FIELD
-        ));
-        ensureYellow();
-
-        index(INDEX, TYPE, "1", SIMPLE_DOC);
-        refresh();
-        SearchResponse response = client().search(
-                searchRequest().source(
-                        searchSource().query(
-                                functionScoreQuery(constantScoreQuery(termQuery(TEXT_FIELD, "value")), new FunctionScoreQueryBuilder.FilterFunctionBuilder[]{
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(gaussDecayFunction(GEO_POINT_FIELD, new GeoPoint(10, 20), "1000km")),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(fieldValueFactorFunction(DOUBLE_FIELD).modifier(FieldValueFactorFunction.Modifier.LN)),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(scriptFunction(new Script("_index['" + TEXT_FIELD + "']['value'].tf()")))
-                                })))).actionGet();
-        SearchResponse responseWithWeights = client().search(
-                searchRequest().source(
-                        searchSource().query(
-                                functionScoreQuery(constantScoreQuery(termQuery(TEXT_FIELD, "value")), new FunctionScoreQueryBuilder.FilterFunctionBuilder[]{
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(gaussDecayFunction(GEO_POINT_FIELD, new GeoPoint(10, 20), "1000km").setWeight(2)),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(fieldValueFactorFunction(DOUBLE_FIELD).modifier(FieldValueFactorFunction.Modifier.LN).setWeight(2)),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(scriptFunction(new Script("_index['" + TEXT_FIELD + "']['value'].tf()")).setWeight(2))
-                                })))).actionGet();
-
-        assertSearchResponse(response);
-        assertThat(response.getHits().getAt(0).getScore(), is(1.0f));
-        assertThat(responseWithWeights.getHits().getAt(0).getScore(), is(8.0f));
-    }
-
-    @Test
-    public void simpleWeightedFunctionsTestWithRandomWeightsAndRandomCombineMode() throws IOException, ExecutionException, InterruptedException {
-        assertAcked(prepareCreate(INDEX).addMapping(
-                TYPE,
-                MAPPING_WITH_DOUBLE_AND_GEO_POINT_AND_TEXT_FIELD));
-        ensureYellow();
 
-        XContentBuilder doc = SIMPLE_DOC;
-        index(INDEX, TYPE, "1", doc);
-        refresh();
-        ScoreFunctionBuilder[] scoreFunctionBuilders = getScoreFunctionBuilders();
-        float[] weights = createRandomWeights(scoreFunctionBuilders.length);
-        float[] scores = getScores(scoreFunctionBuilders);
-        int weightscounter = 0;
-        FunctionScoreQueryBuilder.FilterFunctionBuilder[] filterFunctionBuilders = new FunctionScoreQueryBuilder.FilterFunctionBuilder[scoreFunctionBuilders.length];
-        for (ScoreFunctionBuilder builder : scoreFunctionBuilders) {
-            filterFunctionBuilders[weightscounter] = new FunctionScoreQueryBuilder.FilterFunctionBuilder(builder.setWeight(weights[weightscounter]));
-            weightscounter++;
-        }
-        FiltersFunctionScoreQuery.ScoreMode scoreMode = randomFrom(FiltersFunctionScoreQuery.ScoreMode.AVG, FiltersFunctionScoreQuery.ScoreMode.SUM,
-                FiltersFunctionScoreQuery.ScoreMode.MIN, FiltersFunctionScoreQuery.ScoreMode.MAX, FiltersFunctionScoreQuery.ScoreMode.MULTIPLY);
-        FunctionScoreQueryBuilder withWeights = functionScoreQuery(constantScoreQuery(termQuery(TEXT_FIELD, "value")), filterFunctionBuilders).scoreMode(scoreMode);
-
-        SearchResponse responseWithWeights = client().search(
-                searchRequest().source(searchSource().query(withWeights))
-        ).actionGet();
-
-        double expectedScore = computeExpectedScore(weights, scores, scoreMode);
-        assertThat((float) expectedScore / responseWithWeights.getHits().getAt(0).getScore(), is(1.0f));
-    }
-
-    protected double computeExpectedScore(float[] weights, float[] scores, FiltersFunctionScoreQuery.ScoreMode scoreMode) {
-        double expectedScore;
-        switch(scoreMode) {
-            case MULTIPLY:
-            expectedScore = 1.0;
-                break;
-            case MAX:
-            expectedScore = Float.MAX_VALUE * -1.0;
-                break;
-            case MIN:
-            expectedScore = Float.MAX_VALUE;
-                break;
-            default:
-                expectedScore = 0.0;
-                break;
-        }
-
-        float weightSum = 0;
-        for (int i = 0; i < weights.length; i++) {
-            double functionScore = (double) weights[i] * scores[i];
-            weightSum += weights[i];
-            switch(scoreMode) {
-                case AVG:
-                expectedScore += functionScore;
-                    break;
-                case MAX:
-                expectedScore = Math.max(functionScore, expectedScore);
-                    break;
-                case MIN:
-                expectedScore = Math.min(functionScore, expectedScore);
-                    break;
-                case SUM:
-                expectedScore += functionScore;
-                    break;
-                case MULTIPLY:
-                expectedScore *= functionScore;
-                    break;
-                default:
-                    throw new UnsupportedOperationException();
-            }
-        }
-        if (scoreMode == FiltersFunctionScoreQuery.ScoreMode.AVG) {
-            expectedScore /= weightSum;
-        }
-        return expectedScore;
-    }
-
-    @Test
-    public void simpleWeightedFunctionsTestSingleFunction() throws IOException, ExecutionException, InterruptedException {
-        assertAcked(prepareCreate(INDEX).addMapping(
-                TYPE,
-                MAPPING_WITH_DOUBLE_AND_GEO_POINT_AND_TEXT_FIELD));
-        ensureYellow();
-
-        XContentBuilder doc = jsonBuilder().startObject()
-                .field(TEXT_FIELD, "value")
-                .startObject(GEO_POINT_FIELD)
-                .field("lat", 12)
-                .field("lon", 21)
-                .endObject()
-                .field(DOUBLE_FIELD, 10)
-                .endObject();
-        index(INDEX, TYPE, "1", doc);
-        refresh();
-        ScoreFunctionBuilder[] scoreFunctionBuilders = getScoreFunctionBuilders();
-        ScoreFunctionBuilder scoreFunctionBuilder = scoreFunctionBuilders[randomInt(3)];
-        float[] weights = createRandomWeights(1);
-        float[] scores = getScores(scoreFunctionBuilder);
-        FunctionScoreQueryBuilder withWeights = functionScoreQuery(constantScoreQuery(termQuery(TEXT_FIELD, "value")), scoreFunctionBuilder.setWeight(weights[0]));
-
-        SearchResponse responseWithWeights = client().search(
-                searchRequest().source(searchSource().query(withWeights))
-        ).actionGet();
-
-        assertThat( (double) scores[0] * weights[0]/ responseWithWeights.getHits().getAt(0).getScore(), closeTo(1.0, 1.e-6));
-
-    }
-
-    private float[] getScores(ScoreFunctionBuilder... scoreFunctionBuilders) {
-        float[] scores = new float[scoreFunctionBuilders.length];
-        int scorecounter = 0;
-        for (ScoreFunctionBuilder builder : scoreFunctionBuilders) {
-            SearchResponse response = client().search(
-                    searchRequest().source(
-                            searchSource().query(
-                                    functionScoreQuery(constantScoreQuery(termQuery(TEXT_FIELD, "value")), builder)
-                            ))).actionGet();
-            scores[scorecounter] = response.getHits().getAt(0).getScore();
-            scorecounter++;
-        }
-        return scores;
-    }
-
-    private float[] createRandomWeights(int size) {
-        float[] weights = new float[size];
-        for (int i = 0; i < weights.length; i++) {
-            weights[i] = randomFloat() * (randomBoolean() ? 1.0f : -1.0f) * randomInt(100) + 1.e-6f;
-        }
-        return weights;
-    }
-
-    public ScoreFunctionBuilder[] getScoreFunctionBuilders() {
-        ScoreFunctionBuilder[] builders = new ScoreFunctionBuilder[4];
-        builders[0] = gaussDecayFunction(GEO_POINT_FIELD, new GeoPoint(10, 20), "1000km");
-        builders[1] = randomFunction(10);
-        builders[2] = fieldValueFactorFunction(DOUBLE_FIELD).modifier(FieldValueFactorFunction.Modifier.LN);
-        builders[3] = scriptFunction(new Script("_index['" + TEXT_FIELD + "']['value'].tf()"));
-        return builders;
-    }
-
-    @Test
-    public void checkWeightOnlyCreatesBoostFunction() throws IOException {
-        assertAcked(prepareCreate(INDEX).addMapping(
-                TYPE,
-                MAPPING_WITH_DOUBLE_AND_GEO_POINT_AND_TEXT_FIELD));
-        ensureYellow();
-
-        index(INDEX, TYPE, "1", SIMPLE_DOC);
-        refresh();
-        SearchResponse response = client().search(
-                searchRequest().source(new SearchSourceBuilder().query(QueryBuilders.functionScoreQuery(ScoreFunctionBuilders.weightFactorFunction(2.0f))))
-                ).actionGet();
-        assertSearchResponse(response);
-        assertThat(response.getHits().getAt(0).score(), equalTo(2.0f));
-        response = client().search(
-                searchRequest().source(new SearchSourceBuilder().query(QueryBuilders.functionScoreQuery(ScoreFunctionBuilders.weightFactorFunction(2.0f))))
-        ).actionGet();
-        assertSearchResponse(response);
-        assertThat(response.getHits().getAt(0).score(), equalTo(2.0f));
-        response = client().search(
-                searchRequest().source(searchSource().query(functionScoreQuery(new WeightBuilder().setWeight(2.0f))))
-        ).actionGet();
-        assertSearchResponse(response);
-        assertThat(response.getHits().getAt(0).score(), equalTo(2.0f));
-        response = client().search(
-                searchRequest().source(searchSource().query(functionScoreQuery(weightFactorFunction(2.0f))))
-        ).actionGet();
-        assertSearchResponse(response);
-        assertThat(response.getHits().getAt(0).score(), equalTo(2.0f));
-    } 
 
     @Test
     public void testScriptScoresNested() throws IOException {
@@ -426,7 +76,7 @@ public class FunctionScoreTests extends ESIntegTestCase {
                                                 functionScoreQuery(scriptFunction(new Script("1"))),
                                                 scriptFunction(new Script("_score.doubleValue()"))),
                                         scriptFunction(new Script("_score.doubleValue()"))
-                                        )
+                                )
                         )
                 )
         ).actionGet();
@@ -474,7 +124,7 @@ public class FunctionScoreTests extends ESIntegTestCase {
                                 new FunctionScoreQueryBuilder.FilterFunctionBuilder(scriptFunction(new Script(Float.toString(score)))),
                                 new FunctionScoreQueryBuilder.FilterFunctionBuilder(scriptFunction(new Script(Float.toString(score))))
                         }).scoreMode(FiltersFunctionScoreQuery.ScoreMode.AVG).setMinScore(minScore)))
-        ).actionGet();
+                ).actionGet();
         if (score < minScore) {
             assertThat(searchResponse.getHits().getTotalHits(), is(0l));
         } else {
@@ -536,7 +186,7 @@ public class FunctionScoreTests extends ESIntegTestCase {
         float termQueryScore = 0.19178301f;
         for (CombineFunction combineFunction : CombineFunction.values()) {
             testMinScoreApplied(combineFunction, termQueryScore);
-    }
+        }
     }
 
     protected void testMinScoreApplied(CombineFunction boostMode, float expectedScore) throws InterruptedException, ExecutionException {
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndexedScriptTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndexedScriptTests.java
index d5c2f55..1bba7bf 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndexedScriptTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndexedScriptTests.java
@@ -24,27 +24,23 @@ import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptResponse;
 import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.script.groovy.GroovyPlugin;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.groovy.GroovyScriptEngineService;
 import org.elasticsearch.search.SearchHit;
-import org.elasticsearch.search.aggregations.AggregationBuilders;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
+import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Collections;
-import java.util.HashMap;
 import java.util.List;
-import java.util.Map;
 import java.util.concurrent.ExecutionException;
 
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
@@ -58,7 +54,7 @@ public class IndexedScriptTests extends ESIntegTestCase {
     protected Collection<Class<? extends Plugin>> nodePlugins() {
         return Collections.singleton(GroovyPlugin.class);
     }
-
+    
     @Override
     protected Settings nodeSettings(int nodeOrdinal) {
         Settings.Builder builder = Settings.builder().put(super.nodeSettings(nodeOrdinal));
@@ -95,20 +91,13 @@ public class IndexedScriptTests extends ESIntegTestCase {
         builders.add(client().prepareIndex("test", "scriptTest", "5").setSource("{\"theField\":\"bar\"}"));
 
         indexRandom(true, builders);
-        Map<String, Object> script2Params = new HashMap<>();
-        script2Params.put("factor", 3);
-        SearchResponse searchResponse = client()
-                .prepareSearch()
-                .setSource(
-                        new SearchSourceBuilder().query(QueryBuilders.matchAllQuery()).size(1)
-                                .scriptField("test1", new Script("script1", ScriptType.INDEXED, "groovy", null))
-                                .scriptField("test2", new Script("script2", ScriptType.INDEXED, "groovy", script2Params)))
-                .setIndices("test").setTypes("scriptTest").get();
+        String query = "{ \"query\" : { \"match_all\": {}} , \"script_fields\" : { \"test1\" : { \"script_id\" : \"script1\", \"lang\":\"groovy\" }, \"test2\" : { \"script_id\" : \"script2\", \"lang\":\"groovy\", \"params\":{\"factor\":3}  }}, size:1}";
+        SearchResponse searchResponse = client().prepareSearch().setSource(new BytesArray(query)).setIndices("test").setTypes("scriptTest").get();
         assertHitCount(searchResponse, 5);
         assertTrue(searchResponse.getHits().hits().length == 1);
         SearchHit sh = searchResponse.getHits().getAt(0);
-        assertThat((Integer) sh.field("test1").getValue(), equalTo(2));
-        assertThat((Integer) sh.field("test2").getValue(), equalTo(6));
+        assertThat((Integer)sh.field("test1").getValue(), equalTo(2));
+        assertThat((Integer)sh.field("test2").getValue(), equalTo(6));
     }
 
     // Relates to #10397
@@ -124,12 +113,11 @@ public class IndexedScriptTests extends ESIntegTestCase {
             PutIndexedScriptResponse response = 
                     client().preparePutIndexedScript(GroovyScriptEngineService.NAME, "script1", "{\"script\":\"" + i + "\"}").get();
             assertEquals(i, response.getVersion());
-            SearchResponse searchResponse = client()
-                    .prepareSearch()
-                    .setSource(
-                            new SearchSourceBuilder().query(QueryBuilders.matchAllQuery()).scriptField("test_field",
-                                    new Script("script1", ScriptType.INDEXED, "groovy", null))).setIndices("test_index")
-                    .setTypes("test_type").get();
+            
+            String query = "{"
+                    + " \"query\" : { \"match_all\": {}}, "
+                    + " \"script_fields\" : { \"test_field\" : { \"script_id\" : \"script1\", \"lang\":\"groovy\" } } }";    
+            SearchResponse searchResponse = client().prepareSearch().setSource(new BytesArray(query)).setIndices("test_index").setTypes("test_type").get();
             assertHitCount(searchResponse, 1);
             SearchHit sh = searchResponse.getHits().getAt(0);
             assertThat((Integer)sh.field("test_field").getValue(), equalTo(i));
@@ -165,11 +153,8 @@ public class IndexedScriptTests extends ESIntegTestCase {
         }
         client().prepareIndex("test", "scriptTest", "1").setSource("{\"theField\":\"foo\"}").get();
         refresh();
-        SearchResponse searchResponse = client()
-                .prepareSearch("test")
-                .setSource(
-                        new SearchSourceBuilder().aggregation(AggregationBuilders.terms("test").script(
-                                new Script("script1", ScriptType.INDEXED, null, null)))).get();
+        String source = "{\"aggs\": {\"test\": { \"terms\" : { \"script_id\":\"script1\" } } } }";
+        SearchResponse searchResponse = client().prepareSearch("test").setSource(new BytesArray(source)).get();
         assertHitCount(searchResponse, 1);
         assertThat(searchResponse.getAggregations().get("test"), notNullValue());
     }
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchFieldsTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchFieldsTests.java
index a7618f0..7d9a080 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchFieldsTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchFieldsTests.java
@@ -19,7 +19,9 @@
 
 package org.elasticsearch.messy.tests;
 
+import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.action.index.IndexRequestBuilder;
+import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.Base64;
@@ -31,7 +33,6 @@ import org.elasticsearch.common.joda.Joda;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.mapper.internal.TimestampFieldMapper;
-import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.rest.RestStatus;
 import org.elasticsearch.script.Script;
@@ -39,7 +40,6 @@ import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.groovy.GroovyPlugin;
 import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.search.SearchHitField;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.sort.SortOrder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.joda.time.DateTime;
@@ -80,7 +80,7 @@ public class SearchFieldsTests extends ESIntegTestCase {
     protected Collection<Class<? extends Plugin>> nodePlugins() {
         return Collections.singleton(GroovyPlugin.class);
     }
-
+    
     @Test
     public void testStoredFields() throws Exception {
         createIndex("test");
@@ -529,13 +529,22 @@ public class SearchFieldsTests extends ESIntegTestCase {
         createIndex("test");
         indexRandom(true, client().prepareIndex("test", "type", "1").setSource("test_field", "foobar"));
         refresh();
-        SearchResponse searchResponse = client().prepareSearch("test").setTypes("type").setSource(
-                new SearchSourceBuilder().query(QueryBuilders.matchAllQuery()).fieldDataField("test_field")).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setTypes("type").setSource(new BytesArray(new BytesRef("{\"query\":{\"match_all\":{}},\"fielddata_fields\": \"test_field\"}"))).get();
         assertHitCount(searchResponse, 1);
         Map<String,SearchHitField> fields = searchResponse.getHits().getHits()[0].getFields();
         assertThat((String)fields.get("test_field").value(), equalTo("foobar"));
     }
 
+    @Test(expected = SearchPhaseExecutionException.class)
+    public void testInvalidFieldDataField() throws ExecutionException, InterruptedException {
+        createIndex("test");
+        if (randomBoolean()) {
+            client().prepareSearch("test").setTypes("type").setSource(new BytesArray(new BytesRef("{\"query\":{\"match_all\":{}},\"fielddata_fields\": {}}"))).get();
+        } else {
+            client().prepareSearch("test").setTypes("type").setSource(new BytesArray(new BytesRef("{\"query\":{\"match_all\":{}},\"fielddata_fields\": 1.0}"))).get();
+        }
+    }
+
     @Test
     public void testFieldsPulledFromFieldData() throws Exception {
         createIndex("test");
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchStatsTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchStatsTests.java
index 89d1670..d9cfb7d 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchStatsTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchStatsTests.java
@@ -34,7 +34,6 @@ import org.elasticsearch.index.search.stats.SearchStats.Stats;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.groovy.GroovyPlugin;
-import org.elasticsearch.search.highlight.HighlightBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
@@ -66,7 +65,7 @@ public class SearchStatsTests extends ESIntegTestCase {
     protected Collection<Class<? extends Plugin>> nodePlugins() {
         return Collections.singleton(GroovyPlugin.class);
     }
-
+    
     @Override
     protected int numberOfReplicas() {
         return 0;
@@ -110,7 +109,7 @@ public class SearchStatsTests extends ESIntegTestCase {
         for (int i = 0; i < iters; i++) {
             SearchResponse searchResponse = internalCluster().clientNodeClient().prepareSearch()
                     .setQuery(QueryBuilders.termQuery("field", "value")).setStats("group1", "group2")
-                    .highlighter(new HighlightBuilder().field("field"))
+                    .addHighlightedField("field")
                     .addScriptField("scrip1", new Script("_source.field"))
                     .setSize(100)
                     .execute().actionGet();
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchTimeoutTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchTimeoutTests.java
index 1b53550..2a982df 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchTimeoutTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchTimeoutTests.java
@@ -22,7 +22,6 @@ package org.elasticsearch.messy.tests;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.groovy.GroovyPlugin;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -30,8 +29,8 @@ import org.junit.Test;
 
 import java.util.Collection;
 import java.util.Collections;
-import java.util.concurrent.TimeUnit;
 
+import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
 import static org.elasticsearch.index.query.QueryBuilders.scriptQuery;
 import static org.hamcrest.Matchers.equalTo;
 
@@ -55,7 +54,7 @@ public class SearchTimeoutTests extends ESIntegTestCase {
         client().prepareIndex("test", "type", "1").setSource("field", "value").setRefresh(true).execute().actionGet();
 
         SearchResponse searchResponse = client().prepareSearch("test")
-                .setTimeout(new TimeValue(10, TimeUnit.MILLISECONDS))
+                .setTimeout("10ms")
                 .setQuery(scriptQuery(new Script("Thread.sleep(500); return true;")))
                 .execute().actionGet();
         assertThat(searchResponse.isTimedOut(), equalTo(true));
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SimpleSortTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SimpleSortTests.java
index 5f51b65..47bfb49 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SimpleSortTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SimpleSortTests.java
@@ -30,6 +30,7 @@ import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.search.ShardSearchFailure;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.geo.GeoDistance;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.text.StringAndBytesText;
@@ -45,12 +46,7 @@ import org.elasticsearch.script.Script;
 import org.elasticsearch.script.groovy.GroovyPlugin;
 import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.search.SearchHitField;
-import org.elasticsearch.search.sort.FieldSortBuilder;
-import org.elasticsearch.search.sort.GeoDistanceSortBuilder;
-import org.elasticsearch.search.sort.ScriptSortBuilder;
-import org.elasticsearch.search.sort.SortBuilders;
-import org.elasticsearch.search.sort.SortOrder;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
+import org.elasticsearch.search.sort.*;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.junit.annotations.TestLogging;
 import org.hamcrest.Matchers;
@@ -1852,6 +1848,50 @@ public class SimpleSortTests extends ESIntegTestCase {
         assertThat((Double) searchResponse.getHits().getAt(0).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(3.25, 4, 2, 1, DistanceUnit.KILOMETERS), 1.e-4));
         assertThat((Double) searchResponse.getHits().getAt(1).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(5.25, 4, 2, 1, DistanceUnit.KILOMETERS), 1.e-4));
 
+        //test all the different formats in one
+        createQPoints(qHashes, qPoints);
+        XContentBuilder searchSourceBuilder = jsonBuilder();
+        searchSourceBuilder.startObject().startArray("sort").startObject().startObject("_geo_distance").startArray("location");
+
+        for (int i = 0; i < 4; i++) {
+            int at = randomInt(qPoints.size() - 1);
+            int format = randomInt(3);
+            switch (format) {
+                case 0: {
+                    searchSourceBuilder.value(qHashes.get(at));
+                    break;
+                }
+                case 1: {
+                    searchSourceBuilder.value(qPoints.get(at).lat() + "," + qPoints.get(at).lon());
+                    break;
+                }
+                case 2: {
+                    searchSourceBuilder.value(qPoints.get(at));
+                    break;
+                }
+                case 3: {
+                    searchSourceBuilder.startArray().value(qPoints.get(at).lon()).value(qPoints.get(at).lat()).endArray();
+                    break;
+                }
+            }
+            qHashes.remove(at);
+            qPoints.remove(at);
+        }
+
+        searchSourceBuilder.endArray();
+        searchSourceBuilder.field("order", "asc");
+        searchSourceBuilder.field("unit", "km");
+        searchSourceBuilder.field("sort_mode", "min");
+        searchSourceBuilder.field("distance_type", "plane");
+        searchSourceBuilder.endObject();
+        searchSourceBuilder.endObject();
+        searchSourceBuilder.endArray();
+        searchSourceBuilder.endObject();
+
+        searchResponse = client().prepareSearch().setSource(searchSourceBuilder.bytes()).execute().actionGet();
+        assertOrderedSearchHits(searchResponse, "d1", "d2");
+        assertThat((Double) searchResponse.getHits().getAt(0).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(2.5, 1, 2, 1, DistanceUnit.KILOMETERS), 1.e-4));
+        assertThat((Double) searchResponse.getHits().getAt(1).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(4.5, 1, 2, 1, DistanceUnit.KILOMETERS), 1.e-4));
     }
 
     public void testSinglePointGeoDistanceSort() throws ExecutionException, InterruptedException, IOException {
@@ -1890,25 +1930,40 @@ public class SimpleSortTests extends ESIntegTestCase {
                 .execute().actionGet();
         checkCorrectSortOrderForGeoSort(searchResponse);
 
-        searchResponse = client()
-                .prepareSearch()
-                .setSource(
-                        new SearchSourceBuilder().sort(SortBuilders.geoDistanceSort("location").point(2.0, 2.0)
-                                .unit(DistanceUnit.KILOMETERS).geoDistance(GeoDistance.PLANE))).execute().actionGet();
+        String geoSortRequest = jsonBuilder().startObject().startArray("sort").startObject()
+                .startObject("_geo_distance")
+                .startArray("location").value(2f).value(2f).endArray()
+                .field("unit", "km")
+                .field("distance_type", "plane")
+                .endObject()
+                .endObject().endArray().string();
+        searchResponse = client().prepareSearch().setSource(new BytesArray(geoSortRequest))
+                .execute().actionGet();
         checkCorrectSortOrderForGeoSort(searchResponse);
 
-        searchResponse = client()
-                .prepareSearch()
-                .setSource(
-                        new SearchSourceBuilder().sort(SortBuilders.geoDistanceSort("location").geohashes("s037ms06g7h0")
-                                .unit(DistanceUnit.KILOMETERS).geoDistance(GeoDistance.PLANE))).execute().actionGet();
+        geoSortRequest = jsonBuilder().startObject().startArray("sort").startObject()
+                .startObject("_geo_distance")
+                .field("location", "s037ms06g7h0")
+                .field("unit", "km")
+                .field("distance_type", "plane")
+                .endObject()
+                .endObject().endArray().string();
+        searchResponse = client().prepareSearch().setSource(new BytesArray(geoSortRequest))
+                .execute().actionGet();
         checkCorrectSortOrderForGeoSort(searchResponse);
 
-        searchResponse = client()
-                .prepareSearch()
-                .setSource(
-                        new SearchSourceBuilder().sort(SortBuilders.geoDistanceSort("location").point(2.0, 2.0)
-                                .unit(DistanceUnit.KILOMETERS).geoDistance(GeoDistance.PLANE))).execute().actionGet();
+        geoSortRequest = jsonBuilder().startObject().startArray("sort").startObject()
+                .startObject("_geo_distance")
+                .startObject("location")
+                .field("lat", 2)
+                .field("lon", 2)
+                .endObject()
+                .field("unit", "km")
+                .field("distance_type", "plane")
+                .endObject()
+                .endObject().endArray().string();
+        searchResponse = client().prepareSearch().setSource(new BytesArray(geoSortRequest))
+                .execute().actionGet();
         checkCorrectSortOrderForGeoSort(searchResponse);
     }
 
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/UpdateTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/UpdateTests.java
index 3c2b230..dc76eda 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/UpdateTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/UpdateTests.java
@@ -44,12 +44,7 @@ import org.elasticsearch.script.groovy.GroovyPlugin;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
+import java.util.*;
 import java.util.concurrent.CopyOnWriteArrayList;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.Semaphore;
@@ -58,12 +53,7 @@ import java.util.concurrent.TimeUnit;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThrows;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.lessThanOrEqualTo;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
+import static org.hamcrest.Matchers.*;
 
 public class UpdateTests extends ESIntegTestCase {
 
@@ -258,25 +248,26 @@ public class UpdateTests extends ESIntegTestCase {
                         .setVersionType(VersionType.EXTERNAL).execute(),
                 ActionRequestValidationException.class);
 
+
+        // With force version
+        client().prepareUpdate(indexOrAlias(), "type", "2")
+                .setScript(new Script("ctx._source.text = 'v10'", ScriptService.ScriptType.INLINE, null, null))
+                .setVersion(10).setVersionType(VersionType.FORCE).get();
+
+        GetResponse get = get("test", "type", "2");
+        assertThat(get.getVersion(), equalTo(10l));
+        assertThat((String) get.getSource().get("text"), equalTo("v10"));
+
         // upserts - the combination with versions is a bit weird. Test are here to ensure we do not change our behavior unintentionally
 
         // With internal versions, tt means "if object is there with version X, update it or explode. If it is not there, index.
         client().prepareUpdate(indexOrAlias(), "type", "3")
                 .setScript(new Script("ctx._source.text = 'v2'", ScriptService.ScriptType.INLINE, null, null))
                 .setVersion(10).setUpsert("{ \"text\": \"v0\" }").get();
-        GetResponse get = get("test", "type", "3");
+        get = get("test", "type", "3");
         assertThat(get.getVersion(), equalTo(1l));
         assertThat((String) get.getSource().get("text"), equalTo("v0"));
 
-        // With force version
-        client().prepareUpdate(indexOrAlias(), "type", "4")
-                .setScript(new Script("ctx._source.text = 'v2'", ScriptService.ScriptType.INLINE, null, null))
-                .setVersion(10).setVersionType(VersionType.FORCE).setUpsert("{ \"text\": \"v0\" }").get();
-
-        get = get("test", "type", "4");
-        assertThat(get.getVersion(), equalTo(10l));
-        assertThat((String) get.getSource().get("text"), equalTo("v0"));
-
 
         // retry on conflict is rejected:
         assertThrows(client().prepareUpdate(indexOrAlias(), "type", "1").setVersion(10).setRetryOnConflict(5), ActionRequestValidationException.class);
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovyScriptTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovyScriptTests.java
index 337f4eb..c9fabd7 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovyScriptTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovyScriptTests.java
@@ -22,14 +22,12 @@ package org.elasticsearch.script.groovy;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.lucene.search.function.CombineFunction;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.script.Script;
-import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.groovy.GroovyScriptEngineService;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
-import org.elasticsearch.search.sort.SortBuilders;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
@@ -38,14 +36,9 @@ import java.util.Collection;
 import java.util.Collections;
 import java.util.List;
 
-import static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.functionScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.scriptQuery;
+import static org.elasticsearch.index.query.QueryBuilders.*;
 import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.scriptFunction;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertOrderedSearchHits;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHits;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
 import static org.hamcrest.Matchers.equalTo;
 
 /**
@@ -64,16 +57,16 @@ public class GroovyScriptTests extends ESIntegTestCase {
         client().prepareIndex("test", "doc", "1").setSource("foo", 5).setRefresh(true).get();
 
         // Test that something that would usually be a BigDecimal is transformed into a Double
-        assertScript("def n = 1.23; assert n instanceof Double; return n;");
-        assertScript("def n = 1.23G; assert n instanceof Double; return n;");
-        assertScript("def n = BigDecimal.ONE; assert n instanceof BigDecimal; return n;");
+        assertScript("def n = 1.23; assert n instanceof Double;");
+        assertScript("def n = 1.23G; assert n instanceof Double;");
+        assertScript("def n = BigDecimal.ONE; assert n instanceof BigDecimal;");
     }
 
-    public void assertScript(String scriptString) {
-        Script script = new Script(scriptString, ScriptType.INLINE, "groovy", null);
+    public void assertScript(String script) {
         SearchResponse resp = client().prepareSearch("test")
-                .setSource(new SearchSourceBuilder().query(QueryBuilders.matchAllQuery()).sort(SortBuilders.scriptSort(script, "number")))
-                .get();
+                .setSource(new BytesArray("{\"query\": {\"match_all\": {}}," +
+                        "\"sort\":{\"_script\": {\"script\": \""+ script +
+                        "; 1\", \"type\": \"number\", \"lang\": \"groovy\"}}}")).get();
         assertNoFailures(resp);
     }
 
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovySecurityTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovySecurityTests.java
index f002bd1..043a5d1 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovySecurityTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovySecurityTests.java
@@ -22,14 +22,10 @@ package org.elasticsearch.script.groovy;
 import org.apache.lucene.util.Constants;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.search.ShardSearchFailure;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptException;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
-import org.elasticsearch.search.sort.SortBuilders;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
@@ -54,7 +50,7 @@ public class GroovySecurityTests extends ESIntegTestCase {
         super.setUp();
         assumeTrue("test requires security manager to be enabled", System.getSecurityManager() != null);
     }
-
+    
     @Override
     protected Collection<Class<? extends Plugin>> nodePlugins() {
         return Collections.singleton(GroovyPlugin.class);
@@ -83,7 +79,7 @@ public class GroovySecurityTests extends ESIntegTestCase {
         // Ranges
         assertSuccess("def range = 1..doc['foo'].value; def v = range.get(0)");
         // Maps
-        assertSuccess("def v = doc['foo'].value; def m = [:]; m.put(\"value\", v)");
+        assertSuccess("def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)");
         // Times
         assertSuccess("def t = Instant.now().getMillis()");
         // GroovyCollections
@@ -91,42 +87,40 @@ public class GroovySecurityTests extends ESIntegTestCase {
 
         // Fail cases:
         // AccessControlException[access denied ("java.io.FilePermission" "<<ALL FILES>>" "execute")]
-        assertFailure("pr = Runtime.getRuntime().exec(\"touch /tmp/gotcha\"); pr.waitFor()");
+        assertFailure("pr = Runtime.getRuntime().exec(\\\"touch /tmp/gotcha\\\"); pr.waitFor()");
 
         // AccessControlException[access denied ("java.lang.RuntimePermission" "accessClassInPackage.sun.reflect")]
-        assertFailure("d = new DateTime(); d.getClass().getDeclaredMethod(\"year\").setAccessible(true)");
-        assertFailure("d = new DateTime(); d.\"${'get' + 'Class'}\"()." +
-                        "\"${'getDeclared' + 'Method'}\"(\"year\").\"${'set' + 'Accessible'}\"(false)");
-        assertFailure("Class.forName(\"org.joda.time.DateTime\").getDeclaredMethod(\"year\").setAccessible(true)");
+        assertFailure("d = new DateTime(); d.getClass().getDeclaredMethod(\\\"year\\\").setAccessible(true)");
+        assertFailure("d = new DateTime(); d.\\\"${'get' + 'Class'}\\\"()." +
+                        "\\\"${'getDeclared' + 'Method'}\\\"(\\\"year\\\").\\\"${'set' + 'Accessible'}\\\"(false)");
+        assertFailure("Class.forName(\\\"org.joda.time.DateTime\\\").getDeclaredMethod(\\\"year\\\").setAccessible(true)");
 
         // AccessControlException[access denied ("groovy.security.GroovyCodeSourcePermission" "/groovy/shell")]
         assertFailure("Eval.me('2 + 2')");
         assertFailure("Eval.x(5, 'x + 2')");
 
         // AccessControlException[access denied ("java.lang.RuntimePermission" "accessDeclaredMembers")]
-        assertFailure("d = new Date(); java.lang.reflect.Field f = Date.class.getDeclaredField(\"fastTime\");" +
-                " f.setAccessible(true); f.get(\"fastTime\")");
+        assertFailure("d = new Date(); java.lang.reflect.Field f = Date.class.getDeclaredField(\\\"fastTime\\\");" +
+                " f.setAccessible(true); f.get(\\\"fastTime\\\")");
 
         // AccessControlException[access denied ("java.io.FilePermission" "<<ALL FILES>>" "execute")]
-        assertFailure("def methodName = 'ex'; Runtime.\"${'get' + 'Runtime'}\"().\"${methodName}ec\"(\"touch /tmp/gotcha2\")");
+        assertFailure("def methodName = 'ex'; Runtime.\\\"${'get' + 'Runtime'}\\\"().\\\"${methodName}ec\\\"(\\\"touch /tmp/gotcha2\\\")");
         
         // test a directory we normally have access to, but the groovy script does not.
         Path dir = createTempDir();
         // TODO: figure out the necessary escaping for windows paths here :)
         if (!Constants.WINDOWS) {
             // access denied ("java.io.FilePermission" ".../tempDir-00N" "read")
-            assertFailure("new File(\"" + dir + "\").exists()");
+            assertFailure("new File(\\\"" + dir + "\\\").exists()");
         }
     }
 
     private void assertSuccess(String script) {
         logger.info("--> script: " + script);
-        SearchResponse resp = client()
-                .prepareSearch("test")
-                .setSource(
-                        new SearchSourceBuilder().query(QueryBuilders.matchAllQuery()).sort(
-                                SortBuilders.scriptSort(new Script(script + "; doc['foo'].value + 2", ScriptType.INLINE, "groovy", null),
-                                        "number"))).get();
+        SearchResponse resp = client().prepareSearch("test")
+                .setSource(new BytesArray("{\"query\": {\"match_all\": {}}," +
+                        "\"sort\":{\"_script\": {\"script\": \"" + script +
+                        "; doc['foo'].value + 2\", \"type\": \"number\", \"lang\": \"groovy\"}}}")).get();
         assertNoFailures(resp);
         assertEquals(1, resp.getHits().getTotalHits());
         assertThat(resp.getHits().getAt(0).getSortValues(), equalTo(new Object[]{7.0}));
@@ -134,12 +128,10 @@ public class GroovySecurityTests extends ESIntegTestCase {
 
     private void assertFailure(String script) {
         logger.info("--> script: " + script);
-        SearchResponse resp = client()
-                .prepareSearch("test")
-                .setSource(
-                        new SearchSourceBuilder().query(QueryBuilders.matchAllQuery()).sort(
-                                SortBuilders.scriptSort(new Script(script + "; doc['foo'].value + 2", ScriptType.INLINE, "groovy", null),
-                                        "number"))).get();
+        SearchResponse resp = client().prepareSearch("test")
+                 .setSource(new BytesArray("{\"query\": {\"match_all\": {}}," +
+                         "\"sort\":{\"_script\": {\"script\": \"" + script +
+                         "; doc['foo'].value + 2\", \"type\": \"number\", \"lang\": \"groovy\"}}}")).get();
         assertEquals(0, resp.getHits().getTotalHits());
         ShardSearchFailure fails[] = resp.getShardFailures();
         // TODO: GroovyScriptExecutionException needs work:
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
index 8f0dedc..008a83e 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
@@ -32,13 +32,7 @@ import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.mapper.MapperParsingException;
-import org.elasticsearch.index.query.BoolQueryBuilder;
-import org.elasticsearch.index.query.MatchQueryBuilder;
-import org.elasticsearch.index.query.MultiMatchQueryBuilder;
-import org.elasticsearch.index.query.Operator;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.query.TermQueryBuilder;
-import org.elasticsearch.index.query.WrapperQueryBuilder;
+import org.elasticsearch.index.query.*;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders;
 import org.elasticsearch.index.search.MatchQuery;
 import org.elasticsearch.index.search.MatchQuery.Type;
@@ -60,54 +54,10 @@ import java.util.concurrent.ExecutionException;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
-import static org.elasticsearch.index.query.QueryBuilders.commonTermsQuery;
-import static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.existsQuery;
-import static org.elasticsearch.index.query.QueryBuilders.functionScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.fuzzyQuery;
-import static org.elasticsearch.index.query.QueryBuilders.hasChildQuery;
-import static org.elasticsearch.index.query.QueryBuilders.idsQuery;
-import static org.elasticsearch.index.query.QueryBuilders.indicesQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.missingQuery;
-import static org.elasticsearch.index.query.QueryBuilders.multiMatchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.notQuery;
-import static org.elasticsearch.index.query.QueryBuilders.prefixQuery;
-import static org.elasticsearch.index.query.QueryBuilders.queryStringQuery;
-import static org.elasticsearch.index.query.QueryBuilders.rangeQuery;
-import static org.elasticsearch.index.query.QueryBuilders.regexpQuery;
-import static org.elasticsearch.index.query.QueryBuilders.spanMultiTermQueryBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.spanNearQuery;
-import static org.elasticsearch.index.query.QueryBuilders.spanNotQuery;
-import static org.elasticsearch.index.query.QueryBuilders.spanOrQuery;
-import static org.elasticsearch.index.query.QueryBuilders.spanTermQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termsLookupQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termsQuery;
-import static org.elasticsearch.index.query.QueryBuilders.typeQuery;
-import static org.elasticsearch.index.query.QueryBuilders.wildcardQuery;
-import static org.elasticsearch.index.query.QueryBuilders.wrapperQuery;
+import static org.elasticsearch.index.query.QueryBuilders.*;
 import static org.elasticsearch.test.VersionUtils.randomVersion;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFirstHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHits;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSecondHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThirdHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasId;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasScore;
-import static org.hamcrest.Matchers.allOf;
-import static org.hamcrest.Matchers.closeTo;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.is;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
 
 public class SearchQueryIT extends ESIntegTestCase {
 
@@ -198,6 +148,15 @@ public class SearchQueryIT extends ESIntegTestCase {
     }
 
     @Test
+    public void passQueryAsStringTest() throws Exception {
+        createIndex("test");
+        client().prepareIndex("test", "type1", "1").setSource("field1", "value1_1", "field2", "value2_1").setRefresh(true).get();
+
+        SearchResponse searchResponse = client().prepareSearch().setQuery("{ \"term\" : { \"field1\" : \"value1_1\" }}").get();
+        assertHitCount(searchResponse, 1l);
+    }
+
+    @Test
     public void testIndexOptions() throws Exception {
         assertAcked(prepareCreate("test")
                 .addMapping("type1", "field1", "type=string,index_options=docs"));
@@ -352,6 +311,10 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("2"));
 
+        searchResponse = client().prepareSearch().setQuery("{ \"common\" : { \"field1\" : { \"query\" : \"the lazy fox brown\", \"cutoff_frequency\" : 1, \"minimum_should_match\" : { \"high_freq\" : 4 } } } }").get();
+        assertHitCount(searchResponse, 1l);
+        assertFirstHit(searchResponse, hasId("2"));
+
         // Default
         searchResponse = client().prepareSearch().setQuery(commonTermsQuery("field1", "the lazy fox brown").cutoffFrequency(1)).get();
         assertHitCount(searchResponse, 1l);
@@ -440,6 +403,10 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("2"));
 
+        searchResponse = client().prepareSearch().setQuery("{ \"common\" : { \"field1\" : { \"query\" : \"the fast lazy fox brown\", \"cutoff_frequency\" : 1, \"minimum_should_match\" : { \"high_freq\" : 6 } } } }").get();
+        assertHitCount(searchResponse, 1l);
+        assertFirstHit(searchResponse, hasId("2"));
+
         // Default
         searchResponse = client().prepareSearch().setQuery(commonTermsQuery("field1", "the fast lazy fox brown").cutoffFrequency(1)).get();
         assertHitCount(searchResponse, 1l);
@@ -1489,6 +1456,14 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertHitCount(searchResponse, 2l);
     }
 
+    @Test
+    public void testEmptyTopLevelFilter() {
+        client().prepareIndex("test", "type", "1").setSource("field", "value").setRefresh(true).get();
+
+        SearchResponse searchResponse = client().prepareSearch().setPostFilter("{}").get();
+        assertHitCount(searchResponse, 1l);
+    }
+
     @Test // see #2926
     public void testMustNot() throws IOException, ExecutionException, InterruptedException {
         assertAcked(prepareCreate("test")
@@ -2209,4 +2184,26 @@ public class SearchQueryIT extends ESIntegTestCase {
             assertThat(i + " expected: " + first + " actual: " + actual, Float.compare(first, actual), equalTo(0));
         }
     }
+
+    @Test // see #7686.
+    public void testIdsQueryWithInvalidValues() throws Exception {
+        createIndex("test");
+        indexRandom(true, false, client().prepareIndex("test", "type", "1").setSource("body", "foo"));
+
+        try {
+            client().prepareSearch("test")
+                    .setTypes("type")
+                    .setQuery("{\n" +
+                            "  \"ids\": {\n" +
+                            "    \"values\": [[\"1\"]]\n" +
+                            "  }\n" +
+                            "}")
+                    .get();
+            fail("query is invalid and should have produced a parse exception");
+        } catch (Exception e) {
+            assertThat("query could not be parsed due to bad format: " + e.toString(),
+                    e.toString().contains("Illegal value for id, expecting a string or number, got: START_ARRAY"),
+                    equalTo(true));
+        }
+    }
 }
diff --git a/plugins/lang-javascript/src/test/resources/rest-api-spec/test/lang_javascript/10_basic.yaml b/plugins/lang-javascript/src/test/resources/rest-api-spec/test/lang_javascript/10_basic.yaml
index 6d1625a..ee77a84 100644
--- a/plugins/lang-javascript/src/test/resources/rest-api-spec/test/lang_javascript/10_basic.yaml
+++ b/plugins/lang-javascript/src/test/resources/rest-api-spec/test/lang_javascript/10_basic.yaml
@@ -18,10 +18,9 @@ setup:
             body:
                 script_fields:
                     bar:
-                        script: 
-                            inline: "doc['foo'].value + x"
-                            lang: javascript
-                            params:
-                                x: "bbb"
+                        lang: javascript
+                        script: "doc['foo'].value + x"
+                        params:
+                            x: "bbb"
 
     - match: { hits.hits.0.fields.bar.0: "aaabbb"}
diff --git a/plugins/lang-javascript/src/test/resources/rest-api-spec/test/lang_javascript/20_search.yaml b/plugins/lang-javascript/src/test/resources/rest-api-spec/test/lang_javascript/20_search.yaml
index 742c8f0..24a6c8b 100644
--- a/plugins/lang-javascript/src/test/resources/rest-api-spec/test/lang_javascript/20_search.yaml
+++ b/plugins/lang-javascript/src/test/resources/rest-api-spec/test/lang_javascript/20_search.yaml
@@ -33,9 +33,8 @@
                             lang: js
                 script_fields:
                     sNum1:
-                        script: 
-                            inline: "doc['num1'].value"
-                            lang: js
+                        lang: js
+                        script: "doc['num1'].value"
                 sort:
                     num1:
                         order: asc
@@ -58,9 +57,8 @@
 
                 script_fields:
                     sNum1:
-                        script:
-                            inline: "doc['num1'].value"
-                            lang: js
+                        lang: js
+                        script: "doc['num1'].value"
                 sort:
                     num1:
                         order: asc
@@ -83,9 +81,8 @@
 
                 script_fields:
                     sNum1:
-                        script: 
-                            inline: "doc['num1'].value"
-                            lang: js
+                        lang: js
+                        script: "doc['num1'].value"
                 sort:
                     num1:
                         order: asc
@@ -121,21 +118,17 @@
             body:
                 script_fields:
                     s_obj1:
-                        script: 
-                            inline: "_source.obj1"
-                            lang: js
+                        lang: js
+                        script: "_source.obj1"
                     s_obj1_test:
-                        script: 
-                            inline: "_source.obj1.test"
-                            lang: js
+                        lang: js
+                        script: "_source.obj1.test"
                     s_obj2:
-                        script: 
-                            inline: "_source.obj2"
-                            lang: js
+                        lang: js
+                        script: "_source.obj2"
                     s_obj2_arr2:
-                        script: 
-                            inline: "_source.obj2.arr2"
-                            lang: js
+                        lang: js
+                        script: "_source.obj2.arr2"
 
     - match: { hits.total: 1 }
     - match: { hits.hits.0.fields.s_obj1.0.test: something }
@@ -406,9 +399,8 @@
             body:
                 script_fields:
                     foobar:
-                        script: 
-                            inline: "doc['f'].values.length"
-                            lang: js
+                        lang: js
+                        script: "doc['f'].values.length"
 
 
     - match: { hits.total: 1 }
diff --git a/plugins/lang-python/src/test/resources/rest-api-spec/test/lang_python/10_basic.yaml b/plugins/lang-python/src/test/resources/rest-api-spec/test/lang_python/10_basic.yaml
index 4a811d1..ba7b733 100644
--- a/plugins/lang-python/src/test/resources/rest-api-spec/test/lang_python/10_basic.yaml
+++ b/plugins/lang-python/src/test/resources/rest-api-spec/test/lang_python/10_basic.yaml
@@ -18,10 +18,9 @@ setup:
             body:
                 script_fields:
                     bar:
-                        script: 
-                            inline: "doc['foo'].value + x"
-                            lang: python
-                            params:
-                                x: "bbb"
+                        lang: python
+                        script: "doc['foo'].value + x"
+                        params:
+                            x: "bbb"
 
     - match: { hits.hits.0.fields.bar.0: "aaabbb"}
diff --git a/plugins/lang-python/src/test/resources/rest-api-spec/test/lang_python/20_search.yaml b/plugins/lang-python/src/test/resources/rest-api-spec/test/lang_python/20_search.yaml
index b0f18e1..d19561a 100644
--- a/plugins/lang-python/src/test/resources/rest-api-spec/test/lang_python/20_search.yaml
+++ b/plugins/lang-python/src/test/resources/rest-api-spec/test/lang_python/20_search.yaml
@@ -33,9 +33,8 @@
                             lang: python
                 script_fields:
                     sNum1:
-                        script: 
-                            inline: "doc['num1'].value"
-                            lang: python
+                        lang: python
+                        script: "doc['num1'].value"
                 sort:
                     num1:
                         order: asc
@@ -58,9 +57,8 @@
 
                 script_fields:
                     sNum1:
-                        script: 
-                            inline: "doc['num1'].value"
-                            lang: python
+                        lang: python
+                        script: "doc['num1'].value"
                 sort:
                     num1:
                         order: asc
@@ -83,9 +81,8 @@
 
                 script_fields:
                     sNum1:
-                        script: 
-                            inline: "doc['num1'].value"
-                            lang: python
+                        lang: python
+                        script: "doc['num1'].value"
                 sort:
                     num1:
                         order: asc
@@ -121,21 +118,17 @@
             body:
                 script_fields:
                     s_obj1:
-                        script: 
-                            inline: "_source['obj1']"
-                            lang: python
+                        lang: python
+                        script: "_source['obj1']"
                     s_obj1_test:
-                        script: 
-                            inline: "_source['obj1']['test']"
-                            lang: python
+                        lang: python
+                        script: "_source['obj1']['test']"
                     s_obj2:
-                        script: 
-                            inline: "_source['obj2']"
-                            lang: python
+                        lang: python
+                        script: "_source['obj2']"
                     s_obj2_arr2:
-                        script: 
-                            inline: "_source['obj2']['arr2']"
-                            lang: python
+                        lang: python
+                        script: "_source['obj2']['arr2']"
 
     - match: { hits.total: 1 }
     - match: { hits.hits.0.fields.s_obj1.0.test: something }
diff --git a/plugins/pom.xml b/plugins/pom.xml
index d248c7f..d080a55 100644
--- a/plugins/pom.xml
+++ b/plugins/pom.xml
@@ -388,10 +388,10 @@
         <module>analysis-phonetic</module>
         <module>analysis-smartcn</module>
         <module>analysis-stempel</module>
-        <module>cloud-gce</module>
         <module>delete-by-query</module>
         <module>discovery-azure</module>
         <module>discovery-ec2</module>
+        <module>discovery-gce</module>
         <module>discovery-multicast</module>
         <module>lang-expression</module>
         <module>lang-groovy</module>
diff --git a/qa/smoke-test-plugins/pom.xml b/qa/smoke-test-plugins/pom.xml
index 238ea9c..f9d4b79 100644
--- a/qa/smoke-test-plugins/pom.xml
+++ b/qa/smoke-test-plugins/pom.xml
@@ -271,7 +271,7 @@
 
                  <artifactItem>
                    <groupId>org.elasticsearch.plugin</groupId>
-                   <artifactId>cloud-gce</artifactId>
+                   <artifactId>discovery-gce</artifactId>
                    <version>${elasticsearch.version}</version>
                    <type>zip</type>
                    <overWrite>true</overWrite>
diff --git a/qa/vagrant/pom.xml b/qa/vagrant/pom.xml
index c0f7535..e303da5 100644
--- a/qa/vagrant/pom.xml
+++ b/qa/vagrant/pom.xml
@@ -143,7 +143,7 @@
                                 </artifactItem>
                                 <artifactItem>
                                     <groupId>org.elasticsearch.plugin</groupId>
-                                    <artifactId>cloud-gce</artifactId>
+                                    <artifactId>discovery-gce</artifactId>
                                     <version>${elasticsearch.version}</version>
                                     <type>zip</type>
                                 </artifactItem>
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash b/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash
index c8c7b2d..c81af43 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash
+++ b/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash
@@ -239,18 +239,67 @@ clean_before_test() {
 start_elasticsearch_service() {
     local desiredStatus=${1:-green}
 
+    run_elasticsearch_service 0
+
+    wait_for_elasticsearch_status $desiredStatus
+
+    if [ -r "/tmp/elasticsearch/elasticsearch.pid" ]; then
+        pid=$(cat /tmp/elasticsearch/elasticsearch.pid)
+        [ "x$pid" != "x" ] && [ "$pid" -gt 0 ]
+        echo "Looking for elasticsearch pid...."
+        ps $pid
+    elif is_systemd; then
+        run systemctl is-active elasticsearch.service
+        [ "$status" -eq 0 ]
+
+        run systemctl status elasticsearch.service
+        [ "$status" -eq 0 ]
+
+    elif is_sysvinit; then
+        run service elasticsearch status
+        [ "$status" -eq 0 ]
+    fi
+}
+
+# Start elasticsearch
+# $1 expected status code
+# $2 additional command line args
+run_elasticsearch_service() {
+    local expectedStatus=$1
+    local commandLineArgs=$2
+    # Set the CONF_DIR setting in case we start as a service
+    if [ ! -z "$CONF_DIR" ] ; then
+        if is_dpkg ; then
+            echo "CONF_DIR=$CONF_DIR" >> /etc/default/elasticsearch;
+        elif is_rpm; then
+            echo "CONF_DIR=$CONF_DIR" >> /etc/sysconfig/elasticsearch;
+        fi
+    fi
+
     if [ -f "/tmp/elasticsearch/bin/elasticsearch" ]; then
+        if [ -z "$CONF_DIR" ]; then
+            local CONF_DIR=""
+        fi
+        # we must capture the exit code to compare so we don't want to start as background process in case we expect something other than 0
+        local background=""
+        local timeoutCommand=""
+        if [ "$expectedStatus" = 0 ]; then
+            background="-d"
+        else
+            timeoutCommand="timeout 60s "
+        fi
         # su and the Elasticsearch init script work together to break bats.
         # sudo isolates bats enough from the init script so everything continues
         # to tick along
-        sudo -u elasticsearch bash <<BASH
+        run sudo -u elasticsearch bash <<BASH
 # If jayatana is installed then we try to use it. Elasticsearch should ignore it even when we try.
 # If it doesn't ignore it then Elasticsearch will fail to start because of security errors.
 # This line is attempting to emulate the on login behavior of /usr/share/upstart/sessions/jayatana.conf
 [ -f /usr/share/java/jayatanaag.jar ] && export JAVA_TOOL_OPTIONS="-javaagent:/usr/share/java/jayatanaag.jar"
 # And now we can start Elasticsearch normally, in the background (-d) and with a pidfile (-p).
-/tmp/elasticsearch/bin/elasticsearch -d -p /tmp/elasticsearch/elasticsearch.pid
+$timeoutCommand/tmp/elasticsearch/bin/elasticsearch $background -p /tmp/elasticsearch/elasticsearch.pid -Des.path.conf=$CONF_DIR $commandLineArgs
 BASH
+        [ "$status" -eq "$expectedStatus" ]
     elif is_systemd; then
         run systemctl daemon-reload
         [ "$status" -eq 0 ]
@@ -262,31 +311,11 @@ BASH
         [ "$status" -eq 0 ]
 
         run systemctl start elasticsearch.service
-        [ "$status" -eq 0 ]
+        [ "$status" -eq "$expectedStatus" ]
 
     elif is_sysvinit; then
         run service elasticsearch start
-        [ "$status" -eq 0 ]
-    fi
-
-    wait_for_elasticsearch_status $desiredStatus
-
-    if [ -r "/tmp/elasticsearch/elasticsearch.pid" ]; then
-        pid=$(cat /tmp/elasticsearch/elasticsearch.pid)
-        [ "x$pid" != "x" ] && [ "$pid" -gt 0 ]
-
-        echo "Looking for elasticsearch pid...."
-        ps $pid
-    elif is_systemd; then
-        run systemctl is-active elasticsearch.service
-        [ "$status" -eq 0 ]
-
-        run systemctl status elasticsearch.service
-        [ "$status" -eq 0 ]
-
-    elif is_sysvinit; then
-        run service elasticsearch status
-        [ "$status" -eq 0 ]
+        [ "$status" -eq "$expectedStatus" ]
     fi
 }
 
@@ -325,7 +354,7 @@ wait_for_elasticsearch_status() {
           if [ -e "$ESLOG/elasticsearch.log" ]; then
               cat "$ESLOG/elasticsearch.log"
           else
-              echo "The elasticsearch log doesn't exist. Maybe /vag/log/messages has something:"
+              echo "The elasticsearch log doesn't exist. Maybe /var/log/messages has something:"
               tail -n20 /var/log/messages
           fi
           false
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash b/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash
index a199cb9..bd9e28e 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash
+++ b/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash
@@ -83,6 +83,35 @@ else
     }
 fi
 
+@test "[$GROUP] install jvm-example plugin with a custom CONFIG_FILE and check failure" {
+    local relativePath=${1:-$(readlink -m jvm-example-*.zip)}
+    CONF_FILE="$ESCONFIG/elasticsearch.yml" run sudo -E -u $ESPLUGIN_COMMAND_USER "$ESHOME/bin/plugin" install "file://$relativePath"
+    # this should fail because CONF_FILE is no longer supported
+    [ $status = 1 ]
+    CONF_FILE="$ESCONFIG/elasticsearch.yml" run sudo -E -u $ESPLUGIN_COMMAND_USER "$ESHOME/bin/plugin" remove jvm-example
+    echo "status is $status"
+    [ $status = 1 ]
+}
+
+@test "[$GROUP] start elasticsearch with a custom CONFIG_FILE and check failure" {
+    local CONF_FILE="$ESCONFIG/elasticsearch.yml"
+
+    if is_dpkg; then
+        echo "CONF_FILE=$CONF_FILE" >> /etc/default/elasticsearch;
+    elif is_rpm; then
+        echo "CONF_FILE=$CONF_FILE" >> /etc/sysconfig/elasticsearch;
+    fi
+
+    run_elasticsearch_service 1 -Des.default.config="$CONF_FILE"
+
+    # remove settings again otherwise cleaning up before next testrun will fail
+    if is_dpkg ; then
+        sudo sed -i '/CONF_FILE/d' /etc/default/elasticsearch
+    elif is_rpm; then
+        sudo sed -i '/CONF_FILE/d' /etc/sysconfig/elasticsearch
+    fi
+}
+
 @test "[$GROUP] install jvm-example plugin with a custom path.plugins" {
     # Clean up after the last time this test was run
     rm -rf /tmp/plugins.*
@@ -111,6 +140,9 @@ fi
     move_config
 
     CONF_DIR="$ESCONFIG" install_jvm_example
+    CONF_DIR="$ESCONFIG" start_elasticsearch_service
+    diff  <(curl -s localhost:9200/_cat/configured_example | sed 's/ //g') <(echo "foo")
+    stop_elasticsearch_service
     CONF_DIR="$ESCONFIG" remove_jvm_example
 }
 
@@ -172,7 +204,7 @@ fi
 }
 
 @test "[$GROUP] install gce plugin" {
-    install_and_check_plugin cloud gce google-api-client-*.jar
+    install_and_check_plugin discovery gce google-api-client-*.jar
 }
 
 @test "[$GROUP] install delete by query plugin" {
@@ -276,7 +308,7 @@ fi
 }
 
 @test "[$GROUP] remove gce plugin" {
-    remove_plugin cloud-gce
+    remove_plugin discovery-gce
 }
 
 @test "[$GROUP] remove delete by query plugin" {
@@ -357,21 +389,38 @@ fi
     local relativePath=${1:-$(readlink -m jvm-example-*.zip)}
     sudo -E -u $ESPLUGIN_COMMAND_USER "$ESHOME/bin/plugin" install "file://$relativePath" > /tmp/plugin-cli-output
     local loglines=$(cat /tmp/plugin-cli-output | wc -l)
-    [ "$loglines" = "6" ] || {
-        echo "Expected 6 lines but the output was:"
-        cat /tmp/plugin-cli-output
-        false
-    }
+    if [ "$GROUP" == "TAR PLUGINS" ]; then
+    # tar extraction does not create the plugins directory so the plugin tool will print an additional line that the directory will be created
+        [ "$loglines" -eq "7" ] || {
+            echo "Expected 7 lines but the output was:"
+            cat /tmp/plugin-cli-output
+            false
+        }
+    else
+        [ "$loglines" -eq "6" ] || {
+            echo "Expected 6 lines but the output was:"
+            cat /tmp/plugin-cli-output
+            false
+        }
+    fi
     remove_jvm_example
 
     local relativePath=${1:-$(readlink -m jvm-example-*.zip)}
     sudo -E -u $ESPLUGIN_COMMAND_USER "$ESHOME/bin/plugin" install "file://$relativePath" -Des.logger.level=DEBUG > /tmp/plugin-cli-output
     local loglines=$(cat /tmp/plugin-cli-output | wc -l)
-    [ "$loglines" -gt "6" ] || {
-        echo "Expected more than 6 lines but the output was:"
-        cat /tmp/plugin-cli-output
-        false
-    }
+    if [ "$GROUP" == "TAR PLUGINS" ]; then
+        [ "$loglines" -gt "7" ] || {
+            echo "Expected more than 7 lines but the output was:"
+            cat /tmp/plugin-cli-output
+            false
+        }
+    else
+        [ "$loglines" -gt "6" ] || {
+            echo "Expected more than 6 lines but the output was:"
+            cat /tmp/plugin-cli-output
+            false
+        }
+    fi
     remove_jvm_example
 }
 
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/cluster.stats.json b/rest-api-spec/src/main/resources/rest-api-spec/api/cluster.stats.json
index 42c13dc..2bccb20 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/cluster.stats.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/cluster.stats.json
@@ -20,6 +20,10 @@
             "type": "boolean",
             "description": "Whether to return time and byte values in human-readable format.",
             "default": false
+        },
+        "timeout": {
+          "type" : "time",
+          "description" : "Explicit operation timeout"
         }
       }
     },
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/indices.get.json b/rest-api-spec/src/main/resources/rest-api-spec/api/indices.get.json
index 14fae15..5c426f9 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/indices.get.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/indices.get.json
@@ -13,7 +13,8 @@
         },
         "feature":{
           "type":"list",
-          "description":"A comma-separated list of features"
+          "description":"A comma-separated list of features",
+          "options": ["_settings", "_mappings", "_warmers", "_aliases"]
         }
       },
       "params":{
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/nodes.hot_threads.json b/rest-api-spec/src/main/resources/rest-api-spec/api/nodes.hot_threads.json
index 5b49782..854cde1 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/nodes.hot_threads.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/nodes.hot_threads.json
@@ -24,7 +24,7 @@
           "type" : "number",
           "description" : "Specify the number of threads to provide information for (default: 3)"
         },
-	"ignore_idle_threads": {
+        "ignore_idle_threads": {
           "type" : "boolean",
           "description" : "Don't show threads that are in known-idle places, such as waiting on a socket select or pulling from an empty task queue (default: true)"
         },
@@ -32,6 +32,10 @@
           "type" : "enum",
           "options" : ["cpu", "wait", "block"],
           "description" : "The type to sample (default: cpu)"
+        },
+        "timeout": {
+          "type" : "time",
+          "description" : "Explicit operation timeout"
         }
       }
     },
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/nodes.info.json b/rest-api-spec/src/main/resources/rest-api-spec/api/nodes.info.json
index d8044c8..43be35a 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/nodes.info.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/nodes.info.json
@@ -25,6 +25,10 @@
             "type": "boolean",
             "description": "Whether to return time and byte values in human-readable format.",
             "default": false
+        },
+        "timeout": {
+          "type" : "time",
+          "description" : "Explicit operation timeout"
         }
       }
     },
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/nodes.stats.json b/rest-api-spec/src/main/resources/rest-api-spec/api/nodes.stats.json
index 5eef2c1..8742941 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/nodes.stats.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/nodes.stats.json
@@ -59,6 +59,10 @@
         "types" : {
           "type" : "list",
           "description" : "A comma-separated list of document types for the `indexing` index metric"
+        },
+        "timeout": {
+          "type" : "time",
+          "description" : "Explicit operation timeout"
         }
       }
     },
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/update.json b/rest-api-spec/src/main/resources/rest-api-spec/api/update.json
index 20fc352..37a04cb 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/update.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/update.json
@@ -82,6 +82,10 @@
           "type": "enum",
           "options": ["internal", "force"],
           "description": "Specific version type"
+        },
+        "detect_noop": {
+          "type": "boolean",
+          "description": "Specifying as true will cause Elasticsearch to check if there are changes and, if there arent, turn the update request into a noop."
         }
       }
     },
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/create/35_external_version.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/create/35_external_version.yaml
deleted file mode 100644
index 8ee11b0..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/create/35_external_version.yaml
+++ /dev/null
@@ -1,33 +0,0 @@
----
-"External version":
-
- - do:
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   external
-          version:        5
-
- - match:   { _version: 5}
-
- - do:
-      catch:             conflict
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   external
-          version:        5
-
- - do:
-      catch:              conflict
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   external
-          version:        6
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/create/36_external_gte_version.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/create/36_external_gte_version.yaml
deleted file mode 100644
index febb7a5..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/create/36_external_gte_version.yaml
+++ /dev/null
@@ -1,33 +0,0 @@
----
-"External version":
-
- - do:
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   external_gte
-          version:        5
-
- - match:   { _version: 5}
-
- - do:
-      catch:             conflict
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   external_gte
-          version:        5
-
- - do:
-      catch:              conflict
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   external_gte
-          version:        6
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/create/37_force_version.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/create/37_force_version.yaml
deleted file mode 100644
index 393d16f..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/create/37_force_version.yaml
+++ /dev/null
@@ -1,33 +0,0 @@
----
-"External version":
-
- - do:
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   force
-          version:        5
-
- - match:   { _version: 5}
-
- - do:
-      catch:             conflict
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   force
-          version:        5
-
- - do:
-      catch:              conflict
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   force
-          version:        6
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/10_basic.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/10_basic.yaml
index 7e4c574..44313aa 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/10_basic.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/10_basic.yaml
@@ -35,7 +35,7 @@ setup:
         index: test_index
         name: test_warmer
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
 
   - do:
       indices.delete_warmer:
@@ -55,8 +55,8 @@ setup:
   - do:
       indices.get_warmer: {}
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
+  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {}}
 
 
 ---
@@ -67,8 +67,8 @@ setup:
         index: '*'
         name: '*'
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
+  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {}}
 
 ---
 "Getting warmers for several indices should work using _all":
@@ -78,8 +78,8 @@ setup:
         index: _all
         name: _all
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
+  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {}}
 
 ---
 "Getting all warmers without specifying index should work":
@@ -88,8 +88,8 @@ setup:
       indices.get_warmer:
         name: _all
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
+  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {}}
 
 ---
 "Getting warmers for several indices should work using prefix*":
@@ -99,8 +99,8 @@ setup:
         index: test_i*
         name: test_w*
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
+  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {}}
 
 ---
 "Getting warmers for several indices should work using comma-separated lists":
@@ -110,8 +110,8 @@ setup:
         index: test_index,test_idx
         name: test_warmer,test_warmer2
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
+  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {}}
 
 ---
 "Getting a non-existent warmer on an existing index should return an empty body":
@@ -131,7 +131,7 @@ setup:
         index: test_index
         name: test_warmer,non-existent
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
   - is_false: test_index.warmers.non-existent
 
 --- 
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/20_aliases.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/20_aliases.yaml
index b8a2fa6..96d7344 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/20_aliases.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/20_aliases.yaml
@@ -26,5 +26,5 @@
       indices.get_warmer:
           index: test_alias
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
 
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/all_path_options.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/all_path_options.yaml
index ffad427..b9c64f7 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/all_path_options.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/all_path_options.yaml
@@ -38,8 +38,8 @@ setup:
   - do:
       indices.get_warmer: { index: _all, name: '*' }
 
-  - match: {test_index1.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_index2.warmers.warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index1.warmers.warmer.source.query.match_all: {}}
+  - match: {test_index2.warmers.warmer.source.query.match_all: {}}
   - is_false: foo
 
 ---
@@ -54,9 +54,9 @@ setup:
   - do:
       indices.get_warmer: { index: _all, name: '*' }
 
-  - match: {test_index1.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_index2.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {foo.warmers.warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index1.warmers.warmer.source.query.match_all: {}}
+  - match: {test_index2.warmers.warmer.source.query.match_all: {}}
+  - match: {foo.warmers.warmer.source.query.match_all: {}}
 
 ---
 "put warmer in * index":
@@ -70,9 +70,9 @@ setup:
   - do:
       indices.get_warmer: { index: _all, name: '*' }
 
-  - match: {test_index1.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_index2.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {foo.warmers.warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index1.warmers.warmer.source.query.match_all: {}}
+  - match: {test_index2.warmers.warmer.source.query.match_all: {}}
+  - match: {foo.warmers.warmer.source.query.match_all: {}}
 
 ---
 "put warmer prefix* index":
@@ -86,8 +86,8 @@ setup:
   - do:
       indices.get_warmer: { index: _all, name: '*' }
 
-  - match: {test_index1.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_index2.warmers.warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index1.warmers.warmer.source.query.match_all: {}}
+  - match: {test_index2.warmers.warmer.source.query.match_all: {}}
   - is_false: foo
 
 ---
@@ -102,8 +102,8 @@ setup:
   - do:
       indices.get_warmer: { index: _all, name: '*' }
 
-  - match: {test_index1.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_index2.warmers.warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index1.warmers.warmer.source.query.match_all: {}}
+  - match: {test_index2.warmers.warmer.source.query.match_all: {}}
   - is_false: foo
 
 ---
@@ -117,9 +117,9 @@ setup:
   - do:
       indices.get_warmer: { index: _all, name: '*' }
 
-  - match: {test_index1.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_index2.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {foo.warmers.warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index1.warmers.warmer.source.query.match_all: {}}
+  - match: {test_index2.warmers.warmer.source.query.match_all: {}}
+  - match: {foo.warmers.warmer.source.query.match_all: {}}
 
 ---
 "put warmer with missing name":
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/search/10_source_filtering.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/search/10_source_filtering.yaml
index b49d659..a78a5a2 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/search/10_source_filtering.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/search/10_source_filtering.yaml
@@ -14,12 +14,12 @@
   - do:
       search:
       # stringified for boolean value
-        body: { _source: true, query: { match_all: {} } }
+        body: "{ _source: true, query: { match_all: {} } }"
 
   - length:   { hits.hits: 1  }
   - match: { hits.hits.0._source.count: 1 }
 
-  - do: { search: { body: { _source: false, query: { match_all: {} } } } }
+  - do: { search: { body: "{ _source: false, query: { match_all: {} } }" } }
   - length:   { hits.hits: 1  }
   - is_false: hits.hits.0._source
 
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/template/20_search.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/template/20_search.yaml
index 4da748a..5153f6c 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/template/20_search.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/template/20_search.yaml
@@ -28,11 +28,16 @@
 
   - do:
       search_template:
+        body: { "template": { "id" : "1" }, "params" : { "my_value" : "value1_foo", "my_size" : 1 } }
+  - match: { hits.total: 1 }
+
+  - do:
+      search_template:
         body: {  "id" : "1", "params" : { "my_value" : "value1_foo", "my_size" : 1 } }
   - match: { hits.total: 1 }
 
   - do:
       catch: /Unable.to.find.on.disk.file.script.\[simple1\].using.lang.\[mustache\]/
       search_template:
-        body: { "file" : "simple1"}
+        body: { "template" : "simple1" }
 
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/update/30_internal_version.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/update/30_internal_version.yaml
index 3f13b09..17c4806 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/update/30_internal_version.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/update/30_internal_version.yaml
@@ -2,7 +2,7 @@
 "Internal version":
 
  - do:
-      catch:        conflict
+      catch:        missing
       update:
           index:    test_1
           type:     test
@@ -10,7 +10,14 @@
           version:  1
           body:
             doc:    { foo: baz }
-            upsert: { foo: bar }
+
+ - do:
+      index:
+          index:    test_1
+          type:     test
+          id:       1
+          body:
+            doc:    { foo: baz }
 
  - do:
       catch:        conflict
@@ -18,7 +25,6 @@
           index:    test_1
           type:     test
           id:       1
-          version:  1
+          version:  2
           body:
             doc:    { foo: baz }
-            upsert: { foo: bar }
