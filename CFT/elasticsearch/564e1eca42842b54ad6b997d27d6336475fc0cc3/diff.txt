diff --git a/core/pom.xml b/core/pom.xml
index e367d33..58118a2 100644
--- a/core/pom.xml
+++ b/core/pom.xml
@@ -293,6 +293,7 @@
                                 <include>org/elasticsearch/common/util/MockBigArrays$*.class</include>
                                 <include>org/elasticsearch/node/NodeMocksPlugin.class</include>
                                 <include>org/elasticsearch/node/MockNode.class</include>
+                                <include>org/elasticsearch/common/io/PathUtilsForTesting.class</include>
                             </includes>
                             <excludes>
                                 <!-- unit tests for yaml suite parser & rest spec parser need to be excluded -->
diff --git a/core/src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java b/core/src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java
index 98401cd..bac323d 100644
--- a/core/src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java
+++ b/core/src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java
@@ -259,6 +259,10 @@ public class XAnalyzingSuggester extends Lookup {
 public long ramBytesUsed() {
     return fst == null ? 0 : fst.ramBytesUsed();
   }
+  
+  public int getMaxAnalyzedPathsForOneInput() {
+      return maxAnalyzedPathsForOneInput;
+  }
 
   // Replaces SEP with epsilon or remaps them if
   // we were asked to preserve them:
diff --git a/core/src/main/java/org/apache/lucene/search/vectorhighlight/CustomFieldQuery.java b/core/src/main/java/org/apache/lucene/search/vectorhighlight/CustomFieldQuery.java
index e1a78ac..e1a8d81 100644
--- a/core/src/main/java/org/apache/lucene/search/vectorhighlight/CustomFieldQuery.java
+++ b/core/src/main/java/org/apache/lucene/search/vectorhighlight/CustomFieldQuery.java
@@ -58,28 +58,28 @@ public class CustomFieldQuery extends FieldQuery {
     }
 
     @Override
-    void flatten(Query sourceQuery, IndexReader reader, Collection<Query> flatQueries) throws IOException {
+    void flatten(Query sourceQuery, IndexReader reader, Collection<Query> flatQueries, float boost) throws IOException {
         if (sourceQuery instanceof SpanTermQuery) {
-            super.flatten(new TermQuery(((SpanTermQuery) sourceQuery).getTerm()), reader, flatQueries);
+            super.flatten(new TermQuery(((SpanTermQuery) sourceQuery).getTerm()), reader, flatQueries, boost);
         } else if (sourceQuery instanceof ConstantScoreQuery) {
-            flatten(((ConstantScoreQuery) sourceQuery).getQuery(), reader, flatQueries);
+            flatten(((ConstantScoreQuery) sourceQuery).getQuery(), reader, flatQueries, boost);
         } else if (sourceQuery instanceof FunctionScoreQuery) {
-            flatten(((FunctionScoreQuery) sourceQuery).getSubQuery(), reader, flatQueries);
+            flatten(((FunctionScoreQuery) sourceQuery).getSubQuery(), reader, flatQueries, boost);
         } else if (sourceQuery instanceof FilteredQuery) {
-            flatten(((FilteredQuery) sourceQuery).getQuery(), reader, flatQueries);
+            flatten(((FilteredQuery) sourceQuery).getQuery(), reader, flatQueries, boost);
             flatten(((FilteredQuery) sourceQuery).getFilter(), reader, flatQueries);
         } else if (sourceQuery instanceof MultiPhrasePrefixQuery) {
-            flatten(sourceQuery.rewrite(reader), reader, flatQueries);
+            flatten(sourceQuery.rewrite(reader), reader, flatQueries, boost);
         } else if (sourceQuery instanceof FiltersFunctionScoreQuery) {
-            flatten(((FiltersFunctionScoreQuery) sourceQuery).getSubQuery(), reader, flatQueries);
+            flatten(((FiltersFunctionScoreQuery) sourceQuery).getSubQuery(), reader, flatQueries, boost);
         } else if (sourceQuery instanceof MultiPhraseQuery) {
             MultiPhraseQuery q = ((MultiPhraseQuery) sourceQuery);
             convertMultiPhraseQuery(0, new int[q.getTermArrays().size()], q, q.getTermArrays(), q.getPositions(), reader, flatQueries);
         } else if (sourceQuery instanceof BlendedTermQuery) {
             final BlendedTermQuery blendedTermQuery = (BlendedTermQuery) sourceQuery;
-            flatten(blendedTermQuery.rewrite(reader), reader, flatQueries);
+            flatten(blendedTermQuery.rewrite(reader), reader, flatQueries, boost);
         } else {
-            super.flatten(sourceQuery, reader, flatQueries);
+            super.flatten(sourceQuery, reader, flatQueries, boost);
         }
     }
     
@@ -93,7 +93,7 @@ public class CustomFieldQuery extends FieldQuery {
             if (numTerms > 16) {
                 for (Term[] currentPosTerm : terms) {
                     for (Term term : currentPosTerm) {
-                        super.flatten(new TermQuery(term), reader, flatQueries);    
+                        super.flatten(new TermQuery(term), reader, flatQueries, orig.getBoost());    
                     }
                 }
                 return;
@@ -111,7 +111,7 @@ public class CustomFieldQuery extends FieldQuery {
             }
             PhraseQuery query = queryBuilder.build();
             query.setBoost(orig.getBoost());
-            this.flatten(query, reader, flatQueries);
+            this.flatten(query, reader, flatQueries, orig.getBoost());
         } else {
             Term[] t = terms.get(currentPos);
             for (int i = 0; i < t.length; i++) {
@@ -127,7 +127,7 @@ public class CustomFieldQuery extends FieldQuery {
             return;
         }
         if (sourceFilter instanceof QueryWrapperFilter) {
-            flatten(((QueryWrapperFilter) sourceFilter).getQuery(), reader, flatQueries);
+            flatten(((QueryWrapperFilter) sourceFilter).getQuery(), reader, flatQueries, 1.0F);
         }
     }
 }
diff --git a/core/src/main/java/org/apache/lucene/util/XGeoHashUtils.java b/core/src/main/java/org/apache/lucene/util/XGeoHashUtils.java
new file mode 100644
index 0000000..2b9841e
--- /dev/null
+++ b/core/src/main/java/org/apache/lucene/util/XGeoHashUtils.java
@@ -0,0 +1,279 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.lucene.util;
+
+import java.util.ArrayList;
+import java.util.Collection;
+
+/**
+ * Utilities for converting to/from the GeoHash standard
+ *
+ * The geohash long format is represented as lon/lat (x/y) interleaved with the 4 least significant bits
+ * representing the level (1-12) [xyxy...xyxyllll]
+ *
+ * This differs from a morton encoded value which interleaves lat/lon (y/x).
+ *
+ * @lucene.experimental
+ */
+public class XGeoHashUtils {
+    public static final char[] BASE_32 = {'0', '1', '2', '3', '4', '5', '6',
+            '7', '8', '9', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'j', 'k', 'm', 'n',
+            'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'};
+
+    public static final String BASE_32_STRING = new String(BASE_32);
+
+    public static final int PRECISION = 12;
+    private static final short MORTON_OFFSET = (XGeoUtils.BITS<<1) - (PRECISION*5);
+
+    /**
+     * Encode lon/lat to the geohash based long format (lon/lat interleaved, 4 least significant bits = level)
+     */
+    public static final long longEncode(final double lon, final double lat, final int level) {
+        // shift to appropriate level
+        final short msf = (short)(((12 - level) * 5) + MORTON_OFFSET);
+        return ((BitUtil.flipFlop(XGeoUtils.mortonHash(lon, lat)) >>> msf) << 4) | level;
+    }
+
+    /**
+     * Encode from geohash string to the geohash based long format (lon/lat interleaved, 4 least significant bits = level)
+     */
+    public static final long longEncode(final String hash) {
+        int level = hash.length()-1;
+        long b;
+        long l = 0L;
+        for(char c : hash.toCharArray()) {
+            b = (long)(BASE_32_STRING.indexOf(c));
+            l |= (b<<(level--*5));
+        }
+        return (l<<4)|hash.length();
+    }
+
+    /**
+     * Encode an existing geohash long to the provided precision
+     */
+    public static long longEncode(long geohash, int level) {
+        final short precision = (short)(geohash & 15);
+        if (precision == level) {
+            return geohash;
+        } else if (precision > level) {
+            return ((geohash >>> (((precision - level) * 5) + 4)) << 4) | level;
+        }
+        return ((geohash >>> 4) << (((level - precision) * 5) + 4) | level);
+    }
+
+    /**
+     * Encode to a geohash string from the geohash based long format
+     */
+    public static final String stringEncode(long geoHashLong) {
+        int level = (int)geoHashLong&15;
+        geoHashLong >>>= 4;
+        char[] chars = new char[level];
+        do {
+            chars[--level] = BASE_32[(int)(geoHashLong&31L)];
+            geoHashLong>>>=5;
+        } while(level > 0);
+
+        return new String(chars);
+    }
+
+    /**
+     * Encode to a geohash string from full resolution longitude, latitude)
+     */
+    public static final String stringEncode(final double lon, final double lat) {
+        return stringEncode(lon, lat, 12);
+    }
+
+    /**
+     * Encode to a level specific geohash string from full resolution longitude, latitude
+     */
+    public static final String stringEncode(final double lon, final double lat, final int level) {
+        // bit twiddle to geohash (since geohash is a swapped (lon/lat) encoding)
+        final long hashedVal = BitUtil.flipFlop(XGeoUtils.mortonHash(lon, lat));
+
+        StringBuilder geoHash = new StringBuilder();
+        short precision = 0;
+        final short msf = (XGeoUtils.BITS<<1)-5;
+        long mask = 31L<<msf;
+        do {
+            geoHash.append(BASE_32[(int)((mask & hashedVal)>>>(msf-(precision*5)))]);
+            // next 5 bits
+            mask >>>= 5;
+        } while (++precision < level);
+        return geoHash.toString();
+    }
+
+    /**
+     * Encode to a full precision geohash string from a given morton encoded long value
+     */
+    public static final String stringEncodeFromMortonLong(final long hashedVal) throws Exception {
+        return stringEncode(hashedVal, PRECISION);
+    }
+
+    /**
+     * Encode to a geohash string at a given level from a morton long
+     */
+    public static final String stringEncodeFromMortonLong(long hashedVal, final int level) {
+        // bit twiddle to geohash (since geohash is a swapped (lon/lat) encoding)
+        hashedVal = BitUtil.flipFlop(hashedVal);
+
+        StringBuilder geoHash = new StringBuilder();
+        short precision = 0;
+        final short msf = (XGeoUtils.BITS<<1)-5;
+        long mask = 31L<<msf;
+        do {
+            geoHash.append(BASE_32[(int)((mask & hashedVal)>>>(msf-(precision*5)))]);
+            // next 5 bits
+            mask >>>= 5;
+        } while (++precision < level);
+        return geoHash.toString();
+    }
+
+    /**
+     * Encode to a morton long value from a given geohash string
+     */
+    public static final long mortonEncode(final String hash) {
+        int level = 11;
+        long b;
+        long l = 0L;
+        for(char c : hash.toCharArray()) {
+            b = (long)(BASE_32_STRING.indexOf(c));
+            l |= (b<<((level--*5) + MORTON_OFFSET));
+        }
+        return BitUtil.flipFlop(l);
+    }
+
+    /**
+     * Encode to a morton long value from a given geohash long value
+     */
+    public static final long mortonEncode(final long geoHashLong) {
+        final int level = (int)(geoHashLong&15);
+        final short odd = (short)(level & 1);
+
+        return BitUtil.flipFlop((geoHashLong >>> 4) << odd) << (((12 - level) * 5) + (MORTON_OFFSET - odd));
+    }
+
+    private static final char encode(int x, int y) {
+        return BASE_32[((x & 1) + ((y & 1) * 2) + ((x & 2) * 2) + ((y & 2) * 4) + ((x & 4) * 4)) % 32];
+    }
+
+    /**
+     * Calculate all neighbors of a given geohash cell.
+     *
+     * @param geohash Geohash of the defined cell
+     * @return geohashes of all neighbor cells
+     */
+    public static Collection<? extends CharSequence> neighbors(String geohash) {
+        return addNeighbors(geohash, geohash.length(), new ArrayList<CharSequence>(8));
+    }
+
+    /**
+     * Calculate the geohash of a neighbor of a geohash
+     *
+     * @param geohash the geohash of a cell
+     * @param level   level of the geohash
+     * @param dx      delta of the first grid coordinate (must be -1, 0 or +1)
+     * @param dy      delta of the second grid coordinate (must be -1, 0 or +1)
+     * @return geohash of the defined cell
+     */
+    private final static String neighbor(String geohash, int level, int dx, int dy) {
+        int cell = BASE_32_STRING.indexOf(geohash.charAt(level -1));
+
+        // Decoding the Geohash bit pattern to determine grid coordinates
+        int x0 = cell & 1;  // first bit of x
+        int y0 = cell & 2;  // first bit of y
+        int x1 = cell & 4;  // second bit of x
+        int y1 = cell & 8;  // second bit of y
+        int x2 = cell & 16; // third bit of x
+
+        // combine the bitpattern to grid coordinates.
+        // note that the semantics of x and y are swapping
+        // on each level
+        int x = x0 + (x1 / 2) + (x2 / 4);
+        int y = (y0 / 2) + (y1 / 4);
+
+        if (level == 1) {
+            // Root cells at north (namely "bcfguvyz") or at
+            // south (namely "0145hjnp") do not have neighbors
+            // in north/south direction
+            if ((dy < 0 && y == 0) || (dy > 0 && y == 3)) {
+                return null;
+            } else {
+                return Character.toString(encode(x + dx, y + dy));
+            }
+        } else {
+            // define grid coordinates for next level
+            final int nx = ((level % 2) == 1) ? (x + dx) : (x + dy);
+            final int ny = ((level % 2) == 1) ? (y + dy) : (y + dx);
+
+            // if the defined neighbor has the same parent a the current cell
+            // encode the cell directly. Otherwise find the cell next to this
+            // cell recursively. Since encoding wraps around within a cell
+            // it can be encoded here.
+            // xLimit and YLimit must always be respectively 7 and 3
+            // since x and y semantics are swapping on each level.
+            if (nx >= 0 && nx <= 7 && ny >= 0 && ny <= 3) {
+                return geohash.substring(0, level - 1) + encode(nx, ny);
+            } else {
+                String neighbor = neighbor(geohash, level - 1, dx, dy);
+                return (neighbor != null) ? neighbor + encode(nx, ny) : neighbor;
+            }
+        }
+    }
+
+    /**
+     * Add all geohashes of the cells next to a given geohash to a list.
+     *
+     * @param geohash   Geohash of a specified cell
+     * @param neighbors list to add the neighbors to
+     * @return the given list
+     */
+    public static final <E extends Collection<? super String>> E addNeighbors(String geohash, E neighbors) {
+        return addNeighbors(geohash, geohash.length(), neighbors);
+    }
+
+    /**
+     * Add all geohashes of the cells next to a given geohash to a list.
+     *
+     * @param geohash   Geohash of a specified cell
+     * @param length    level of the given geohash
+     * @param neighbors list to add the neighbors to
+     * @return the given list
+     */
+    public static final <E extends Collection<? super String>> E addNeighbors(String geohash, int length, E neighbors) {
+        String south = neighbor(geohash, length, 0, -1);
+        String north = neighbor(geohash, length, 0, +1);
+        if (north != null) {
+            neighbors.add(neighbor(north, length, -1, 0));
+            neighbors.add(north);
+            neighbors.add(neighbor(north, length, +1, 0));
+        }
+
+        neighbors.add(neighbor(geohash, length, -1, 0));
+        neighbors.add(neighbor(geohash, length, +1, 0));
+
+        if (south != null) {
+            neighbors.add(neighbor(south, length, -1, 0));
+            neighbors.add(south);
+            neighbors.add(neighbor(south, length, +1, 0));
+        }
+
+        return neighbors;
+    }
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/apache/lucene/util/XGeoProjectionUtils.java b/core/src/main/java/org/apache/lucene/util/XGeoProjectionUtils.java
new file mode 100644
index 0000000..5d13c2f
--- /dev/null
+++ b/core/src/main/java/org/apache/lucene/util/XGeoProjectionUtils.java
@@ -0,0 +1,383 @@
+package org.apache.lucene.util;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Reusable geo-spatial projection utility methods.
+ *
+ * @lucene.experimental
+ */
+public class XGeoProjectionUtils {
+    // WGS84 earth-ellipsoid major (a) minor (b) radius, (f) flattening and eccentricity (e)
+    static final double SEMIMAJOR_AXIS = 6_378_137; // [m]
+    static final double FLATTENING = 1.0/298.257223563;
+    static final double SEMIMINOR_AXIS = SEMIMAJOR_AXIS * (1.0 - FLATTENING); //6_356_752.31420; // [m]
+    static final double ECCENTRICITY = StrictMath.sqrt((2.0 - FLATTENING) * FLATTENING);
+    static final double PI_OVER_2 = StrictMath.PI / 2.0D;
+    static final double SEMIMAJOR_AXIS2 = SEMIMAJOR_AXIS * SEMIMAJOR_AXIS;
+    static final double SEMIMINOR_AXIS2 = SEMIMINOR_AXIS * SEMIMINOR_AXIS;
+
+    /**
+     * Converts from geocentric earth-centered earth-fixed to geodesic lat/lon/alt
+     * @param x Cartesian x coordinate
+     * @param y Cartesian y coordinate
+     * @param z Cartesian z coordinate
+     * @param lla 0: longitude 1: latitude: 2: altitude
+     * @return double array as 0: longitude 1: latitude 2: altitude
+     */
+    public static final double[] ecfToLLA(final double x, final double y, final double z, double[] lla) {
+        boolean atPole = false;
+        final double ad_c = 1.0026000D;
+        final double e2 = (SEMIMAJOR_AXIS2 - SEMIMINOR_AXIS2)/(SEMIMAJOR_AXIS2);
+        final double ep2 = (SEMIMAJOR_AXIS2 - SEMIMINOR_AXIS2)/(SEMIMINOR_AXIS2);
+        final double cos67P5 = 0.38268343236508977D;
+
+        if (lla == null) {
+            lla = new double[3];
+        }
+
+        if (x != 0.0) {
+            lla[0] = StrictMath.atan2(y,x);
+        } else {
+            if (y > 0) {
+                lla[0] = PI_OVER_2;
+            } else if (y < 0) {
+                lla[0] = -PI_OVER_2;
+            } else {
+                atPole = true;
+                lla[0] = 0.0D;
+                if (z > 0.0) {
+                    lla[1] = PI_OVER_2;
+                } else if (z < 0.0) {
+                    lla[1] = -PI_OVER_2;
+                } else {
+                    lla[1] = PI_OVER_2;
+                    lla[2] = -SEMIMINOR_AXIS;
+                    return lla;
+                }
+            }
+        }
+
+        final double w2 = x*x + y*y;
+        final double w = StrictMath.sqrt(w2);
+        final double t0 = z * ad_c;
+        final double s0 = StrictMath.sqrt(t0 * t0 + w2);
+        final double sinB0 = t0 / s0;
+        final double cosB0 = w / s0;
+        final double sin3B0 = sinB0 * sinB0 * sinB0;
+        final double t1 = z + SEMIMINOR_AXIS * ep2 * sin3B0;
+        final double sum = w - SEMIMAJOR_AXIS * e2 * cosB0 * cosB0 * cosB0;
+        final double s1 = StrictMath.sqrt(t1 * t1 + sum * sum);
+        final double sinP1 = t1 / s1;
+        final double cosP1 = sum / s1;
+        final double rn = SEMIMAJOR_AXIS / StrictMath.sqrt(1.0D - e2 * sinP1 * sinP1);
+
+        if (cosP1 >= cos67P5) {
+            lla[2] = w / cosP1 - rn;
+        } else if (cosP1 <= -cos67P5) {
+            lla[2] = w / -cosP1 - rn;
+        } else {
+            lla[2] = z / sinP1 + rn * (e2 - 1.0);
+        }
+        if (!atPole) {
+            lla[1] = StrictMath.atan(sinP1/cosP1);
+        }
+        lla[0] = StrictMath.toDegrees(lla[0]);
+        lla[1] = StrictMath.toDegrees(lla[1]);
+
+        return lla;
+    }
+
+    /**
+     * Converts from geodesic lon lat alt to geocentric earth-centered earth-fixed
+     * @param lon geodesic longitude
+     * @param lat geodesic latitude
+     * @param alt geodesic altitude
+     * @param ecf reusable earth-centered earth-fixed result
+     * @return either a new ecef array or the reusable ecf parameter
+     */
+    public static final double[] llaToECF(double lon, double lat, double alt, double[] ecf) {
+        lon = StrictMath.toRadians(lon);
+        lat = StrictMath.toRadians(lat);
+
+        final double sl = StrictMath.sin(lat);
+        final double s2 = sl*sl;
+        final double cl = StrictMath.cos(lat);
+        final double ge2 = (SEMIMAJOR_AXIS2 - SEMIMINOR_AXIS2)/(SEMIMAJOR_AXIS2);
+
+        if (ecf == null) {
+            ecf = new double[3];
+        }
+
+        if (lat < -PI_OVER_2 && lat > -1.001D * PI_OVER_2) {
+            lat = -PI_OVER_2;
+        } else if (lat > PI_OVER_2 && lat < 1.001D * PI_OVER_2) {
+            lat = PI_OVER_2;
+        }
+        assert (lat >= -PI_OVER_2) || (lat <= PI_OVER_2);
+
+        if (lon > StrictMath.PI) {
+            lon -= (2*StrictMath.PI);
+        }
+
+        final double rn = SEMIMAJOR_AXIS / StrictMath.sqrt(1.0D - ge2 * s2);
+        ecf[0] = (rn+alt) * cl * StrictMath.cos(lon);
+        ecf[1] = (rn+alt) * cl * StrictMath.sin(lon);
+        ecf[2] = ((rn*(1.0-ge2))+alt)*sl;
+
+        return ecf;
+    }
+
+    /**
+     * Converts from lat lon alt (in degrees) to East North Up right-hand coordinate system
+     * @param lon longitude in degrees
+     * @param lat latitude in degrees
+     * @param alt altitude in meters
+     * @param centerLon reference point longitude in degrees
+     * @param centerLat reference point latitude in degrees
+     * @param centerAlt reference point altitude in meters
+     * @param enu result east, north, up coordinate
+     * @return east, north, up coordinate
+     */
+    public static double[] llaToENU(final double lon, final double lat, final double alt, double centerLon,
+                                    double centerLat, final double centerAlt, double[] enu) {
+        if (enu == null) {
+            enu = new double[3];
+        }
+
+        // convert point to ecf coordinates
+        final double[] ecf = llaToECF(lon, lat, alt, null);
+
+        // convert from ecf to enu
+        return ecfToENU(ecf[0], ecf[1], ecf[2], centerLon, centerLat, centerAlt, enu);
+    }
+
+    /**
+     * Converts from East North Up right-hand rule to lat lon alt in degrees
+     * @param x easting (in meters)
+     * @param y northing (in meters)
+     * @param z up (in meters)
+     * @param centerLon reference point longitude (in degrees)
+     * @param centerLat reference point latitude (in degrees)
+     * @param centerAlt reference point altitude (in meters)
+     * @param lla resulting lat, lon, alt point (in degrees)
+     * @return lat, lon, alt point (in degrees)
+     */
+    public static double[] enuToLLA(final double x, final double y, final double z, final double centerLon,
+                                    final double centerLat, final double centerAlt, double[] lla) {
+        // convert enuToECF
+        if (lla == null) {
+            lla = new double[3];
+        }
+
+        // convert enuToECF, storing intermediate result in lla
+        lla = enuToECF(x, y, z, centerLon, centerLat, centerAlt, lla);
+
+        // convert ecf to LLA
+        return ecfToLLA(lla[0], lla[1], lla[2], lla);
+    }
+
+    /**
+     * Convert from Earth-Centered-Fixed to Easting, Northing, Up Right Hand System
+     * @param x ECF X coordinate (in meters)
+     * @param y ECF Y coordinate (in meters)
+     * @param z ECF Z coordinate (in meters)
+     * @param centerLon ENU origin longitude (in degrees)
+     * @param centerLat ENU origin latitude (in degrees)
+     * @param centerAlt ENU altitude (in meters)
+     * @param enu reusable enu result
+     * @return Easting, Northing, Up coordinate
+     */
+    public static double[] ecfToENU(double x, double y, double z, final double centerLon,
+                                    final double centerLat, final double centerAlt, double[] enu) {
+        if (enu == null) {
+            enu = new double[3];
+        }
+
+        // create rotation matrix and rotate to enu orientation
+        final double[][] phi = createPhiTransform(centerLon, centerLat, null);
+
+        // convert origin to ENU
+        final double[] originECF = llaToECF(centerLon, centerLat, centerAlt, null);
+        final double[] originENU = new double[3];
+        originENU[0] = ((phi[0][0] * originECF[0]) + (phi[0][1] * originECF[1]) + (phi[0][2] * originECF[2]));
+        originENU[1] = ((phi[1][0] * originECF[0]) + (phi[1][1] * originECF[1]) + (phi[1][2] * originECF[2]));
+        originENU[2] = ((phi[2][0] * originECF[0]) + (phi[2][1] * originECF[1]) + (phi[2][2] * originECF[2]));
+
+        // rotate then translate
+        enu[0] = ((phi[0][0] * x) + (phi[0][1] * y) + (phi[0][2] * z)) - originENU[0];
+        enu[1] = ((phi[1][0] * x) + (phi[1][1] * y) + (phi[1][2] * z)) - originENU[1];
+        enu[2] = ((phi[2][0] * x) + (phi[2][1] * y) + (phi[2][2] * z)) - originENU[2];
+
+        return enu;
+    }
+
+    /**
+     * Convert from Easting, Northing, Up Right-Handed system to Earth Centered Fixed system
+     * @param x ENU x coordinate (in meters)
+     * @param y ENU y coordinate (in meters)
+     * @param z ENU z coordinate (in meters)
+     * @param centerLon ENU origin longitude (in degrees)
+     * @param centerLat ENU origin latitude (in degrees)
+     * @param centerAlt ENU origin altitude (in meters)
+     * @param ecf reusable ecf result
+     * @return ecf result coordinate
+     */
+    public static double[] enuToECF(final double x, final double y, final double z, double centerLon,
+                                    double centerLat, final double centerAlt, double[] ecf) {
+        if (ecf == null) {
+            ecf = new double[3];
+        }
+
+        double[][] phi = createTransposedPhiTransform(centerLon, centerLat, null);
+        double[] ecfOrigin = llaToECF(centerLon, centerLat, centerAlt, null);
+
+        // rotate and translate
+        ecf[0] = (phi[0][0]*x + phi[0][1]*y + phi[0][2]*z) + ecfOrigin[0];
+        ecf[1] = (phi[1][0]*x + phi[1][1]*y + phi[1][2]*z) + ecfOrigin[1];
+        ecf[2] = (phi[2][0]*x + phi[2][1]*y + phi[2][2]*z) + ecfOrigin[2];
+
+        return ecf;
+    }
+
+    /**
+     * Create the rotation matrix for converting Earth Centered Fixed to Easting Northing Up
+     * @param originLon ENU origin longitude (in degrees)
+     * @param originLat ENU origin latitude (in degrees)
+     * @param phiMatrix reusable phi matrix result
+     * @return phi rotation matrix
+     */
+    private static double[][] createPhiTransform(double originLon, double originLat, double[][] phiMatrix) {
+
+        if (phiMatrix == null) {
+            phiMatrix = new double[3][3];
+        }
+
+        originLon = StrictMath.toRadians(originLon);
+        originLat = StrictMath.toRadians(originLat);
+
+        final double sLon = StrictMath.sin(originLon);
+        final double cLon = StrictMath.cos(originLon);
+        final double sLat = StrictMath.sin(originLat);
+        final double cLat = StrictMath.cos(originLat);
+
+        phiMatrix[0][0] = -sLon;
+        phiMatrix[0][1] = cLon;
+        phiMatrix[0][2] = 0.0D;
+        phiMatrix[1][0] = -sLat * cLon;
+        phiMatrix[1][1] = -sLat * sLon;
+        phiMatrix[1][2] = cLat;
+        phiMatrix[2][0] = cLat * cLon;
+        phiMatrix[2][1] = cLat * sLon;
+        phiMatrix[2][2] = sLat;
+
+        return phiMatrix;
+    }
+
+    /**
+     * Create the transposed rotation matrix for converting Easting Northing Up coordinates to Earth Centered Fixed
+     * @param originLon ENU origin longitude (in degrees)
+     * @param originLat ENU origin latitude (in degrees)
+     * @param phiMatrix reusable phi rotation matrix result
+     * @return transposed phi rotation matrix
+     */
+    private static double[][] createTransposedPhiTransform(double originLon, double originLat, double[][] phiMatrix) {
+
+        if (phiMatrix == null) {
+            phiMatrix = new double[3][3];
+        }
+
+        originLon = StrictMath.toRadians(originLon);
+        originLat = StrictMath.toRadians(originLat);
+
+        final double sLat = StrictMath.sin(originLat);
+        final double cLat = StrictMath.cos(originLat);
+        final double sLon = StrictMath.sin(originLon);
+        final double cLon = StrictMath.cos(originLon);
+
+        phiMatrix[0][0] = -sLon;
+        phiMatrix[1][0] = cLon;
+        phiMatrix[2][0] = 0.0D;
+        phiMatrix[0][1] = -sLat * cLon;
+        phiMatrix[1][1] = -sLat * sLon;
+        phiMatrix[2][1] = cLat;
+        phiMatrix[0][2] = cLat * cLon;
+        phiMatrix[1][2] = cLat * sLon;
+        phiMatrix[2][2] = sLat;
+
+        return phiMatrix;
+    }
+
+    /**
+     * Finds a point along a bearing from a given lon,lat geolocation using vincenty's distance formula
+     *
+     * @param lon origin longitude in degrees
+     * @param lat origin latitude in degrees
+     * @param bearing azimuthal bearing in degrees
+     * @param dist distance in meters
+     * @param pt resulting point
+     * @return the point along a bearing at a given distance in meters
+     */
+    public static final double[] pointFromLonLatBearing(double lon, double lat, double bearing, double dist, double[] pt) {
+
+        if (pt == null) {
+            pt = new double[2];
+        }
+
+        final double alpha1 = StrictMath.toRadians(bearing);
+        final double cosA1 = StrictMath.cos(alpha1);
+        final double sinA1 = StrictMath.sin(alpha1);
+        final double tanU1 = (1-FLATTENING) * StrictMath.tan(StrictMath.toRadians(lat));
+        final double cosU1 = 1 / StrictMath.sqrt((1+tanU1*tanU1));
+        final double sinU1 = tanU1*cosU1;
+        final double sig1 = StrictMath.atan2(tanU1, cosA1);
+        final double sinAlpha = cosU1 * sinA1;
+        final double cosSqAlpha = 1 - sinAlpha*sinAlpha;
+        final double uSq = cosSqAlpha * (SEMIMAJOR_AXIS2 - SEMIMINOR_AXIS2) / SEMIMINOR_AXIS2;
+        final double A = 1 + uSq/16384D*(4096D + uSq * (-768D + uSq * (320D - 175D*uSq)));
+        final double B = uSq/1024D * (256D + uSq * (-128D + uSq * (74D - 47D * uSq)));
+
+        double sigma = dist / (SEMIMINOR_AXIS*A);
+        double sigmaP;
+        double sinSigma, cosSigma, cos2SigmaM, deltaSigma;
+
+        do {
+            cos2SigmaM = StrictMath.cos(2*sig1 + sigma);
+            sinSigma = StrictMath.sin(sigma);
+            cosSigma = StrictMath.cos(sigma);
+
+            deltaSigma = B * sinSigma * (cos2SigmaM + (B/4D) * (cosSigma*(-1+2*cos2SigmaM*cos2SigmaM)-
+                    (B/6) * cos2SigmaM*(-3+4*sinSigma*sinSigma)*(-3+4*cos2SigmaM*cos2SigmaM)));
+            sigmaP = sigma;
+            sigma = dist / (SEMIMINOR_AXIS*A) + deltaSigma;
+        } while (StrictMath.abs(sigma-sigmaP) > 1E-12);
+
+        final double tmp = sinU1*sinSigma - cosU1*cosSigma*cosA1;
+        final double lat2 = StrictMath.atan2(sinU1*cosSigma + cosU1*sinSigma*cosA1,
+                (1-FLATTENING) * StrictMath.sqrt(sinAlpha*sinAlpha + tmp*tmp));
+        final double lambda = StrictMath.atan2(sinSigma*sinA1, cosU1*cosSigma - sinU1*sinSigma*cosA1);
+        final double c = FLATTENING/16 * cosSqAlpha * (4 + FLATTENING * (4 - 3 * cosSqAlpha));
+
+        final double lam = lambda - (1-c) * FLATTENING * sinAlpha *
+                (sigma + c * sinSigma * (cos2SigmaM + c * cosSigma * (-1 + 2* cos2SigmaM*cos2SigmaM)));
+        pt[0] = lon + StrictMath.toDegrees(lam);
+        pt[1] = StrictMath.toDegrees(lat2);
+
+        return pt;
+    }
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/apache/lucene/util/XGeoUtils.java b/core/src/main/java/org/apache/lucene/util/XGeoUtils.java
new file mode 100644
index 0000000..7e97306
--- /dev/null
+++ b/core/src/main/java/org/apache/lucene/util/XGeoUtils.java
@@ -0,0 +1,429 @@
+package org.apache.lucene.util;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.ArrayList;
+
+/**
+ * Basic reusable geo-spatial utility methods
+ *
+ * @lucene.experimental
+ */
+public final class XGeoUtils {
+    private static final short MIN_LON = -180;
+    private static final short MIN_LAT = -90;
+    public static final short BITS = 31;
+    private static final double LON_SCALE = (0x1L<<BITS)/360.0D;
+    private static final double LAT_SCALE = (0x1L<<BITS)/180.0D;
+    public static final double TOLERANCE = 1E-6;
+
+    /** Minimum longitude value. */
+    public static final double MIN_LON_INCL = -180.0D;
+
+    /** Maximum longitude value. */
+    public static final double MAX_LON_INCL = 180.0D;
+
+    /** Minimum latitude value. */
+    public static final double MIN_LAT_INCL = -90.0D;
+
+    /** Maximum latitude value. */
+    public static final double MAX_LAT_INCL = 90.0D;
+
+    // magic numbers for bit interleaving
+    private static final long MAGIC[] = {
+            0x5555555555555555L, 0x3333333333333333L,
+            0x0F0F0F0F0F0F0F0FL, 0x00FF00FF00FF00FFL,
+            0x0000FFFF0000FFFFL, 0x00000000FFFFFFFFL,
+            0xAAAAAAAAAAAAAAAAL
+    };
+    // shift values for bit interleaving
+    private static final short SHIFT[] = {1, 2, 4, 8, 16};
+
+    public static double LOG2 = StrictMath.log(2);
+
+    // No instance:
+    private XGeoUtils() {
+    }
+
+    public static Long mortonHash(final double lon, final double lat) {
+        return interleave(scaleLon(lon), scaleLat(lat));
+    }
+
+    public static double mortonUnhashLon(final long hash) {
+        return unscaleLon(deinterleave(hash));
+    }
+
+    public static double mortonUnhashLat(final long hash) {
+        return unscaleLat(deinterleave(hash >>> 1));
+    }
+
+    private static long scaleLon(final double val) {
+        return (long) ((val-MIN_LON) * LON_SCALE);
+    }
+
+    private static long scaleLat(final double val) {
+        return (long) ((val-MIN_LAT) * LAT_SCALE);
+    }
+
+    private static double unscaleLon(final long val) {
+        return (val / LON_SCALE) + MIN_LON;
+    }
+
+    private static double unscaleLat(final long val) {
+        return (val / LAT_SCALE) + MIN_LAT;
+    }
+
+    /**
+     * Interleaves the first 32 bits of each long value
+     *
+     * Adapted from: http://graphics.stanford.edu/~seander/bithacks.html#InterleaveBMN
+     */
+    public static long interleave(long v1, long v2) {
+        v1 = (v1 | (v1 << SHIFT[4])) & MAGIC[4];
+        v1 = (v1 | (v1 << SHIFT[3])) & MAGIC[3];
+        v1 = (v1 | (v1 << SHIFT[2])) & MAGIC[2];
+        v1 = (v1 | (v1 << SHIFT[1])) & MAGIC[1];
+        v1 = (v1 | (v1 << SHIFT[0])) & MAGIC[0];
+        v2 = (v2 | (v2 << SHIFT[4])) & MAGIC[4];
+        v2 = (v2 | (v2 << SHIFT[3])) & MAGIC[3];
+        v2 = (v2 | (v2 << SHIFT[2])) & MAGIC[2];
+        v2 = (v2 | (v2 << SHIFT[1])) & MAGIC[1];
+        v2 = (v2 | (v2 << SHIFT[0])) & MAGIC[0];
+
+        return (v2<<1) | v1;
+    }
+
+    /**
+     * Deinterleaves long value back to two concatenated 32bit values
+     */
+    public static long deinterleave(long b) {
+        b &= MAGIC[0];
+        b = (b ^ (b >>> SHIFT[0])) & MAGIC[1];
+        b = (b ^ (b >>> SHIFT[1])) & MAGIC[2];
+        b = (b ^ (b >>> SHIFT[2])) & MAGIC[3];
+        b = (b ^ (b >>> SHIFT[3])) & MAGIC[4];
+        b = (b ^ (b >>> SHIFT[4])) & MAGIC[5];
+        return b;
+    }
+
+    public static double compare(final double v1, final double v2) {
+        final double compare = v1-v2;
+        return Math.abs(compare) <= TOLERANCE ? 0 : compare;
+    }
+
+    /**
+     * Puts longitude in range of -180 to +180.
+     */
+    public static double normalizeLon(double lon_deg) {
+        if (lon_deg >= -180 && lon_deg <= 180) {
+            return lon_deg; //common case, and avoids slight double precision shifting
+        }
+        double off = (lon_deg + 180) % 360;
+        if (off < 0) {
+            return 180 + off;
+        } else if (off == 0 && lon_deg > 0) {
+            return 180;
+        } else {
+            return -180 + off;
+        }
+    }
+
+    /**
+     * Puts latitude in range of -90 to 90.
+     */
+    public static double normalizeLat(double lat_deg) {
+        if (lat_deg >= -90 && lat_deg <= 90) {
+            return lat_deg; //common case, and avoids slight double precision shifting
+        }
+        double off = Math.abs((lat_deg + 90) % 360);
+        return (off <= 180 ? off : 360-off) - 90;
+    }
+
+    public static final boolean bboxContains(final double lon, final double lat, final double minLon,
+                                             final double minLat, final double maxLon, final double maxLat) {
+        return (compare(lon, minLon) >= 0 && compare(lon, maxLon) <= 0
+                && compare(lat, minLat) >= 0 && compare(lat, maxLat) <= 0);
+    }
+
+    /**
+     * simple even-odd point in polygon computation
+     *    1.  Determine if point is contained in the longitudinal range
+     *    2.  Determine whether point crosses the edge by computing the latitudinal delta
+     *        between the end-point of a parallel vector (originating at the point) and the
+     *        y-component of the edge sink
+     *
+     * NOTE: Requires polygon point (x,y) order either clockwise or counter-clockwise
+     */
+    public static boolean pointInPolygon(double[] x, double[] y, double lat, double lon) {
+        assert x.length == y.length;
+        boolean inPoly = false;
+        /**
+         * Note: This is using a euclidean coordinate system which could result in
+         * upwards of 110KM error at the equator.
+         * TODO convert coordinates to cylindrical projection (e.g. mercator)
+         */
+        for (int i = 1; i < x.length; i++) {
+            if (x[i] < lon && x[i-1] >= lon || x[i-1] < lon && x[i] >= lon) {
+                if (y[i] + (lon - x[i]) / (x[i-1] - x[i]) * (y[i-1] - y[i]) < lat) {
+                    inPoly = !inPoly;
+                }
+            }
+        }
+        return inPoly;
+    }
+
+    public static String geoTermToString(long term) {
+        StringBuilder s = new StringBuilder(64);
+        final int numberOfLeadingZeros = Long.numberOfLeadingZeros(term);
+        for (int i = 0; i < numberOfLeadingZeros; i++) {
+            s.append('0');
+        }
+        if (term != 0) {
+            s.append(Long.toBinaryString(term));
+        }
+        return s.toString();
+    }
+
+
+    public static boolean rectDisjoint(final double aMinX, final double aMinY, final double aMaxX, final double aMaxY,
+                                       final double bMinX, final double bMinY, final double bMaxX, final double bMaxY) {
+        return (aMaxX < bMinX || aMinX > bMaxX || aMaxY < bMinY || aMinY > bMaxY);
+    }
+
+    /**
+     * Computes whether a rectangle is wholly within another rectangle (shared boundaries allowed)
+     */
+    public static boolean rectWithin(final double aMinX, final double aMinY, final double aMaxX, final double aMaxY,
+                                     final double bMinX, final double bMinY, final double bMaxX, final double bMaxY) {
+        return !(aMinX < bMinX || aMinY < bMinY || aMaxX > bMaxX || aMaxY > bMaxY);
+    }
+
+    public static boolean rectCrosses(final double aMinX, final double aMinY, final double aMaxX, final double aMaxY,
+                                      final double bMinX, final double bMinY, final double bMaxX, final double bMaxY) {
+        return !(rectDisjoint(aMinX, aMinY, aMaxX, aMaxY, bMinX, bMinY, bMaxX, bMaxY) ||
+                rectWithin(aMinX, aMinY, aMaxX, aMaxY, bMinX, bMinY, bMaxX, bMaxY));
+    }
+
+    /**
+     * Computes whether rectangle a contains rectangle b (touching allowed)
+     */
+    public static boolean rectContains(final double aMinX, final double aMinY, final double aMaxX, final double aMaxY,
+                                       final double bMinX, final double bMinY, final double bMaxX, final double bMaxY) {
+        return !(bMinX < aMinX || bMinY < aMinY || bMaxX > aMaxX || bMaxY > aMaxY);
+    }
+
+    /**
+     * Computes whether a rectangle intersects another rectangle (crosses, within, touching, etc)
+     */
+    public static boolean rectIntersects(final double aMinX, final double aMinY, final double aMaxX, final double aMaxY,
+                                         final double bMinX, final double bMinY, final double bMaxX, final double bMaxY) {
+        return !((aMaxX < bMinX || aMinX > bMaxX || aMaxY < bMinY || aMinY > bMaxY) );
+    }
+
+    /**
+     * Computes whether a rectangle crosses a shape. (touching not allowed)
+     */
+    public static boolean rectCrossesPoly(final double rMinX, final double rMinY, final double rMaxX,
+                                          final double rMaxY, final double[] shapeX, final double[] shapeY,
+                                          final double sMinX, final double sMinY, final double sMaxX,
+                                          final double sMaxY) {
+        // short-circuit: if the bounding boxes are disjoint then the shape does not cross
+        if (rectDisjoint(rMinX, rMinY, rMaxX, rMaxY, sMinX, sMinY, sMaxX, sMaxY)) {
+            return false;
+        }
+
+        final double[][] bbox = new double[][] { {rMinX, rMinY}, {rMaxX, rMinY}, {rMaxX, rMaxY}, {rMinX, rMaxY}, {rMinX, rMinY} };
+        final int polyLength = shapeX.length-1;
+        double d, s, t, a1, b1, c1, a2, b2, c2;
+        double x00, y00, x01, y01, x10, y10, x11, y11;
+
+        // computes the intersection point between each bbox edge and the polygon edge
+        for (short b=0; b<4; ++b) {
+            a1 = bbox[b+1][1]-bbox[b][1];
+            b1 = bbox[b][0]-bbox[b+1][0];
+            c1 = a1*bbox[b+1][0] + b1*bbox[b+1][1];
+            for (int p=0; p<polyLength; ++p) {
+                a2 = shapeY[p+1]-shapeY[p];
+                b2 = shapeX[p]-shapeX[p+1];
+                // compute determinant
+                d = a1*b2 - a2*b1;
+                if (d != 0) {
+                    // lines are not parallel, check intersecting points
+                    c2 = a2*shapeX[p+1] + b2*shapeY[p+1];
+                    s = (1/d)*(b2*c1 - b1*c2);
+                    t = (1/d)*(a1*c2 - a2*c1);
+                    x00 = StrictMath.min(bbox[b][0], bbox[b+1][0]) - TOLERANCE;
+                    x01 = StrictMath.max(bbox[b][0], bbox[b+1][0]) + TOLERANCE;
+                    y00 = StrictMath.min(bbox[b][1], bbox[b+1][1]) - TOLERANCE;
+                    y01 = StrictMath.max(bbox[b][1], bbox[b+1][1]) + TOLERANCE;
+                    x10 = StrictMath.min(shapeX[p], shapeX[p+1]) - TOLERANCE;
+                    x11 = StrictMath.max(shapeX[p], shapeX[p+1]) + TOLERANCE;
+                    y10 = StrictMath.min(shapeY[p], shapeY[p+1]) - TOLERANCE;
+                    y11 = StrictMath.max(shapeY[p], shapeY[p+1]) + TOLERANCE;
+                    // check whether the intersection point is touching one of the line segments
+                    boolean touching = ((x00 == s && y00 == t) || (x01 == s && y01 == t))
+                            || ((x10 == s && y10 == t) || (x11 == s && y11 == t));
+                    // if line segments are not touching and the intersection point is within the range of either segment
+                    if (!(touching || x00 > s || x01 < s || y00 > t || y01 < t || x10 > s || x11 < s || y10 > t || y11 < t)) {
+                        return true;
+                    }
+                }
+            } // for each poly edge
+        } // for each bbox edge
+        return false;
+    }
+
+    /**
+     * Converts a given circle (defined as a point/radius) to an approximated line-segment polygon
+     *
+     * @param lon longitudinal center of circle (in degrees)
+     * @param lat latitudinal center of circle (in degrees)
+     * @param radius distance radius of circle (in meters)
+     * @return a list of lon/lat points representing the circle
+     */
+    @SuppressWarnings({"unchecked","rawtypes"})
+    public static ArrayList<double[]> circleToPoly(final double lon, final double lat, final double radius) {
+        double angle;
+        // a little under-sampling (to limit the number of polygonal points): using archimedes estimation of pi
+        final int sides = 25;
+        ArrayList<double[]> geometry = new ArrayList();
+        double[] lons = new double[sides];
+        double[] lats = new double[sides];
+
+        double[] pt = new double[2];
+        final int sidesLen = sides-1;
+        for (int i=0; i<sidesLen; ++i) {
+            angle = (i*360/sides);
+            pt = XGeoProjectionUtils.pointFromLonLatBearing(lon, lat, angle, radius, pt);
+            lons[i] = pt[0];
+            lats[i] = pt[1];
+        }
+        // close the poly
+        lons[sidesLen] = lons[0];
+        lats[sidesLen] = lats[0];
+        geometry.add(lons);
+        geometry.add(lats);
+
+        return geometry;
+    }
+
+    /**
+     * Computes whether a rectangle is within a given polygon (shared boundaries allowed)
+     */
+    public static boolean rectWithinPoly(final double rMinX, final double rMinY, final double rMaxX, final double rMaxY,
+                                         final double[] shapeX, final double[] shapeY, final double sMinX,
+                                         final double sMinY, final double sMaxX, final double sMaxY) {
+        // check if rectangle crosses poly (to handle concave/pacman polys), then check that all 4 corners
+        // are contained
+        return !(rectCrossesPoly(rMinX, rMinY, rMaxX, rMaxY, shapeX, shapeY, sMinX, sMinY, sMaxX, sMaxY) ||
+                !pointInPolygon(shapeX, shapeY, rMinY, rMinX) || !pointInPolygon(shapeX, shapeY, rMinY, rMaxX) ||
+                !pointInPolygon(shapeX, shapeY, rMaxY, rMaxX) || !pointInPolygon(shapeX, shapeY, rMaxY, rMinX));
+    }
+
+    private static boolean rectAnyCornersOutsideCircle(final double rMinX, final double rMinY, final double rMaxX, final double rMaxY,
+                                                       final double centerLon, final double centerLat, final double radius) {
+        return (SloppyMath.haversin(centerLat, centerLon, rMinY, rMinX)*1000.0 > radius
+                || SloppyMath.haversin(centerLat, centerLon, rMaxY, rMinX)*1000.0 > radius
+                || SloppyMath.haversin(centerLat, centerLon, rMaxY, rMaxX)*1000.0 > radius
+                || SloppyMath.haversin(centerLat, centerLon, rMinY, rMaxX)*1000.0 > radius);
+    }
+
+    private static boolean rectAnyCornersInCircle(final double rMinX, final double rMinY, final double rMaxX, final double rMaxY,
+                                                  final double centerLon, final double centerLat, final double radius) {
+        return (SloppyMath.haversin(centerLat, centerLon, rMinY, rMinX)*1000.0 <= radius
+                || SloppyMath.haversin(centerLat, centerLon, rMaxY, rMinX)*1000.0 <= radius
+                || SloppyMath.haversin(centerLat, centerLon, rMaxY, rMaxX)*1000.0 <= radius
+                || SloppyMath.haversin(centerLat, centerLon, rMinY, rMaxX)*1000.0 <= radius);
+    }
+
+    public static boolean rectWithinCircle(final double rMinX, final double rMinY, final double rMaxX, final double rMaxY,
+                                           final double centerLon, final double centerLat, final double radius) {
+        return !(rectAnyCornersOutsideCircle(rMinX, rMinY, rMaxX, rMaxY, centerLon, centerLat, radius));
+    }
+
+    /**
+     * Computes whether a rectangle crosses a circle
+     */
+    public static boolean rectCrossesCircle(final double rMinX, final double rMinY, final double rMaxX, final double rMaxY,
+                                            final double centerLon, final double centerLat, final double radius) {
+        return rectAnyCornersInCircle(rMinX, rMinY, rMaxX, rMaxY, centerLon, centerLat, radius)
+                || lineCrossesSphere(rMinX, rMinY, 0, rMaxX, rMinY, 0, centerLon, centerLat, 0, radius)
+                || lineCrossesSphere(rMaxX, rMinY, 0, rMaxX, rMaxY, 0, centerLon, centerLat, 0, radius)
+                || lineCrossesSphere(rMaxX, rMaxY, 0, rMinX, rMaxY, 0, centerLon, centerLat, 0, radius)
+                || lineCrossesSphere(rMinX, rMaxY, 0, rMinX, rMinY, 0, centerLon, centerLat, 0, radius);
+    }
+
+    /**
+     * Computes whether or a 3dimensional line segment intersects or crosses a sphere
+     *
+     * @param lon1 longitudinal location of the line segment start point (in degrees)
+     * @param lat1 latitudinal location of the line segment start point (in degrees)
+     * @param alt1 altitude of the line segment start point (in degrees)
+     * @param lon2 longitudinal location of the line segment end point (in degrees)
+     * @param lat2 latitudinal location of the line segment end point (in degrees)
+     * @param alt2 altitude of the line segment end point (in degrees)
+     * @param centerLon longitudinal location of center search point (in degrees)
+     * @param centerLat latitudinal location of center search point (in degrees)
+     * @param centerAlt altitude of the center point (in meters)
+     * @param radius search sphere radius (in meters)
+     * @return whether the provided line segment is a secant of the
+     */
+    private static boolean lineCrossesSphere(double lon1, double lat1, double alt1, double lon2,
+                                             double lat2, double alt2, double centerLon, double centerLat,
+                                             double centerAlt, double radius) {
+        // convert to cartesian 3d (in meters)
+        double[] ecf1 = XGeoProjectionUtils.llaToECF(lon1, lat1, alt1, null);
+        double[] ecf2 = XGeoProjectionUtils.llaToECF(lon2, lat2, alt2, null);
+        double[] cntr = XGeoProjectionUtils.llaToECF(centerLon, centerLat, centerAlt, null);
+
+        final double dX = ecf2[0] - ecf1[0];
+        final double dY = ecf2[1] - ecf1[1];
+        final double dZ = ecf2[2] - ecf1[2];
+        final double fX = ecf1[0] - cntr[0];
+        final double fY = ecf1[1] - cntr[1];
+        final double fZ = ecf1[2] - cntr[2];
+
+        final double a = dX*dX + dY*dY + dZ*dZ;
+        final double b = 2 * (fX*dX + fY*dY + fZ*dZ);
+        final double c = (fX*fX + fY*fY + fZ*fZ) - (radius*radius);
+
+        double discrim = (b*b)-(4*a*c);
+        if (discrim < 0) {
+            return false;
+        }
+
+        discrim = StrictMath.sqrt(discrim);
+        final double a2 = 2*a;
+        final double t1 = (-b - discrim)/a2;
+        final double t2 = (-b + discrim)/a2;
+
+        if ( (t1 < 0 || t1 > 1) ) {
+            return !(t2 < 0 || t2 > 1);
+        }
+
+        return true;
+    }
+
+    public static boolean isValidLat(double lat) {
+        return Double.isNaN(lat) == false && lat >= MIN_LAT_INCL && lat <= MAX_LAT_INCL;
+    }
+
+    public static boolean isValidLon(double lon) {
+        return Double.isNaN(lon) == false && lon >= MIN_LON_INCL && lon <= MAX_LON_INCL;
+    }
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/ElasticsearchException.java b/core/src/main/java/org/elasticsearch/ElasticsearchException.java
index 81521c6..a9b0ba5 100644
--- a/core/src/main/java/org/elasticsearch/ElasticsearchException.java
+++ b/core/src/main/java/org/elasticsearch/ElasticsearchException.java
@@ -26,7 +26,6 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.logging.support.LoggerMessageFormat;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.discovery.Discovery;
 import org.elasticsearch.index.Index;
 import org.elasticsearch.index.IndexNotFoundException;
 import org.elasticsearch.index.shard.ShardId;
@@ -596,8 +595,7 @@ public class ElasticsearchException extends RuntimeException implements ToXConte
                 ResourceNotFoundException.class,
                 IndexNotFoundException.class,
                 ShardNotFoundException.class,
-                NotSerializableExceptionWrapper.class,
-                Discovery.FailedToCommitClusterStateException.class
+                NotSerializableExceptionWrapper.class
         };
         Map<String, Constructor<? extends ElasticsearchException>> mapping = new HashMap<>(exceptions.length);
         for (Class<? extends ElasticsearchException> e : exceptions) {
diff --git a/core/src/main/java/org/elasticsearch/action/ActionRequest.java b/core/src/main/java/org/elasticsearch/action/ActionRequest.java
index 3cae644..24cf680 100644
--- a/core/src/main/java/org/elasticsearch/action/ActionRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/ActionRequest.java
@@ -30,7 +30,7 @@ import java.io.IOException;
  */
 public abstract class ActionRequest<T extends ActionRequest> extends TransportRequest {
 
-    protected ActionRequest() {
+    public ActionRequest() {
         super();
     }
 
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthRequest.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthRequest.java
index debbf6a..32114f6 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthRequest.java
@@ -47,7 +47,7 @@ public class ClusterHealthRequest extends MasterNodeReadRequest<ClusterHealthReq
     private String waitForNodes = "";
     private Priority waitForEvents = null;
 
-    ClusterHealthRequest() {
+    public ClusterHealthRequest() {
     }
 
     public ClusterHealthRequest(String... indices) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java
index 2d99769..6f9180e 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java
@@ -124,7 +124,7 @@ public class TransportClusterHealthAction extends TransportMasterNodeReadAction<
         if (request.waitForNodes().isEmpty()) {
             waitFor--;
         }
-        if (request.indices().length == 0) { // check that they actually exists in the meta data
+        if (request.indices() == null || request.indices().length == 0) { // check that they actually exists in the meta data
             waitFor--;
         }
 
@@ -199,7 +199,7 @@ public class TransportClusterHealthAction extends TransportMasterNodeReadAction<
         if (request.waitForActiveShards() != -1 && response.getActiveShards() >= request.waitForActiveShards()) {
             waitForCounter++;
         }
-        if (request.indices().length > 0) {
+        if (request.indices() != null && request.indices().length > 0) {
             try {
                 indexNameExpressionResolver.concreteIndices(clusterState, IndicesOptions.strictExpand(), request.indices());
                 waitForCounter++;
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/NodesHotThreadsRequest.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/NodesHotThreadsRequest.java
index f57903f..7c8f797 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/NodesHotThreadsRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/NodesHotThreadsRequest.java
@@ -38,7 +38,7 @@ public class NodesHotThreadsRequest extends BaseNodesRequest<NodesHotThreadsRequ
     boolean ignoreIdleThreads = true;
 
     // for serialization
-    NodesHotThreadsRequest() {
+    public NodesHotThreadsRequest() {
 
     }
 
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/TransportNodesHotThreadsAction.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/TransportNodesHotThreadsAction.java
index 59fd913..4a5a9bb 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/TransportNodesHotThreadsAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/TransportNodesHotThreadsAction.java
@@ -94,11 +94,11 @@ public class TransportNodesHotThreadsAction extends TransportNodesAction<NodesHo
         return false;
     }
 
-    static class NodeRequest extends BaseNodeRequest {
+    public static class NodeRequest extends BaseNodeRequest {
 
         NodesHotThreadsRequest request;
 
-        NodeRequest() {
+        public NodeRequest() {
         }
 
         NodeRequest(String nodeId, NodesHotThreadsRequest request) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/TransportNodesInfoAction.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/TransportNodesInfoAction.java
index 6540754..f974d3b 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/TransportNodesInfoAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/TransportNodesInfoAction.java
@@ -88,11 +88,11 @@ public class TransportNodesInfoAction extends TransportNodesAction<NodesInfoRequ
         return false;
     }
 
-    static class NodeInfoRequest extends BaseNodeRequest {
+    public static class NodeInfoRequest extends BaseNodeRequest {
 
         NodesInfoRequest request;
 
-        NodeInfoRequest() {
+        public NodeInfoRequest() {
         }
 
         NodeInfoRequest(String nodeId, NodesInfoRequest request) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodesStatsRequest.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodesStatsRequest.java
index 9c23117..b0d7d76 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodesStatsRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodesStatsRequest.java
@@ -42,7 +42,7 @@ public class NodesStatsRequest extends BaseNodesRequest<NodesStatsRequest> {
     private boolean breaker;
     private boolean script;
 
-    protected NodesStatsRequest() {
+    public NodesStatsRequest() {
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/node/stats/TransportNodesStatsAction.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/node/stats/TransportNodesStatsAction.java
index b3d1bd4..450834d 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/node/stats/TransportNodesStatsAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/node/stats/TransportNodesStatsAction.java
@@ -88,11 +88,11 @@ public class TransportNodesStatsAction extends TransportNodesAction<NodesStatsRe
         return false;
     }
 
-    static class NodeStatsRequest extends BaseNodeRequest {
+    public static class NodeStatsRequest extends BaseNodeRequest {
 
         NodesStatsRequest request;
 
-        NodeStatsRequest() {
+        public NodeStatsRequest() {
         }
 
         NodeStatsRequest(String nodeId, NodesStatsRequest request) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/DeleteRepositoryRequest.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/DeleteRepositoryRequest.java
index d99a666..bb3b17b 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/DeleteRepositoryRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/DeleteRepositoryRequest.java
@@ -37,7 +37,7 @@ public class DeleteRepositoryRequest extends AcknowledgedRequest<DeleteRepositor
 
     private String name;
 
-    DeleteRepositoryRequest() {
+    public DeleteRepositoryRequest() {
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/GetRepositoriesRequest.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/GetRepositoriesRequest.java
index 4f5f99b..b43dbf3 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/GetRepositoriesRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/GetRepositoriesRequest.java
@@ -36,7 +36,7 @@ public class GetRepositoriesRequest extends MasterNodeReadRequest<GetRepositorie
 
     private String[] repositories = Strings.EMPTY_ARRAY;
 
-    GetRepositoriesRequest() {
+    public GetRepositoriesRequest() {
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/PutRepositoryRequest.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/PutRepositoryRequest.java
index fa5ed48..3d0977f 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/PutRepositoryRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/PutRepositoryRequest.java
@@ -55,7 +55,7 @@ public class PutRepositoryRequest extends AcknowledgedRequest<PutRepositoryReque
 
     private Settings settings = EMPTY_SETTINGS;
 
-    PutRepositoryRequest() {
+    public PutRepositoryRequest() {
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/VerifyRepositoryRequest.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/VerifyRepositoryRequest.java
index 1062924..3330577 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/VerifyRepositoryRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/VerifyRepositoryRequest.java
@@ -37,7 +37,7 @@ public class VerifyRepositoryRequest extends AcknowledgedRequest<VerifyRepositor
 
     private String name;
 
-    VerifyRepositoryRequest() {
+    public VerifyRepositoryRequest() {
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequest.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequest.java
index 8dfae30..4dbef01 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequest.java
@@ -79,7 +79,7 @@ public class CreateSnapshotRequest extends MasterNodeRequest<CreateSnapshotReque
 
     private boolean waitForCompletion;
 
-    CreateSnapshotRequest() {
+    public CreateSnapshotRequest() {
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/GetSnapshotsRequest.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/GetSnapshotsRequest.java
index 03edadd..cccd697 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/GetSnapshotsRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/GetSnapshotsRequest.java
@@ -41,7 +41,7 @@ public class GetSnapshotsRequest extends MasterNodeRequest<GetSnapshotsRequest>
 
     private String[] snapshots = Strings.EMPTY_ARRAY;
 
-    GetSnapshotsRequest() {
+    public GetSnapshotsRequest() {
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequest.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequest.java
index 6f49b46..7b349e4 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequest.java
@@ -64,7 +64,7 @@ public class RestoreSnapshotRequest extends MasterNodeRequest<RestoreSnapshotReq
     private Settings indexSettings = EMPTY_SETTINGS;
     private String[] ignoreIndexSettings = Strings.EMPTY_ARRAY;
 
-    RestoreSnapshotRequest() {
+    public RestoreSnapshotRequest() {
     }
 
     /**
@@ -537,7 +537,9 @@ public class RestoreSnapshotRequest extends MasterNodeRequest<RestoreSnapshotReq
                         throw new IllegalArgumentException("malformed ignore_index_settings section, should be an array of strings");
                     }
             } else {
-                throw new IllegalArgumentException("Unknown parameter " + name);
+                if (IndicesOptions.isIndicesOptions(name) == false) {
+                    throw new IllegalArgumentException("Unknown parameter " + name);
+                }
             }
         }
         indicesOptions(IndicesOptions.fromMap((Map<String, Object>) source, IndicesOptions.lenientExpandOpen()));
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportNodesSnapshotsStatus.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportNodesSnapshotsStatus.java
index 25701a8..d22383a 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportNodesSnapshotsStatus.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportNodesSnapshotsStatus.java
@@ -137,7 +137,7 @@ public class TransportNodesSnapshotsStatus extends TransportNodesAction<Transpor
         return true;
     }
 
-    static class Request extends BaseNodesRequest<Request> {
+    public static class Request extends BaseNodesRequest<Request> {
 
         private SnapshotId[] snapshotIds;
 
@@ -203,11 +203,11 @@ public class TransportNodesSnapshotsStatus extends TransportNodesAction<Transpor
     }
 
 
-    static class NodeRequest extends BaseNodeRequest {
+    public static class NodeRequest extends BaseNodeRequest {
 
         private SnapshotId[] snapshotIds;
 
-        NodeRequest() {
+        public NodeRequest() {
         }
 
         NodeRequest(String nodeId, TransportNodesSnapshotsStatus.Request request) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsRequest.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsRequest.java
index d33f9ac..845d305 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsRequest.java
@@ -30,7 +30,7 @@ import java.io.IOException;
  */
 public class ClusterStatsRequest extends BaseNodesRequest<ClusterStatsRequest> {
 
-    ClusterStatsRequest() {
+    public ClusterStatsRequest() {
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java
index 5ed40c5..daddae4 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java
@@ -145,11 +145,11 @@ public class TransportClusterStatsAction extends TransportNodesAction<ClusterSta
         return false;
     }
 
-    static class ClusterStatsNodeRequest extends BaseNodeRequest {
+    public static class ClusterStatsNodeRequest extends BaseNodeRequest {
 
         ClusterStatsRequest request;
 
-        ClusterStatsNodeRequest() {
+        public ClusterStatsNodeRequest() {
         }
 
         ClusterStatsNodeRequest(String nodeId, ClusterStatsRequest request) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheRequest.java
index a5563d6..3c068b7 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheRequest.java
@@ -37,7 +37,7 @@ public class ClearIndicesCacheRequest extends BroadcastRequest<ClearIndicesCache
     private String[] fields = null;
     
 
-    ClearIndicesCacheRequest() {
+    public ClearIndicesCacheRequest() {
     }
 
     public ClearIndicesCacheRequest(String... indices) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/close/CloseIndexRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/close/CloseIndexRequest.java
index 0072b0a..092a65f 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/close/CloseIndexRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/close/CloseIndexRequest.java
@@ -39,7 +39,7 @@ public class CloseIndexRequest extends AcknowledgedRequest<CloseIndexRequest> im
     private String[] indices;
     private IndicesOptions indicesOptions = IndicesOptions.fromOptions(false, false, true, false);
 
-    CloseIndexRequest() {
+    public CloseIndexRequest() {
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java
index 48342f9..62b8722 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java
@@ -78,7 +78,7 @@ public class CreateIndexRequest extends AcknowledgedRequest<CreateIndexRequest>
 
     private boolean updateAllTypes = false;
 
-    CreateIndexRequest() {
+    public CreateIndexRequest() {
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequest.java
index 2ac92bd..7c957ea 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequest.java
@@ -44,7 +44,7 @@ public class DeleteIndexRequest extends MasterNodeRequest<DeleteIndexRequest> im
     private IndicesOptions indicesOptions = IndicesOptions.fromOptions(false, true, true, true);
     private TimeValue timeout = AcknowledgedRequest.DEFAULT_ACK_TIMEOUT;
 
-    DeleteIndexRequest() {
+    public DeleteIndexRequest() {
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/exists/indices/IndicesExistsRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/exists/indices/IndicesExistsRequest.java
index e822f45..bc0112f 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/exists/indices/IndicesExistsRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/exists/indices/IndicesExistsRequest.java
@@ -37,7 +37,7 @@ public class IndicesExistsRequest extends MasterNodeReadRequest<IndicesExistsReq
     private IndicesOptions indicesOptions = IndicesOptions.fromOptions(false, false, true, true);
 
     // for serialization
-    IndicesExistsRequest() {
+    public IndicesExistsRequest() {
 
     }
 
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/exists/types/TypesExistsRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/exists/types/TypesExistsRequest.java
index 8eeb742..85f46a9 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/exists/types/TypesExistsRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/exists/types/TypesExistsRequest.java
@@ -38,7 +38,7 @@ public class TypesExistsRequest extends MasterNodeReadRequest<TypesExistsRequest
 
     private IndicesOptions indicesOptions = IndicesOptions.strictExpandOpen();
 
-    TypesExistsRequest() {
+    public TypesExistsRequest() {
     }
 
     public TypesExistsRequest(String[] indices, String... types) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/flush/FlushRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/flush/FlushRequest.java
index 57d9455..ad8a719 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/flush/FlushRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/flush/FlushRequest.java
@@ -42,7 +42,7 @@ public class FlushRequest extends BroadcastRequest<FlushRequest> {
     private boolean force = false;
     private boolean waitIfOngoing = false;
 
-    FlushRequest() {
+    public FlushRequest() {
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsIndexRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsIndexRequest.java
index fefcce6..5984443 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsIndexRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsIndexRequest.java
@@ -29,7 +29,7 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 
 import java.io.IOException;
 
-class GetFieldMappingsIndexRequest extends SingleShardRequest<GetFieldMappingsIndexRequest> {
+public class GetFieldMappingsIndexRequest extends SingleShardRequest<GetFieldMappingsIndexRequest> {
 
     private boolean probablySingleFieldRequest;
     private boolean includeDefaults;
@@ -38,7 +38,7 @@ class GetFieldMappingsIndexRequest extends SingleShardRequest<GetFieldMappingsIn
 
     private OriginalIndices originalIndices;
 
-    GetFieldMappingsIndexRequest() {
+    public GetFieldMappingsIndexRequest() {
     }
 
     GetFieldMappingsIndexRequest(GetFieldMappingsRequest other, String index, boolean probablySingleFieldRequest) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequest.java
index aa2e7ab..a708939 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequest.java
@@ -65,7 +65,7 @@ public class PutMappingRequest extends AcknowledgedRequest<PutMappingRequest> im
 
     private boolean updateAllTypes = false;
 
-    PutMappingRequest() {
+    public PutMappingRequest() {
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/open/OpenIndexRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/open/OpenIndexRequest.java
index 94a66d8..3f3b5f5 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/open/OpenIndexRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/open/OpenIndexRequest.java
@@ -39,7 +39,7 @@ public class OpenIndexRequest extends AcknowledgedRequest<OpenIndexRequest> impl
     private String[] indices;
     private IndicesOptions indicesOptions = IndicesOptions.fromOptions(false, false, false, true);
 
-    OpenIndexRequest() {
+    public OpenIndexRequest() {
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/refresh/RefreshRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/refresh/RefreshRequest.java
index b0cb49c..ab9186c 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/refresh/RefreshRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/refresh/RefreshRequest.java
@@ -33,7 +33,7 @@ import org.elasticsearch.action.support.broadcast.BroadcastRequest;
  */
 public class RefreshRequest extends BroadcastRequest<RefreshRequest> {
 
-    RefreshRequest() {
+    public RefreshRequest() {
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/settings/put/UpdateSettingsRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/settings/put/UpdateSettingsRequest.java
index 501da32..c654d69 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/settings/put/UpdateSettingsRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/settings/put/UpdateSettingsRequest.java
@@ -48,7 +48,7 @@ public class UpdateSettingsRequest extends AcknowledgedRequest<UpdateSettingsReq
     private IndicesOptions indicesOptions = IndicesOptions.fromOptions(false, false, true, true);
     private Settings settings = EMPTY_SETTINGS;
 
-    UpdateSettingsRequest() {
+    public UpdateSettingsRequest() {
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoresRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoresRequest.java
index 0c0b338..06ac26e 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoresRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoresRequest.java
@@ -46,7 +46,7 @@ public class IndicesShardStoresRequest extends MasterNodeReadRequest<IndicesShar
         this.indices = indices;
     }
 
-    IndicesShardStoresRequest() {
+    public IndicesShardStoresRequest() {
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateRequest.java
index fe42f7e..d6a6795 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateRequest.java
@@ -74,7 +74,7 @@ public class PutIndexTemplateRequest extends MasterNodeRequest<PutIndexTemplateR
     
     private Map<String, IndexMetaData.Custom> customs = new HashMap<>();
 
-    PutIndexTemplateRequest() {
+    public PutIndexTemplateRequest() {
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/ShardUpgradeRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/ShardUpgradeRequest.java
index 9731a98..550a5b1 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/ShardUpgradeRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/ShardUpgradeRequest.java
@@ -30,11 +30,11 @@ import java.io.IOException;
 /**
  *
  */
-final class ShardUpgradeRequest extends BroadcastShardRequest {
+public final class ShardUpgradeRequest extends BroadcastShardRequest {
 
     private UpgradeRequest request = new UpgradeRequest();
 
-    ShardUpgradeRequest() {
+    public ShardUpgradeRequest() {
     }
 
     ShardUpgradeRequest(ShardId shardId, UpgradeRequest request) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeSettingsRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeSettingsRequest.java
index f1e32f6..278367c 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeSettingsRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeSettingsRequest.java
@@ -39,7 +39,7 @@ public class UpgradeSettingsRequest extends AcknowledgedRequest<UpgradeSettingsR
 
     private Map<String, Tuple<Version, String>> versions;
 
-    UpgradeSettingsRequest() {
+    public UpgradeSettingsRequest() {
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ShardValidateQueryRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ShardValidateQueryRequest.java
index 648ab21..808d1a5 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ShardValidateQueryRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ShardValidateQueryRequest.java
@@ -32,7 +32,7 @@ import java.io.IOException;
 /**
  * Internal validate request executed directly against a specific index shard.
  */
-class ShardValidateQueryRequest extends BroadcastShardRequest {
+public class ShardValidateQueryRequest extends BroadcastShardRequest {
 
     private BytesReference source;
     private String[] types = Strings.EMPTY_ARRAY;
@@ -43,7 +43,7 @@ class ShardValidateQueryRequest extends BroadcastShardRequest {
     @Nullable
     private String[] filteringAliases;
 
-    ShardValidateQueryRequest() {
+    public ShardValidateQueryRequest() {
 
     }
 
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequest.java
index 3499852..20fb541 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequest.java
@@ -55,7 +55,7 @@ public class ValidateQueryRequest extends BroadcastRequest<ValidateQueryRequest>
 
     long nowInMillis;
 
-    ValidateQueryRequest() {
+    public ValidateQueryRequest() {
         this(Strings.EMPTY_ARRAY);
     }
 
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/TransportRenderSearchTemplateAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/TransportRenderSearchTemplateAction.java
index ab3090a..55b1d5e 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/TransportRenderSearchTemplateAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/TransportRenderSearchTemplateAction.java
@@ -38,7 +38,7 @@ public class TransportRenderSearchTemplateAction extends HandledTransportAction<
     private final ScriptService scriptService;
 
     @Inject
-    protected TransportRenderSearchTemplateAction(ScriptService scriptService, Settings settings, ThreadPool threadPool,
+    public TransportRenderSearchTemplateAction(ScriptService scriptService, Settings settings, ThreadPool threadPool,
             TransportService transportService, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver) {
         super(settings, RenderSearchTemplateAction.NAME, threadPool, transportService, actionFilters, indexNameExpressionResolver, RenderSearchTemplateRequest.class);
         this.scriptService = scriptService;
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequest.java
index 50747dd..61b033f 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequest.java
@@ -42,7 +42,7 @@ public class DeleteWarmerRequest extends AcknowledgedRequest<DeleteWarmerRequest
     private IndicesOptions indicesOptions = IndicesOptions.fromOptions(false, false, true, false);
     private String[] indices = Strings.EMPTY_ARRAY;
 
-    DeleteWarmerRequest() {
+    public DeleteWarmerRequest() {
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/put/PutWarmerRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/put/PutWarmerRequest.java
index 981af19..dbf136d 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/put/PutWarmerRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/put/PutWarmerRequest.java
@@ -44,7 +44,7 @@ public class PutWarmerRequest extends AcknowledgedRequest<PutWarmerRequest> impl
 
     private SearchRequest searchRequest;
 
-    PutWarmerRequest() {
+    public PutWarmerRequest() {
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/BulkShardRequest.java b/core/src/main/java/org/elasticsearch/action/bulk/BulkShardRequest.java
index 6bda7b2..ec15038 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/BulkShardRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/BulkShardRequest.java
@@ -37,7 +37,7 @@ public class BulkShardRequest extends ReplicationRequest<BulkShardRequest> {
 
     private boolean refresh;
 
-    BulkShardRequest() {
+    public BulkShardRequest() {
     }
 
     BulkShardRequest(BulkRequest bulkRequest, String index, int shardId, boolean refresh, BulkItemRequest[] items) {
diff --git a/core/src/main/java/org/elasticsearch/action/exists/ExistsRequest.java b/core/src/main/java/org/elasticsearch/action/exists/ExistsRequest.java
index 32ff0b1..0000676 100644
--- a/core/src/main/java/org/elasticsearch/action/exists/ExistsRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/exists/ExistsRequest.java
@@ -55,7 +55,7 @@ public class ExistsRequest extends BroadcastRequest<ExistsRequest> {
 
     long nowInMillis;
 
-    ExistsRequest() {
+    public ExistsRequest() {
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/exists/ShardExistsRequest.java b/core/src/main/java/org/elasticsearch/action/exists/ShardExistsRequest.java
index 276e6ea..d57b1d9 100644
--- a/core/src/main/java/org/elasticsearch/action/exists/ShardExistsRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/exists/ShardExistsRequest.java
@@ -29,7 +29,7 @@ import org.elasticsearch.index.shard.ShardId;
 
 import java.io.IOException;
 
-class ShardExistsRequest extends BroadcastShardRequest {
+public class ShardExistsRequest extends BroadcastShardRequest {
 
     private float minScore;
 
@@ -42,7 +42,7 @@ class ShardExistsRequest extends BroadcastShardRequest {
     @Nullable
     private String[] filteringAliases;
 
-    ShardExistsRequest() {
+    public ShardExistsRequest() {
     }
 
     ShardExistsRequest(ShardId shardId, @Nullable String[] filteringAliases, ExistsRequest request) {
diff --git a/core/src/main/java/org/elasticsearch/action/explain/ExplainRequest.java b/core/src/main/java/org/elasticsearch/action/explain/ExplainRequest.java
index e7d703e..2b796b0 100644
--- a/core/src/main/java/org/elasticsearch/action/explain/ExplainRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/explain/ExplainRequest.java
@@ -49,7 +49,7 @@ public class ExplainRequest extends SingleShardRequest<ExplainRequest> {
 
     long nowInMillis;
 
-    ExplainRequest() {
+    public ExplainRequest() {
     }
 
     public ExplainRequest(String index, String type, String id) {
diff --git a/core/src/main/java/org/elasticsearch/action/get/GetRequest.java b/core/src/main/java/org/elasticsearch/action/get/GetRequest.java
index 108abc9..8403282 100644
--- a/core/src/main/java/org/elasticsearch/action/get/GetRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/get/GetRequest.java
@@ -63,7 +63,7 @@ public class GetRequest extends SingleShardRequest<GetRequest> implements Realti
     private long version = Versions.MATCH_ANY;
     private boolean ignoreErrorsOnGeneratedFields;
 
-    GetRequest() {
+    public GetRequest() {
         type = "_all";
     }
 
diff --git a/core/src/main/java/org/elasticsearch/action/get/MultiGetShardRequest.java b/core/src/main/java/org/elasticsearch/action/get/MultiGetShardRequest.java
index a45c1c1..6715319 100644
--- a/core/src/main/java/org/elasticsearch/action/get/MultiGetShardRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/get/MultiGetShardRequest.java
@@ -40,7 +40,7 @@ public class MultiGetShardRequest extends SingleShardRequest<MultiGetShardReques
     IntArrayList locations;
     List<MultiGetRequest.Item> items;
 
-    MultiGetShardRequest() {
+    public MultiGetShardRequest() {
 
     }
 
diff --git a/core/src/main/java/org/elasticsearch/action/percolate/PercolateShardRequest.java b/core/src/main/java/org/elasticsearch/action/percolate/PercolateShardRequest.java
index b03c667..cdd967a 100644
--- a/core/src/main/java/org/elasticsearch/action/percolate/PercolateShardRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/percolate/PercolateShardRequest.java
@@ -39,7 +39,7 @@ public class PercolateShardRequest extends BroadcastShardRequest {
     private int numberOfShards;
     private long startTime;
 
-    PercolateShardRequest() {
+    public PercolateShardRequest() {
     }
 
     PercolateShardRequest(ShardId shardId, int numberOfShards, PercolateRequest request) {
diff --git a/core/src/main/java/org/elasticsearch/action/percolate/TransportShardMultiPercolateAction.java b/core/src/main/java/org/elasticsearch/action/percolate/TransportShardMultiPercolateAction.java
index 9155889..9803229 100644
--- a/core/src/main/java/org/elasticsearch/action/percolate/TransportShardMultiPercolateAction.java
+++ b/core/src/main/java/org/elasticsearch/action/percolate/TransportShardMultiPercolateAction.java
@@ -118,7 +118,7 @@ public class TransportShardMultiPercolateAction extends TransportSingleShardActi
         private String preference;
         private List<Item> items;
 
-        Request() {
+        public Request() {
         }
 
         Request(MultiPercolateRequest multiPercolateRequest, String concreteIndex, int shardId, String preference) {
diff --git a/core/src/main/java/org/elasticsearch/action/search/SearchRequest.java b/core/src/main/java/org/elasticsearch/action/search/SearchRequest.java
index c423188..abc1ba5 100644
--- a/core/src/main/java/org/elasticsearch/action/search/SearchRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/search/SearchRequest.java
@@ -252,48 +252,6 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
     }
 
     /**
-     * The source of the search request. Consider using either {@link #source(byte[])} or
-     * {@link #source(org.elasticsearch.search.builder.SearchSourceBuilder)}.
-     */
-    public SearchRequest source(String source) {
-        this.source = new BytesArray(source);
-        return this;
-    }
-
-    /**
-     * The source of the search request in the form of a map.
-     */
-    public SearchRequest source(Map source) {
-        try {
-            XContentBuilder builder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
-            builder.map(source);
-            return source(builder);
-        } catch (IOException e) {
-            throw new ElasticsearchGenerationException("Failed to generate [" + source + "]", e);
-        }
-    }
-
-    public SearchRequest source(XContentBuilder builder) {
-        this.source = builder.bytes();
-        return this;
-    }
-
-    /**
-     * The search source to execute.
-     */
-    public SearchRequest source(byte[] source) {
-        return source(source, 0, source.length);
-    }
-
-
-    /**
-     * The search source to execute.
-     */
-    public SearchRequest source(byte[] source, int offset, int length) {
-        return source(new BytesArray(source, offset, length));
-    }
-
-    /**
      * The search source to execute.
      */
     public SearchRequest source(BytesReference source) {
@@ -301,6 +259,7 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
         return this;
     }
 
+
     /**
      * The search source to execute.
      */
@@ -327,51 +286,6 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
         return this;
     }
 
-    public SearchRequest extraSource(Map extraSource) {
-        try {
-            XContentBuilder builder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
-            builder.map(extraSource);
-            return extraSource(builder);
-        } catch (IOException e) {
-            throw new ElasticsearchGenerationException("Failed to generate [" + extraSource + "]", e);
-        }
-    }
-
-    public SearchRequest extraSource(XContentBuilder builder) {
-        this.extraSource = builder.bytes();
-        return this;
-    }
-
-    /**
-     * Allows to provide additional source that will use used as well.
-     */
-    public SearchRequest extraSource(String source) {
-        this.extraSource = new BytesArray(source);
-        return this;
-    }
-
-    /**
-     * Allows to provide additional source that will be used as well.
-     */
-    public SearchRequest extraSource(byte[] source) {
-        return extraSource(source, 0, source.length);
-    }
-
-    /**
-     * Allows to provide additional source that will be used as well.
-     */
-    public SearchRequest extraSource(byte[] source, int offset, int length) {
-        return extraSource(new BytesArray(source, offset, length));
-    }
-
-    /**
-     * Allows to provide additional source that will be used as well.
-     */
-    public SearchRequest extraSource(BytesReference source) {
-        this.extraSource = source;
-        return this;
-    }
-
     /**
      * Allows to provide template as source.
      */
diff --git a/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
index f636534..7caf828 100644
--- a/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
@@ -806,27 +806,19 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
     }
 
     /**
-     * Sets the source of the request as a json string. Note, settings anything other
+     * Sets the source of the request as a SearchSourceBuilder. Note, settings anything other
      * than the search type will cause this source to be overridden, consider using
-     * {@link #setExtraSource(String)}.
+     * {@link #setExtraSource(SearchSourceBuilder)} instead.
      */
-    public SearchRequestBuilder setSource(String source) {
+    public SearchRequestBuilder setSource(SearchSourceBuilder source) {
         request.source(source);
         return this;
     }
 
     /**
-     * Sets the source of the request as a json string. Allows to set other parameters.
-     */
-    public SearchRequestBuilder setExtraSource(String source) {
-        request.extraSource(source);
-        return this;
-    }
-
-    /**
      * Sets the source of the request as a json string. Note, settings anything other
      * than the search type will cause this source to be overridden, consider using
-     * {@link #setExtraSource(BytesReference)}.
+     * {@link #setExtraSource(SearchSourceBuilder)} instead.
      */
     public SearchRequestBuilder setSource(BytesReference source) {
         request.source(source);
@@ -834,78 +826,11 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
     }
 
     /**
-     * Sets the source of the request as a json string. Note, settings anything other
-     * than the search type will cause this source to be overridden, consider using
-     * {@link #setExtraSource(byte[])}.
-     */
-    public SearchRequestBuilder setSource(byte[] source) {
-        request.source(source);
-        return this;
-    }
-
-    /**
-     * Sets the source of the request as a json string. Allows to set other parameters.
-     */
-    public SearchRequestBuilder setExtraSource(BytesReference source) {
-        request.extraSource(source);
-        return this;
-    }
-
-    /**
-     * Sets the source of the request as a json string. Allows to set other parameters.
+     * Sets the an addtional source of the request as a SearchSourceBuilder. All values and
+     * settings set on the extra source will override the corresponding settings on the specified
+     * source.
      */
-    public SearchRequestBuilder setExtraSource(byte[] source) {
-        request.extraSource(source);
-        return this;
-    }
-
-    /**
-     * Sets the source of the request as a json string. Note, settings anything other
-     * than the search type will cause this source to be overridden, consider using
-     * {@link #setExtraSource(byte[])}.
-     */
-    public SearchRequestBuilder setSource(byte[] source, int offset, int length) {
-        request.source(source, offset, length);
-        return this;
-    }
-
-    /**
-     * Sets the source of the request as a json string. Allows to set other parameters.
-     */
-    public SearchRequestBuilder setExtraSource(byte[] source, int offset, int length) {
-        request.extraSource(source, offset, length);
-        return this;
-    }
-
-    /**
-     * Sets the source of the request as a json string. Note, settings anything other
-     * than the search type will cause this source to be overridden, consider using
-     * {@link #setExtraSource(byte[])}.
-     */
-    public SearchRequestBuilder setSource(XContentBuilder builder) {
-        request.source(builder);
-        return this;
-    }
-
-    /**
-     * Sets the source of the request as a json string. Allows to set other parameters.
-     */
-    public SearchRequestBuilder setExtraSource(XContentBuilder builder) {
-        request.extraSource(builder);
-        return this;
-    }
-
-    /**
-     * Sets the source of the request as a map. Note, setting anything other than the
-     * search type will cause this source to be overridden, consider using
-     * {@link #setExtraSource(java.util.Map)}.
-     */
-    public SearchRequestBuilder setSource(Map source) {
-        request.source(source);
-        return this;
-    }
-
-    public SearchRequestBuilder setExtraSource(Map source) {
+    public SearchRequestBuilder setExtraSource(SearchSourceBuilder source) {
         request.extraSource(source);
         return this;
     }
@@ -913,39 +838,11 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
     /**
      * template stuff
      */
-
     public SearchRequestBuilder setTemplate(Template template) {
         request.template(template);
         return this;
     }
 
-    /**
-     * @deprecated Use {@link #setTemplate(Template)} instead.
-     */
-    @Deprecated
-    public SearchRequestBuilder setTemplateName(String templateName) {
-        request.templateName(templateName);
-        return this;
-    }
-
-    /**
-     * @deprecated Use {@link #setTemplate(Template)} instead.
-     */
-    @Deprecated
-    public SearchRequestBuilder setTemplateType(ScriptService.ScriptType templateType) {
-        request.templateType(templateType);
-        return this;
-    }
-
-    /**
-     * @deprecated Use {@link #setTemplate(Template)} instead.
-     */
-    @Deprecated
-    public SearchRequestBuilder setTemplateParams(Map<String, Object> templateParams) {
-        request.templateParams(templateParams);
-        return this;
-    }
-
     public SearchRequestBuilder setTemplateSource(String source) {
         request.templateSource(source);
         return this;
diff --git a/core/src/main/java/org/elasticsearch/action/suggest/ShardSuggestRequest.java b/core/src/main/java/org/elasticsearch/action/suggest/ShardSuggestRequest.java
index 794dd9b..80facf7 100644
--- a/core/src/main/java/org/elasticsearch/action/suggest/ShardSuggestRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/suggest/ShardSuggestRequest.java
@@ -30,11 +30,11 @@ import java.io.IOException;
 /**
  * Internal suggest request executed directly against a specific index shard.
  */
-final class ShardSuggestRequest extends BroadcastShardRequest {
+public final class ShardSuggestRequest extends BroadcastShardRequest {
 
     private BytesReference suggestSource;
 
-    ShardSuggestRequest() {
+    public ShardSuggestRequest() {
     }
 
     ShardSuggestRequest(ShardId shardId, SuggestRequest request) {
diff --git a/core/src/main/java/org/elasticsearch/action/suggest/SuggestRequest.java b/core/src/main/java/org/elasticsearch/action/suggest/SuggestRequest.java
index c75e262..764975e 100644
--- a/core/src/main/java/org/elasticsearch/action/suggest/SuggestRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/suggest/SuggestRequest.java
@@ -58,7 +58,7 @@ public final class SuggestRequest extends BroadcastRequest<SuggestRequest> {
 
     private BytesReference suggestSource;
 
-    SuggestRequest() {
+    public SuggestRequest() {
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/support/IndicesOptions.java b/core/src/main/java/org/elasticsearch/action/support/IndicesOptions.java
index c46a007..793dbe0 100644
--- a/core/src/main/java/org/elasticsearch/action/support/IndicesOptions.java
+++ b/core/src/main/java/org/elasticsearch/action/support/IndicesOptions.java
@@ -154,6 +154,16 @@ public class IndicesOptions {
                 defaultSettings);
     }
 
+    /**
+     * Returns true if the name represents a valid name for one of the indices option
+     * false otherwise
+     */
+    public static boolean isIndicesOptions(String name) {
+        return "expand_wildcards".equals(name) || "expandWildcards".equals(name) ||
+                "ignore_unavailable".equals(name) || "ignoreUnavailable".equals(name) ||
+                "allow_no_indices".equals(name) || "allowNoIndices".equals(name);
+    }
+
     public static IndicesOptions fromParameters(Object wildcardsString, Object ignoreUnavailableString, Object allowNoIndicesString, IndicesOptions defaultSettings) {
         if (wildcardsString == null && ignoreUnavailableString == null && allowNoIndicesString == null) {
             return defaultSettings;
diff --git a/core/src/main/java/org/elasticsearch/action/support/broadcast/BroadcastShardRequest.java b/core/src/main/java/org/elasticsearch/action/support/broadcast/BroadcastShardRequest.java
index e416cd5..8e22a90 100644
--- a/core/src/main/java/org/elasticsearch/action/support/broadcast/BroadcastShardRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/support/broadcast/BroadcastShardRequest.java
@@ -38,7 +38,7 @@ public abstract class BroadcastShardRequest extends TransportRequest implements
 
     protected OriginalIndices originalIndices;
 
-    protected BroadcastShardRequest() {
+    public BroadcastShardRequest() {
     }
 
     protected BroadcastShardRequest(ShardId shardId, BroadcastRequest request) {
diff --git a/core/src/main/java/org/elasticsearch/action/support/broadcast/node/TransportBroadcastByNodeAction.java b/core/src/main/java/org/elasticsearch/action/support/broadcast/node/TransportBroadcastByNodeAction.java
index b7625b0..76b3995 100644
--- a/core/src/main/java/org/elasticsearch/action/support/broadcast/node/TransportBroadcastByNodeAction.java
+++ b/core/src/main/java/org/elasticsearch/action/support/broadcast/node/TransportBroadcastByNodeAction.java
@@ -19,16 +19,8 @@
 
 package org.elasticsearch.action.support.broadcast.node;
 
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.FailedNodeException;
-import org.elasticsearch.action.IndicesRequest;
-import org.elasticsearch.action.NoShardAvailableActionException;
-import org.elasticsearch.action.ShardOperationFailedException;
-import org.elasticsearch.action.support.ActionFilters;
-import org.elasticsearch.action.support.DefaultShardOperationFailedException;
-import org.elasticsearch.action.support.HandledTransportAction;
-import org.elasticsearch.action.support.IndicesOptions;
-import org.elasticsearch.action.support.TransportActions;
+import org.elasticsearch.action.*;
+import org.elasticsearch.action.support.*;
 import org.elasticsearch.action.support.broadcast.BroadcastRequest;
 import org.elasticsearch.action.support.broadcast.BroadcastResponse;
 import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;
@@ -45,21 +37,13 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.BaseTransportResponseHandler;
-import org.elasticsearch.transport.NodeShouldNotConnectException;
-import org.elasticsearch.transport.TransportChannel;
-import org.elasticsearch.transport.TransportException;
-import org.elasticsearch.transport.TransportRequest;
-import org.elasticsearch.transport.TransportRequestHandler;
-import org.elasticsearch.transport.TransportResponse;
-import org.elasticsearch.transport.TransportService;
+import org.elasticsearch.transport.*;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.concurrent.Callable;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicReferenceArray;
 
@@ -100,15 +84,10 @@ public abstract class TransportBroadcastByNodeAction<Request extends BroadcastRe
 
         transportNodeBroadcastAction = actionName + "[n]";
 
-        transportService.registerRequestHandler(transportNodeBroadcastAction, new Callable<NodeRequest>() {
-            @Override
-            public NodeRequest call() throws Exception {
-                return new NodeRequest();
-            }
-        }, executor, new BroadcastByNodeTransportRequestHandler());
+        transportService.registerRequestHandler(transportNodeBroadcastAction, NodeRequest::new, executor, new BroadcastByNodeTransportRequestHandler());
     }
 
-    private final Response newResponse(
+    private Response newResponse(
             Request request,
             AtomicReferenceArray responses,
             List<NoShardAvailableActionException> unavailableShardExceptions,
@@ -253,7 +232,7 @@ public abstract class TransportBroadcastByNodeAction<Request extends BroadcastRe
                 if (shard.assignedToNode()) {
                     String nodeId = shard.currentNodeId();
                     if (!nodeIds.containsKey(nodeId)) {
-                        nodeIds.put(nodeId, new ArrayList<ShardRouting>());
+                        nodeIds.put(nodeId, new ArrayList<>());
                     }
                     nodeIds.get(nodeId).add(shard);
                 } else {
@@ -405,14 +384,14 @@ public abstract class TransportBroadcastByNodeAction<Request extends BroadcastRe
         }
     }
 
-    protected class NodeRequest extends TransportRequest implements IndicesRequest {
+    public class NodeRequest extends TransportRequest implements IndicesRequest {
         private String nodeId;
 
         private List<ShardRouting> shards;
 
         protected Request indicesLevelRequest;
 
-        protected NodeRequest() {
+        public NodeRequest() {
         }
 
         public NodeRequest(String nodeId, Request request, List<ShardRouting> shards) {
diff --git a/core/src/main/java/org/elasticsearch/action/support/nodes/BaseNodeRequest.java b/core/src/main/java/org/elasticsearch/action/support/nodes/BaseNodeRequest.java
index e25577e..35b303c 100644
--- a/core/src/main/java/org/elasticsearch/action/support/nodes/BaseNodeRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/support/nodes/BaseNodeRequest.java
@@ -32,7 +32,7 @@ public abstract class BaseNodeRequest extends TransportRequest {
 
     private String nodeId;
 
-    protected BaseNodeRequest() {
+    public BaseNodeRequest() {
 
     }
 
diff --git a/core/src/main/java/org/elasticsearch/action/support/single/shard/SingleShardRequest.java b/core/src/main/java/org/elasticsearch/action/support/single/shard/SingleShardRequest.java
index edc5473..5d3484f 100644
--- a/core/src/main/java/org/elasticsearch/action/support/single/shard/SingleShardRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/support/single/shard/SingleShardRequest.java
@@ -49,7 +49,7 @@ public abstract class SingleShardRequest<T extends SingleShardRequest> extends A
     ShardId internalShardId;
     private boolean threadedOperation = true;
 
-    protected SingleShardRequest() {
+    public SingleShardRequest() {
     }
 
     protected SingleShardRequest(String index) {
diff --git a/core/src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsShardRequest.java b/core/src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsShardRequest.java
index ee02e69..5f541b0 100644
--- a/core/src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsShardRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsShardRequest.java
@@ -37,7 +37,7 @@ public class MultiTermVectorsShardRequest extends SingleShardRequest<MultiTermVe
     IntArrayList locations;
     List<TermVectorsRequest> requests;
 
-    MultiTermVectorsShardRequest() {
+    public MultiTermVectorsShardRequest() {
 
     }
 
diff --git a/core/src/main/java/org/elasticsearch/action/termvectors/dfs/DfsOnlyRequest.java b/core/src/main/java/org/elasticsearch/action/termvectors/dfs/DfsOnlyRequest.java
index 0171a90..86d575d 100644
--- a/core/src/main/java/org/elasticsearch/action/termvectors/dfs/DfsOnlyRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/termvectors/dfs/DfsOnlyRequest.java
@@ -44,7 +44,7 @@ public class DfsOnlyRequest extends BroadcastRequest<DfsOnlyRequest> {
 
     long nowInMillis;
 
-    DfsOnlyRequest() {
+    public DfsOnlyRequest() {
 
     }
 
diff --git a/core/src/main/java/org/elasticsearch/action/termvectors/dfs/ShardDfsOnlyRequest.java b/core/src/main/java/org/elasticsearch/action/termvectors/dfs/ShardDfsOnlyRequest.java
index 687910c..95a9a82 100644
--- a/core/src/main/java/org/elasticsearch/action/termvectors/dfs/ShardDfsOnlyRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/termvectors/dfs/ShardDfsOnlyRequest.java
@@ -29,11 +29,11 @@ import org.elasticsearch.search.internal.ShardSearchTransportRequest;
 
 import java.io.IOException;
 
-class ShardDfsOnlyRequest extends BroadcastShardRequest {
+public class ShardDfsOnlyRequest extends BroadcastShardRequest {
 
     private ShardSearchTransportRequest shardSearchRequest = new ShardSearchTransportRequest();
 
-    ShardDfsOnlyRequest() {
+    public ShardDfsOnlyRequest() {
 
     }
 
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/ESPolicy.java b/core/src/main/java/org/elasticsearch/bootstrap/ESPolicy.java
index 4f20496..fdc0d4e 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/ESPolicy.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/ESPolicy.java
@@ -61,7 +61,35 @@ final class ESPolicy extends Policy {
             }
         }
 
+        // Special handling for broken AWS code which destroys all SSL security
+        // REMOVE THIS when https://github.com/aws/aws-sdk-java/pull/432 is fixed
+        if (permission instanceof RuntimePermission && "accessClassInPackage.sun.security.ssl".equals(permission.getName())) {
+            for (StackTraceElement element : Thread.currentThread().getStackTrace()) {
+                if ("com.amazonaws.http.conn.ssl.SdkTLSSocketFactory".equals(element.getClassName()) &&
+                      "verifyMasterSecret".equals(element.getMethodName())) {
+                    // we found the horrible method: the hack begins!
+                    // force the aws code to back down, by throwing an exception that it catches.
+                    rethrow(new IllegalAccessException("no amazon, you cannot do this."));
+                }
+            }
+        }
         // otherwise defer to template + dynamic file permissions
         return template.implies(domain, permission) || dynamic.implies(permission);
     }
+
+    /**
+     * Classy puzzler to rethrow any checked exception as an unchecked one.
+     */
+    private static class Rethrower<T extends Throwable> {
+        private void rethrow(Throwable t) throws T {
+            throw (T) t;
+        }
+    }
+
+    /**
+     * Rethrows <code>t</code> (identical object).
+     */
+    private void rethrow(Throwable t) {
+        new Rethrower<Error>().rethrow(t);
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/JarHell.java b/core/src/main/java/org/elasticsearch/bootstrap/JarHell.java
index 9062140..a5c71e3 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/JarHell.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/JarHell.java
@@ -26,6 +26,7 @@ import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
 
 import java.io.IOException;
+import java.net.MalformedURLException;
 import java.net.URL;
 import java.net.URLClassLoader;
 import java.nio.file.FileVisitResult;
@@ -70,21 +71,43 @@ public class JarHell {
     }
 
     /**
-     * Checks the current classloader for duplicate classes
+     * Checks the current classpath for duplicate classes
      * @throws IllegalStateException if jar hell was found
      */
     public static void checkJarHell() throws Exception {
         ClassLoader loader = JarHell.class.getClassLoader();
-        if (loader instanceof URLClassLoader == false) {
-           return;
-        }
         ESLogger logger = Loggers.getLogger(JarHell.class);
         if (logger.isDebugEnabled()) {
             logger.debug("java.class.path: {}", System.getProperty("java.class.path"));
             logger.debug("sun.boot.class.path: {}", System.getProperty("sun.boot.class.path"));
-            logger.debug("classloader urls: {}", Arrays.toString(((URLClassLoader)loader).getURLs()));
+            if (loader instanceof URLClassLoader ) {
+                logger.debug("classloader urls: {}", Arrays.toString(((URLClassLoader)loader).getURLs()));
+             }
+        }
+        checkJarHell(parseClassPath());
+    }
+    
+    /**
+     * Parses the classpath into a set of URLs
+     */
+    @SuppressForbidden(reason = "resolves against CWD because that is how classpaths work")
+    public static URL[] parseClassPath()  {
+        String elements[] = System.getProperty("java.class.path").split(System.getProperty("path.separator"));
+        URL urlElements[] = new URL[elements.length];
+        for (int i = 0; i < elements.length; i++) {
+            String element = elements[i];
+            // empty classpath element behaves like CWD.
+            if (element.isEmpty()) {
+                element = System.getProperty("user.dir");
+            }
+            try {
+                urlElements[i] = PathUtils.get(element).toUri().toURL();
+            } catch (MalformedURLException e) {
+                // should not happen, as we use the filesystem API
+                throw new RuntimeException(e);
+            }
         }
-        checkJarHell(((URLClassLoader) loader).getURLs());
+        return urlElements;
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/Security.java b/core/src/main/java/org/elasticsearch/bootstrap/Security.java
index 4b32baa..a6bcb2f 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/Security.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/Security.java
@@ -24,7 +24,6 @@ import org.elasticsearch.env.Environment;
 
 import java.io.*;
 import java.net.URL;
-import java.net.URLClassLoader;
 import java.nio.file.AccessMode;
 import java.nio.file.FileAlreadyExistsException;
 import java.nio.file.Files;
@@ -121,8 +120,8 @@ final class Security {
     private static final Map<Pattern,String> SPECIAL_JARS;
     static {
         Map<Pattern,String> m = new IdentityHashMap<>();
-        m.put(Pattern.compile(".*lucene-core-.*\\.jar$"),    "es.security.jar.lucene.core");
-        m.put(Pattern.compile(".*securemock-.*\\.jar$"),     "es.security.jar.elasticsearch.securemock");
+        m.put(Pattern.compile(".*lucene-core-.*\\.jar$"),              "es.security.jar.lucene.core");
+        m.put(Pattern.compile(".*securemock-.*\\.jar$"),               "es.security.jar.elasticsearch.securemock");
         SPECIAL_JARS = Collections.unmodifiableMap(m);
     }
 
@@ -133,27 +132,21 @@ final class Security {
      */
     @SuppressForbidden(reason = "proper use of URL")
     static void setCodebaseProperties() {
-        ClassLoader loader = Security.class.getClassLoader();
-        if (loader instanceof URLClassLoader) {
-            for (URL url : ((URLClassLoader)loader).getURLs()) {
-                for (Map.Entry<Pattern,String> e : SPECIAL_JARS.entrySet()) {
-                    if (e.getKey().matcher(url.getPath()).matches()) {
-                        String prop = e.getValue();
-                        if (System.getProperty(prop) != null) {
-                            throw new IllegalStateException("property: " + prop + " is unexpectedly set: " + System.getProperty(prop));
-                        }
-                        System.setProperty(prop, url.toString());
+        for (URL url : JarHell.parseClassPath()) {
+            for (Map.Entry<Pattern,String> e : SPECIAL_JARS.entrySet()) {
+                if (e.getKey().matcher(url.getPath()).matches()) {
+                    String prop = e.getValue();
+                    if (System.getProperty(prop) != null) {
+                        throw new IllegalStateException("property: " + prop + " is unexpectedly set: " + System.getProperty(prop));
                     }
+                    System.setProperty(prop, url.toString());
                 }
             }
-            for (String prop : SPECIAL_JARS.values()) {
-                if (System.getProperty(prop) == null) {
-                    System.setProperty(prop, "file:/dev/null"); // no chance to be interpreted as "all"
-                }
+        }
+        for (String prop : SPECIAL_JARS.values()) {
+            if (System.getProperty(prop) == null) {
+                System.setProperty(prop, "file:/dev/null"); // no chance to be interpreted as "all"
             }
-        } else {
-            // we could try to parse the classpath or something, but screw it for now.
-            throw new UnsupportedOperationException("Unsupported system classloader type: " + loader.getClass());
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/cluster/ClusterModule.java b/core/src/main/java/org/elasticsearch/cluster/ClusterModule.java
index 531e770..58f2c00 100644
--- a/core/src/main/java/org/elasticsearch/cluster/ClusterModule.java
+++ b/core/src/main/java/org/elasticsearch/cluster/ClusterModule.java
@@ -48,7 +48,6 @@ import org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders;
 import org.elasticsearch.cluster.routing.allocation.decider.AwarenessAllocationDecider;
 import org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider;
 import org.elasticsearch.cluster.routing.allocation.decider.ConcurrentRebalanceAllocationDecider;
-import org.elasticsearch.cluster.routing.allocation.decider.DisableAllocationDecider;
 import org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider;
 import org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider;
 import org.elasticsearch.cluster.routing.allocation.decider.FilterAllocationDecider;
@@ -115,8 +114,7 @@ public class ClusterModule extends AbstractModule {
             RebalanceOnlyWhenActiveAllocationDecider.class,
             ClusterRebalanceAllocationDecider.class,
             ConcurrentRebalanceAllocationDecider.class,
-            EnableAllocationDecider.class, // new enable allocation logic should proceed old disable allocation logic
-            DisableAllocationDecider.class,
+            EnableAllocationDecider.class,
             AwarenessAllocationDecider.class,
             ShardsLimitAllocationDecider.class,
             NodeVersionAllocationDecider.class,
@@ -156,9 +154,6 @@ public class ClusterModule extends AbstractModule {
         registerClusterDynamicSetting(ConcurrentRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE, Validator.INTEGER);
         registerClusterDynamicSetting(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, Validator.EMPTY);
         registerClusterDynamicSetting(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE, Validator.EMPTY);
-        registerClusterDynamicSetting(DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_NEW_ALLOCATION, Validator.EMPTY);
-        registerClusterDynamicSetting(DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_ALLOCATION, Validator.EMPTY);
-        registerClusterDynamicSetting(DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_REPLICA_ALLOCATION, Validator.EMPTY);
         registerClusterDynamicSetting(ZenDiscovery.SETTING_REJOIN_ON_MASTER_GONE, Validator.BOOLEAN);
         registerClusterDynamicSetting(DiscoverySettings.NO_MASTER_BLOCK, Validator.EMPTY);
         registerClusterDynamicSetting(FilterAllocationDecider.CLUSTER_ROUTING_INCLUDE_GROUP + "*", Validator.EMPTY);
@@ -196,7 +191,6 @@ public class ClusterModule extends AbstractModule {
         registerClusterDynamicSetting(DestructiveOperations.REQUIRES_NAME, Validator.EMPTY);
         registerClusterDynamicSetting(DiscoverySettings.PUBLISH_TIMEOUT, Validator.TIME_NON_NEGATIVE);
         registerClusterDynamicSetting(DiscoverySettings.PUBLISH_DIFF_ENABLE, Validator.BOOLEAN);
-        registerClusterDynamicSetting(DiscoverySettings.COMMIT_TIMEOUT, Validator.TIME_NON_NEGATIVE);
         registerClusterDynamicSetting(HierarchyCircuitBreakerService.TOTAL_CIRCUIT_BREAKER_LIMIT_SETTING, Validator.MEMORY_SIZE);
         registerClusterDynamicSetting(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING, Validator.MEMORY_SIZE);
         registerClusterDynamicSetting(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING, Validator.NON_NEGATIVE_DOUBLE);
@@ -222,9 +216,6 @@ public class ClusterModule extends AbstractModule {
         registerIndexDynamicSetting(FilterAllocationDecider.INDEX_ROUTING_EXCLUDE_GROUP + "*", Validator.EMPTY);
         registerIndexDynamicSetting(EnableAllocationDecider.INDEX_ROUTING_ALLOCATION_ENABLE, Validator.EMPTY);
         registerIndexDynamicSetting(EnableAllocationDecider.INDEX_ROUTING_REBALANCE_ENABLE, Validator.EMPTY);
-        registerIndexDynamicSetting(DisableAllocationDecider.INDEX_ROUTING_ALLOCATION_DISABLE_ALLOCATION, Validator.EMPTY);
-        registerIndexDynamicSetting(DisableAllocationDecider.INDEX_ROUTING_ALLOCATION_DISABLE_NEW_ALLOCATION, Validator.EMPTY);
-        registerIndexDynamicSetting(DisableAllocationDecider.INDEX_ROUTING_ALLOCATION_DISABLE_REPLICA_ALLOCATION, Validator.EMPTY);
         registerIndexDynamicSetting(TranslogConfig.INDEX_TRANSLOG_FS_TYPE, Validator.EMPTY);
         registerIndexDynamicSetting(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, Validator.NON_NEGATIVE_INTEGER);
         registerIndexDynamicSetting(IndexMetaData.SETTING_AUTO_EXPAND_REPLICAS, Validator.EMPTY);
diff --git a/core/src/main/java/org/elasticsearch/cluster/ClusterState.java b/core/src/main/java/org/elasticsearch/cluster/ClusterState.java
index 21962fb..2bae507 100644
--- a/core/src/main/java/org/elasticsearch/cluster/ClusterState.java
+++ b/core/src/main/java/org/elasticsearch/cluster/ClusterState.java
@@ -52,7 +52,10 @@ import org.elasticsearch.discovery.local.LocalDiscovery;
 import org.elasticsearch.discovery.zen.publish.PublishClusterStateAction;
 
 import java.io.IOException;
-import java.util.*;
+import java.util.EnumSet;
+import java.util.HashMap;
+import java.util.Locale;
+import java.util.Map;
 
 /**
  * Represents the current state of the cluster.
@@ -253,7 +256,7 @@ public class ClusterState implements ToXContent, Diffable<ClusterState> {
     }
 
     // Used for testing and logging to determine how this cluster state was send over the wire
-    public boolean wasReadFromDiff() {
+    boolean wasReadFromDiff() {
         return wasReadFromDiff;
     }
 
@@ -293,16 +296,6 @@ public class ClusterState implements ToXContent, Diffable<ClusterState> {
         }
     }
 
-    /**
-     * a cluster state supersedes another state iff they are from the same master and the version this state is higher thant the other state.
-     * <p/>
-     * In essence that means that all the changes from the other cluster state are also reflected by the current one
-     */
-    public boolean supersedes(ClusterState other) {
-        return this.nodes().masterNodeId() != null && this.nodes().masterNodeId().equals(other.nodes().masterNodeId()) && this.version() > other.version();
-
-    }
-
     public enum Metric {
         VERSION("version"),
         MASTER_NODE("master_node"),
@@ -821,7 +814,6 @@ public class ClusterState implements ToXContent, Diffable<ClusterState> {
             builder.fromDiff(true);
             return builder.build();
         }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/cluster/action/index/NodeIndexDeletedAction.java b/core/src/main/java/org/elasticsearch/cluster/action/index/NodeIndexDeletedAction.java
index 769551d..3373263 100644
--- a/core/src/main/java/org/elasticsearch/cluster/action/index/NodeIndexDeletedAction.java
+++ b/core/src/main/java/org/elasticsearch/cluster/action/index/NodeIndexDeletedAction.java
@@ -133,12 +133,12 @@ public class NodeIndexDeletedAction extends AbstractComponent {
         }
     }
 
-    static class NodeIndexDeletedMessage extends TransportRequest {
+    public static class NodeIndexDeletedMessage extends TransportRequest {
 
         String index;
         String nodeId;
 
-        NodeIndexDeletedMessage() {
+        public NodeIndexDeletedMessage() {
         }
 
         NodeIndexDeletedMessage(String index, String nodeId) {
@@ -161,12 +161,12 @@ public class NodeIndexDeletedAction extends AbstractComponent {
         }
     }
 
-    static class NodeIndexStoreDeletedMessage extends TransportRequest {
+    public static class NodeIndexStoreDeletedMessage extends TransportRequest {
 
         String index;
         String nodeId;
 
-        NodeIndexStoreDeletedMessage() {
+        public NodeIndexStoreDeletedMessage() {
         }
 
         NodeIndexStoreDeletedMessage(String index, String nodeId) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/action/index/NodeMappingRefreshAction.java b/core/src/main/java/org/elasticsearch/cluster/action/index/NodeMappingRefreshAction.java
index c6cb7b5..1322915 100644
--- a/core/src/main/java/org/elasticsearch/cluster/action/index/NodeMappingRefreshAction.java
+++ b/core/src/main/java/org/elasticsearch/cluster/action/index/NodeMappingRefreshAction.java
@@ -79,7 +79,7 @@ public class NodeMappingRefreshAction extends AbstractComponent {
         private String[] types;
         private String nodeId;
 
-        NodeMappingRefreshRequest() {
+        public NodeMappingRefreshRequest() {
         }
 
         public NodeMappingRefreshRequest(String index, String indexUUID, String[] types, String nodeId) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java b/core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java
index 41ddb49..7036158 100644
--- a/core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java
+++ b/core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java
@@ -244,7 +244,7 @@ public class ShardStateAction extends AbstractComponent {
         }
     }
 
-    static class ShardRoutingEntry extends TransportRequest {
+    public static class ShardRoutingEntry extends TransportRequest {
 
         ShardRouting shardRouting;
         String indexUUID = IndexMetaData.INDEX_UUID_NA_VALUE;
@@ -253,7 +253,7 @@ public class ShardStateAction extends AbstractComponent {
 
         volatile boolean processed; // state field, no need to serialize
 
-        ShardRoutingEntry() {
+        public ShardRoutingEntry() {
         }
 
         ShardRoutingEntry(ShardRouting shardRouting, String indexUUID, String message, @Nullable Throwable failure) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java b/core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java
index 943bca9..dc4691e 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java
@@ -22,7 +22,6 @@ package org.elasticsearch.cluster.metadata;
 import com.carrotsearch.hppc.cursors.ObjectCursor;
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
 import com.google.common.base.Preconditions;
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.Diff;
@@ -33,7 +32,6 @@ import org.elasticsearch.cluster.block.ClusterBlockLevel;
 import org.elasticsearch.cluster.node.DiscoveryNodeFilters;
 import org.elasticsearch.cluster.routing.HashFunction;
 import org.elasticsearch.cluster.routing.Murmur3HashFunction;
-import org.elasticsearch.common.Classes;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
@@ -218,19 +216,19 @@ public class IndexMetaData implements Diffable<IndexMetaData>, FromXContentBuild
         this.totalNumberOfShards = numberOfShards() * (numberOfReplicas() + 1);
         this.aliases = aliases;
 
-        ImmutableMap<String, String> requireMap = settings.getByPrefix("index.routing.allocation.require.").getAsMap();
+        Map<String, String> requireMap = settings.getByPrefix("index.routing.allocation.require.").getAsMap();
         if (requireMap.isEmpty()) {
             requireFilters = null;
         } else {
             requireFilters = DiscoveryNodeFilters.buildFromKeyValue(AND, requireMap);
         }
-        ImmutableMap<String, String> includeMap = settings.getByPrefix("index.routing.allocation.include.").getAsMap();
+        Map<String, String> includeMap = settings.getByPrefix("index.routing.allocation.include.").getAsMap();
         if (includeMap.isEmpty()) {
             includeFilters = null;
         } else {
             includeFilters = DiscoveryNodeFilters.buildFromKeyValue(OR, includeMap);
         }
-        ImmutableMap<String, String> excludeMap = settings.getByPrefix("index.routing.allocation.exclude.").getAsMap();
+        Map<String, String> excludeMap = settings.getByPrefix("index.routing.allocation.exclude.").getAsMap();
         if (excludeMap.isEmpty()) {
             excludeFilters = null;
         } else {
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java
index c0004c9..376b0d1 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java
@@ -184,11 +184,9 @@ public class AwarenessAllocationDecider extends AllocationDecider {
             // build the count of shards per attribute value
             ObjectIntHashMap<String> shardPerAttribute = new ObjectIntHashMap<>();
             for (ShardRouting assignedShard : allocation.routingNodes().assignedShards(shardRouting)) {
-                // if the shard is relocating, then make sure we count it as part of the node it is relocating to
-                if (assignedShard.relocating()) {
-                    RoutingNode relocationNode = allocation.routingNodes().node(assignedShard.relocatingNodeId());
-                    shardPerAttribute.addTo(relocationNode.node().attributes().get(awarenessAttribute), 1);
-                } else if (assignedShard.started() || assignedShard.initializing()) {
+                if (assignedShard.started() || assignedShard.initializing()) {
+                    // Note: this also counts relocation targets as that will be the new location of the shard.
+                    // Relocation sources should not be counted as the shard is moving away
                     RoutingNode routingNode = allocation.routingNodes().node(assignedShard.currentNodeId());
                     shardPerAttribute.addTo(routingNode.node().attributes().get(awarenessAttribute), 1);
                 }
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DisableAllocationDecider.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DisableAllocationDecider.java
deleted file mode 100644
index bf3c833..0000000
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DisableAllocationDecider.java
+++ /dev/null
@@ -1,133 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cluster.routing.allocation.decider;
-
-import org.elasticsearch.cluster.routing.RoutingNode;
-import org.elasticsearch.cluster.routing.ShardRouting;
-import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.node.settings.NodeSettingsService;
-
-/**
- * This {@link AllocationDecider} prevents cluster-wide shard allocations. The
- * behavior of this {@link AllocationDecider} can be changed in real-time via
- * the cluster settings API. It respects the following settings:
- * <ul>
- * <li><tt>cluster.routing.allocation.disable_new_allocation</tt> - if set to
- * <code>true</code> no new shard-allocation are allowed. Note: this setting is
- * only applied if the allocated shard is a primary and it has not been
- * allocated before the this setting was applied.</li>
- * <p/>
- * <li><tt>cluster.routing.allocation.disable_allocation</tt> - if set to
- * <code>true</code> cluster wide allocations are disabled</li>
- * <p/>
- * <li><tt>cluster.routing.allocation.disable_replica_allocation</tt> - if set
- * to <code>true</code> cluster wide replica allocations are disabled while
- * primary shards can still be allocated</li>
- * </ul>
- * <p/>
- * <p>
- * Note: all of the above settings might be ignored if the allocation happens on
- * a shard that explicitly ignores disabled allocations via
- * {@link RoutingAllocation#ignoreDisable()}. Which is set if allocation are
- * explicit.
- * </p>
- *
- * @deprecated In favour for {@link EnableAllocationDecider}.
- */
-@Deprecated
-public class DisableAllocationDecider extends AllocationDecider {
-
-    public static final String NAME = "disable";
-
-    public static final String CLUSTER_ROUTING_ALLOCATION_DISABLE_NEW_ALLOCATION = "cluster.routing.allocation.disable_new_allocation";
-    public static final String CLUSTER_ROUTING_ALLOCATION_DISABLE_ALLOCATION = "cluster.routing.allocation.disable_allocation";
-    public static final String CLUSTER_ROUTING_ALLOCATION_DISABLE_REPLICA_ALLOCATION = "cluster.routing.allocation.disable_replica_allocation";
-
-    public static final String INDEX_ROUTING_ALLOCATION_DISABLE_NEW_ALLOCATION = "index.routing.allocation.disable_new_allocation";
-    public static final String INDEX_ROUTING_ALLOCATION_DISABLE_ALLOCATION = "index.routing.allocation.disable_allocation";
-    public static final String INDEX_ROUTING_ALLOCATION_DISABLE_REPLICA_ALLOCATION = "index.routing.allocation.disable_replica_allocation";
-
-    class ApplySettings implements NodeSettingsService.Listener {
-        @Override
-        public void onRefreshSettings(Settings settings) {
-            boolean disableNewAllocation = settings.getAsBoolean(CLUSTER_ROUTING_ALLOCATION_DISABLE_NEW_ALLOCATION, DisableAllocationDecider.this.disableNewAllocation);
-            if (disableNewAllocation != DisableAllocationDecider.this.disableNewAllocation) {
-                logger.info("updating [cluster.routing.allocation.disable_new_allocation] from [{}] to [{}]", DisableAllocationDecider.this.disableNewAllocation, disableNewAllocation);
-                DisableAllocationDecider.this.disableNewAllocation = disableNewAllocation;
-            }
-
-            boolean disableAllocation = settings.getAsBoolean(CLUSTER_ROUTING_ALLOCATION_DISABLE_ALLOCATION, DisableAllocationDecider.this.disableAllocation);
-            if (disableAllocation != DisableAllocationDecider.this.disableAllocation) {
-                logger.info("updating [cluster.routing.allocation.disable_allocation] from [{}] to [{}]", DisableAllocationDecider.this.disableAllocation, disableAllocation);
-                DisableAllocationDecider.this.disableAllocation = disableAllocation;
-            }
-
-            boolean disableReplicaAllocation = settings.getAsBoolean(CLUSTER_ROUTING_ALLOCATION_DISABLE_REPLICA_ALLOCATION, DisableAllocationDecider.this.disableReplicaAllocation);
-            if (disableReplicaAllocation != DisableAllocationDecider.this.disableReplicaAllocation) {
-                logger.info("updating [cluster.routing.allocation.disable_replica_allocation] from [{}] to [{}]", DisableAllocationDecider.this.disableReplicaAllocation, disableReplicaAllocation);
-                DisableAllocationDecider.this.disableReplicaAllocation = disableReplicaAllocation;
-            }
-        }
-    }
-
-    private volatile boolean disableNewAllocation;
-    private volatile boolean disableAllocation;
-    private volatile boolean disableReplicaAllocation;
-
-    @Inject
-    public DisableAllocationDecider(Settings settings, NodeSettingsService nodeSettingsService) {
-        super(settings);
-        this.disableNewAllocation = settings.getAsBoolean(CLUSTER_ROUTING_ALLOCATION_DISABLE_NEW_ALLOCATION, false);
-        this.disableAllocation = settings.getAsBoolean(CLUSTER_ROUTING_ALLOCATION_DISABLE_ALLOCATION, false);
-        this.disableReplicaAllocation = settings.getAsBoolean(CLUSTER_ROUTING_ALLOCATION_DISABLE_REPLICA_ALLOCATION, false);
-
-        nodeSettingsService.addListener(new ApplySettings());
-    }
-
-    @Override
-    public Decision canAllocate(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) {
-        if (allocation.ignoreDisable()) {
-            return allocation.decision(Decision.YES, NAME, "allocation disabling is ignored");
-        }
-        Settings indexSettings = allocation.routingNodes().metaData().index(shardRouting.index()).settings();
-        if (shardRouting.primary() && shardRouting.allocatedPostIndexCreate() == false) {
-            // if its primary, and it hasn't been allocated post API (meaning its a "fresh newly created shard"), only disable allocation
-            // on a special disable allocation flag
-            if (indexSettings.getAsBoolean(INDEX_ROUTING_ALLOCATION_DISABLE_NEW_ALLOCATION, disableNewAllocation)) {
-                return allocation.decision(Decision.NO, NAME, "new primary allocation is disabled");
-            } else {
-                return allocation.decision(Decision.YES, NAME, "new primary allocation is enabled");
-            }
-        }
-        if (indexSettings.getAsBoolean(INDEX_ROUTING_ALLOCATION_DISABLE_ALLOCATION, disableAllocation)) {
-            return allocation.decision(Decision.NO, NAME, "all allocation is disabled");
-        }
-        if (indexSettings.getAsBoolean(INDEX_ROUTING_ALLOCATION_DISABLE_REPLICA_ALLOCATION, disableReplicaAllocation)) {
-            if (shardRouting.primary()) {
-                return allocation.decision(Decision.YES, NAME, "primary allocation is enabled");
-            } else {
-                return allocation.decision(Decision.NO, NAME, "replica allocation is disabled");
-            }
-        }
-        return allocation.decision(Decision.YES, NAME, "all allocation is enabled");
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/FilterAllocationDecider.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/FilterAllocationDecider.java
index ce0a702..138a08b 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/FilterAllocationDecider.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/FilterAllocationDecider.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.cluster.routing.allocation.decider;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.node.DiscoveryNodeFilters;
 import org.elasticsearch.cluster.routing.RoutingNode;
@@ -29,6 +28,8 @@ import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.node.settings.NodeSettingsService;
 
+import java.util.Map;
+
 import static org.elasticsearch.cluster.node.DiscoveryNodeFilters.OpType.AND;
 import static org.elasticsearch.cluster.node.DiscoveryNodeFilters.OpType.OR;
 
@@ -77,19 +78,19 @@ public class FilterAllocationDecider extends AllocationDecider {
     @Inject
     public FilterAllocationDecider(Settings settings, NodeSettingsService nodeSettingsService) {
         super(settings);
-        ImmutableMap<String, String> requireMap = settings.getByPrefix(CLUSTER_ROUTING_REQUIRE_GROUP).getAsMap();
+        Map<String, String> requireMap = settings.getByPrefix(CLUSTER_ROUTING_REQUIRE_GROUP).getAsMap();
         if (requireMap.isEmpty()) {
             clusterRequireFilters = null;
         } else {
             clusterRequireFilters = DiscoveryNodeFilters.buildFromKeyValue(AND, requireMap);
         }
-        ImmutableMap<String, String> includeMap = settings.getByPrefix(CLUSTER_ROUTING_INCLUDE_GROUP).getAsMap();
+        Map<String, String> includeMap = settings.getByPrefix(CLUSTER_ROUTING_INCLUDE_GROUP).getAsMap();
         if (includeMap.isEmpty()) {
             clusterIncludeFilters = null;
         } else {
             clusterIncludeFilters = DiscoveryNodeFilters.buildFromKeyValue(OR, includeMap);
         }
-        ImmutableMap<String, String> excludeMap = settings.getByPrefix(CLUSTER_ROUTING_EXCLUDE_GROUP).getAsMap();
+        Map<String, String> excludeMap = settings.getByPrefix(CLUSTER_ROUTING_EXCLUDE_GROUP).getAsMap();
         if (excludeMap.isEmpty()) {
             clusterExcludeFilters = null;
         } else {
@@ -148,15 +149,15 @@ public class FilterAllocationDecider extends AllocationDecider {
     class ApplySettings implements NodeSettingsService.Listener {
         @Override
         public void onRefreshSettings(Settings settings) {
-            ImmutableMap<String, String> requireMap = settings.getByPrefix(CLUSTER_ROUTING_REQUIRE_GROUP).getAsMap();
+            Map<String, String> requireMap = settings.getByPrefix(CLUSTER_ROUTING_REQUIRE_GROUP).getAsMap();
             if (!requireMap.isEmpty()) {
                 clusterRequireFilters = DiscoveryNodeFilters.buildFromKeyValue(AND, requireMap);
             }
-            ImmutableMap<String, String> includeMap = settings.getByPrefix(CLUSTER_ROUTING_INCLUDE_GROUP).getAsMap();
+            Map<String, String> includeMap = settings.getByPrefix(CLUSTER_ROUTING_INCLUDE_GROUP).getAsMap();
             if (!includeMap.isEmpty()) {
                 clusterIncludeFilters = DiscoveryNodeFilters.buildFromKeyValue(OR, includeMap);
             }
-            ImmutableMap<String, String> excludeMap = settings.getByPrefix(CLUSTER_ROUTING_EXCLUDE_GROUP).getAsMap();
+            Map<String, String> excludeMap = settings.getByPrefix(CLUSTER_ROUTING_EXCLUDE_GROUP).getAsMap();
             if (!excludeMap.isEmpty()) {
                 clusterExcludeFilters = DiscoveryNodeFilters.buildFromKeyValue(OR, excludeMap);
             }
diff --git a/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java b/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java
index 08cdbbb..b992c36 100644
--- a/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java
@@ -40,6 +40,7 @@ import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.text.StringText;
+import org.elasticsearch.common.transport.BoundTransportAddress;
 import org.elasticsearch.common.transport.TransportAddress;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.*;
@@ -481,14 +482,8 @@ public class InternalClusterService extends AbstractLifecycleComponent<ClusterSe
                 // we publish here before we send a notification to all the listeners, since if it fails
                 // we don't want to notify
                 if (newClusterState.nodes().localNodeMaster()) {
-                    logger.debug("publishing cluster state version [{}]", newClusterState.version());
-                    try {
-                        discoveryService.publish(clusterChangedEvent, ackListener);
-                    } catch (Discovery.FailedToCommitClusterStateException t) {
-                        logger.warn("failing [{}]: failed to commit cluster state version [{}]", t, source, newClusterState.version());
-                        updateTask.onFailure(source, t);
-                        return;
-                    }
+                    logger.debug("publishing cluster state version {}", newClusterState.version());
+                    discoveryService.publish(clusterChangedEvent, ackListener);
                 }
 
                 // update the current cluster state
diff --git a/core/src/main/java/org/elasticsearch/common/blobstore/BlobContainer.java b/core/src/main/java/org/elasticsearch/common/blobstore/BlobContainer.java
index ce36c24..5d00e36 100644
--- a/core/src/main/java/org/elasticsearch/common/blobstore/BlobContainer.java
+++ b/core/src/main/java/org/elasticsearch/common/blobstore/BlobContainer.java
@@ -19,11 +19,10 @@
 
 package org.elasticsearch.common.blobstore;
 
-import com.google.common.collect.ImmutableMap;
+import org.elasticsearch.common.bytes.BytesReference;
 
 import java.io.IOException;
 import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.Collection;
 import java.util.Map;
 
@@ -37,14 +36,19 @@ public interface BlobContainer {
     boolean blobExists(String blobName);
 
     /**
-     * Creates a new {@link InputStream} for the given blob name
+     * Creates a new InputStream for the given blob name
      */
-    InputStream openInput(String blobName) throws IOException;
+    InputStream readBlob(String blobName) throws IOException;
 
     /**
-     * Creates a new OutputStream for the given blob name
+     * Reads blob content from the input stream and writes it to the blob store
      */
-    OutputStream createOutput(String blobName) throws IOException;
+    void writeBlob(String blobName, InputStream inputStream, long blobSize) throws IOException;
+
+    /**
+     * Writes bytes to the blob
+     */
+    void writeBlob(String blobName, BytesReference bytes) throws IOException;
 
     /**
      * Deletes a blob with giving name.
diff --git a/core/src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobContainer.java b/core/src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobContainer.java
index e61ace1..5b9a8bd 100644
--- a/core/src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobContainer.java
+++ b/core/src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobContainer.java
@@ -25,7 +25,9 @@ import org.elasticsearch.common.blobstore.BlobMetaData;
 import org.elasticsearch.common.blobstore.BlobPath;
 import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;
 import org.elasticsearch.common.blobstore.support.PlainBlobMetaData;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.collect.MapBuilder;
+import org.elasticsearch.common.io.Streams;
 
 import java.io.*;
 import java.nio.file.DirectoryStream;
@@ -83,25 +85,28 @@ public class FsBlobContainer extends AbstractBlobContainer {
     }
 
     @Override
-    public InputStream openInput(String name) throws IOException {
+    public InputStream readBlob(String name) throws IOException {
         return new BufferedInputStream(Files.newInputStream(path.resolve(name)), blobStore.bufferSizeInBytes());
     }
 
     @Override
-    public OutputStream createOutput(String blobName) throws IOException {
+    public void writeBlob(String blobName, InputStream inputStream, long blobSize) throws IOException {
         final Path file = path.resolve(blobName);
-        return new BufferedOutputStream(new FilterOutputStream(Files.newOutputStream(file)) {
-
-            @Override // FilterOutputStream#write(byte[] b, int off, int len) is trappy writes every single byte
-            public void write(byte[] b, int off, int len) throws IOException { out.write(b, off, len);}
+        try (OutputStream outputStream = Files.newOutputStream(file)) {
+            Streams.copy(inputStream, outputStream, new byte[blobStore.bufferSizeInBytes()]);
+        }
+        IOUtils.fsync(file, false);
+        IOUtils.fsync(path, true);
+    }
 
-            @Override
-            public void close() throws IOException {
-                super.close();
-                IOUtils.fsync(file, false);
-                IOUtils.fsync(path, true);
-            }
-        }, blobStore.bufferSizeInBytes());
+    @Override
+    public void writeBlob(String blobName, BytesReference data) throws IOException {
+        final Path file = path.resolve(blobName);
+        try (OutputStream outputStream = Files.newOutputStream(file)) {
+            data.writeTo(outputStream);
+        }
+        IOUtils.fsync(file, false);
+        IOUtils.fsync(path, true);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/common/blobstore/support/AbstractBlobContainer.java b/core/src/main/java/org/elasticsearch/common/blobstore/support/AbstractBlobContainer.java
index 44f44f2..9166491 100644
--- a/core/src/main/java/org/elasticsearch/common/blobstore/support/AbstractBlobContainer.java
+++ b/core/src/main/java/org/elasticsearch/common/blobstore/support/AbstractBlobContainer.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.common.blobstore.support;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.blobstore.BlobContainer;
 import org.elasticsearch.common.blobstore.BlobMetaData;
 import org.elasticsearch.common.blobstore.BlobPath;
diff --git a/core/src/main/java/org/elasticsearch/common/blobstore/support/AbstractLegacyBlobContainer.java b/core/src/main/java/org/elasticsearch/common/blobstore/support/AbstractLegacyBlobContainer.java
new file mode 100644
index 0000000..9df7f26
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/common/blobstore/support/AbstractLegacyBlobContainer.java
@@ -0,0 +1,78 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.blobstore.support;
+
+import org.elasticsearch.common.blobstore.BlobPath;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.io.Streams;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.OutputStream;
+
+/**
+ * Temporary compatibility interface.
+ *
+ * This class should be removed after S3 and Azure containers migrate to the new model
+ */
+@Deprecated
+public abstract class AbstractLegacyBlobContainer extends AbstractBlobContainer {
+
+    protected AbstractLegacyBlobContainer(BlobPath path) {
+        super(path);
+    }
+
+    /**
+     * Creates a new {@link InputStream} for the given blob name
+     * <p/>
+     * This method is deprecated and is used only for compatibility with older blob containers
+     * The new blob containers should use readBlob/writeBlob methods instead
+     */
+    @Deprecated
+    protected abstract InputStream openInput(String blobName) throws IOException;
+
+    /**
+     * Creates a new OutputStream for the given blob name
+     * <p/>
+     * This method is deprecated and is used only for compatibility with older blob containers
+     * The new blob containers should override readBlob/writeBlob methods instead
+     */
+    @Deprecated
+    protected abstract OutputStream createOutput(String blobName) throws IOException;
+
+    @Override
+    public InputStream readBlob(String blobName) throws IOException {
+        return openInput(blobName);
+    }
+
+    @Override
+    public void writeBlob(String blobName, InputStream inputStream, long blobSize) throws IOException {
+        try (OutputStream stream = createOutput(blobName)) {
+            Streams.copy(inputStream, stream);
+        }
+    }
+
+    @Override
+    public void writeBlob(String blobName, BytesReference data) throws IOException {
+        try (OutputStream stream = createOutput(blobName)) {
+            data.writeTo(stream);
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/common/blobstore/url/URLBlobContainer.java b/core/src/main/java/org/elasticsearch/common/blobstore/url/URLBlobContainer.java
index ec3f710..9dfaa9c 100644
--- a/core/src/main/java/org/elasticsearch/common/blobstore/url/URLBlobContainer.java
+++ b/core/src/main/java/org/elasticsearch/common/blobstore/url/URLBlobContainer.java
@@ -23,11 +23,11 @@ import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.blobstore.BlobMetaData;
 import org.elasticsearch.common.blobstore.BlobPath;
 import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;
+import org.elasticsearch.common.bytes.BytesReference;
 
 import java.io.BufferedInputStream;
 import java.io.IOException;
 import java.io.InputStream;
-import java.io.OutputStream;
 import java.net.URL;
 
 /**
@@ -99,12 +99,17 @@ public class URLBlobContainer extends AbstractBlobContainer {
     }
 
     @Override
-    public InputStream openInput(String name) throws IOException {
+    public InputStream readBlob(String name) throws IOException {
         return new BufferedInputStream(new URL(path, name).openStream(), blobStore.bufferSizeInBytes());
     }
 
     @Override
-    public OutputStream createOutput(String blobName) throws IOException {
+    public void writeBlob(String blobName, InputStream inputStream, long blobSize) throws IOException {
+        throw new UnsupportedOperationException("URL repository doesn't support this operation");
+    }
+
+    @Override
+    public void writeBlob(String blobName, BytesReference data) throws IOException {
         throw new UnsupportedOperationException("URL repository doesn't support this operation");
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/geo/GeoHashUtils.java b/core/src/main/java/org/elasticsearch/common/geo/GeoHashUtils.java
deleted file mode 100644
index de37ddb..0000000
--- a/core/src/main/java/org/elasticsearch/common/geo/GeoHashUtils.java
+++ /dev/null
@@ -1,481 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.elasticsearch.common.geo;
-
-
-import java.util.ArrayList;
-import java.util.Collection;
-
-
-/**
- * Utilities for encoding and decoding geohashes. Based on
- * http://en.wikipedia.org/wiki/Geohash.
- */
-// LUCENE MONITOR: monitor against spatial package
-// replaced with native DECODE_MAP
-public class GeoHashUtils {
-
-    private static final char[] BASE_32 = {'0', '1', '2', '3', '4', '5', '6',
-            '7', '8', '9', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'j', 'k', 'm', 'n',
-            'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'};
-
-    public static final int PRECISION = 12;
-    private static final int[] BITS = {16, 8, 4, 2, 1};
-
-    private GeoHashUtils() {
-    }
-
-    public static String encode(double latitude, double longitude) {
-        return encode(latitude, longitude, PRECISION);
-    }
-
-    /**
-     * Encodes the given latitude and longitude into a geohash
-     *
-     * @param latitude  Latitude to encode
-     * @param longitude Longitude to encode
-     * @return Geohash encoding of the longitude and latitude
-     */
-    public static String encode(double latitude, double longitude, int precision) {
-//        double[] latInterval = {-90.0, 90.0};
-//        double[] lngInterval = {-180.0, 180.0};
-        double latInterval0 = -90.0;
-        double latInterval1 = 90.0;
-        double lngInterval0 = -180.0;
-        double lngInterval1 = 180.0;
-
-        final StringBuilder geohash = new StringBuilder();
-        boolean isEven = true;
-
-        int bit = 0;
-        int ch = 0;
-
-        while (geohash.length() < precision) {
-            double mid = 0.0;
-            if (isEven) {
-//                mid = (lngInterval[0] + lngInterval[1]) / 2D;
-                mid = (lngInterval0 + lngInterval1) / 2D;
-                if (longitude > mid) {
-                    ch |= BITS[bit];
-//                    lngInterval[0] = mid;
-                    lngInterval0 = mid;
-                } else {
-//                    lngInterval[1] = mid;
-                    lngInterval1 = mid;
-                }
-            } else {
-//                mid = (latInterval[0] + latInterval[1]) / 2D;
-                mid = (latInterval0 + latInterval1) / 2D;
-                if (latitude > mid) {
-                    ch |= BITS[bit];
-//                    latInterval[0] = mid;
-                    latInterval0 = mid;
-                } else {
-//                    latInterval[1] = mid;
-                    latInterval1 = mid;
-                }
-            }
-
-            isEven = !isEven;
-
-            if (bit < 4) {
-                bit++;
-            } else {
-                geohash.append(BASE_32[ch]);
-                bit = 0;
-                ch = 0;
-            }
-        }
-
-        return geohash.toString();
-    }
-
-    private static final char encode(int x, int y) {
-        return BASE_32[((x & 1) + ((y & 1) * 2) + ((x & 2) * 2) + ((y & 2) * 4) + ((x & 4) * 4)) % 32];
-    }
-
-    /**
-     * Calculate all neighbors of a given geohash cell.
-     *
-     * @param geohash Geohash of the defined cell
-     * @return geohashes of all neighbor cells
-     */
-    public static Collection<? extends CharSequence> neighbors(String geohash) {
-        return addNeighbors(geohash, geohash.length(), new ArrayList<CharSequence>(8));
-    }
-    
-    /**
-     * Calculate the geohash of a neighbor of a geohash
-     *
-     * @param geohash the geohash of a cell
-     * @param level   level of the geohash
-     * @param dx      delta of the first grid coordinate (must be -1, 0 or +1)
-     * @param dy      delta of the second grid coordinate (must be -1, 0 or +1)
-     * @return geohash of the defined cell
-     */
-    private final static String neighbor(String geohash, int level, int dx, int dy) {
-        int cell = decode(geohash.charAt(level - 1));
-
-        // Decoding the Geohash bit pattern to determine grid coordinates
-        int x0 = cell & 1;  // first bit of x
-        int y0 = cell & 2;  // first bit of y
-        int x1 = cell & 4;  // second bit of x
-        int y1 = cell & 8;  // second bit of y
-        int x2 = cell & 16; // third bit of x
-
-        // combine the bitpattern to grid coordinates.
-        // note that the semantics of x and y are swapping
-        // on each level
-        int x = x0 + (x1 / 2) + (x2 / 4);
-        int y = (y0 / 2) + (y1 / 4);
-
-        if (level == 1) {
-            // Root cells at north (namely "bcfguvyz") or at
-            // south (namely "0145hjnp") do not have neighbors
-            // in north/south direction
-            if ((dy < 0 && y == 0) || (dy > 0 && y == 3)) {
-                return null;
-            } else {
-                return Character.toString(encode(x + dx, y + dy));
-            }
-        } else {
-            // define grid coordinates for next level
-            final int nx = ((level % 2) == 1) ? (x + dx) : (x + dy);
-            final int ny = ((level % 2) == 1) ? (y + dy) : (y + dx);
-
-            // if the defined neighbor has the same parent a the current cell
-            // encode the cell directly. Otherwise find the cell next to this
-            // cell recursively. Since encoding wraps around within a cell
-            // it can be encoded here.
-            // xLimit and YLimit must always be respectively 7 and 3
-            // since x and y semantics are swapping on each level.
-            if (nx >= 0 && nx <= 7 && ny >= 0 && ny <= 3) {
-                return geohash.substring(0, level - 1) + encode(nx, ny);
-            } else {
-                String neighbor = neighbor(geohash, level - 1, dx, dy);
-                if(neighbor != null) {
-                    return neighbor + encode(nx, ny); 
-                } else {
-                    return null;
-                }
-            }
-        }
-    }
-
-    /**
-     * Add all geohashes of the cells next to a given geohash to a list.
-     *
-     * @param geohash   Geohash of a specified cell
-     * @param neighbors list to add the neighbors to
-     * @return the given list
-     */
-    public static final <E extends Collection<? super String>> E addNeighbors(String geohash, E neighbors) {
-        return addNeighbors(geohash, geohash.length(), neighbors);
-    }
-    
-    /**
-     * Add all geohashes of the cells next to a given geohash to a list.
-     *
-     * @param geohash   Geohash of a specified cell
-     * @param length    level of the given geohash
-     * @param neighbors list to add the neighbors to
-     * @return the given list
-     */
-    public static final <E extends Collection<? super String>> E addNeighbors(String geohash, int length, E neighbors) {
-        String south = neighbor(geohash, length, 0, -1);
-        String north = neighbor(geohash, length, 0, +1);
-        if (north != null) {
-            neighbors.add(neighbor(north, length, -1, 0));
-            neighbors.add(north);
-            neighbors.add(neighbor(north, length, +1, 0));
-        }
-
-        neighbors.add(neighbor(geohash, length, -1, 0));
-        neighbors.add(neighbor(geohash, length, +1, 0));
-
-        if (south != null) {
-            neighbors.add(neighbor(south, length, -1, 0));
-            neighbors.add(south);
-            neighbors.add(neighbor(south, length, +1, 0));
-        }
-
-        return neighbors;
-    }
-
-    private static final int decode(char geo) {
-        switch (geo) {
-            case '0':
-                return 0;
-            case '1':
-                return 1;
-            case '2':
-                return 2;
-            case '3':
-                return 3;
-            case '4':
-                return 4;
-            case '5':
-                return 5;
-            case '6':
-                return 6;
-            case '7':
-                return 7;
-            case '8':
-                return 8;
-            case '9':
-                return 9;
-            case 'b':
-                return 10;
-            case 'c':
-                return 11;
-            case 'd':
-                return 12;
-            case 'e':
-                return 13;
-            case 'f':
-                return 14;
-            case 'g':
-                return 15;
-            case 'h':
-                return 16;
-            case 'j':
-                return 17;
-            case 'k':
-                return 18;
-            case 'm':
-                return 19;
-            case 'n':
-                return 20;
-            case 'p':
-                return 21;
-            case 'q':
-                return 22;
-            case 'r':
-                return 23;
-            case 's':
-                return 24;
-            case 't':
-                return 25;
-            case 'u':
-                return 26;
-            case 'v':
-                return 27;
-            case 'w':
-                return 28;
-            case 'x':
-                return 29;
-            case 'y':
-                return 30;
-            case 'z':
-                return 31;
-            default:
-                throw new IllegalArgumentException("the character '" + geo + "' is not a valid geohash character");
-        }
-    }
-
-    /**
-     * Decodes the given geohash
-     *
-     * @param geohash Geohash to decocde
-     * @return {@link GeoPoint} at the center of cell, given by the geohash
-     */
-    public static GeoPoint decode(String geohash) {
-        return decode(geohash, new GeoPoint());
-    }
-
-    /**
-     * Decodes the given geohash into a latitude and longitude
-     *
-     * @param geohash Geohash to decocde
-     * @return the given {@link GeoPoint} reseted to the center of
-     *         cell, given by the geohash
-     */
-    public static GeoPoint decode(String geohash, GeoPoint ret) {
-        double[] interval = decodeCell(geohash);
-        return ret.reset((interval[0] + interval[1]) / 2D, (interval[2] + interval[3]) / 2D);
-    }
-
-    private static double[] decodeCell(String geohash) {
-        double[] interval = {-90.0, 90.0, -180.0, 180.0};
-        boolean isEven = true;
-
-        for (int i = 0; i < geohash.length(); i++) {
-            final int cd = decode(geohash.charAt(i));
-
-            for (int mask : BITS) {
-                if (isEven) {
-                    if ((cd & mask) != 0) {
-                        interval[2] = (interval[2] + interval[3]) / 2D;
-                    } else {
-                        interval[3] = (interval[2] + interval[3]) / 2D;
-                    }
-                } else {
-                    if ((cd & mask) != 0) {
-                        interval[0] = (interval[0] + interval[1]) / 2D;
-                    } else {
-                        interval[1] = (interval[0] + interval[1]) / 2D;
-                    }
-                }
-                isEven = !isEven;
-            }
-        }
-        return interval;
-    }
-    
-    //========== long-based encodings for geohashes ========================================
-
-
-    /**
-     * Encodes latitude and longitude information into a single long with variable precision.
-     * Up to 12 levels of precision are supported which should offer sub-metre resolution.
-     *
-     * @param latitude
-     * @param longitude
-     * @param precision The required precision between 1 and 12
-     * @return A single long where 4 bits are used for holding the precision and the remaining 
-     * 60 bits are reserved for 5 bit cell identifiers giving up to 12 layers. 
-     */
-    public static long encodeAsLong(double latitude, double longitude, int precision) {
-        if((precision>12)||(precision<1))
-        {
-            throw new IllegalArgumentException("Illegal precision length of "+precision+
-                    ". Long-based geohashes only support precisions between 1 and 12");
-        }
-        double latInterval0 = -90.0;
-        double latInterval1 = 90.0;
-        double lngInterval0 = -180.0;
-        double lngInterval1 = 180.0;
-
-        long geohash = 0l;
-        boolean isEven = true;
-
-        int bit = 0;
-        int ch = 0;
-
-        int geohashLength=0;
-        while (geohashLength < precision) {
-            double mid = 0.0;
-            if (isEven) {
-                mid = (lngInterval0 + lngInterval1) / 2D;
-                if (longitude > mid) {
-                    ch |= BITS[bit];
-                    lngInterval0 = mid;
-                } else {
-                    lngInterval1 = mid;
-                }
-            } else {
-                mid = (latInterval0 + latInterval1) / 2D;
-                if (latitude > mid) {
-                    ch |= BITS[bit];
-                    latInterval0 = mid;
-                } else {
-                    latInterval1 = mid;
-                }
-            }
-
-            isEven = !isEven;
-
-            if (bit < 4) {
-                bit++;
-            } else {
-                geohashLength++;
-                geohash|=ch;
-                if(geohashLength<precision){
-                    geohash<<=5;
-                }
-                bit = 0;
-                ch = 0;
-            }
-        }
-        geohash<<=4;
-        geohash|=precision;
-        return geohash;
-    }
-    
-    /**
-     * Formats a geohash held as a long as a more conventional 
-     * String-based geohash
-     * @param geohashAsLong a geohash encoded as a long
-     * @return A traditional base32-based String representation of a geohash 
-     */
-    public static String toString(long geohashAsLong)
-    {
-        int precision = (int) (geohashAsLong&15);
-        char[] chars = new char[precision];
-        geohashAsLong >>= 4;
-        for (int i = precision - 1; i >= 0 ; i--) {
-            chars[i] =  BASE_32[(int) (geohashAsLong & 31)];
-            geohashAsLong >>= 5;
-        }
-        return new String(chars);        
-    }
-
-    
-    
-    public static GeoPoint decode(long geohash) {
-        GeoPoint point = new GeoPoint();
-        decode(geohash, point);
-        return point;
-    }    
-    
-    /**
-     * Decodes the given long-format geohash into a latitude and longitude
-     *
-     * @param geohash long format Geohash to decode
-     * @param ret The Geopoint into which the latitude and longitude will be stored
-     */
-    public static void decode(long geohash, GeoPoint ret) {
-        double[] interval = decodeCell(geohash);
-        ret.reset((interval[0] + interval[1]) / 2D, (interval[2] + interval[3]) / 2D);
-
-    }    
-    
-    private static double[] decodeCell(long geohash) {
-        double[] interval = {-90.0, 90.0, -180.0, 180.0};
-        boolean isEven = true;
-        
-        int precision= (int) (geohash&15);
-        geohash>>=4;
-        int[]cds=new int[precision];
-        for (int i = precision-1; i >=0 ; i--) {            
-            cds[i] = (int) (geohash&31);
-            geohash>>=5;
-        }
-
-        for (int i = 0; i <cds.length ; i++) {            
-            final int cd = cds[i];
-            for (int mask : BITS) {
-                if (isEven) {
-                    if ((cd & mask) != 0) {
-                        interval[2] = (interval[2] + interval[3]) / 2D;
-                    } else {
-                        interval[3] = (interval[2] + interval[3]) / 2D;
-                    }
-                } else {
-                    if ((cd & mask) != 0) {
-                        interval[0] = (interval[0] + interval[1]) / 2D;
-                    } else {
-                        interval[1] = (interval[0] + interval[1]) / 2D;
-                    }
-                }
-                isEven = !isEven;
-            }
-        }
-        return interval;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/common/geo/GeoPoint.java b/core/src/main/java/org/elasticsearch/common/geo/GeoPoint.java
index 59d1356..6b4ffd2 100644
--- a/core/src/main/java/org/elasticsearch/common/geo/GeoPoint.java
+++ b/core/src/main/java/org/elasticsearch/common/geo/GeoPoint.java
@@ -20,6 +20,10 @@
 package org.elasticsearch.common.geo;
 
 
+import org.apache.lucene.util.BitUtil;
+import org.apache.lucene.util.XGeoHashUtils;
+import org.apache.lucene.util.XGeoUtils;
+
 /**
  *
  */
@@ -27,6 +31,7 @@ public final class GeoPoint {
 
     private double lat;
     private double lon;
+    private final static double TOLERANCE = XGeoUtils.TOLERANCE;
 
     public GeoPoint() {
     }
@@ -34,7 +39,7 @@ public final class GeoPoint {
     /**
      * Create a new Geopointform a string. This String must either be a geohash
      * or a lat-lon tuple.
-     *   
+     *
      * @param value String to create the point from
      */
     public GeoPoint(String value) {
@@ -73,11 +78,22 @@ public final class GeoPoint {
         return this;
     }
 
-    public GeoPoint resetFromGeoHash(String hash) {
-        GeoHashUtils.decode(hash, this);
+    public GeoPoint resetFromIndexHash(long hash) {
+        lon = XGeoUtils.mortonUnhashLon(hash);
+        lat = XGeoUtils.mortonUnhashLat(hash);
         return this;
     }
 
+    public GeoPoint resetFromGeoHash(String geohash) {
+        final long hash = XGeoHashUtils.mortonEncode(geohash);
+        return this.reset(XGeoUtils.mortonUnhashLat(hash), XGeoUtils.mortonUnhashLon(hash));
+    }
+
+    public GeoPoint resetFromGeoHash(long geohashLong) {
+        final int level = (int)(12 - (geohashLong&15));
+        return this.resetFromIndexHash(BitUtil.flipFlop((geohashLong >>> 4) << ((level * 5) + 2)));
+    }
+
     public final double lat() {
         return this.lat;
     }
@@ -95,11 +111,11 @@ public final class GeoPoint {
     }
 
     public final String geohash() {
-        return GeoHashUtils.encode(lat, lon);
+        return XGeoHashUtils.stringEncode(lon, lat);
     }
 
     public final String getGeohash() {
-        return GeoHashUtils.encode(lat, lon);
+        return XGeoHashUtils.stringEncode(lon, lat);
     }
 
     @Override
@@ -107,10 +123,14 @@ public final class GeoPoint {
         if (this == o) return true;
         if (o == null || getClass() != o.getClass()) return false;
 
-        GeoPoint geoPoint = (GeoPoint) o;
+        final GeoPoint geoPoint = (GeoPoint) o;
+        final double lonCompare = geoPoint.lon - lon;
+        final double latCompare = geoPoint.lat - lat;
 
-        if (Double.compare(geoPoint.lat, lat) != 0) return false;
-        if (Double.compare(geoPoint.lon, lon) != 0) return false;
+        if ((lonCompare < -TOLERANCE || lonCompare > TOLERANCE)
+                || (latCompare < -TOLERANCE || latCompare > TOLERANCE)) {
+            return false;
+        }
 
         return true;
     }
@@ -136,4 +156,16 @@ public final class GeoPoint {
         point.resetFromString(latLon);
         return point;
     }
-}
+
+    public static GeoPoint fromGeohash(String geohash) {
+        return new GeoPoint().resetFromGeoHash(geohash);
+    }
+
+    public static GeoPoint fromGeohash(long geohashLong) {
+        return new GeoPoint().resetFromGeoHash(geohashLong);
+    }
+
+    public static GeoPoint fromIndexLong(long indexLong) {
+        return new GeoPoint().resetFromIndexHash(indexLong);
+    }
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/common/inject/DefaultConstructionProxyFactory.java b/core/src/main/java/org/elasticsearch/common/inject/DefaultConstructionProxyFactory.java
index 52ad8fb..437ad03 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/DefaultConstructionProxyFactory.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/DefaultConstructionProxyFactory.java
@@ -43,12 +43,6 @@ class DefaultConstructionProxyFactory<T> implements ConstructionProxyFactory<T>
         @SuppressWarnings("unchecked") // the injection point is for a constructor of T
         final Constructor<T> constructor = (Constructor<T>) injectionPoint.getMember();
 
-        // Use FastConstructor if the constructor is public.
-        if (Modifier.isPublic(constructor.getModifiers())) {
-        } else {
-            constructor.setAccessible(true);
-        }
-
         return new ConstructionProxy<T>() {
             @Override
             public T newInstance(Object... arguments) throws InvocationTargetException {
@@ -57,7 +51,7 @@ class DefaultConstructionProxyFactory<T> implements ConstructionProxyFactory<T>
                 } catch (InstantiationException e) {
                     throw new AssertionError(e); // shouldn't happen, we know this is a concrete type
                 } catch (IllegalAccessException e) {
-                    throw new AssertionError(e); // a security manager is blocking us, we're hosed
+                    throw new AssertionError("Wrong access modifiers on " + constructor, e); // a security manager is blocking us, we're hosed
                 }
             }
 
diff --git a/core/src/main/java/org/elasticsearch/common/inject/SingleFieldInjector.java b/core/src/main/java/org/elasticsearch/common/inject/SingleFieldInjector.java
index dd0e09e..a1375de 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/SingleFieldInjector.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/SingleFieldInjector.java
@@ -39,9 +39,6 @@ class SingleFieldInjector implements SingleMemberInjector {
         this.injectionPoint = injectionPoint;
         this.field = (Field) injectionPoint.getMember();
         this.dependency = injectionPoint.getDependencies().get(0);
-
-        // Ewwwww...
-        field.setAccessible(true);
         factory = injector.getInternalFactory(dependency.getKey(), errors);
     }
 
diff --git a/core/src/main/java/org/elasticsearch/common/inject/SingleMethodInjector.java b/core/src/main/java/org/elasticsearch/common/inject/SingleMethodInjector.java
index aa2f81d..65f7b06 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/SingleMethodInjector.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/SingleMethodInjector.java
@@ -49,10 +49,6 @@ class SingleMethodInjector implements SingleMemberInjector {
         if (!Modifier.isPrivate(modifiers) && !Modifier.isProtected(modifiers)) {
         }
 
-        if (!Modifier.isPublic(modifiers)) {
-            method.setAccessible(true);
-        }
-
         return new MethodInvoker() {
             @Override
             public Object invoke(Object target, Object... parameters)
diff --git a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/AssistedConstructor.java b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/AssistedConstructor.java
index 3973d9e..e3410bf 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/AssistedConstructor.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/AssistedConstructor.java
@@ -88,7 +88,6 @@ class AssistedConstructor<T> {
      * supplied arguments.
      */
     public T newInstance(Object[] args) throws Throwable {
-        constructor.setAccessible(true);
         try {
             return constructor.newInstance(args);
         } catch (InvocationTargetException e) {
diff --git a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider2.java b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider2.java
index 3eef8f2..821d729 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider2.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider2.java
@@ -53,7 +53,7 @@ import static org.elasticsearch.common.inject.internal.Annotations.getKey;
  * @author jessewilson@google.com (Jesse Wilson)
  * @author dtm@google.com (Daniel Martin)
  */
-final class FactoryProvider2<F> implements InvocationHandler, Provider<F> {
+public final class FactoryProvider2<F> implements InvocationHandler, Provider<F> {
 
     /**
      * if a factory method parameter isn't annotated, it gets this annotation.
@@ -173,7 +173,7 @@ final class FactoryProvider2<F> implements InvocationHandler, Provider<F> {
      * all factory methods will be able to build the target types.
      */
     @Inject
-    void initialize(Injector injector) {
+    public void initialize(Injector injector) {
         if (this.injector != null) {
             throw new ConfigurationException(Collections.singletonList(new Message(FactoryProvider2.class,
                 "Factories.create() factories may only be used in one Injector!")));
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/ProviderMethod.java b/core/src/main/java/org/elasticsearch/common/inject/internal/ProviderMethod.java
index 23dba6f..84fbae4 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/ProviderMethod.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/ProviderMethod.java
@@ -54,8 +54,6 @@ public class ProviderMethod<T> implements ProviderWithDependencies<T> {
         this.method = method;
         this.parameterProviders = parameterProviders;
         this.exposed = method.getAnnotation(Exposed.class) != null;
-
-        method.setAccessible(true);
     }
 
     public Key<T> getKey() {
diff --git a/core/src/main/java/org/elasticsearch/common/inject/multibindings/MapBinder.java b/core/src/main/java/org/elasticsearch/common/inject/multibindings/MapBinder.java
index 5b1757a..01bd59f 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/multibindings/MapBinder.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/multibindings/MapBinder.java
@@ -17,6 +17,7 @@
 package org.elasticsearch.common.inject.multibindings;
 
 import com.google.common.collect.ImmutableSet;
+
 import org.elasticsearch.common.inject.*;
 import org.elasticsearch.common.inject.binder.LinkedBindingBuilder;
 import org.elasticsearch.common.inject.multibindings.Multibinder.RealMultibinder;
@@ -227,7 +228,7 @@ public abstract class MapBinder<K, V> {
      * <p/>
      * <p>We use a subclass to hide 'implements Module' from the public API.
      */
-    private static final class RealMapBinder<K, V> extends MapBinder<K, V> implements Module {
+    public static final class RealMapBinder<K, V> extends MapBinder<K, V> implements Module {
         private final TypeLiteral<V> valueType;
         private final Key<Map<K, V>> mapKey;
         private final Key<Map<K, Provider<V>>> providerMapKey;
@@ -260,8 +261,48 @@ public abstract class MapBinder<K, V> {
                     binder.getProvider(valueKey)));
             return binder.bind(valueKey);
         }
+        
+        public static class MapBinderProviderWithDependencies<K,V> implements ProviderWithDependencies<Map<K, Provider<V>>> {
+            private Map<K, Provider<V>> providerMap;
+            
+            @SuppressWarnings("rawtypes") // code is silly stupid with generics
+            private final RealMapBinder binder;
+            private final Set<Dependency<?>> dependencies;
+            private final Provider<Set<Entry<K, Provider<V>>>> provider;
+            
+            @SuppressWarnings("rawtypes") // code is silly stupid with generics
+            MapBinderProviderWithDependencies(RealMapBinder binder, Set<Dependency<?>> dependencies, Provider<Set<Entry<K, Provider<V>>>> provider) {
+                this.binder = binder;
+                this.dependencies = dependencies;
+                this.provider = provider;
+            }
 
-        @Override
+            @SuppressWarnings({"unchecked", "unused"}) // code is silly stupid with generics
+            @Inject
+            public void initialize() {
+                binder.binder = null;
+
+                Map<K, Provider<V>> providerMapMutable = new LinkedHashMap<>();
+                for (Entry<K, Provider<V>> entry : provider.get()) {
+                    Multibinder.checkConfiguration(providerMapMutable.put(entry.getKey(), entry.getValue()) == null,
+                            "Map injection failed due to duplicated key \"%s\"", entry.getKey());
+                }
+
+                providerMap = Collections.unmodifiableMap(providerMapMutable);
+            }
+
+            @Override
+            public Map<K, Provider<V>> get() {
+                return providerMap;
+            }
+
+            @Override
+            public Set<Dependency<?>> getDependencies() {
+                return dependencies;
+            }
+        }
+
+        @Override @SuppressWarnings({"rawtypes", "unchecked"}) // code is silly stupid with generics
         public void configure(Binder binder) {
             Multibinder.checkConfiguration(!isInitialized(), "MapBinder was already initialized");
 
@@ -271,33 +312,7 @@ public abstract class MapBinder<K, V> {
             // binds a Map<K, Provider<V>> from a collection of Map<Entry<K, Provider<V>>
             final Provider<Set<Entry<K, Provider<V>>>> entrySetProvider = binder
                     .getProvider(entrySetBinder.getSetKey());
-            binder.bind(providerMapKey).toProvider(new ProviderWithDependencies<Map<K, Provider<V>>>() {
-                private Map<K, Provider<V>> providerMap;
-
-                @SuppressWarnings("unused")
-                @Inject
-                void initialize() {
-                    RealMapBinder.this.binder = null;
-
-                    Map<K, Provider<V>> providerMapMutable = new LinkedHashMap<>();
-                    for (Entry<K, Provider<V>> entry : entrySetProvider.get()) {
-                        Multibinder.checkConfiguration(providerMapMutable.put(entry.getKey(), entry.getValue()) == null,
-                                "Map injection failed due to duplicated key \"%s\"", entry.getKey());
-                    }
-
-                    providerMap = Collections.unmodifiableMap(providerMapMutable);
-                }
-
-                @Override
-                public Map<K, Provider<V>> get() {
-                    return providerMap;
-                }
-
-                @Override
-                public Set<Dependency<?>> getDependencies() {
-                    return dependencies;
-                }
-            });
+            binder.bind(providerMapKey).toProvider(new MapBinderProviderWithDependencies(RealMapBinder.this, dependencies, entrySetProvider));
 
             final Provider<Map<K, Provider<V>>> mapProvider = binder.getProvider(providerMapKey);
             binder.bind(mapKey).toProvider(new ProviderWithDependencies<Map<K, V>>() {
diff --git a/core/src/main/java/org/elasticsearch/common/inject/multibindings/Multibinder.java b/core/src/main/java/org/elasticsearch/common/inject/multibindings/Multibinder.java
index 50c87d7..87ee3c3 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/multibindings/Multibinder.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/multibindings/Multibinder.java
@@ -193,7 +193,7 @@ public abstract class Multibinder<T> {
      * <p>We use a subclass to hide 'implements Module, Provider' from the public
      * API.
      */
-    static final class RealMultibinder<T> extends Multibinder<T>
+    public static final class RealMultibinder<T> extends Multibinder<T>
             implements Module, Provider<Set<T>>, HasDependencies {
 
         private final TypeLiteral<T> elementType;
@@ -236,7 +236,7 @@ public abstract class Multibinder<T> {
          * contents are only evaluated when get() is invoked.
          */
         @Inject
-        void initialize(Injector injector) {
+        public void initialize(Injector injector) {
             providers = new ArrayList<>();
             List<Dependency<?>> dependencies = new ArrayList<>();
             for (Binding<?> entry : injector.findBindingsByType(elementType)) {
diff --git a/core/src/main/java/org/elasticsearch/common/io/PathUtils.java b/core/src/main/java/org/elasticsearch/common/io/PathUtils.java
index ada11bf..0002c1e 100644
--- a/core/src/main/java/org/elasticsearch/common/io/PathUtils.java
+++ b/core/src/main/java/org/elasticsearch/common/io/PathUtils.java
@@ -43,8 +43,8 @@ public final class PathUtils {
     /** the actual JDK default */
     static final FileSystem ACTUAL_DEFAULT = FileSystems.getDefault();
     
-    /** can be changed by tests (via reflection) */
-    private static volatile FileSystem DEFAULT = ACTUAL_DEFAULT;
+    /** can be changed by tests */
+    static volatile FileSystem DEFAULT = ACTUAL_DEFAULT;
     
     /** 
      * Returns a {@code Path} from name components.
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/all/AllTermQuery.java b/core/src/main/java/org/elasticsearch/common/lucene/all/AllTermQuery.java
index 9853659..a59af2c 100644
--- a/core/src/main/java/org/elasticsearch/common/lucene/all/AllTermQuery.java
+++ b/core/src/main/java/org/elasticsearch/common/lucene/all/AllTermQuery.java
@@ -64,6 +64,9 @@ public final class AllTermQuery extends Query {
 
     @Override
     public Query rewrite(IndexReader reader) throws IOException {
+        if (getBoost() != 1f) {
+            return super.rewrite(reader);
+        }
         boolean fieldExists = false;
         boolean hasPayloads = false;
         for (LeafReaderContext context : reader.leaves()) {
@@ -98,7 +101,7 @@ public final class AllTermQuery extends Query {
         final CollectionStatistics collectionStats = searcher.collectionStatistics(term.field());
         final TermStatistics termStats = searcher.termStatistics(term, termStates);
         final Similarity similarity = searcher.getSimilarity(needsScores);
-        final SimWeight stats = similarity.computeWeight(getBoost(), collectionStats, termStats);
+        final SimWeight stats = similarity.computeWeight(collectionStats, termStats);
         return new Weight(this) {
 
             @Override
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/search/MultiPhrasePrefixQuery.java b/core/src/main/java/org/elasticsearch/common/lucene/search/MultiPhrasePrefixQuery.java
index 3990cca..3d870bc 100644
--- a/core/src/main/java/org/elasticsearch/common/lucene/search/MultiPhrasePrefixQuery.java
+++ b/core/src/main/java/org/elasticsearch/common/lucene/search/MultiPhrasePrefixQuery.java
@@ -120,6 +120,9 @@ public class MultiPhrasePrefixQuery extends Query {
 
     @Override
     public Query rewrite(IndexReader reader) throws IOException {
+        if (getBoost() != 1.0F) {
+            return super.rewrite(reader);
+        }
         if (termArrays.isEmpty()) {
             return new MatchNoDocsQuery();
         }
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/search/function/BoostScoreFunction.java b/core/src/main/java/org/elasticsearch/common/lucene/search/function/BoostScoreFunction.java
deleted file mode 100644
index 13b4526..0000000
--- a/core/src/main/java/org/elasticsearch/common/lucene/search/function/BoostScoreFunction.java
+++ /dev/null
@@ -1,70 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common.lucene.search.function;
-
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.search.Explanation;
-
-/**
- *
- */
-@Deprecated
-public class BoostScoreFunction extends ScoreFunction {
-
-    public static final String BOOST_WEIGHT_ERROR_MESSAGE = "'boost_factor' and 'weight' cannot be used together. Use 'weight'.";
-
-    private final float boost;
-
-    public BoostScoreFunction(float boost) {
-        super(CombineFunction.MULT);
-        this.boost = boost;
-    }
-
-    public float getBoost() {
-        return boost;
-    }
-
-    @Override
-    public LeafScoreFunction getLeafScoreFunction(LeafReaderContext ctx) {
-        return new LeafScoreFunction() {
-
-            @Override
-            public double score(int docId, float subQueryScore) {
-                return boost;
-            }
-
-            @Override
-            public Explanation explainScore(int docId, Explanation subQueryScore) {
-                return Explanation.match(boost, "static boost factor", Explanation.match(boost, "boostFactor"));
-            }
-        };
-    }
-
-    @Override
-    public boolean needsScores() {
-        return false;
-    }
-
-    @Override
-    public String toString() {
-        return "boost[" + boost + "]";
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/search/function/CombineFunction.java b/core/src/main/java/org/elasticsearch/common/lucene/search/function/CombineFunction.java
index 30c8f01..41a5b85 100644
--- a/core/src/main/java/org/elasticsearch/common/lucene/search/function/CombineFunction.java
+++ b/core/src/main/java/org/elasticsearch/common/lucene/search/function/CombineFunction.java
@@ -24,8 +24,8 @@ import org.apache.lucene.search.Explanation;
 public enum CombineFunction {
     MULT {
         @Override
-        public float combine(double queryBoost, double queryScore, double funcScore, double maxBoost) {
-            return toFloat(queryBoost * queryScore * Math.min(funcScore, maxBoost));
+        public float combine(double queryScore, double funcScore, double maxBoost) {
+            return toFloat(queryScore * Math.min(funcScore, maxBoost));
         }
 
         @Override
@@ -34,21 +34,20 @@ public enum CombineFunction {
         }
 
         @Override
-        public Explanation explain(float queryBoost, Explanation queryExpl, Explanation funcExpl, float maxBoost) {
-            float score = queryBoost * Math.min(funcExpl.getValue(), maxBoost) * queryExpl.getValue();
+        public Explanation explain(Explanation queryExpl, Explanation funcExpl, float maxBoost) {
             Explanation boostExpl = Explanation.match(maxBoost, "maxBoost");
             Explanation minExpl = Explanation.match(
                     Math.min(funcExpl.getValue(), maxBoost),
                     "min of:",
                     funcExpl, boostExpl);
-            return Explanation.match(score, "function score, product of:",
-                    queryExpl, minExpl, Explanation.match(queryBoost, "queryBoost"));
+            return Explanation.match(queryExpl.getValue() * minExpl.getValue(),
+                    "function score, product of:", queryExpl, minExpl);
         }
     },
     REPLACE {
         @Override
-        public float combine(double queryBoost, double queryScore, double funcScore, double maxBoost) {
-            return toFloat(queryBoost * Math.min(funcScore, maxBoost));
+        public float combine(double queryScore, double funcScore, double maxBoost) {
+            return toFloat(Math.min(funcScore, maxBoost));
         }
 
         @Override
@@ -57,22 +56,19 @@ public enum CombineFunction {
         }
 
         @Override
-        public Explanation explain(float queryBoost, Explanation queryExpl, Explanation funcExpl, float maxBoost) {
-            float score = queryBoost * Math.min(funcExpl.getValue(), maxBoost);
+        public Explanation explain(Explanation queryExpl, Explanation funcExpl, float maxBoost) {
             Explanation boostExpl = Explanation.match(maxBoost, "maxBoost");
-            Explanation minExpl = Explanation.match(
+            return Explanation.match(
                     Math.min(funcExpl.getValue(), maxBoost),
                     "min of:",
                     funcExpl, boostExpl);
-            return Explanation.match(score, "function score, product of:",
-                    minExpl, Explanation.match(queryBoost, "queryBoost"));
         }
 
     },
     SUM {
         @Override
-        public float combine(double queryBoost, double queryScore, double funcScore, double maxBoost) {
-            return toFloat(queryBoost * (queryScore + Math.min(funcScore, maxBoost)));
+        public float combine(double queryScore, double funcScore, double maxBoost) {
+            return toFloat(queryScore + Math.min(funcScore, maxBoost));
         }
 
         @Override
@@ -81,21 +77,18 @@ public enum CombineFunction {
         }
 
         @Override
-        public Explanation explain(float queryBoost, Explanation queryExpl, Explanation funcExpl, float maxBoost) {
-            float score = queryBoost * (Math.min(funcExpl.getValue(), maxBoost) + queryExpl.getValue());
+        public Explanation explain(Explanation queryExpl, Explanation funcExpl, float maxBoost) {
             Explanation minExpl = Explanation.match(Math.min(funcExpl.getValue(), maxBoost), "min of:",
                     funcExpl, Explanation.match(maxBoost, "maxBoost"));
-            Explanation sumExpl = Explanation.match(Math.min(funcExpl.getValue(), maxBoost) + queryExpl.getValue(), "sum of",
+            return Explanation.match(Math.min(funcExpl.getValue(), maxBoost) + queryExpl.getValue(), "sum of",
                     queryExpl, minExpl);
-            return Explanation.match(score, "function score, product of:",
-                    sumExpl, Explanation.match(queryBoost, "queryBoost"));
         }
 
     },
     AVG {
         @Override
-        public float combine(double queryBoost, double queryScore, double funcScore, double maxBoost) {
-            return toFloat((queryBoost * (Math.min(funcScore, maxBoost) + queryScore) / 2.0));
+        public float combine(double queryScore, double funcScore, double maxBoost) {
+            return toFloat((Math.min(funcScore, maxBoost) + queryScore) / 2.0);
         }
 
         @Override
@@ -104,22 +97,19 @@ public enum CombineFunction {
         }
 
         @Override
-        public Explanation explain(float queryBoost, Explanation queryExpl, Explanation funcExpl, float maxBoost) {
-            float score = toFloat(queryBoost * (queryExpl.getValue() + Math.min(funcExpl.getValue(), maxBoost)) / 2.0);
+        public Explanation explain(Explanation queryExpl, Explanation funcExpl, float maxBoost) {
             Explanation minExpl = Explanation.match(Math.min(funcExpl.getValue(), maxBoost), "min of:",
                     funcExpl, Explanation.match(maxBoost, "maxBoost"));
-            Explanation avgExpl = Explanation.match(
+            return Explanation.match(
                     toFloat((Math.min(funcExpl.getValue(), maxBoost) + queryExpl.getValue()) / 2.0), "avg of",
                     queryExpl, minExpl);
-            return Explanation.match(score, "function score, product of:",
-                    avgExpl, Explanation.match(queryBoost, "queryBoost"));
         }
 
     },
     MIN {
         @Override
-        public float combine(double queryBoost, double queryScore, double funcScore, double maxBoost) {
-            return toFloat(queryBoost * Math.min(queryScore, Math.min(funcScore, maxBoost)));
+        public float combine(double queryScore, double funcScore, double maxBoost) {
+            return toFloat(Math.min(queryScore, Math.min(funcScore, maxBoost)));
         }
 
         @Override
@@ -128,23 +118,20 @@ public enum CombineFunction {
         }
 
         @Override
-        public Explanation explain(float queryBoost, Explanation queryExpl, Explanation funcExpl, float maxBoost) {
-            float score = toFloat(queryBoost * Math.min(queryExpl.getValue(), Math.min(funcExpl.getValue(), maxBoost)));
+        public Explanation explain(Explanation queryExpl, Explanation funcExpl, float maxBoost) {
             Explanation innerMinExpl = Explanation.match(
                     Math.min(funcExpl.getValue(), maxBoost), "min of:",
                     funcExpl, Explanation.match(maxBoost, "maxBoost"));
-            Explanation outerMinExpl = Explanation.match(
+            return Explanation.match(
                     Math.min(Math.min(funcExpl.getValue(), maxBoost), queryExpl.getValue()), "min of",
                     queryExpl, innerMinExpl);
-            return Explanation.match(score, "function score, product of:",
-                    outerMinExpl, Explanation.match(queryBoost, "queryBoost"));
         }
 
     },
     MAX {
         @Override
-        public float combine(double queryBoost, double queryScore, double funcScore, double maxBoost) {
-            return toFloat(queryBoost * (Math.max(queryScore, Math.min(funcScore, maxBoost))));
+        public float combine(double queryScore, double funcScore, double maxBoost) {
+            return toFloat(Math.max(queryScore, Math.min(funcScore, maxBoost)));
         }
 
         @Override
@@ -153,21 +140,18 @@ public enum CombineFunction {
         }
 
         @Override
-        public Explanation explain(float queryBoost, Explanation queryExpl, Explanation funcExpl, float maxBoost) {
-            float score = toFloat(queryBoost * Math.max(queryExpl.getValue(), Math.min(funcExpl.getValue(), maxBoost)));
+        public Explanation explain(Explanation queryExpl, Explanation funcExpl, float maxBoost) {
             Explanation innerMinExpl = Explanation.match(
                     Math.min(funcExpl.getValue(), maxBoost), "min of:",
                     funcExpl, Explanation.match(maxBoost, "maxBoost"));
-            Explanation outerMaxExpl = Explanation.match(
+            return Explanation.match(
                     Math.max(Math.min(funcExpl.getValue(), maxBoost), queryExpl.getValue()), "max of:",
                     queryExpl, innerMinExpl);
-            return Explanation.match(score, "function score, product of:",
-                    outerMaxExpl, Explanation.match(queryBoost, "queryBoost"));
         }
 
     };
 
-    public abstract float combine(double queryBoost, double queryScore, double funcScore, double maxBoost);
+    public abstract float combine(double queryScore, double funcScore, double maxBoost);
 
     public abstract String getName();
 
@@ -181,5 +165,5 @@ public enum CombineFunction {
         return Double.compare(floatVersion, input) == 0 || input == 0.0d ? 0 : 1.d - (floatVersion) / input;
     }
 
-    public abstract Explanation explain(float queryBoost, Explanation queryExpl, Explanation funcExpl, float maxBoost);
+    public abstract Explanation explain(Explanation queryExpl, Explanation funcExpl, float maxBoost);
 }
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/search/function/CustomBoostFactorScorer.java b/core/src/main/java/org/elasticsearch/common/lucene/search/function/CustomBoostFactorScorer.java
index b4ddaf2..709c7df 100644
--- a/core/src/main/java/org/elasticsearch/common/lucene/search/function/CustomBoostFactorScorer.java
+++ b/core/src/main/java/org/elasticsearch/common/lucene/search/function/CustomBoostFactorScorer.java
@@ -21,13 +21,11 @@ package org.elasticsearch.common.lucene.search.function;
 
 import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.Weight;
-import org.apache.lucene.util.BytesRef;
 
 import java.io.IOException;
 
 abstract class CustomBoostFactorScorer extends Scorer {
 
-    final float subQueryBoost;
     final Scorer scorer;
     final float maxBoost;
     final CombineFunction scoreCombiner;
@@ -43,7 +41,6 @@ abstract class CustomBoostFactorScorer extends Scorer {
         } else {
             nextDoc = new MinScoreNextDoc();
         }
-        this.subQueryBoost = w.getQuery().getBoost();
         this.scorer = scorer;
         this.maxBoost = maxBoost;
         this.scoreCombiner = scoreCombiner;
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java b/core/src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java
index e95da1d..ebe25b8 100644
--- a/core/src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java
+++ b/core/src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java
@@ -114,6 +114,9 @@ public class FiltersFunctionScoreQuery extends Query {
 
     @Override
     public Query rewrite(IndexReader reader) throws IOException {
+        if (getBoost() != 1.0F) {
+            return super.rewrite(reader);
+        }
         Query newQ = subQuery.rewrite(reader);
         if (newQ == subQuery)
             return this;
@@ -158,14 +161,12 @@ public class FiltersFunctionScoreQuery extends Query {
 
         @Override
         public float getValueForNormalization() throws IOException {
-            float sum = subQueryWeight.getValueForNormalization();
-            sum *= getBoost() * getBoost();
-            return sum;
+            return subQueryWeight.getValueForNormalization();
         }
 
         @Override
-        public void normalize(float norm, float topLevelBoost) {
-            subQueryWeight.normalize(norm, topLevelBoost * getBoost());
+        public void normalize(float norm, float boost) {
+            subQueryWeight.normalize(norm, boost);
         }
 
         @Override
@@ -219,10 +220,7 @@ public class FiltersFunctionScoreQuery extends Query {
                 }
             }
             if (filterExplanations.size() == 0) {
-                float sc = getBoost() * subQueryExpl.getValue();
-                return Explanation.match(sc, "function score, no filter match, product of:",
-                        subQueryExpl,
-                        Explanation.match(getBoost(), "queryBoost"));
+                return subQueryExpl;
             }
 
             // Second: Compute the factor that would have been computed by the
@@ -266,7 +264,7 @@ public class FiltersFunctionScoreQuery extends Query {
                     CombineFunction.toFloat(factor),
                     "function score, score mode [" + scoreMode.toString().toLowerCase(Locale.ROOT) + "]",
                     filterExplanations);
-            return combineFunction.explain(getBoost(), subQueryExpl, factorExplanation, maxBoost);
+            return combineFunction.explain(subQueryExpl, factorExplanation, maxBoost);
         }
     }
 
@@ -348,7 +346,7 @@ public class FiltersFunctionScoreQuery extends Query {
                     }
                 }
             }
-            return scoreCombiner.combine(subQueryBoost, subQueryScore, factor, maxBoost);
+            return scoreCombiner.combine(subQueryScore, factor, maxBoost);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/search/function/FunctionScoreQuery.java b/core/src/main/java/org/elasticsearch/common/lucene/search/function/FunctionScoreQuery.java
index 448eda8..2a88296 100644
--- a/core/src/main/java/org/elasticsearch/common/lucene/search/function/FunctionScoreQuery.java
+++ b/core/src/main/java/org/elasticsearch/common/lucene/search/function/FunctionScoreQuery.java
@@ -76,6 +76,9 @@ public class FunctionScoreQuery extends Query {
 
     @Override
     public Query rewrite(IndexReader reader) throws IOException {
+        if (getBoost() != 1.0F) {
+            return super.rewrite(reader);
+        }
         Query newQ = subQuery.rewrite(reader);
         if (newQ == subQuery) {
             return this;
@@ -117,14 +120,12 @@ public class FunctionScoreQuery extends Query {
 
         @Override
         public float getValueForNormalization() throws IOException {
-            float sum = subQueryWeight.getValueForNormalization();
-            sum *= getBoost() * getBoost();
-            return sum;
+            return subQueryWeight.getValueForNormalization();
         }
 
         @Override
-        public void normalize(float norm, float topLevelBoost) {
-            subQueryWeight.normalize(norm, topLevelBoost * getBoost());
+        public void normalize(float norm, float boost) {
+            subQueryWeight.normalize(norm, boost);
         }
 
         @Override
@@ -148,7 +149,7 @@ public class FunctionScoreQuery extends Query {
             }
             if (function != null) {
                 Explanation functionExplanation = function.getLeafScoreFunction(context).explainScore(doc, subQueryExpl);
-                return combineFunction.explain(getBoost(), subQueryExpl, functionExplanation, maxBoost);
+                return combineFunction.explain(subQueryExpl, functionExplanation, maxBoost);
             } else {
                 return subQueryExpl;
             }
@@ -174,9 +175,9 @@ public class FunctionScoreQuery extends Query {
             // are needed
             float score = needsScores ? scorer.score() : 0f;
             if (function == null) {
-                return subQueryBoost * score;
+                return score;
             } else {
-                return scoreCombiner.combine(subQueryBoost, score,
+                return scoreCombiner.combine(score,
                         function.score(scorer.docID(), score), maxBoost);
             }
         }
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/search/function/WeightFactorFunction.java b/core/src/main/java/org/elasticsearch/common/lucene/search/function/WeightFactorFunction.java
index 2def071..c585da42 100644
--- a/core/src/main/java/org/elasticsearch/common/lucene/search/function/WeightFactorFunction.java
+++ b/core/src/main/java/org/elasticsearch/common/lucene/search/function/WeightFactorFunction.java
@@ -35,9 +35,6 @@ public class WeightFactorFunction extends ScoreFunction {
 
     public WeightFactorFunction(float weight, ScoreFunction scoreFunction) {
         super(CombineFunction.MULT);
-        if (scoreFunction instanceof BoostScoreFunction) {
-            throw new IllegalArgumentException(BoostScoreFunction.BOOST_WEIGHT_ERROR_MESSAGE);
-        }
         if (scoreFunction == null) {
             this.scoreFunction = SCORE_ONE;
         } else {
diff --git a/core/src/main/java/org/elasticsearch/common/settings/Settings.java b/core/src/main/java/org/elasticsearch/common/settings/Settings.java
index 0dc0a7d..6b41c01 100644
--- a/core/src/main/java/org/elasticsearch/common/settings/Settings.java
+++ b/core/src/main/java/org/elasticsearch/common/settings/Settings.java
@@ -21,7 +21,6 @@ package org.elasticsearch.common.settings;
 
 import com.google.common.base.Charsets;
 import com.google.common.collect.ImmutableMap;
-import com.google.common.collect.ImmutableSortedMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.Booleans;
 import org.elasticsearch.common.Strings;
@@ -31,12 +30,7 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.property.PropertyPlaceholder;
 import org.elasticsearch.common.settings.loader.SettingsLoader;
 import org.elasticsearch.common.settings.loader.SettingsLoaderFactory;
-import org.elasticsearch.common.unit.ByteSizeUnit;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.unit.MemorySizeValue;
-import org.elasticsearch.common.unit.RatioValue;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.unit.*;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
@@ -45,17 +39,7 @@ import java.io.InputStream;
 import java.io.InputStreamReader;
 import java.nio.file.Files;
 import java.nio.file.Path;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.LinkedHashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Properties;
-import java.util.Set;
+import java.util.*;
 import java.util.concurrent.TimeUnit;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
@@ -86,13 +70,12 @@ public final class Settings implements ToXContent {
         return settingsRequireUnits;
     }
 
-    private ImmutableMap<String, String> settings;
+    private SortedMap<String, String> settings;
     private final ImmutableMap<String, String> forcedUnderscoreSettings;
 
     Settings(Map<String, String> settings) {
         // we use a sorted map for consistent serialization when using getAsMap()
-        // TODO: use Collections.unmodifiableMap with a TreeMap
-        this.settings = ImmutableSortedMap.copyOf(settings);
+        this.settings = Collections.unmodifiableSortedMap(new TreeMap<>(settings));
         Map<String, String> forcedUnderscoreSettings = null;
         for (Map.Entry<String, String> entry : settings.entrySet()) {
             String toUnderscoreCase = Strings.toUnderscoreCase(entry.getKey());
@@ -108,9 +91,10 @@ public final class Settings implements ToXContent {
 
     /**
      * The settings as a flat {@link java.util.Map}.
+     * @return an unmodifiable map of settings
      */
-    public ImmutableMap<String, String> getAsMap() {
-        return this.settings;
+    public Map<String, String> getAsMap() {
+        return Collections.unmodifiableMap(this.settings);
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/common/util/concurrent/FutureUtils.java b/core/src/main/java/org/elasticsearch/common/util/concurrent/FutureUtils.java
index 55bb61b..ef39156 100644
--- a/core/src/main/java/org/elasticsearch/common/util/concurrent/FutureUtils.java
+++ b/core/src/main/java/org/elasticsearch/common/util/concurrent/FutureUtils.java
@@ -34,4 +34,5 @@ public class FutureUtils {
         }
         return false;
     }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/discovery/Discovery.java b/core/src/main/java/org/elasticsearch/discovery/Discovery.java
index 13eb86f..0d431f5 100644
--- a/core/src/main/java/org/elasticsearch/discovery/Discovery.java
+++ b/core/src/main/java/org/elasticsearch/discovery/Discovery.java
@@ -19,17 +19,15 @@
 
 package org.elasticsearch.discovery;
 
-import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.cluster.ClusterChangedEvent;
+import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.routing.RoutingService;
+import org.elasticsearch.cluster.routing.allocation.AllocationService;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.component.LifecycleComponent;
-import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.node.service.NodeService;
 
-import java.io.IOException;
-
 /**
  * A pluggable module allowing to implement discovery of other nodes, publishing of the cluster
  * state to all nodes, electing a master of the cluster that raises cluster state change
@@ -62,29 +60,11 @@ public interface Discovery extends LifecycleComponent<Discovery> {
      *
      * The {@link AckListener} allows to keep track of the ack received from nodes, and verify whether
      * they updated their own cluster state or not.
-     *
-     * The method is guaranteed to throw a {@link FailedToCommitClusterStateException} if the change is not committed and should be rejected.
-     * Any other exception signals the something wrong happened but the change is committed.
      */
     void publish(ClusterChangedEvent clusterChangedEvent, AckListener ackListener);
 
-    interface AckListener {
+    public static interface AckListener {
         void onNodeAck(DiscoveryNode node, @Nullable Throwable t);
         void onTimeout();
     }
-
-    class FailedToCommitClusterStateException extends ElasticsearchException {
-
-        public FailedToCommitClusterStateException(StreamInput in) throws IOException {
-            super(in);
-        }
-
-        public FailedToCommitClusterStateException(String msg, Object... args) {
-            super(msg, args);
-        }
-
-        public FailedToCommitClusterStateException(String msg, Throwable cause, Object... args) {
-            super(msg, cause, args);
-        }
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/discovery/DiscoverySettings.java b/core/src/main/java/org/elasticsearch/discovery/DiscoverySettings.java
index 20f2c96..acce73c 100644
--- a/core/src/main/java/org/elasticsearch/discovery/DiscoverySettings.java
+++ b/core/src/main/java/org/elasticsearch/discovery/DiscoverySettings.java
@@ -35,22 +35,11 @@ import java.util.EnumSet;
  */
 public class DiscoverySettings extends AbstractComponent {
 
-    /**
-     * sets the timeout for a complete publishing cycle, including both sending and committing. the master
-     * will continute to process the next cluster state update after this time has elapsed
-     **/
     public static final String PUBLISH_TIMEOUT = "discovery.zen.publish_timeout";
-
-    /**
-     * sets the timeout for receiving enough acks for a specific cluster state and committing it. failing
-     * to receive responses within this window will cause the cluster state change to be rejected.
-     */
-    public static final String COMMIT_TIMEOUT = "discovery.zen.commit_timeout";
     public static final String NO_MASTER_BLOCK = "discovery.zen.no_master_block";
     public static final String PUBLISH_DIFF_ENABLE = "discovery.zen.publish_diff.enable";
 
     public static final TimeValue DEFAULT_PUBLISH_TIMEOUT = TimeValue.timeValueSeconds(30);
-    public static final TimeValue DEFAULT_COMMIT_TIMEOUT = TimeValue.timeValueSeconds(30);
     public static final String DEFAULT_NO_MASTER_BLOCK = "write";
     public final static int NO_MASTER_BLOCK_ID = 2;
     public final static boolean DEFAULT_PUBLISH_DIFF_ENABLE = true;
@@ -59,17 +48,15 @@ public class DiscoverySettings extends AbstractComponent {
     public final static ClusterBlock NO_MASTER_BLOCK_WRITES = new ClusterBlock(NO_MASTER_BLOCK_ID, "no master", true, false, RestStatus.SERVICE_UNAVAILABLE, EnumSet.of(ClusterBlockLevel.WRITE, ClusterBlockLevel.METADATA_WRITE));
 
     private volatile ClusterBlock noMasterBlock;
-    private volatile TimeValue publishTimeout;
-    private volatile TimeValue commitTimeout;
-    private volatile boolean publishDiff;
+    private volatile TimeValue publishTimeout = DEFAULT_PUBLISH_TIMEOUT;
+    private volatile boolean publishDiff = DEFAULT_PUBLISH_DIFF_ENABLE;
 
     @Inject
     public DiscoverySettings(Settings settings, NodeSettingsService nodeSettingsService) {
         super(settings);
         nodeSettingsService.addListener(new ApplySettings());
         this.noMasterBlock = parseNoMasterBlock(settings.get(NO_MASTER_BLOCK, DEFAULT_NO_MASTER_BLOCK));
-        this.publishTimeout = settings.getAsTime(PUBLISH_TIMEOUT, DEFAULT_PUBLISH_TIMEOUT);
-        this.commitTimeout = settings.getAsTime(COMMIT_TIMEOUT, new TimeValue(Math.min(DEFAULT_COMMIT_TIMEOUT.millis(), publishTimeout.millis())));
+        this.publishTimeout = settings.getAsTime(PUBLISH_TIMEOUT, publishTimeout);
         this.publishDiff = settings.getAsBoolean(PUBLISH_DIFF_ENABLE, DEFAULT_PUBLISH_DIFF_ENABLE);
     }
 
@@ -80,10 +67,6 @@ public class DiscoverySettings extends AbstractComponent {
         return publishTimeout;
     }
 
-    public TimeValue getCommitTimeout() {
-        return commitTimeout;
-    }
-
     public ClusterBlock getNoMasterBlock() {
         return noMasterBlock;
     }
@@ -98,17 +81,6 @@ public class DiscoverySettings extends AbstractComponent {
                 if (newPublishTimeout.millis() != publishTimeout.millis()) {
                     logger.info("updating [{}] from [{}] to [{}]", PUBLISH_TIMEOUT, publishTimeout, newPublishTimeout);
                     publishTimeout = newPublishTimeout;
-                    if (settings.getAsTime(COMMIT_TIMEOUT, null) == null && commitTimeout.millis() > publishTimeout.millis()) {
-                        logger.info("reducing default [{}] to [{}] due to publish timeout change", COMMIT_TIMEOUT, publishTimeout);
-                        commitTimeout = publishTimeout;
-                    }
-                }
-            }
-            TimeValue newCommitTimeout = settings.getAsTime(COMMIT_TIMEOUT, null);
-            if (newCommitTimeout != null) {
-                if (newCommitTimeout.millis() != commitTimeout.millis()) {
-                    logger.info("updating [{}] from [{}] to [{}]", COMMIT_TIMEOUT, commitTimeout, newCommitTimeout);
-                    commitTimeout = newCommitTimeout;
                 }
             }
             String newNoMasterBlockValue = settings.get(NO_MASTER_BLOCK);
diff --git a/core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java b/core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java
index 7c3d4c1..03cdd4b 100644
--- a/core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java
+++ b/core/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java
@@ -20,7 +20,14 @@
 package org.elasticsearch.discovery.local;
 
 import org.elasticsearch.Version;
-import org.elasticsearch.cluster.*;
+import org.elasticsearch.cluster.ClusterChangedEvent;
+import org.elasticsearch.cluster.ClusterName;
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.ClusterStateNonMasterUpdateTask;
+import org.elasticsearch.cluster.Diff;
+import org.elasticsearch.cluster.IncompatibleClusterStateVersionException;
+import org.elasticsearch.cluster.ProcessedClusterStateNonMasterUpdateTask;
 import org.elasticsearch.cluster.block.ClusterBlocks;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodeService;
@@ -35,11 +42,17 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
-import org.elasticsearch.discovery.*;
+import org.elasticsearch.discovery.AckClusterStatePublishResponseHandler;
+import org.elasticsearch.discovery.BlockingClusterStatePublishResponseHandler;
+import org.elasticsearch.discovery.Discovery;
+import org.elasticsearch.discovery.DiscoveryService;
+import org.elasticsearch.discovery.DiscoverySettings;
+import org.elasticsearch.discovery.InitialStateDiscoveryListener;
 import org.elasticsearch.node.service.NodeService;
 import org.elasticsearch.transport.TransportService;
 
 import java.util.HashSet;
+import java.util.Objects;
 import java.util.Queue;
 import java.util.Set;
 import java.util.concurrent.ConcurrentMap;
@@ -330,9 +343,9 @@ public class LocalDiscovery extends AbstractLifecycleComponent<Discovery> implem
                         }
                         try {
                             newNodeSpecificClusterState = discovery.lastProcessedClusterState.readDiffFrom(StreamInput.wrap(clusterStateDiffBytes)).apply(discovery.lastProcessedClusterState);
-                            logger.trace("sending diff cluster state version [{}] with size {} to [{}]", clusterState.version(), clusterStateDiffBytes.length, discovery.localNode.getName());
+                            logger.debug("sending diff cluster state version with size {} to [{}]", clusterStateDiffBytes.length, discovery.localNode.getName());
                         } catch (IncompatibleClusterStateVersionException ex) {
-                            logger.warn("incompatible cluster state version [{}] - resending complete cluster state", ex, clusterState.version());
+                            logger.warn("incompatible cluster state version - resending complete cluster state", ex);
                         }
                     }
                     if (newNodeSpecificClusterState == null) {
@@ -354,7 +367,7 @@ public class LocalDiscovery extends AbstractLifecycleComponent<Discovery> implem
                     discovery.clusterService.submitStateUpdateTask("local-disco-receive(from master)", new ProcessedClusterStateNonMasterUpdateTask() {
                         @Override
                         public ClusterState execute(ClusterState currentState) {
-                            if (currentState.supersedes(nodeSpecificClusterState)) {
+                            if (nodeSpecificClusterState.version() < currentState.version() && Objects.equals(nodeSpecificClusterState.nodes().masterNodeId(), currentState.nodes().masterNodeId())) {
                                 return currentState;
                             }
 
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java b/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java
index 834230b..ca863b7 100644
--- a/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java
+++ b/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java
@@ -19,10 +19,16 @@
 
 package org.elasticsearch.discovery.zen;
 
-import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.Version;
-import org.elasticsearch.cluster.*;
+import org.elasticsearch.cluster.ClusterChangedEvent;
+import org.elasticsearch.cluster.ClusterName;
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.ClusterStateNonMasterUpdateTask;
+import org.elasticsearch.cluster.ClusterStateUpdateTask;
+import org.elasticsearch.cluster.ProcessedClusterStateNonMasterUpdateTask;
+import org.elasticsearch.cluster.ProcessedClusterStateUpdateTask;
 import org.elasticsearch.cluster.block.ClusterBlocks;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
@@ -41,6 +47,7 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
 import org.elasticsearch.discovery.Discovery;
 import org.elasticsearch.discovery.DiscoverySettings;
 import org.elasticsearch.discovery.InitialStateDiscoveryListener;
@@ -55,13 +62,22 @@ import org.elasticsearch.discovery.zen.publish.PublishClusterStateAction;
 import org.elasticsearch.node.service.NodeService;
 import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.*;
+import org.elasticsearch.transport.EmptyTransportResponseHandler;
+import org.elasticsearch.transport.TransportChannel;
+import org.elasticsearch.transport.TransportException;
+import org.elasticsearch.transport.TransportRequest;
+import org.elasticsearch.transport.TransportRequestHandler;
+import org.elasticsearch.transport.TransportResponse;
+import org.elasticsearch.transport.TransportService;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.HashSet;
 import java.util.List;
+import java.util.Objects;
+import java.util.Queue;
 import java.util.Set;
+import java.util.concurrent.BlockingQueue;
 import java.util.concurrent.CopyOnWriteArrayList;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicInteger;
@@ -183,7 +199,7 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
         this.nodesFD = new NodesFaultDetection(settings, threadPool, transportService, clusterName);
         this.nodesFD.addListener(new NodeFaultDetectionListener());
 
-        this.publishClusterState = new PublishClusterStateAction(settings, transportService, this, new NewPendingClusterStateListener(), discoverySettings, clusterName);
+        this.publishClusterState = new PublishClusterStateAction(settings, transportService, this, new NewClusterStateListener(), discoverySettings);
         this.pingService.setPingContextProvider(this);
         this.membership = new MembershipAction(settings, clusterService, transportService, this, new MembershipListener());
 
@@ -313,25 +329,7 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
             throw new IllegalStateException("Shouldn't publish state when not master");
         }
         nodesFD.updateNodesAndPing(clusterChangedEvent.state());
-        try {
-            publishClusterState.publish(clusterChangedEvent, electMaster.minimumMasterNodes(), ackListener);
-        } catch (FailedToCommitClusterStateException t) {
-            // cluster service logs a WARN message
-            logger.debug("failed to publish cluster state version [{}] (not enough nodes acknowledged, min master nodes [{}])", clusterChangedEvent.state().version(), electMaster.minimumMasterNodes());
-            clusterService.submitStateUpdateTask("zen-disco-failed-to-publish", Priority.IMMEDIATE, new ClusterStateUpdateTask() {
-                @Override
-                public ClusterState execute(ClusterState currentState) {
-                    return rejoin(currentState, "failed to publish to min_master_nodes");
-                }
-
-                @Override
-                public void onFailure(String source, Throwable t) {
-                    logger.error("unexpected failure during [{}]", t, source);
-                }
-
-            });
-            throw t;
-        }
+        publishClusterState.publish(clusterChangedEvent, ackListener);
     }
 
     /**
@@ -342,12 +340,6 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
         return joinThreadControl.joinThreadActive();
     }
 
-
-    // used for testing
-    public ClusterState[] pendingClusterStates() {
-        return publishClusterState.pendingStatesQueue().pendingClusterStates();
-    }
-
     /**
      * the main function of a join thread. This function is guaranteed to join the cluster
      * or spawn a new join thread upon failure to do so.
@@ -418,7 +410,7 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
                         return joinThreadControl.stopRunningThreadAndRejoin(currentState, "master_switched_while_finalizing_join");
                     }
 
-                    // Note: we do not have to start master fault detection here because it's set at {@link #processNextPendingClusterState }
+                    // Note: we do not have to start master fault detection here because it's set at {@link #handleNewClusterStateFromMaster }
                     // when the first cluster state arrives.
                     joinThreadControl.markThreadAsDone(currentThread);
                     return currentState;
@@ -624,7 +616,9 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
                         .masterNodeId(null).build();
 
                 // flush any pending cluster states from old master, so it will not be set as master again
-                publishClusterState.pendingStatesQueue().failAllStatesAndClear(new ElasticsearchException("master left [{}]", reason));
+                ArrayList<ProcessClusterState> pendingNewClusterStates = new ArrayList<>();
+                processNewClusterStates.drainTo(pendingNewClusterStates);
+                logger.trace("removed [{}] pending cluster states", pendingNewClusterStates.size());
 
                 if (rejoinOnMasterGone) {
                     return rejoin(ClusterState.builder(currentState).nodes(discoveryNodes).build(), "master left (reason = " + reason + ")");
@@ -670,98 +664,181 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
         });
     }
 
-    void processNextPendingClusterState(String reason) {
-        clusterService.submitStateUpdateTask("zen-disco-receive(from master [" + reason + "])", Priority.URGENT, new ProcessedClusterStateNonMasterUpdateTask() {
-            ClusterState newClusterState = null;
+    static class ProcessClusterState {
+        final ClusterState clusterState;
+        volatile boolean processed;
 
-            @Override
-            public ClusterState execute(ClusterState currentState) {
-                newClusterState = publishClusterState.pendingStatesQueue().getNextClusterStateToProcess();
+        ProcessClusterState(ClusterState clusterState) {
+            this.clusterState = clusterState;
+        }
+    }
 
-                // all pending states have been processed
-                if (newClusterState == null) {
-                    return currentState;
+    private final BlockingQueue<ProcessClusterState> processNewClusterStates = ConcurrentCollections.newBlockingQueue();
+
+    void handleNewClusterStateFromMaster(ClusterState newClusterState, final PublishClusterStateAction.NewClusterStateListener.NewStateProcessed newStateProcessed) {
+        final ClusterName incomingClusterName = newClusterState.getClusterName();
+        /* The cluster name can still be null if the state comes from a node that is prev 1.1.1*/
+        if (incomingClusterName != null && !incomingClusterName.equals(this.clusterName)) {
+            logger.warn("received cluster state from [{}] which is also master but with a different cluster name [{}]", newClusterState.nodes().masterNode(), incomingClusterName);
+            newStateProcessed.onNewClusterStateFailed(new IllegalStateException("received state from a node that is not part of the cluster"));
+            return;
+        }
+        if (localNodeMaster()) {
+            logger.debug("received cluster state from [{}] which is also master with cluster name [{}]", newClusterState.nodes().masterNode(), incomingClusterName);
+            final ClusterState newState = newClusterState;
+            clusterService.submitStateUpdateTask("zen-disco-master_receive_cluster_state_from_another_master [" + newState.nodes().masterNode() + "]", Priority.URGENT, new ProcessedClusterStateUpdateTask() {
+                @Override
+                public ClusterState execute(ClusterState currentState) {
+                    return handleAnotherMaster(currentState, newState.nodes().masterNode(), newState.version(), "via a new cluster state");
+                }
+
+                @Override
+                public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {
+                    newStateProcessed.onNewClusterStateProcessed();
                 }
 
+                @Override
+                public void onFailure(String source, Throwable t) {
+                    logger.error("unexpected failure during [{}]", t, source);
+                    newStateProcessed.onNewClusterStateFailed(t);
+                }
+
+            });
+        } else {
+            if (newClusterState.nodes().localNode() == null) {
+                logger.warn("received a cluster state from [{}] and not part of the cluster, should not happen", newClusterState.nodes().masterNode());
+                newStateProcessed.onNewClusterStateFailed(new IllegalStateException("received state from a node that is not part of the cluster"));
+            } else {
+
+                final ProcessClusterState processClusterState = new ProcessClusterState(newClusterState);
+                processNewClusterStates.add(processClusterState);
+
                 assert newClusterState.nodes().masterNode() != null : "received a cluster state without a master";
                 assert !newClusterState.blocks().hasGlobalBlock(discoverySettings.getNoMasterBlock()) : "received a cluster state with a master block";
 
-                if (currentState.nodes().localNodeMaster()) {
-                    return handleAnotherMaster(currentState, newClusterState.nodes().masterNode(), newClusterState.version(), "via a new cluster state");
-                }
+                clusterService.submitStateUpdateTask("zen-disco-receive(from master [" + newClusterState.nodes().masterNode() + "])", Priority.URGENT, new ProcessedClusterStateNonMasterUpdateTask() {
+                    @Override
+                    public ClusterState execute(ClusterState currentState) {
+                        // we already processed it in a previous event
+                        if (processClusterState.processed) {
+                            return currentState;
+                        }
 
-                if (shouldIgnoreOrRejectNewClusterState(logger, currentState, newClusterState)) {
-                    return currentState;
-                }
+                        // TODO: once improvement that we can do is change the message structure to include version and masterNodeId
+                        // at the start, this will allow us to keep the "compressed bytes" around, and only parse the first page
+                        // to figure out if we need to use it or not, and only once we picked the latest one, parse the whole state
 
-                // check to see that we monitor the correct master of the cluster
-                if (masterFD.masterNode() == null || !masterFD.masterNode().equals(newClusterState.nodes().masterNode())) {
-                    masterFD.restart(newClusterState.nodes().masterNode(), "new cluster state received and we are monitoring the wrong master [" + masterFD.masterNode() + "]");
-                }
 
-                if (currentState.blocks().hasGlobalBlock(discoverySettings.getNoMasterBlock())) {
-                    // its a fresh update from the master as we transition from a start of not having a master to having one
-                    logger.debug("got first state from fresh master [{}]", newClusterState.nodes().masterNodeId());
-                    long count = clusterJoinsCounter.incrementAndGet();
-                    logger.trace("updated cluster join cluster to [{}]", count);
+                        ClusterState updatedState = selectNextStateToProcess(processNewClusterStates);
+                        if (updatedState == null) {
+                            updatedState = currentState;
+                        }
+                        if (shouldIgnoreOrRejectNewClusterState(logger, currentState, updatedState)) {
+                            return currentState;
+                        }
+
+                        // we don't need to do this, since we ping the master, and get notified when it has moved from being a master
+                        // because it doesn't have enough master nodes...
+                        //if (!electMaster.hasEnoughMasterNodes(newState.nodes())) {
+                        //    return disconnectFromCluster(newState, "not enough master nodes on new cluster state wreceived from [" + newState.nodes().masterNode() + "]");
+                        //}
 
-                    return newClusterState;
-                }
+                        // check to see that we monitor the correct master of the cluster
+                        if (masterFD.masterNode() == null || !masterFD.masterNode().equals(updatedState.nodes().masterNode())) {
+                            masterFD.restart(updatedState.nodes().masterNode(), "new cluster state received and we are monitoring the wrong master [" + masterFD.masterNode() + "]");
+                        }
 
+                        if (currentState.blocks().hasGlobalBlock(discoverySettings.getNoMasterBlock())) {
+                            // its a fresh update from the master as we transition from a start of not having a master to having one
+                            logger.debug("got first state from fresh master [{}]", updatedState.nodes().masterNodeId());
+                            long count = clusterJoinsCounter.incrementAndGet();
+                            logger.trace("updated cluster join cluster to [{}]", count);
 
-                // some optimizations to make sure we keep old objects where possible
-                ClusterState.Builder builder = ClusterState.builder(newClusterState);
+                            return updatedState;
+                        }
 
-                // if the routing table did not change, use the original one
-                if (newClusterState.routingTable().version() == currentState.routingTable().version()) {
-                    builder.routingTable(currentState.routingTable());
-                }
-                // same for metadata
-                if (newClusterState.metaData().version() == currentState.metaData().version()) {
-                    builder.metaData(currentState.metaData());
-                } else {
-                    // if its not the same version, only copy over new indices or ones that changed the version
-                    MetaData.Builder metaDataBuilder = MetaData.builder(newClusterState.metaData()).removeAllIndices();
-                    for (IndexMetaData indexMetaData : newClusterState.metaData()) {
-                        IndexMetaData currentIndexMetaData = currentState.metaData().index(indexMetaData.index());
-                        if (currentIndexMetaData != null && currentIndexMetaData.isSameUUID(indexMetaData.indexUUID()) &&
-                                currentIndexMetaData.version() == indexMetaData.version()) {
-                            // safe to reuse
-                            metaDataBuilder.put(currentIndexMetaData, false);
+
+                        // some optimizations to make sure we keep old objects where possible
+                        ClusterState.Builder builder = ClusterState.builder(updatedState);
+
+                        // if the routing table did not change, use the original one
+                        if (updatedState.routingTable().version() == currentState.routingTable().version()) {
+                            builder.routingTable(currentState.routingTable());
+                        }
+                        // same for metadata
+                        if (updatedState.metaData().version() == currentState.metaData().version()) {
+                            builder.metaData(currentState.metaData());
                         } else {
-                            metaDataBuilder.put(indexMetaData, false);
+                            // if its not the same version, only copy over new indices or ones that changed the version
+                            MetaData.Builder metaDataBuilder = MetaData.builder(updatedState.metaData()).removeAllIndices();
+                            for (IndexMetaData indexMetaData : updatedState.metaData()) {
+                                IndexMetaData currentIndexMetaData = currentState.metaData().index(indexMetaData.index());
+                                if (currentIndexMetaData != null && currentIndexMetaData.isSameUUID(indexMetaData.indexUUID()) &&
+                                        currentIndexMetaData.version() == indexMetaData.version()) {
+                                    // safe to reuse
+                                    metaDataBuilder.put(currentIndexMetaData, false);
+                                } else {
+                                    metaDataBuilder.put(indexMetaData, false);
+                                }
+                            }
+                            builder.metaData(metaDataBuilder);
                         }
+
+                        return builder.build();
                     }
-                    builder.metaData(metaDataBuilder);
-                }
 
-                return builder.build();
-            }
+                    @Override
+                    public void onFailure(String source, Throwable t) {
+                        logger.error("unexpected failure during [{}]", t, source);
+                        newStateProcessed.onNewClusterStateFailed(t);
+                    }
 
-            @Override
-            public void onFailure(String source, Throwable t) {
-                logger.error("unexpected failure during [{}]", t, source);
-                if (newClusterState != null) {
-                    try {
-                        publishClusterState.pendingStatesQueue().markAsFailed(newClusterState, t);
-                    } catch (Throwable unexpected) {
-                        logger.error("unexpected exception while failing [{}]", unexpected, source);
+                    @Override
+                    public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {
+                        sendInitialStateEventIfNeeded();
+                        newStateProcessed.onNewClusterStateProcessed();
                     }
-                }
+                });
             }
+        }
+    }
 
-            @Override
-            public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {
-                try {
-                    sendInitialStateEventIfNeeded();
-                    if (newClusterState != null) {
-                        publishClusterState.pendingStatesQueue().markAsProcessed(newClusterState);
-                    }
-                } catch (Throwable t) {
-                    onFailure(source, t);
-                }
+    /**
+     * Picks the cluster state with highest version with the same master from the queue. All cluster states with
+     * lower versions are ignored. If a cluster state with a different master is seen the processing logic stops and the
+     * last processed state is returned.
+     */
+    static ClusterState selectNextStateToProcess(Queue<ProcessClusterState> processNewClusterStates) {
+        // try and get the state with the highest version out of all the ones with the same master node id
+        ProcessClusterState stateToProcess = processNewClusterStates.poll();
+        if (stateToProcess == null) {
+            return null;
+        }
+        stateToProcess.processed = true;
+        while (true) {
+            ProcessClusterState potentialState = processNewClusterStates.peek();
+            // nothing else in the queue, bail
+            if (potentialState == null) {
+                break;
             }
-        });
+            // if its not from the same master, then bail
+            if (!Objects.equals(stateToProcess.clusterState.nodes().masterNodeId(), potentialState.clusterState.nodes().masterNodeId())) {
+                break;
+            }
+            // we are going to use it for sure, poll (remove) it
+            potentialState = processNewClusterStates.poll();
+            if (potentialState == null) {
+                // might happen if the queue is drained
+                break;
+            }
+            potentialState.processed = true;
+
+            if (potentialState.clusterState.version() > stateToProcess.clusterState.version()) {
+                // we found a new one
+                stateToProcess = potentialState;
+            }
+        }
+        return stateToProcess.clusterState;
     }
 
     /**
@@ -771,8 +848,13 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
      * If the second condition fails we ignore the cluster state.
      */
     static boolean shouldIgnoreOrRejectNewClusterState(ESLogger logger, ClusterState currentState, ClusterState newClusterState) {
-        validateStateIsFromCurrentMaster(logger, currentState.nodes(), newClusterState);
-        if (currentState.supersedes(newClusterState)) {
+        if (currentState.nodes().masterNodeId() == null) {
+            return false;
+        }
+        if (!currentState.nodes().masterNodeId().equals(newClusterState.nodes().masterNodeId())) {
+            logger.warn("received a cluster state from a different master then the current one, rejecting (received {}, current {})", newClusterState.nodes().masterNode(), currentState.nodes().masterNode());
+            throw new IllegalStateException("cluster state from a different master than the current one, rejecting (received " + newClusterState.nodes().masterNode() + ", current " + currentState.nodes().masterNode() + ")");
+        } else if (newClusterState.version() < currentState.version()) {
             // if the new state has a smaller version, and it has the same master node, then no need to process it
             logger.debug("received a cluster state that has a lower version than the current one, ignoring (received {}, current {})", newClusterState.version(), currentState.version());
             return true;
@@ -781,21 +863,6 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
         }
     }
 
-    /**
-     * In the case we follow an elected master the new cluster state needs to have the same elected master
-     * This method checks for this and throws an exception if needed
-     */
-
-    public static void validateStateIsFromCurrentMaster(ESLogger logger, DiscoveryNodes currentNodes, ClusterState newClusterState) {
-        if (currentNodes.masterNodeId() == null) {
-            return;
-        }
-        if (!currentNodes.masterNodeId().equals(newClusterState.nodes().masterNodeId())) {
-            logger.warn("received a cluster state from a different master than the current one, rejecting (received {}, current {})", newClusterState.nodes().masterNode(), currentNodes.masterNode());
-            throw new IllegalStateException("cluster state from a different master than the current one, rejecting (received " + newClusterState.nodes().masterNode() + ", current " + currentNodes.masterNode() + ")");
-        }
-    }
-
     void handleJoinRequest(final DiscoveryNode node, final MembershipAction.JoinCallback callback) {
 
         if (!transportService.addressSupported(node.address().getClass())) {
@@ -988,11 +1055,11 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
         }
     }
 
-    private class NewPendingClusterStateListener implements PublishClusterStateAction.NewPendingClusterStateListener {
+    private class NewClusterStateListener implements PublishClusterStateAction.NewClusterStateListener {
 
         @Override
-        public void onNewClusterState(String reason) {
-            processNextPendingClusterState(reason);
+        public void onNewClusterState(ClusterState clusterState, NewStateProcessed newStateProcessed) {
+            handleNewClusterStateFromMaster(clusterState, newStateProcessed);
         }
     }
 
@@ -1026,6 +1093,11 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
                 return;
             }
 
+            // nodes pre 1.4.0 do not send this information
+            if (pingRequest.masterNode() == null) {
+                return;
+            }
+
             if (pingsWhileMaster.incrementAndGet() < maxPingsFromAnotherMaster) {
                 logger.trace("got a ping from another master {}. current ping count: [{}]", pingRequest.masterNode(), pingsWhileMaster.get());
                 return;
@@ -1059,7 +1131,7 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
         return rejoinOnMasterGone;
     }
 
-    static class RejoinClusterRequest extends TransportRequest {
+    public static class RejoinClusterRequest extends TransportRequest {
 
         private String fromNodeId;
 
@@ -1067,7 +1139,7 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
             this.fromNodeId = fromNodeId;
         }
 
-        RejoinClusterRequest() {
+        public RejoinClusterRequest() {
         }
 
         @Override
@@ -1228,4 +1300,4 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
         }
 
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java b/core/src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java
index 9dfaef7..5ec3f9a 100644
--- a/core/src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java
+++ b/core/src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java
@@ -386,14 +386,14 @@ public class MasterFaultDetection extends FaultDetection {
     }
 
 
-    private static class MasterPingRequest extends TransportRequest {
+    public static class MasterPingRequest extends TransportRequest {
 
         private String nodeId;
 
         private String masterNodeId;
         private ClusterName clusterName;
 
-        private MasterPingRequest() {
+        public MasterPingRequest() {
         }
 
         private MasterPingRequest(String nodeId, String masterNodeId, ClusterName clusterName) {
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/fd/NodesFaultDetection.java b/core/src/main/java/org/elasticsearch/discovery/zen/fd/NodesFaultDetection.java
index 41a524b..a79c003 100644
--- a/core/src/main/java/org/elasticsearch/discovery/zen/fd/NodesFaultDetection.java
+++ b/core/src/main/java/org/elasticsearch/discovery/zen/fd/NodesFaultDetection.java
@@ -271,7 +271,7 @@ public class NodesFaultDetection extends FaultDetection {
 
         private long clusterStateVersion = ClusterState.UNKNOWN_VERSION;
 
-        PingRequest() {
+        public PingRequest() {
         }
 
         PingRequest(String nodeId, ClusterName clusterName, DiscoveryNode masterNode, long clusterStateVersion) {
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/membership/MembershipAction.java b/core/src/main/java/org/elasticsearch/discovery/zen/membership/MembershipAction.java
index dae17ef..419ed94 100644
--- a/core/src/main/java/org/elasticsearch/discovery/zen/membership/MembershipAction.java
+++ b/core/src/main/java/org/elasticsearch/discovery/zen/membership/MembershipAction.java
@@ -105,11 +105,11 @@ public class MembershipAction extends AbstractComponent {
                 .txGet(timeout.millis(), TimeUnit.MILLISECONDS);
     }
 
-    static class JoinRequest extends TransportRequest {
+    public static class JoinRequest extends TransportRequest {
 
         DiscoveryNode node;
 
-        private JoinRequest() {
+        public JoinRequest() {
         }
 
         private JoinRequest(DiscoveryNode node) {
@@ -156,9 +156,9 @@ public class MembershipAction extends AbstractComponent {
         }
     }
 
-    static class ValidateJoinRequest extends TransportRequest {
+    public static class ValidateJoinRequest extends TransportRequest {
 
-        ValidateJoinRequest() {
+        public ValidateJoinRequest() {
         }
     }
 
@@ -171,11 +171,11 @@ public class MembershipAction extends AbstractComponent {
         }
     }
 
-    static class LeaveRequest extends TransportRequest {
+    public static class LeaveRequest extends TransportRequest {
 
         private DiscoveryNode node;
 
-        private LeaveRequest() {
+        public LeaveRequest() {
         }
 
         private LeaveRequest(DiscoveryNode node) {
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java b/core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java
index 741f877..7e24ebc 100644
--- a/core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java
+++ b/core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java
@@ -523,13 +523,13 @@ public class UnicastZenPing extends AbstractLifecycleComponent<ZenPing> implemen
         }
     }
 
-    static class UnicastPingRequest extends TransportRequest {
+    public static class UnicastPingRequest extends TransportRequest {
 
         int id;
         TimeValue timeout;
         PingResponse pingResponse;
 
-        UnicastPingRequest() {
+        public UnicastPingRequest() {
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/publish/PendingClusterStatesQueue.java b/core/src/main/java/org/elasticsearch/discovery/zen/publish/PendingClusterStatesQueue.java
deleted file mode 100644
index fc894f3..0000000
--- a/core/src/main/java/org/elasticsearch/discovery/zen/publish/PendingClusterStatesQueue.java
+++ /dev/null
@@ -1,286 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.discovery.zen.publish;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.common.logging.ESLogger;
-
-import java.util.ArrayList;
-import java.util.Locale;
-import java.util.Objects;
-
-/**
- * A queue that holds all "in-flight" incoming cluster states from the master. Once a master commits a cluster
- * state, it is made available via {@link #getNextClusterStateToProcess()}. The class also takes care of batching
- * cluster states for processing and failures.
- * <p/>
- * The queue is bound by {@link #maxQueueSize}. When the queue is at capacity and a new cluster state is inserted
- * the oldest cluster state will be dropped. This is safe because:
- * 1) Under normal operations, master will publish & commit a cluster state before processing another change (i.e., the queue length is 1)
- * 2) If the master fails to commit a change, it will step down, causing a master election, which will flush the queue.
- * 3) In general it's safe to process the incoming cluster state as a replacement to the cluster state that's dropped.
- * a) If the dropped cluster is from the same master as the incoming one is, it is likely to be superseded by the incoming state (or another state in the queue).
- * This is only not true in very extreme cases of out of order delivery.
- * b) If the dropping cluster state is not from the same master, it means that:
- * i) we are no longer following the master of the dropped cluster state but follow the incoming one
- * ii) we are no longer following any master, in which case it doesn't matter which cluster state will be processed first.
- * <p/>
- * The class is fully thread safe and can be used concurrently.
- */
-public class PendingClusterStatesQueue {
-
-    interface StateProcessedListener {
-
-        void onNewClusterStateProcessed();
-
-        void onNewClusterStateFailed(Throwable t);
-    }
-
-    final ArrayList<ClusterStateContext> pendingStates = new ArrayList<>();
-    final ESLogger logger;
-    final int maxQueueSize;
-
-    public PendingClusterStatesQueue(ESLogger logger, int maxQueueSize) {
-        this.logger = logger;
-        this.maxQueueSize = maxQueueSize;
-    }
-
-    /** Add an incoming, not yet committed cluster state */
-    public synchronized void addPending(ClusterState state) {
-        pendingStates.add(new ClusterStateContext(state));
-        if (pendingStates.size() > maxQueueSize) {
-            ClusterStateContext context = pendingStates.remove(0);
-            logger.warn("dropping pending state [{}]. more than [{}] pending states.", context, maxQueueSize);
-            if (context.committed()) {
-                context.listener.onNewClusterStateFailed(new ElasticsearchException("too many pending states ([{}] pending)", maxQueueSize));
-            }
-        }
-    }
-
-    /**
-     * Mark a previously added cluster state as committed. This will make it available via {@link #getNextClusterStateToProcess()}
-     * When the cluster state is processed (or failed), the supplied listener will be called
-     **/
-    public synchronized ClusterState markAsCommitted(String stateUUID, StateProcessedListener listener) {
-        final ClusterStateContext context = findState(stateUUID);
-        if (context == null) {
-            listener.onNewClusterStateFailed(new IllegalStateException("can't resolve cluster state with uuid [" + stateUUID + "] to commit"));
-            return null;
-        }
-        if (context.committed()) {
-            listener.onNewClusterStateFailed(new IllegalStateException("cluster state with uuid [" + stateUUID + "] is already committed"));
-            return null;
-        }
-        context.markAsCommitted(listener);
-        return context.state;
-    }
-
-    /**
-     * mark that the processing of the given state has failed. All committed states that are {@link ClusterState#supersedes(ClusterState)}-ed
-     * by this failed state, will be failed as well
-     */
-    public synchronized void markAsFailed(ClusterState state, Throwable reason) {
-        final ClusterStateContext failedContext = findState(state.stateUUID());
-        if (failedContext == null) {
-            throw new IllegalArgumentException("can't resolve failed cluster state with uuid [" + state.stateUUID() + "], version [" + state.version() + "]");
-        }
-        if (failedContext.committed() == false) {
-            throw new IllegalArgumentException("failed cluster state is not committed " + state);
-        }
-
-        // fail all committed states which are batch together with the failed state
-        ArrayList<ClusterStateContext> statesToRemove = new ArrayList<>();
-        for (int index = 0; index < pendingStates.size(); index++) {
-            final ClusterStateContext pendingContext = pendingStates.get(index);
-            if (pendingContext.committed() == false) {
-                continue;
-            }
-            final ClusterState pendingState = pendingContext.state;
-            if (pendingContext.equals(failedContext)) {
-                statesToRemove.add(pendingContext);
-                pendingContext.listener.onNewClusterStateFailed(reason);
-            } else if (state.supersedes(pendingState)) {
-                statesToRemove.add(pendingContext);
-                logger.debug("failing committed state {} together with state {}", pendingContext, failedContext);
-                pendingContext.listener.onNewClusterStateFailed(reason);
-            }
-        }
-        pendingStates.removeAll(statesToRemove);
-        assert findState(state.stateUUID()) == null : "state was marked as processed but can still be found in pending list " + state;
-    }
-
-    /**
-     * indicates that a cluster state was successfully processed. Any committed state that is {@link ClusterState#supersedes(ClusterState)}-ed
-     * by the processed state will be marked as processed as well.
-     * <p/>
-     * NOTE: successfully processing a state indicates we are following the master it came from. Any committed state from another master will
-     * be failed by this method
-     */
-    public synchronized void markAsProcessed(ClusterState state) {
-        if (findState(state.stateUUID()) == null) {
-            throw new IllegalStateException("can't resolve processed cluster state with uuid [" + state.stateUUID() + "], version [" + state.version() + "]");
-        }
-        final DiscoveryNode currentMaster = state.nodes().masterNode();
-        assert currentMaster != null : "processed cluster state mast have a master. " + state;
-
-        // fail or remove any incoming state from a different master
-        // respond to any committed state from the same master with same or lower version (we processed a higher version)
-        ArrayList<ClusterStateContext> contextsToRemove = new ArrayList<>();
-        for (int index = 0; index < pendingStates.size(); index++) {
-            final ClusterStateContext pendingContext = pendingStates.get(index);
-            final ClusterState pendingState = pendingContext.state;
-            final DiscoveryNode pendingMasterNode = pendingState.nodes().masterNode();
-            if (Objects.equals(currentMaster, pendingMasterNode) == false) {
-                contextsToRemove.add(pendingContext);
-                if (pendingContext.committed()) {
-                    // this is a committed state , warn
-                    logger.warn("received a cluster state (uuid[{}]/v[{}]) from a different master than the current one, rejecting (received {}, current {})",
-                            pendingState.stateUUID(), pendingState.version(),
-                            pendingMasterNode, currentMaster);
-                    pendingContext.listener.onNewClusterStateFailed(
-                            new IllegalStateException("cluster state from a different master than the current one, rejecting (received " + pendingMasterNode + ", current " + currentMaster + ")")
-                    );
-                } else {
-                    logger.trace("removing non-committed state with uuid[{}]/v[{}] from [{}] - a state from [{}] was successfully processed",
-                            pendingState.stateUUID(), pendingState.version(), pendingMasterNode,
-                            currentMaster
-                    );
-                }
-            } else if (state.supersedes(pendingState) && pendingContext.committed()) {
-                logger.trace("processing pending state uuid[{}]/v[{}] together with state uuid[{}]/v[{}]",
-                        pendingState.stateUUID(), pendingState.version(), state.stateUUID(), state.version()
-                );
-                contextsToRemove.add(pendingContext);
-                pendingContext.listener.onNewClusterStateProcessed();
-            } else if (pendingState.stateUUID().equals(state.stateUUID())) {
-                assert pendingContext.committed() : "processed cluster state is not committed " + state;
-                contextsToRemove.add(pendingContext);
-                pendingContext.listener.onNewClusterStateProcessed();
-            }
-        }
-        // now ack the processed state
-        pendingStates.removeAll(contextsToRemove);
-        assert findState(state.stateUUID()) == null : "state was marked as processed but can still be found in pending list " + state;
-
-    }
-
-    ClusterStateContext findState(String stateUUID) {
-        for (int i = 0; i < pendingStates.size(); i++) {
-            final ClusterStateContext context = pendingStates.get(i);
-            if (context.stateUUID().equals(stateUUID)) {
-                return context;
-            }
-        }
-        return null;
-    }
-
-    /** clear the incoming queue. any committed state will be failed */
-    public synchronized void failAllStatesAndClear(Throwable reason) {
-        for (ClusterStateContext pendingState : pendingStates) {
-            if (pendingState.committed()) {
-                pendingState.listener.onNewClusterStateFailed(reason);
-            }
-        }
-        pendingStates.clear();
-    }
-
-    /**
-     * Gets the next committed state to process.
-     * <p/>
-     * The method tries to batch operation by getting the cluster state the highest possible committed states
-     * which succeeds the first committed state in queue (i.e., it comes from the same master).
-     */
-    public synchronized ClusterState getNextClusterStateToProcess() {
-        if (pendingStates.isEmpty()) {
-            return null;
-        }
-
-        ClusterStateContext stateToProcess = null;
-        int index = 0;
-        for (; index < pendingStates.size(); index++) {
-            ClusterStateContext potentialState = pendingStates.get(index);
-            if (potentialState.committed()) {
-                stateToProcess = potentialState;
-                break;
-            }
-        }
-        if (stateToProcess == null) {
-            return null;
-        }
-
-        // now try to find the highest committed state from the same master
-        for (; index < pendingStates.size(); index++) {
-            ClusterStateContext potentialState = pendingStates.get(index);
-
-            if (potentialState.state.supersedes(stateToProcess.state) && potentialState.committed()) {
-                // we found a new one
-                stateToProcess = potentialState;
-            }
-        }
-        assert stateToProcess.committed() : "should only return committed cluster state. found " + stateToProcess.state;
-        return stateToProcess.state;
-    }
-
-    /** returns all pending states, committed or not */
-    public synchronized ClusterState[] pendingClusterStates() {
-        ArrayList<ClusterState> states = new ArrayList<>();
-        for (ClusterStateContext context : pendingStates) {
-            states.add(context.state);
-        }
-        return states.toArray(new ClusterState[states.size()]);
-    }
-
-    static class ClusterStateContext {
-        final ClusterState state;
-        StateProcessedListener listener;
-
-        ClusterStateContext(ClusterState clusterState) {
-            this.state = clusterState;
-        }
-
-        void markAsCommitted(StateProcessedListener listener) {
-            if (this.listener != null) {
-                throw new IllegalStateException(toString() + "is already committed");
-            }
-            this.listener = listener;
-        }
-
-        boolean committed() {
-            return listener != null;
-        }
-
-        public String stateUUID() {
-            return state.stateUUID();
-        }
-
-        @Override
-        public String toString() {
-            return String.format(
-                    Locale.ROOT,
-                    "[uuid[%s], v[%d], m[%s]]",
-                    stateUUID(),
-                    state.version(),
-                    state.nodes().masterNodeId()
-            );
-        }
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateAction.java b/core/src/main/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateAction.java
index 2813932..7db74c7 100644
--- a/core/src/main/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateAction.java
+++ b/core/src/main/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateAction.java
@@ -19,11 +19,13 @@
 
 package org.elasticsearch.discovery.zen.publish;
 
-import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.Version;
-import org.elasticsearch.cluster.*;
+import org.elasticsearch.cluster.ClusterChangedEvent;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.Diff;
+import org.elasticsearch.cluster.IncompatibleClusterStateVersionException;
 import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.cluster.node.DiscoveryNodes;
+import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.compress.Compressor;
@@ -38,14 +40,21 @@ import org.elasticsearch.discovery.BlockingClusterStatePublishResponseHandler;
 import org.elasticsearch.discovery.Discovery;
 import org.elasticsearch.discovery.DiscoverySettings;
 import org.elasticsearch.discovery.zen.DiscoveryNodesProvider;
-import org.elasticsearch.discovery.zen.ZenDiscovery;
 import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.*;
+import org.elasticsearch.transport.BytesTransportRequest;
+import org.elasticsearch.transport.EmptyTransportResponseHandler;
+import org.elasticsearch.transport.TransportChannel;
+import org.elasticsearch.transport.TransportException;
+import org.elasticsearch.transport.TransportRequestHandler;
+import org.elasticsearch.transport.TransportRequestOptions;
+import org.elasticsearch.transport.TransportResponse;
+import org.elasticsearch.transport.TransportService;
 
 import java.io.IOException;
-import java.util.*;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.TimeUnit;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
 import java.util.concurrent.atomic.AtomicBoolean;
 
 /**
@@ -53,264 +62,176 @@ import java.util.concurrent.atomic.AtomicBoolean;
  */
 public class PublishClusterStateAction extends AbstractComponent {
 
-    public static final String SEND_ACTION_NAME = "internal:discovery/zen/publish/send";
-    public static final String COMMIT_ACTION_NAME = "internal:discovery/zen/publish/commit";
+    public static final String ACTION_NAME = "internal:discovery/zen/publish";
 
-    public static final String SETTINGS_MAX_PENDING_CLUSTER_STATES = "discovery.zen.publish.max_pending_cluster_states";
+    public interface NewClusterStateListener {
 
-    public interface NewPendingClusterStateListener {
+        interface NewStateProcessed {
 
-        /** a new cluster state has been committed and is ready to process via {@link #pendingStatesQueue()} */
-        void onNewClusterState(String reason);
+            void onNewClusterStateProcessed();
+
+            void onNewClusterStateFailed(Throwable t);
+        }
+
+        void onNewClusterState(ClusterState clusterState, NewStateProcessed newStateProcessed);
     }
 
     private final TransportService transportService;
     private final DiscoveryNodesProvider nodesProvider;
-    private final NewPendingClusterStateListener newPendingClusterStatelistener;
+    private final NewClusterStateListener listener;
     private final DiscoverySettings discoverySettings;
-    private final ClusterName clusterName;
-    private final PendingClusterStatesQueue pendingStatesQueue;
 
     public PublishClusterStateAction(Settings settings, TransportService transportService, DiscoveryNodesProvider nodesProvider,
-                                     NewPendingClusterStateListener listener, DiscoverySettings discoverySettings, ClusterName clusterName) {
+                                     NewClusterStateListener listener, DiscoverySettings discoverySettings) {
         super(settings);
         this.transportService = transportService;
         this.nodesProvider = nodesProvider;
-        this.newPendingClusterStatelistener = listener;
+        this.listener = listener;
         this.discoverySettings = discoverySettings;
-        this.clusterName = clusterName;
-        this.pendingStatesQueue = new PendingClusterStatesQueue(logger, settings.getAsInt(SETTINGS_MAX_PENDING_CLUSTER_STATES, 25));
-        transportService.registerRequestHandler(SEND_ACTION_NAME, BytesTransportRequest.class, ThreadPool.Names.SAME, new SendClusterStateRequestHandler());
-        transportService.registerRequestHandler(COMMIT_ACTION_NAME, CommitClusterStateRequest.class, ThreadPool.Names.SAME, new CommitClusterStateRequestHandler());
+        transportService.registerRequestHandler(ACTION_NAME, BytesTransportRequest.class, ThreadPool.Names.SAME, new PublishClusterStateRequestHandler());
     }
 
     public void close() {
-        transportService.removeHandler(SEND_ACTION_NAME);
-        transportService.removeHandler(COMMIT_ACTION_NAME);
-    }
-
-    public PendingClusterStatesQueue pendingStatesQueue() {
-        return pendingStatesQueue;
+        transportService.removeHandler(ACTION_NAME);
     }
 
-    /**
-     * publishes a cluster change event to other nodes. if at least minMasterNodes acknowledge the change it is committed and will
-     * be processed by the master and the other nodes.
-     * <p/>
-     * The method is guaranteed to throw a {@link Discovery.FailedToCommitClusterStateException} if the change is not committed and should be rejected.
-     * Any other exception signals the something wrong happened but the change is committed.
-     */
-    public void publish(final ClusterChangedEvent clusterChangedEvent, final int minMasterNodes, final Discovery.AckListener ackListener) throws Discovery.FailedToCommitClusterStateException {
-        final DiscoveryNodes nodes;
-        final SendingController sendingController;
-        final Set<DiscoveryNode> nodesToPublishTo;
-        final Map<Version, BytesReference> serializedStates;
-        final Map<Version, BytesReference> serializedDiffs;
-        final boolean sendFullVersion;
-        try {
-            nodes = clusterChangedEvent.state().nodes();
-            nodesToPublishTo = new HashSet<>(nodes.size());
-            DiscoveryNode localNode = nodes.localNode();
-            final int totalMasterNodes = nodes.masterNodes().size();
-            for (final DiscoveryNode node : nodes) {
-                if (node.equals(localNode) == false) {
-                    nodesToPublishTo.add(node);
-                }
-            }
-            sendFullVersion = !discoverySettings.getPublishDiff() || clusterChangedEvent.previousState() == null;
-            serializedStates = new HashMap<>();
-            serializedDiffs = new HashMap<>();
-
-            // we build these early as a best effort not to commit in the case of error.
-            // sadly this is not water tight as it may that a failed diff based publishing to a node
-            // will cause a full serialization based on an older version, which may fail after the
-            // change has been committed.
-            buildDiffAndSerializeStates(clusterChangedEvent.state(), clusterChangedEvent.previousState(),
-                    nodesToPublishTo, sendFullVersion, serializedStates, serializedDiffs);
-
-            final BlockingClusterStatePublishResponseHandler publishResponseHandler = new AckClusterStatePublishResponseHandler(nodesToPublishTo, ackListener);
-            sendingController = new SendingController(clusterChangedEvent.state(), minMasterNodes, totalMasterNodes, publishResponseHandler);
-        } catch (Throwable t) {
-            throw new Discovery.FailedToCommitClusterStateException("unexpected error while preparing to publish", t);
-        }
-
-        try {
-            innerPublish(clusterChangedEvent, nodesToPublishTo, sendingController, sendFullVersion, serializedStates, serializedDiffs);
-        } catch (Discovery.FailedToCommitClusterStateException t) {
-            throw t;
-        } catch (Throwable t) {
-            // try to fail committing, in cause it's still on going
-            if (sendingController.markAsFailed("unexpected error [" + t.getMessage() + "]")) {
-                // signal the change should be rejected
-                throw new Discovery.FailedToCommitClusterStateException("unexpected error [{}]", t, t.getMessage());
-            } else {
-                throw t;
+    public void publish(ClusterChangedEvent clusterChangedEvent, final Discovery.AckListener ackListener) {
+        Set<DiscoveryNode> nodesToPublishTo = new HashSet<>(clusterChangedEvent.state().nodes().size());
+        DiscoveryNode localNode = nodesProvider.nodes().localNode();
+        for (final DiscoveryNode node : clusterChangedEvent.state().nodes()) {
+            if (node.equals(localNode)) {
+                continue;
             }
+            nodesToPublishTo.add(node);
         }
+        publish(clusterChangedEvent, nodesToPublishTo, new AckClusterStatePublishResponseHandler(nodesToPublishTo, ackListener));
     }
 
-    private void innerPublish(final ClusterChangedEvent clusterChangedEvent, final Set<DiscoveryNode> nodesToPublishTo,
-                              final SendingController sendingController, final boolean sendFullVersion,
-                              final Map<Version, BytesReference> serializedStates, final Map<Version, BytesReference> serializedDiffs) {
+    private void publish(final ClusterChangedEvent clusterChangedEvent, final Set<DiscoveryNode> nodesToPublishTo,
+                         final BlockingClusterStatePublishResponseHandler publishResponseHandler) {
+
+        Map<Version, BytesReference> serializedStates = new HashMap<>();
+        Map<Version, BytesReference> serializedDiffs = new HashMap<>();
 
         final ClusterState clusterState = clusterChangedEvent.state();
         final ClusterState previousState = clusterChangedEvent.previousState();
+        final AtomicBoolean timedOutWaitingForNodes = new AtomicBoolean(false);
         final TimeValue publishTimeout = discoverySettings.getPublishTimeout();
-
-        final long publishingStartInNanos = System.nanoTime();
+        final boolean sendFullVersion = !discoverySettings.getPublishDiff() || previousState == null;
+        Diff<ClusterState> diff = null;
 
         for (final DiscoveryNode node : nodesToPublishTo) {
+
             // try and serialize the cluster state once (or per version), so we don't serialize it
             // per node when we send it over the wire, compress it while we are at it...
             // we don't send full version if node didn't exist in the previous version of cluster state
             if (sendFullVersion || !previousState.nodes().nodeExists(node.id())) {
-                sendFullClusterState(clusterState, serializedStates, node, publishTimeout, sendingController);
+                sendFullClusterState(clusterState, serializedStates, node, timedOutWaitingForNodes, publishTimeout, publishResponseHandler);
             } else {
-                sendClusterStateDiff(clusterState, serializedDiffs, serializedStates, node, publishTimeout, sendingController);
-            }
-        }
-
-        sendingController.waitForCommit(discoverySettings.getCommitTimeout());
-
-        try {
-            long timeLeftInNanos = Math.max(0, publishTimeout.nanos() - (System.nanoTime() - publishingStartInNanos));
-            final BlockingClusterStatePublishResponseHandler publishResponseHandler = sendingController.getPublishResponseHandler();
-            sendingController.setPublishingTimedOut(!publishResponseHandler.awaitAllNodes(TimeValue.timeValueNanos(timeLeftInNanos)));
-            if (sendingController.getPublishingTimedOut()) {
-                DiscoveryNode[] pendingNodes = publishResponseHandler.pendingNodes();
-                // everyone may have just responded
-                if (pendingNodes.length > 0) {
-                    logger.warn("timed out waiting for all nodes to process published state [{}] (timeout [{}], pending nodes: {})", clusterState.version(), publishTimeout, pendingNodes);
+                if (diff == null) {
+                    diff = clusterState.diff(previousState);
                 }
+                sendClusterStateDiff(clusterState, diff, serializedDiffs, node, timedOutWaitingForNodes, publishTimeout, publishResponseHandler);
             }
-        } catch (InterruptedException e) {
-            // ignore & restore interrupt
-            Thread.currentThread().interrupt();
         }
-    }
 
-    private void buildDiffAndSerializeStates(ClusterState clusterState, ClusterState previousState, Set<DiscoveryNode> nodesToPublishTo,
-                                             boolean sendFullVersion, Map<Version, BytesReference> serializedStates, Map<Version, BytesReference> serializedDiffs) {
-        Diff<ClusterState> diff = null;
-        for (final DiscoveryNode node : nodesToPublishTo) {
+        if (publishTimeout.millis() > 0) {
+            // only wait if the publish timeout is configured...
             try {
-                if (sendFullVersion || !previousState.nodes().nodeExists(node.id())) {
-                    // will send a full reference
-                    if (serializedStates.containsKey(node.version()) == false) {
-                        serializedStates.put(node.version(), serializeFullClusterState(clusterState, node.version()));
-                    }
-                } else {
-                    // will send a diff
-                    if (diff == null) {
-                        diff = clusterState.diff(previousState);
-                    }
-                    if (serializedDiffs.containsKey(node.version()) == false) {
-                        serializedDiffs.put(node.version(), serializeDiffClusterState(diff, node.version()));
+                timedOutWaitingForNodes.set(!publishResponseHandler.awaitAllNodes(publishTimeout));
+                if (timedOutWaitingForNodes.get()) {
+                    DiscoveryNode[] pendingNodes = publishResponseHandler.pendingNodes();
+                    // everyone may have just responded
+                    if (pendingNodes.length > 0) {
+                        logger.warn("timed out waiting for all nodes to process published state [{}] (timeout [{}], pending nodes: {})", clusterState.version(), publishTimeout, pendingNodes);
                     }
                 }
-            } catch (IOException e) {
-                throw new ElasticsearchException("failed to serialize cluster_state for publishing to node {}", e, node);
+            } catch (InterruptedException e) {
+                // ignore & restore interrupt
+                Thread.currentThread().interrupt();
             }
         }
     }
 
-    private void sendFullClusterState(ClusterState clusterState, Map<Version, BytesReference> serializedStates,
-                                      DiscoveryNode node, TimeValue publishTimeout, SendingController sendingController) {
-        BytesReference bytes = serializedStates.get(node.version());
+    private void sendFullClusterState(ClusterState clusterState, @Nullable Map<Version, BytesReference> serializedStates,
+                                      DiscoveryNode node, AtomicBoolean timedOutWaitingForNodes, TimeValue publishTimeout,
+                                      BlockingClusterStatePublishResponseHandler publishResponseHandler) {
+        BytesReference bytes = null;
+        if (serializedStates != null) {
+            bytes = serializedStates.get(node.version());
+        }
         if (bytes == null) {
             try {
                 bytes = serializeFullClusterState(clusterState, node.version());
-                serializedStates.put(node.version(), bytes);
+                if (serializedStates != null) {
+                    serializedStates.put(node.version(), bytes);
+                }
             } catch (Throwable e) {
                 logger.warn("failed to serialize cluster_state before publishing it to node {}", e, node);
-                sendingController.onNodeSendFailed(node, e);
+                publishResponseHandler.onFailure(node, e);
                 return;
             }
         }
-        sendClusterStateToNode(clusterState, bytes, node, publishTimeout, sendingController, false, serializedStates);
+        publishClusterStateToNode(clusterState, bytes, node, timedOutWaitingForNodes, publishTimeout, publishResponseHandler, false);
     }
 
-    private void sendClusterStateDiff(ClusterState clusterState,
-                                      Map<Version, BytesReference> serializedDiffs, Map<Version, BytesReference> serializedStates,
-                                      DiscoveryNode node, TimeValue publishTimeout, SendingController sendingController) {
+    private void sendClusterStateDiff(ClusterState clusterState, Diff diff, Map<Version, BytesReference> serializedDiffs, DiscoveryNode node,
+                                      AtomicBoolean timedOutWaitingForNodes, TimeValue publishTimeout,
+                                      BlockingClusterStatePublishResponseHandler publishResponseHandler) {
         BytesReference bytes = serializedDiffs.get(node.version());
-        assert bytes != null : "failed to find serialized diff for node " + node + " of version [" + node.version() + "]";
-        sendClusterStateToNode(clusterState, bytes, node, publishTimeout, sendingController, true, serializedStates);
+        if (bytes == null) {
+            try {
+                bytes = serializeDiffClusterState(diff, node.version());
+                serializedDiffs.put(node.version(), bytes);
+            } catch (Throwable e) {
+                logger.warn("failed to serialize diff of cluster_state before publishing it to node {}", e, node);
+                publishResponseHandler.onFailure(node, e);
+                return;
+            }
+        }
+        publishClusterStateToNode(clusterState, bytes, node, timedOutWaitingForNodes, publishTimeout, publishResponseHandler, true);
     }
 
-    private void sendClusterStateToNode(final ClusterState clusterState, BytesReference bytes,
-                                        final DiscoveryNode node,
-                                        final TimeValue publishTimeout,
-                                        final SendingController sendingController,
-                                        final boolean sendDiffs, final Map<Version, BytesReference> serializedStates) {
+    private void publishClusterStateToNode(final ClusterState clusterState, BytesReference bytes,
+                                           final DiscoveryNode node, final AtomicBoolean timedOutWaitingForNodes,
+                                           final TimeValue publishTimeout,
+                                           final BlockingClusterStatePublishResponseHandler publishResponseHandler,
+                                           final boolean sendDiffs) {
         try {
-
-            // -> no need to put a timeout on the options here, because we want the response to eventually be received
-            //  and not log an error if it arrives after the timeout
-            // -> no need to compress, we already compressed the bytes
             TransportRequestOptions options = TransportRequestOptions.options().withType(TransportRequestOptions.Type.STATE).withCompress(false);
-            transportService.sendRequest(node, SEND_ACTION_NAME,
+            // no need to put a timeout on the options here, because we want the response to eventually be received
+            // and not log an error if it arrives after the timeout
+            transportService.sendRequest(node, ACTION_NAME,
                     new BytesTransportRequest(bytes, node.version()),
-                    options,
+                    options, // no need to compress, we already compressed the bytes
+
                     new EmptyTransportResponseHandler(ThreadPool.Names.SAME) {
 
                         @Override
                         public void handleResponse(TransportResponse.Empty response) {
-                            if (sendingController.getPublishingTimedOut()) {
+                            if (timedOutWaitingForNodes.get()) {
                                 logger.debug("node {} responded for cluster state [{}] (took longer than [{}])", node, clusterState.version(), publishTimeout);
                             }
-                            sendingController.onNodeSendAck(node);
+                            publishResponseHandler.onResponse(node);
                         }
 
                         @Override
                         public void handleException(TransportException exp) {
                             if (sendDiffs && exp.unwrapCause() instanceof IncompatibleClusterStateVersionException) {
                                 logger.debug("resending full cluster state to node {} reason {}", node, exp.getDetailedMessage());
-                                sendFullClusterState(clusterState, serializedStates, node, publishTimeout, sendingController);
+                                sendFullClusterState(clusterState, null, node, timedOutWaitingForNodes, publishTimeout, publishResponseHandler);
                             } else {
                                 logger.debug("failed to send cluster state to {}", exp, node);
-                                sendingController.onNodeSendFailed(node, exp);
+                                publishResponseHandler.onFailure(node, exp);
                             }
                         }
                     });
         } catch (Throwable t) {
             logger.warn("error sending cluster state to {}", t, node);
-            sendingController.onNodeSendFailed(node, t);
-        }
-    }
-
-    private void sendCommitToNode(final DiscoveryNode node, final ClusterState clusterState, final SendingController sendingController) {
-        try {
-            logger.trace("sending commit for cluster state (uuid: [{}], version [{}]) to [{}]", clusterState.stateUUID(), clusterState.version(), node);
-            TransportRequestOptions options = TransportRequestOptions.options().withType(TransportRequestOptions.Type.STATE);
-            // no need to put a timeout on the options here, because we want the response to eventually be received
-            // and not log an error if it arrives after the timeout
-            transportService.sendRequest(node, COMMIT_ACTION_NAME,
-                    new CommitClusterStateRequest(clusterState.stateUUID()),
-                    options,
-                    new EmptyTransportResponseHandler(ThreadPool.Names.SAME) {
-
-                        @Override
-                        public void handleResponse(TransportResponse.Empty response) {
-                            if (sendingController.getPublishingTimedOut()) {
-                                logger.debug("node {} responded to cluster state commit [{}]", node, clusterState.version());
-                            }
-                            sendingController.getPublishResponseHandler().onResponse(node);
-                        }
-
-                        @Override
-                        public void handleException(TransportException exp) {
-                            logger.debug("failed to commit cluster state (uuid [{}], version [{}]) to {}", exp, clusterState.stateUUID(), clusterState.version(), node);
-                            sendingController.getPublishResponseHandler().onFailure(node, exp);
-                        }
-                    });
-        } catch (Throwable t) {
-            logger.warn("error sending cluster state commit (uuid [{}], version [{}]) to {}", t, clusterState.stateUUID(), clusterState.version(), node);
-            sendingController.getPublishResponseHandler().onFailure(node, t);
+            publishResponseHandler.onFailure(node, t);
         }
     }
 
-
     public static BytesReference serializeFullClusterState(ClusterState clusterState, Version nodeVersion) throws IOException {
         BytesStreamOutput bStream = new BytesStreamOutput();
         try (StreamOutput stream = CompressorFactory.defaultCompressor().streamOutput(bStream)) {
@@ -331,279 +252,63 @@ public class PublishClusterStateAction extends AbstractComponent {
         return bStream.bytes();
     }
 
-    private Object lastSeenClusterStateMutex = new Object();
-    private ClusterState lastSeenClusterState;
-
-    protected void handleIncomingClusterStateRequest(BytesTransportRequest request, TransportChannel channel) throws IOException {
-        Compressor compressor = CompressorFactory.compressor(request.bytes());
-        StreamInput in;
-        if (compressor != null) {
-            in = compressor.streamInput(request.bytes().streamInput());
-        } else {
-            in = request.bytes().streamInput();
-        }
-        in.setVersion(request.version());
-        synchronized (lastSeenClusterStateMutex) {
-            final ClusterState incomingState;
-            // If true we received full cluster state - otherwise diffs
-            if (in.readBoolean()) {
-                incomingState = ClusterState.Builder.readFrom(in, nodesProvider.nodes().localNode());
-                logger.debug("received full cluster state version [{}] with size [{}]", incomingState.version(), request.bytes().length());
-            } else if (lastSeenClusterState != null) {
-                Diff<ClusterState> diff = lastSeenClusterState.readDiffFrom(in);
-                incomingState = diff.apply(lastSeenClusterState);
-                logger.debug("received diff cluster state version [{}] with uuid [{}], diff size [{}]", incomingState.version(), incomingState.stateUUID(), request.bytes().length());
-            } else {
-                logger.debug("received diff for but don't have any local cluster state - requesting full state");
-                throw new IncompatibleClusterStateVersionException("have no local cluster state");
-            }
-            // sanity check incoming state
-            validateIncomingState(incomingState, lastSeenClusterState);
-
-            pendingStatesQueue.addPending(incomingState);
-            lastSeenClusterState = incomingState;
-            lastSeenClusterState.status(ClusterState.ClusterStateStatus.RECEIVED);
-        }
-        channel.sendResponse(TransportResponse.Empty.INSTANCE);
-    }
-
-    // package private for testing
-
-    /**
-     * does simple sanity check of the incoming cluster state. Throws an exception on rejections.
-     */
-    void validateIncomingState(ClusterState incomingState, ClusterState lastSeenClusterState) {
-        final ClusterName incomingClusterName = incomingState.getClusterName();
-        if (!incomingClusterName.equals(this.clusterName)) {
-            logger.warn("received cluster state from [{}] which is also master but with a different cluster name [{}]", incomingState.nodes().masterNode(), incomingClusterName);
-            throw new IllegalStateException("received state from a node that is not part of the cluster");
-        }
-        final DiscoveryNodes currentNodes = nodesProvider.nodes();
-
-        if (currentNodes.localNode().equals(incomingState.nodes().localNode()) == false) {
-            logger.warn("received a cluster state from [{}] and not part of the cluster, should not happen", incomingState.nodes().masterNode());
-            throw new IllegalStateException("received state from a node that is not part of the cluster");
-        }
-
-        ZenDiscovery.validateStateIsFromCurrentMaster(logger, currentNodes, incomingState);
-    }
-
-    protected void handleCommitRequest(CommitClusterStateRequest request, final TransportChannel channel) {
-        final ClusterState state = pendingStatesQueue.markAsCommitted(request.stateUUID, new PendingClusterStatesQueue.StateProcessedListener() {
-            @Override
-            public void onNewClusterStateProcessed() {
-                try {
-                    // send a response to the master to indicate that this cluster state has been processed post committing it.
-                    channel.sendResponse(TransportResponse.Empty.INSTANCE);
-                } catch (Throwable e) {
-                    logger.debug("failed to send response on cluster state processed", e);
-                    onNewClusterStateFailed(e);
-                }
-            }
-
-            @Override
-            public void onNewClusterStateFailed(Throwable t) {
-                try {
-                    channel.sendResponse(t);
-                } catch (Throwable e) {
-                    logger.debug("failed to send response on cluster state processed", e);
-                }
-            }
-        });
-        if (state != null) {
-            newPendingClusterStatelistener.onNewClusterState("master " + state.nodes().masterNode() + " committed version [" + state.version() + "]");
-        }
-    }
-
-    private class SendClusterStateRequestHandler implements TransportRequestHandler<BytesTransportRequest> {
+    private class PublishClusterStateRequestHandler implements TransportRequestHandler<BytesTransportRequest> {
+        private ClusterState lastSeenClusterState;
 
         @Override
         public void messageReceived(BytesTransportRequest request, final TransportChannel channel) throws Exception {
-            handleIncomingClusterStateRequest(request, channel);
-        }
-    }
-
-    private class CommitClusterStateRequestHandler implements TransportRequestHandler<CommitClusterStateRequest> {
-        @Override
-        public void messageReceived(CommitClusterStateRequest request, final TransportChannel channel) throws Exception {
-            handleCommitRequest(request, channel);
-        }
-    }
-
-    protected static class CommitClusterStateRequest extends TransportRequest {
-
-        String stateUUID;
-
-        public CommitClusterStateRequest() {
-        }
-
-        public CommitClusterStateRequest(String stateUUID) {
-            this.stateUUID = stateUUID;
-        }
-
-        @Override
-        public void readFrom(StreamInput in) throws IOException {
-            super.readFrom(in);
-            stateUUID = in.readString();
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            super.writeTo(out);
-            out.writeString(stateUUID);
-        }
-    }
-
-
-    /**
-     * Coordinates acknowledgments of the sent cluster state from the different nodes. Commits the change
-     * after `minimum_master_nodes` have successfully responded or fails the entire change. After committing
-     * the cluster state, will trigger a commit message to all nodes that responded previously and responds immediately
-     * to all future acknowledgments.
-     */
-    class SendingController {
-
-        private final ClusterState clusterState;
-
-        public BlockingClusterStatePublishResponseHandler getPublishResponseHandler() {
-            return publishResponseHandler;
-        }
-
-        private final BlockingClusterStatePublishResponseHandler publishResponseHandler;
-        final ArrayList<DiscoveryNode> sendAckedBeforeCommit = new ArrayList<>();
-
-        // writes and reads of these are protected under synchronization
-        final CountDownLatch committedOrFailedLatch; // 0 count indicates that a decision was made w.r.t committing or failing
-        boolean committed;  // true if cluster state was committed
-        int neededMastersToCommit; // number of master nodes acks still needed before committing
-        int pendingMasterNodes; // how many master node still need to respond
-
-        // an external marker to note that the publishing process is timed out. This is useful for proper logging.
-        final AtomicBoolean publishingTimedOut = new AtomicBoolean();
-
-        private SendingController(ClusterState clusterState, int minMasterNodes, int totalMasterNodes, BlockingClusterStatePublishResponseHandler publishResponseHandler) {
-            this.clusterState = clusterState;
-            this.publishResponseHandler = publishResponseHandler;
-            this.neededMastersToCommit = Math.max(0, minMasterNodes - 1); // we are one of the master nodes
-            this.pendingMasterNodes = totalMasterNodes - 1;
-            if (this.neededMastersToCommit > this.pendingMasterNodes) {
-                throw new Discovery.FailedToCommitClusterStateException("not enough masters to ack sent cluster state. [{}] needed , have [{}]", neededMastersToCommit, pendingMasterNodes);
-            }
-            this.committed = neededMastersToCommit == 0;
-            this.committedOrFailedLatch = new CountDownLatch(committed ? 0 : 1);
-        }
-
-        public void waitForCommit(TimeValue commitTimeout) {
-            boolean timedout = false;
-            try {
-                timedout = committedOrFailedLatch.await(commitTimeout.millis(), TimeUnit.MILLISECONDS) == false;
-            } catch (InterruptedException e) {
-                // the commit check bellow will either translate to an exception or we are committed and we can safely continue
-            }
-
-            if (timedout) {
-                markAsFailed("timed out waiting for commit (commit timeout [" + commitTimeout + "]");
-            }
-            if (isCommitted() == false) {
-                throw new Discovery.FailedToCommitClusterStateException("{} enough masters to ack sent cluster state. [{}] left",
-                        timedout ? "timed out while waiting for" : "failed to get", neededMastersToCommit);
-            }
-        }
-
-        synchronized public boolean isCommitted() {
-            return committed;
-        }
-
-        synchronized public void onNodeSendAck(DiscoveryNode node) {
-            if (committed) {
-                assert sendAckedBeforeCommit.isEmpty();
-                sendCommitToNode(node, clusterState, this);
-            } else if (committedOrFailed()) {
-                logger.trace("ignoring ack from [{}] for cluster state version [{}]. already failed", node, clusterState.version());
+            Compressor compressor = CompressorFactory.compressor(request.bytes());
+            StreamInput in;
+            if (compressor != null) {
+                in = compressor.streamInput(request.bytes().streamInput());
             } else {
-                // we're still waiting
-                sendAckedBeforeCommit.add(node);
-                if (node.isMasterNode()) {
-                    checkForCommitOrFailIfNoPending(node);
+                in = request.bytes().streamInput();
+            }
+            in.setVersion(request.version());
+            synchronized (this) {
+                // If true we received full cluster state - otherwise diffs
+                if (in.readBoolean()) {
+                    lastSeenClusterState = ClusterState.Builder.readFrom(in, nodesProvider.nodes().localNode());
+                    logger.debug("received full cluster state version {} with size {}", lastSeenClusterState.version(), request.bytes().length());
+                } else if (lastSeenClusterState != null) {
+                    Diff<ClusterState> diff = lastSeenClusterState.readDiffFrom(in);
+                    lastSeenClusterState = diff.apply(lastSeenClusterState);
+                    logger.debug("received diff cluster state version {} with uuid {}, diff size {}", lastSeenClusterState.version(), lastSeenClusterState.stateUUID(), request.bytes().length());
+                } else {
+                    logger.debug("received diff for but don't have any local cluster state - requesting full state");
+                    throw new IncompatibleClusterStateVersionException("have no local cluster state");
                 }
+                lastSeenClusterState.status(ClusterState.ClusterStateStatus.RECEIVED);
             }
-        }
 
-        private synchronized boolean committedOrFailed() {
-            return committedOrFailedLatch.getCount() == 0;
-        }
+            try {
+                listener.onNewClusterState(lastSeenClusterState, new NewClusterStateListener.NewStateProcessed() {
+                    @Override
+                    public void onNewClusterStateProcessed() {
+                        try {
+                            channel.sendResponse(TransportResponse.Empty.INSTANCE);
+                        } catch (Throwable e) {
+                            logger.debug("failed to send response on cluster state processed", e);
+                        }
+                    }
 
-        /**
-         * check if enough master node responded to commit the change. fails the commit
-         * if there are no more pending master nodes but not enough acks to commit.
-         */
-        synchronized private void checkForCommitOrFailIfNoPending(DiscoveryNode masterNode) {
-            logger.trace("master node {} acked cluster state version [{}]. processing ... (current pending [{}], needed [{}])",
-                    masterNode, clusterState.version(), pendingMasterNodes, neededMastersToCommit);
-            neededMastersToCommit--;
-            if (neededMastersToCommit == 0) {
-                if (markAsCommitted()) {
-                    for (DiscoveryNode nodeToCommit : sendAckedBeforeCommit) {
-                        sendCommitToNode(nodeToCommit, clusterState, this);
+                    @Override
+                    public void onNewClusterStateFailed(Throwable t) {
+                        try {
+                            channel.sendResponse(t);
+                        } catch (Throwable e) {
+                            logger.debug("failed to send response on cluster state processed", e);
+                        }
                     }
-                    sendAckedBeforeCommit.clear();
+                });
+            } catch (Exception e) {
+                logger.warn("unexpected error while processing cluster state version [{}]", e, lastSeenClusterState.version());
+                try {
+                    channel.sendResponse(e);
+                } catch (Throwable e1) {
+                    logger.debug("failed to send response on cluster state processed", e1);
                 }
             }
-            decrementPendingMasterAcksAndChangeForFailure();
-        }
-
-        synchronized private void decrementPendingMasterAcksAndChangeForFailure() {
-            pendingMasterNodes--;
-            if (pendingMasterNodes == 0 && neededMastersToCommit > 0) {
-                markAsFailed("no more pending master nodes, but failed to reach needed acks ([" + neededMastersToCommit + "] left)");
-            }
-        }
-
-        synchronized public void onNodeSendFailed(DiscoveryNode node, Throwable t) {
-            if (node.isMasterNode()) {
-                logger.trace("master node {} failed to ack cluster state version [{}]. processing ... (current pending [{}], needed [{}])",
-                        node, clusterState.version(), pendingMasterNodes, neededMastersToCommit);
-                decrementPendingMasterAcksAndChangeForFailure();
-            }
-            publishResponseHandler.onFailure(node, t);
-        }
-
-        /**
-         * tries and commit the current state, if a decision wasn't made yet
-         *
-         * @return true if successful
-         */
-        synchronized private boolean markAsCommitted() {
-            if (committedOrFailed()) {
-                return committed;
-            }
-            logger.trace("committing version [{}]", clusterState.version());
-            committed = true;
-            committedOrFailedLatch.countDown();
-            return true;
-        }
-
-        /**
-         * tries marking the publishing as failed, if a decision wasn't made yet
-         *
-         * @return true if the publishing was failed and the cluster state is *not* committed
-         **/
-        synchronized private boolean markAsFailed(String reason) {
-            if (committedOrFailed()) {
-                return committed == false;
-            }
-            logger.trace("failed to commit version [{}]. {}", clusterState.version(), reason);
-            committed = false;
-            committedOrFailedLatch.countDown();
-            return true;
-        }
-
-        public boolean getPublishingTimedOut() {
-            return publishingTimedOut.get();
-        }
-
-        public void setPublishingTimedOut(boolean isTimedOut) {
-            publishingTimedOut.set(isTimedOut);
         }
     }
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/gateway/LocalAllocateDangledIndices.java b/core/src/main/java/org/elasticsearch/gateway/LocalAllocateDangledIndices.java
index 36c3af0..e783007 100644
--- a/core/src/main/java/org/elasticsearch/gateway/LocalAllocateDangledIndices.java
+++ b/core/src/main/java/org/elasticsearch/gateway/LocalAllocateDangledIndices.java
@@ -188,12 +188,12 @@ public class LocalAllocateDangledIndices extends AbstractComponent {
         }
     }
 
-    static class AllocateDangledRequest extends TransportRequest {
+    public static class AllocateDangledRequest extends TransportRequest {
 
         DiscoveryNode fromNode;
         IndexMetaData[] indices;
 
-        AllocateDangledRequest() {
+        public AllocateDangledRequest() {
         }
 
         AllocateDangledRequest(DiscoveryNode fromNode, IndexMetaData[] indices) {
diff --git a/core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayMetaState.java b/core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayMetaState.java
index accccc4..240c00a 100644
--- a/core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayMetaState.java
+++ b/core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayMetaState.java
@@ -120,7 +120,7 @@ public class TransportNodesListGatewayMetaState extends TransportNodesAction<Tra
         return true;
     }
 
-    static class Request extends BaseNodesRequest<Request> {
+    public static class Request extends BaseNodesRequest<Request> {
 
         public Request() {
         }
@@ -177,9 +177,9 @@ public class TransportNodesListGatewayMetaState extends TransportNodesAction<Tra
     }
 
 
-    static class NodeRequest extends BaseNodeRequest {
+    public static class NodeRequest extends BaseNodeRequest {
 
-        NodeRequest() {
+        public NodeRequest() {
         }
 
         NodeRequest(String nodeId, TransportNodesListGatewayMetaState.Request request) {
diff --git a/core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java b/core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java
index 11cbbef..ae7b5bd 100644
--- a/core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java
+++ b/core/src/main/java/org/elasticsearch/gateway/TransportNodesListGatewayStartedShards.java
@@ -160,7 +160,7 @@ public class TransportNodesListGatewayStartedShards extends TransportNodesAction
         return true;
     }
 
-    static class Request extends BaseNodesRequest<Request> {
+    public static class Request extends BaseNodesRequest<Request> {
 
         private ShardId shardId;
         private String indexUUID;
@@ -233,12 +233,12 @@ public class TransportNodesListGatewayStartedShards extends TransportNodesAction
     }
 
 
-    static class NodeRequest extends BaseNodeRequest {
+    public static class NodeRequest extends BaseNodeRequest {
 
         private ShardId shardId;
         private String indexUUID;
 
-        NodeRequest() {
+        public NodeRequest() {
         }
 
         NodeRequest(String nodeId, TransportNodesListGatewayStartedShards.Request request) {
@@ -275,7 +275,7 @@ public class TransportNodesListGatewayStartedShards extends TransportNodesAction
         private long version = -1;
         private Throwable storeException = null;
 
-        NodeGatewayStartedShards() {
+        public NodeGatewayStartedShards() {
         }
         public NodeGatewayStartedShards(DiscoveryNode node, long version) {
             this(node, version, null);
diff --git a/core/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java b/core/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java
index fa0c1a9..634e38f 100644
--- a/core/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java
+++ b/core/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java
@@ -135,7 +135,8 @@ public class NettyHttpServerTransport extends AbstractLifecycleComponent<HttpSer
 
     protected volatile List<Channel> serverChannels = new ArrayList<>();
 
-    protected OpenChannelsHandler serverOpenChannels;
+    // package private for testing
+    OpenChannelsHandler serverOpenChannels;
 
     protected volatile HttpServerAdapter httpServerAdapter;
 
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java b/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java
index 188dc80..bb991ef 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.index.mapper;
 
 import com.google.common.collect.ImmutableMap;
-import com.google.common.collect.ImmutableSortedMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseFieldMatcher;
@@ -36,33 +35,10 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.analysis.AnalysisService;
-import org.elasticsearch.index.mapper.core.BinaryFieldMapper;
-import org.elasticsearch.index.mapper.core.BooleanFieldMapper;
-import org.elasticsearch.index.mapper.core.ByteFieldMapper;
-import org.elasticsearch.index.mapper.core.CompletionFieldMapper;
-import org.elasticsearch.index.mapper.core.DateFieldMapper;
-import org.elasticsearch.index.mapper.core.DoubleFieldMapper;
-import org.elasticsearch.index.mapper.core.FloatFieldMapper;
-import org.elasticsearch.index.mapper.core.IntegerFieldMapper;
-import org.elasticsearch.index.mapper.core.LongFieldMapper;
-import org.elasticsearch.index.mapper.core.ShortFieldMapper;
-import org.elasticsearch.index.mapper.core.StringFieldMapper;
-import org.elasticsearch.index.mapper.core.TokenCountFieldMapper;
-import org.elasticsearch.index.mapper.core.TypeParsers;
+import org.elasticsearch.index.mapper.core.*;
 import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
 import org.elasticsearch.index.mapper.geo.GeoShapeFieldMapper;
-import org.elasticsearch.index.mapper.internal.AllFieldMapper;
-import org.elasticsearch.index.mapper.internal.FieldNamesFieldMapper;
-import org.elasticsearch.index.mapper.internal.IdFieldMapper;
-import org.elasticsearch.index.mapper.internal.IndexFieldMapper;
-import org.elasticsearch.index.mapper.internal.ParentFieldMapper;
-import org.elasticsearch.index.mapper.internal.RoutingFieldMapper;
-import org.elasticsearch.index.mapper.internal.SourceFieldMapper;
-import org.elasticsearch.index.mapper.internal.TTLFieldMapper;
-import org.elasticsearch.index.mapper.internal.TimestampFieldMapper;
-import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
-import org.elasticsearch.index.mapper.internal.UidFieldMapper;
-import org.elasticsearch.index.mapper.internal.VersionFieldMapper;
+import org.elasticsearch.index.mapper.internal.*;
 import org.elasticsearch.index.mapper.ip.IpFieldMapper;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.index.mapper.object.RootObjectMapper;
@@ -71,11 +47,7 @@ import org.elasticsearch.index.similarity.SimilarityLookupService;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptService;
 
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
+import java.util.*;
 
 import static org.elasticsearch.index.mapper.MapperBuilders.doc;
 
@@ -96,7 +68,7 @@ public class DocumentMapperParser {
 
     private volatile ImmutableMap<String, Mapper.TypeParser> typeParsers;
     private volatile ImmutableMap<String, Mapper.TypeParser> rootTypeParsers;
-    private volatile ImmutableMap<String, Mapper.TypeParser> additionalRootMappers;
+    private volatile SortedMap<String, Mapper.TypeParser> additionalRootMappers;
 
     public DocumentMapperParser(@IndexSettings Settings indexSettings, MapperService mapperService, AnalysisService analysisService,
                                 SimilarityLookupService similarityLookupService, ScriptService scriptService) {
@@ -145,7 +117,7 @@ public class DocumentMapperParser {
                 .put(IdFieldMapper.NAME, new IdFieldMapper.TypeParser())
                 .put(FieldNamesFieldMapper.NAME, new FieldNamesFieldMapper.TypeParser())
                 .immutableMap();
-        additionalRootMappers = ImmutableSortedMap.<String, Mapper.TypeParser>of();
+        additionalRootMappers = Collections.emptySortedMap();
         indexVersionCreated = Version.indexCreated(indexSettings);
     }
 
@@ -162,10 +134,10 @@ public class DocumentMapperParser {
             rootTypeParsers = new MapBuilder<>(rootTypeParsers)
                     .put(type, typeParser)
                     .immutableMap();
-            additionalRootMappers = ImmutableSortedMap.<String, Mapper.TypeParser>naturalOrder()
-                    .putAll(additionalRootMappers)
-                    .put(type, typeParser)
-                    .build();
+            SortedMap<String, Mapper.TypeParser> newAdditionalRootMappers = new TreeMap<>();
+            newAdditionalRootMappers.putAll(additionalRootMappers);
+            newAdditionalRootMappers.put(type, typeParser);
+            additionalRootMappers = Collections.unmodifiableSortedMap(newAdditionalRootMappers);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java
index 9e24b50..686cfcf 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java
@@ -219,6 +219,9 @@ public class DateFieldMapper extends NumberFieldMapper {
 
             @Override
             public Query rewrite(IndexReader reader) throws IOException {
+                if (getBoost() != 1.0F) {
+                    return super.rewrite(reader);
+                }
                 return innerRangeQuery(lowerTerm, upperTerm, includeLower, includeUpper, timeZone, forcedDateParser);
             }
 
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java
index 08974cb..863d71f 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java
@@ -26,11 +26,11 @@ import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexOptions;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.NumericUtils;
+import org.apache.lucene.util.XGeoHashUtils;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.Explicit;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.geo.GeoDistance;
-import org.elasticsearch.common.geo.GeoHashUtils;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.geo.GeoUtils;
 import org.elasticsearch.common.settings.Settings;
@@ -97,7 +97,7 @@ public class GeoPointFieldMapper extends FieldMapper implements ArrayValueMapper
         public static final boolean ENABLE_LATLON = false;
         public static final boolean ENABLE_GEOHASH = false;
         public static final boolean ENABLE_GEOHASH_PREFIX = false;
-        public static final int GEO_HASH_PRECISION = GeoHashUtils.PRECISION;
+        public static final int GEO_HASH_PRECISION = XGeoHashUtils.PRECISION;
 
         public static final Explicit<Boolean> IGNORE_MALFORMED = new Explicit(false, false);
         public static final Explicit<Boolean> COERCE = new Explicit(false, false);
@@ -719,7 +719,7 @@ public class GeoPointFieldMapper extends FieldMapper implements ArrayValueMapper
         }
         if (fieldType().isGeohashEnabled()) {
             if (geohash == null) {
-                geohash = GeoHashUtils.encode(point.lat(), point.lon());
+                geohash = XGeoHashUtils.stringEncode(point.lon(), point.lat());
             }
             addGeohashField(context, geohash);
         }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java
index 99b348e..0f08a7c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.index.query;
 
-import org.elasticsearch.common.geo.GeoHashUtils;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
@@ -65,7 +64,7 @@ public class GeoBoundingBoxQueryBuilder extends QueryBuilder {
     }
 
     public GeoBoundingBoxQueryBuilder topLeft(String geohash) {
-        return topLeft(GeoHashUtils.decode(geohash));
+        return topLeft(GeoPoint.fromGeohash(geohash));
     }
 
     /**
@@ -85,7 +84,7 @@ public class GeoBoundingBoxQueryBuilder extends QueryBuilder {
     }
 
     public GeoBoundingBoxQueryBuilder bottomRight(String geohash) {
-        return bottomRight(GeoHashUtils.decode(geohash));
+        return bottomRight(GeoPoint.fromGeohash(geohash));
     }
 
     /**
@@ -105,7 +104,7 @@ public class GeoBoundingBoxQueryBuilder extends QueryBuilder {
     }
 
     public GeoBoundingBoxQueryBuilder bottomLeft(String geohash) {
-        return bottomLeft(GeoHashUtils.decode(geohash));
+        return bottomLeft(GeoPoint.fromGeohash(geohash));
     }
     
     /**
@@ -125,7 +124,7 @@ public class GeoBoundingBoxQueryBuilder extends QueryBuilder {
     }
 
     public GeoBoundingBoxQueryBuilder topRight(String geohash) {
-        return topRight(GeoHashUtils.decode(geohash));
+        return topRight(GeoPoint.fromGeohash(geohash));
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java
index 8201381..52bfb54 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java
@@ -22,7 +22,6 @@ package org.elasticsearch.index.query;
 import org.apache.lucene.search.Query;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.geo.GeoDistance;
-import org.elasticsearch.common.geo.GeoHashUtils;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.geo.GeoUtils;
 import org.elasticsearch.common.inject.Inject;
@@ -95,7 +94,7 @@ public class GeoDistanceQueryParser implements QueryParser {
                         } else if (currentName.equals(GeoPointFieldMapper.Names.LON)) {
                             point.resetLon(parser.doubleValue());
                         } else if (currentName.equals(GeoPointFieldMapper.Names.GEOHASH)) {
-                            GeoHashUtils.decode(parser.text(), point);
+                            point.resetFromGeoHash(parser.text());
                         } else {
                             throw new QueryParsingException(parseContext, "[geo_distance] query does not support [" + currentFieldName
                                     + "]");
@@ -120,7 +119,7 @@ public class GeoDistanceQueryParser implements QueryParser {
                     point.resetLon(parser.doubleValue());
                     fieldName = currentFieldName.substring(0, currentFieldName.length() - GeoPointFieldMapper.Names.LON_SUFFIX.length());
                 } else if (currentFieldName.endsWith(GeoPointFieldMapper.Names.GEOHASH_SUFFIX)) {
-                    GeoHashUtils.decode(parser.text(), point);
+                    point.resetFromGeoHash(parser.text());
                     fieldName = currentFieldName.substring(0, currentFieldName.length() - GeoPointFieldMapper.Names.GEOHASH_SUFFIX.length());
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java
index f60d944..0f077aa 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java
@@ -22,7 +22,6 @@ package org.elasticsearch.index.query;
 import org.apache.lucene.search.Query;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.geo.GeoDistance;
-import org.elasticsearch.common.geo.GeoHashUtils;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.geo.GeoUtils;
 import org.elasticsearch.common.inject.Inject;
@@ -150,7 +149,7 @@ public class GeoDistanceRangeQueryParser implements QueryParser {
                     point.resetLon(parser.doubleValue());
                     fieldName = currentFieldName.substring(0, currentFieldName.length() - GeoPointFieldMapper.Names.LON_SUFFIX.length());
                 } else if (currentFieldName.endsWith(GeoPointFieldMapper.Names.GEOHASH_SUFFIX)) {
-                    GeoHashUtils.decode(parser.text(), point);
+                    point.resetFromGeoHash(parser.text());
                     fieldName = currentFieldName.substring(0, currentFieldName.length() - GeoPointFieldMapper.Names.GEOHASH_SUFFIX.length());
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java
index 400384b..da1dac2 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.index.query;
 
-import org.elasticsearch.common.geo.GeoHashUtils;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
@@ -57,7 +56,7 @@ public class GeoPolygonQueryBuilder extends QueryBuilder {
     }
 
     public GeoPolygonQueryBuilder addPoint(String geohash) {
-        return addPoint(GeoHashUtils.decode(geohash));
+        return addPoint(GeoPoint.fromGeohash(geohash));
     }
 
     public GeoPolygonQueryBuilder addPoint(GeoPoint point) {
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java b/core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java
index 814aca4..c4e4e8b 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java
@@ -20,10 +20,10 @@
 package org.elasticsearch.index.query;
 
 import org.apache.lucene.search.Query;
+import org.apache.lucene.util.XGeoHashUtils;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.geo.GeoHashUtils;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.geo.GeoUtils;
 import org.elasticsearch.common.inject.Inject;
@@ -31,9 +31,7 @@ import org.elasticsearch.common.unit.DistanceUnit;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.core.StringFieldMapper;
 import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
 
 import java.io.IOException;
@@ -126,7 +124,7 @@ public class GeohashCellQuery {
         }
 
         public Builder point(double lat, double lon) {
-            this.geohash = GeoHashUtils.encode(lat, lon);
+            this.geohash = XGeoHashUtils.stringEncode(lon, lat);
             return this;
         }
 
@@ -258,7 +256,7 @@ public class GeohashCellQuery {
 
             Query filter;
             if (neighbors) {
-                filter = create(parseContext, geoFieldType, geohash, GeoHashUtils.addNeighbors(geohash, new ArrayList<CharSequence>(8)));
+                filter = create(parseContext, geoFieldType, geohash, XGeoHashUtils.addNeighbors(geohash, new ArrayList<>(8)));
             } else {
                 filter = create(parseContext, geoFieldType, geohash, null);
             }
diff --git a/core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java
index 4df64d5..6a89647 100644
--- a/core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java
@@ -209,9 +209,12 @@ public class HasChildQueryParser implements QueryParser {
 
         @Override
         public Query rewrite(IndexReader reader) throws IOException {
+            if (getBoost() != 1.0F) {
+                return super.rewrite(reader);
+            }
+            String joinField = ParentFieldMapper.joinField(parentType);
             IndexSearcher indexSearcher = new IndexSearcher(reader);
             indexSearcher.setQueryCache(null);
-            String joinField = ParentFieldMapper.joinField(parentType);
             IndexParentChildFieldData indexParentChildFieldData = parentChildIndexFieldData.loadGlobal(indexSearcher.getIndexReader());
             MultiDocValues.OrdinalMap ordinalMap = ParentChildIndexFieldData.getOrdinalMap(indexParentChildFieldData, parentType);
             return JoinUtil.createJoinQuery(joinField, innerQuery, toQuery, indexSearcher, scoreMode, ordinalMap, minChildren, maxChildren);
diff --git a/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java
index 9059865..b094915 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java
@@ -146,6 +146,13 @@ public class MultiMatchQueryBuilder extends QueryBuilder implements BoostableQue
             return type;
         }
     }
+    
+    /**
+     * Returns the type (for testing)
+     */
+    public MultiMatchQueryBuilder.Type getType() {
+        return type;
+    }
 
     /**
      * Constructs a new text query.
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java
index 3dc2427..db16951 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java
@@ -89,8 +89,7 @@ import java.util.Locale;
  * <p>
  * See {@link GaussDecayFunctionBuilder} and {@link GaussDecayFunctionParser}
  * for an example. The parser furthermore needs to be registered in the
- * {@link org.elasticsearch.index.query.functionscore.FunctionScoreModule
- * FunctionScoreModule}.
+ * {@link org.elasticsearch.search.SearchModule SearchModule}.
  *
  * **/
 
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java
index dc7571a..dec90b1 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java
@@ -35,8 +35,6 @@ public class FunctionScoreQueryBuilder extends QueryBuilder implements Boostable
 
     private final QueryBuilder queryBuilder;
 
-    private final QueryBuilder filterBuilder;
-
     private Float boost;
 
     private Float maxBoost;
@@ -57,11 +55,9 @@ public class FunctionScoreQueryBuilder extends QueryBuilder implements Boostable
      */
     public FunctionScoreQueryBuilder(QueryBuilder queryBuilder) {
         this.queryBuilder = queryBuilder;
-        this.filterBuilder = null;
     }
 
     public FunctionScoreQueryBuilder() {
-        this.filterBuilder = null;
         this.queryBuilder = null;
     }
 
@@ -75,7 +71,6 @@ public class FunctionScoreQueryBuilder extends QueryBuilder implements Boostable
             throw new IllegalArgumentException("function_score: function must not be null");
         }
         queryBuilder = null;
-        filterBuilder = null;
         this.filters.add(null);
         this.scoreFunctions.add(scoreFunctionBuilder);
     }
@@ -161,10 +156,6 @@ public class FunctionScoreQueryBuilder extends QueryBuilder implements Boostable
             builder.field("query");
             queryBuilder.toXContent(builder, params);
         }
-        if (filterBuilder != null) {
-            builder.field("filter");
-            filterBuilder.toXContent(builder, params);
-        }
         builder.startArray("functions");
         for (int i = 0; i < filters.size(); i++) {
             builder.startObject();
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java
index c2c6494..879bfea 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java
@@ -21,7 +21,6 @@ package org.elasticsearch.index.query.functionscore;
 
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.ImmutableMap.Builder;
-
 import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.ConstantScoreQuery;
@@ -31,20 +30,14 @@ import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.lucene.search.Queries;
-import org.elasticsearch.common.lucene.search.function.CombineFunction;
-import org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery;
-import org.elasticsearch.common.lucene.search.function.FunctionScoreQuery;
-import org.elasticsearch.common.lucene.search.function.ScoreFunction;
-import org.elasticsearch.common.lucene.search.function.WeightFactorFunction;
+import org.elasticsearch.common.lucene.search.function.*;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParser;
 import org.elasticsearch.index.query.QueryParsingException;
-import org.elasticsearch.index.query.functionscore.factor.FactorParser;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Arrays;
 
 /**
  *
@@ -55,7 +48,6 @@ public class FunctionScoreQueryParser implements QueryParser {
 
     // For better readability of error message
     static final String MISPLACED_FUNCTION_MESSAGE_PREFIX = "you can either define [functions] array or a single function, not both. ";
-    static final String MISPLACED_BOOST_FUNCTION_MESSAGE_SUFFIX = " did you mean [boost] instead?";
 
     public static final ParseField WEIGHT_FIELD = new ParseField("weight");
     private static final ParseField FILTER_FIELD = new ParseField("filter").withAllDeprecated("query");
@@ -124,7 +116,7 @@ public class FunctionScoreQueryParser implements QueryParser {
             } else if ("functions".equals(currentFieldName)) {
                 if (singleFunctionFound) {
                     String errorString = "already found [" + singleFunctionName + "], now encountering [functions].";
-                    handleMisplacedFunctionsDeclaration(errorString, singleFunctionName);
+                    handleMisplacedFunctionsDeclaration(errorString);
                 }
                 currentFieldName = parseFiltersAndFunctions(parseContext, parser, filterFunctions, currentFieldName);
                 functionArrayFound = true;
@@ -141,7 +133,7 @@ public class FunctionScoreQueryParser implements QueryParser {
                 }
                 if (functionArrayFound) {
                     String errorString = "already found [functions] array, now encountering [" + currentFieldName + "].";
-                    handleMisplacedFunctionsDeclaration(errorString, currentFieldName);
+                    handleMisplacedFunctionsDeclaration(errorString);
                 }
                 if (filterFunctions.size() > 0) {
                     throw new ElasticsearchParseException("failed to parse [{}] query. already found function [{}], now encountering [{}]. use [functions] array if you want to define several functions.", NAME, singleFunctionName, currentFieldName);
@@ -191,12 +183,8 @@ public class FunctionScoreQueryParser implements QueryParser {
         }
     }
 
-    private void handleMisplacedFunctionsDeclaration(String errorString, String functionName) {
-        errorString = MISPLACED_FUNCTION_MESSAGE_PREFIX + errorString;
-        if (Arrays.asList(FactorParser.NAMES).contains(functionName)) {
-            errorString = errorString + MISPLACED_BOOST_FUNCTION_MESSAGE_SUFFIX;
-        }
-        throw new ElasticsearchParseException("failed to parse [{}] query. [{}]", NAME, errorString);
+    private void handleMisplacedFunctionsDeclaration(String errorString) {
+        throw new ElasticsearchParseException("failed to parse [{}] query. [{}]", NAME, MISPLACED_FUNCTION_MESSAGE_PREFIX + errorString);
     }
 
     private String parseFiltersAndFunctions(QueryParseContext parseContext, XContentParser parser,
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionBuilders.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionBuilders.java
index ea8e255..23c1ca1 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionBuilders.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionBuilders.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.index.query.functionscore;
 
 import org.elasticsearch.index.query.functionscore.exp.ExponentialDecayFunctionBuilder;
-import org.elasticsearch.index.query.functionscore.factor.FactorBuilder;
 import org.elasticsearch.index.query.functionscore.fieldvaluefactor.FieldValueFactorFunctionBuilder;
 import org.elasticsearch.index.query.functionscore.gauss.GaussDecayFunctionBuilder;
 import org.elasticsearch.index.query.functionscore.lin.LinearDecayFunctionBuilder;
@@ -29,8 +28,6 @@ import org.elasticsearch.index.query.functionscore.script.ScriptScoreFunctionBui
 import org.elasticsearch.index.query.functionscore.weight.WeightBuilder;
 import org.elasticsearch.script.Script;
 
-import java.util.Map;
-
 public class ScoreFunctionBuilders {
    
     public static ExponentialDecayFunctionBuilder exponentialDecayFunction(String fieldName, Object origin, Object scale) {
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionParserMapper.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionParserMapper.java
index fe33532..37a6f80 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionParserMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionParserMapper.java
@@ -23,7 +23,6 @@ import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.query.functionscore.exp.ExponentialDecayFunctionParser;
-import org.elasticsearch.index.query.functionscore.factor.FactorParser;
 import org.elasticsearch.index.query.functionscore.fieldvaluefactor.FieldValueFactorFunctionParser;
 import org.elasticsearch.index.query.functionscore.gauss.GaussDecayFunctionParser;
 import org.elasticsearch.index.query.functionscore.lin.LinearDecayFunctionParser;
@@ -42,8 +41,7 @@ public class ScoreFunctionParserMapper {
     @Inject
     public ScoreFunctionParserMapper(Set<ScoreFunctionParser> parsers) {
         Map<String, ScoreFunctionParser> map = new HashMap<>();
-        // build-in parsers
-        addParser(new FactorParser(), map);
+        // built-in parsers
         addParser(new ScriptScoreFunctionParser(), map);
         addParser(new GaussDecayFunctionParser(), map);
         addParser(new LinearDecayFunctionParser(), map);
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/factor/FactorBuilder.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/factor/FactorBuilder.java
deleted file mode 100644
index 3a176c4..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/factor/FactorBuilder.java
+++ /dev/null
@@ -1,67 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query.functionscore.factor;
-
-import org.elasticsearch.common.lucene.search.function.BoostScoreFunction;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilder;
-
-import java.io.IOException;
-
-/**
- * A query that simply applies the boost factor to another query (multiply it).
- *
- *
- */
-@Deprecated
-public class FactorBuilder extends ScoreFunctionBuilder {
-
-    private Float boostFactor;
-
-    /**
-     * Sets the boost factor for this query.
-     */
-    public FactorBuilder boostFactor(float boost) {
-        this.boostFactor = new Float(boost);
-        return this;
-    }
-
-    @Override
-    public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        if (boostFactor != null) {
-            builder.field("boost_factor", boostFactor.floatValue());
-        }
-    }
-
-    @Override
-    public String getName() {
-        return FactorParser.NAMES[0];
-    }
-
-    @Override
-    public ScoreFunctionBuilder setWeight(float weight) {
-        throw new IllegalArgumentException(BoostScoreFunction.BOOST_WEIGHT_ERROR_MESSAGE);
-    }
-
-    @Override
-    public void buildWeight(XContentBuilder builder) throws IOException {
-        //we do not want the weight to be written for boost_factor as it does not make sense to have it
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/factor/FactorParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/factor/FactorParser.java
deleted file mode 100644
index a1c8d20..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/factor/FactorParser.java
+++ /dev/null
@@ -1,55 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query.functionscore.factor;
-
-import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
-
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.lucene.search.function.BoostScoreFunction;
-import org.elasticsearch.common.lucene.search.function.ScoreFunction;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.index.query.QueryParsingException;
-
-import java.io.IOException;
-
-/**
- *
- */
-@Deprecated
-public class FactorParser implements ScoreFunctionParser {
-
-    public static String[] NAMES = { "boost_factor", "boostFactor" };
-
-    @Inject
-    public FactorParser() {
-    }
-
-    @Override
-    public ScoreFunction parse(QueryParseContext parseContext, XContentParser parser) throws IOException, QueryParsingException {
-        float boostFactor = parser.floatValue();
-        return new BoostScoreFunction(boostFactor);
-    }
-
-    @Override
-    public String[] getNames() {
-        return NAMES;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
index bf463db..8d90eaa 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
@@ -1350,7 +1350,8 @@ public class IndexShard extends AbstractIndexShardComponent {
     }
 
     private final EngineConfig newEngineConfig(TranslogConfig translogConfig, QueryCachingPolicy cachingPolicy) {
-        final TranslogRecoveryPerformer translogRecoveryPerformer = new TranslogRecoveryPerformer(shardId, mapperService, queryParserService, indexAliasesService, indexCache) {
+        final TranslogRecoveryPerformer translogRecoveryPerformer = new TranslogRecoveryPerformer(shardId, mapperService, queryParserService,
+                indexAliasesService, indexCache, logger) {
             @Override
             protected void operationProcessed() {
                 assert recoveryState != null;
diff --git a/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java b/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java
index 7cb4285..76d2efd 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java
@@ -27,6 +27,7 @@ import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentParser;
@@ -34,12 +35,7 @@ import org.elasticsearch.index.aliases.IndexAliasesService;
 import org.elasticsearch.index.cache.IndexCache;
 import org.elasticsearch.index.engine.Engine;
 import org.elasticsearch.index.engine.IgnoreOnRecoveryEngineException;
-import org.elasticsearch.index.mapper.DocumentMapperForType;
-import org.elasticsearch.index.mapper.MapperException;
-import org.elasticsearch.index.mapper.MapperService;
-import org.elasticsearch.index.mapper.MapperUtils;
-import org.elasticsearch.index.mapper.Mapping;
-import org.elasticsearch.index.mapper.Uid;
+import org.elasticsearch.index.mapper.*;
 import org.elasticsearch.index.query.IndexQueryParserService;
 import org.elasticsearch.index.query.ParsedQuery;
 import org.elasticsearch.index.query.QueryParsingException;
@@ -60,15 +56,18 @@ public class TranslogRecoveryPerformer {
     private final IndexQueryParserService queryParserService;
     private final IndexAliasesService indexAliasesService;
     private final IndexCache indexCache;
+    private final ESLogger logger;
     private final Map<String, Mapping> recoveredTypes = new HashMap<>();
     private final ShardId shardId;
 
-    protected TranslogRecoveryPerformer(ShardId shardId, MapperService mapperService, IndexQueryParserService queryParserService, IndexAliasesService indexAliasesService, IndexCache indexCache) {
+    protected TranslogRecoveryPerformer(ShardId shardId, MapperService mapperService, IndexQueryParserService queryParserService,
+                                        IndexAliasesService indexAliasesService, IndexCache indexCache, ESLogger logger) {
         this.shardId = shardId;
         this.mapperService = mapperService;
         this.queryParserService = queryParserService;
         this.indexAliasesService = indexAliasesService;
         this.indexCache = indexCache;
+        this.logger = logger;
     }
 
     protected DocumentMapperForType docMapper(String type) {
@@ -153,6 +152,9 @@ public class TranslogRecoveryPerformer {
                                     .routing(create.routing()).parent(create.parent()).timestamp(create.timestamp()).ttl(create.ttl()),
                             create.version(), create.versionType().versionTypeForReplicationAndRecovery(), Engine.Operation.Origin.RECOVERY, true, false);
                     maybeAddMappingUpdate(engineCreate.type(), engineCreate.parsedDoc().dynamicMappingsUpdate(), engineCreate.id(), allowMappingUpdates);
+                    if (logger.isTraceEnabled()) {
+                        logger.trace("[translog] recover [create] op of [{}][{}]", create.type(), create.id());
+                    }
                     engine.create(engineCreate);
                     break;
                 case SAVE:
@@ -161,11 +163,17 @@ public class TranslogRecoveryPerformer {
                                     .routing(index.routing()).parent(index.parent()).timestamp(index.timestamp()).ttl(index.ttl()),
                             index.version(), index.versionType().versionTypeForReplicationAndRecovery(), Engine.Operation.Origin.RECOVERY, true);
                     maybeAddMappingUpdate(engineIndex.type(), engineIndex.parsedDoc().dynamicMappingsUpdate(), engineIndex.id(), allowMappingUpdates);
+                    if (logger.isTraceEnabled()) {
+                        logger.trace("[translog] recover [index] op of [{}][{}]", index.type(), index.id());
+                    }
                     engine.index(engineIndex);
                     break;
                 case DELETE:
                     Translog.Delete delete = (Translog.Delete) operation;
                     Uid uid = Uid.createUid(delete.uid().text());
+                    if (logger.isTraceEnabled()) {
+                        logger.trace("[translog] recover [delete] op of [{}][{}]", uid.type(), uid.id());
+                    }
                     engine.delete(new Engine.Delete(uid.type(), uid.id(), delete.uid(), delete.version(),
                             delete.versionType().versionTypeForReplicationAndRecovery(), Engine.Operation.Origin.RECOVERY, System.nanoTime(), false));
                     break;
diff --git a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java
index 3730b67..ed42e2f 100644
--- a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java
+++ b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java
@@ -36,11 +36,11 @@ import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.metadata.SnapshotId;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.blobstore.BlobContainer;
 import org.elasticsearch.common.blobstore.BlobMetaData;
 import org.elasticsearch.common.blobstore.BlobPath;
 import org.elasticsearch.common.blobstore.BlobStore;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
@@ -71,7 +71,6 @@ import org.elasticsearch.repositories.blobstore.LegacyBlobStoreFormat;
 import java.io.FilterInputStream;
 import java.io.IOException;
 import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.ArrayList;
 import java.util.Collections;
 import java.util.HashMap;
@@ -230,8 +229,8 @@ public class BlobStoreIndexShardRepository extends AbstractComponent implements
         BlobContainer testBlobContainer = blobStore.blobContainer(basePath.add(testBlobPrefix(seed)));
         DiscoveryNode localNode = clusterService.localNode();
         if (testBlobContainer.blobExists("master.dat")) {
-            try (OutputStream outputStream = testBlobContainer.createOutput("data-" + localNode.getId() + ".dat")) {
-                outputStream.write(Strings.toUTF8Bytes(seed));
+            try  {
+                testBlobContainer.writeBlob("data-" + localNode.getId() + ".dat", new BytesArray(seed));
             } catch (IOException exp) {
                 throw new RepositoryVerificationException(repositoryName, "store location [" + blobStore + "] is not accessible on the node [" + localNode + "]", exp);
             }
@@ -647,12 +646,7 @@ public class BlobStoreIndexShardRepository extends AbstractComponent implements
                     final InputStreamIndexInput inputStreamIndexInput = new InputStreamIndexInput(indexInput, fileInfo.partBytes());
                     InputStream inputStream = snapshotRateLimiter == null ? inputStreamIndexInput : new RateLimitingInputStream(inputStreamIndexInput, snapshotRateLimiter, snapshotThrottleListener);
                     inputStream = new AbortableInputStream(inputStream, fileInfo.physicalName());
-                    try (OutputStream output = blobContainer.createOutput(fileInfo.partName(i))) {
-                        int len;
-                        while ((len = inputStream.read(buffer)) > 0) {
-                            output.write(buffer, 0, len);
-                        }
-                    }
+                    blobContainer.writeBlob(fileInfo.partName(i), inputStream, fileInfo.partBytes());
                 }
                 Store.verify(indexInput);
                 snapshotStatus.addProcessedFile(fileInfo.length());
@@ -768,8 +762,7 @@ public class BlobStoreIndexShardRepository extends AbstractComponent implements
 
         @Override
         protected InputStream openSlice(long slice) throws IOException {
-            return container.openInput(info.partName(slice));
-
+            return container.readBlob(info.partName(slice));
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/indices/flush/SyncedFlushService.java b/core/src/main/java/org/elasticsearch/indices/flush/SyncedFlushService.java
index 5cd034a..b27945c 100644
--- a/core/src/main/java/org/elasticsearch/indices/flush/SyncedFlushService.java
+++ b/core/src/main/java/org/elasticsearch/indices/flush/SyncedFlushService.java
@@ -435,10 +435,10 @@ public class SyncedFlushService extends AbstractComponent {
         return new InFlightOpsResponse(opCount);
     }
 
-    final static class PreSyncedFlushRequest extends TransportRequest {
+    public final static class PreSyncedFlushRequest extends TransportRequest {
         private ShardId shardId;
 
-        PreSyncedFlushRequest() {
+        public PreSyncedFlushRequest() {
         }
 
         public PreSyncedFlushRequest(ShardId shardId) {
@@ -500,7 +500,7 @@ public class SyncedFlushService extends AbstractComponent {
         }
     }
 
-    static final class SyncedFlushRequest extends TransportRequest {
+    public static final class SyncedFlushRequest extends TransportRequest {
 
         private String syncId;
         private Engine.CommitId expectedCommitId;
@@ -600,7 +600,7 @@ public class SyncedFlushService extends AbstractComponent {
     }
 
 
-    static final class InFlightOpsRequest extends TransportRequest {
+    public static final class InFlightOpsRequest extends TransportRequest {
 
         private ShardId shardId;
 
diff --git a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryCleanFilesRequest.java b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryCleanFilesRequest.java
index b0d224c..c040c25 100644
--- a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryCleanFilesRequest.java
+++ b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryCleanFilesRequest.java
@@ -30,7 +30,7 @@ import java.io.IOException;
 /**
  *
  */
-class RecoveryCleanFilesRequest extends TransportRequest {
+public class RecoveryCleanFilesRequest extends TransportRequest {
 
     private long recoveryId;
     private ShardId shardId;
@@ -38,7 +38,7 @@ class RecoveryCleanFilesRequest extends TransportRequest {
     private Store.MetadataSnapshot snapshotFiles;
     private int totalTranslogOps = RecoveryState.Translog.UNKNOWN;
 
-    RecoveryCleanFilesRequest() {
+    public RecoveryCleanFilesRequest() {
     }
 
     RecoveryCleanFilesRequest(long recoveryId, ShardId shardId, Store.MetadataSnapshot snapshotFiles, int totalTranslogOps) {
diff --git a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryFileChunkRequest.java b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryFileChunkRequest.java
index e0c0701..8fd08d9 100644
--- a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryFileChunkRequest.java
+++ b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryFileChunkRequest.java
@@ -34,7 +34,7 @@ import java.io.IOException;
 /**
  *
  */
-public final class RecoveryFileChunkRequest extends TransportRequest {  // public for testing
+public final class RecoveryFileChunkRequest extends TransportRequest {
     private boolean lastChunk;
     private long recoveryId;
     private ShardId shardId;
@@ -45,7 +45,7 @@ public final class RecoveryFileChunkRequest extends TransportRequest {  // publi
 
     private int totalTranslogOps;
 
-    RecoveryFileChunkRequest() {
+    public RecoveryFileChunkRequest() {
     }
 
     public RecoveryFileChunkRequest(long recoveryId, ShardId shardId, StoreFileMetaData metaData, long position, BytesReference content,
diff --git a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryFilesInfoRequest.java b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryFilesInfoRequest.java
index d28ae27..d9a2a19 100644
--- a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryFilesInfoRequest.java
+++ b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryFilesInfoRequest.java
@@ -31,7 +31,7 @@ import java.util.List;
 /**
  *
  */
-class RecoveryFilesInfoRequest extends TransportRequest {
+public class RecoveryFilesInfoRequest extends TransportRequest {
 
     private long recoveryId;
     private ShardId shardId;
@@ -43,7 +43,7 @@ class RecoveryFilesInfoRequest extends TransportRequest {
 
     int totalTranslogOps;
 
-    RecoveryFilesInfoRequest() {
+    public RecoveryFilesInfoRequest() {
     }
 
     RecoveryFilesInfoRequest(long recoveryId, ShardId shardId, List<String> phase1FileNames, List<Long> phase1FileSizes,
diff --git a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryFinalizeRecoveryRequest.java b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryFinalizeRecoveryRequest.java
index e3f6ac1..e8d5c0f 100644
--- a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryFinalizeRecoveryRequest.java
+++ b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryFinalizeRecoveryRequest.java
@@ -29,13 +29,13 @@ import java.io.IOException;
 /**
  *
  */
-class RecoveryFinalizeRecoveryRequest extends TransportRequest {
+public class RecoveryFinalizeRecoveryRequest extends TransportRequest {
 
     private long recoveryId;
 
     private ShardId shardId;
 
-    RecoveryFinalizeRecoveryRequest() {
+    public RecoveryFinalizeRecoveryRequest() {
     }
 
     RecoveryFinalizeRecoveryRequest(long recoveryId, ShardId shardId) {
diff --git a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryPrepareForTranslogOperationsRequest.java b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryPrepareForTranslogOperationsRequest.java
index dbc4a15..3d5d705 100644
--- a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryPrepareForTranslogOperationsRequest.java
+++ b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryPrepareForTranslogOperationsRequest.java
@@ -29,13 +29,13 @@ import java.io.IOException;
 /**
  *
  */
-class RecoveryPrepareForTranslogOperationsRequest extends TransportRequest {
+public class RecoveryPrepareForTranslogOperationsRequest extends TransportRequest {
 
     private long recoveryId;
     private ShardId shardId;
     private int totalTranslogOps = RecoveryState.Translog.UNKNOWN;
 
-    RecoveryPrepareForTranslogOperationsRequest() {
+    public RecoveryPrepareForTranslogOperationsRequest() {
     }
 
     RecoveryPrepareForTranslogOperationsRequest(long recoveryId, ShardId shardId, int totalTranslogOps) {
diff --git a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java
index 4fb2872..2b6be28 100644
--- a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java
+++ b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java
@@ -304,6 +304,7 @@ public class RecoveryTarget extends AbstractComponent {
                 assert recoveryStatus.indexShard().recoveryState() == recoveryStatus.state();
                 try {
                     recoveryStatus.indexShard().performBatchRecovery(request.operations());
+                    channel.sendResponse(TransportResponse.Empty.INSTANCE);
                 } catch (TranslogRecoveryPerformer.BatchOperationException exception) {
                     MapperException mapperException = (MapperException) ExceptionsHelper.unwrap(exception, MapperException.class);
                     if (mapperException == null) {
@@ -346,8 +347,6 @@ public class RecoveryTarget extends AbstractComponent {
                     });
                 }
             }
-            channel.sendResponse(TransportResponse.Empty.INSTANCE);
-
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTranslogOperationsRequest.java b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTranslogOperationsRequest.java
index 345c179..5cc294a 100644
--- a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTranslogOperationsRequest.java
+++ b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTranslogOperationsRequest.java
@@ -31,14 +31,14 @@ import java.util.List;
 /**
  *
  */
-class RecoveryTranslogOperationsRequest extends TransportRequest {
+public class RecoveryTranslogOperationsRequest extends TransportRequest {
 
     private long recoveryId;
     private ShardId shardId;
     private List<Translog.Operation> operations;
     private int totalTranslogOps = RecoveryState.Translog.UNKNOWN;
 
-    RecoveryTranslogOperationsRequest() {
+    public RecoveryTranslogOperationsRequest() {
     }
 
     RecoveryTranslogOperationsRequest(long recoveryId, ShardId shardId, List<Translog.Operation> operations, int totalTranslogOps) {
diff --git a/core/src/main/java/org/elasticsearch/indices/recovery/StartRecoveryRequest.java b/core/src/main/java/org/elasticsearch/indices/recovery/StartRecoveryRequest.java
index 9cbe305..31280dc 100644
--- a/core/src/main/java/org/elasticsearch/indices/recovery/StartRecoveryRequest.java
+++ b/core/src/main/java/org/elasticsearch/indices/recovery/StartRecoveryRequest.java
@@ -47,7 +47,7 @@ public class StartRecoveryRequest extends TransportRequest {
 
     private RecoveryState.Type recoveryType;
 
-    StartRecoveryRequest() {
+    public StartRecoveryRequest() {
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/indices/store/IndicesStore.java b/core/src/main/java/org/elasticsearch/indices/store/IndicesStore.java
index f87e2c4..7ecd66f 100644
--- a/core/src/main/java/org/elasticsearch/indices/store/IndicesStore.java
+++ b/core/src/main/java/org/elasticsearch/indices/store/IndicesStore.java
@@ -396,13 +396,13 @@ public class IndicesStore extends AbstractComponent implements ClusterStateListe
         }
     }
 
-    private static class ShardActiveRequest extends TransportRequest {
+    public static class ShardActiveRequest extends TransportRequest {
         protected TimeValue timeout = null;
         private ClusterName clusterName;
         private String indexUUID;
         private ShardId shardId;
 
-        ShardActiveRequest() {
+        public ShardActiveRequest() {
         }
 
         ShardActiveRequest(ClusterName clusterName, String indexUUID, ShardId shardId, TimeValue timeout) {
diff --git a/core/src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java b/core/src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java
index d72e145..9b96fde 100644
--- a/core/src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java
+++ b/core/src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java
@@ -258,7 +258,7 @@ public class TransportNodesListShardStoreMetaData extends TransportNodesAction<T
     }
 
 
-    static class Request extends BaseNodesRequest<Request> {
+    public static class Request extends BaseNodesRequest<Request> {
 
         private ShardId shardId;
 
@@ -331,13 +331,13 @@ public class TransportNodesListShardStoreMetaData extends TransportNodesAction<T
     }
 
 
-    static class NodeRequest extends BaseNodeRequest {
+    public static class NodeRequest extends BaseNodeRequest {
 
         private ShardId shardId;
 
         private boolean unallocated;
 
-        NodeRequest() {
+        public NodeRequest() {
         }
 
         NodeRequest(String nodeId, TransportNodesListShardStoreMetaData.Request request) {
diff --git a/core/src/main/java/org/elasticsearch/monitor/jvm/JvmInfo.java b/core/src/main/java/org/elasticsearch/monitor/jvm/JvmInfo.java
index a2b12ea..599c860 100644
--- a/core/src/main/java/org/elasticsearch/monitor/jvm/JvmInfo.java
+++ b/core/src/main/java/org/elasticsearch/monitor/jvm/JvmInfo.java
@@ -73,7 +73,16 @@ public class JvmInfo implements Streamable, ToXContent {
             // ignore
         }
         info.inputArguments = runtimeMXBean.getInputArguments().toArray(new String[runtimeMXBean.getInputArguments().size()]);
-        info.bootClassPath = runtimeMXBean.getBootClassPath();
+        try {
+            info.bootClassPath = runtimeMXBean.getBootClassPath();
+        } catch (UnsupportedOperationException e) {
+            // oracle java 9
+            info.bootClassPath = System.getProperty("sun.boot.class.path");
+            if (info.bootClassPath == null) {
+                // something else
+                info.bootClassPath = "<unknown>";
+            }
+        }
         info.classPath = runtimeMXBean.getClassPath();
         info.systemProperties = runtimeMXBean.getSystemProperties();
 
diff --git a/core/src/main/java/org/elasticsearch/monitor/os/OsProbe.java b/core/src/main/java/org/elasticsearch/monitor/os/OsProbe.java
index 8dc2179..21d4b6b 100644
--- a/core/src/main/java/org/elasticsearch/monitor/os/OsProbe.java
+++ b/core/src/main/java/org/elasticsearch/monitor/os/OsProbe.java
@@ -157,12 +157,10 @@ public class OsProbe {
      */
     private static Method getMethod(String methodName) {
         try {
-            Method method = osMxBean.getClass().getMethod(methodName);
-            method.setAccessible(true);
-            return method;
+            return Class.forName("com.sun.management.OperatingSystemMXBean").getMethod(methodName);
         } catch (Throwable t) {
             // not available
+            return null;
         }
-        return null;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/monitor/process/ProcessProbe.java b/core/src/main/java/org/elasticsearch/monitor/process/ProcessProbe.java
index 822f4f5..eca3327 100644
--- a/core/src/main/java/org/elasticsearch/monitor/process/ProcessProbe.java
+++ b/core/src/main/java/org/elasticsearch/monitor/process/ProcessProbe.java
@@ -38,8 +38,8 @@ public class ProcessProbe {
     private static final Method getCommittedVirtualMemorySize;
 
     static {
-        getMaxFileDescriptorCountField = getMethod("getMaxFileDescriptorCount");
-        getOpenFileDescriptorCountField = getMethod("getOpenFileDescriptorCount");
+        getMaxFileDescriptorCountField = getUnixMethod("getMaxFileDescriptorCount");
+        getOpenFileDescriptorCountField = getUnixMethod("getOpenFileDescriptorCount");
         getProcessCpuLoad = getMethod("getProcessCpuLoad");
         getProcessCpuTime = getMethod("getProcessCpuTime");
         getCommittedVirtualMemorySize = getMethod("getCommittedVirtualMemorySize");
@@ -163,12 +163,23 @@ public class ProcessProbe {
      */
     private static Method getMethod(String methodName) {
         try {
-            Method method = osMxBean.getClass().getDeclaredMethod(methodName);
-            method.setAccessible(true);
-            return method;
+            return Class.forName("com.sun.management.OperatingSystemMXBean").getMethod(methodName);
         } catch (Throwable t) {
             // not available
+            return null;
+        }
+    }
+    
+    /**
+     * Returns a given method of the UnixOperatingSystemMXBean,
+     * or null if the method is not found or unavailable.
+     */
+    private static Method getUnixMethod(String methodName) {
+        try {
+            return Class.forName("com.sun.management.UnixOperatingSystemMXBean").getMethod(methodName);
+        } catch (Throwable t) {
+            // not available
+            return null;
         }
-        return null;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/node/Node.java b/core/src/main/java/org/elasticsearch/node/Node.java
index e0532ed..a6fa59e 100644
--- a/core/src/main/java/org/elasticsearch/node/Node.java
+++ b/core/src/main/java/org/elasticsearch/node/Node.java
@@ -238,10 +238,8 @@ public class Node implements Releasable {
 
         ESLogger logger = Loggers.getLogger(Node.class, settings.get("name"));
         logger.info("starting ...");
-
         // hack around dependency injection problem (for now...)
         injector.getInstance(Discovery.class).setRoutingService(injector.getInstance(RoutingService.class));
-
         for (Class<? extends LifecycleComponent> plugin : pluginsService.nodeServices()) {
             injector.getInstance(plugin).start();
         }
diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginManager.java b/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
index ee9e6b3..3eded07 100644
--- a/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
+++ b/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
@@ -41,7 +41,6 @@ import java.io.IOException;
 import java.io.OutputStream;
 import java.net.MalformedURLException;
 import java.net.URL;
-import java.net.URLClassLoader;
 import java.nio.file.*;
 import java.nio.file.attribute.BasicFileAttributes;
 import java.nio.file.attribute.PosixFileAttributeView;
@@ -315,10 +314,7 @@ public class PluginManager {
     private void jarHellCheck(Path candidate, boolean isolated) throws IOException {
         // create list of current jars in classpath
         final List<URL> jars = new ArrayList<>();
-        ClassLoader loader = PluginManager.class.getClassLoader();
-        if (loader instanceof URLClassLoader) {
-            Collections.addAll(jars, ((URLClassLoader) loader).getURLs());
-        }
+        jars.addAll(Arrays.asList(JarHell.parseClassPath()));
 
         // read existing bundles. this does some checks on the installation too.
         List<Bundle> bundles = PluginsService.getPluginBundles(environment.pluginsFile());
diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginManagerCliParser.java b/core/src/main/java/org/elasticsearch/plugins/PluginManagerCliParser.java
index c24b823..db6afd1 100644
--- a/core/src/main/java/org/elasticsearch/plugins/PluginManagerCliParser.java
+++ b/core/src/main/java/org/elasticsearch/plugins/PluginManagerCliParser.java
@@ -34,6 +34,7 @@ import org.elasticsearch.plugins.PluginManager.OutputMode;
 
 import java.net.MalformedURLException;
 import java.net.URL;
+import java.net.URLDecoder;
 import java.util.Locale;
 
 import static org.elasticsearch.common.cli.CliToolConfig.Builder.cmd;
@@ -221,7 +222,7 @@ public class PluginManagerCliParser extends CliTool {
             if (name != null) {
                 terminal.println("-> Installing " + Strings.coalesceToEmpty(name) + "...");
             } else {
-                terminal.println("-> Installing from " + url + "...");
+                terminal.println("-> Installing from " + URLDecoder.decode(url.toString(), "UTF-8") + "...");
             }
             pluginManager.downloadAndExtract(name, terminal);
             return ExitStatus.OK;
diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginsService.java b/core/src/main/java/org/elasticsearch/plugins/PluginsService.java
index c70349f..5834efc 100644
--- a/core/src/main/java/org/elasticsearch/plugins/PluginsService.java
+++ b/core/src/main/java/org/elasticsearch/plugins/PluginsService.java
@@ -47,6 +47,7 @@ import java.nio.file.DirectoryStream;
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.Collection;
 import java.util.Collections;
 import java.util.HashMap;
@@ -331,12 +332,7 @@ public class PluginsService extends AbstractComponent {
             // pluginmanager does it, but we do it again, in case lusers mess with jar files manually
             try {
                 final List<URL> jars = new ArrayList<>();
-                ClassLoader parentLoader = getClass().getClassLoader();
-                if (parentLoader instanceof URLClassLoader) {
-                    for (URL url : ((URLClassLoader) parentLoader).getURLs()) {
-                        jars.add(url);
-                    }
-                }
+                jars.addAll(Arrays.asList(JarHell.parseClassPath()));
                 jars.addAll(bundle.urls);
                 JarHell.checkJarHell(jars.toArray(new URL[0]));
             } catch (Exception e) {
diff --git a/core/src/main/java/org/elasticsearch/repositories/VerifyNodeRepositoryAction.java b/core/src/main/java/org/elasticsearch/repositories/VerifyNodeRepositoryAction.java
index 11b898d..24e6511 100644
--- a/core/src/main/java/org/elasticsearch/repositories/VerifyNodeRepositoryAction.java
+++ b/core/src/main/java/org/elasticsearch/repositories/VerifyNodeRepositoryAction.java
@@ -121,12 +121,12 @@ public class VerifyNodeRepositoryAction  extends AbstractComponent {
         blobStoreIndexShardRepository.verify(verificationToken);
     }
 
-    static class VerifyNodeRepositoryRequest extends TransportRequest {
+    public static class VerifyNodeRepositoryRequest extends TransportRequest {
 
         private String repository;
         private String verificationToken;
 
-        VerifyNodeRepositoryRequest() {
+        public VerifyNodeRepositoryRequest() {
         }
 
         VerifyNodeRepositoryRequest(String repository, String verificationToken) {
diff --git a/core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java b/core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java
index 109202b..4b2c608 100644
--- a/core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java
+++ b/core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java
@@ -577,9 +577,7 @@ public abstract class BlobStoreRepository extends AbstractLifecycleComponent<Rep
         if (snapshotsBlobContainer.blobExists(SNAPSHOTS_FILE)) {
             snapshotsBlobContainer.deleteBlob(SNAPSHOTS_FILE);
         }
-        try (OutputStream output = snapshotsBlobContainer.createOutput(SNAPSHOTS_FILE)) {
-            bRef.writeTo(output);
-        }
+        snapshotsBlobContainer.writeBlob(SNAPSHOTS_FILE, bRef);
     }
 
     /**
@@ -591,7 +589,7 @@ public abstract class BlobStoreRepository extends AbstractLifecycleComponent<Rep
      * @throws IOException I/O errors
      */
     protected List<SnapshotId> readSnapshotList() throws IOException {
-        try (InputStream blob = snapshotsBlobContainer.openInput(SNAPSHOTS_FILE)) {
+        try (InputStream blob = snapshotsBlobContainer.readBlob(SNAPSHOTS_FILE)) {
             final byte[] data = ByteStreams.toByteArray(blob);
             ArrayList<SnapshotId> snapshots = new ArrayList<>();
             try (XContentParser parser = XContentHelper.createParser(new BytesArray(data))) {
@@ -643,9 +641,7 @@ public abstract class BlobStoreRepository extends AbstractLifecycleComponent<Rep
                 byte[] testBytes = Strings.toUTF8Bytes(seed);
                 BlobContainer testContainer = blobStore().blobContainer(basePath().add(testBlobPrefix(seed)));
                 String blobName = "master.dat";
-                try (OutputStream outputStream = testContainer.createOutput(blobName + "-temp")) {
-                    outputStream.write(testBytes);
-                }
+                testContainer.writeBlob(blobName + "-temp", new BytesArray(testBytes));
                 // Make sure that move is supported
                 testContainer.move(blobName + "-temp", blobName);
                 return seed;
diff --git a/core/src/main/java/org/elasticsearch/repositories/blobstore/ChecksumBlobStoreFormat.java b/core/src/main/java/org/elasticsearch/repositories/blobstore/ChecksumBlobStoreFormat.java
index cc1323e..718e9da 100644
--- a/core/src/main/java/org/elasticsearch/repositories/blobstore/ChecksumBlobStoreFormat.java
+++ b/core/src/main/java/org/elasticsearch/repositories/blobstore/ChecksumBlobStoreFormat.java
@@ -36,9 +36,7 @@ import org.elasticsearch.common.lucene.store.IndexOutputOutputStream;
 import org.elasticsearch.common.xcontent.*;
 import org.elasticsearch.gateway.CorruptStateException;
 
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
+import java.io.*;
 import java.util.Locale;
 
 /**
@@ -94,7 +92,7 @@ public class ChecksumBlobStoreFormat<T extends ToXContent> extends BlobStoreForm
      * @throws IOException
      */
     public T readBlob(BlobContainer blobContainer, String blobName) throws IOException {
-        try (InputStream inputStream = blobContainer.openInput(blobName)) {
+        try (InputStream inputStream = blobContainer.readBlob(blobName)) {
             byte[] bytes = ByteStreams.toByteArray(inputStream);
             final String resourceDesc = "ChecksumBlobStoreFormat.readBlob(blob=\"" + blobName + "\")";
             try (ByteArrayIndexInput indexInput = new ByteArrayIndexInput(resourceDesc, bytes)) {
@@ -163,9 +161,9 @@ public class ChecksumBlobStoreFormat<T extends ToXContent> extends BlobStoreForm
      */
     protected void writeBlob(T obj, BlobContainer blobContainer, String blobName) throws IOException {
         BytesReference bytes = write(obj);
-        try (OutputStream outputStream = blobContainer.createOutput(blobName)) {
+        try (ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream()) {
             final String resourceDesc = "ChecksumBlobStoreFormat.writeBlob(blob=\"" + blobName + "\")";
-            try (OutputStreamIndexOutput indexOutput = new OutputStreamIndexOutput(resourceDesc, outputStream, BUFFER_SIZE)) {
+            try (OutputStreamIndexOutput indexOutput = new OutputStreamIndexOutput(resourceDesc, byteArrayOutputStream, BUFFER_SIZE)) {
                 CodecUtil.writeHeader(indexOutput, codec, VERSION);
                 try (OutputStream indexOutputOutputStream = new IndexOutputOutputStream(indexOutput) {
                     @Override
@@ -177,6 +175,7 @@ public class ChecksumBlobStoreFormat<T extends ToXContent> extends BlobStoreForm
                 }
                 CodecUtil.writeFooter(indexOutput);
             }
+            blobContainer.writeBlob(blobName, new BytesArray(byteArrayOutputStream.toByteArray()));
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/repositories/blobstore/LegacyBlobStoreFormat.java b/core/src/main/java/org/elasticsearch/repositories/blobstore/LegacyBlobStoreFormat.java
index 5fcc8ec..f9d5c98 100644
--- a/core/src/main/java/org/elasticsearch/repositories/blobstore/LegacyBlobStoreFormat.java
+++ b/core/src/main/java/org/elasticsearch/repositories/blobstore/LegacyBlobStoreFormat.java
@@ -52,7 +52,7 @@ public class LegacyBlobStoreFormat<T extends ToXContent> extends BlobStoreFormat
      * @throws IOException
      */
     public T readBlob(BlobContainer blobContainer, String blobName) throws IOException {
-        try (InputStream inputStream = blobContainer.openInput(blobName)) {
+        try (InputStream inputStream = blobContainer.readBlob(blobName)) {
             return read(new BytesArray(ByteStreams.toByteArray(inputStream)));
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/repositories/uri/URLIndexShardRepository.java b/core/src/main/java/org/elasticsearch/repositories/uri/URLIndexShardRepository.java
index 6479b77..ab9ec72 100644
--- a/core/src/main/java/org/elasticsearch/repositories/uri/URLIndexShardRepository.java
+++ b/core/src/main/java/org/elasticsearch/repositories/uri/URLIndexShardRepository.java
@@ -31,7 +31,7 @@ import org.elasticsearch.repositories.RepositoryName;
 public class URLIndexShardRepository extends BlobStoreIndexShardRepository {
 
     @Inject
-    URLIndexShardRepository(Settings settings, RepositoryName repositoryName, IndicesService indicesService, ClusterService clusterService) {
+    public URLIndexShardRepository(Settings settings, RepositoryName repositoryName, IndicesService indicesService, ClusterService clusterService) {
         super(settings, repositoryName, indicesService, clusterService);
     }
 
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/put/RestPutWarmerAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/put/RestPutWarmerAction.java
index d3bd3e2..4c421cc 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/put/RestPutWarmerAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/put/RestPutWarmerAction.java
@@ -28,6 +28,7 @@ import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.AcknowledgedRestListener;
+import org.elasticsearch.search.builder.SearchSourceBuilder;
 
 import static org.elasticsearch.rest.RestRequest.Method.POST;
 import static org.elasticsearch.rest.RestRequest.Method.PUT;
@@ -67,6 +68,6 @@ public class RestPutWarmerAction extends BaseRestHandler {
         putWarmerRequest.searchRequest(searchRequest);
         putWarmerRequest.timeout(request.paramAsTime("timeout", putWarmerRequest.timeout()));
         putWarmerRequest.masterNodeTimeout(request.paramAsTime("master_timeout", putWarmerRequest.masterNodeTimeout()));
-        client.admin().indices().putWarmer(putWarmerRequest, new AcknowledgedRestListener<PutWarmerResponse>(channel));
+        client.admin().indices().putWarmer(putWarmerRequest, new AcknowledgedRestListener<>(channel));
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/cat/RestCountAction.java b/core/src/main/java/org/elasticsearch/rest/action/cat/RestCountAction.java
index 554465b..72057a9 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/cat/RestCountAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/cat/RestCountAction.java
@@ -41,7 +41,7 @@ import static org.elasticsearch.rest.RestRequest.Method.GET;
 public class RestCountAction extends AbstractCatAction {
 
     @Inject
-    protected RestCountAction(Settings settings, RestController restController, RestController controller, Client client) {
+    public RestCountAction(Settings settings, RestController restController, RestController controller, Client client) {
         super(settings, controller, client);
         restController.registerHandler(GET, "/_cat/count", this);
         restController.registerHandler(GET, "/_cat/count/{index}", this);
diff --git a/core/src/main/java/org/elasticsearch/rest/action/cat/RestRecoveryAction.java b/core/src/main/java/org/elasticsearch/rest/action/cat/RestRecoveryAction.java
index 4c9b3de..47265d9 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/cat/RestRecoveryAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/cat/RestRecoveryAction.java
@@ -50,7 +50,7 @@ import static org.elasticsearch.rest.RestRequest.Method.GET;
 public class RestRecoveryAction extends AbstractCatAction {
 
     @Inject
-    protected RestRecoveryAction(Settings settings, RestController restController, RestController controller, Client client) {
+    public RestRecoveryAction(Settings settings, RestController restController, RestController controller, Client client) {
         super(settings, controller, client);
         restController.registerHandler(GET, "/_cat/recovery", this);
         restController.registerHandler(GET, "/_cat/recovery/{index}", this);
diff --git a/core/src/main/java/org/elasticsearch/script/ScriptContextRegistry.java b/core/src/main/java/org/elasticsearch/script/ScriptContextRegistry.java
index 226c931..e9681c7 100644
--- a/core/src/main/java/org/elasticsearch/script/ScriptContextRegistry.java
+++ b/core/src/main/java/org/elasticsearch/script/ScriptContextRegistry.java
@@ -36,7 +36,7 @@ public final class ScriptContextRegistry {
 
     private final ImmutableMap<String, ScriptContext> scriptContexts;
 
-    ScriptContextRegistry(Iterable<ScriptContext.Plugin> customScriptContexts) {
+    public ScriptContextRegistry(Iterable<ScriptContext.Plugin> customScriptContexts) {
         Map<String, ScriptContext> scriptContexts = new HashMap<>();
         for (ScriptContext.Standard scriptContext : ScriptContext.Standard.values()) {
             scriptContexts.put(scriptContext.getKey(), scriptContext);
diff --git a/core/src/main/java/org/elasticsearch/search/action/SearchServiceTransportAction.java b/core/src/main/java/org/elasticsearch/search/action/SearchServiceTransportAction.java
index 4ab0543..70806c6 100644
--- a/core/src/main/java/org/elasticsearch/search/action/SearchServiceTransportAction.java
+++ b/core/src/main/java/org/elasticsearch/search/action/SearchServiceTransportAction.java
@@ -205,10 +205,10 @@ public class SearchServiceTransportAction extends AbstractComponent {
         });
     }
 
-    static class ScrollFreeContextRequest extends TransportRequest {
+    public static class ScrollFreeContextRequest extends TransportRequest {
         private long id;
 
-        ScrollFreeContextRequest() {
+        public ScrollFreeContextRequest() {
         }
 
         ScrollFreeContextRequest(ClearScrollRequest request, long id) {
@@ -237,10 +237,10 @@ public class SearchServiceTransportAction extends AbstractComponent {
         }
     }
 
-    static class SearchFreeContextRequest extends ScrollFreeContextRequest implements IndicesRequest {
+    public static class SearchFreeContextRequest extends ScrollFreeContextRequest implements IndicesRequest {
         private OriginalIndices originalIndices;
 
-        SearchFreeContextRequest() {
+        public SearchFreeContextRequest() {
         }
 
         SearchFreeContextRequest(SearchRequest request, long id) {
@@ -313,9 +313,9 @@ public class SearchServiceTransportAction extends AbstractComponent {
         }
     }
 
-    static class ClearScrollContextsRequest extends TransportRequest {
+    public static class ClearScrollContextsRequest extends TransportRequest {
 
-        ClearScrollContextsRequest() {
+        public ClearScrollContextsRequest() {
         }
 
         ClearScrollContextsRequest(TransportRequest request) {
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/BucketsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/BucketsAggregator.java
index e34bc95..e0b27bb 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/BucketsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/BucketsAggregator.java
@@ -40,7 +40,7 @@ import java.util.Map;
  */
 public abstract class BucketsAggregator extends AggregatorBase {
 
-    private final BigArrays bigArrays;
+    protected final BigArrays bigArrays;
     private IntArray docCounts;
 
     public BucketsAggregator(String name, AggregatorFactories factories, AggregationContext context, Aggregator parent,
@@ -67,7 +67,7 @@ public abstract class BucketsAggregator extends AggregatorBase {
     /**
      * Utility method to collect the given doc in the given bucket (identified by the bucket ordinal)
      */
-    public final void collectBucket(LeafBucketCollector subCollector, int doc, long bucketOrd) throws IOException {
+    public void collectBucket(LeafBucketCollector subCollector, int doc, long bucketOrd) throws IOException {
         grow(bucketOrd + 1);
         collectExistingBucket(subCollector, doc, bucketOrd);
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGrid.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGrid.java
index 71ee3b1..2f9856a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGrid.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGrid.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.search.aggregations.bucket.geogrid;
 
+import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation;
 
 import java.util.List;
@@ -32,7 +33,7 @@ public interface GeoHashGrid extends MultiBucketsAggregation {
      * A bucket that is associated with a {@code geohash_grid} cell. The key of the bucket is the {@cod geohash} of the cell
      */
     public static interface Bucket extends MultiBucketsAggregation.Bucket {
-
+        public GeoPoint getCentroid();
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridAggregator.java
index d92c55d..41af33d 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridAggregator.java
@@ -20,7 +20,10 @@ package org.elasticsearch.search.aggregations.bucket.geogrid;
 
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.SortedNumericDocValues;
+import org.apache.lucene.util.XGeoHashUtils;
+import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.lease.Releasables;
+import org.elasticsearch.common.util.LongArray;
 import org.elasticsearch.common.util.LongHash;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
@@ -30,7 +33,6 @@ import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.bucket.BucketsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
 
 import java.io.IOException;
 import java.util.Arrays;
@@ -47,10 +49,11 @@ public class GeoHashGridAggregator extends BucketsAggregator {
 
     private final int requiredSize;
     private final int shardSize;
-    private final ValuesSource.Numeric valuesSource;
+    private final GeoHashGridParser.GeoGridFactory.CellIdSource valuesSource;
     private final LongHash bucketOrds;
+    private LongArray bucketCentroids;
 
-    public GeoHashGridAggregator(String name, AggregatorFactories factories, ValuesSource.Numeric valuesSource,
+    public GeoHashGridAggregator(String name, AggregatorFactories factories, GeoHashGridParser.GeoGridFactory.CellIdSource valuesSource,
             int requiredSize, int shardSize, AggregationContext aggregationContext, Aggregator parent, List<PipelineAggregator> pipelineAggregators,
             Map<String, Object> metaData) throws IOException {
         super(name, factories, aggregationContext, parent, pipelineAggregators, metaData);
@@ -58,6 +61,7 @@ public class GeoHashGridAggregator extends BucketsAggregator {
         this.requiredSize = requiredSize;
         this.shardSize = shardSize;
         bucketOrds = new LongHash(1, aggregationContext.bigArrays());
+        bucketCentroids = aggregationContext.bigArrays().newLongArray(1, true);
     }
 
     @Override
@@ -66,6 +70,28 @@ public class GeoHashGridAggregator extends BucketsAggregator {
     }
 
     @Override
+    public void collectBucket(LeafBucketCollector subCollector, int doc, long bucketOrd) throws IOException {
+        bucketCentroids = bigArrays.grow(bucketCentroids, bucketOrd + 1);
+        super.collectBucket(subCollector, doc, bucketOrd);
+    }
+
+    protected final void adjustCentroid(long bucketOrd, long geohash) {
+        final int numDocs = getDocCounts().get(bucketOrd);
+        final GeoPoint oldCentroid = new GeoPoint();
+        final GeoPoint nextLoc = new GeoPoint();
+
+        if (numDocs > 1) {
+            final long curCentroid = bucketCentroids.get(bucketOrd);
+            oldCentroid.resetFromGeoHash(curCentroid);
+            nextLoc.resetFromGeoHash(geohash);
+            bucketCentroids.set(bucketOrd, XGeoHashUtils.longEncode(oldCentroid.lon() + (nextLoc.lon() - oldCentroid.lon()) / numDocs,
+                    oldCentroid.lat() + (nextLoc.lat() - oldCentroid.lat()) / numDocs, XGeoHashUtils.PRECISION));
+        } else {
+            bucketCentroids.set(bucketOrd, geohash);
+        }
+    }
+
+    @Override
     public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,
             final LeafBucketCollector sub) throws IOException {
         final SortedNumericDocValues values = valuesSource.longValues(ctx);
@@ -78,7 +104,8 @@ public class GeoHashGridAggregator extends BucketsAggregator {
 
                 long previous = Long.MAX_VALUE;
                 for (int i = 0; i < valuesCount; ++i) {
-                    final long val = values.valueAt(i);
+                    final long valFullRes = values.valueAt(i);
+                    final long val = XGeoHashUtils.longEncode(valFullRes, valuesSource.precision());
                     if (previous != val || i == 0) {
                         long bucketOrdinal = bucketOrds.add(val);
                         if (bucketOrdinal < 0) { // already seen
@@ -87,6 +114,7 @@ public class GeoHashGridAggregator extends BucketsAggregator {
                         } else {
                             collectBucket(sub, doc, bucketOrdinal);
                         }
+                        adjustCentroid(bucketOrdinal, valFullRes);
                         previous = val;
                     }
                 }
@@ -100,7 +128,7 @@ public class GeoHashGridAggregator extends BucketsAggregator {
         long bucketOrd;
 
         public OrdinalBucket() {
-            super(0, 0, (InternalAggregations) null);
+            super(0, 0, new GeoPoint(), (InternalAggregations) null);
         }
 
     }
@@ -118,6 +146,7 @@ public class GeoHashGridAggregator extends BucketsAggregator {
             }
 
             spare.geohashAsLong = bucketOrds.get(i);
+            spare.centroid.resetFromGeoHash(bucketCentroids.get(i));
             spare.docCount = bucketDocCount(i);
             spare.bucketOrd = i;
             spare = (OrdinalBucket) ordered.insertWithOverflow(spare);
@@ -141,6 +170,7 @@ public class GeoHashGridAggregator extends BucketsAggregator {
     @Override
     public void doClose() {
         Releasables.close(bucketOrds);
+        Releasables.close(bucketCentroids);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java
index 0fe78c1..0025880 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java
@@ -20,7 +20,7 @@ package org.elasticsearch.search.aggregations.bucket.geogrid;
 
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.SortedNumericDocValues;
-import org.elasticsearch.common.geo.GeoHashUtils;
+import org.apache.lucene.util.XGeoHashUtils;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.fielddata.MultiGeoPointValues;
@@ -29,7 +29,6 @@ import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.index.fielddata.SortingNumericDocValues;
 import org.elasticsearch.index.query.GeoBoundingBoxQueryBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.AggregatorBase;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.NonCollectingAggregator;
@@ -111,7 +110,7 @@ public class GeoHashGridParser implements Aggregator.Parser {
     }
 
 
-    private static class GeoGridFactory extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint> {
+    static class GeoGridFactory extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint> {
 
         private int precision;
         private int requiredSize;
@@ -143,20 +142,17 @@ public class GeoHashGridParser implements Aggregator.Parser {
             if (collectsFromSingleBucket == false) {
                 return asMultiBucketAggregator(this, aggregationContext, parent);
             }
-            ValuesSource.Numeric cellIdSource = new CellIdSource(valuesSource, precision);
+            CellIdSource cellIdSource = new CellIdSource(valuesSource, precision);
             return new GeoHashGridAggregator(name, factories, cellIdSource, requiredSize, shardSize, aggregationContext, parent, pipelineAggregators,
                     metaData);
 
         }
 
         private static class CellValues extends SortingNumericDocValues {
-
             private MultiGeoPointValues geoValues;
-            private int precision;
 
-            protected CellValues(MultiGeoPointValues geoValues, int precision) {
+            protected CellValues(MultiGeoPointValues geoValues) {
                 this.geoValues = geoValues;
-                this.precision = precision;
             }
 
             @Override
@@ -165,14 +161,13 @@ public class GeoHashGridParser implements Aggregator.Parser {
                 resize(geoValues.count());
                 for (int i = 0; i < count(); ++i) {
                     GeoPoint target = geoValues.valueAt(i);
-                    values[i] = GeoHashUtils.encodeAsLong(target.getLat(), target.getLon(), precision);
+                    values[i] = XGeoHashUtils.longEncode(target.getLon(), target.getLat(), XGeoHashUtils.PRECISION);
                 }
                 sort();
             }
-
         }
 
-        private static class CellIdSource extends ValuesSource.Numeric {
+        static class CellIdSource extends ValuesSource.Numeric {
             private final ValuesSource.GeoPoint valuesSource;
             private final int precision;
 
@@ -182,6 +177,10 @@ public class GeoHashGridParser implements Aggregator.Parser {
                 this.precision = precision;
             }
 
+            public int precision() {
+                return precision;
+            }
+
             @Override
             public boolean isFloatingPoint() {
                 return false;
@@ -189,7 +188,7 @@ public class GeoHashGridParser implements Aggregator.Parser {
 
             @Override
             public SortedNumericDocValues longValues(LeafReaderContext ctx) {
-                return new CellValues(valuesSource.geoPointValues(ctx), precision);
+                return new CellValues(valuesSource.geoPointValues(ctx));
             }
 
             @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/InternalGeoHashGrid.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/InternalGeoHashGrid.java
index 8bf9cdd..50c1d73 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/InternalGeoHashGrid.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/InternalGeoHashGrid.java
@@ -19,12 +19,13 @@
 package org.elasticsearch.search.aggregations.bucket.geogrid;
 
 import org.apache.lucene.util.PriorityQueue;
-import org.elasticsearch.common.geo.GeoHashUtils;
+import org.apache.lucene.util.XGeoHashUtils;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.util.LongObjectPagedHashMap;
 import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.search.aggregations.AggregationStreams;
 import org.elasticsearch.search.aggregations.Aggregations;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -86,26 +87,28 @@ public class InternalGeoHashGrid extends InternalMultiBucketAggregation<Internal
 
         protected long geohashAsLong;
         protected long docCount;
+        protected GeoPoint centroid;
         protected InternalAggregations aggregations;
 
         public Bucket() {
             // For Serialization only
         }
 
-        public Bucket(long geohashAsLong, long docCount, InternalAggregations aggregations) {
+        public Bucket(long geohashAsLong, long docCount, GeoPoint centroid, InternalAggregations aggregations) {
             this.docCount = docCount;
             this.aggregations = aggregations;
             this.geohashAsLong = geohashAsLong;
+            this.centroid = centroid;
         }
 
         @Override
         public String getKeyAsString() {
-            return GeoHashUtils.toString(geohashAsLong);
+            return XGeoHashUtils.stringEncode(geohashAsLong);
         }
 
         @Override
         public GeoPoint getKey() {
-            return GeoHashUtils.decode(geohashAsLong);
+            return GeoPoint.fromGeohash(geohashAsLong);
         }
 
         @Override
@@ -114,6 +117,11 @@ public class InternalGeoHashGrid extends InternalMultiBucketAggregation<Internal
         }
 
         @Override
+        public GeoPoint getCentroid() {
+            return centroid;
+        }
+
+        @Override
         public Aggregations getAggregations() {
             return aggregations;
         }
@@ -132,18 +140,23 @@ public class InternalGeoHashGrid extends InternalMultiBucketAggregation<Internal
         public Bucket reduce(List<? extends Bucket> buckets, ReduceContext context) {
             List<InternalAggregations> aggregationsList = new ArrayList<>(buckets.size());
             long docCount = 0;
+            double cLon = 0;
+            double cLat = 0;
             for (Bucket bucket : buckets) {
                 docCount += bucket.docCount;
+                cLon += (bucket.docCount * bucket.centroid.lon());
+                cLat += (bucket.docCount * bucket.centroid.lat());
                 aggregationsList.add(bucket.aggregations);
             }
             final InternalAggregations aggs = InternalAggregations.reduce(aggregationsList, context);
-            return new Bucket(geohashAsLong, docCount, aggs);
+            return new Bucket(geohashAsLong, docCount, new GeoPoint(cLat/docCount, cLon/docCount), aggs);
         }
 
         @Override
         public void readFrom(StreamInput in) throws IOException {
             geohashAsLong = in.readLong();
             docCount = in.readVLong();
+            centroid = GeoPoint.fromGeohash(in.readLong());
             aggregations = InternalAggregations.readAggregations(in);
         }
 
@@ -151,6 +164,7 @@ public class InternalGeoHashGrid extends InternalMultiBucketAggregation<Internal
         public void writeTo(StreamOutput out) throws IOException {
             out.writeLong(geohashAsLong);
             out.writeVLong(docCount);
+            out.writeLong(XGeoHashUtils.longEncode(centroid.lon(), centroid.lat(), XGeoHashUtils.PRECISION));
             aggregations.writeTo(out);
         }
 
@@ -159,6 +173,7 @@ public class InternalGeoHashGrid extends InternalMultiBucketAggregation<Internal
             builder.startObject();
             builder.field(CommonFields.KEY, getKeyAsString());
             builder.field(CommonFields.DOC_COUNT, docCount);
+            builder.array(GeoFields.CENTROID, centroid.getLon(), centroid.getLat());
             aggregations.toXContentInternal(builder, params);
             builder.endObject();
             return builder;
@@ -190,7 +205,7 @@ public class InternalGeoHashGrid extends InternalMultiBucketAggregation<Internal
 
     @Override
     public Bucket createBucket(InternalAggregations aggregations, Bucket prototype) {
-        return new Bucket(prototype.geohashAsLong, prototype.docCount, aggregations);
+        return new Bucket(prototype.geohashAsLong, prototype.docCount, prototype.centroid, aggregations);
     }
 
     @Override
@@ -284,4 +299,7 @@ public class InternalGeoHashGrid extends InternalMultiBucketAggregation<Internal
         }
     }
 
+    public static final class GeoFields {
+        public static final XContentBuilderString CENTROID = new XContentBuilderString("centroid");
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/format/ValueFormatter.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/format/ValueFormatter.java
index 8316e91..4e0060d 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/format/ValueFormatter.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/format/ValueFormatter.java
@@ -18,7 +18,7 @@
  */
 package org.elasticsearch.search.aggregations.support.format;
 
-import org.elasticsearch.common.geo.GeoHashUtils;
+import org.apache.lucene.util.XGeoHashUtils;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
@@ -250,7 +250,7 @@ public interface ValueFormatter extends Streamable {
 
         @Override
         public String format(long value) {
-            return GeoHashUtils.toString(value);
+            return XGeoHashUtils.stringEncode(value);
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/search/highlight/CustomQueryScorer.java b/core/src/main/java/org/elasticsearch/search/highlight/CustomQueryScorer.java
index a2d7624..6914d85 100644
--- a/core/src/main/java/org/elasticsearch/search/highlight/CustomQueryScorer.java
+++ b/core/src/main/java/org/elasticsearch/search/highlight/CustomQueryScorer.java
@@ -79,15 +79,15 @@ public final class CustomQueryScorer extends QueryScorer {
                                            Map<String, WeightedSpanTerm> terms) throws IOException {
             if (query instanceof FunctionScoreQuery) {
                 query = ((FunctionScoreQuery) query).getSubQuery();
-                extract(query, terms);
+                extract(query, query.getBoost(), terms);
             } else if (query instanceof FiltersFunctionScoreQuery) {
                 query = ((FiltersFunctionScoreQuery) query).getSubQuery();
-                extract(query, terms);
+                extract(query, query.getBoost(), terms);
             } else if (query instanceof FilteredQuery) {
                 query = ((FilteredQuery) query).getQuery();
-                extract(query, terms);
+                extract(query, 1F, terms);
             } else {
-                extractWeightedTerms(terms, query);
+                extractWeightedTerms(terms, query, query.getBoost());
             }
         }
 
diff --git a/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java b/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java
index 5221e73..981c277 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java
@@ -20,12 +20,7 @@
 package org.elasticsearch.search.internal;
 
 import org.apache.lucene.search.BooleanClause.Occur;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.*;
 import org.apache.lucene.util.Counter;
 import org.elasticsearch.action.search.SearchType;
 import org.elasticsearch.cache.recycler.PageCacheRecycler;
@@ -33,8 +28,8 @@ import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.lucene.search.Queries;
-import org.elasticsearch.common.lucene.search.function.BoostScoreFunction;
 import org.elasticsearch.common.lucene.search.function.FunctionScoreQuery;
+import org.elasticsearch.common.lucene.search.function.WeightFactorFunction;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.index.IndexService;
@@ -67,11 +62,7 @@ import org.elasticsearch.search.rescore.RescoreSearchContext;
 import org.elasticsearch.search.suggest.SuggestionSearchContext;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
+import java.util.*;
 
 /**
  *
@@ -206,7 +197,7 @@ public class DefaultSearchContext extends SearchContext {
             parsedQuery(ParsedQuery.parsedMatchAllQuery());
         }
         if (queryBoost() != 1.0f) {
-            parsedQuery(new ParsedQuery(new FunctionScoreQuery(query(), new BoostScoreFunction(queryBoost)), parsedQuery()));
+            parsedQuery(new ParsedQuery(new FunctionScoreQuery(query(), new WeightFactorFunction(queryBoost)), parsedQuery()));
         }
         Query searchFilter = searchFilter(types());
         if (searchFilter != null) {
diff --git a/core/src/main/java/org/elasticsearch/search/suggest/context/GeolocationContextMapping.java b/core/src/main/java/org/elasticsearch/search/suggest/context/GeolocationContextMapping.java
index a02868a..f2d168f 100644
--- a/core/src/main/java/org/elasticsearch/search/suggest/context/GeolocationContextMapping.java
+++ b/core/src/main/java/org/elasticsearch/search/suggest/context/GeolocationContextMapping.java
@@ -24,12 +24,12 @@ import org.apache.lucene.analysis.PrefixAnalyzer.PrefixTokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.index.DocValuesType;
 import org.apache.lucene.index.IndexableField;
+import org.apache.lucene.util.XGeoHashUtils;
 import org.apache.lucene.util.automaton.Automata;
 import org.apache.lucene.util.automaton.Automaton;
 import org.apache.lucene.util.automaton.Operations;
 import org.apache.lucene.util.fst.FST;
 import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.common.geo.GeoHashUtils;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.geo.GeoUtils;
 import org.elasticsearch.common.unit.DistanceUnit;
@@ -227,7 +227,7 @@ public class GeolocationContextMapping extends ContextMapping {
                 if(parser.nextToken() == Token.VALUE_NUMBER) {
                     double lat = parser.doubleValue();
                     if(parser.nextToken() == Token.END_ARRAY) {
-                        return Collections.singleton(GeoHashUtils.encode(lat, lon));
+                        return Collections.singleton(XGeoHashUtils.stringEncode(lon, lat));
                     } else {
                         throw new ElasticsearchParseException("only two values expected");
                     }
@@ -294,7 +294,7 @@ public class GeolocationContextMapping extends ContextMapping {
      * @return new geolocation query
      */
     public static GeoQuery query(String name, double lat, double lon, int ... precisions) {
-        return query(name, GeoHashUtils.encode(lat, lon), precisions);
+        return query(name, XGeoHashUtils.stringEncode(lon, lat), precisions);
     }
 
     public static GeoQuery query(String name, double lat, double lon, String ... precisions) {
@@ -302,7 +302,7 @@ public class GeolocationContextMapping extends ContextMapping {
         for (int i = 0 ; i < precisions.length; i++) {
             precisionInts[i] = GeoUtils.geoHashLevelsForPrecision(precisions[i]);
         }
-        return query(name, GeoHashUtils.encode(lat, lon), precisionInts);
+        return query(name, XGeoHashUtils.stringEncode(lon, lat), precisionInts);
     }
 
     /**
@@ -574,7 +574,7 @@ public class GeolocationContextMapping extends ContextMapping {
          * @return this
          */
         public Builder addDefaultLocation(double lat, double lon) {
-            this.defaultLocations.add(GeoHashUtils.encode(lat, lon));
+            this.defaultLocations.add(XGeoHashUtils.stringEncode(lon, lat));
             return this;
         }
 
@@ -604,7 +604,7 @@ public class GeolocationContextMapping extends ContextMapping {
         @Override
         public GeolocationContextMapping build() {
             if(precisions.isEmpty()) {
-                precisions.add(GeoHashUtils.PRECISION);
+                precisions.add(XGeoHashUtils.PRECISION);
             }
             int[] precisionArray = precisions.toArray();
             Arrays.sort(precisionArray);
@@ -670,7 +670,7 @@ public class GeolocationContextMapping extends ContextMapping {
                     int precision = Math.min(p, geohash.length());
                     String truncatedGeohash = geohash.substring(0, precision);
                     if(mapping.neighbors) {
-                        GeoHashUtils.addNeighbors(truncatedGeohash, precision, locations);
+                        XGeoHashUtils.addNeighbors(truncatedGeohash, precision, locations);
                     }
                     locations.add(truncatedGeohash);
                 }
diff --git a/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java b/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java
index 3616725..105f851 100644
--- a/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java
+++ b/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java
@@ -1036,14 +1036,14 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
     /**
      * Internal class that is used to send notifications about finished shard restore operations to master node
      */
-    static class UpdateIndexShardRestoreStatusRequest extends TransportRequest {
+    public static class UpdateIndexShardRestoreStatusRequest extends TransportRequest {
         private SnapshotId snapshotId;
         private ShardId shardId;
         private ShardRestoreStatus status;
 
         volatile boolean processed; // state field, no need to serialize
 
-        private UpdateIndexShardRestoreStatusRequest() {
+        public UpdateIndexShardRestoreStatusRequest() {
 
         }
 
diff --git a/core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java b/core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java
index 57c4a4e..43d71ba 100644
--- a/core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java
+++ b/core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java
@@ -410,7 +410,7 @@ public class SnapshotShardsService extends AbstractLifecycleComponent<SnapshotSh
     /**
      * Internal request that is used to send changes in snapshot status to master
      */
-    private static class UpdateIndexShardSnapshotStatusRequest extends TransportRequest {
+    public static class UpdateIndexShardSnapshotStatusRequest extends TransportRequest {
         private SnapshotId snapshotId;
         private ShardId shardId;
         private SnapshotsInProgress.ShardSnapshotStatus status;
diff --git a/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java b/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
index 9389154..77b86a0 100644
--- a/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
+++ b/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.threadpool;
 
 import com.google.common.collect.ImmutableMap;
-import com.google.common.util.concurrent.MoreExecutors;
 import org.apache.lucene.util.Counter;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.component.AbstractComponent;
@@ -102,6 +101,8 @@ public class ThreadPool extends AbstractComponent {
 
     private boolean settingsListenerIsSet = false;
 
+    static final Executor DIRECT_EXECUTOR = command -> command.run();
+
 
     public ThreadPool(String name) {
         this(Settings.builder().put("name", name).build());
@@ -151,7 +152,7 @@ public class ThreadPool extends AbstractComponent {
             executors.put(entry.getKey(), build(entry.getKey(), entry.getValue(), Settings.EMPTY));
         }
 
-        executors.put(Names.SAME, new ExecutorHolder(MoreExecutors.directExecutor(), new Info(Names.SAME, "same")));
+        executors.put(Names.SAME, new ExecutorHolder(DIRECT_EXECUTOR, new Info(Names.SAME, "same")));
         if (!executors.get(Names.GENERIC).info.getType().equals("cached")) {
             throw new IllegalArgumentException("generic thread pool must be of type cached");
         }
@@ -322,7 +323,7 @@ public class ThreadPool extends AbstractComponent {
             } else {
                 logger.debug("creating thread_pool [{}], type [{}]", name, type);
             }
-            return new ExecutorHolder(MoreExecutors.directExecutor(), new Info(name, type));
+            return new ExecutorHolder(DIRECT_EXECUTOR, new Info(name, type));
         } else if ("cached".equals(type)) {
             TimeValue defaultKeepAlive = defaultSettings.getAsTime("keep_alive", timeValueMinutes(5));
             if (previousExecutorHolder != null) {
@@ -614,7 +615,7 @@ public class ThreadPool extends AbstractComponent {
         public final Info info;
 
         ExecutorHolder(Executor executor, Info info) {
-            assert executor instanceof EsThreadPoolExecutor || executor == MoreExecutors.directExecutor();
+            assert executor instanceof EsThreadPoolExecutor || executor == DIRECT_EXECUTOR;
             this.executor = executor;
             this.info = info;
         }
diff --git a/core/src/main/java/org/elasticsearch/transport/RequestHandlerRegistry.java b/core/src/main/java/org/elasticsearch/transport/RequestHandlerRegistry.java
index 2aa6818..344ed9e 100644
--- a/core/src/main/java/org/elasticsearch/transport/RequestHandlerRegistry.java
+++ b/core/src/main/java/org/elasticsearch/transport/RequestHandlerRegistry.java
@@ -81,12 +81,15 @@ public class RequestHandlerRegistry<Request extends TransportRequest> {
             } catch (NoSuchMethodException e) {
                 throw new IllegalStateException("failed to create constructor (does it have a default constructor?) for request " + request, e);
             }
-            this.requestConstructor.setAccessible(true);
         }
 
         @Override
         public Request call() throws Exception {
-            return requestConstructor.newInstance();
+            try {
+                return requestConstructor.newInstance();
+            } catch (IllegalAccessException e) {
+                throw new IllegalStateException("Could not access '" + requestConstructor + "'. Implementations must be a public class and have a public no-arg ctor.", e);
+            }
         }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/transport/TransportRequest.java b/core/src/main/java/org/elasticsearch/transport/TransportRequest.java
index bd2e83d..ddf5417 100644
--- a/core/src/main/java/org/elasticsearch/transport/TransportRequest.java
+++ b/core/src/main/java/org/elasticsearch/transport/TransportRequest.java
@@ -36,7 +36,7 @@ public abstract class TransportRequest extends TransportMessage<TransportRequest
         }
     }
 
-    protected TransportRequest() {
+    public TransportRequest() {
     }
 
     protected TransportRequest(TransportRequest request) {
diff --git a/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java b/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
index 991484a..672de6b 100644
--- a/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
+++ b/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
@@ -184,7 +184,8 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
 
     protected final BigArrays bigArrays;
     protected final ThreadPool threadPool;
-    protected volatile OpenChannelsHandler serverOpenChannels;
+    // package private for testing
+    volatile OpenChannelsHandler serverOpenChannels;
     protected volatile ClientBootstrap clientBootstrap;
     // node id to actual channel
     protected final ConcurrentMap<DiscoveryNode, NodeChannels> connectedNodes = newConcurrentMap();
diff --git a/core/src/main/resources/org/elasticsearch/bootstrap/security.policy b/core/src/main/resources/org/elasticsearch/bootstrap/security.policy
index 4f984aa..befeef4 100644
--- a/core/src/main/resources/org/elasticsearch/bootstrap/security.policy
+++ b/core/src/main/resources/org/elasticsearch/bootstrap/security.policy
@@ -36,6 +36,10 @@ grant codeBase "${es.security.jar.lucene.core}" {
   permission java.lang.RuntimePermission "accessClassInPackage.sun.misc";
 };
 
+//// test framework permissions.
+//// These are mock objects and test management that we allow test framework libs
+//// to provide on our behalf. But tests themselves cannot do this stuff!
+
 grant codeBase "${es.security.jar.elasticsearch.securemock}" {
   // needed to support creation of mocks
   permission java.lang.RuntimePermission "reflectionFactoryAccess";
@@ -80,12 +84,8 @@ grant {
   permission java.lang.RuntimePermission "getProtectionDomain";
 
   // reflection hacks:
-  // needed for mock filesystems in tests (to capture implCloseChannel)
-  permission java.lang.RuntimePermission "accessClassInPackage.sun.nio.ch";
   // needed by groovy engine
   permission java.lang.RuntimePermission "accessClassInPackage.sun.reflect";
-  // needed by aws core sdk (TODO: look into this)
-  permission java.lang.RuntimePermission "accessClassInPackage.sun.security.ssl";
   
   // needed by RandomizedRunner
   permission java.lang.RuntimePermission "accessDeclaredMembers";
diff --git a/core/src/test/java/org/elasticsearch/ESExceptionTests.java b/core/src/test/java/org/elasticsearch/ESExceptionTests.java
index dea127a..a43352f 100644
--- a/core/src/test/java/org/elasticsearch/ESExceptionTests.java
+++ b/core/src/test/java/org/elasticsearch/ESExceptionTests.java
@@ -24,6 +24,7 @@ import org.apache.lucene.index.IndexFormatTooNewException;
 import org.apache.lucene.index.IndexFormatTooOldException;
 import org.apache.lucene.store.AlreadyClosedException;
 import org.apache.lucene.store.LockObtainFailedException;
+import org.apache.lucene.util.Constants;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.ShardSearchFailure;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
@@ -324,7 +325,12 @@ public class ESExceptionTests extends ESTestCase {
             } else {
                 assertEquals(e.getCause().getClass(), NotSerializableExceptionWrapper.class);
             }
-            assertArrayEquals(e.getStackTrace(), ex.getStackTrace());
+            // TODO: fix this test
+            // on java 9, expected:<sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)> 
+            //            but was:<sun.reflect.NativeMethodAccessorImpl.invoke0(java.base@9.0/Native Method)>
+            if (!Constants.JRE_IS_MINIMUM_JAVA9) {
+                assertArrayEquals(e.getStackTrace(), ex.getStackTrace());
+            }
             assertTrue(e.getStackTrace().length > 1);
             ElasticsearchAssertions.assertVersionSerializable(VersionUtils.randomVersion(getRandom()), t);
             ElasticsearchAssertions.assertVersionSerializable(VersionUtils.randomVersion(getRandom()), ex);
diff --git a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
index 1a699e7..32e30cf 100644
--- a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
+++ b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
@@ -21,6 +21,8 @@ package org.elasticsearch;
 import com.fasterxml.jackson.core.JsonLocation;
 import com.fasterxml.jackson.core.JsonParseException;
 import com.google.common.collect.ImmutableSet;
+
+import org.apache.lucene.util.Constants;
 import org.codehaus.groovy.runtime.typehandling.GroovyCastException;
 import org.elasticsearch.action.FailedNodeException;
 import org.elasticsearch.action.RoutingMissingException;
@@ -567,12 +569,15 @@ public class ExceptionSerializationTests extends ESTestCase {
             }
             Throwable deserialized = serialize(t);
             assertTrue(deserialized instanceof NotSerializableExceptionWrapper);
-            assertArrayEquals(t.getStackTrace(), deserialized.getStackTrace());
-            assertEquals(t.getSuppressed().length, deserialized.getSuppressed().length);
-            if (t.getSuppressed().length > 0) {
-                assertTrue(deserialized.getSuppressed()[0] instanceof NotSerializableExceptionWrapper);
-                assertArrayEquals(t.getSuppressed()[0].getStackTrace(), deserialized.getSuppressed()[0].getStackTrace());
-                assertTrue(deserialized.getSuppressed()[1] instanceof NullPointerException);
+            // TODO: fix this test for more java 9 differences
+            if (!Constants.JRE_IS_MINIMUM_JAVA9) {
+                assertArrayEquals(t.getStackTrace(), deserialized.getStackTrace());
+                assertEquals(t.getSuppressed().length, deserialized.getSuppressed().length);
+                if (t.getSuppressed().length > 0) {
+                    assertTrue(deserialized.getSuppressed()[0] instanceof NotSerializableExceptionWrapper);
+                    assertArrayEquals(t.getSuppressed()[0].getStackTrace(), deserialized.getSuppressed()[0].getStackTrace());
+                    assertTrue(deserialized.getSuppressed()[1] instanceof NullPointerException);
+                }
             }
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/action/search/SearchRequestBuilderTests.java b/core/src/test/java/org/elasticsearch/action/search/SearchRequestBuilderTests.java
index 3c2bada..1a05794 100644
--- a/core/src/test/java/org/elasticsearch/action/search/SearchRequestBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/action/search/SearchRequestBuilderTests.java
@@ -21,6 +21,7 @@ package org.elasticsearch.action.search;
 
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.transport.TransportClient;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
@@ -95,7 +96,7 @@ public class SearchRequestBuilderTests extends ESTestCase {
     public void testStringSourceToString() {
         SearchRequestBuilder searchRequestBuilder = client.prepareSearch();
         String source = "{ \"query\" : { \"match_all\" : {} } }";
-        searchRequestBuilder.setSource(source);
+        searchRequestBuilder.setSource(new BytesArray(source));
         assertThat(searchRequestBuilder.toString(), equalTo(source));
     }
 
@@ -109,7 +110,7 @@ public class SearchRequestBuilderTests extends ESTestCase {
         xContentBuilder.endObject();
         xContentBuilder.endObject();
         xContentBuilder.endObject();
-        searchRequestBuilder.setSource(xContentBuilder);
+        searchRequestBuilder.setSource(xContentBuilder.bytes());
         assertThat(searchRequestBuilder.toString(), equalTo(XContentHelper.convertToJson(xContentBuilder.bytes(), false, true)));
     }
 
@@ -124,7 +125,7 @@ public class SearchRequestBuilderTests extends ESTestCase {
                 "            }\n" +
                 "        }\n" +
                 "        }";
-        SearchRequestBuilder searchRequestBuilder = client.prepareSearch().setSource(source);
+        SearchRequestBuilder searchRequestBuilder = client.prepareSearch().setSource(new BytesArray(source));
         String preToString = searchRequestBuilder.request().source().toUtf8();
         assertThat(searchRequestBuilder.toString(), equalTo(source));
         String postToString = searchRequestBuilder.request().source().toUtf8();
diff --git a/core/src/test/java/org/elasticsearch/action/support/TransportActionFilterChainTests.java b/core/src/test/java/org/elasticsearch/action/support/TransportActionFilterChainTests.java
index b1fd92a..148fc70 100644
--- a/core/src/test/java/org/elasticsearch/action/support/TransportActionFilterChainTests.java
+++ b/core/src/test/java/org/elasticsearch/action/support/TransportActionFilterChainTests.java
@@ -447,7 +447,7 @@ public class TransportActionFilterChainTests extends ESTestCase {
         void execute(String action, ActionResponse response, ActionListener listener, ActionFilterChain chain);
     }
 
-    private static class TestRequest extends ActionRequest {
+    public static class TestRequest extends ActionRequest {
         @Override
         public ActionRequestValidationException validate() {
             return null;
diff --git a/core/src/test/java/org/elasticsearch/action/support/replication/ShardReplicationTests.java b/core/src/test/java/org/elasticsearch/action/support/replication/ShardReplicationTests.java
index 4753f61..03a6bb7 100644
--- a/core/src/test/java/org/elasticsearch/action/support/replication/ShardReplicationTests.java
+++ b/core/src/test/java/org/elasticsearch/action/support/replication/ShardReplicationTests.java
@@ -551,12 +551,12 @@ public class ShardReplicationTests extends ESTestCase {
         };
     }
 
-    static class Request extends ReplicationRequest<Request> {
+    public static class Request extends ReplicationRequest<Request> {
         int shardId;
         public AtomicBoolean processedOnPrimary = new AtomicBoolean();
         public AtomicInteger processedOnReplicas = new AtomicInteger();
 
-        Request() {
+        public Request() {
         }
 
         Request(ShardId shardId) {
diff --git a/core/src/test/java/org/elasticsearch/bootstrap/BootstrapForTesting.java b/core/src/test/java/org/elasticsearch/bootstrap/BootstrapForTesting.java
index 3220cef..e31fc7d 100644
--- a/core/src/test/java/org/elasticsearch/bootstrap/BootstrapForTesting.java
+++ b/core/src/test/java/org/elasticsearch/bootstrap/BootstrapForTesting.java
@@ -86,7 +86,7 @@ public class BootstrapForTesting {
                 // initialize paths the same exact way as bootstrap.
                 Permissions perms = new Permissions();
                 // add permissions to everything in classpath
-                for (URL url : ((URLClassLoader)BootstrapForTesting.class.getClassLoader()).getURLs()) {
+                for (URL url : JarHell.parseClassPath()) {
                     Path path = PathUtils.get(url.toURI());
                     // resource itself
                     perms.add(new FilePermission(path.toString(), "read,readlink"));
@@ -115,7 +115,7 @@ public class BootstrapForTesting {
                     perms.add(new FilePermission(coverageDir.resolve("jacoco-it.exec").toString(), "read,write"));
                 }
                 Policy.setPolicy(new ESPolicy(perms));
-                System.setSecurityManager(new XTestSecurityManager());
+                System.setSecurityManager(new TestSecurityManager());
                 Security.selfTest();
             } catch (Exception e) {
                 throw new RuntimeException("unable to install test security manager", e);
diff --git a/core/src/test/java/org/elasticsearch/bootstrap/SecurityTests.java b/core/src/test/java/org/elasticsearch/bootstrap/SecurityTests.java
index eaa2592..d0685b1 100644
--- a/core/src/test/java/org/elasticsearch/bootstrap/SecurityTests.java
+++ b/core/src/test/java/org/elasticsearch/bootstrap/SecurityTests.java
@@ -200,22 +200,6 @@ public class SecurityTests extends ESTestCase {
         } catch (IOException expected) {}
     }
 
-    /** We only grant this to special jars */
-    public void testUnsafeAccess() throws Exception {
-        assumeTrue("test requires security manager", System.getSecurityManager() != null);
-        try {
-            // class could be legitimately loaded, so we might not fail until setAccessible
-            Class.forName("sun.misc.Unsafe")
-                 .getDeclaredField("theUnsafe")
-                 .setAccessible(true);
-            fail("didn't get expected exception");
-        } catch (SecurityException expected) {
-            // ok
-        } catch (Exception somethingElse) {
-            assumeNoException("perhaps JVM doesn't have Unsafe?", somethingElse);
-        }
-    }
-
     /** can't execute processes */
     public void testProcessExecution() throws Exception {
         assumeTrue("test requires security manager", System.getSecurityManager() != null);
diff --git a/core/src/test/java/org/elasticsearch/bootstrap/XTestSecurityManager.java b/core/src/test/java/org/elasticsearch/bootstrap/XTestSecurityManager.java
deleted file mode 100644
index c626274..0000000
--- a/core/src/test/java/org/elasticsearch/bootstrap/XTestSecurityManager.java
+++ /dev/null
@@ -1,113 +0,0 @@
-package org.elasticsearch.bootstrap;
-
-import java.security.AccessController;
-import java.security.PrivilegedAction;
-
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-// the above license header is a lie, here is the real one.
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link SecurityManager} that prevents tests calling {@link System#exit(int)}.
- * Only the test runner itself is allowed to exit the JVM.
- * All other security checks are handled by the default security policy.
- * <p>
- * Use this with {@code -Djava.security.manager=org.apache.lucene.util.TestSecurityManager}.
- */ 
-// TODO: remove me when https://issues.apache.org/jira/browse/LUCENE-6794 is committed
-public final class XTestSecurityManager extends SecurityManager {
-  
-  static final String JUNIT4_TEST_RUNNER_PACKAGE = "com.carrotsearch.ant.tasks.junit4.";
-  static final String ECLIPSE_TEST_RUNNER_PACKAGE = "org.eclipse.jdt.internal.junit.runner.";
-  static final String IDEA_TEST_RUNNER_PACKAGE = "com.intellij.rt.execution.junit.";
-
-  /**
-   * Creates a new TestSecurityManager. This ctor is called on JVM startup,
-   * when {@code -Djava.security.manager=org.apache.lucene.util.TestSecurityManager}
-   * is passed to JVM.
-   */
-  public XTestSecurityManager() {
-    super();
-  }
-
-  /**
-   * {@inheritDoc}
-   * <p>This method inspects the stack trace and checks who is calling
-   * {@link System#exit(int)} and similar methods
-   * @throws SecurityException if the caller of this method is not the test runner itself.
-   */
-  @Override
-  public void checkExit(final int status) {
-    AccessController.doPrivileged((PrivilegedAction<Void>) () -> {
-        final String systemClassName = System.class.getName(),
-                runtimeClassName = Runtime.class.getName();
-        String exitMethodHit = null;
-        for (final StackTraceElement se : Thread.currentThread().getStackTrace()) {
-          final String className = se.getClassName(), methodName = se.getMethodName();
-          if (
-                  ("exit".equals(methodName) || "halt".equals(methodName)) &&
-                          (systemClassName.equals(className) || runtimeClassName.equals(className))
-                  ) {
-            exitMethodHit = className + '#' + methodName + '(' + status + ')';
-            continue;
-          }
-
-          if (exitMethodHit != null) {
-            if (className.startsWith(JUNIT4_TEST_RUNNER_PACKAGE) ||
-                    className.startsWith(ECLIPSE_TEST_RUNNER_PACKAGE) ||
-                    className.startsWith(IDEA_TEST_RUNNER_PACKAGE)) {
-              // this exit point is allowed, we return normally from closure:
-              return /*void*/ null;
-            } else {
-              // anything else in stack trace is not allowed, break and throw SecurityException below:
-              break;
-            }
-          }
-        }
-
-        if (exitMethodHit == null) {
-          // should never happen, only if JVM hides stack trace - replace by generic:
-          exitMethodHit = "JVM exit method";
-        }
-        throw new SecurityException(exitMethodHit + " calls are not allowed because they terminate the test runner's JVM.");
-    });
-    
-    // we passed the stack check, delegate to super, so default policy can still deny permission:
-    super.checkExit(status);
-  }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java b/core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java
index 8151876..8e71f3d 100644
--- a/core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java
+++ b/core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.bwcompat;
 
-import com.google.common.util.concurrent.ListenableFuture;
-
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.TestUtil;
@@ -50,6 +48,7 @@ import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.sort.SortOrder;
 import org.elasticsearch.test.ESIntegTestCase;
+import org.elasticsearch.test.InternalTestCluster;
 import org.elasticsearch.test.VersionUtils;
 import org.elasticsearch.test.hamcrest.ElasticsearchAssertions;
 import org.hamcrest.Matchers;
@@ -72,6 +71,7 @@ import java.util.Locale;
 import java.util.Map;
 import java.util.SortedSet;
 import java.util.TreeSet;
+import java.util.concurrent.Future;
 
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
@@ -122,20 +122,20 @@ public class OldIndexBackwardsCompatibilityIT extends ESIntegTestCase {
     }
 
     void setupCluster() throws Exception {
-        ListenableFuture<List<String>> replicas = internalCluster().startNodesAsync(1); // for replicas
+        InternalTestCluster.Async<List<String>> replicas = internalCluster().startNodesAsync(1); // for replicas
 
         Path baseTempDir = createTempDir();
         // start single data path node
         Settings.Builder nodeSettings = Settings.builder()
             .put("path.data", baseTempDir.resolve("single-path").toAbsolutePath())
             .put("node.master", false); // workaround for dangling index loading issue when node is master
-        ListenableFuture<String> singleDataPathNode = internalCluster().startNodeAsync(nodeSettings.build());
+        InternalTestCluster.Async<String> singleDataPathNode = internalCluster().startNodeAsync(nodeSettings.build());
 
         // start multi data path node
         nodeSettings = Settings.builder()
             .put("path.data", baseTempDir.resolve("multi-path1").toAbsolutePath() + "," + baseTempDir.resolve("multi-path2").toAbsolutePath())
             .put("node.master", false); // workaround for dangling index loading issue when node is master
-        ListenableFuture<String> multiDataPathNode = internalCluster().startNodeAsync(nodeSettings.build());
+        InternalTestCluster.Async<String> multiDataPathNode = internalCluster().startNodeAsync(nodeSettings.build());
 
         // find single data path dir
         Path[] nodePaths = internalCluster().getInstance(NodeEnvironment.class, singleDataPathNode.get()).nodeDataPaths();
diff --git a/core/src/test/java/org/elasticsearch/client/transport/TransportClientNodesServiceTests.java b/core/src/test/java/org/elasticsearch/client/transport/TransportClientNodesServiceTests.java
index a1e10b0..98cef97 100644
--- a/core/src/test/java/org/elasticsearch/client/transport/TransportClientNodesServiceTests.java
+++ b/core/src/test/java/org/elasticsearch/client/transport/TransportClientNodesServiceTests.java
@@ -173,7 +173,7 @@ public class TransportClientNodesServiceTests extends ESTestCase {
         }
     }
 
-    private static class TestRequest extends TransportRequest {
+    public static class TestRequest extends TransportRequest {
 
     }
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/ClusterServiceIT.java b/core/src/test/java/org/elasticsearch/cluster/ClusterServiceIT.java
index 88a920e..72327e4 100644
--- a/core/src/test/java/org/elasticsearch/cluster/ClusterServiceIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/ClusterServiceIT.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.cluster;
 
-import com.google.common.util.concurrent.ListenableFuture;
 import org.apache.log4j.Level;
 import org.apache.log4j.Logger;
 import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
@@ -37,6 +36,7 @@ import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
+import org.elasticsearch.test.InternalTestCluster;
 import org.elasticsearch.test.MockLogAppender;
 import org.elasticsearch.test.junit.annotations.TestLogging;
 import org.elasticsearch.threadpool.ThreadPool;
@@ -297,8 +297,8 @@ public class ClusterServiceIT extends ESIntegTestCase {
                 .put("discovery.type", "local")
                 .build();
 
-        ListenableFuture<String> master = internalCluster().startNodeAsync(settings);
-        ListenableFuture<String> nonMaster = internalCluster().startNodeAsync(settingsBuilder().put(settings).put("node.master", false).build());
+        InternalTestCluster.Async<String> master = internalCluster().startNodeAsync(settings);
+        InternalTestCluster.Async<String> nonMaster = internalCluster().startNodeAsync(settingsBuilder().put(settings).put("node.master", false).build());
         master.get();
         ensureGreen(); // make sure we have a cluster
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffPublishingTests.java b/core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffPublishingTests.java
new file mode 100644
index 0000000..1df483b
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffPublishingTests.java
@@ -0,0 +1,629 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cluster;
+
+import com.google.common.collect.ImmutableMap;
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.block.ClusterBlocks;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.cluster.metadata.MetaData;
+import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.cluster.node.DiscoveryNodes;
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.collect.ImmutableOpenMap;
+import org.elasticsearch.common.collect.Tuple;
+import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.discovery.Discovery;
+import org.elasticsearch.discovery.DiscoverySettings;
+import org.elasticsearch.discovery.zen.DiscoveryNodesProvider;
+import org.elasticsearch.discovery.zen.publish.PublishClusterStateAction;
+import org.elasticsearch.node.service.NodeService;
+import org.elasticsearch.node.settings.NodeSettingsService;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.junit.annotations.TestLogging;
+import org.elasticsearch.test.transport.MockTransportService;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.elasticsearch.transport.TransportConnectionListener;
+import org.elasticsearch.transport.TransportService;
+import org.elasticsearch.transport.local.LocalTransport;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.CopyOnWriteArrayList;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+import static org.hamcrest.Matchers.containsString;
+import static org.hamcrest.Matchers.emptyIterable;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.notNullValue;
+import static org.hamcrest.Matchers.nullValue;
+
+public class ClusterStateDiffPublishingTests extends ESTestCase {
+
+    protected ThreadPool threadPool;
+    protected Map<String, MockNode> nodes = new HashMap<>();
+
+    public static class MockNode {
+        public final DiscoveryNode discoveryNode;
+        public final MockTransportService service;
+        public final PublishClusterStateAction action;
+        public final MockDiscoveryNodesProvider nodesProvider;
+
+        public MockNode(DiscoveryNode discoveryNode, MockTransportService service, PublishClusterStateAction action, MockDiscoveryNodesProvider nodesProvider) {
+            this.discoveryNode = discoveryNode;
+            this.service = service;
+            this.action = action;
+            this.nodesProvider = nodesProvider;
+        }
+
+        public void connectTo(DiscoveryNode node) {
+            service.connectToNode(node);
+            nodesProvider.addNode(node);
+        }
+    }
+
+    public MockNode createMockNode(final String name, Settings settings, Version version) throws Exception {
+        return createMockNode(name, settings, version, new PublishClusterStateAction.NewClusterStateListener() {
+            @Override
+            public void onNewClusterState(ClusterState clusterState, NewStateProcessed newStateProcessed) {
+                logger.debug("Node [{}] onNewClusterState version [{}], uuid [{}]", name, clusterState.version(), clusterState.stateUUID());
+                newStateProcessed.onNewClusterStateProcessed();
+            }
+        });
+    }
+
+    public MockNode createMockNode(String name, Settings settings, Version version, PublishClusterStateAction.NewClusterStateListener listener) throws Exception {
+        MockTransportService service = buildTransportService(
+                Settings.builder().put(settings).put("name", name, TransportService.SETTING_TRACE_LOG_INCLUDE, "", TransportService.SETTING_TRACE_LOG_EXCLUDE, "NOTHING").build(),
+                version
+        );
+        DiscoveryNode discoveryNode = new DiscoveryNode(name, name, service.boundAddress().publishAddress(), ImmutableMap.<String, String>of(), version);
+        MockDiscoveryNodesProvider nodesProvider = new MockDiscoveryNodesProvider(discoveryNode);
+        PublishClusterStateAction action = buildPublishClusterStateAction(settings, service, nodesProvider, listener);
+        MockNode node = new MockNode(discoveryNode, service, action, nodesProvider);
+        nodesProvider.addNode(discoveryNode);
+        final CountDownLatch latch = new CountDownLatch(nodes.size() * 2 + 1);
+        TransportConnectionListener waitForConnection = new TransportConnectionListener() {
+            @Override
+            public void onNodeConnected(DiscoveryNode node) {
+                latch.countDown();
+            }
+
+            @Override
+            public void onNodeDisconnected(DiscoveryNode node) {
+                fail("disconnect should not be called " + node);
+            }
+        };
+        node.service.addConnectionListener(waitForConnection);
+        for (MockNode curNode : nodes.values()) {
+            curNode.service.addConnectionListener(waitForConnection);
+            curNode.connectTo(node.discoveryNode);
+            node.connectTo(curNode.discoveryNode);
+        }
+        node.connectTo(node.discoveryNode);
+        assertThat("failed to wait for all nodes to connect", latch.await(5, TimeUnit.SECONDS), equalTo(true));
+        for (MockNode curNode : nodes.values()) {
+            curNode.service.removeConnectionListener(waitForConnection);
+        }
+        node.service.removeConnectionListener(waitForConnection);
+        if (nodes.put(name, node) != null) {
+            fail("Node with the name " + name + " already exist");
+        }
+        return node;
+    }
+
+    public MockTransportService service(String name) {
+        MockNode node = nodes.get(name);
+        if (node != null) {
+            return node.service;
+        }
+        return null;
+    }
+
+    public PublishClusterStateAction action(String name) {
+        MockNode node = nodes.get(name);
+        if (node != null) {
+            return node.action;
+        }
+        return null;
+    }
+
+    @Override
+    @Before
+    public void setUp() throws Exception {
+        super.setUp();
+        threadPool = new ThreadPool(getClass().getName());
+    }
+
+    @Override
+    @After
+    public void tearDown() throws Exception {
+        super.tearDown();
+        for (MockNode curNode : nodes.values()) {
+            curNode.action.close();
+            curNode.service.close();
+        }
+        terminate(threadPool);
+    }
+
+    protected MockTransportService buildTransportService(Settings settings, Version version) {
+        MockTransportService transportService = new MockTransportService(settings, new LocalTransport(settings, threadPool, version, new NamedWriteableRegistry()), threadPool);
+        transportService.start();
+        return transportService;
+    }
+
+    protected PublishClusterStateAction buildPublishClusterStateAction(Settings settings, MockTransportService transportService, MockDiscoveryNodesProvider nodesProvider,
+                                                                       PublishClusterStateAction.NewClusterStateListener listener) {
+        DiscoverySettings discoverySettings = new DiscoverySettings(settings, new NodeSettingsService(settings));
+        return new PublishClusterStateAction(settings, transportService, nodesProvider, listener, discoverySettings);
+    }
+
+
+    static class MockDiscoveryNodesProvider implements DiscoveryNodesProvider {
+
+        private DiscoveryNodes discoveryNodes = DiscoveryNodes.EMPTY_NODES;
+
+        public MockDiscoveryNodesProvider(DiscoveryNode localNode) {
+            discoveryNodes = DiscoveryNodes.builder().put(localNode).localNodeId(localNode.id()).build();
+        }
+
+        public void addNode(DiscoveryNode node) {
+            discoveryNodes = DiscoveryNodes.builder(discoveryNodes).put(node).build();
+        }
+
+        @Override
+        public DiscoveryNodes nodes() {
+            return discoveryNodes;
+        }
+
+        @Override
+        public NodeService nodeService() {
+            assert false;
+            throw new UnsupportedOperationException("Shouldn't be here");
+        }
+    }
+
+
+    @Test
+    @TestLogging("cluster:DEBUG,discovery.zen.publish:DEBUG")
+    public void testSimpleClusterStatePublishing() throws Exception {
+        MockNewClusterStateListener mockListenerA = new MockNewClusterStateListener();
+        MockNode nodeA = createMockNode("nodeA", Settings.EMPTY, Version.CURRENT, mockListenerA);
+
+        MockNewClusterStateListener mockListenerB = new MockNewClusterStateListener();
+        MockNode nodeB = createMockNode("nodeB", Settings.EMPTY, Version.CURRENT, mockListenerB);
+
+        // Initial cluster state
+        DiscoveryNodes discoveryNodes = DiscoveryNodes.builder().put(nodeA.discoveryNode).localNodeId(nodeA.discoveryNode.id()).build();
+        ClusterState clusterState = ClusterState.builder(new ClusterName("test")).nodes(discoveryNodes).build();
+
+        // cluster state update - add nodeB
+        discoveryNodes = DiscoveryNodes.builder(discoveryNodes).put(nodeB.discoveryNode).build();
+        ClusterState previousClusterState = clusterState;
+        clusterState = ClusterState.builder(clusterState).nodes(discoveryNodes).incrementVersion().build();
+        mockListenerB.add(new NewClusterStateExpectation() {
+            @Override
+            public void check(ClusterState clusterState, PublishClusterStateAction.NewClusterStateListener.NewStateProcessed newStateProcessed) {
+                assertFalse(clusterState.wasReadFromDiff());
+            }
+        });
+        publishStateDiffAndWait(nodeA.action, clusterState, previousClusterState);
+
+        // cluster state update - add block
+        previousClusterState = clusterState;
+        clusterState = ClusterState.builder(clusterState).blocks(ClusterBlocks.builder().addGlobalBlock(MetaData.CLUSTER_READ_ONLY_BLOCK)).incrementVersion().build();
+        mockListenerB.add(new NewClusterStateExpectation() {
+            @Override
+            public void check(ClusterState clusterState, PublishClusterStateAction.NewClusterStateListener.NewStateProcessed newStateProcessed) {
+                assertTrue(clusterState.wasReadFromDiff());
+                assertThat(clusterState.blocks().global().size(), equalTo(1));
+            }
+        });
+        publishStateDiffAndWait(nodeA.action, clusterState, previousClusterState);
+
+        // cluster state update - remove block
+        previousClusterState = clusterState;
+        clusterState = ClusterState.builder(clusterState).blocks(ClusterBlocks.EMPTY_CLUSTER_BLOCK).incrementVersion().build();
+        mockListenerB.add(new NewClusterStateExpectation() {
+            @Override
+            public void check(ClusterState clusterState, PublishClusterStateAction.NewClusterStateListener.NewStateProcessed newStateProcessed) {
+                assertTrue(clusterState.wasReadFromDiff());
+                assertThat(clusterState.blocks().global().size(), equalTo(0));
+            }
+        });
+        publishStateDiffAndWait(nodeA.action, clusterState, previousClusterState);
+
+        // Adding new node - this node should get full cluster state while nodeB should still be getting diffs
+
+        MockNewClusterStateListener mockListenerC = new MockNewClusterStateListener();
+        MockNode nodeC = createMockNode("nodeC", Settings.EMPTY, Version.CURRENT, mockListenerC);
+
+        // cluster state update 3 - register node C
+        previousClusterState = clusterState;
+        discoveryNodes = DiscoveryNodes.builder(discoveryNodes).put(nodeC.discoveryNode).build();
+        clusterState = ClusterState.builder(clusterState).nodes(discoveryNodes).incrementVersion().build();
+        mockListenerB.add(new NewClusterStateExpectation() {
+            @Override
+            public void check(ClusterState clusterState, PublishClusterStateAction.NewClusterStateListener.NewStateProcessed newStateProcessed) {
+                assertTrue(clusterState.wasReadFromDiff());
+                assertThat(clusterState.blocks().global().size(), equalTo(0));
+            }
+        });
+        mockListenerC.add(new NewClusterStateExpectation() {
+            @Override
+            public void check(ClusterState clusterState, PublishClusterStateAction.NewClusterStateListener.NewStateProcessed newStateProcessed) {
+                // First state
+                assertFalse(clusterState.wasReadFromDiff());
+            }
+        });
+        publishStateDiffAndWait(nodeA.action, clusterState, previousClusterState);
+
+        // cluster state update 4 - update settings
+        previousClusterState = clusterState;
+        MetaData metaData = MetaData.builder(clusterState.metaData()).transientSettings(Settings.settingsBuilder().put("foo", "bar").build()).build();
+        clusterState = ClusterState.builder(clusterState).metaData(metaData).incrementVersion().build();
+        NewClusterStateExpectation expectation = new NewClusterStateExpectation() {
+            @Override
+            public void check(ClusterState clusterState, PublishClusterStateAction.NewClusterStateListener.NewStateProcessed newStateProcessed) {
+                assertTrue(clusterState.wasReadFromDiff());
+                assertThat(clusterState.blocks().global().size(), equalTo(0));
+            }
+        };
+        mockListenerB.add(expectation);
+        mockListenerC.add(expectation);
+        publishStateDiffAndWait(nodeA.action, clusterState, previousClusterState);
+
+        // cluster state update - skipping one version change - should request full cluster state
+        previousClusterState = ClusterState.builder(clusterState).incrementVersion().build();
+        clusterState = ClusterState.builder(clusterState).incrementVersion().build();
+        expectation = new NewClusterStateExpectation() {
+            @Override
+            public void check(ClusterState clusterState, PublishClusterStateAction.NewClusterStateListener.NewStateProcessed newStateProcessed) {
+                assertFalse(clusterState.wasReadFromDiff());
+            }
+        };
+        mockListenerB.add(expectation);
+        mockListenerC.add(expectation);
+        publishStateDiffAndWait(nodeA.action, clusterState, previousClusterState);
+
+        // cluster state update - skipping one version change - should request full cluster state
+        previousClusterState = ClusterState.builder(clusterState).incrementVersion().build();
+        clusterState = ClusterState.builder(clusterState).incrementVersion().build();
+        expectation = new NewClusterStateExpectation() {
+            @Override
+            public void check(ClusterState clusterState, PublishClusterStateAction.NewClusterStateListener.NewStateProcessed newStateProcessed) {
+                assertFalse(clusterState.wasReadFromDiff());
+            }
+        };
+        mockListenerB.add(expectation);
+        mockListenerC.add(expectation);
+        publishStateDiffAndWait(nodeA.action, clusterState, previousClusterState);
+
+        // node B becomes the master and sends a version of the cluster state that goes back
+        discoveryNodes = DiscoveryNodes.builder(discoveryNodes)
+                .put(nodeA.discoveryNode)
+                .put(nodeB.discoveryNode)
+                .put(nodeC.discoveryNode)
+                .build();
+        previousClusterState = ClusterState.builder(new ClusterName("test")).nodes(discoveryNodes).build();
+        clusterState = ClusterState.builder(clusterState).nodes(discoveryNodes).incrementVersion().build();
+        expectation = new NewClusterStateExpectation() {
+            @Override
+            public void check(ClusterState clusterState, PublishClusterStateAction.NewClusterStateListener.NewStateProcessed newStateProcessed) {
+                assertFalse(clusterState.wasReadFromDiff());
+            }
+        };
+        mockListenerA.add(expectation);
+        mockListenerC.add(expectation);
+        publishStateDiffAndWait(nodeB.action, clusterState, previousClusterState);
+    }
+
+    @Test
+    @TestLogging("cluster:DEBUG,discovery.zen.publish:DEBUG")
+    public void testUnexpectedDiffPublishing() throws Exception {
+
+        MockNode nodeA = createMockNode("nodeA", Settings.EMPTY, Version.CURRENT, new PublishClusterStateAction.NewClusterStateListener() {
+            @Override
+            public void onNewClusterState(ClusterState clusterState, NewStateProcessed newStateProcessed) {
+                fail("Shouldn't send cluster state to myself");
+            }
+        });
+
+        MockNewClusterStateListener mockListenerB = new MockNewClusterStateListener();
+        MockNode nodeB = createMockNode("nodeB", Settings.EMPTY, Version.CURRENT, mockListenerB);
+
+        // Initial cluster state with both states - the second node still shouldn't get diff even though it's present in the previous cluster state
+        DiscoveryNodes discoveryNodes = DiscoveryNodes.builder().put(nodeA.discoveryNode).put(nodeB.discoveryNode).localNodeId(nodeA.discoveryNode.id()).build();
+        ClusterState previousClusterState = ClusterState.builder(new ClusterName("test")).nodes(discoveryNodes).build();
+        ClusterState clusterState = ClusterState.builder(previousClusterState).incrementVersion().build();
+        mockListenerB.add(new NewClusterStateExpectation() {
+            @Override
+            public void check(ClusterState clusterState, PublishClusterStateAction.NewClusterStateListener.NewStateProcessed newStateProcessed) {
+                assertFalse(clusterState.wasReadFromDiff());
+            }
+        });
+        publishStateDiffAndWait(nodeA.action, clusterState, previousClusterState);
+
+        // cluster state update - add block
+        previousClusterState = clusterState;
+        clusterState = ClusterState.builder(clusterState).blocks(ClusterBlocks.builder().addGlobalBlock(MetaData.CLUSTER_READ_ONLY_BLOCK)).incrementVersion().build();
+        mockListenerB.add(new NewClusterStateExpectation() {
+            @Override
+            public void check(ClusterState clusterState, PublishClusterStateAction.NewClusterStateListener.NewStateProcessed newStateProcessed) {
+                assertTrue(clusterState.wasReadFromDiff());
+            }
+        });
+        publishStateDiffAndWait(nodeA.action, clusterState, previousClusterState);
+    }
+
+    @Test
+    @TestLogging("cluster:DEBUG,discovery.zen.publish:DEBUG")
+    public void testDisablingDiffPublishing() throws Exception {
+        Settings noDiffPublishingSettings = Settings.builder().put(DiscoverySettings.PUBLISH_DIFF_ENABLE, false).build();
+
+        MockNode nodeA = createMockNode("nodeA", noDiffPublishingSettings, Version.CURRENT, new PublishClusterStateAction.NewClusterStateListener() {
+            @Override
+            public void onNewClusterState(ClusterState clusterState, NewStateProcessed newStateProcessed) {
+                fail("Shouldn't send cluster state to myself");
+            }
+        });
+
+        MockNode nodeB = createMockNode("nodeB", noDiffPublishingSettings, Version.CURRENT, new PublishClusterStateAction.NewClusterStateListener() {
+            @Override
+            public void onNewClusterState(ClusterState clusterState, NewStateProcessed newStateProcessed) {
+                logger.debug("Got cluster state update, version [{}], guid [{}], from diff [{}]", clusterState.version(), clusterState.stateUUID(), clusterState.wasReadFromDiff());
+                assertFalse(clusterState.wasReadFromDiff());
+                newStateProcessed.onNewClusterStateProcessed();
+            }
+        });
+
+        // Initial cluster state
+        DiscoveryNodes discoveryNodes = DiscoveryNodes.builder().put(nodeA.discoveryNode).localNodeId(nodeA.discoveryNode.id()).build();
+        ClusterState clusterState = ClusterState.builder(new ClusterName("test")).nodes(discoveryNodes).build();
+
+        // cluster state update - add nodeB
+        discoveryNodes = DiscoveryNodes.builder(discoveryNodes).put(nodeB.discoveryNode).build();
+        ClusterState previousClusterState = clusterState;
+        clusterState = ClusterState.builder(clusterState).nodes(discoveryNodes).incrementVersion().build();
+        publishStateDiffAndWait(nodeA.action, clusterState, previousClusterState);
+
+        // cluster state update - add block
+        previousClusterState = clusterState;
+        clusterState = ClusterState.builder(clusterState).blocks(ClusterBlocks.builder().addGlobalBlock(MetaData.CLUSTER_READ_ONLY_BLOCK)).incrementVersion().build();
+        publishStateDiffAndWait(nodeA.action, clusterState, previousClusterState);
+    }
+
+
+    @Test
+    @TestLogging("cluster:DEBUG,discovery.zen.publish:DEBUG")
+    public void testSimultaneousClusterStatePublishing() throws Exception {
+        int numberOfNodes = randomIntBetween(2, 10);
+        int numberOfIterations = randomIntBetween(50, 200);
+        Settings settings = Settings.builder().put(DiscoverySettings.PUBLISH_TIMEOUT, "100ms").put(DiscoverySettings.PUBLISH_DIFF_ENABLE, true).build();
+        MockNode[] nodes = new MockNode[numberOfNodes];
+        DiscoveryNodes.Builder discoveryNodesBuilder = DiscoveryNodes.builder();
+        for (int i = 0; i < nodes.length; i++) {
+            final String name = "node" + i;
+            nodes[i] = createMockNode(name, settings, Version.CURRENT, new PublishClusterStateAction.NewClusterStateListener() {
+                @Override
+                public synchronized void onNewClusterState(ClusterState clusterState, NewStateProcessed newStateProcessed) {
+                    assertProperMetaDataForVersion(clusterState.metaData(), clusterState.version());
+                    if (randomInt(10) < 2) {
+                        // Cause timeouts from time to time
+                        try {
+                            Thread.sleep(randomInt(110));
+                        } catch (InterruptedException ex) {
+                            Thread.currentThread().interrupt();
+                        }
+                    }
+                    newStateProcessed.onNewClusterStateProcessed();
+                }
+            });
+            discoveryNodesBuilder.put(nodes[i].discoveryNode);
+        }
+
+        AssertingAckListener[] listeners = new AssertingAckListener[numberOfIterations];
+        DiscoveryNodes discoveryNodes = discoveryNodesBuilder.build();
+        MetaData metaData = MetaData.EMPTY_META_DATA;
+        ClusterState clusterState = ClusterState.builder(new ClusterName("test")).metaData(metaData).build();
+        ClusterState previousState;
+        for (int i = 0; i < numberOfIterations; i++) {
+            previousState = clusterState;
+            metaData = buildMetaDataForVersion(metaData, i + 1);
+            clusterState = ClusterState.builder(clusterState).incrementVersion().metaData(metaData).nodes(discoveryNodes).build();
+            listeners[i] = publishStateDiff(nodes[0].action, clusterState, previousState);
+        }
+
+        for (int i = 0; i < numberOfIterations; i++) {
+            listeners[i].await(1, TimeUnit.SECONDS);
+        }
+    }
+
+    @Test
+    @TestLogging("cluster:DEBUG,discovery.zen.publish:DEBUG")
+    public void testSerializationFailureDuringDiffPublishing() throws Exception {
+
+        MockNode nodeA = createMockNode("nodeA", Settings.EMPTY, Version.CURRENT, new PublishClusterStateAction.NewClusterStateListener() {
+            @Override
+            public void onNewClusterState(ClusterState clusterState, NewStateProcessed newStateProcessed) {
+                fail("Shouldn't send cluster state to myself");
+            }
+        });
+
+        MockNewClusterStateListener mockListenerB = new MockNewClusterStateListener();
+        MockNode nodeB = createMockNode("nodeB", Settings.EMPTY, Version.CURRENT, mockListenerB);
+
+        // Initial cluster state with both states - the second node still shouldn't get diff even though it's present in the previous cluster state
+        DiscoveryNodes discoveryNodes = DiscoveryNodes.builder().put(nodeA.discoveryNode).put(nodeB.discoveryNode).localNodeId(nodeA.discoveryNode.id()).build();
+        ClusterState previousClusterState = ClusterState.builder(new ClusterName("test")).nodes(discoveryNodes).build();
+        ClusterState clusterState = ClusterState.builder(previousClusterState).incrementVersion().build();
+        mockListenerB.add(new NewClusterStateExpectation() {
+            @Override
+            public void check(ClusterState clusterState, PublishClusterStateAction.NewClusterStateListener.NewStateProcessed newStateProcessed) {
+                assertFalse(clusterState.wasReadFromDiff());
+            }
+        });
+        publishStateDiffAndWait(nodeA.action, clusterState, previousClusterState);
+
+        // cluster state update - add block
+        previousClusterState = clusterState;
+        clusterState = ClusterState.builder(clusterState).blocks(ClusterBlocks.builder().addGlobalBlock(MetaData.CLUSTER_READ_ONLY_BLOCK)).incrementVersion().build();
+        mockListenerB.add(new NewClusterStateExpectation() {
+            @Override
+            public void check(ClusterState clusterState, PublishClusterStateAction.NewClusterStateListener.NewStateProcessed newStateProcessed) {
+                assertTrue(clusterState.wasReadFromDiff());
+            }
+        });
+
+        ClusterState unserializableClusterState = new ClusterState(clusterState.version(), clusterState.stateUUID(), clusterState) {
+            @Override
+            public Diff<ClusterState> diff(ClusterState previousState) {
+                return new Diff<ClusterState>() {
+                    @Override
+                    public ClusterState apply(ClusterState part) {
+                        fail("this diff shouldn't be applied");
+                        return part;
+                    }
+
+                    @Override
+                    public void writeTo(StreamOutput out) throws IOException {
+                        throw new IOException("Simulated failure of diff serialization");
+                    }
+                };
+            }
+        };
+        List<Tuple<DiscoveryNode, Throwable>> errors = publishStateDiff(nodeA.action, unserializableClusterState, previousClusterState).awaitErrors(1, TimeUnit.SECONDS);
+        assertThat(errors.size(), equalTo(1));
+        assertThat(errors.get(0).v2().getMessage(), containsString("Simulated failure of diff serialization"));
+    }
+
+    private MetaData buildMetaDataForVersion(MetaData metaData, long version) {
+        ImmutableOpenMap.Builder<String, IndexMetaData> indices = ImmutableOpenMap.builder(metaData.indices());
+        indices.put("test" + version, IndexMetaData.builder("test" + version).settings(Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT))
+                .numberOfShards((int) version).numberOfReplicas(0).build());
+        return MetaData.builder(metaData)
+                .transientSettings(Settings.builder().put("test", version).build())
+                .indices(indices.build())
+                .build();
+    }
+
+    private void assertProperMetaDataForVersion(MetaData metaData, long version) {
+        for (long i = 1; i <= version; i++) {
+            assertThat(metaData.index("test" + i), notNullValue());
+            assertThat(metaData.index("test" + i).numberOfShards(), equalTo((int) i));
+        }
+        assertThat(metaData.index("test" + (version + 1)), nullValue());
+        assertThat(metaData.transientSettings().get("test"), equalTo(Long.toString(version)));
+    }
+
+    public void publishStateDiffAndWait(PublishClusterStateAction action, ClusterState state, ClusterState previousState) throws InterruptedException {
+        publishStateDiff(action, state, previousState).await(1, TimeUnit.SECONDS);
+    }
+
+    public AssertingAckListener publishStateDiff(PublishClusterStateAction action, ClusterState state, ClusterState previousState) throws InterruptedException {
+        AssertingAckListener assertingAckListener = new AssertingAckListener(state.nodes().getSize() - 1);
+        ClusterChangedEvent changedEvent = new ClusterChangedEvent("test update", state, previousState);
+        action.publish(changedEvent, assertingAckListener);
+        return assertingAckListener;
+    }
+
+    public static class AssertingAckListener implements Discovery.AckListener {
+        private final List<Tuple<DiscoveryNode, Throwable>> errors = new CopyOnWriteArrayList<>();
+        private final AtomicBoolean timeoutOccured = new AtomicBoolean();
+        private final CountDownLatch countDown;
+
+        public AssertingAckListener(int nodeCount) {
+            countDown = new CountDownLatch(nodeCount);
+        }
+
+        @Override
+        public void onNodeAck(DiscoveryNode node, @Nullable Throwable t) {
+            if (t != null) {
+                errors.add(new Tuple<>(node, t));
+            }
+            countDown.countDown();
+        }
+
+        @Override
+        public void onTimeout() {
+            timeoutOccured.set(true);
+            // Fast forward the counter - no reason to wait here
+            long currentCount = countDown.getCount();
+            for (long i = 0; i < currentCount; i++) {
+                countDown.countDown();
+            }
+        }
+
+        public void await(long timeout, TimeUnit unit) throws InterruptedException {
+            assertThat(awaitErrors(timeout, unit), emptyIterable());
+        }
+
+        public List<Tuple<DiscoveryNode, Throwable>> awaitErrors(long timeout, TimeUnit unit) throws InterruptedException {
+            countDown.await(timeout, unit);
+            assertFalse(timeoutOccured.get());
+            return errors;
+        }
+
+    }
+
+    public interface NewClusterStateExpectation {
+        void check(ClusterState clusterState, PublishClusterStateAction.NewClusterStateListener.NewStateProcessed newStateProcessed);
+    }
+
+    public static class MockNewClusterStateListener implements PublishClusterStateAction.NewClusterStateListener {
+        CopyOnWriteArrayList<NewClusterStateExpectation> expectations = new CopyOnWriteArrayList();
+
+        @Override
+        public void onNewClusterState(ClusterState clusterState, NewStateProcessed newStateProcessed) {
+            final NewClusterStateExpectation expectation;
+            try {
+                expectation = expectations.remove(0);
+            } catch (ArrayIndexOutOfBoundsException ex) {
+                fail("Unexpected cluster state update " + clusterState.prettyPrint());
+                return;
+            }
+            expectation.check(clusterState, newStateProcessed);
+            newStateProcessed.onNewClusterStateProcessed();
+        }
+
+        public void add(NewClusterStateExpectation expectation) {
+            expectations.add(expectation);
+        }
+    }
+
+    public static class DelegatingClusterState extends ClusterState {
+
+        public DelegatingClusterState(ClusterState clusterState) {
+            super(clusterState.version(), clusterState.stateUUID(), clusterState);
+        }
+
+
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/cluster/ClusterStateTests.java b/core/src/test/java/org/elasticsearch/cluster/ClusterStateTests.java
deleted file mode 100644
index 19f90f2..0000000
--- a/core/src/test/java/org/elasticsearch/cluster/ClusterStateTests.java
+++ /dev/null
@@ -1,54 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.cluster;
-
-import com.carrotsearch.randomizedtesting.annotations.Repeat;
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.common.transport.DummyTransportAddress;
-import org.elasticsearch.test.ESTestCase;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public class ClusterStateTests extends ESTestCase {
-
-    public void testSupersedes() {
-        final DiscoveryNode node1 = new DiscoveryNode("node1", DummyTransportAddress.INSTANCE, Version.CURRENT);
-        final DiscoveryNode node2 = new DiscoveryNode("node2", DummyTransportAddress.INSTANCE, Version.CURRENT);
-        final DiscoveryNodes nodes = DiscoveryNodes.builder().put(node1).put(node2).build();
-        ClusterState noMaster1 = ClusterState.builder(ClusterName.DEFAULT).version(randomInt(5)).nodes(nodes).build();
-        ClusterState noMaster2 = ClusterState.builder(ClusterName.DEFAULT).version(randomInt(5)).nodes(nodes).build();
-        ClusterState withMaster1a = ClusterState.builder(ClusterName.DEFAULT).version(randomInt(5)).nodes(DiscoveryNodes.builder(nodes).masterNodeId(node1.id())).build();
-        ClusterState withMaster1b = ClusterState.builder(ClusterName.DEFAULT).version(randomInt(5)).nodes(DiscoveryNodes.builder(nodes).masterNodeId(node1.id())).build();
-        ClusterState withMaster2 = ClusterState.builder(ClusterName.DEFAULT).version(randomInt(5)).nodes(DiscoveryNodes.builder(nodes).masterNodeId(node2.id())).build();
-
-        // states with no master should never supersede anything
-        assertFalse(noMaster1.supersedes(noMaster2));
-        assertFalse(noMaster1.supersedes(withMaster1a));
-
-        // states should never supersede states from another master
-        assertFalse(withMaster1a.supersedes(withMaster2));
-        assertFalse(withMaster1a.supersedes(noMaster1));
-
-        // state from the same master compare by version
-        assertThat(withMaster1a.supersedes(withMaster1b), equalTo(withMaster1a.version() > withMaster1b.version()));
-
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesIT.java b/core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesIT.java
index ab01ea1..fd4e994 100644
--- a/core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesIT.java
@@ -21,47 +21,34 @@ package org.elasticsearch.cluster;
 
 import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
 import org.elasticsearch.client.Client;
-import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.common.Priority;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.discovery.Discovery;
 import org.elasticsearch.discovery.DiscoverySettings;
-import org.elasticsearch.discovery.zen.ZenDiscovery;
 import org.elasticsearch.discovery.zen.elect.ElectMasterService;
-import org.elasticsearch.discovery.zen.fd.FaultDetection;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
-import org.elasticsearch.test.disruption.NetworkDelaysPartition;
 import org.elasticsearch.test.junit.annotations.TestLogging;
-import org.elasticsearch.test.transport.MockTransportService;
 import org.junit.Test;
 
-import java.util.*;
-import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.TimeUnit;
-import java.util.concurrent.atomic.AtomicReference;
 import java.util.function.Predicate;
 
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.test.ESIntegTestCase.Scope;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
-import static org.hamcrest.Matchers.*;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
+import static org.hamcrest.Matchers.empty;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.isOneOf;
+import static org.hamcrest.Matchers.not;
 
 @ClusterScope(scope = Scope.TEST, numDataNodes = 0)
 @ESIntegTestCase.SuppressLocalMode
 public class MinimumMasterNodesIT extends ESIntegTestCase {
 
-    @Override
-    protected Collection<Class<? extends Plugin>> nodePlugins() {
-        final HashSet<Class<? extends Plugin>> classes = new HashSet<>(super.nodePlugins());
-        classes.add(MockTransportService.TestPlugin.class);
-        return classes;
-    }
-
     @Test
     @TestLogging("cluster.service:TRACE,discovery.zen:TRACE,gateway:TRACE,transport.tracer:TRACE")
     public void simpleMinimumMasterNodes() throws Exception {
@@ -350,69 +337,4 @@ public class MinimumMasterNodesIT extends ESIntegTestCase {
         logger.info("--> verifying no node left and master is up");
         assertFalse(client().admin().cluster().prepareHealth().setWaitForNodes(Integer.toString(nodeCount)).get().isTimedOut());
     }
-
-    public void testCanNotPublishWithoutMinMastNodes() throws Exception {
-        Settings settings = settingsBuilder()
-                .put("discovery.type", "zen")
-                .put(FaultDetection.SETTING_PING_TIMEOUT, "1h") // disable it
-                .put(ZenDiscovery.SETTING_PING_TIMEOUT, "200ms")
-                .put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES, 2)
-                .put(DiscoverySettings.COMMIT_TIMEOUT, "100ms") // speed things up
-                .build();
-        internalCluster().startNodesAsync(3, settings).get();
-
-        final String master = internalCluster().getMasterName();
-        Set<String> otherNodes = new HashSet<>(Arrays.asList(internalCluster().getNodeNames()));
-        otherNodes.remove(master);
-        NetworkDelaysPartition partition = new NetworkDelaysPartition(Collections.singleton(master), otherNodes, 60000, random());
-        internalCluster().setDisruptionScheme(partition);
-        partition.startDisrupting();
-
-        final CountDownLatch latch = new CountDownLatch(1);
-        final AtomicReference<Throwable> failure = new AtomicReference<>();
-        logger.debug("--> submitting for cluster state to be rejected");
-        final ClusterService masterClusterService = internalCluster().clusterService(master);
-        masterClusterService.submitStateUpdateTask("test", new ProcessedClusterStateUpdateTask() {
-            @Override
-            public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {
-                latch.countDown();
-            }
-
-            @Override
-            public ClusterState execute(ClusterState currentState) throws Exception {
-                MetaData.Builder metaData = MetaData.builder(currentState.metaData()).persistentSettings(
-                        Settings.builder().put(currentState.metaData().persistentSettings()).put("_SHOULD_NOT_BE_THERE_", true).build()
-                );
-                return ClusterState.builder(currentState).metaData(metaData).build();
-            }
-
-            @Override
-            public void onFailure(String source, Throwable t) {
-                failure.set(t);
-                latch.countDown();
-            }
-        });
-
-        logger.debug("--> waiting for cluster state to be processed/rejected");
-        latch.await();
-
-        assertThat(failure.get(), instanceOf(Discovery.FailedToCommitClusterStateException.class));
-        assertBusy(new Runnable() {
-            @Override
-            public void run() {
-                assertThat(masterClusterService.state().nodes().masterNode(), nullValue());
-            }
-        });
-
-        partition.stopDisrupting();
-
-        logger.debug("--> waiting for cluster to heal");
-        assertNoTimeout(client().admin().cluster().prepareHealth().setWaitForNodes("3").setWaitForEvents(Priority.LANGUID));
-
-        for (String node : internalCluster().getNodeNames()) {
-            Settings nodeSetting = internalCluster().clusterService(node).state().metaData().settings();
-            assertThat(node + " processed the cluster state despite of a min master node violation", nodeSetting.get("_SHOULD_NOT_BE_THERE_"), nullValue());
-        }
-
-    }
 }
diff --git a/core/src/test/java/org/elasticsearch/cluster/ack/AckIT.java b/core/src/test/java/org/elasticsearch/cluster/ack/AckIT.java
index dd48217..610e0c5 100644
--- a/core/src/test/java/org/elasticsearch/cluster/ack/AckIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/ack/AckIT.java
@@ -49,11 +49,15 @@ import org.junit.Test;
 import java.util.List;
 import java.util.concurrent.TimeUnit;
 
-import static org.elasticsearch.cluster.metadata.IndexMetaData.*;
+import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
+import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
+import static org.elasticsearch.cluster.metadata.IndexMetaData.State;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.test.ESIntegTestCase.ClusterScope;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.hamcrest.Matchers.*;
+import static org.hamcrest.Matchers.anyOf;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.notNullValue;
 
 @ClusterScope(minNumDataNodes = 2)
 public class AckIT extends ESIntegTestCase {
@@ -71,7 +75,7 @@ public class AckIT extends ESIntegTestCase {
         createIndex("test");
 
         assertAcked(client().admin().indices().prepareUpdateSettings("test")
-                .setSettings(Settings.builder().put("refresh_interval", 9999, TimeUnit.MILLISECONDS)));
+                    .setSettings(Settings.builder().put("refresh_interval", 9999, TimeUnit.MILLISECONDS)));
 
         for (Client client : clients()) {
             String refreshInterval = getLocalClusterState(client).metaData().index("test").settings().get("index.refresh_interval");
diff --git a/core/src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteIT.java b/core/src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteIT.java
index 0eecc58..2df7958 100644
--- a/core/src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteIT.java
@@ -32,7 +32,7 @@ import org.elasticsearch.cluster.routing.allocation.RoutingExplanations;
 import org.elasticsearch.cluster.routing.allocation.command.AllocateAllocationCommand;
 import org.elasticsearch.cluster.routing.allocation.command.MoveAllocationCommand;
 import org.elasticsearch.cluster.routing.allocation.decider.Decision;
-import org.elasticsearch.cluster.routing.allocation.decider.DisableAllocationDecider;
+import org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider;
 import org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider.Allocation;
 import org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider;
 import org.elasticsearch.common.Priority;
@@ -71,8 +71,8 @@ public class ClusterRerouteIT extends ESIntegTestCase {
     @Test
     public void rerouteWithCommands_disableAllocationSettings() throws Exception {
         Settings commonSettings = settingsBuilder()
-                .put(DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_NEW_ALLOCATION, true)
-                .put(DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_ALLOCATION, true)
+                .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, "none")
+                .put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE, "none")
                 .build();
         rerouteWithCommands(commonSettings);
     }
@@ -149,8 +149,8 @@ public class ClusterRerouteIT extends ESIntegTestCase {
     @Test
     public void rerouteWithAllocateLocalGateway_disableAllocationSettings() throws Exception {
         Settings commonSettings = settingsBuilder()
-                .put(DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_NEW_ALLOCATION, true)
-                .put(DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_ALLOCATION, true)
+                .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, "none")
+                .put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE, "none")
                 .build();
         rerouteWithAllocateLocalGateway(commonSettings);
     }
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingServiceTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingServiceTests.java
index 960ccc5..c2ba1cb 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingServiceTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingServiceTests.java
@@ -36,6 +36,9 @@ import org.junit.After;
 import org.junit.Before;
 import org.junit.Test;
 
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.List;
 import java.util.concurrent.atomic.AtomicBoolean;
 
 import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
@@ -111,23 +114,32 @@ public class RoutingServiceTests extends ESAllocationTestCase {
         // starting replicas
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING))).build();
         assertFalse("no shards should be unassigned", clusterState.getRoutingNodes().hasUnassigned());
+        String nodeId = null;
+        final List<ShardRouting> allShards = clusterState.getRoutingNodes().routingTable().allShards("test");
+        // we need to find the node with the replica otherwise we will not reroute
+        for (ShardRouting shardRouting : allShards) {
+            if (shardRouting.primary() == false) {
+                nodeId = shardRouting.currentNodeId();
+                break;
+            }
+        }
+        assertNotNull(nodeId);
         // remove node2 and reroute
+
         ClusterState prevState = clusterState;
-        clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes()).remove("node2")).build();
+        clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes()).remove(nodeId)).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
         // We need to update the routing service's last attempted run to
         // signal that the GatewayAllocator tried to allocated it but
         // it was delayed
-        routingService.setUnassignedShardsAllocatedTimestamp(System.currentTimeMillis());
-        ClusterState newState = clusterState;
+        RoutingNodes.UnassignedShards unassigned = clusterState.getRoutingNodes().unassigned();
+        assertEquals(1, unassigned.size());
+        ShardRouting next = unassigned.iterator().next();
+        routingService.setUnassignedShardsAllocatedTimestamp(next.unassignedInfo().getTimestampInMillis() + randomIntBetween(0, 99));
 
+        ClusterState newState = clusterState;
         routingService.clusterChanged(new ClusterChangedEvent("test", newState, prevState));
-        assertBusy(new Runnable() {
-            @Override
-            public void run() {
-                assertTrue("routing service should have run a reroute", routingService.hasReroutedAndClear());
-            }
-        });
+        assertBusy(() -> assertTrue("routing service should have run a reroute", routingService.hasReroutedAndClear()));
         // verify the registration has been reset
         assertThat(routingService.getRegisteredNextDelaySetting(), equalTo(Long.MAX_VALUE));
     }
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java
index 21e4d29..a983d88 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java
@@ -31,7 +31,7 @@ import org.elasticsearch.cluster.routing.allocation.command.AllocateAllocationCo
 import org.elasticsearch.cluster.routing.allocation.command.AllocationCommands;
 import org.elasticsearch.cluster.routing.allocation.command.CancelAllocationCommand;
 import org.elasticsearch.cluster.routing.allocation.command.MoveAllocationCommand;
-import org.elasticsearch.cluster.routing.allocation.decider.DisableAllocationDecider;
+import org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.logging.ESLogger;
@@ -100,8 +100,8 @@ public class AllocationCommandsTests extends ESAllocationTestCase {
     @Test
     public void allocateCommand() {
         AllocationService allocation = createAllocationService(settingsBuilder()
-                .put(DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_NEW_ALLOCATION, true)
-                .put(DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_ALLOCATION, true)
+                .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, "none")
+                .put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE, "none")
                 .build());
 
         logger.info("--> building initial routing table");
@@ -189,8 +189,8 @@ public class AllocationCommandsTests extends ESAllocationTestCase {
     @Test
     public void cancelCommand() {
         AllocationService allocation = createAllocationService(settingsBuilder()
-                .put(DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_NEW_ALLOCATION, true)
-                .put(DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_ALLOCATION, true)
+                .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, "none")
+                .put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE, "none")
                 .build());
 
         logger.info("--> building initial routing table");
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java
index 7498905..7f050f3 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java
@@ -28,9 +28,13 @@ import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.cluster.routing.RoutingTable;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.ShardRoutingState;
+import org.elasticsearch.cluster.routing.allocation.command.AllocationCommands;
+import org.elasticsearch.cluster.routing.allocation.command.CancelAllocationCommand;
+import org.elasticsearch.cluster.routing.allocation.command.MoveAllocationCommand;
 import org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
+import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.test.ESAllocationTestCase;
 import org.junit.Test;
 
@@ -853,6 +857,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
                         .put(newNode("A-1", ImmutableMap.of("zone", "a")))
                         .put(newNode("A-2", ImmutableMap.of("zone", "a")))
                         .put(newNode("A-3", ImmutableMap.of("zone", "a")))
+                        .put(newNode("A-4", ImmutableMap.of("zone", "a")))
                         .put(newNode("B-0", ImmutableMap.of("zone", "b")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
@@ -866,5 +871,25 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(1));
         assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(3));
         assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).size(), equalTo(1)); // Unassigned shard is expected.
+
+        // Cancel all initializing shards and move started primary to another node.
+        AllocationCommands commands = new AllocationCommands();
+        String primaryNode = null;
+        for (ShardRouting routing : routingTable.allShards()) {
+            if (routing.primary()) {
+                primaryNode = routing.currentNodeId();
+            } else if (routing.initializing()) {
+                commands.add(new CancelAllocationCommand(routing.shardId(), routing.currentNodeId(), false));
+            }
+        }
+        commands.add(new MoveAllocationCommand(new ShardId("test", 0), primaryNode, "A-4"));
+
+        routingTable = strategy.reroute(clusterState, commands).routingTable();
+        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
+
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(RELOCATING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(4)); // +1 for relocating shard.
+        assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).size(), equalTo(1)); // Still 1 unassigned.
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/DisableAllocationTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/DisableAllocationTests.java
deleted file mode 100644
index b074e97..0000000
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/DisableAllocationTests.java
+++ /dev/null
@@ -1,146 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cluster.routing.allocation;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.cluster.routing.RoutingTable;
-import org.elasticsearch.cluster.routing.allocation.decider.DisableAllocationDecider;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.Loggers;
-import org.elasticsearch.test.ESAllocationTestCase;
-import org.junit.Test;
-
-import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
-import static org.elasticsearch.cluster.routing.ShardRoutingState.STARTED;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.hamcrest.Matchers.equalTo;
-
-/**
- */
-public class DisableAllocationTests extends ESAllocationTestCase {
-
-    private final ESLogger logger = Loggers.getLogger(DisableAllocationTests.class);
-
-    @Test
-    public void testClusterDisableAllocation() {
-        AllocationService strategy = createAllocationService(settingsBuilder()
-                .put(DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_NEW_ALLOCATION, true)
-                .put(DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_ALLOCATION, true)
-                .build());
-
-        logger.info("Building initial routing table");
-
-        MetaData metaData = MetaData.builder()
-                .put(IndexMetaData.builder("test").settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(1))
-                .build();
-
-        RoutingTable routingTable = RoutingTable.builder()
-                .addAsNew(metaData.index("test"))
-                .build();
-
-        ClusterState clusterState = ClusterState.builder(org.elasticsearch.cluster.ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable).build();
-
-        logger.info("--> adding two nodes and do rerouting");
-        clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1"))
-                .put(newNode("node2"))
-        ).build();
-        routingTable = strategy.reroute(clusterState).routingTable();
-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
-
-    }
-
-    @Test
-    public void testClusterDisableReplicaAllocation() {
-        AllocationService strategy = createAllocationService(settingsBuilder()
-                .put("cluster.routing.allocation.disable_replica_allocation", true)
-                .build());
-
-        logger.info("Building initial routing table");
-
-        MetaData metaData = MetaData.builder()
-                .put(IndexMetaData.builder("test").settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(1))
-                .build();
-
-        RoutingTable routingTable = RoutingTable.builder()
-                .addAsNew(metaData.index("test"))
-                .build();
-
-        ClusterState clusterState = ClusterState.builder(org.elasticsearch.cluster.ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable).build();
-
-        logger.info("--> adding two nodes do rerouting");
-        clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1"))
-                .put(newNode("node2"))
-        ).build();
-        routingTable = strategy.reroute(clusterState).routingTable();
-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
-
-        logger.info("--> start the shards (primaries)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-
-        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
-    }
-
-    @Test
-    public void testIndexDisableAllocation() {
-        AllocationService strategy = createAllocationService(settingsBuilder()
-                .build());
-
-        MetaData metaData = MetaData.builder()
-                .put(IndexMetaData.builder("disabled").settings(settings(Version.CURRENT).put(DisableAllocationDecider.INDEX_ROUTING_ALLOCATION_DISABLE_ALLOCATION, true).put(DisableAllocationDecider.INDEX_ROUTING_ALLOCATION_DISABLE_NEW_ALLOCATION, true)).numberOfShards(1).numberOfReplicas(1))
-                .put(IndexMetaData.builder("enabled").settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(1))
-                .build();
-
-        RoutingTable routingTable = RoutingTable.builder()
-                .addAsNew(metaData.index("disabled"))
-                .addAsNew(metaData.index("enabled"))
-                .build();
-
-        ClusterState clusterState = ClusterState.builder(org.elasticsearch.cluster.ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable).build();
-
-        logger.info("--> adding two nodes and do rerouting");
-        clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1"))
-                .put(newNode("node2"))
-        ).build();
-        routingTable = strategy.reroute(clusterState).routingTable();
-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
-        logger.info("--> start the shards (primaries)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        logger.info("--> start the shards (replicas)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
-        clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-
-        logger.info("--> verify only enabled index has been routed");
-        assertThat(clusterState.getRoutingNodes().shardsWithState("enabled", STARTED).size(), equalTo(2));
-        assertThat(clusterState.getRoutingNodes().shardsWithState("disabled", STARTED).size(), equalTo(0));
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/cluster/settings/ClusterSettingsIT.java b/core/src/test/java/org/elasticsearch/cluster/settings/ClusterSettingsIT.java
index 6112fe0..6b99852 100644
--- a/core/src/test/java/org/elasticsearch/cluster/settings/ClusterSettingsIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/settings/ClusterSettingsIT.java
@@ -24,7 +24,7 @@ import org.elasticsearch.action.admin.cluster.settings.ClusterUpdateSettingsResp
 import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.cluster.routing.allocation.decider.DisableAllocationDecider;
+import org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.ByteSizeUnit;
 import org.elasticsearch.common.util.concurrent.EsExecutors;
@@ -63,8 +63,8 @@ public class ClusterSettingsIT extends ESIntegTestCase {
         String key1 = IndicesStore.INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC;
         int value1 = 10;
 
-        String key2 = DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_ALLOCATION;
-        boolean value2 = true;
+        String key2 = EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE;
+        boolean value2 = false;
 
         Settings transientSettings1 = Settings.builder().put(key1, value1, ByteSizeUnit.BYTES).build();
         Settings persistentSettings1 = Settings.builder().put(key2, value2).build();
diff --git a/core/src/test/java/org/elasticsearch/common/blobstore/BlobStoreTests.java b/core/src/test/java/org/elasticsearch/common/blobstore/BlobStoreTests.java
index b5447dc..fdf07cb 100644
--- a/core/src/test/java/org/elasticsearch/common/blobstore/BlobStoreTests.java
+++ b/core/src/test/java/org/elasticsearch/common/blobstore/BlobStoreTests.java
@@ -22,6 +22,7 @@ import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.BytesRefBuilder;
 import org.apache.lucene.util.LuceneTestCase;
 import org.elasticsearch.common.blobstore.fs.FsBlobStore;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.ByteSizeUnit;
 import org.elasticsearch.common.unit.ByteSizeValue;
@@ -30,7 +31,6 @@ import org.junit.Test;
 
 import java.io.IOException;
 import java.io.InputStream;
-import java.io.OutputStream;
 import java.nio.file.Path;
 import java.util.Arrays;
 import java.util.HashMap;
@@ -47,10 +47,8 @@ public class BlobStoreTests extends ESTestCase {
         final BlobStore store = newBlobStore();
         final BlobContainer container = store.blobContainer(new BlobPath());
         byte[] data = randomBytes(randomIntBetween(10, scaledRandomIntBetween(1024, 1 << 16)));
-        try (OutputStream stream = container.createOutput("foobar")) {
-            stream.write(data);
-        }
-        try (InputStream stream = container.openInput("foobar")) {
+        container.writeBlob("foobar", new BytesArray(data));
+        try (InputStream stream = container.readBlob("foobar")) {
             BytesRefBuilder target = new BytesRefBuilder();
             while (target.length() < data.length) {
                 byte[] buffer = new byte[scaledRandomIntBetween(1, data.length - target.length())];
@@ -115,15 +113,13 @@ public class BlobStoreTests extends ESTestCase {
 
     protected byte[] createRandomBlob(BlobContainer container, String name, int length) throws IOException {
         byte[] data = randomBytes(length);
-        try (OutputStream stream = container.createOutput(name)) {
-            stream.write(data);
-        }
+        container.writeBlob(name, new BytesArray(data));
         return data;
     }
 
     protected byte[] readBlobFully(BlobContainer container, String name, int length) throws IOException {
         byte[] data = new byte[length];
-        try (InputStream inputStream = container.openInput(name)) {
+        try (InputStream inputStream = container.readBlob(name)) {
             assertThat(inputStream.read(data), equalTo(length));
             assertThat(inputStream.read(), equalTo(-1));
         }
diff --git a/core/src/test/java/org/elasticsearch/common/geo/GeoHashTests.java b/core/src/test/java/org/elasticsearch/common/geo/GeoHashTests.java
index c38deb1..063fd76 100644
--- a/core/src/test/java/org/elasticsearch/common/geo/GeoHashTests.java
+++ b/core/src/test/java/org/elasticsearch/common/geo/GeoHashTests.java
@@ -19,19 +19,19 @@
 package org.elasticsearch.common.geo;
 
 import org.elasticsearch.test.ESTestCase;
+import org.apache.lucene.util.XGeoHashUtils;
 import org.junit.Test;
 
 
 
 /**
- * Tests for {@link GeoHashUtils}
+ * Tests for {@link org.apache.lucene.util.XGeoHashUtils}
  */
 public class GeoHashTests extends ESTestCase {
-
-
     @Test
     public void testGeohashAsLongRoutines()  {
-        
+        final GeoPoint expected = new GeoPoint();
+        final GeoPoint actual = new GeoPoint();
         //Ensure that for all points at all supported levels of precision
         // that the long encoding of a geohash is compatible with its 
         // String based counterpart
@@ -41,19 +41,25 @@ public class GeoHashTests extends ESTestCase {
             {
                 for(int p=1;p<=12;p++)
                 {
-                    long geoAsLong = GeoHashUtils.encodeAsLong(lat,lng,p);
-                    String geohash = GeoHashUtils.encode(lat,lng,p);
-                    
-                    String geohashFromLong=GeoHashUtils.toString(geoAsLong);
+                    long geoAsLong = XGeoHashUtils.longEncode(lng, lat, p);
+
+                    // string encode from geohashlong encoded location
+                    String geohashFromLong = XGeoHashUtils.stringEncode(geoAsLong);
+
+                    // string encode from full res lat lon
+                    String geohash = XGeoHashUtils.stringEncode(lng, lat, p);
+
+                    // ensure both strings are the same
                     assertEquals(geohash, geohashFromLong);
-                    GeoPoint pos=GeoHashUtils.decode(geohash);
-                    GeoPoint pos2=GeoHashUtils.decode(geoAsLong);
-                    assertEquals(pos, pos2);
+
+                    // decode from the full-res geohash string
+                    expected.resetFromGeoHash(geohash);
+                    // decode from the geohash encoded long
+                    actual.resetFromGeoHash(geoAsLong);
+
+                    assertEquals(expected, actual);
                 }
             }
-            
-        }        
+        }
     }
-
-
-}
+}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/common/io/PathUtilsForTesting.java b/core/src/test/java/org/elasticsearch/common/io/PathUtilsForTesting.java
new file mode 100644
index 0000000..25c092c
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/io/PathUtilsForTesting.java
@@ -0,0 +1,41 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.io;
+
+import org.apache.lucene.util.LuceneTestCase;
+
+import java.nio.file.FileSystem;
+
+/** 
+ * Exposes some package private stuff in PathUtils for framework purposes only! 
+ */
+public class PathUtilsForTesting {
+    
+    /** Sets a new default filesystem for testing */
+    public static void setup() {
+        FileSystem mock = LuceneTestCase.getBaseTempDirForTestClass().getFileSystem();
+        PathUtils.DEFAULT = mock;
+    }
+    
+    /** Resets filesystem back to the real system default */
+    public static void teardown() {
+        PathUtils.DEFAULT = PathUtils.ACTUAL_DEFAULT;
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java b/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
index f558680..c1a6f75 100644
--- a/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
+++ b/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
@@ -30,7 +30,11 @@ import org.elasticsearch.action.get.GetResponse;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.index.IndexResponse;
 import org.elasticsearch.client.Client;
-import org.elasticsearch.cluster.*;
+import org.elasticsearch.cluster.ClusterChangedEvent;
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.ClusterStateListener;
+import org.elasticsearch.cluster.ClusterStateUpdateTask;
 import org.elasticsearch.cluster.action.shard.ShardStateAction;
 import org.elasticsearch.cluster.block.ClusterBlock;
 import org.elasticsearch.cluster.block.ClusterBlockLevel;
@@ -63,7 +67,16 @@ import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.InternalTestCluster;
 import org.elasticsearch.test.discovery.ClusterDiscoveryConfiguration;
-import org.elasticsearch.test.disruption.*;
+import org.elasticsearch.test.disruption.BlockClusterStateProcessing;
+import org.elasticsearch.test.disruption.IntermittentLongGCDisruption;
+import org.elasticsearch.test.disruption.LongGCDisruption;
+import org.elasticsearch.test.disruption.NetworkDelaysPartition;
+import org.elasticsearch.test.disruption.NetworkDisconnectPartition;
+import org.elasticsearch.test.disruption.NetworkPartition;
+import org.elasticsearch.test.disruption.NetworkUnresponsivePartition;
+import org.elasticsearch.test.disruption.ServiceDisruptionScheme;
+import org.elasticsearch.test.disruption.SingleNodeDisruption;
+import org.elasticsearch.test.disruption.SlowClusterStateProcessing;
 import org.elasticsearch.test.junit.annotations.TestLogging;
 import org.elasticsearch.test.transport.MockTransportService;
 import org.elasticsearch.transport.TransportException;
@@ -74,8 +87,22 @@ import org.junit.Before;
 import org.junit.Test;
 
 import java.io.IOException;
-import java.util.*;
-import java.util.concurrent.*;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+import java.util.Set;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.Future;
+import java.util.concurrent.Semaphore;
+import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicReference;
@@ -84,7 +111,10 @@ import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.test.ESIntegTestCase.ClusterScope;
 import static org.elasticsearch.test.ESIntegTestCase.Scope;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.hamcrest.Matchers.*;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.is;
+import static org.hamcrest.Matchers.not;
+import static org.hamcrest.Matchers.nullValue;
 
 @ClusterScope(scope = Scope.TEST, numDataNodes = 0, transportClientRatio = 0)
 @ESIntegTestCase.SuppressLocalMode
@@ -755,7 +785,7 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
      */
     @Test
     public void unicastSinglePingResponseContainsMaster() throws Exception {
-        List<String> nodes = startCluster(4, -1, new int[]{0});
+        List<String> nodes = startCluster(4, -1, new int[] {0});
         // Figure out what is the elected master node
         final String masterNode = internalCluster().getMasterName();
         logger.info("---> legit elected master node=" + masterNode);
@@ -851,11 +881,7 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
 
         logger.info("blocking cluster state publishing from master [{}] to non master [{}]", masterNode, nonMasterNode);
         MockTransportService masterTransportService = (MockTransportService) internalCluster().getInstance(TransportService.class, masterNode);
-        if (randomBoolean()) {
-            masterTransportService.addFailToSendNoConnectRule(discoveryNodes.localNode(), PublishClusterStateAction.SEND_ACTION_NAME);
-        } else {
-            masterTransportService.addFailToSendNoConnectRule(discoveryNodes.localNode(), PublishClusterStateAction.COMMIT_ACTION_NAME);
-        }
+        masterTransportService.addFailToSendNoConnectRule(discoveryNodes.localNode(), PublishClusterStateAction.ACTION_NAME);
 
         logger.info("allowing requests from non master [{}] to master [{}], waiting for two join request", nonMasterNode, masterNode);
         final CountDownLatch countDownLatch = new CountDownLatch(2);
@@ -876,10 +902,6 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
         nonMasterTransportService.clearRule(discoveryNodes.masterNode());
 
         ensureStableCluster(2);
-
-        // shutting down the nodes, to avoid the leakage check tripping
-        // on the states associated with the commit requests we may have dropped
-        internalCluster().stopRandomNonMasterNode();
     }
 
 
@@ -996,9 +1018,9 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
         try {
             configureUnicastCluster(5, null, 1);
             // we could probably write a test without a dedicated master node but it is easier if we use one
-            Future<String> masterNodeFuture = internalCluster().startMasterOnlyNodeAsync();
+            InternalTestCluster.Async<String> masterNodeFuture = internalCluster().startMasterOnlyNodeAsync();
             // node_1 will have the shard in the beginning
-            Future<String> node1Future = internalCluster().startDataOnlyNodeAsync();
+            InternalTestCluster.Async<String> node1Future = internalCluster().startDataOnlyNodeAsync();
             final String masterNode = masterNodeFuture.get();
             final String node_1 = node1Future.get();
             logger.info("--> creating index [test] with one shard and zero replica");
@@ -1062,7 +1084,7 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
             final Client node1Client = internalCluster().client(node_1);
             final Client node4Client = internalCluster().client(node_4);
             logger.info("--> index doc");
-            logLocalClusterStates(node1Client, node2Client, node3Client, node4Client);
+            logLocalClusterStates(node1Client, node2Client, node3Client,  node4Client);
             assertTrue(node3Client.prepareIndex("test", "doc").setSource("{\"text\":\"a\"}").get().isCreated());
             //sometimes refresh and sometimes flush
             int refreshOrFlushType = randomIntBetween(1, 2);
@@ -1178,8 +1200,8 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
     @Test
     public void searchWithRelocationAndSlowClusterStateProcessing() throws Exception {
         configureUnicastCluster(3, null, 1);
-        Future<String> masterNodeFuture = internalCluster().startMasterOnlyNodeAsync();
-        Future<String> node_1Future = internalCluster().startDataOnlyNodeAsync();
+        InternalTestCluster.Async<String> masterNodeFuture = internalCluster().startMasterOnlyNodeAsync();
+        InternalTestCluster.Async<String> node_1Future = internalCluster().startDataOnlyNodeAsync();
 
         final String node_1 = node_1Future.get();
         final String masterNode = masterNodeFuture.get();
@@ -1191,7 +1213,7 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
         );
         ensureGreen("test");
 
-        Future<String> node_2Future = internalCluster().startDataOnlyNodeAsync();
+        InternalTestCluster.Async<String> node_2Future = internalCluster().startDataOnlyNodeAsync();
         final String node_2 = node_2Future.get();
         List<IndexRequestBuilder> indexRequestBuilderList = new ArrayList<>();
         for (int i = 0; i < 100; i++) {
@@ -1243,8 +1265,8 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
     @Test
     public void testIndicesDeleted() throws Exception {
         configureUnicastCluster(3, null, 2);
-        Future<List<String>> masterNodes = internalCluster().startMasterOnlyNodesAsync(2);
-        Future<String> dataNode = internalCluster().startDataOnlyNodeAsync();
+        InternalTestCluster.Async<List<String>> masterNodes= internalCluster().startMasterOnlyNodesAsync(2);
+        InternalTestCluster.Async<String> dataNode = internalCluster().startDataOnlyNodeAsync();
         dataNode.get();
         masterNodes.get();
         ensureStableCluster(3);
diff --git a/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryIT.java b/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryIT.java
index fbe6baa..5520436 100644
--- a/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryIT.java
+++ b/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryIT.java
@@ -209,7 +209,7 @@ public class ZenDiscoveryIT extends ESIntegTestCase {
 
         final CountDownLatch latch = new CountDownLatch(1);
         final AtomicReference<Exception> reference = new AtomicReference<>();
-        internalCluster().getInstance(TransportService.class, noneMasterNode).sendRequest(node, PublishClusterStateAction.SEND_ACTION_NAME, new BytesTransportRequest(bytes, Version.CURRENT), new EmptyTransportResponseHandler(ThreadPool.Names.SAME) {
+        internalCluster().getInstance(TransportService.class, noneMasterNode).sendRequest(node, PublishClusterStateAction.ACTION_NAME, new BytesTransportRequest(bytes, Version.CURRENT), new EmptyTransportResponseHandler(ThreadPool.Names.SAME) {
 
             @Override
             public void handleResponse(TransportResponse.Empty response) {
diff --git a/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryUnitTests.java b/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryUnitTests.java
index e1d6bee..6bd2bd7 100644
--- a/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryUnitTests.java
+++ b/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryUnitTests.java
@@ -19,16 +19,19 @@
 
 package org.elasticsearch.discovery.zen;
 
-import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.common.transport.DummyTransportAddress;
 import org.elasticsearch.test.ESTestCase;
 
+import java.util.Collections;
+import java.util.LinkedList;
+import java.util.Queue;
+
+import static org.elasticsearch.discovery.zen.ZenDiscovery.ProcessClusterState;
 import static org.elasticsearch.discovery.zen.ZenDiscovery.shouldIgnoreOrRejectNewClusterState;
-import static org.hamcrest.Matchers.containsString;
+import static org.hamcrest.Matchers.*;
+import static org.hamcrest.core.IsNull.nullValue;
 
 /**
  */
@@ -38,9 +41,9 @@ public class ZenDiscoveryUnitTests extends ESTestCase {
         ClusterName clusterName = new ClusterName("abc");
 
         DiscoveryNodes.Builder currentNodes = DiscoveryNodes.builder();
-        currentNodes.masterNodeId("a").put(new DiscoveryNode("a", DummyTransportAddress.INSTANCE, Version.CURRENT));
+        currentNodes.masterNodeId("a");
         DiscoveryNodes.Builder newNodes = DiscoveryNodes.builder();
-        newNodes.masterNodeId("a").put(new DiscoveryNode("a", DummyTransportAddress.INSTANCE, Version.CURRENT));
+        newNodes.masterNodeId("a");
 
         ClusterState.Builder currentState = ClusterState.builder(clusterName);
         currentState.nodes(currentNodes);
@@ -58,8 +61,7 @@ public class ZenDiscoveryUnitTests extends ESTestCase {
         assertFalse("should not ignore, because new state's version is higher to current state's version", shouldIgnoreOrRejectNewClusterState(logger, currentState.build(), newState.build()));
 
         currentNodes = DiscoveryNodes.builder();
-        currentNodes.masterNodeId("b").put(new DiscoveryNode("b", DummyTransportAddress.INSTANCE, Version.CURRENT));
-        ;
+        currentNodes.masterNodeId("b");
         // version isn't taken into account, so randomize it to ensure this.
         if (randomBoolean()) {
             currentState.version(2);
@@ -89,4 +91,53 @@ public class ZenDiscoveryUnitTests extends ESTestCase {
         }
         assertFalse("should not ignore, because current state doesn't have a master", shouldIgnoreOrRejectNewClusterState(logger, currentState.build(), newState.build()));
     }
+
+    public void testSelectNextStateToProcess_empty() {
+        Queue<ProcessClusterState> queue = new LinkedList<>();
+        assertThat(ZenDiscovery.selectNextStateToProcess(queue), nullValue());
+    }
+
+    public void testSelectNextStateToProcess() {
+        ClusterName clusterName = new ClusterName("abc");
+        DiscoveryNodes nodes = DiscoveryNodes.builder().masterNodeId("a").build();
+
+        int numUpdates = scaledRandomIntBetween(50, 100);
+        LinkedList<ProcessClusterState> queue = new LinkedList<>();
+        for (int i = 0; i < numUpdates; i++) {
+            queue.add(new ProcessClusterState(ClusterState.builder(clusterName).version(i).nodes(nodes).build()));
+        }
+        ProcessClusterState mostRecent = queue.get(numUpdates - 1);
+        Collections.shuffle(queue, getRandom());
+
+        assertThat(ZenDiscovery.selectNextStateToProcess(queue), sameInstance(mostRecent.clusterState));
+        assertThat(mostRecent.processed, is(true));
+        assertThat(queue.size(), equalTo(0));
+    }
+
+    public void testSelectNextStateToProcess_differentMasters() {
+        ClusterName clusterName = new ClusterName("abc");
+        DiscoveryNodes nodes1 = DiscoveryNodes.builder().masterNodeId("a").build();
+        DiscoveryNodes nodes2 = DiscoveryNodes.builder().masterNodeId("b").build();
+
+        LinkedList<ProcessClusterState> queue = new LinkedList<>();
+        ProcessClusterState thirdMostRecent = new ProcessClusterState(ClusterState.builder(clusterName).version(1).nodes(nodes1).build());
+        queue.offer(thirdMostRecent);
+        ProcessClusterState secondMostRecent = new ProcessClusterState(ClusterState.builder(clusterName).version(2).nodes(nodes1).build());
+        queue.offer(secondMostRecent);
+        ProcessClusterState mostRecent = new ProcessClusterState(ClusterState.builder(clusterName).version(3).nodes(nodes1).build());
+        queue.offer(mostRecent);
+        Collections.shuffle(queue, getRandom());
+        queue.offer(new ProcessClusterState(ClusterState.builder(clusterName).version(4).nodes(nodes2).build()));
+        queue.offer(new ProcessClusterState(ClusterState.builder(clusterName).version(5).nodes(nodes1).build()));
+
+
+        assertThat(ZenDiscovery.selectNextStateToProcess(queue), sameInstance(mostRecent.clusterState));
+        assertThat(thirdMostRecent.processed, is(true));
+        assertThat(secondMostRecent.processed, is(true));
+        assertThat(mostRecent.processed, is(true));
+        assertThat(queue.size(), equalTo(2));
+        assertThat(queue.get(0).processed, is(false));
+        assertThat(queue.get(1).processed, is(false));
+    }
+
 }
diff --git a/core/src/test/java/org/elasticsearch/discovery/zen/publish/PendingClusterStatesQueueTests.java b/core/src/test/java/org/elasticsearch/discovery/zen/publish/PendingClusterStatesQueueTests.java
deleted file mode 100644
index a8e9f00..0000000
--- a/core/src/test/java/org/elasticsearch/discovery/zen/publish/PendingClusterStatesQueueTests.java
+++ /dev/null
@@ -1,224 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.discovery.zen.publish;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.ClusterName;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.common.transport.DummyTransportAddress;
-import org.elasticsearch.discovery.zen.publish.PendingClusterStatesQueue.ClusterStateContext;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.*;
-
-import static org.hamcrest.Matchers.*;
-
-public class PendingClusterStatesQueueTests extends ESTestCase {
-
-    public void testSelectNextStateToProcess_empty() {
-        PendingClusterStatesQueue queue = new PendingClusterStatesQueue(logger, randomIntBetween(1, 200));
-        assertThat(queue.getNextClusterStateToProcess(), nullValue());
-    }
-
-    public void testDroppingStatesAtCapacity() {
-        List<ClusterState> states = randomStates(scaledRandomIntBetween(10, 300), "master1", "master2", "master3", "master4");
-        Collections.shuffle(states, random());
-        // insert half of the states
-        final int numberOfStateToDrop = states.size() / 2;
-        List<ClusterState> stateToDrop = states.subList(0, numberOfStateToDrop);
-        final int queueSize = states.size() - numberOfStateToDrop;
-        PendingClusterStatesQueue queue = createQueueWithStates(stateToDrop, queueSize);
-        List<ClusterStateContext> committedContexts = randomCommitStates(queue);
-        for (ClusterState state : states.subList(numberOfStateToDrop, states.size())) {
-            queue.addPending(state);
-        }
-
-        assertThat(queue.pendingClusterStates().length, equalTo(queueSize));
-        // check all committed states got a failure due to the drop
-        for (ClusterStateContext context : committedContexts) {
-            assertThat(((MockListener) context.listener).failure, notNullValue());
-        }
-
-        // all states that should have dropped are indeed dropped.
-        for (ClusterState state : stateToDrop) {
-            assertThat(queue.findState(state.stateUUID()), nullValue());
-        }
-
-    }
-
-    public void testSimpleQueueSameMaster() {
-        final int numUpdates = scaledRandomIntBetween(50, 100);
-        List<ClusterState> states = randomStates(numUpdates, "master");
-        Collections.shuffle(states, random());
-        PendingClusterStatesQueue queue;
-        queue = createQueueWithStates(states);
-
-        // no state is committed yet
-        assertThat(queue.getNextClusterStateToProcess(), nullValue());
-
-        ClusterState highestCommitted = null;
-        for (ClusterStateContext context : randomCommitStates(queue)) {
-            if (highestCommitted == null || context.state.supersedes(highestCommitted)) {
-                highestCommitted = context.state;
-            }
-        }
-
-        assertThat(queue.getNextClusterStateToProcess(), sameInstance(highestCommitted));
-
-        queue.markAsProcessed(highestCommitted);
-
-        // now there is nothing more to process
-        assertThat(queue.getNextClusterStateToProcess(), nullValue());
-    }
-
-    public void testProcessedStateCleansStatesFromOtherMasters() {
-        List<ClusterState> states = randomStates(scaledRandomIntBetween(10, 300), "master1", "master2", "master3", "master4");
-        PendingClusterStatesQueue queue = createQueueWithStates(states);
-        List<ClusterStateContext> committedContexts = randomCommitStates(queue);
-        ClusterState randomCommitted = randomFrom(committedContexts).state;
-        queue.markAsProcessed(randomCommitted);
-        final String processedMaster = randomCommitted.nodes().masterNodeId();
-
-        // now check that queue doesn't contain anything pending from another master
-        for (ClusterStateContext context : queue.pendingStates) {
-            final String pendingMaster = context.state.nodes().masterNodeId();
-            assertThat("found a cluster state from [" + pendingMaster
-                            + "], after a state from [" + processedMaster + "] was proccessed",
-                    pendingMaster, equalTo(processedMaster));
-        }
-        // and check all committed contexts from another master were failed
-        for (ClusterStateContext context : committedContexts) {
-            if (context.state.nodes().masterNodeId().equals(processedMaster) == false) {
-                assertThat(((MockListener) context.listener).failure, notNullValue());
-            }
-        }
-    }
-
-    public void testFailedStateCleansSupersededStatesOnly() {
-        List<ClusterState> states = randomStates(scaledRandomIntBetween(10, 50), "master1", "master2", "master3", "master4");
-        PendingClusterStatesQueue queue = createQueueWithStates(states);
-        List<ClusterStateContext> committedContexts = randomCommitStates(queue);
-        ClusterState toFail = randomFrom(committedContexts).state;
-        queue.markAsFailed(toFail, new ElasticsearchException("boo!"));
-        final Map<String, ClusterStateContext> committedContextsById = new HashMap<>();
-        for (ClusterStateContext context : committedContexts) {
-            committedContextsById.put(context.stateUUID(), context);
-        }
-
-        // now check that queue doesn't contain superseded states
-        for (ClusterStateContext context : queue.pendingStates) {
-            if (context.committed()) {
-                assertFalse("found a committed cluster state, which is superseded by a failed state.\nFound:" + context.state + "\nfailed:" + toFail,
-                        toFail.supersedes(context.state));
-            }
-        }
-        // check no state has been erroneously removed
-        for (ClusterState state : states) {
-            ClusterStateContext pendingContext = queue.findState(state.stateUUID());
-            if (pendingContext != null) {
-                continue;
-            }
-            if (state.equals(toFail)) {
-                continue;
-            }
-            assertThat("non-committed states should never be removed", committedContextsById, hasKey(state.stateUUID()));
-            final ClusterStateContext context = committedContextsById.get(state.stateUUID());
-            assertThat("removed state is not superseded by failed state. \nRemoved state:" + context + "\nfailed: " + toFail,
-                    toFail.supersedes(context.state), equalTo(true));
-            assertThat("removed state was failed with wrong exception", ((MockListener) context.listener).failure, notNullValue());
-            assertThat("removed state was failed with wrong exception", ((MockListener) context.listener).failure.getMessage(), containsString("boo"));
-        }
-    }
-
-    public void testFailAllAndClear() {
-        List<ClusterState> states = randomStates(scaledRandomIntBetween(10, 50), "master1", "master2", "master3", "master4");
-        PendingClusterStatesQueue queue = createQueueWithStates(states);
-        List<ClusterStateContext> committedContexts = randomCommitStates(queue);
-        queue.failAllStatesAndClear(new ElasticsearchException("boo!"));
-        assertThat(queue.pendingStates, empty());
-        assertThat(queue.getNextClusterStateToProcess(), nullValue());
-        for (ClusterStateContext context : committedContexts) {
-            assertThat("state was failed with wrong exception", ((MockListener) context.listener).failure, notNullValue());
-            assertThat("state was failed with wrong exception", ((MockListener) context.listener).failure.getMessage(), containsString("boo"));
-        }
-    }
-
-    protected List<ClusterStateContext> randomCommitStates(PendingClusterStatesQueue queue) {
-        List<ClusterStateContext> committedContexts = new ArrayList<>();
-        for (int iter = randomInt(queue.pendingStates.size() - 1); iter >= 0; iter--) {
-            ClusterState state = queue.markAsCommitted(randomFrom(queue.pendingStates).stateUUID(), new MockListener());
-            if (state != null) {
-                // null cluster state means we committed twice
-                committedContexts.add(queue.findState(state.stateUUID()));
-            }
-        }
-        return committedContexts;
-    }
-
-    PendingClusterStatesQueue createQueueWithStates(List<ClusterState> states) {
-        return createQueueWithStates(states, states.size() * 2); // we don't care about limits (there are dedicated tests for that)
-    }
-
-    PendingClusterStatesQueue createQueueWithStates(List<ClusterState> states, int maxQueueSize) {
-        PendingClusterStatesQueue queue;
-        queue = new PendingClusterStatesQueue(logger, maxQueueSize);
-        for (ClusterState state : states) {
-            queue.addPending(state);
-        }
-        return queue;
-    }
-
-    List<ClusterState> randomStates(int count, String... masters) {
-        ArrayList<ClusterState> states = new ArrayList<>(count);
-        ClusterState[] lastClusterStatePerMaster = new ClusterState[masters.length];
-        for (; count > 0; count--) {
-            int masterIndex = randomInt(masters.length - 1);
-            ClusterState state = lastClusterStatePerMaster[masterIndex];
-            if (state == null) {
-                state = ClusterState.builder(ClusterName.DEFAULT).nodes(DiscoveryNodes.builder()
-                                .put(new DiscoveryNode(masters[masterIndex], DummyTransportAddress.INSTANCE, Version.CURRENT)).masterNodeId(masters[masterIndex]).build()
-                ).build();
-            } else {
-                state = ClusterState.builder(state).incrementVersion().build();
-            }
-            states.add(state);
-            lastClusterStatePerMaster[masterIndex] = state;
-        }
-        return states;
-    }
-
-    static class MockListener implements PendingClusterStatesQueue.StateProcessedListener {
-        volatile boolean processed;
-        volatile Throwable failure;
-
-        @Override
-        public void onNewClusterStateProcessed() {
-            processed = true;
-        }
-
-        @Override
-        public void onNewClusterStateFailed(Throwable t) {
-            failure = t;
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateActionTests.java b/core/src/test/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateActionTests.java
deleted file mode 100644
index b9b4109..0000000
--- a/core/src/test/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateActionTests.java
+++ /dev/null
@@ -1,881 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.zen.publish;
-
-import com.google.common.collect.Maps;
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.*;
-import org.elasticsearch.cluster.block.ClusterBlocks;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.collect.ImmutableOpenMap;
-import org.elasticsearch.common.collect.Tuple;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.discovery.Discovery;
-import org.elasticsearch.discovery.DiscoverySettings;
-import org.elasticsearch.discovery.zen.DiscoveryNodesProvider;
-import org.elasticsearch.node.service.NodeService;
-import org.elasticsearch.node.settings.NodeSettingsService;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.junit.annotations.TestLogging;
-import org.elasticsearch.test.transport.MockTransportService;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.*;
-import org.elasticsearch.transport.local.LocalTransport;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.*;
-import java.util.concurrent.CopyOnWriteArrayList;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.atomic.AtomicBoolean;
-import java.util.concurrent.atomic.AtomicReference;
-
-import static com.google.common.collect.Maps.newHashMap;
-import static org.hamcrest.Matchers.*;
-
-@TestLogging("discovery.zen.publish:TRACE")
-public class PublishClusterStateActionTests extends ESTestCase {
-
-    protected ThreadPool threadPool;
-    protected Map<String, MockNode> nodes = newHashMap();
-
-    public static class MockNode implements PublishClusterStateAction.NewPendingClusterStateListener, DiscoveryNodesProvider {
-        public final DiscoveryNode discoveryNode;
-        public final MockTransportService service;
-        public MockPublishAction action;
-        public final ClusterStateListener listener;
-
-        public volatile ClusterState clusterState;
-
-        private final ESLogger logger;
-
-        public MockNode(DiscoveryNode discoveryNode, MockTransportService service, @Nullable ClusterStateListener listener, ESLogger logger) {
-            this.discoveryNode = discoveryNode;
-            this.service = service;
-            this.listener = listener;
-            this.logger = logger;
-            this.clusterState = ClusterState.builder(ClusterName.DEFAULT).nodes(DiscoveryNodes.builder().put(discoveryNode).localNodeId(discoveryNode.id()).build()).build();
-        }
-
-        public MockNode setAsMaster() {
-            this.clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes()).masterNodeId(discoveryNode.id())).build();
-            return this;
-        }
-
-        public MockNode resetMasterId() {
-            this.clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes()).masterNodeId(null)).build();
-            return this;
-        }
-
-
-        public void connectTo(DiscoveryNode node) {
-            service.connectToNode(node);
-        }
-
-        @Override
-        public void onNewClusterState(String reason) {
-            ClusterState newClusterState = action.pendingStatesQueue().getNextClusterStateToProcess();
-            logger.debug("[{}] received version [{}], uuid [{}]", discoveryNode.name(), newClusterState.version(), newClusterState.stateUUID());
-            if (listener != null) {
-                ClusterChangedEvent event = new ClusterChangedEvent("", newClusterState, clusterState);
-                listener.clusterChanged(event);
-            }
-            if (clusterState.nodes().masterNode() == null || newClusterState.supersedes(clusterState)) {
-                clusterState = newClusterState;
-            }
-            action.pendingStatesQueue().markAsProcessed(newClusterState);
-        }
-
-        @Override
-        public DiscoveryNodes nodes() {
-            return clusterState.nodes();
-        }
-
-        @Override
-        public NodeService nodeService() {
-            assert false;
-            throw new UnsupportedOperationException("Shouldn't be here");
-        }
-    }
-
-    public MockNode createMockNode(final String name) throws Exception {
-        return createMockNode(name, Settings.EMPTY, Version.CURRENT);
-    }
-
-    public MockNode createMockNode(String name, Settings settings) throws Exception {
-        return createMockNode(name, settings, Version.CURRENT);
-    }
-
-    public MockNode createMockNode(final String name, Settings settings, Version version) throws Exception {
-        return createMockNode(name, settings, version, null);
-    }
-
-    public MockNode createMockNode(String name, Settings settings, Version version, @Nullable ClusterStateListener listener) throws Exception {
-        settings = Settings.builder()
-                .put("name", name)
-                .put(TransportService.SETTING_TRACE_LOG_INCLUDE, "", TransportService.SETTING_TRACE_LOG_EXCLUDE, "NOTHING")
-                .put(settings)
-                .build();
-
-        MockTransportService service = buildTransportService(settings, version);
-        DiscoveryNode discoveryNode = new DiscoveryNode(name, name, service.boundAddress().publishAddress(),
-                Maps.newHashMap(settings.getByPrefix("node.").getAsMap()), version);
-        MockNode node = new MockNode(discoveryNode, service, listener, logger);
-        node.action = buildPublishClusterStateAction(settings, service, node, node);
-        final CountDownLatch latch = new CountDownLatch(nodes.size() * 2 + 1);
-        TransportConnectionListener waitForConnection = new TransportConnectionListener() {
-            @Override
-            public void onNodeConnected(DiscoveryNode node) {
-                latch.countDown();
-            }
-
-            @Override
-            public void onNodeDisconnected(DiscoveryNode node) {
-                fail("disconnect should not be called " + node);
-            }
-        };
-        node.service.addConnectionListener(waitForConnection);
-        for (MockNode curNode : nodes.values()) {
-            curNode.service.addConnectionListener(waitForConnection);
-            curNode.connectTo(node.discoveryNode);
-            node.connectTo(curNode.discoveryNode);
-        }
-        node.connectTo(node.discoveryNode);
-        assertThat("failed to wait for all nodes to connect", latch.await(5, TimeUnit.SECONDS), equalTo(true));
-        for (MockNode curNode : nodes.values()) {
-            curNode.service.removeConnectionListener(waitForConnection);
-        }
-        node.service.removeConnectionListener(waitForConnection);
-        if (nodes.put(name, node) != null) {
-            fail("Node with the name " + name + " already exist");
-        }
-        return node;
-    }
-
-    public MockTransportService service(String name) {
-        MockNode node = nodes.get(name);
-        if (node != null) {
-            return node.service;
-        }
-        return null;
-    }
-
-    public PublishClusterStateAction action(String name) {
-        MockNode node = nodes.get(name);
-        if (node != null) {
-            return node.action;
-        }
-        return null;
-    }
-
-    @Override
-    @Before
-    public void setUp() throws Exception {
-        super.setUp();
-        threadPool = new ThreadPool(getClass().getName());
-    }
-
-    @Override
-    @After
-    public void tearDown() throws Exception {
-        super.tearDown();
-        for (MockNode curNode : nodes.values()) {
-            curNode.action.close();
-            curNode.service.close();
-        }
-        terminate(threadPool);
-    }
-
-    protected MockTransportService buildTransportService(Settings settings, Version version) {
-        MockTransportService transportService = new MockTransportService(settings, new LocalTransport(settings, threadPool, version, new NamedWriteableRegistry()), threadPool);
-        transportService.start();
-        return transportService;
-    }
-
-    protected MockPublishAction buildPublishClusterStateAction(Settings settings, MockTransportService transportService, DiscoveryNodesProvider nodesProvider,
-                                                               PublishClusterStateAction.NewPendingClusterStateListener listener) {
-        DiscoverySettings discoverySettings = new DiscoverySettings(settings, new NodeSettingsService(settings));
-        return new MockPublishAction(settings, transportService, nodesProvider, listener, discoverySettings, ClusterName.DEFAULT);
-    }
-
-    @Test
-    public void testSimpleClusterStatePublishing() throws Exception {
-        MockNode nodeA = createMockNode("nodeA", Settings.EMPTY, Version.CURRENT).setAsMaster();
-        MockNode nodeB = createMockNode("nodeB", Settings.EMPTY, Version.CURRENT);
-
-        // Initial cluster state
-        ClusterState clusterState = nodeA.clusterState;
-
-        // cluster state update - add nodeB
-        DiscoveryNodes discoveryNodes = DiscoveryNodes.builder(clusterState.nodes()).put(nodeB.discoveryNode).build();
-        ClusterState previousClusterState = clusterState;
-        clusterState = ClusterState.builder(clusterState).nodes(discoveryNodes).incrementVersion().build();
-        publishStateAndWait(nodeA.action, clusterState, previousClusterState);
-        assertSameStateFromFull(nodeB.clusterState, clusterState);
-
-        // cluster state update - add block
-        previousClusterState = clusterState;
-        clusterState = ClusterState.builder(clusterState).blocks(ClusterBlocks.builder().addGlobalBlock(MetaData.CLUSTER_READ_ONLY_BLOCK)).incrementVersion().build();
-        publishStateAndWait(nodeA.action, clusterState, previousClusterState);
-        assertSameStateFromDiff(nodeB.clusterState, clusterState);
-        assertThat(nodeB.clusterState.blocks().global().size(), equalTo(1));
-
-        // cluster state update - remove block
-        previousClusterState = clusterState;
-        clusterState = ClusterState.builder(clusterState).blocks(ClusterBlocks.EMPTY_CLUSTER_BLOCK).incrementVersion().build();
-        publishStateAndWait(nodeA.action, clusterState, previousClusterState);
-        assertSameStateFromDiff(nodeB.clusterState, clusterState);
-        assertTrue(nodeB.clusterState.wasReadFromDiff());
-
-        // Adding new node - this node should get full cluster state while nodeB should still be getting diffs
-
-        MockNode nodeC = createMockNode("nodeC", Settings.EMPTY, Version.CURRENT);
-
-        // cluster state update 3 - register node C
-        previousClusterState = clusterState;
-        discoveryNodes = DiscoveryNodes.builder(discoveryNodes).put(nodeC.discoveryNode).build();
-        clusterState = ClusterState.builder(clusterState).nodes(discoveryNodes).incrementVersion().build();
-        publishStateAndWait(nodeA.action, clusterState, previousClusterState);
-        assertSameStateFromDiff(nodeB.clusterState, clusterState);
-        // First state
-        assertSameStateFromFull(nodeC.clusterState, clusterState);
-
-        // cluster state update 4 - update settings
-        previousClusterState = clusterState;
-        MetaData metaData = MetaData.builder(clusterState.metaData()).transientSettings(Settings.settingsBuilder().put("foo", "bar").build()).build();
-        clusterState = ClusterState.builder(clusterState).metaData(metaData).incrementVersion().build();
-        publishStateAndWait(nodeA.action, clusterState, previousClusterState);
-        assertSameStateFromDiff(nodeB.clusterState, clusterState);
-        assertThat(nodeB.clusterState.blocks().global().size(), equalTo(0));
-        assertSameStateFromDiff(nodeC.clusterState, clusterState);
-        assertThat(nodeC.clusterState.blocks().global().size(), equalTo(0));
-
-        // cluster state update - skipping one version change - should request full cluster state
-        previousClusterState = ClusterState.builder(clusterState).incrementVersion().build();
-        clusterState = ClusterState.builder(clusterState).incrementVersion().build();
-        publishStateAndWait(nodeA.action, clusterState, previousClusterState);
-        assertSameStateFromFull(nodeB.clusterState, clusterState);
-        assertSameStateFromFull(nodeC.clusterState, clusterState);
-        assertFalse(nodeC.clusterState.wasReadFromDiff());
-
-        // node A steps down from being master
-        nodeA.resetMasterId();
-        nodeB.resetMasterId();
-        nodeC.resetMasterId();
-
-        // node B becomes the master and sends a version of the cluster state that goes back
-        discoveryNodes = DiscoveryNodes.builder(discoveryNodes)
-                .put(nodeA.discoveryNode)
-                .put(nodeB.discoveryNode)
-                .put(nodeC.discoveryNode)
-                .masterNodeId(nodeB.discoveryNode.id())
-                .localNodeId(nodeB.discoveryNode.id())
-                .build();
-        previousClusterState = ClusterState.builder(new ClusterName("test")).nodes(discoveryNodes).build();
-        clusterState = ClusterState.builder(clusterState).nodes(discoveryNodes).incrementVersion().build();
-        publishStateAndWait(nodeB.action, clusterState, previousClusterState);
-        assertSameStateFromFull(nodeA.clusterState, clusterState);
-        assertSameStateFromFull(nodeC.clusterState, clusterState);
-    }
-
-    @Test
-    public void testUnexpectedDiffPublishing() throws Exception {
-
-        MockNode nodeA = createMockNode("nodeA", Settings.EMPTY, Version.CURRENT, new ClusterStateListener() {
-            @Override
-            public void clusterChanged(ClusterChangedEvent event) {
-                fail("Shouldn't send cluster state to myself");
-            }
-        }).setAsMaster();
-
-        MockNode nodeB = createMockNode("nodeB", Settings.EMPTY, Version.CURRENT);
-
-        // Initial cluster state with both states - the second node still shouldn't get diff even though it's present in the previous cluster state
-        DiscoveryNodes discoveryNodes = DiscoveryNodes.builder(nodeA.nodes()).put(nodeB.discoveryNode).build();
-        ClusterState previousClusterState = ClusterState.builder(ClusterName.DEFAULT).nodes(discoveryNodes).build();
-        ClusterState clusterState = ClusterState.builder(previousClusterState).incrementVersion().build();
-        publishStateAndWait(nodeA.action, clusterState, previousClusterState);
-        assertSameStateFromFull(nodeB.clusterState, clusterState);
-
-        // cluster state update - add block
-        previousClusterState = clusterState;
-        clusterState = ClusterState.builder(clusterState).blocks(ClusterBlocks.builder().addGlobalBlock(MetaData.CLUSTER_READ_ONLY_BLOCK)).incrementVersion().build();
-        publishStateAndWait(nodeA.action, clusterState, previousClusterState);
-        assertSameStateFromDiff(nodeB.clusterState, clusterState);
-    }
-
-    @Test
-    public void testDisablingDiffPublishing() throws Exception {
-        Settings noDiffPublishingSettings = Settings.builder().put(DiscoverySettings.PUBLISH_DIFF_ENABLE, false).build();
-
-        MockNode nodeA = createMockNode("nodeA", noDiffPublishingSettings, Version.CURRENT, new ClusterStateListener() {
-            @Override
-            public void clusterChanged(ClusterChangedEvent event) {
-                fail("Shouldn't send cluster state to myself");
-            }
-        });
-
-        MockNode nodeB = createMockNode("nodeB", noDiffPublishingSettings, Version.CURRENT, new ClusterStateListener() {
-            @Override
-            public void clusterChanged(ClusterChangedEvent event) {
-                assertFalse(event.state().wasReadFromDiff());
-            }
-        });
-
-        // Initial cluster state
-        DiscoveryNodes discoveryNodes = DiscoveryNodes.builder().put(nodeA.discoveryNode).localNodeId(nodeA.discoveryNode.id()).masterNodeId(nodeA.discoveryNode.id()).build();
-        ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT).nodes(discoveryNodes).build();
-
-        // cluster state update - add nodeB
-        discoveryNodes = DiscoveryNodes.builder(discoveryNodes).put(nodeB.discoveryNode).build();
-        ClusterState previousClusterState = clusterState;
-        clusterState = ClusterState.builder(clusterState).nodes(discoveryNodes).incrementVersion().build();
-        publishStateAndWait(nodeA.action, clusterState, previousClusterState);
-
-        // cluster state update - add block
-        previousClusterState = clusterState;
-        clusterState = ClusterState.builder(clusterState).blocks(ClusterBlocks.builder().addGlobalBlock(MetaData.CLUSTER_READ_ONLY_BLOCK)).incrementVersion().build();
-        publishStateAndWait(nodeA.action, clusterState, previousClusterState);
-    }
-
-
-    /**
-     * Test not waiting on publishing works correctly (i.e., publishing times out)
-     */
-    @Test
-    public void testSimultaneousClusterStatePublishing() throws Exception {
-        int numberOfNodes = randomIntBetween(2, 10);
-        int numberOfIterations = scaledRandomIntBetween(5, 50);
-        Settings settings = Settings.builder().put(DiscoverySettings.PUBLISH_DIFF_ENABLE, randomBoolean()).build();
-        MockNode master = createMockNode("node0", settings, Version.CURRENT, new ClusterStateListener() {
-            @Override
-            public void clusterChanged(ClusterChangedEvent event) {
-                assertProperMetaDataForVersion(event.state().metaData(), event.state().version());
-            }
-        }).setAsMaster();
-        DiscoveryNodes.Builder discoveryNodesBuilder = DiscoveryNodes.builder(master.nodes());
-        for (int i = 1; i < numberOfNodes; i++) {
-            final String name = "node" + i;
-            final MockNode node = createMockNode(name, settings, Version.CURRENT, new ClusterStateListener() {
-                @Override
-                public void clusterChanged(ClusterChangedEvent event) {
-                    assertProperMetaDataForVersion(event.state().metaData(), event.state().version());
-                }
-            });
-            discoveryNodesBuilder.put(node.discoveryNode);
-        }
-
-        AssertingAckListener[] listeners = new AssertingAckListener[numberOfIterations];
-        DiscoveryNodes discoveryNodes = discoveryNodesBuilder.build();
-        MetaData metaData = MetaData.EMPTY_META_DATA;
-        ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT).metaData(metaData).build();
-        ClusterState previousState;
-        for (int i = 0; i < numberOfIterations; i++) {
-            previousState = clusterState;
-            metaData = buildMetaDataForVersion(metaData, i + 1);
-            clusterState = ClusterState.builder(clusterState).incrementVersion().metaData(metaData).nodes(discoveryNodes).build();
-            listeners[i] = publishState(master.action, clusterState, previousState);
-        }
-
-        for (int i = 0; i < numberOfIterations; i++) {
-            listeners[i].await(1, TimeUnit.SECONDS);
-        }
-
-        // set the master cs
-        master.clusterState = clusterState;
-
-        for (MockNode node : nodes.values()) {
-            assertSameState(node.clusterState, clusterState);
-            assertThat(node.clusterState.nodes().localNode(), equalTo(node.discoveryNode));
-        }
-    }
-
-    @Test
-    public void testSerializationFailureDuringDiffPublishing() throws Exception {
-
-        MockNode nodeA = createMockNode("nodeA", Settings.EMPTY, Version.CURRENT, new ClusterStateListener() {
-            @Override
-            public void clusterChanged(ClusterChangedEvent event) {
-                fail("Shouldn't send cluster state to myself");
-            }
-        }).setAsMaster();
-
-        MockNode nodeB = createMockNode("nodeB", Settings.EMPTY, Version.CURRENT);
-
-        // Initial cluster state with both states - the second node still shouldn't get diff even though it's present in the previous cluster state
-        DiscoveryNodes discoveryNodes = DiscoveryNodes.builder(nodeA.nodes()).put(nodeB.discoveryNode).build();
-        ClusterState previousClusterState = ClusterState.builder(ClusterName.DEFAULT).nodes(discoveryNodes).build();
-        ClusterState clusterState = ClusterState.builder(previousClusterState).incrementVersion().build();
-        publishStateAndWait(nodeA.action, clusterState, previousClusterState);
-        assertSameStateFromFull(nodeB.clusterState, clusterState);
-
-        // cluster state update - add block
-        previousClusterState = clusterState;
-        clusterState = ClusterState.builder(clusterState).blocks(ClusterBlocks.builder().addGlobalBlock(MetaData.CLUSTER_READ_ONLY_BLOCK)).incrementVersion().build();
-
-        ClusterState unserializableClusterState = new ClusterState(clusterState.version(), clusterState.stateUUID(), clusterState) {
-            @Override
-            public Diff<ClusterState> diff(ClusterState previousState) {
-                return new Diff<ClusterState>() {
-                    @Override
-                    public ClusterState apply(ClusterState part) {
-                        fail("this diff shouldn't be applied");
-                        return part;
-                    }
-
-                    @Override
-                    public void writeTo(StreamOutput out) throws IOException {
-                        throw new IOException("Simulated failure of diff serialization");
-                    }
-                };
-            }
-        };
-        try {
-            publishStateAndWait(nodeA.action, unserializableClusterState, previousClusterState);
-            fail("cluster state published despite of diff errors");
-        } catch (Discovery.FailedToCommitClusterStateException e) {
-            assertThat(e.getCause(), notNullValue());
-            assertThat(e.getCause().getMessage(), containsString("failed to serialize"));
-        }
-    }
-
-
-    public void testFailToPublishWithLessThanMinMasterNodes() throws Exception {
-        final int masterNodes = randomIntBetween(1, 10);
-
-        MockNode master = createMockNode("master");
-        DiscoveryNodes.Builder discoveryNodesBuilder = DiscoveryNodes.builder().put(master.discoveryNode);
-        for (int i = 1; i < masterNodes; i++) {
-            discoveryNodesBuilder.put(createMockNode("node" + i).discoveryNode);
-        }
-        final int dataNodes = randomIntBetween(0, 5);
-        final Settings dataSettings = Settings.builder().put("node.master", false).build();
-        for (int i = 0; i < dataNodes; i++) {
-            discoveryNodesBuilder.put(createMockNode("data_" + i, dataSettings).discoveryNode);
-        }
-        discoveryNodesBuilder.localNodeId(master.discoveryNode.id()).masterNodeId(master.discoveryNode.id());
-        DiscoveryNodes discoveryNodes = discoveryNodesBuilder.build();
-        MetaData metaData = MetaData.EMPTY_META_DATA;
-        ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT).metaData(metaData).nodes(discoveryNodes).build();
-        ClusterState previousState = master.clusterState;
-        try {
-            publishState(master.action, clusterState, previousState, masterNodes + randomIntBetween(1, 5));
-            fail("cluster state publishing didn't fail despite of not having enough nodes");
-        } catch (Discovery.FailedToCommitClusterStateException expected) {
-            logger.debug("failed to publish as expected", expected);
-        }
-    }
-
-    public void testPublishingWithSendingErrors() throws Exception {
-        int goodNodes = randomIntBetween(2, 5);
-        int errorNodes = randomIntBetween(1, 5);
-        int timeOutNodes = randomBoolean() ? 0 : randomIntBetween(1, 5); // adding timeout nodes will force timeout errors
-        final int numberOfMasterNodes = goodNodes + errorNodes + timeOutNodes + 1; // master
-        final boolean expectingToCommit = randomBoolean();
-        Settings.Builder settings = Settings.builder();
-        // make sure we have a reasonable timeout if we expect to timeout, o.w. one that will make the test "hang"
-        settings.put(DiscoverySettings.COMMIT_TIMEOUT, expectingToCommit == false && timeOutNodes > 0 ? "100ms" : "1h")
-                .put(DiscoverySettings.PUBLISH_TIMEOUT, "5ms"); // test is about committing
-
-        MockNode master = createMockNode("master", settings.build());
-
-        // randomize things a bit
-        int[] nodeTypes = new int[goodNodes + errorNodes + timeOutNodes];
-        for (int i = 0; i < goodNodes; i++) {
-            nodeTypes[i] = 0;
-        }
-        for (int i = goodNodes; i < goodNodes + errorNodes; i++) {
-            nodeTypes[i] = 1;
-        }
-        for (int i = goodNodes + errorNodes; i < nodeTypes.length; i++) {
-            nodeTypes[i] = 2;
-        }
-        Collections.shuffle(Arrays.asList(nodeTypes), random());
-
-        DiscoveryNodes.Builder discoveryNodesBuilder = DiscoveryNodes.builder().put(master.discoveryNode);
-        for (int i = 0; i < nodeTypes.length; i++) {
-            final MockNode mockNode = createMockNode("node" + i);
-            discoveryNodesBuilder.put(mockNode.discoveryNode);
-            switch (nodeTypes[i]) {
-                case 1:
-                    mockNode.action.errorOnSend.set(true);
-                    break;
-                case 2:
-                    mockNode.action.timeoutOnSend.set(true);
-                    break;
-            }
-        }
-        final int dataNodes = randomIntBetween(0, 3); // data nodes don't matter
-        for (int i = 0; i < dataNodes; i++) {
-            final MockNode mockNode = createMockNode("data_" + i, Settings.builder().put("node.master", false).build());
-            discoveryNodesBuilder.put(mockNode.discoveryNode);
-            if (randomBoolean()) {
-                // we really don't care - just chaos monkey
-                mockNode.action.errorOnCommit.set(randomBoolean());
-                mockNode.action.errorOnSend.set(randomBoolean());
-                mockNode.action.timeoutOnCommit.set(randomBoolean());
-                mockNode.action.timeoutOnSend.set(randomBoolean());
-            }
-        }
-
-        final int minMasterNodes;
-        final String expectedBehavior;
-        if (expectingToCommit) {
-            minMasterNodes = randomIntBetween(0, goodNodes + 1); // count master
-            expectedBehavior = "succeed";
-        } else {
-            minMasterNodes = randomIntBetween(goodNodes + 2, numberOfMasterNodes); // +2 because of master
-            expectedBehavior = timeOutNodes > 0 ? "timeout" : "fail";
-        }
-        logger.info("--> expecting commit to {}. good nodes [{}], errors [{}], timeouts [{}]. min_master_nodes [{}]",
-                expectedBehavior, goodNodes + 1, errorNodes, timeOutNodes, minMasterNodes);
-
-        discoveryNodesBuilder.localNodeId(master.discoveryNode.id()).masterNodeId(master.discoveryNode.id());
-        DiscoveryNodes discoveryNodes = discoveryNodesBuilder.build();
-        MetaData metaData = MetaData.EMPTY_META_DATA;
-        ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT).metaData(metaData).nodes(discoveryNodes).build();
-        ClusterState previousState = master.clusterState;
-        try {
-            publishState(master.action, clusterState, previousState, minMasterNodes);
-            if (expectingToCommit == false) {
-                fail("cluster state publishing didn't fail despite of not have enough nodes");
-            }
-        } catch (Discovery.FailedToCommitClusterStateException exception) {
-            logger.debug("failed to publish as expected", exception);
-            if (expectingToCommit) {
-                throw exception;
-            }
-            assertThat(exception.getMessage(), containsString(timeOutNodes > 0 ? "timed out" : "failed"));
-        }
-    }
-
-    public void testIncomingClusterStateValidation() throws Exception {
-        MockNode node = createMockNode("node");
-
-        logger.info("--> testing acceptances of any master when having no master");
-        ClusterState state = ClusterState.builder(node.clusterState)
-                .nodes(DiscoveryNodes.builder(node.nodes()).masterNodeId(randomAsciiOfLength(10))).incrementVersion().build();
-        node.action.validateIncomingState(state, null);
-
-        // now set a master node
-        node.clusterState = ClusterState.builder(node.clusterState).nodes(DiscoveryNodes.builder(node.nodes()).masterNodeId("master")).build();
-        logger.info("--> testing rejection of another master");
-        try {
-            node.action.validateIncomingState(state, node.clusterState);
-            fail("node accepted state from another master");
-        } catch (IllegalStateException OK) {
-        }
-
-        logger.info("--> test state from the current master is accepted");
-        node.action.validateIncomingState(ClusterState.builder(node.clusterState)
-                .nodes(DiscoveryNodes.builder(node.nodes()).masterNodeId("master")).build(), node.clusterState);
-
-
-        logger.info("--> testing rejection of another cluster name");
-        try {
-            node.action.validateIncomingState(ClusterState.builder(new ClusterName(randomAsciiOfLength(10))).nodes(node.nodes()).build(), node.clusterState);
-            fail("node accepted state with another cluster name");
-        } catch (IllegalStateException OK) {
-        }
-
-        logger.info("--> testing rejection of a cluster state with wrong local node");
-        try {
-            state = ClusterState.builder(node.clusterState)
-                    .nodes(DiscoveryNodes.builder(node.nodes()).localNodeId("_non_existing_").build())
-                    .incrementVersion().build();
-            node.action.validateIncomingState(state, node.clusterState);
-            fail("node accepted state with non-existence local node");
-        } catch (IllegalStateException OK) {
-        }
-
-        try {
-            MockNode otherNode = createMockNode("otherNode");
-            state = ClusterState.builder(node.clusterState).nodes(
-                    DiscoveryNodes.builder(node.nodes()).put(otherNode.discoveryNode).localNodeId(otherNode.discoveryNode.id()).build()
-            ).incrementVersion().build();
-            node.action.validateIncomingState(state, node.clusterState);
-            fail("node accepted state with existent but wrong local node");
-        } catch (IllegalStateException OK) {
-        }
-
-        logger.info("--> testing acceptance of an old cluster state");
-        state = node.clusterState;
-        node.clusterState = ClusterState.builder(node.clusterState).incrementVersion().build();
-        node.action.validateIncomingState(state, node.clusterState);
-
-        // an older version from a *new* master is also OK!
-        ClusterState previousState = ClusterState.builder(node.clusterState).incrementVersion().build();
-        state = ClusterState.builder(node.clusterState)
-                .nodes(DiscoveryNodes.builder(node.clusterState.nodes()).masterNodeId("_new_master_").build())
-                .build();
-        // remove the master of the node (but still have a previous cluster state with it)!
-        node.resetMasterId();
-
-        node.action.validateIncomingState(state, previousState);
-    }
-
-    public void testInterleavedPublishCommit() throws Throwable {
-        MockNode node = createMockNode("node").setAsMaster();
-        final CapturingTransportChannel channel = new CapturingTransportChannel();
-
-        List<ClusterState> states = new ArrayList<>();
-        final int numOfStates = scaledRandomIntBetween(3, 10);
-        for (int i = 1; i <= numOfStates; i++) {
-            states.add(ClusterState.builder(node.clusterState).version(i).stateUUID(ClusterState.UNKNOWN_UUID).build());
-        }
-
-        final ClusterState finalState = states.get(numOfStates - 1);
-        Collections.shuffle(states, random());
-
-        logger.info("--> publishing states");
-        for (ClusterState state : states) {
-            node.action.handleIncomingClusterStateRequest(
-                    new BytesTransportRequest(PublishClusterStateAction.serializeFullClusterState(state, Version.CURRENT), Version.CURRENT),
-                    channel);
-            assertThat(channel.response.get(), equalTo((TransportResponse) TransportResponse.Empty.INSTANCE));
-            assertThat(channel.error.get(), nullValue());
-            channel.clear();
-        }
-
-        logger.info("--> committing states");
-
-        Collections.shuffle(states, random());
-        for (ClusterState state : states) {
-            node.action.handleCommitRequest(new PublishClusterStateAction.CommitClusterStateRequest(state.stateUUID()), channel);
-            assertThat(channel.response.get(), equalTo((TransportResponse) TransportResponse.Empty.INSTANCE));
-            if (channel.error.get() != null) {
-                throw channel.error.get();
-            }
-        }
-        channel.clear();
-
-        //now check the last state held
-        assertSameState(node.clusterState, finalState);
-    }
-
-    /**
-     * Tests that cluster is committed or times out. It should never be the case that we fail
-     * an update due to a commit timeout, but it ends up being committed anyway
-     */
-    public void testTimeoutOrCommit() throws Exception {
-        Settings settings = Settings.builder()
-                .put(DiscoverySettings.COMMIT_TIMEOUT, "1ms").build(); // short but so we will sometime commit sometime timeout
-
-        MockNode master = createMockNode("master", settings);
-        MockNode node = createMockNode("node", settings);
-        ClusterState state = ClusterState.builder(master.clusterState)
-                .nodes(DiscoveryNodes.builder(master.clusterState.nodes()).put(node.discoveryNode).masterNodeId(master.discoveryNode.id())).build();
-
-        for (int i = 0; i < 10; i++) {
-            state = ClusterState.builder(state).incrementVersion().build();
-            logger.debug("--> publishing version [{}], UUID [{}]", state.version(), state.stateUUID());
-            boolean success;
-            try {
-                publishState(master.action, state, master.clusterState, 2).await(1, TimeUnit.HOURS);
-                success = true;
-            } catch (Discovery.FailedToCommitClusterStateException OK) {
-                success = false;
-            }
-            logger.debug("--> publishing [{}], verifying...", success ? "succeeded" : "failed");
-
-            if (success) {
-                assertSameState(node.clusterState, state);
-            } else {
-                assertThat(node.clusterState.stateUUID(), not(equalTo(state.stateUUID())));
-            }
-        }
-    }
-
-
-    private MetaData buildMetaDataForVersion(MetaData metaData, long version) {
-        ImmutableOpenMap.Builder<String, IndexMetaData> indices = ImmutableOpenMap.builder(metaData.indices());
-        indices.put("test" + version, IndexMetaData.builder("test" + version).settings(Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT))
-                .numberOfShards((int) version).numberOfReplicas(0).build());
-        return MetaData.builder(metaData)
-                .transientSettings(Settings.builder().put("test", version).build())
-                .indices(indices.build())
-                .build();
-    }
-
-    private void assertProperMetaDataForVersion(MetaData metaData, long version) {
-        for (long i = 1; i <= version; i++) {
-            assertThat(metaData.index("test" + i), notNullValue());
-            assertThat(metaData.index("test" + i).numberOfShards(), equalTo((int) i));
-        }
-        assertThat(metaData.index("test" + (version + 1)), nullValue());
-        assertThat(metaData.transientSettings().get("test"), equalTo(Long.toString(version)));
-    }
-
-    public void publishStateAndWait(PublishClusterStateAction action, ClusterState state, ClusterState previousState) throws InterruptedException {
-        publishState(action, state, previousState).await(1, TimeUnit.SECONDS);
-    }
-
-    public AssertingAckListener publishState(PublishClusterStateAction action, ClusterState state, ClusterState previousState) throws InterruptedException {
-        final int minimumMasterNodes = randomIntBetween(-1, state.nodes().getMasterNodes().size());
-        return publishState(action, state, previousState, minimumMasterNodes);
-    }
-
-    public AssertingAckListener publishState(PublishClusterStateAction action, ClusterState state, ClusterState previousState, int minMasterNodes) throws InterruptedException {
-        AssertingAckListener assertingAckListener = new AssertingAckListener(state.nodes().getSize() - 1);
-        ClusterChangedEvent changedEvent = new ClusterChangedEvent("test update", state, previousState);
-        action.publish(changedEvent, minMasterNodes, assertingAckListener);
-        return assertingAckListener;
-    }
-
-    public static class AssertingAckListener implements Discovery.AckListener {
-        private final List<Tuple<DiscoveryNode, Throwable>> errors = new CopyOnWriteArrayList<>();
-        private final AtomicBoolean timeoutOccurred = new AtomicBoolean();
-        private final CountDownLatch countDown;
-
-        public AssertingAckListener(int nodeCount) {
-            countDown = new CountDownLatch(nodeCount);
-        }
-
-        @Override
-        public void onNodeAck(DiscoveryNode node, @Nullable Throwable t) {
-            if (t != null) {
-                errors.add(new Tuple<>(node, t));
-            }
-            countDown.countDown();
-        }
-
-        @Override
-        public void onTimeout() {
-            timeoutOccurred.set(true);
-            // Fast forward the counter - no reason to wait here
-            long currentCount = countDown.getCount();
-            for (long i = 0; i < currentCount; i++) {
-                countDown.countDown();
-            }
-        }
-
-        public void await(long timeout, TimeUnit unit) throws InterruptedException {
-            assertThat(awaitErrors(timeout, unit), emptyIterable());
-        }
-
-        public List<Tuple<DiscoveryNode, Throwable>> awaitErrors(long timeout, TimeUnit unit) throws InterruptedException {
-            countDown.await(timeout, unit);
-            assertFalse(timeoutOccurred.get());
-            return errors;
-        }
-
-    }
-
-    void assertSameState(ClusterState actual, ClusterState expected) {
-        assertThat(actual, notNullValue());
-        final String reason = "\n--> actual ClusterState: " + actual.prettyPrint() + "\n--> expected ClusterState:" + expected.prettyPrint();
-        assertThat("unequal UUIDs" + reason, actual.stateUUID(), equalTo(expected.stateUUID()));
-        assertThat("unequal versions" + reason, actual.version(), equalTo(expected.version()));
-    }
-
-    void assertSameStateFromDiff(ClusterState actual, ClusterState expected) {
-        assertSameState(actual, expected);
-        assertTrue(actual.wasReadFromDiff());
-    }
-
-    void assertSameStateFromFull(ClusterState actual, ClusterState expected) {
-        assertSameState(actual, expected);
-        assertFalse(actual.wasReadFromDiff());
-    }
-
-    static class MockPublishAction extends PublishClusterStateAction {
-
-        AtomicBoolean timeoutOnSend = new AtomicBoolean();
-        AtomicBoolean errorOnSend = new AtomicBoolean();
-        AtomicBoolean timeoutOnCommit = new AtomicBoolean();
-        AtomicBoolean errorOnCommit = new AtomicBoolean();
-
-        public MockPublishAction(Settings settings, TransportService transportService, DiscoveryNodesProvider nodesProvider, NewPendingClusterStateListener listener, DiscoverySettings discoverySettings, ClusterName clusterName) {
-            super(settings, transportService, nodesProvider, listener, discoverySettings, clusterName);
-        }
-
-        @Override
-        protected void handleIncomingClusterStateRequest(BytesTransportRequest request, TransportChannel channel) throws IOException {
-            if (errorOnSend.get()) {
-                throw new ElasticsearchException("forced error on incoming cluster state");
-            }
-            if (timeoutOnSend.get()) {
-                return;
-            }
-            super.handleIncomingClusterStateRequest(request, channel);
-        }
-
-        @Override
-        protected void handleCommitRequest(PublishClusterStateAction.CommitClusterStateRequest request, TransportChannel channel) {
-            if (errorOnCommit.get()) {
-                throw new ElasticsearchException("forced error on incoming commit");
-            }
-            if (timeoutOnCommit.get()) {
-                return;
-            }
-            super.handleCommitRequest(request, channel);
-        }
-    }
-
-    static class CapturingTransportChannel implements TransportChannel {
-
-        AtomicReference<TransportResponse> response = new AtomicReference<>();
-        AtomicReference<Throwable> error = new AtomicReference<>();
-
-        public void clear() {
-            response.set(null);
-            error.set(null);
-        }
-
-        @Override
-        public String action() {
-            return "_noop_";
-        }
-
-        @Override
-        public String getProfileName() {
-            return "_noop_";
-        }
-
-        @Override
-        public void sendResponse(TransportResponse response) throws IOException {
-            this.response.set(response);
-            assertThat(error.get(), nullValue());
-        }
-
-        @Override
-        public void sendResponse(TransportResponse response, TransportResponseOptions options) throws IOException {
-            this.response.set(response);
-            assertThat(error.get(), nullValue());
-        }
-
-        @Override
-        public void sendResponse(Throwable error) throws IOException {
-            this.error.set(error);
-            assertThat(response.get(), nullValue());
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/gateway/MetaDataWriteDataNodesIT.java b/core/src/test/java/org/elasticsearch/gateway/MetaDataWriteDataNodesIT.java
index 7afb831..90e61e3 100644
--- a/core/src/test/java/org/elasticsearch/gateway/MetaDataWriteDataNodesIT.java
+++ b/core/src/test/java/org/elasticsearch/gateway/MetaDataWriteDataNodesIT.java
@@ -60,8 +60,8 @@ public class MetaDataWriteDataNodesIT extends ESIntegTestCase {
     public void testMetaIsRemovedIfAllShardsFromIndexRemoved() throws Exception {
         // this test checks that the index state is removed from a data only node once all shards have been allocated away from it
         String masterNode = internalCluster().startMasterOnlyNode(Settings.EMPTY);
-        Future<String> nodeName1 = internalCluster().startDataOnlyNodeAsync();
-        Future<String> nodeName2 = internalCluster().startDataOnlyNodeAsync();
+        InternalTestCluster.Async<String> nodeName1 = internalCluster().startDataOnlyNodeAsync();
+        InternalTestCluster.Async<String> nodeName2 = internalCluster().startDataOnlyNodeAsync();
         String node1 = nodeName1.get();
         String node2 = nodeName2.get();
 
diff --git a/core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java b/core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java
index b280389..159576d 100644
--- a/core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java
+++ b/core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java
@@ -44,7 +44,6 @@ import org.elasticsearch.search.sort.SortOrder;
 import org.elasticsearch.snapshots.SnapshotState;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.InternalTestCluster;
-import org.elasticsearch.test.junit.annotations.TestLogging;
 import org.elasticsearch.test.transport.MockTransportService;
 import org.elasticsearch.transport.TransportException;
 import org.elasticsearch.transport.TransportRequest;
@@ -60,7 +59,6 @@ import java.util.List;
 import java.util.concurrent.CopyOnWriteArrayList;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.ExecutionException;
-import java.util.concurrent.Future;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicInteger;
 
@@ -690,8 +688,8 @@ public class IndexWithShadowReplicasIT extends ESIntegTestCase {
         Settings fooSettings = Settings.builder().put(nodeSettings).put("node.affinity", "foo").build();
         Settings barSettings = Settings.builder().put(nodeSettings).put("node.affinity", "bar").build();
 
-        final Future<List<String>> fooNodes = internalCluster().startNodesAsync(2, fooSettings);
-        final Future<List<String>> barNodes = internalCluster().startNodesAsync(2, barSettings);
+        final InternalTestCluster.Async<List<String>> fooNodes = internalCluster().startNodesAsync(2, fooSettings);
+        final InternalTestCluster.Async<List<String>> barNodes = internalCluster().startNodesAsync(2, barSettings);
         fooNodes.get();
         barNodes.get();
         String IDX = "test";
diff --git a/core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java b/core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java
index 35939d3..8e1c81a 100644
--- a/core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java
+++ b/core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java
@@ -42,13 +42,13 @@ import org.apache.lucene.util.TestUtil;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.support.TransportActions;
-import org.elasticsearch.bwcompat.OldIndexBackwardsCompatibilityIT;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.Base64;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.FileSystemUtils;
+import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.lucene.uid.Versions;
 import org.elasticsearch.common.settings.Settings;
@@ -256,7 +256,7 @@ public class InternalEngineTests extends ESTestCase {
             public void onFailedEngine(ShardId shardId, String reason, @Nullable Throwable t) {
                 // we don't need to notify anybody in this test
             }
-        }, new TranslogHandler(shardId.index().getName()), IndexSearcher.getDefaultQueryCache(), IndexSearcher.getDefaultQueryCachingPolicy(), new IndexSearcherWrappingService(new HashSet<>(Arrays.asList(wrappers))), translogConfig);
+        }, new TranslogHandler(shardId.index().getName(), logger), IndexSearcher.getDefaultQueryCache(), IndexSearcher.getDefaultQueryCachingPolicy(), new IndexSearcherWrappingService(new HashSet<>(Arrays.asList(wrappers))), translogConfig);
         try {
             config.setCreate(Lucene.indexExists(store.directory()) == false);
         } catch (IOException e) {
@@ -1979,8 +1979,8 @@ public class InternalEngineTests extends ESTestCase {
 
         public final AtomicInteger recoveredOps = new AtomicInteger(0);
 
-        public TranslogHandler(String indexName) {
-            super(new ShardId("test", 0), null, null, null, null);
+        public TranslogHandler(String indexName, ESLogger logger) {
+            super(new ShardId("test", 0), null, null, null, null, logger);
             Settings settings = Settings.settingsBuilder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).build();
             RootObjectMapper.Builder rootBuilder = new RootObjectMapper.Builder("test");
             Index index = new Index(indexName);
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapperTests.java b/core/src/test/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapperTests.java
index da2151f..4bc25ed 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapperTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapperTests.java
@@ -18,9 +18,9 @@
  */
 package org.elasticsearch.index.mapper.geo;
 
+import org.apache.lucene.util.XGeoHashUtils;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.geo.GeoHashUtils;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.mapper.DocumentMapper;
@@ -78,7 +78,7 @@ public class GeoPointFieldMapperTests extends ESSingleNodeTestCase {
 
         assertThat(doc.rootDoc().getField("point.lat"), notNullValue());
         assertThat(doc.rootDoc().getField("point.lon"), notNullValue());
-        assertThat(doc.rootDoc().get("point.geohash"), equalTo(GeoHashUtils.encode(1.2, 1.3)));
+        assertThat(doc.rootDoc().get("point.geohash"), equalTo(XGeoHashUtils.stringEncode(1.3, 1.2)));
     }
 
     @Test
@@ -97,7 +97,7 @@ public class GeoPointFieldMapperTests extends ESSingleNodeTestCase {
 
         assertThat(doc.rootDoc().getField("point.lat"), notNullValue());
         assertThat(doc.rootDoc().getField("point.lon"), notNullValue());
-        assertThat(doc.rootDoc().get("point.geohash"), equalTo(GeoHashUtils.encode(1.2, 1.3)));
+        assertThat(doc.rootDoc().get("point.geohash"), equalTo(XGeoHashUtils.stringEncode(1.3, 1.2)));
     }
 
     @Test
@@ -110,13 +110,13 @@ public class GeoPointFieldMapperTests extends ESSingleNodeTestCase {
 
         ParsedDocument doc = defaultMapper.parse("test", "type", "1", XContentFactory.jsonBuilder()
                 .startObject()
-                .field("point", GeoHashUtils.encode(1.2, 1.3))
+                .field("point", XGeoHashUtils.stringEncode(1.3, 1.2))
                 .endObject()
                 .bytes());
 
         assertThat(doc.rootDoc().getField("point.lat"), notNullValue());
         assertThat(doc.rootDoc().getField("point.lon"), notNullValue());
-        assertThat(doc.rootDoc().get("point.geohash"), equalTo(GeoHashUtils.encode(1.2, 1.3)));
+        assertThat(doc.rootDoc().get("point.geohash"), equalTo(XGeoHashUtils.stringEncode(1.3, 1.2)));
     }
 
     @Test
@@ -129,7 +129,7 @@ public class GeoPointFieldMapperTests extends ESSingleNodeTestCase {
 
         ParsedDocument doc = defaultMapper.parse("test", "type", "1", XContentFactory.jsonBuilder()
                 .startObject()
-                .field("point", GeoHashUtils.encode(1.2, 1.3))
+                .field("point", XGeoHashUtils.stringEncode(1.3, 1.2))
                 .endObject()
                 .bytes());
 
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/geo/GeohashMappingGeoPointTests.java b/core/src/test/java/org/elasticsearch/index/mapper/geo/GeohashMappingGeoPointTests.java
index 537df6b..6338f4b 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/geo/GeohashMappingGeoPointTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/geo/GeohashMappingGeoPointTests.java
@@ -19,7 +19,7 @@
 
 package org.elasticsearch.index.mapper.geo;
 
-import org.elasticsearch.common.geo.GeoHashUtils;
+import org.apache.lucene.util.XGeoHashUtils;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.FieldMapper;
@@ -83,13 +83,13 @@ public class GeohashMappingGeoPointTests extends ESSingleNodeTestCase {
 
         ParsedDocument doc = defaultMapper.parse("test", "type", "1", XContentFactory.jsonBuilder()
                 .startObject()
-                .field("point", GeoHashUtils.encode(1.2, 1.3))
+                .field("point", XGeoHashUtils.stringEncode(1.3, 1.2))
                 .endObject()
                 .bytes());
 
         MatcherAssert.assertThat(doc.rootDoc().getField("point.lat"), nullValue());
         MatcherAssert.assertThat(doc.rootDoc().getField("point.lon"), nullValue());
-        MatcherAssert.assertThat(doc.rootDoc().get("point.geohash"), equalTo(GeoHashUtils.encode(1.2, 1.3)));
+        MatcherAssert.assertThat(doc.rootDoc().get("point.geohash"), equalTo(XGeoHashUtils.stringEncode(1.3, 1.2)));
         MatcherAssert.assertThat(doc.rootDoc().get("point"), notNullValue());
     }
 
diff --git a/core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java b/core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java
index 47fde53..96c849a 100644
--- a/core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java
@@ -32,6 +32,7 @@ import org.apache.lucene.queries.TermsQuery;
 import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.BoostQuery;
 import org.apache.lucene.search.ConstantScoreQuery;
 import org.apache.lucene.search.DisjunctionMaxQuery;
 import org.apache.lucene.search.FuzzyQuery;
@@ -69,7 +70,6 @@ import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.compress.CompressedXContent;
 import org.elasticsearch.common.lucene.search.MoreLikeThisQuery;
 import org.elasticsearch.common.lucene.search.Queries;
-import org.elasticsearch.common.lucene.search.function.BoostScoreFunction;
 import org.elasticsearch.common.lucene.search.function.FunctionScoreQuery;
 import org.elasticsearch.common.lucene.search.function.WeightFactorFunction;
 import org.elasticsearch.common.settings.Settings;
@@ -198,25 +198,30 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
     public void testQueryStringBoostsBuilder() throws Exception {
         IndexQueryParserService queryParser = queryParser();
         QueryStringQueryBuilder builder = queryStringQuery("field:boosted^2");
+        Query expected = new BoostQuery(new TermQuery(new Term("field", "boosted")), 2);
         Query parsedQuery = queryParser.parse(builder).query();
-        assertThat(parsedQuery, instanceOf(TermQuery.class));
-        assertThat(((TermQuery) parsedQuery).getTerm(), equalTo(new Term("field", "boosted")));
-        assertThat(parsedQuery.getBoost(), equalTo(2.0f));
+        assertEquals(expected, parsedQuery);
+
         builder.boost(2.0f);
+        expected = new BoostQuery(new TermQuery(new Term("field", "boosted")), 4);
         parsedQuery = queryParser.parse(builder).query();
-        assertThat(parsedQuery.getBoost(), equalTo(4.0f));
+        assertEquals(expected, parsedQuery);
 
         builder = queryStringQuery("((field:boosted^2) AND (field:foo^1.5))^3");
+        expected = new BoostQuery(new BooleanQuery.Builder()
+            .add(new BoostQuery(new TermQuery(new Term("field", "boosted")), 2), Occur.MUST)
+            .add(new BoostQuery(new TermQuery(new Term("field", "foo")), 1.5f), Occur.MUST)
+            .build(), 3);
         parsedQuery = queryParser.parse(builder).query();
-        assertThat(parsedQuery, instanceOf(BooleanQuery.class));
-        assertThat(assertBooleanSubQuery(parsedQuery, TermQuery.class, 0).getTerm(), equalTo(new Term("field", "boosted")));
-        assertThat(assertBooleanSubQuery(parsedQuery, TermQuery.class, 0).getBoost(), equalTo(2.0f));
-        assertThat(assertBooleanSubQuery(parsedQuery, TermQuery.class, 1).getTerm(), equalTo(new Term("field", "foo")));
-        assertThat(assertBooleanSubQuery(parsedQuery, TermQuery.class, 1).getBoost(), equalTo(1.5f));
-        assertThat(parsedQuery.getBoost(), equalTo(3.0f));
+        assertEquals(expected, parsedQuery);
+
         builder.boost(2.0f);
+        expected = new BoostQuery(new BooleanQuery.Builder()
+            .add(new BoostQuery(new TermQuery(new Term("field", "boosted")), 2), Occur.MUST)
+            .add(new BoostQuery(new TermQuery(new Term("field", "foo")), 1.5f), Occur.MUST)
+            .build(), 6);
         parsedQuery = queryParser.parse(builder).query();
-        assertThat(parsedQuery.getBoost(), equalTo(6.0f));
+        assertEquals(expected, parsedQuery);
     }
 
     @Test
@@ -1947,10 +1952,8 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
 
             BooleanQuery.Builder expected = new BooleanQuery.Builder();
             expected.add(new TermQuery(new Term("foobar", "banon")), Occur.SHOULD);
-            TermQuery tq1 = new TermQuery(new Term("name.first", "banon"));
-            tq1.setBoost(2);
-            TermQuery tq2 = new TermQuery(new Term("name.last", "banon"));
-            tq2.setBoost(3);
+            Query tq1 = new BoostQuery(new TermQuery(new Term("name.first", "banon")), 2);
+            Query tq2 = new BoostQuery(new TermQuery(new Term("name.last", "banon")), 3);
             expected.add(new DisjunctionMaxQuery(Arrays.<Query>asList(tq1, tq2), 0f), Occur.SHOULD);
             assertEquals(expected.build(), rewrittenQuery);
         }
@@ -2053,18 +2056,7 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
         IndexQueryParserService queryParser = queryParser();
         String query = jsonBuilder().startObject().startObject("function_score")
                 .startArray("functions")
-                .startObject().field("weight", 2).field("boost_factor", 2).endObject()
-                .endArray()
-                .endObject().endObject().string();
-        try {
-            queryParser.parse(query).query();
-            fail("Expect exception here because boost_factor must not have a weight");
-        } catch (QueryParsingException e) {
-            assertThat(e.getDetailedMessage(), containsString(BoostScoreFunction.BOOST_WEIGHT_ERROR_MESSAGE));
-        }
-        query = jsonBuilder().startObject().startObject("function_score")
-                .startArray("functions")
-                .startObject().field("boost_factor",2).endObject()
+                .startObject().startObject("script_score").field("script", "3").endObject().endObject()
                 .endArray()
                 .field("weight", 2)
                 .endObject().endObject().string();
@@ -2077,7 +2069,7 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
         query = jsonBuilder().startObject().startObject("function_score")
                 .field("weight", 2)
                 .startArray("functions")
-                .startObject().field("boost_factor",2).endObject()
+                .startObject().endObject()
                 .endArray()
                 .endObject().endObject().string();
         try {
diff --git a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryIT.java b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryIT.java
index 0076e3b..0c9fc74 100644
--- a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryIT.java
+++ b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryIT.java
@@ -99,7 +99,7 @@ public class TemplateQueryIT extends ESIntegTestCase {
                 "        }\n" +
                 "    }\n" +
                 "}";
-        SearchResponse sr = client().prepareSearch().setSource(request)
+        SearchResponse sr = client().prepareSearch().setSource(new BytesArray(request))
                 .execute().actionGet();
         assertNoFailures(sr);
         assertThat(sr.getHits().hits().length, equalTo(0));
@@ -115,7 +115,7 @@ public class TemplateQueryIT extends ESIntegTestCase {
                 "    \"size\":0" +
                 "}";
 
-        sr = client().prepareSearch().setSource(request)
+        sr = client().prepareSearch().setSource(new BytesArray(request))
                 .execute().actionGet();
         assertNoFailures(sr);
         assertThat(sr.getHits().hits().length, equalTo(0));
diff --git a/core/src/test/java/org/elasticsearch/index/search/geo/GeoHashUtilsTests.java b/core/src/test/java/org/elasticsearch/index/search/geo/GeoHashUtilsTests.java
deleted file mode 100644
index 4410981..0000000
--- a/core/src/test/java/org/elasticsearch/index/search/geo/GeoHashUtilsTests.java
+++ /dev/null
@@ -1,153 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.search.geo;
-
-import org.elasticsearch.common.geo.GeoHashUtils;
-import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.List;
-
-/**
- *
- */
-public class GeoHashUtilsTests extends ESTestCase {
-
-    /**
-     * Pass condition: lat=42.6, lng=-5.6 should be encoded as "ezs42e44yx96",
-     * lat=57.64911 lng=10.40744 should be encoded as "u4pruydqqvj8"
-     */
-    @Test
-    public void testEncode() {
-        String hash = GeoHashUtils.encode(42.6, -5.6);
-        assertEquals("ezs42e44yx96", hash);
-
-        hash = GeoHashUtils.encode(57.64911, 10.40744);
-        assertEquals("u4pruydqqvj8", hash);
-    }
-
-    /**
-     * Pass condition: lat=52.3738007, lng=4.8909347 should be encoded and then
-     * decoded within 0.00001 of the original value
-     */
-    @Test
-    public void testDecodePreciseLongitudeLatitude() {
-        String hash = GeoHashUtils.encode(52.3738007, 4.8909347);
-
-        GeoPoint point = GeoHashUtils.decode(hash);
-
-        assertEquals(52.3738007, point.lat(), 0.00001D);
-        assertEquals(4.8909347, point.lon(), 0.00001D);
-    }
-
-    /**
-     * Pass condition: lat=84.6, lng=10.5 should be encoded and then decoded
-     * within 0.00001 of the original value
-     */
-    @Test
-    public void testDecodeImpreciseLongitudeLatitude() {
-        String hash = GeoHashUtils.encode(84.6, 10.5);
-
-        GeoPoint point = GeoHashUtils.decode(hash);
-
-        assertEquals(84.6, point.lat(), 0.00001D);
-        assertEquals(10.5, point.lon(), 0.00001D);
-    }
-
-    /*
-    * see https://issues.apache.org/jira/browse/LUCENE-1815 for details
-    */
-
-    @Test
-    public void testDecodeEncode() {
-        String geoHash = "u173zq37x014";
-        assertEquals(geoHash, GeoHashUtils.encode(52.3738007, 4.8909347));
-        GeoPoint decode = GeoHashUtils.decode(geoHash);
-        assertEquals(52.37380061d, decode.lat(), 0.000001d);
-        assertEquals(4.8909343d, decode.lon(), 0.000001d);
-
-        assertEquals(geoHash, GeoHashUtils.encode(decode.lat(), decode.lon()));
-    }
-
-    @Test
-    public void testNeighbours() {
-        String geohash = "gcpv";
-        List<String> expectedNeighbors = new ArrayList<>();
-        expectedNeighbors.add("gcpw");
-        expectedNeighbors.add("gcpy");
-        expectedNeighbors.add("u10n");
-        expectedNeighbors.add("gcpt");
-        expectedNeighbors.add("u10j");
-        expectedNeighbors.add("gcps");
-        expectedNeighbors.add("gcpu");
-        expectedNeighbors.add("u10h");
-        Collection<? super String> neighbors = new ArrayList<>();
-        GeoHashUtils.addNeighbors(geohash, neighbors );
-        assertEquals(expectedNeighbors, neighbors);
-
-        // Border odd geohash
-        geohash = "u09x";
-        expectedNeighbors = new ArrayList<>();
-        expectedNeighbors.add("u0c2");
-        expectedNeighbors.add("u0c8");
-        expectedNeighbors.add("u0cb");
-        expectedNeighbors.add("u09r");
-        expectedNeighbors.add("u09z");
-        expectedNeighbors.add("u09q");
-        expectedNeighbors.add("u09w");
-        expectedNeighbors.add("u09y");
-        neighbors = new ArrayList<>();
-        GeoHashUtils.addNeighbors(geohash, neighbors );
-        assertEquals(expectedNeighbors, neighbors);
-
-        // Border even geohash
-        geohash = "u09tv";
-        expectedNeighbors = new ArrayList<>();
-        expectedNeighbors.add("u09wh");
-        expectedNeighbors.add("u09wj");
-        expectedNeighbors.add("u09wn");
-        expectedNeighbors.add("u09tu");
-        expectedNeighbors.add("u09ty");
-        expectedNeighbors.add("u09ts");
-        expectedNeighbors.add("u09tt");
-        expectedNeighbors.add("u09tw");
-        neighbors = new ArrayList<>();
-        GeoHashUtils.addNeighbors(geohash, neighbors );
-        assertEquals(expectedNeighbors, neighbors);
-
-        // Border even and odd geohash
-        geohash = "ezzzz";
-        expectedNeighbors = new ArrayList<>();
-        expectedNeighbors.add("gbpbn");
-        expectedNeighbors.add("gbpbp");
-        expectedNeighbors.add("u0000");
-        expectedNeighbors.add("ezzzy");
-        expectedNeighbors.add("spbpb");
-        expectedNeighbors.add("ezzzw");
-        expectedNeighbors.add("ezzzx");
-        expectedNeighbors.add("spbp8");
-        neighbors = new ArrayList<>();
-        GeoHashUtils.addNeighbors(geohash, neighbors );
-        assertEquals(expectedNeighbors, neighbors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/search/geo/GeoPointParsingTests.java b/core/src/test/java/org/elasticsearch/index/search/geo/GeoPointParsingTests.java
index 3089884..191d2e7 100644
--- a/core/src/test/java/org/elasticsearch/index/search/geo/GeoPointParsingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/search/geo/GeoPointParsingTests.java
@@ -20,8 +20,8 @@
 package org.elasticsearch.index.search.geo;
 
 
+import org.apache.lucene.util.XGeoHashUtils;
 import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.common.geo.GeoHashUtils;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.geo.GeoUtils;
 import org.elasticsearch.common.xcontent.XContentBuilder;
@@ -54,7 +54,7 @@ public class GeoPointParsingTests  extends ESTestCase {
         assertCloseTo(point.resetLat(0), 0, 0);
         assertCloseTo(point.resetLon(lon), 0, lon);
         assertCloseTo(point.resetLon(0), 0, 0);
-        assertCloseTo(point.resetFromGeoHash(GeoHashUtils.encode(lat, lon)), lat, lon);
+        assertCloseTo(point.resetFromGeoHash(XGeoHashUtils.stringEncode(lon, lat)), lat, lon);
         assertCloseTo(point.reset(0, 0), 0, 0);
         assertCloseTo(point.resetFromString(Double.toString(lat) + ", " + Double.toHexString(lon)), lat, lon);
         assertCloseTo(point.reset(0, 0), 0, 0);
@@ -98,7 +98,7 @@ public class GeoPointParsingTests  extends ESTestCase {
     public void testInvalidPointLatHashMix() throws IOException {
         XContentBuilder content = JsonXContent.contentBuilder();
         content.startObject();
-        content.field("lat", 0).field("geohash", GeoHashUtils.encode(0, 0));
+        content.field("lat", 0).field("geohash", XGeoHashUtils.stringEncode(0, 0));
         content.endObject();
 
         XContentParser parser = JsonXContent.jsonXContent.createParser(content.bytes());
@@ -111,7 +111,7 @@ public class GeoPointParsingTests  extends ESTestCase {
     public void testInvalidPointLonHashMix() throws IOException {
         XContentBuilder content = JsonXContent.contentBuilder();
         content.startObject();
-        content.field("lon", 0).field("geohash", GeoHashUtils.encode(0, 0));
+        content.field("lon", 0).field("geohash", XGeoHashUtils.stringEncode(0, 0));
         content.endObject();
 
         XContentParser parser = JsonXContent.jsonXContent.createParser(content.bytes());
@@ -161,7 +161,7 @@ public class GeoPointParsingTests  extends ESTestCase {
 
     private static XContentParser geohash(double lat, double lon) throws IOException {
         XContentBuilder content = JsonXContent.contentBuilder();
-        content.value(GeoHashUtils.encode(lat, lon));
+        content.value(XGeoHashUtils.stringEncode(lon, lat));
         XContentParser parser = JsonXContent.jsonXContent.createParser(content.bytes());
         parser.nextToken();
         return parser;
diff --git a/core/src/test/java/org/elasticsearch/index/search/geo/GeoUtilsTests.java b/core/src/test/java/org/elasticsearch/index/search/geo/GeoUtilsTests.java
index 326b144..bf5b7f9 100644
--- a/core/src/test/java/org/elasticsearch/index/search/geo/GeoUtilsTests.java
+++ b/core/src/test/java/org/elasticsearch/index/search/geo/GeoUtilsTests.java
@@ -24,6 +24,7 @@ import com.spatial4j.core.distance.DistanceUtils;
 import org.apache.lucene.spatial.prefix.tree.Cell;
 import org.apache.lucene.spatial.prefix.tree.GeohashPrefixTree;
 import org.apache.lucene.spatial.prefix.tree.QuadPrefixTree;
+import org.apache.lucene.util.XGeoHashUtils;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.geo.GeoPoint;
@@ -432,7 +433,7 @@ public class GeoUtilsTests extends ESTestCase {
     @Test
     public void testParseGeoPoint_geohash() throws IOException {
         for (int i = 0; i < 100; i++) {
-            int geoHashLength = randomIntBetween(1, 20);
+            int geoHashLength = randomIntBetween(1, XGeoHashUtils.PRECISION);
             StringBuilder geohashBuilder = new StringBuilder(geoHashLength);
             for (int j = 0; j < geoHashLength; j++) {
                 geohashBuilder.append(BASE_32[randomInt(BASE_32.length - 1)]);
@@ -442,7 +443,7 @@ public class GeoUtilsTests extends ESTestCase {
             parser.nextToken();
             GeoPoint point = GeoUtils.parseGeoPoint(parser);
             assertThat(point.lat(), allOf(lessThanOrEqualTo(90.0), greaterThanOrEqualTo(-90.0)));
-            assertThat(point.lon(), allOf(lessThanOrEqualTo(180.0), greaterThan(-180.0)));
+            assertThat(point.lon(), allOf(lessThanOrEqualTo(180.0), greaterThanOrEqualTo(-180.0)));
             jsonBytes = jsonBuilder().startObject().field("geohash", geohashBuilder.toString()).endObject().bytes();
             parser = XContentHelper.createParser(jsonBytes);
             while (parser.currentToken() != Token.VALUE_STRING) {
@@ -450,7 +451,7 @@ public class GeoUtilsTests extends ESTestCase {
             }
             point = GeoUtils.parseGeoPoint(parser);
             assertThat(point.lat(), allOf(lessThanOrEqualTo(90.0), greaterThanOrEqualTo(-90.0)));
-            assertThat(point.lon(), allOf(lessThanOrEqualTo(180.0), greaterThan(-180.0)));
+            assertThat(point.lon(), allOf(lessThanOrEqualTo(180.0), greaterThanOrEqualTo(-180.0)));
         }
     }
 
diff --git a/core/src/test/java/org/elasticsearch/indices/IndicesLifecycleListenerIT.java b/core/src/test/java/org/elasticsearch/indices/IndicesLifecycleListenerIT.java
index 75eb00d..3e95685 100644
--- a/core/src/test/java/org/elasticsearch/indices/IndicesLifecycleListenerIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/IndicesLifecycleListenerIT.java
@@ -25,6 +25,7 @@ import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.ShardRoutingState;
 import org.elasticsearch.cluster.routing.allocation.command.MoveAllocationCommand;
+import org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.settings.Settings;
@@ -48,7 +49,6 @@ import java.util.function.BooleanSupplier;
 
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.cluster.routing.allocation.decider.DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_ALLOCATION;
 import static org.elasticsearch.common.settings.Settings.builder;
 import static org.elasticsearch.index.shard.IndexShardState.CLOSED;
 import static org.elasticsearch.index.shard.IndexShardState.CREATED;
@@ -167,14 +167,14 @@ public class IndicesLifecycleListenerIT extends ESIntegTestCase {
         //add a node: 3 out of the 6 shards will be relocated to it
         //disable allocation before starting a new node, as we need to register the listener first
         assertAcked(client().admin().cluster().prepareUpdateSettings()
-                .setPersistentSettings(builder().put(CLUSTER_ROUTING_ALLOCATION_DISABLE_ALLOCATION, true)));
+                .setPersistentSettings(builder().put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, "none")));
         String node2 = internalCluster().startNode();
         IndexShardStateChangeListener stateChangeListenerNode2 = new IndexShardStateChangeListener();
         //add a listener that keeps track of the shard state changes
         internalCluster().getInstance(IndicesLifecycle.class, node2).addListener(stateChangeListenerNode2);
         //re-enable allocation
         assertAcked(client().admin().cluster().prepareUpdateSettings()
-                .setPersistentSettings(builder().put(CLUSTER_ROUTING_ALLOCATION_DISABLE_ALLOCATION, false)));
+                .setPersistentSettings(builder().put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, "all")));
         ensureGreen();
 
         //the 3 relocated shards get closed on the first node
diff --git a/core/src/test/java/org/elasticsearch/indices/analysis/PreBuiltAnalyzerIntegrationIT.java b/core/src/test/java/org/elasticsearch/indices/analysis/PreBuiltAnalyzerIntegrationIT.java
index 418083c..208922b 100644
--- a/core/src/test/java/org/elasticsearch/indices/analysis/PreBuiltAnalyzerIntegrationIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/analysis/PreBuiltAnalyzerIntegrationIT.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.indices.analysis;
 
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
@@ -28,7 +29,7 @@ import org.elasticsearch.test.ESBackcompatTestCase;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
-import java.lang.reflect.Field;
+import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.HashMap;
@@ -161,44 +162,18 @@ public class PreBuiltAnalyzerIntegrationIT extends ESIntegTestCase {
         }
     }
 
-    // the close() method of a lucene analyzer sets the storedValue field to null
-    // we simply check this via reflection - ugly but works
-    private void assertLuceneAnalyzersAreNotClosed(Map<PreBuiltAnalyzers, List<Version>> loadedAnalyzers) throws IllegalAccessException, NoSuchFieldException {
+    // ensure analyzers are still open by checking there is no ACE
+    private void assertLuceneAnalyzersAreNotClosed(Map<PreBuiltAnalyzers, List<Version>> loadedAnalyzers) throws IOException {
         for (Map.Entry<PreBuiltAnalyzers, List<Version>> preBuiltAnalyzerEntry : loadedAnalyzers.entrySet()) {
-            PreBuiltAnalyzers preBuiltAnalyzer = preBuiltAnalyzerEntry.getKey();
             for (Version version : preBuiltAnalyzerEntry.getValue()) {
                 Analyzer analyzer = preBuiltAnalyzerEntry.getKey().getCache().get(version);
-
-                Field field = getFieldFromClass("storedValue", analyzer);
-                boolean currentAccessible = field.isAccessible();
-                field.setAccessible(true);
-                Object storedValue = field.get(analyzer);
-                field.setAccessible(currentAccessible);
-
-                assertThat(String.format(Locale.ROOT, "Analyzer %s in version %s seems to be closed", preBuiltAnalyzer.name(), version), storedValue, is(notNullValue()));
+                try (TokenStream stream = analyzer.tokenStream("foo", "bar")) {
+                    stream.reset();
+                    while (stream.incrementToken()) {
+                    }
+                    stream.end();
+                }
             }
         }
     }
-
-    /**
-     * Searches for a field until it finds, loops through all superclasses
-     */
-    private Field getFieldFromClass(String fieldName, Object obj) {
-        Field field = null;
-        boolean storedValueFieldFound = false;
-        Class clazz = obj.getClass();
-        while (!storedValueFieldFound) {
-            try {
-                field = clazz.getDeclaredField(fieldName);
-                storedValueFieldFound = true;
-            } catch (NoSuchFieldException e) {
-                clazz = clazz.getSuperclass();
-            }
-
-            if (Object.class.equals(clazz)) throw new RuntimeException("Could not find storedValue field in class" + clazz);
-        }
-
-        return field;
-    }
-
 }
diff --git a/core/src/test/java/org/elasticsearch/indices/analyze/HunspellServiceIT.java b/core/src/test/java/org/elasticsearch/indices/analyze/HunspellServiceIT.java
index d21a840..96fc85a 100644
--- a/core/src/test/java/org/elasticsearch/indices/analyze/HunspellServiceIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/analyze/HunspellServiceIT.java
@@ -29,8 +29,6 @@ import org.elasticsearch.test.ESIntegTestCase.Scope;
 import org.hamcrest.Matchers;
 import org.junit.Test;
 
-import java.lang.reflect.Field;
-
 import static org.elasticsearch.indices.analysis.HunspellService.*;
 import static org.hamcrest.Matchers.notNullValue;
 
@@ -114,10 +112,8 @@ public class HunspellServiceIT extends ESIntegTestCase {
         }
     }
 
-    // TODO: open up a getter on Dictionary
+    // TODO: on next upgrade of lucene, just use new getter
     private void assertIgnoreCase(boolean expected, Dictionary dictionary) throws Exception {
-        Field f = Dictionary.class.getDeclaredField("ignoreCase");
-        f.setAccessible(true);
-        assertEquals(expected, f.getBoolean(dictionary));
+        // assertEquals(expected, dictionary.getIgnoreCase());
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/indices/recovery/IndexRecoveryIT.java b/core/src/test/java/org/elasticsearch/indices/recovery/IndexRecoveryIT.java
index 396d20e..6609ce8 100644
--- a/core/src/test/java/org/elasticsearch/indices/recovery/IndexRecoveryIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/recovery/IndexRecoveryIT.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.indices.recovery;
 
-import com.google.common.util.concurrent.ListenableFuture;
 import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
 import org.elasticsearch.action.admin.cluster.node.stats.NodeStats;
 import org.elasticsearch.action.admin.cluster.node.stats.NodesStatsResponse;
@@ -50,6 +49,7 @@ import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.snapshots.SnapshotState;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
+import org.elasticsearch.test.InternalTestCluster;
 import org.elasticsearch.test.junit.annotations.TestLogging;
 import org.elasticsearch.test.store.MockFSDirectoryService;
 import org.elasticsearch.test.transport.MockTransportService;
@@ -539,8 +539,8 @@ public class IndexRecoveryIT extends ESIntegTestCase {
         // start a master node
         internalCluster().startNode(nodeSettings);
 
-        ListenableFuture<String> blueFuture = internalCluster().startNodeAsync(Settings.builder().put("node.color", "blue").put(nodeSettings).build());
-        ListenableFuture<String> redFuture = internalCluster().startNodeAsync(Settings.builder().put("node.color", "red").put(nodeSettings).build());
+        InternalTestCluster.Async<String> blueFuture = internalCluster().startNodeAsync(Settings.builder().put("node.color", "blue").put(nodeSettings).build());
+        InternalTestCluster.Async<String> redFuture = internalCluster().startNodeAsync(Settings.builder().put("node.color", "red").put(nodeSettings).build());
         final String blueNodeName = blueFuture.get();
         final String redNodeName = redFuture.get();
 
diff --git a/core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationIT.java b/core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationIT.java
index c5ceb77..7a8d650 100644
--- a/core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationIT.java
@@ -235,9 +235,9 @@ public class IndicesStoreIntegrationIT extends ESIntegTestCase {
     @Test
     @TestLogging("cluster.service:TRACE")
     public void testShardActiveElsewhereDoesNotDeleteAnother() throws Exception {
-        Future<String> masterFuture = internalCluster().startNodeAsync(
+        InternalTestCluster.Async<String> masterFuture = internalCluster().startNodeAsync(
                 Settings.builder().put("node.master", true, "node.data", false).build());
-        Future<List<String>> nodesFutures = internalCluster().startNodesAsync(4,
+        InternalTestCluster.Async<List<String>> nodesFutures = internalCluster().startNodesAsync(4,
                 Settings.builder().put("node.master", false, "node.data", true).build());
 
         final String masterNode = masterFuture.get();
diff --git a/core/src/test/java/org/elasticsearch/network/DirectBufferNetworkIT.java b/core/src/test/java/org/elasticsearch/network/DirectBufferNetworkIT.java
deleted file mode 100644
index 6d79de9..0000000
--- a/core/src/test/java/org/elasticsearch/network/DirectBufferNetworkIT.java
+++ /dev/null
@@ -1,145 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.network;
-
-import org.elasticsearch.action.index.IndexRequestBuilder;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.hamcrest.Matchers;
-import org.junit.Test;
-
-import java.io.ByteArrayOutputStream;
-import java.lang.reflect.Field;
-import java.nio.ByteBuffer;
-
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-
-/**
- */
-public class DirectBufferNetworkIT extends ESIntegTestCase {
-
-    @Override
-    protected Settings nodeSettings(int nodeOrdinal) {
-        return Settings.builder()
-            .put(Node.HTTP_ENABLED, true)
-            .put(super.nodeSettings(nodeOrdinal)).build();
-    }
-
-    /**
-     * This test validates that using large data sets (large docs + large API requests) don't
-     * cause a large direct byte buffer to be allocated internally in the sun.nio buffer cache.
-     * <p/>
-     * See {@link org.elasticsearch.common.netty.NettyUtils#DEFAULT_GATHERING} for more info.
-     */
-    @Test
-    public void verifySaneDirectBufferAllocations() throws Exception {
-        createIndex("test");
-
-        int estimatedBytesSize = scaledRandomIntBetween(ByteSizeValue.parseBytesSizeValue("1.1mb", "estimatedBytesSize").bytesAsInt(),
-                                                        ByteSizeValue.parseBytesSizeValue("1.5mb", "estimatedBytesSize").bytesAsInt());
-        byte[] data = new byte[estimatedBytesSize];
-        getRandom().nextBytes(data);
-
-        ByteArrayOutputStream docOut = new ByteArrayOutputStream();
-        // we use smile to automatically use the binary mapping
-        XContentBuilder doc = XContentFactory.smileBuilder(docOut).startObject().startObject("doc").field("value", data).endObject();
-        doc.close();
-        byte[] docBytes = docOut.toByteArray();
-
-        int numDocs = randomIntBetween(2, 5);
-        logger.info("indexing [{}] docs, each with size [{}]", numDocs, estimatedBytesSize);
-        IndexRequestBuilder[] builders = new IndexRequestBuilder[numDocs];
-        for (int i = 0; i < numDocs; ++i) {
-            builders[i] = client().prepareIndex("test", "type").setSource(docBytes);
-        }
-        indexRandom(true, builders);
-        logger.info("done indexing");
-
-        logger.info("executing random client search for all docs");
-        assertHitCount(client().prepareSearch("test").setFrom(0).setSize(numDocs).get(), numDocs);
-        logger.info("executing transport client search for all docs");
-        assertHitCount(internalCluster().transportClient().prepareSearch("test").setFrom(0).setSize(numDocs).get(), numDocs);
-
-        logger.info("executing HTTP search for all docs");
-        // simulate large HTTP call as well
-        httpClient().method("GET").path("/test/_search").addParam("size", Integer.toString(numDocs)).execute();
-
-        logger.info("validating large direct buffer not allocated");
-        validateNoLargeDirectBufferAllocated();
-    }
-    
-    /**
-     * Validates that all the thread local allocated ByteBuffer in sun.nio under the Util$BufferCache
-     * are not greater than 1mb.
-     */
-    private void validateNoLargeDirectBufferAllocated() throws Exception {
-        // Make the fields in the Thread class that store ThreadLocals
-        // accessible
-        Field threadLocalsField = Thread.class.getDeclaredField("threadLocals");
-        threadLocalsField.setAccessible(true);
-        // Make the underlying array of ThreadLoad.ThreadLocalMap.Entry objects
-        // accessible
-        Class<?> tlmClass = Class.forName("java.lang.ThreadLocal$ThreadLocalMap");
-        Field tableField = tlmClass.getDeclaredField("table");
-        tableField.setAccessible(true);
-
-        for (Thread thread : Thread.getAllStackTraces().keySet()) {
-            if (thread == null) {
-                continue;
-            }
-            Object threadLocalMap = threadLocalsField.get(thread);
-            if (threadLocalMap == null) {
-                continue;
-            }
-            Object[] table = (Object[]) tableField.get(threadLocalMap);
-            if (table == null) {
-                continue;
-            }
-            for (Object entry : table) {
-                if (entry == null) {
-                    continue;
-                }
-                Field valueField = entry.getClass().getDeclaredField("value");
-                valueField.setAccessible(true);
-                Object value = valueField.get(entry);
-                if (value == null) {
-                    continue;
-                }
-                if (!value.getClass().getName().equals("sun.nio.ch.Util$BufferCache")) {
-                    continue;
-                }
-                Field buffersField = value.getClass().getDeclaredField("buffers");
-                buffersField.setAccessible(true);
-                Object[] buffers = (Object[]) buffersField.get(value);
-                for (Object buffer : buffers) {
-                    if (buffer == null) {
-                        continue;
-                    }
-                    assertThat(((ByteBuffer) buffer).capacity(), Matchers.lessThan(1 * 1024 * 1024));
-                }
-            }
-        }
-
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java b/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java
index 5b0aa3f..6eca391 100644
--- a/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java
+++ b/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java
@@ -44,7 +44,7 @@ import org.elasticsearch.index.percolator.PercolatorException;
 import org.elasticsearch.index.query.MatchQueryBuilder;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.QueryParsingException;
-import org.elasticsearch.index.query.functionscore.factor.FactorBuilder;
+import org.elasticsearch.index.query.functionscore.weight.WeightBuilder;
 import org.elasticsearch.index.query.support.QueryInnerHitBuilder;
 import org.elasticsearch.rest.RestStatus;
 import org.elasticsearch.script.Script;
@@ -54,43 +54,16 @@ import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.NavigableSet;
-import java.util.Set;
-import java.util.TreeSet;
+import java.util.*;
 
 import static org.elasticsearch.action.percolate.PercolateSourceBuilder.docBuilder;
 import static org.elasticsearch.common.settings.Settings.builder;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.smileBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.yamlBuilder;
+import static org.elasticsearch.common.xcontent.XContentFactory.*;
 import static org.elasticsearch.index.query.QueryBuilders.*;
 import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.scriptFunction;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAllSuccessful;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertMatchCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.hamcrest.Matchers.anyOf;
-import static org.hamcrest.Matchers.arrayContaining;
-import static org.hamcrest.Matchers.arrayContainingInAnyOrder;
-import static org.hamcrest.Matchers.arrayWithSize;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.emptyArray;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
 
 /**
  *
@@ -1446,7 +1419,7 @@ public class PercolatorIT extends ESIntegTestCase {
                 .setSize(5)
                 .setPercolateDoc(docBuilder().setDoc(jsonBuilder().startObject().field("field1", "The quick brown fox jumps over the lazy dog").endObject()))
                 .setHighlightBuilder(new HighlightBuilder().field("field1"))
-                .setPercolateQuery(functionScoreQuery(matchAllQuery()).add(new FactorBuilder().boostFactor(5.5f)))
+                .setPercolateQuery(functionScoreQuery(matchAllQuery()).add(new WeightBuilder().setWeight(5.5f)))
                 .setScore(true)
                 .execute().actionGet();
         assertNoFailures(response);
@@ -1478,7 +1451,7 @@ public class PercolatorIT extends ESIntegTestCase {
                 .setSize(5)
                 .setPercolateDoc(docBuilder().setDoc(jsonBuilder().startObject().field("field1", "The quick brown fox jumps over the lazy dog").endObject()))
                 .setHighlightBuilder(new HighlightBuilder().field("field1"))
-                .setPercolateQuery(functionScoreQuery(matchAllQuery()).add(new FactorBuilder().boostFactor(5.5f)))
+                .setPercolateQuery(functionScoreQuery(matchAllQuery()).add(new WeightBuilder().setWeight(5.5f)))
                 .setSortByScore(true)
                 .execute().actionGet();
         assertMatchCount(response, 5l);
@@ -1510,7 +1483,7 @@ public class PercolatorIT extends ESIntegTestCase {
                 .setSize(5)
                 .setPercolateDoc(docBuilder().setDoc(jsonBuilder().startObject().field("field1", "The quick brown fox jumps over the lazy dog").endObject()))
                 .setHighlightBuilder(new HighlightBuilder().field("field1").highlightQuery(QueryBuilders.matchQuery("field1", "jumps")))
-                .setPercolateQuery(functionScoreQuery(matchAllQuery()).add(new FactorBuilder().boostFactor(5.5f)))
+                .setPercolateQuery(functionScoreQuery(matchAllQuery()).add(new WeightBuilder().setWeight(5.5f)))
                 .setSortByScore(true)
                 .execute().actionGet();
         assertMatchCount(response, 5l);
@@ -1547,7 +1520,7 @@ public class PercolatorIT extends ESIntegTestCase {
                 .setSize(5)
                 .setGetRequest(Requests.getRequest("test").type("type").id("1"))
                 .setHighlightBuilder(new HighlightBuilder().field("field1"))
-                .setPercolateQuery(functionScoreQuery(matchAllQuery()).add(new FactorBuilder().boostFactor(5.5f)))
+                .setPercolateQuery(functionScoreQuery(matchAllQuery()).add(new WeightBuilder().setWeight(5.5f)))
                 .setSortByScore(true)
                 .execute().actionGet();
         assertMatchCount(response, 5l);
diff --git a/core/src/test/java/org/elasticsearch/plugins/PluginManagerCliTests.java b/core/src/test/java/org/elasticsearch/plugins/PluginManagerCliTests.java
index 0c9db6c..f21609a 100644
--- a/core/src/test/java/org/elasticsearch/plugins/PluginManagerCliTests.java
+++ b/core/src/test/java/org/elasticsearch/plugins/PluginManagerCliTests.java
@@ -19,12 +19,16 @@
 
 package org.elasticsearch.plugins;
 
+import org.elasticsearch.common.cli.CliTool;
 import org.elasticsearch.common.cli.CliToolTestCase;
 import org.junit.Test;
 
 import java.io.IOException;
+import java.net.MalformedURLException;
+import java.nio.file.Path;
 
 import static org.elasticsearch.common.cli.CliTool.ExitStatus.OK_AND_EXIT;
+import static org.elasticsearch.common.cli.CliTool.ExitStatus.IO_ERROR;
 import static org.hamcrest.Matchers.*;
 
 public class PluginManagerCliTests extends CliToolTestCase {
@@ -50,4 +54,13 @@ public class PluginManagerCliTests extends CliToolTestCase {
         assertThat(new PluginManagerCliParser(terminal).execute(args("list -h")), is(OK_AND_EXIT));
         assertTerminalOutputContainsHelpFile(terminal, "/org/elasticsearch/plugins/plugin-list.help");
     }
+
+    public void testUrlSpacesInPath() throws MalformedURLException {
+        CliToolTestCase.CaptureOutputTerminal terminal = new CliToolTestCase.CaptureOutputTerminal();
+        Path tmpDir = createTempDir().resolve("foo deps");
+        String finalDir = tmpDir.toAbsolutePath().toUri().toURL().toString();
+        logger.warn(finalDir);
+        CliTool.ExitStatus execute = new PluginManagerCliParser(terminal).execute(args("install " + finalDir));
+        assertThat(execute.status(), is(IO_ERROR.status()));
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/recovery/RelocationIT.java b/core/src/test/java/org/elasticsearch/recovery/RelocationIT.java
index 3328a46..b8b8551 100644
--- a/core/src/test/java/org/elasticsearch/recovery/RelocationIT.java
+++ b/core/src/test/java/org/elasticsearch/recovery/RelocationIT.java
@@ -21,7 +21,6 @@ package org.elasticsearch.recovery;
 
 import com.carrotsearch.hppc.IntHashSet;
 import com.carrotsearch.hppc.procedures.IntProcedure;
-import com.google.common.util.concurrent.ListenableFuture;
 import org.apache.lucene.index.IndexFileNames;
 import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
 import org.elasticsearch.action.admin.cluster.state.ClusterStateResponse;
@@ -59,6 +58,7 @@ import org.elasticsearch.search.SearchHits;
 import org.elasticsearch.test.BackgroundIndexer;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
+import org.elasticsearch.test.InternalTestCluster;
 import org.elasticsearch.test.junit.annotations.TestLogging;
 import org.elasticsearch.test.transport.MockTransportService;
 import org.elasticsearch.transport.Transport;
@@ -362,11 +362,12 @@ public class RelocationIT extends ESIntegTestCase {
     }
 
     @Test
+    @AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch/issues/13542")
     public void testMoveShardsWhileRelocation() throws Exception {
         final String indexName = "test";
 
-        ListenableFuture<String> blueFuture = internalCluster().startNodeAsync(Settings.builder().put("node.color", "blue").build());
-        ListenableFuture<String> redFuture = internalCluster().startNodeAsync(Settings.builder().put("node.color", "red").build());
+        InternalTestCluster.Async<String> blueFuture = internalCluster().startNodeAsync(Settings.builder().put("node.color", "blue").build());
+        InternalTestCluster.Async<String> redFuture = internalCluster().startNodeAsync(Settings.builder().put("node.color", "red").build());
         internalCluster().startNode(Settings.builder().put("node.color", "green").build());
         final String blueNodeName = blueFuture.get();
         final String redNodeName = redFuture.get();
diff --git a/core/src/test/java/org/elasticsearch/script/GroovyScriptIT.java b/core/src/test/java/org/elasticsearch/script/GroovyScriptIT.java
index ed4c999..c24d499 100644
--- a/core/src/test/java/org/elasticsearch/script/GroovyScriptIT.java
+++ b/core/src/test/java/org/elasticsearch/script/GroovyScriptIT.java
@@ -22,6 +22,7 @@ package org.elasticsearch.script;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.lucene.search.function.CombineFunction;
 import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.groovy.GroovyScriptEngineService;
@@ -53,9 +54,9 @@ public class GroovyScriptIT extends ESIntegTestCase {
 
     public void assertScript(String script) {
         SearchResponse resp = client().prepareSearch("test")
-                .setSource("{\"query\": {\"match_all\": {}}," +
+                .setSource(new BytesArray("{\"query\": {\"match_all\": {}}," +
                         "\"sort\":{\"_script\": {\"script\": \""+ script +
-                        "; 1\", \"type\": \"number\", \"lang\": \"groovy\"}}}").get();
+                        "; 1\", \"type\": \"number\", \"lang\": \"groovy\"}}}")).get();
         assertNoFailures(resp);
     }
 
diff --git a/core/src/test/java/org/elasticsearch/script/GroovySecurityIT.java b/core/src/test/java/org/elasticsearch/script/GroovySecurityIT.java
index 847fe28..a97bc5a 100644
--- a/core/src/test/java/org/elasticsearch/script/GroovySecurityIT.java
+++ b/core/src/test/java/org/elasticsearch/script/GroovySecurityIT.java
@@ -22,6 +22,7 @@ package org.elasticsearch.script;
 import org.apache.lucene.util.Constants;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.search.ShardSearchFailure;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
@@ -106,9 +107,9 @@ public class GroovySecurityIT extends ESIntegTestCase {
     private void assertSuccess(String script) {
         logger.info("--> script: " + script);
         SearchResponse resp = client().prepareSearch("test")
-                .setSource("{\"query\": {\"match_all\": {}}," +
-                        "\"sort\":{\"_script\": {\"script\": \""+ script +
-                        "; doc['foo'].value + 2\", \"type\": \"number\", \"lang\": \"groovy\"}}}").get();
+                .setSource(new BytesArray("{\"query\": {\"match_all\": {}}," +
+                        "\"sort\":{\"_script\": {\"script\": \"" + script +
+                        "; doc['foo'].value + 2\", \"type\": \"number\", \"lang\": \"groovy\"}}}")).get();
         assertNoFailures(resp);
         assertEquals(1, resp.getHits().getTotalHits());
         assertThat(resp.getHits().getAt(0).getSortValues(), equalTo(new Object[]{7.0}));
@@ -117,9 +118,9 @@ public class GroovySecurityIT extends ESIntegTestCase {
     private void assertFailure(String script) {
         logger.info("--> script: " + script);
         SearchResponse resp = client().prepareSearch("test")
-                 .setSource("{\"query\": {\"match_all\": {}}," +
-                            "\"sort\":{\"_script\": {\"script\": \""+ script +
-                            "; doc['foo'].value + 2\", \"type\": \"number\", \"lang\": \"groovy\"}}}").get();
+                 .setSource(new BytesArray("{\"query\": {\"match_all\": {}}," +
+                         "\"sort\":{\"_script\": {\"script\": \"" + script +
+                         "; doc['foo'].value + 2\", \"type\": \"number\", \"lang\": \"groovy\"}}}")).get();
         assertEquals(0, resp.getHits().getTotalHits());
         ShardSearchFailure fails[] = resp.getShardFailures();
         // TODO: GroovyScriptExecutionException needs work:
diff --git a/core/src/test/java/org/elasticsearch/script/IndexedScriptIT.java b/core/src/test/java/org/elasticsearch/script/IndexedScriptIT.java
index 1142206..d82ae3d 100644
--- a/core/src/test/java/org/elasticsearch/script/IndexedScriptIT.java
+++ b/core/src/test/java/org/elasticsearch/script/IndexedScriptIT.java
@@ -24,6 +24,7 @@ import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptResponse;
 import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.script.expression.ExpressionScriptEngineService;
@@ -81,7 +82,7 @@ public class IndexedScriptIT extends ESIntegTestCase {
 
         indexRandom(true, builders);
         String query = "{ \"query\" : { \"match_all\": {}} , \"script_fields\" : { \"test1\" : { \"script_id\" : \"script1\", \"lang\":\"groovy\" }, \"test2\" : { \"script_id\" : \"script2\", \"lang\":\"groovy\", \"params\":{\"factor\":3}  }}, size:1}";
-        SearchResponse searchResponse = client().prepareSearch().setSource(query).setIndices("test").setTypes("scriptTest").get();
+        SearchResponse searchResponse = client().prepareSearch().setSource(new BytesArray(query)).setIndices("test").setTypes("scriptTest").get();
         assertHitCount(searchResponse, 5);
         assertTrue(searchResponse.getHits().hits().length == 1);
         SearchHit sh = searchResponse.getHits().getAt(0);
@@ -106,7 +107,7 @@ public class IndexedScriptIT extends ESIntegTestCase {
             String query = "{"
                     + " \"query\" : { \"match_all\": {}}, "
                     + " \"script_fields\" : { \"test_field\" : { \"script_id\" : \"script1\", \"lang\":\"groovy\" } } }";    
-            SearchResponse searchResponse = client().prepareSearch().setSource(query).setIndices("test_index").setTypes("test_type").get();
+            SearchResponse searchResponse = client().prepareSearch().setSource(new BytesArray(query)).setIndices("test_index").setTypes("test_type").get();
             assertHitCount(searchResponse, 1);
             SearchHit sh = searchResponse.getHits().getAt(0);
             assertThat((Integer)sh.field("test_field").getValue(), equalTo(i));
@@ -143,7 +144,7 @@ public class IndexedScriptIT extends ESIntegTestCase {
         client().prepareIndex("test", "scriptTest", "1").setSource("{\"theField\":\"foo\"}").get();
         refresh();
         String source = "{\"aggs\": {\"test\": { \"terms\" : { \"script_id\":\"script1\" } } } }";
-        SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setSource(new BytesArray(source)).get();
         assertHitCount(searchResponse, 1);
         assertThat(searchResponse.getAggregations().get("test"), notNullValue());
     }
@@ -166,14 +167,14 @@ public class IndexedScriptIT extends ESIntegTestCase {
         }
         try {
             String query = "{ \"script_fields\" : { \"test1\" : { \"script_id\" : \"script1\", \"lang\":\"expression\" }}}";
-            client().prepareSearch().setSource(query).setIndices("test").setTypes("scriptTest").get();
+            client().prepareSearch().setSource(new BytesArray(query)).setIndices("test").setTypes("scriptTest").get();
             fail("search script should have been rejected");
         } catch(Exception e) {
             assertThat(e.toString(), containsString("scripts of type [indexed], operation [search] and lang [expression] are disabled"));
         }
         try {
             String source = "{\"aggs\": {\"test\": { \"terms\" : { \"script_id\":\"script1\", \"script_lang\":\"expression\" } } } }";
-            client().prepareSearch("test").setSource(source).get();
+            client().prepareSearch("test").setSource(new BytesArray(source)).get();
         } catch(Exception e) {
             assertThat(e.toString(), containsString("scripts of type [indexed], operation [aggs] and lang [expression] are disabled"));
         }
diff --git a/core/src/test/java/org/elasticsearch/script/NativeScriptTests.java b/core/src/test/java/org/elasticsearch/script/NativeScriptTests.java
index e97b97f..cf7f7b1 100644
--- a/core/src/test/java/org/elasticsearch/script/NativeScriptTests.java
+++ b/core/src/test/java/org/elasticsearch/script/NativeScriptTests.java
@@ -95,7 +95,7 @@ public class NativeScriptTests extends ESTestCase {
         }
     }
 
-    static class MyNativeScriptFactory implements NativeScriptFactory {
+    public static class MyNativeScriptFactory implements NativeScriptFactory {
         @Override
         public ExecutableScript newScript(@Nullable Map<String, Object> params) {
             return new MyScript();
diff --git a/core/src/test/java/org/elasticsearch/script/OnDiskScriptIT.java b/core/src/test/java/org/elasticsearch/script/OnDiskScriptIT.java
index 636c833..617ae01 100644
--- a/core/src/test/java/org/elasticsearch/script/OnDiskScriptIT.java
+++ b/core/src/test/java/org/elasticsearch/script/OnDiskScriptIT.java
@@ -20,6 +20,7 @@ package org.elasticsearch.script;
 
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 import org.elasticsearch.search.SearchHit;
@@ -62,7 +63,7 @@ public class OnDiskScriptIT extends ESIntegTestCase {
         indexRandom(true, builders);
 
         String query = "{ \"query\" : { \"match_all\": {}} , \"script_fields\" : { \"test1\" : { \"script_file\" : \"script1\" }, \"test2\" : { \"script_file\" : \"script2\", \"params\":{\"factor\":3}  }}, size:1}";
-        SearchResponse searchResponse = client().prepareSearch().setSource(query).setIndices("test").setTypes("scriptTest").get();
+        SearchResponse searchResponse = client().prepareSearch().setSource(new BytesArray(query)).setIndices("test").setTypes("scriptTest").get();
         assertHitCount(searchResponse, 5);
         assertTrue(searchResponse.getHits().hits().length == 1);
         SearchHit sh = searchResponse.getHits().getAt(0);
@@ -81,7 +82,7 @@ public class OnDiskScriptIT extends ESIntegTestCase {
         indexRandom(true, builders);
 
         String query = "{ \"query\" : { \"match_all\": {}} , \"script_fields\" : { \"test1\" : { \"script_file\" : \"script1\" }, \"test2\" : { \"script_file\" : \"script1\", \"lang\":\"expression\"  }}, size:1}";
-        SearchResponse searchResponse = client().prepareSearch().setSource(query).setIndices("test").setTypes("scriptTest").get();
+        SearchResponse searchResponse = client().prepareSearch().setSource(new BytesArray(query)).setIndices("test").setTypes("scriptTest").get();
         assertHitCount(searchResponse, 5);
         assertTrue(searchResponse.getHits().hits().length == 1);
         SearchHit sh = searchResponse.getHits().getAt(0);
@@ -103,14 +104,14 @@ public class OnDiskScriptIT extends ESIntegTestCase {
 
         String source = "{\"aggs\": {\"test\": { \"terms\" : { \"script_file\":\"script1\", \"lang\": \"expression\" } } } }";
         try {
-            client().prepareSearch("test").setSource(source).get();
+            client().prepareSearch("test").setSource(new BytesArray(source)).get();
             fail("aggs script should have been rejected");
         } catch(Exception e) {
             assertThat(e.toString(), containsString("scripts of type [file], operation [aggs] and lang [expression] are disabled"));
         }
 
         String query = "{ \"query\" : { \"match_all\": {}} , \"script_fields\" : { \"test1\" : { \"script_file\" : \"script1\", \"lang\":\"expression\" }}, size:1}";
-        SearchResponse searchResponse = client().prepareSearch().setSource(query).setIndices("test").setTypes("scriptTest").get();
+        SearchResponse searchResponse = client().prepareSearch().setSource(new BytesArray(query)).setIndices("test").setTypes("scriptTest").get();
         assertHitCount(searchResponse, 5);
         assertTrue(searchResponse.getHits().hits().length == 1);
         SearchHit sh = searchResponse.getHits().getAt(0);
@@ -124,14 +125,14 @@ public class OnDiskScriptIT extends ESIntegTestCase {
         refresh();
         String source = "{\"aggs\": {\"test\": { \"terms\" : { \"script_file\":\"script1\", \"lang\": \"mustache\" } } } }";
         try {
-            client().prepareSearch("test").setSource(source).get();
+            client().prepareSearch("test").setSource(new BytesArray(source)).get();
             fail("aggs script should have been rejected");
         } catch(Exception e) {
             assertThat(e.toString(), containsString("scripts of type [file], operation [aggs] and lang [mustache] are disabled"));
         }
         String query = "{ \"query\" : { \"match_all\": {}} , \"script_fields\" : { \"test1\" : { \"script_file\" : \"script1\", \"lang\":\"mustache\" }}, size:1}";
         try {
-            client().prepareSearch().setSource(query).setIndices("test").setTypes("scriptTest").get();
+            client().prepareSearch().setSource(new BytesArray(query)).setIndices("test").setTypes("scriptTest").get();
             fail("search script should have been rejected");
         } catch(Exception e) {
             assertThat(e.toString(), containsString("scripts of type [file], operation [search] and lang [mustache] are disabled"));
diff --git a/core/src/test/java/org/elasticsearch/script/ScriptFieldIT.java b/core/src/test/java/org/elasticsearch/script/ScriptFieldIT.java
index d3f0923..4fdfbb0 100644
--- a/core/src/test/java/org/elasticsearch/script/ScriptFieldIT.java
+++ b/core/src/test/java/org/elasticsearch/script/ScriptFieldIT.java
@@ -77,7 +77,7 @@ public class ScriptFieldIT extends ESIntegTestCase {
         }
     }
 
-    static class IntArrayScriptFactory implements NativeScriptFactory {
+    public static class IntArrayScriptFactory implements NativeScriptFactory {
         @Override
         public ExecutableScript newScript(@Nullable Map<String, Object> params) {
             return new IntScript();
@@ -96,7 +96,7 @@ public class ScriptFieldIT extends ESIntegTestCase {
         }
     }
 
-    static class LongArrayScriptFactory implements NativeScriptFactory {
+    public static class LongArrayScriptFactory implements NativeScriptFactory {
         @Override
         public ExecutableScript newScript(@Nullable Map<String, Object> params) {
             return new LongScript();
@@ -115,7 +115,7 @@ public class ScriptFieldIT extends ESIntegTestCase {
         }
     }
 
-    static class FloatArrayScriptFactory implements NativeScriptFactory {
+    public static class FloatArrayScriptFactory implements NativeScriptFactory {
         @Override
         public ExecutableScript newScript(@Nullable Map<String, Object> params) {
             return new FloatScript();
@@ -134,7 +134,7 @@ public class ScriptFieldIT extends ESIntegTestCase {
         }
     }
 
-    static class DoubleArrayScriptFactory implements NativeScriptFactory {
+    public static class DoubleArrayScriptFactory implements NativeScriptFactory {
         @Override
         public ExecutableScript newScript(@Nullable Map<String, Object> params) {
             return new DoubleScript();
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java
index 04f5400..d5834c9 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java
@@ -51,8 +51,7 @@ import java.util.concurrent.TimeUnit;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.*;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
 import static org.hamcrest.Matchers.*;
 import static org.hamcrest.core.IsNull.notNullValue;
 
@@ -100,9 +99,9 @@ public class DateHistogramIT extends ESIntegTestCase {
         assertAcked(prepareCreate("empty_bucket_idx").addMapping("type", "value", "type=integer"));
         List<IndexRequestBuilder> builders = new ArrayList<>();
         for (int i = 0; i < 2; i++) {
-            builders.add(client().prepareIndex("empty_bucket_idx", "type", ""+i).setSource(jsonBuilder()
+            builders.add(client().prepareIndex("empty_bucket_idx", "type", "" + i).setSource(jsonBuilder()
                     .startObject()
-                    .field("value", i*2)
+                    .field("value", i * 2)
                     .endObject()));
         }
         builders.addAll(Arrays.asList(
@@ -167,9 +166,9 @@ public class DateHistogramIT extends ESIntegTestCase {
 
     @Test
     public void singleValuedField_WithTimeZone() throws Exception {
-         SearchResponse response = client().prepareSearch("idx")
-                    .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(1).timeZone("+01:00")).execute()
-                    .actionGet();
+        SearchResponse response = client().prepareSearch("idx")
+                .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.DAY).minDocCount(1).timeZone("+01:00")).execute()
+                .actionGet();
         DateTimeZone tz = DateTimeZone.forID("+01:00");
         assertSearchResponse(response);
 
@@ -252,7 +251,7 @@ public class DateHistogramIT extends ESIntegTestCase {
                 .addAggregation(dateHistogram("histo")
                         .field("date")
                         .interval(DateHistogramInterval.MONTH)
-.order(Histogram.Order.KEY_DESC))
+                        .order(Histogram.Order.KEY_DESC))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -275,7 +274,7 @@ public class DateHistogramIT extends ESIntegTestCase {
                 .addAggregation(dateHistogram("histo")
                         .field("date")
                         .interval(DateHistogramInterval.MONTH)
-.order(Histogram.Order.COUNT_ASC))
+                        .order(Histogram.Order.COUNT_ASC))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -298,7 +297,7 @@ public class DateHistogramIT extends ESIntegTestCase {
                 .addAggregation(dateHistogram("histo")
                         .field("date")
                         .interval(DateHistogramInterval.MONTH)
-.order(Histogram.Order.COUNT_DESC))
+                        .order(Histogram.Order.COUNT_DESC))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -319,7 +318,7 @@ public class DateHistogramIT extends ESIntegTestCase {
     public void singleValuedField_WithSubAggregation() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo").field("date").interval(DateHistogramInterval.MONTH)
-                    .subAggregation(sum("sum").field("value")))
+                        .subAggregation(sum("sum").field("value")))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -425,7 +424,7 @@ public class DateHistogramIT extends ESIntegTestCase {
                 .addAggregation(dateHistogram("histo")
                         .field("date")
                         .interval(DateHistogramInterval.MONTH)
-                                .order(Histogram.Order.aggregation("sum", true))
+                        .order(Histogram.Order.aggregation("sum", true))
                         .subAggregation(max("sum").field("value")))
                 .execute().actionGet();
 
@@ -449,7 +448,7 @@ public class DateHistogramIT extends ESIntegTestCase {
                 .addAggregation(dateHistogram("histo")
                         .field("date")
                         .interval(DateHistogramInterval.MONTH)
-                                .order(Histogram.Order.aggregation("sum", false))
+                        .order(Histogram.Order.aggregation("sum", false))
                         .subAggregation(max("sum").field("value")))
                 .execute().actionGet();
 
@@ -473,7 +472,7 @@ public class DateHistogramIT extends ESIntegTestCase {
                 .addAggregation(dateHistogram("histo")
                         .field("date")
                         .interval(DateHistogramInterval.MONTH)
-                                .order(Histogram.Order.aggregation("stats", "sum", true))
+                        .order(Histogram.Order.aggregation("stats", "sum", true))
                         .subAggregation(stats("stats").field("value")))
                 .execute().actionGet();
 
@@ -497,7 +496,7 @@ public class DateHistogramIT extends ESIntegTestCase {
                 .addAggregation(dateHistogram("histo")
                         .field("date")
                         .interval(DateHistogramInterval.MONTH)
-                                .order(Histogram.Order.aggregation("stats", "sum", false))
+                        .order(Histogram.Order.aggregation("stats", "sum", false))
                         .subAggregation(stats("stats").field("value")))
                 .execute().actionGet();
 
@@ -520,8 +519,8 @@ public class DateHistogramIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("date")
-.script(new Script("new DateTime(_value).plusMonths(1).getMillis()"))
-                                .interval(DateHistogramInterval.MONTH)).execute().actionGet();
+                        .script(new Script("new DateTime(_value).plusMonths(1).getMillis()"))
+                        .interval(DateHistogramInterval.MONTH)).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -614,7 +613,7 @@ public class DateHistogramIT extends ESIntegTestCase {
                 .addAggregation(dateHistogram("histo")
                         .field("dates")
                         .interval(DateHistogramInterval.MONTH)
-.order(Histogram.Order.COUNT_DESC))
+                        .order(Histogram.Order.COUNT_DESC))
                 .execute().actionGet();
 
         assertSearchResponse(response);
@@ -646,7 +645,7 @@ public class DateHistogramIT extends ESIntegTestCase {
 
     /**
      * The script will change to document date values to the following:
-     *
+     * <p/>
      * doc 1: [ Feb 2, Mar 3]
      * doc 2: [ Mar 2, Apr 3]
      * doc 3: [ Mar 15, Apr 16]
@@ -659,8 +658,8 @@ public class DateHistogramIT extends ESIntegTestCase {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("dates")
-                                .script(new Script("new DateTime(_value, DateTimeZone.UTC).plusMonths(1).getMillis()"))
-                                .interval(DateHistogramInterval.MONTH)).execute().actionGet();
+                        .script(new Script("new DateTime(_value, DateTimeZone.UTC).plusMonths(1).getMillis()"))
+                        .interval(DateHistogramInterval.MONTH)).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -701,22 +700,21 @@ public class DateHistogramIT extends ESIntegTestCase {
 
     /**
      * The script will change to document date values to the following:
-     *
+     * <p/>
      * doc 1: [ Feb 2, Mar 3]
      * doc 2: [ Mar 2, Apr 3]
      * doc 3: [ Mar 15, Apr 16]
      * doc 4: [ Apr 2, May 3]
      * doc 5: [ Apr 15, May 16]
      * doc 6: [ Apr 23, May 24]
-     *
      */
     @Test
     public void multiValuedField_WithValueScript_WithInheritedSubAggregator() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
                         .field("dates")
-                                .script(new Script("new DateTime((long)_value, DateTimeZone.UTC).plusMonths(1).getMillis()"))
-                                .interval(DateHistogramInterval.MONTH).subAggregation(max("max"))).execute().actionGet();
+                        .script(new Script("new DateTime((long)_value, DateTimeZone.UTC).plusMonths(1).getMillis()"))
+                        .interval(DateHistogramInterval.MONTH).subAggregation(max("max"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -815,8 +813,8 @@ public class DateHistogramIT extends ESIntegTestCase {
     public void script_SingleValue_WithSubAggregator_Inherited() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
-.script(new Script("doc['date'].value")).interval(DateHistogramInterval.MONTH)
-                                .subAggregation(max("max"))).execute().actionGet();
+                        .script(new Script("doc['date'].value")).interval(DateHistogramInterval.MONTH)
+                        .subAggregation(max("max"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -915,8 +913,8 @@ public class DateHistogramIT extends ESIntegTestCase {
     public void script_MultiValued_WithAggregatorInherited() throws Exception {
         SearchResponse response = client().prepareSearch("idx")
                 .addAggregation(dateHistogram("histo")
-.script(new Script("doc['dates'].values")).interval(DateHistogramInterval.MONTH)
-                                .subAggregation(max("max"))).execute().actionGet();
+                        .script(new Script("doc['dates'].values")).interval(DateHistogramInterval.MONTH)
+                        .subAggregation(max("max"))).execute().actionGet();
 
         assertSearchResponse(response);
 
@@ -1154,7 +1152,7 @@ public class DateHistogramIT extends ESIntegTestCase {
                             .field("date")
                             .interval(DateHistogramInterval.days(interval))
                             .minDocCount(0)
-                            // when explicitly specifying a format, the extended bounds should be defined by the same format
+                                    // when explicitly specifying a format, the extended bounds should be defined by the same format
                             .extendedBounds(format(boundsMin, pattern), format(boundsMax, pattern))
                             .format(pattern))
                     .execute().actionGet();
@@ -1232,7 +1230,7 @@ public class DateHistogramIT extends ESIntegTestCase {
                 .addAggregation(
                         dateHistogram("histo").field("date").interval(DateHistogramInterval.hours(1)).timeZone(timezone.getID()).minDocCount(0)
                                 .extendedBounds("now/d", "now/d+23h")
-                                ).execute().actionGet();
+                ).execute().actionGet();
         assertSearchResponse(response);
 
         assertThat("Expected 24 buckets for one day aggregation with hourly interval", response.getHits().totalHits(), equalTo(2l));
@@ -1246,7 +1244,7 @@ public class DateHistogramIT extends ESIntegTestCase {
         for (int i = 0; i < buckets.size(); i++) {
             Histogram.Bucket bucket = buckets.get(i);
             assertThat(bucket, notNullValue());
-            assertThat("Bucket " + i +" had wrong key", (DateTime) bucket.getKey(), equalTo(new DateTime(timeZoneStartToday.getMillis() + (i * 60 * 60 * 1000), DateTimeZone.UTC)));
+            assertThat("Bucket " + i + " had wrong key", (DateTime) bucket.getKey(), equalTo(new DateTime(timeZoneStartToday.getMillis() + (i * 60 * 60 * 1000), DateTimeZone.UTC)));
             if (i == 0 || i == 12) {
                 assertThat(bucket.getDocCount(), equalTo(1l));
             } else {
@@ -1274,7 +1272,7 @@ public class DateHistogramIT extends ESIntegTestCase {
                         .interval(DateHistogramInterval.DAY))
                 .execute().actionGet();
 
-        assertThat(response.getHits().getTotalHits(), equalTo(5l));
+        assertSearchHits(response, "0", "1", "2", "3", "4");
 
         Histogram histo = response.getAggregations().get("date_histo");
         List<? extends Histogram.Bucket> buckets = histo.getBuckets();
@@ -1349,7 +1347,7 @@ public class DateHistogramIT extends ESIntegTestCase {
         ensureSearchable("test8209");
         SearchResponse response = client().prepareSearch("test8209")
                 .addAggregation(dateHistogram("histo").field("d").interval(DateHistogramInterval.MONTH).timeZone("CET")
-                .minDocCount(0))
+                        .minDocCount(0))
                 .execute().actionGet();
         assertSearchResponse(response);
         Histogram histo = response.getAggregations().get("histo");
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridIT.java
index d385ea9..e0601f2 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridIT.java
@@ -20,11 +20,13 @@ package org.elasticsearch.search.aggregations.bucket;
 
 import com.carrotsearch.hppc.ObjectIntHashMap;
 import com.carrotsearch.hppc.ObjectIntMap;
+import com.carrotsearch.hppc.ObjectObjectHashMap;
+import com.carrotsearch.hppc.ObjectObjectMap;
 import com.carrotsearch.hppc.cursors.ObjectIntCursor;
 
+import org.apache.lucene.util.XGeoHashUtils;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.common.geo.GeoHashUtils;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.query.GeoBoundingBoxQueryBuilder;
@@ -54,7 +56,7 @@ public class GeoHashGridIT extends ESIntegTestCase {
 
     static ObjectIntMap<String> expectedDocCountsForGeoHash = null;
     static ObjectIntMap<String> multiValuedExpectedDocCountsForGeoHash = null;
-    static int highestPrecisionGeohash = 12;
+    static ObjectObjectMap<String, GeoPoint> expectedCentroidsForGeoHash = null;
     static int numDocs = 100;
 
     static String smallestGeoHash = null;
@@ -72,6 +74,15 @@ public class GeoHashGridIT extends ESIntegTestCase {
         return indexCity(index, name, Arrays.<String>asList(latLon));
     }
 
+    private GeoPoint updateCentroid(GeoPoint centroid, double lat, double lon, final int docCount) {
+        if (centroid == null) {
+            return new GeoPoint(lat, lon);
+        }
+        final double newLon = centroid.lon() + (lon - centroid.lon()) / docCount;
+        final double newLat = centroid.lat() + (lat - centroid.lat()) / docCount;
+        return centroid.reset(newLat, newLon);
+    }
+
     @Override
     public void setupSuiteScopeCluster() throws Exception {
         createIndex("idx_unmapped");
@@ -82,21 +93,26 @@ public class GeoHashGridIT extends ESIntegTestCase {
         List<IndexRequestBuilder> cities = new ArrayList<>();
         Random random = getRandom();
         expectedDocCountsForGeoHash = new ObjectIntHashMap<>(numDocs * 2);
+        expectedCentroidsForGeoHash = new ObjectObjectHashMap<>(numDocs *2);
         for (int i = 0; i < numDocs; i++) {
             //generate random point
             double lat = (180d * random.nextDouble()) - 90d;
             double lng = (360d * random.nextDouble()) - 180d;
-            String randomGeoHash = GeoHashUtils.encode(lat, lng, highestPrecisionGeohash);
+            String randomGeoHash = XGeoHashUtils.stringEncode(lng, lat, XGeoHashUtils.PRECISION);
             //Index at the highest resolution
             cities.add(indexCity("idx", randomGeoHash, lat + ", " + lng));
             expectedDocCountsForGeoHash.put(randomGeoHash, expectedDocCountsForGeoHash.getOrDefault(randomGeoHash, 0) + 1);
+            expectedCentroidsForGeoHash.put(randomGeoHash, updateCentroid(expectedCentroidsForGeoHash.getOrDefault(randomGeoHash,
+                    null), lat, lng, expectedDocCountsForGeoHash.get(randomGeoHash)));
             //Update expected doc counts for all resolutions..
-            for (int precision = highestPrecisionGeohash - 1; precision > 0; precision--) {
-                String hash = GeoHashUtils.encode(lat, lng, precision);
+            for (int precision = XGeoHashUtils.PRECISION - 1; precision > 0; precision--) {
+                String hash = XGeoHashUtils.stringEncode(lng, lat, precision);
                 if ((smallestGeoHash == null) || (hash.length() < smallestGeoHash.length())) {
                     smallestGeoHash = hash;
                 }
                 expectedDocCountsForGeoHash.put(hash, expectedDocCountsForGeoHash.getOrDefault(hash, 0) + 1);
+                expectedCentroidsForGeoHash.put(hash, updateCentroid(expectedCentroidsForGeoHash.getOrDefault(hash,
+                        null), lat, lng, expectedDocCountsForGeoHash.get(hash)));
             }
         }
         indexRandom(true, cities);
@@ -115,8 +131,8 @@ public class GeoHashGridIT extends ESIntegTestCase {
                 double lng = (360d * random.nextDouble()) - 180d;
                 points.add(lat + "," + lng);
                 // Update expected doc counts for all resolutions..
-                for (int precision = highestPrecisionGeohash; precision > 0; precision--) {
-                    final String geoHash = GeoHashUtils.encode(lat, lng, precision);
+                for (int precision = XGeoHashUtils.PRECISION; precision > 0; precision--) {
+                    final String geoHash = XGeoHashUtils.stringEncode(lng, lat, precision);
                     geoHashes.add(geoHash);
                 }
             }
@@ -133,7 +149,7 @@ public class GeoHashGridIT extends ESIntegTestCase {
 
     @Test
     public void simple() throws Exception {
-        for (int precision = 1; precision <= highestPrecisionGeohash; precision++) {
+        for (int precision = 1; precision <= XGeoHashUtils.PRECISION; precision++) {
             SearchResponse response = client().prepareSearch("idx")
                     .addAggregation(geohashGrid("geohashgrid")
                             .field("location")
@@ -153,11 +169,15 @@ public class GeoHashGridIT extends ESIntegTestCase {
 
                 long bucketCount = cell.getDocCount();
                 int expectedBucketCount = expectedDocCountsForGeoHash.get(geohash);
+                GeoPoint centroid = cell.getCentroid();
+                GeoPoint expectedCentroid = expectedCentroidsForGeoHash.get(geohash);
                 assertNotSame(bucketCount, 0);
                 assertEquals("Geohash " + geohash + " has wrong doc count ",
                         expectedBucketCount, bucketCount);
+                assertEquals("Geohash " + geohash + " has wrong centroid ",
+                        expectedCentroid, centroid);
                 GeoPoint geoPoint = (GeoPoint) propertiesKeys[i];
-                assertThat(GeoHashUtils.encode(geoPoint.lat(), geoPoint.lon(), precision), equalTo(geohash));
+                assertThat(XGeoHashUtils.stringEncode(geoPoint.lon(), geoPoint.lat(), precision), equalTo(geohash));
                 assertThat((long) propertiesDocCounts[i], equalTo(bucketCount));
             }
         }
@@ -165,7 +185,7 @@ public class GeoHashGridIT extends ESIntegTestCase {
 
     @Test
     public void multivalued() throws Exception {
-        for (int precision = 1; precision <= highestPrecisionGeohash; precision++) {
+        for (int precision = 1; precision <= XGeoHashUtils.PRECISION; precision++) {
             SearchResponse response = client().prepareSearch("multi_valued_idx")
                     .addAggregation(geohashGrid("geohashgrid")
                             .field("location")
@@ -192,7 +212,7 @@ public class GeoHashGridIT extends ESIntegTestCase {
     public void filtered() throws Exception {
         GeoBoundingBoxQueryBuilder bbox = new GeoBoundingBoxQueryBuilder("location");
         bbox.topLeft(smallestGeoHash).bottomRight(smallestGeoHash).queryName("bbox");
-        for (int precision = 1; precision <= highestPrecisionGeohash; precision++) {
+        for (int precision = 1; precision <= XGeoHashUtils.PRECISION; precision++) {
             SearchResponse response = client().prepareSearch("idx")
                     .addAggregation(
                             AggregationBuilders.filter("filtered").filter(bbox)
@@ -224,7 +244,7 @@ public class GeoHashGridIT extends ESIntegTestCase {
 
     @Test
     public void unmapped() throws Exception {
-        for (int precision = 1; precision <= highestPrecisionGeohash; precision++) {
+        for (int precision = 1; precision <= XGeoHashUtils.PRECISION; precision++) {
             SearchResponse response = client().prepareSearch("idx_unmapped")
                     .addAggregation(geohashGrid("geohashgrid")
                             .field("location")
@@ -242,7 +262,7 @@ public class GeoHashGridIT extends ESIntegTestCase {
 
     @Test
     public void partiallyUnmapped() throws Exception {
-        for (int precision = 1; precision <= highestPrecisionGeohash; precision++) {
+        for (int precision = 1; precision <= XGeoHashUtils.PRECISION; precision++) {
             SearchResponse response = client().prepareSearch("idx", "idx_unmapped")
                     .addAggregation(geohashGrid("geohashgrid")
                             .field("location")
@@ -267,7 +287,7 @@ public class GeoHashGridIT extends ESIntegTestCase {
 
     @Test
     public void testTopMatch() throws Exception {
-        for (int precision = 1; precision <= highestPrecisionGeohash; precision++) {
+        for (int precision = 1; precision <= XGeoHashUtils.PRECISION; precision++) {
             SearchResponse response = client().prepareSearch("idx")
                     .addAggregation(geohashGrid("geohashgrid")
                             .field("location")
@@ -301,7 +321,7 @@ public class GeoHashGridIT extends ESIntegTestCase {
     @Test
     // making sure this doesn't runs into an OOME
     public void sizeIsZero() {
-        for (int precision = 1; precision <= highestPrecisionGeohash; precision++) {
+        for (int precision = 1; precision <= XGeoHashUtils.PRECISION; precision++) {
             final int size = randomBoolean() ? 0 : randomIntBetween(1, Integer.MAX_VALUE);
             final int shardSize = randomBoolean() ? -1 : 0;
             SearchResponse response = client().prepareSearch("idx")
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardReduceIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardReduceIT.java
index 3436b4c..b71ee5b 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardReduceIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardReduceIT.java
@@ -18,9 +18,9 @@
  */
 package org.elasticsearch.search.aggregations.bucket;
 
+import org.apache.lucene.util.XGeoHashUtils;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.common.geo.GeoHashUtils;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
 import org.elasticsearch.search.aggregations.bucket.filter.Filter;
@@ -65,7 +65,7 @@ public class ShardReduceIT extends ESIntegTestCase {
                 .startObject()
                 .field("value", value)
                 .field("ip", "10.0.0." + value)
-                .field("location", GeoHashUtils.encode(52, 5, 12))
+                .field("location", XGeoHashUtils.stringEncode(5, 52, XGeoHashUtils.PRECISION))
                 .field("date", date)
                 .field("term-l", 1)
                 .field("term-d", 1.5)
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TopHitsIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TopHitsIT.java
index e0d56d1..ffab844 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TopHitsIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TopHitsIT.java
@@ -24,6 +24,7 @@ import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.search.SearchType;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.script.Script;
@@ -614,8 +615,8 @@ public class TopHitsIT extends ESIntegTestCase {
                 "}";
         try {
             client().prepareSearch("idx").setTypes("type")
-                    .setSource(source)
-                    .get();
+                    .setSource(new BytesArray(source))
+                            .get();
             fail();
         } catch (SearchPhaseExecutionException e) {
             assertThat(e.toString(), containsString("Aggregator [top_tags_hits] of type [top_hits] cannot accept sub-aggregations"));
diff --git a/core/src/test/java/org/elasticsearch/search/basic/TransportSearchFailuresIT.java b/core/src/test/java/org/elasticsearch/search/basic/TransportSearchFailuresIT.java
index 5ea8b40..c7005db 100644
--- a/core/src/test/java/org/elasticsearch/search/basic/TransportSearchFailuresIT.java
+++ b/core/src/test/java/org/elasticsearch/search/basic/TransportSearchFailuresIT.java
@@ -30,6 +30,7 @@ import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.Requests;
 import org.elasticsearch.common.Priority;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
@@ -66,7 +67,7 @@ public class TransportSearchFailuresIT extends ESIntegTestCase {
         assertThat(refreshResponse.getFailedShards(), equalTo(0));
         for (int i = 0; i < 5; i++) {
             try {
-                SearchResponse searchResponse = client().search(searchRequest("test").source("{ xxx }".getBytes(Charsets.UTF_8))).actionGet();
+                SearchResponse searchResponse = client().search(searchRequest("test").source(new BytesArray("{ xxx }"))).actionGet();
                 assertThat(searchResponse.getTotalShards(), equalTo(test.numPrimaries));
                 assertThat(searchResponse.getSuccessfulShards(), equalTo(0));
                 assertThat(searchResponse.getFailedShards(), equalTo(test.numPrimaries));
@@ -95,7 +96,7 @@ public class TransportSearchFailuresIT extends ESIntegTestCase {
 
         for (int i = 0; i < 5; i++) {
             try {
-                SearchResponse searchResponse = client().search(searchRequest("test").source("{ xxx }".getBytes(Charsets.UTF_8))).actionGet();
+                SearchResponse searchResponse = client().search(searchRequest("test").source(new BytesArray("{ xxx }"))).actionGet();
                 assertThat(searchResponse.getTotalShards(), equalTo(test.numPrimaries));
                 assertThat(searchResponse.getSuccessfulShards(), equalTo(0));
                 assertThat(searchResponse.getFailedShards(), equalTo(test.numPrimaries));
diff --git a/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java b/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java
index 969a2d7..353ee6d 100644
--- a/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java
@@ -26,6 +26,7 @@ import org.elasticsearch.action.search.MultiSearchResponse;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.client.Requests;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.xcontent.XContentBuilder;
@@ -320,7 +321,7 @@ public class TransportTwoNodesSearchIT extends ESIntegTestCase {
 
 
         //SearchResponse searchResponse = client().search(searchRequest("test").source(source).searchType(DFS_QUERY_AND_FETCH).scroll(new Scroll(timeValueMinutes(10)))).actionGet();
-        SearchResponse searchResponse = client().prepareSearch("test").setSearchType(DFS_QUERY_AND_FETCH).setScroll("10m").setSource(source.buildAsBytes()).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setSearchType(DFS_QUERY_AND_FETCH).setScroll("10m").setSource(source).get();
         assertNoFailures(searchResponse);
         assertThat(searchResponse.getHits().totalHits(), equalTo(100l));
         assertThat(searchResponse.getHits().hits().length, equalTo(60)); // 20 per shard
@@ -377,7 +378,7 @@ public class TransportTwoNodesSearchIT extends ESIntegTestCase {
 
         logger.info("Start Testing failed search with wrong query");
         try {
-            SearchResponse searchResponse = client().search(searchRequest("test").source("{ xxx }".getBytes(Charsets.UTF_8))).actionGet();
+            SearchResponse searchResponse = client().search(searchRequest("test").source(new BytesArray("{ xxx }"))).actionGet();
             assertThat(searchResponse.getTotalShards(), equalTo(test.numPrimaries));
             assertThat(searchResponse.getSuccessfulShards(), equalTo(0));
             assertThat(searchResponse.getFailedShards(), equalTo(test.numPrimaries));
diff --git a/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java b/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java
index 9724e8d..43dd815 100644
--- a/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java
@@ -27,6 +27,7 @@ import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.search.SearchType;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.collect.HppcMaps;
 import org.elasticsearch.common.lucene.search.function.CombineFunction;
 import org.elasticsearch.common.settings.Settings;
@@ -1478,12 +1479,12 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
 
         SearchResponse resp;
         resp = client().prepareSearch("test")
-                .setSource("{\"query\": {\"has_child\": {\"type\": \"posts\", \"query\": {\"match\": {\"field\": \"bar\"}}}}}").get();
+                .setSource(new BytesArray("{\"query\": {\"has_child\": {\"type\": \"posts\", \"query\": {\"match\": {\"field\": \"bar\"}}}}}")).get();
         assertHitCount(resp, 1L);
 
         // Now reverse the order for the type after the query
         resp = client().prepareSearch("test")
-                .setSource("{\"query\": {\"has_child\": {\"query\": {\"match\": {\"field\": \"bar\"}}, \"type\": \"posts\"}}}").get();
+                .setSource(new BytesArray("{\"query\": {\"has_child\": {\"query\": {\"match\": {\"field\": \"bar\"}}, \"type\": \"posts\"}}}")).get();
         assertHitCount(resp, 1L);
 
     }
diff --git a/core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginIT.java b/core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginIT.java
index 6635f1e..e07bb73 100644
--- a/core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginIT.java
+++ b/core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginIT.java
@@ -27,6 +27,7 @@ import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.termvectors.TermVectorsRequest;
 import org.elasticsearch.action.termvectors.TermVectorsResponse;
 import org.elasticsearch.common.Priority;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.plugins.Plugin;
@@ -90,7 +91,7 @@ public class FetchSubPhasePluginIT extends ESIntegTestCase {
         String searchSource = jsonBuilder().startObject()
                 .field("term_vectors_fetch", "test")
                 .endObject().string();
-        SearchResponse response = client().prepareSearch().setSource(searchSource).get();
+        SearchResponse response = client().prepareSearch().setSource(new BytesArray(searchSource)).get();
         assertSearchResponse(response);
         assertThat(((Map<String, Integer>) response.getHits().getAt(0).field("term_vectors_fetch").getValues().get(0)).get("i"), equalTo(2));
         assertThat(((Map<String, Integer>) response.getHits().getAt(0).field("term_vectors_fetch").getValues().get(0)).get("am"), equalTo(2));
diff --git a/core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java b/core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java
index 2d2d728..e285970 100644
--- a/core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java
+++ b/core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java
@@ -53,6 +53,7 @@ import static org.elasticsearch.search.builder.SearchSourceBuilder.searchSource;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
 import static org.hamcrest.Matchers.*;
 
+
 public class DecayFunctionScoreIT extends ESIntegTestCase {
 
     @Test
@@ -348,7 +349,7 @@ public class DecayFunctionScoreIT extends ESIntegTestCase {
         SearchHits sh = sr.getHits();
         assertThat(sh.getTotalHits(), equalTo((long) (1)));
         assertThat(sh.getAt(0).getId(), equalTo("1"));
-        assertThat((double) sh.getAt(0).score(), closeTo(0.30685282, 1.e-5));
+        assertThat((double) sh.getAt(0).score(), closeTo(0.153426408, 1.e-5));
 
         response = client().search(
                 searchRequest().searchType(SearchType.QUERY_THEN_FETCH).source(
@@ -359,7 +360,7 @@ public class DecayFunctionScoreIT extends ESIntegTestCase {
         sh = sr.getHits();
         assertThat(sh.getTotalHits(), equalTo((long) (1)));
         assertThat(sh.getAt(0).getId(), equalTo("1"));
-        assertThat((double) sh.getAt(0).score(), closeTo(1.0, 1.e-5));
+        assertThat((double) sh.getAt(0).score(), closeTo(0.5, 1.e-5));
 
         response = client().search(
                 searchRequest().searchType(SearchType.QUERY_THEN_FETCH).source(
@@ -370,7 +371,7 @@ public class DecayFunctionScoreIT extends ESIntegTestCase {
         sh = sr.getHits();
         assertThat(sh.getTotalHits(), equalTo((long) (1)));
         assertThat(sh.getAt(0).getId(), equalTo("1"));
-        assertThat((double) sh.getAt(0).score(), closeTo(2.0 * (0.30685282 + 0.5), 1.e-5));
+        assertThat((double) sh.getAt(0).score(), closeTo(0.30685282 + 0.5, 1.e-5));
         logger.info("--> Hit[0] {} Explanation:\n {}", sr.getHits().getAt(0).id(), sr.getHits().getAt(0).explanation());
 
         response = client().search(
@@ -382,7 +383,7 @@ public class DecayFunctionScoreIT extends ESIntegTestCase {
         sh = sr.getHits();
         assertThat(sh.getTotalHits(), equalTo((long) (1)));
         assertThat(sh.getAt(0).getId(), equalTo("1"));
-        assertThat((double) sh.getAt(0).score(), closeTo((0.30685282 + 0.5), 1.e-5));
+        assertThat((double) sh.getAt(0).score(), closeTo((0.30685282 + 0.5) / 2, 1.e-5));
 
         response = client().search(
                 searchRequest().searchType(SearchType.QUERY_THEN_FETCH).source(
@@ -393,7 +394,7 @@ public class DecayFunctionScoreIT extends ESIntegTestCase {
         sh = sr.getHits();
         assertThat(sh.getTotalHits(), equalTo((long) (1)));
         assertThat(sh.getAt(0).getId(), equalTo("1"));
-        assertThat((double) sh.getAt(0).score(), closeTo(2.0 * (0.30685282), 1.e-5));
+        assertThat((double) sh.getAt(0).score(), closeTo(0.30685282, 1.e-5));
 
         response = client().search(
                 searchRequest().searchType(SearchType.QUERY_THEN_FETCH).source(
@@ -404,7 +405,7 @@ public class DecayFunctionScoreIT extends ESIntegTestCase {
         sh = sr.getHits();
         assertThat(sh.getTotalHits(), equalTo((long) (1)));
         assertThat(sh.getAt(0).getId(), equalTo("1"));
-        assertThat((double) sh.getAt(0).score(), closeTo(1.0, 1.e-5));
+        assertThat((double) sh.getAt(0).score(), closeTo(0.5, 1.e-5));
 
     }
 
@@ -797,21 +798,8 @@ public class DecayFunctionScoreIT extends ESIntegTestCase {
         refresh();
 
         XContentBuilder query = XContentFactory.jsonBuilder();
-        // query that contains a functions[] array but also a single function
-        query.startObject().startObject("function_score").startArray("functions").startObject().field("boost_factor", "1.3").endObject().endArray().field("boost_factor", "1").endObject().endObject();
-        try {
-            client().search(
-                    searchRequest().source(
-                            searchSource().query(query))).actionGet();
-            fail("Search should result in SearchPhaseExecutionException");
-        } catch (SearchPhaseExecutionException e) {
-            logger.info(e.shardFailures()[0].reason());
-            assertThat(e.shardFailures()[0].reason(), containsString("already found [functions] array, now encountering [boost_factor]. did you mean [boost] instead?"));
-        }
-
-        query = XContentFactory.jsonBuilder();
         // query that contains a single function and a functions[] array
-        query.startObject().startObject("function_score").field("boost_factor", "1").startArray("functions").startObject().field("boost_factor", "1.3").endObject().endArray().endObject().endObject();
+        query.startObject().startObject("function_score").field("weight", "1").startArray("functions").startObject().startObject("script_score").field("script", "3").endObject().endObject().endArray().endObject().endObject();
         try {
             client().search(
                     searchRequest().source(
@@ -819,7 +807,7 @@ public class DecayFunctionScoreIT extends ESIntegTestCase {
             fail("Search should result in SearchPhaseExecutionException");
         } catch (SearchPhaseExecutionException e) {
             logger.info(e.shardFailures()[0].reason());
-            assertThat(e.shardFailures()[0].reason(), containsString("already found [boost_factor], now encountering [functions]. did you mean [boost] instead?"));
+            assertThat(e.shardFailures()[0].reason(), containsString("already found [weight], now encountering [functions]."));
         }
 
         query = XContentFactory.jsonBuilder();
@@ -887,7 +875,7 @@ public class DecayFunctionScoreIT extends ESIntegTestCase {
                 "              \"text\": \"baseball\"\n" +
                 "            }\n" +
                 "          },\n" +
-                "          \"boost_factor\": 2\n" +
+                "          \"weight\": 2\n" +
                 "        },\n" +
                 "        {\n" +
                 "          \"filter\": {\n" +
diff --git a/core/src/test/java/org/elasticsearch/search/functionscore/ExplainableScriptIT.java b/core/src/test/java/org/elasticsearch/search/functionscore/ExplainableScriptIT.java
index ad9c0af..1b942f6 100644
--- a/core/src/test/java/org/elasticsearch/search/functionscore/ExplainableScriptIT.java
+++ b/core/src/test/java/org/elasticsearch/search/functionscore/ExplainableScriptIT.java
@@ -96,7 +96,7 @@ public class ExplainableScriptIT extends ESIntegTestCase {
         }
     }
 
-    static class MyNativeScriptFactory implements NativeScriptFactory {
+    public static class MyNativeScriptFactory implements NativeScriptFactory {
         @Override
         public ExecutableScript newScript(@Nullable Map<String, Object> params) {
             return new MyScript();
diff --git a/core/src/test/java/org/elasticsearch/search/functionscore/FunctionScoreFieldValueIT.java b/core/src/test/java/org/elasticsearch/search/functionscore/FunctionScoreFieldValueIT.java
index 24d0670..6f6a719 100644
--- a/core/src/test/java/org/elasticsearch/search/functionscore/FunctionScoreFieldValueIT.java
+++ b/core/src/test/java/org/elasticsearch/search/functionscore/FunctionScoreFieldValueIT.java
@@ -21,6 +21,7 @@ package org.elasticsearch.search.functionscore;
 
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.lucene.search.function.FieldValueFactorFunction;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
@@ -146,8 +147,8 @@ public class FunctionScoreFieldValueIT extends ESIntegTestCase {
             "  }" +
             "}";
           response = client().prepareSearch("test")
-          .setSource(querySource)
-          .get();
+          .setSource(new BytesArray(querySource))
+                  .get();
           assertFailures(response);
         } catch (SearchPhaseExecutionException e) {
           // This is fine, the query will throw an exception if executed
diff --git a/core/src/test/java/org/elasticsearch/search/functionscore/FunctionScoreIT.java b/core/src/test/java/org/elasticsearch/search/functionscore/FunctionScoreIT.java
index 7e13816..d86e591 100644
--- a/core/src/test/java/org/elasticsearch/search/functionscore/FunctionScoreIT.java
+++ b/core/src/test/java/org/elasticsearch/search/functionscore/FunctionScoreIT.java
@@ -23,6 +23,7 @@ import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.search.SearchType;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.lucene.search.function.FieldValueFactorFunction;
 import org.elasticsearch.common.xcontent.XContentBuilder;
@@ -178,7 +179,7 @@ public class FunctionScoreIT extends ESIntegTestCase {
 
         assertThat(
                 responseWithWeights.getHits().getAt(0).getExplanation().toString(),
-                equalTo("6.0 = function score, product of:\n  1.0 = ConstantScore(text_field:value), product of:\n    1.0 = boost\n    1.0 = queryNorm\n  6.0 = min of:\n    6.0 = function score, score mode [multiply]\n      1.0 = function score, product of:\n        1.0 = match filter: *:*\n        1.0 = Function for field geo_point_field:\n          1.0 = exp(-0.5*pow(MIN of: [Math.max(arcDistance([10.0, 20.0](=doc value),[10.0, 20.0](=origin)) - 0.0(=offset), 0)],2.0)/7.213475204444817E11)\n      2.0 = function score, product of:\n        1.0 = match filter: *:*\n        2.0 = product of:\n          1.0 = field value function: ln(doc['double_field'].value * factor=1.0)\n          2.0 = weight\n      3.0 = function score, product of:\n        1.0 = match filter: *:*\n        3.0 = product of:\n          1.0 = script score function, computed with script:\"[script: _index['text_field']['value'].tf(), type: inline, lang: null, params: null]\n            1.0 = _score: \n              1.0 = ConstantScore(text_field:value), product of:\n                1.0 = boost\n                1.0 = queryNorm\n          3.0 = weight\n    3.4028235E38 = maxBoost\n  1.0 = queryBoost\n"));
+                equalTo("6.0 = function score, product of:\n  1.0 = ConstantScore(text_field:value), product of:\n    1.0 = boost\n    1.0 = queryNorm\n  6.0 = min of:\n    6.0 = function score, score mode [multiply]\n      1.0 = function score, product of:\n        1.0 = match filter: *:*\n        1.0 = Function for field geo_point_field:\n          1.0 = exp(-0.5*pow(MIN of: [Math.max(arcDistance([10.0, 20.0](=doc value),[10.0, 20.0](=origin)) - 0.0(=offset), 0)],2.0)/7.213475204444817E11)\n      2.0 = function score, product of:\n        1.0 = match filter: *:*\n        2.0 = product of:\n          1.0 = field value function: ln(doc['double_field'].value * factor=1.0)\n          2.0 = weight\n      3.0 = function score, product of:\n        1.0 = match filter: *:*\n        3.0 = product of:\n          1.0 = script score function, computed with script:\"[script: _index['text_field']['value'].tf(), type: inline, lang: null, params: null]\n            1.0 = _score: \n              1.0 = ConstantScore(text_field:value), product of:\n                1.0 = boost\n                1.0 = queryNorm\n          3.0 = weight\n    3.4028235E38 = maxBoost\n"));
         responseWithWeights = client().search(
                 searchRequest().source(
                         searchSource().query(
@@ -186,7 +187,7 @@ public class FunctionScoreIT extends ESIntegTestCase {
                                 .explain(true))).actionGet();
         assertThat(
                 responseWithWeights.getHits().getAt(0).getExplanation().toString(),
-                equalTo("4.0 = function score, product of:\n  1.0 = ConstantScore(text_field:value), product of:\n    1.0 = boost\n    1.0 = queryNorm\n  4.0 = min of:\n    4.0 = product of:\n      1.0 = constant score 1.0 - no function provided\n      4.0 = weight\n    3.4028235E38 = maxBoost\n  1.0 = queryBoost\n"));
+                equalTo("4.0 = function score, product of:\n  1.0 = ConstantScore(text_field:value), product of:\n    1.0 = boost\n    1.0 = queryNorm\n  4.0 = min of:\n    4.0 = product of:\n      1.0 = constant score 1.0 - no function provided\n      4.0 = weight\n    3.4028235E38 = maxBoost\n"));
 
     }
 
@@ -378,8 +379,8 @@ public class FunctionScoreIT extends ESIntegTestCase {
                 .endObject()
                 .endObject().string();
         SearchResponse response = client().search(
-                searchRequest().source(query)
-        ).actionGet();
+                searchRequest().source(new BytesArray(query))
+                ).actionGet();
         assertSearchResponse(response);
         assertThat(response.getHits().getAt(0).score(), equalTo(2.0f));
 
@@ -391,7 +392,7 @@ public class FunctionScoreIT extends ESIntegTestCase {
                 .endObject()
                 .endObject().string();
         response = client().search(
-                searchRequest().source(query)
+                searchRequest().source(new BytesArray(query))
         ).actionGet();
         assertSearchResponse(response);
         assertThat(response.getHits().getAt(0).score(), equalTo(2.0f));
diff --git a/core/src/test/java/org/elasticsearch/search/geo/GeoDistanceIT.java b/core/src/test/java/org/elasticsearch/search/geo/GeoDistanceIT.java
index 9c370ef..d59baf1 100644
--- a/core/src/test/java/org/elasticsearch/search/geo/GeoDistanceIT.java
+++ b/core/src/test/java/org/elasticsearch/search/geo/GeoDistanceIT.java
@@ -19,10 +19,10 @@
 
 package org.elasticsearch.search.geo;
 
+import org.apache.lucene.util.XGeoHashUtils;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.geo.GeoDistance;
-import org.elasticsearch.common.geo.GeoHashUtils;
 import org.elasticsearch.common.unit.DistanceUnit;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
@@ -669,7 +669,7 @@ public class GeoDistanceIT extends ESIntegTestCase {
 
         XContentBuilder source = JsonXContent.contentBuilder()
                 .startObject()
-                    .field("pin", GeoHashUtils.encode(lat, lon))
+                    .field("pin", XGeoHashUtils.stringEncode(lon, lat))
                 .endObject();
 
         assertAcked(prepareCreate("locations").addMapping("location", mapping));
diff --git a/core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java b/core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java
index 599772c..4d5ae30 100644
--- a/core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java
+++ b/core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java
@@ -28,6 +28,7 @@ import org.apache.lucene.spatial.prefix.tree.GeohashPrefixTree;
 import org.apache.lucene.spatial.query.SpatialArgs;
 import org.apache.lucene.spatial.query.SpatialOperation;
 import org.apache.lucene.spatial.query.UnsupportedSpatialOperation;
+import org.apache.lucene.util.XGeoHashUtils;
 import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
 import org.elasticsearch.action.bulk.BulkItemResponse;
 import org.elasticsearch.action.bulk.BulkResponse;
@@ -35,7 +36,6 @@ import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.Priority;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.geo.GeoHashUtils;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.geo.GeoUtils;
 import org.elasticsearch.common.geo.builders.MultiPolygonBuilder;
@@ -54,7 +54,14 @@ import java.io.ByteArrayOutputStream;
 import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.io.InputStream;
-import java.util.*;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Random;
 import java.util.zip.GZIPInputStream;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
@@ -458,8 +465,8 @@ public class GeoFilterIT extends ESIntegTestCase {
         String geohash = randomhash(10);
         logger.info("Testing geohash_cell filter for [{}]", geohash);
 
-        Collection<? extends CharSequence> neighbors = GeoHashUtils.neighbors(geohash);
-        Collection<? extends CharSequence> parentNeighbors = GeoHashUtils.neighbors(geohash.substring(0, geohash.length() - 1));
+        Collection<? extends CharSequence> neighbors = XGeoHashUtils.neighbors(geohash);
+        Collection<? extends CharSequence> parentNeighbors = XGeoHashUtils.neighbors(geohash.substring(0, geohash.length() - 1));
 
         logger.info("Neighbors {}", neighbors);
         logger.info("Parent Neighbors {}", parentNeighbors);
@@ -496,7 +503,7 @@ public class GeoFilterIT extends ESIntegTestCase {
         expectedCounts.put(geoHashCellQuery("pin", geohash.substring(0, geohash.length() - 1), true), 2L + neighbors.size() + parentNeighbors.size());
 
         // Testing point formats and precision
-        GeoPoint point = GeoHashUtils.decode(geohash);
+        GeoPoint point = GeoPoint.fromGeohash(geohash);
         int precision = geohash.length();
 
         expectedCounts.put(geoHashCellQuery("pin", point).neighbors(true).precision(precision), 1L + neighbors.size());
@@ -552,24 +559,24 @@ public class GeoFilterIT extends ESIntegTestCase {
     @Test
     public void testNeighbors() {
         // Simple root case
-        assertThat(GeoHashUtils.addNeighbors("7", new ArrayList<String>()), containsInAnyOrder("4", "5", "6", "d", "e", "h", "k", "s"));
+        assertThat(XGeoHashUtils.addNeighbors("7", new ArrayList<String>()), containsInAnyOrder("4", "5", "6", "d", "e", "h", "k", "s"));
 
         // Root cases (Outer cells)
-        assertThat(GeoHashUtils.addNeighbors("0", new ArrayList<String>()), containsInAnyOrder("1", "2", "3", "p", "r"));
-        assertThat(GeoHashUtils.addNeighbors("b", new ArrayList<String>()), containsInAnyOrder("8", "9", "c", "x", "z"));
-        assertThat(GeoHashUtils.addNeighbors("p", new ArrayList<String>()), containsInAnyOrder("n", "q", "r", "0", "2"));
-        assertThat(GeoHashUtils.addNeighbors("z", new ArrayList<String>()), containsInAnyOrder("8", "b", "w", "x", "y"));
+        assertThat(XGeoHashUtils.addNeighbors("0", new ArrayList<String>()), containsInAnyOrder("1", "2", "3", "p", "r"));
+        assertThat(XGeoHashUtils.addNeighbors("b", new ArrayList<String>()), containsInAnyOrder("8", "9", "c", "x", "z"));
+        assertThat(XGeoHashUtils.addNeighbors("p", new ArrayList<String>()), containsInAnyOrder("n", "q", "r", "0", "2"));
+        assertThat(XGeoHashUtils.addNeighbors("z", new ArrayList<String>()), containsInAnyOrder("8", "b", "w", "x", "y"));
 
         // Root crossing dateline
-        assertThat(GeoHashUtils.addNeighbors("2", new ArrayList<String>()), containsInAnyOrder("0", "1", "3", "8", "9", "p", "r", "x"));
-        assertThat(GeoHashUtils.addNeighbors("r", new ArrayList<String>()), containsInAnyOrder("0", "2", "8", "n", "p", "q", "w", "x"));
+        assertThat(XGeoHashUtils.addNeighbors("2", new ArrayList<String>()), containsInAnyOrder("0", "1", "3", "8", "9", "p", "r", "x"));
+        assertThat(XGeoHashUtils.addNeighbors("r", new ArrayList<String>()), containsInAnyOrder("0", "2", "8", "n", "p", "q", "w", "x"));
 
         // level1: simple case
-        assertThat(GeoHashUtils.addNeighbors("dk", new ArrayList<String>()), containsInAnyOrder("d5", "d7", "de", "dh", "dj", "dm", "ds", "dt"));
+        assertThat(XGeoHashUtils.addNeighbors("dk", new ArrayList<String>()), containsInAnyOrder("d5", "d7", "de", "dh", "dj", "dm", "ds", "dt"));
 
         // Level1: crossing cells
-        assertThat(GeoHashUtils.addNeighbors("d5", new ArrayList<String>()), containsInAnyOrder("d4", "d6", "d7", "dh", "dk", "9f", "9g", "9u"));
-        assertThat(GeoHashUtils.addNeighbors("d0", new ArrayList<String>()), containsInAnyOrder("d1", "d2", "d3", "9b", "9c", "6p", "6r", "3z"));
+        assertThat(XGeoHashUtils.addNeighbors("d5", new ArrayList<String>()), containsInAnyOrder("d4", "d6", "d7", "dh", "dk", "9f", "9g", "9u"));
+        assertThat(XGeoHashUtils.addNeighbors("d0", new ArrayList<String>()), containsInAnyOrder("d1", "d2", "d3", "9b", "9c", "6p", "6r", "3z"));
     }
 
     public static double distance(double lat1, double lon1, double lat2, double lon2) {
diff --git a/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java b/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
index 05947e1..9993137 100644
--- a/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
@@ -634,7 +634,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                         .field(new HighlightBuilder.Field("field1").numOfFragments(2))
                         .field(new HighlightBuilder.Field("field2").preTags("<field2>").postTags("</field2>").fragmentSize(50).requireFieldMatch(false)));
 
-        SearchResponse searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
         assertHighlight(searchResponse, 0, "field1", 0, 2, equalTo(" <global>test</global>"));
         assertHighlight(searchResponse, 0, "field1", 1, 2, equalTo(" <global>test</global>"));
@@ -713,13 +713,13 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchSourceBuilder searchSource = SearchSourceBuilder.searchSource().query(termQuery("field1", "quick"))
                 .highlight(highlight().forceSource(true).field("field1"));
-        assertFailures(client().prepareSearch("test").setSource(searchSource.buildAsBytes()),
+        assertFailures(client().prepareSearch("test").setSource(searchSource),
                 RestStatus.BAD_REQUEST,
                 containsString("source is forced for fields [field1] but type [type1] has disabled _source"));
 
         searchSource = SearchSourceBuilder.searchSource().query(termQuery("field1", "quick"))
                 .highlight(highlight().forceSource(true).field("field*"));
-        assertFailures(client().prepareSearch("test").setSource(searchSource.buildAsBytes()),
+        assertFailures(client().prepareSearch("test").setSource(searchSource),
                 RestStatus.BAD_REQUEST,
                 matches("source is forced for fields \\[field\\d, field\\d\\] but type \\[type1\\] has disabled _source"));
     }
@@ -738,7 +738,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .query(termQuery("field1", "test"))
                 .highlight(highlight().field("field1").order("score").preTags("<xxx>").postTags("</xxx>"));
 
-        SearchResponse searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("this is a <xxx>test</xxx>"));
 
@@ -747,7 +747,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .query(termQuery("_all", "test"))
                 .highlight(highlight().field("field1").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
-        searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
+        searchResponse = client().prepareSearch("test").setSource(source).get();
 
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("this is a <xxx>test</xxx>"));
 
@@ -756,7 +756,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .query(termQuery("_all", "quick"))
                 .highlight(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
-        searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
+        searchResponse = client().prepareSearch("test").setSource(source).get();
 
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <xxx>quick</xxx> brown fox jumps over the lazy dog"));
 
@@ -765,7 +765,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .query(prefixQuery("_all", "qui"))
                 .highlight(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
-        searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
+        searchResponse = client().prepareSearch("test").setSource(source).get();
 
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <xxx>quick</xxx> brown fox jumps over the lazy dog"));
 
@@ -774,7 +774,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .query(constantScoreQuery(prefixQuery("_all", "qui")))
                 .highlight(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
-        searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
+        searchResponse = client().prepareSearch("test").setSource(source).get();
 
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <xxx>quick</xxx> brown fox jumps over the lazy dog"));
 
@@ -783,7 +783,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .query(boolQuery().should(constantScoreQuery(prefixQuery("_all", "qui"))))
                 .highlight(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
-        searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
+        searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <xxx>quick</xxx> brown fox jumps over the lazy dog"));
     }
 
@@ -801,7 +801,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .query(termQuery("field1", "test"))
                 .highlight(highlight().field("field1", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>"));
 
-        SearchResponse searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("this is a <xxx>test</xxx>"));
 
@@ -810,7 +810,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .query(termQuery("_all", "test"))
                 .highlight(highlight().field("field1", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
-        searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
+        searchResponse = client().prepareSearch("test").setSource(source).get();
 
         // LUCENE 3.1 UPGRADE: Caused adding the space at the end...
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("this is a <xxx>test</xxx>"));
@@ -820,7 +820,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .query(termQuery("_all", "quick"))
                 .highlight(highlight().field("field2", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
-        searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
+        searchResponse = client().prepareSearch("test").setSource(source).get();
 
         // LUCENE 3.1 UPGRADE: Caused adding the space at the end...
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <xxx>quick</xxx> brown fox jumps over the lazy dog"));
@@ -830,7 +830,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .query(prefixQuery("_all", "qui"))
                 .highlight(highlight().field("field2", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
-        searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
+        searchResponse = client().prepareSearch("test").setSource(source).get();
 
         // LUCENE 3.1 UPGRADE: Caused adding the space at the end...
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <xxx>quick</xxx> brown fox jumps over the lazy dog"));
@@ -1399,7 +1399,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .query(boostingQuery().positive(termQuery("field2", "brown")).negative(termQuery("field2", "foobar")).negativeBoost(0.5f))
                 .highlight(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
 
-        SearchResponse searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The quick <x>brown</x> fox jumps over the lazy dog"));
     }
@@ -1418,7 +1418,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .query(boostingQuery().positive(termQuery("field2", "brown")).negative(termQuery("field2", "foobar")).negativeBoost(0.5f))
                 .highlight(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
 
-        SearchResponse searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The quick <x>brown</x> fox jumps over the lazy dog"));
     }
@@ -1438,7 +1438,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .query(commonTermsQuery("field2", "quick brown").cutoffFrequency(100))
                 .highlight(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
 
-        SearchResponse searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <x>quick</x> <x>brown</x> fox jumps over the lazy dog"));
     }
 
@@ -1453,7 +1453,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         SearchSourceBuilder source = searchSource().query(commonTermsQuery("field2", "quick brown").cutoffFrequency(100))
                 .highlight(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
 
-        SearchResponse searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <x>quick</x> <x>brown</x> fox jumps over the lazy dog"));
     }
@@ -2372,7 +2372,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchSourceBuilder source = searchSource().query(prefixQuery("field2", "qui"))
                 .highlight(highlight().field("field2"));
-        SearchResponse searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <em>quick</em> brown fox jumps over the lazy dog!"));
 
     }
@@ -2387,7 +2387,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field2");
         SearchSourceBuilder source = searchSource().query(fuzzyQuery("field2", "quck"))
                 .highlight(highlight().field("field2"));
-        SearchResponse searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <em>quick</em> brown fox jumps over the lazy dog!"));
     }
@@ -2402,7 +2402,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field2");
         SearchSourceBuilder source = searchSource().query(regexpQuery("field2", "qu[a-l]+k"))
                 .highlight(highlight().field("field2"));
-        SearchResponse searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <em>quick</em> brown fox jumps over the lazy dog!"));
     }
@@ -2417,13 +2417,13 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field2");
         SearchSourceBuilder source = searchSource().query(wildcardQuery("field2", "qui*"))
                 .highlight(highlight().field("field2"));
-        SearchResponse searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <em>quick</em> brown fox jumps over the lazy dog!"));
 
         source = searchSource().query(wildcardQuery("field2", "qu*k"))
                 .highlight(highlight().field("field2"));
-        searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
+        searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHitCount(searchResponse, 1l);
 
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <em>quick</em> brown fox jumps over the lazy dog!"));
@@ -2439,7 +2439,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field2");
         SearchSourceBuilder source = searchSource().query(rangeQuery("field2").gte("aaaa").lt("zzzz"))
                 .highlight(highlight().field("field2"));
-        SearchResponse searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("<em>aaab</em>"));
     }
@@ -2454,7 +2454,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field2");
         SearchSourceBuilder source = searchSource().query(queryStringQuery("qui*").defaultField("field2"))
                 .highlight(highlight().field("field2"));
-        SearchResponse searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <em>quick</em> brown fox jumps over the lazy dog!"));
     }
 
@@ -2470,7 +2470,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource().query(constantScoreQuery(regexpQuery("field1", "pho[a-z]+")))
                 .highlight(highlight().field("field1"));
-        SearchResponse searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("The <em>photography</em> word will get highlighted"));
     }
 
@@ -2489,7 +2489,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .should(matchQuery("field1", "test"))
                 .should(constantScoreQuery(queryStringQuery("field1:photo*"))))
                 .highlight(highlight().field("field1"));
-        SearchResponse searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("The <em>photography</em> word will get highlighted"));
     }
 
@@ -2505,7 +2505,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource().query(boolQuery().must(prefixQuery("field1", "photo")).should(matchQuery("field1", "test").minimumShouldMatch("0")))
                 .highlight(highlight().field("field1"));
-        SearchResponse searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("The <em>photography</em> word will get highlighted"));
     }
 
@@ -2521,7 +2521,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource().query(boolQuery().must(queryStringQuery("field1:photo*")).filter(missingQuery("field_null")))
                 .highlight(highlight().field("field1"));
-        SearchResponse searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("The <em>photography</em> word will get highlighted"));
     }
 
diff --git a/core/src/test/java/org/elasticsearch/search/query/MultiMatchQueryIT.java b/core/src/test/java/org/elasticsearch/search/query/MultiMatchQueryIT.java
index ca138d9..24eb8cb 100644
--- a/core/src/test/java/org/elasticsearch/search/query/MultiMatchQueryIT.java
+++ b/core/src/test/java/org/elasticsearch/search/query/MultiMatchQueryIT.java
@@ -629,8 +629,6 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
     }
 
     private MultiMatchQueryBuilder.Type getType(MultiMatchQueryBuilder builder) throws NoSuchFieldException, IllegalAccessException {
-        Field field = MultiMatchQueryBuilder.class.getDeclaredField("type");
-        field.setAccessible(true);
-        return (MultiMatchQueryBuilder.Type) field.get(builder);
+        return builder.getType();
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java b/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java
index e41c451..3a857bf 100644
--- a/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java
+++ b/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java
@@ -21,6 +21,7 @@ package org.elasticsearch.search.query;
 
 import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.BoolQueryBuilder;
 import org.elasticsearch.index.query.SimpleQueryStringBuilder;
@@ -269,7 +270,7 @@ public class SimpleQueryStringIT extends ESIntegTestCase {
                         .flags(SimpleQueryStringFlag.NONE)).get();
         assertHitCount(searchResponse, 0l);
 
-        searchResponse = client().prepareSearch().setSource("{\n" +
+        searchResponse = client().prepareSearch().setSource(new BytesArray("{\n" +
                 "  \"query\": {\n" +
                 "    \"simple_query_string\": {\n" +
                 "      \"query\": \"foo|bar\",\n" +
@@ -277,7 +278,7 @@ public class SimpleQueryStringIT extends ESIntegTestCase {
                 "      \"flags\": \"NONE\"\n" +
                 "    }\n" +
                 "  }\n" +
-                "}").get();
+                "}")).get();
         assertHitCount(searchResponse, 1l);
 
         searchResponse = client().prepareSearch().setQuery(
diff --git a/core/src/test/java/org/elasticsearch/search/sort/SimpleSortIT.java b/core/src/test/java/org/elasticsearch/search/sort/SimpleSortIT.java
index f8752ff..e487819 100644
--- a/core/src/test/java/org/elasticsearch/search/sort/SimpleSortIT.java
+++ b/core/src/test/java/org/elasticsearch/search/sort/SimpleSortIT.java
@@ -30,6 +30,7 @@ import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.search.ShardSearchFailure;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.geo.GeoDistance;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.text.StringAndBytesText;
@@ -1856,16 +1857,16 @@ public class SimpleSortIT extends ESIntegTestCase {
                 .addSort(geoDistanceSortBuilder.sortMode("min").order(SortOrder.ASC).geoDistance(GeoDistance.PLANE).unit(DistanceUnit.KILOMETERS))
                 .execute().actionGet();
         assertOrderedSearchHits(searchResponse, "d1", "d2");
-        assertThat((Double) searchResponse.getHits().getAt(0).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(2.5, 1, 2, 1, DistanceUnit.KILOMETERS), 1.e-5));
-        assertThat((Double) searchResponse.getHits().getAt(1).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(4.5, 1, 2, 1, DistanceUnit.KILOMETERS), 1.e-5));
+        assertThat((Double) searchResponse.getHits().getAt(0).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(2.5, 1, 2, 1, DistanceUnit.KILOMETERS), 1.e-4));
+        assertThat((Double) searchResponse.getHits().getAt(1).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(4.5, 1, 2, 1, DistanceUnit.KILOMETERS), 1.e-4));
 
         searchResponse = client().prepareSearch()
                 .setQuery(matchAllQuery())
                 .addSort(geoDistanceSortBuilder.sortMode("max").order(SortOrder.ASC).geoDistance(GeoDistance.PLANE).unit(DistanceUnit.KILOMETERS))
                 .execute().actionGet();
         assertOrderedSearchHits(searchResponse, "d1", "d2");
-        assertThat((Double) searchResponse.getHits().getAt(0).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(3.25, 4, 2, 1, DistanceUnit.KILOMETERS), 1.e-5));
-        assertThat((Double) searchResponse.getHits().getAt(1).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(5.25, 4, 2, 1, DistanceUnit.KILOMETERS), 1.e-5));
+        assertThat((Double) searchResponse.getHits().getAt(0).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(3.25, 4, 2, 1, DistanceUnit.KILOMETERS), 1.e-4));
+        assertThat((Double) searchResponse.getHits().getAt(1).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(5.25, 4, 2, 1, DistanceUnit.KILOMETERS), 1.e-4));
 
         //test all the different formats in one
         createQPoints(qHashes, qPoints);
@@ -1907,10 +1908,10 @@ public class SimpleSortIT extends ESIntegTestCase {
         searchSourceBuilder.endArray();
         searchSourceBuilder.endObject();
 
-        searchResponse = client().prepareSearch().setSource(searchSourceBuilder).execute().actionGet();
+        searchResponse = client().prepareSearch().setSource(searchSourceBuilder.bytes()).execute().actionGet();
         assertOrderedSearchHits(searchResponse, "d1", "d2");
-        assertThat((Double) searchResponse.getHits().getAt(0).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(2.5, 1, 2, 1, DistanceUnit.KILOMETERS), 1.e-5));
-        assertThat((Double) searchResponse.getHits().getAt(1).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(4.5, 1, 2, 1, DistanceUnit.KILOMETERS), 1.e-5));
+        assertThat((Double) searchResponse.getHits().getAt(0).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(2.5, 1, 2, 1, DistanceUnit.KILOMETERS), 1.e-4));
+        assertThat((Double) searchResponse.getHits().getAt(1).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(4.5, 1, 2, 1, DistanceUnit.KILOMETERS), 1.e-4));
     }
 
     public void testSinglePointGeoDistanceSort() throws ExecutionException, InterruptedException, IOException {
@@ -1956,7 +1957,7 @@ public class SimpleSortIT extends ESIntegTestCase {
                 .field("distance_type", "plane")
                 .endObject()
                 .endObject().endArray().string();
-        searchResponse = client().prepareSearch().setSource(geoSortRequest)
+        searchResponse = client().prepareSearch().setSource(new BytesArray(geoSortRequest))
                 .execute().actionGet();
         checkCorrectSortOrderForGeoSort(searchResponse);
 
@@ -1967,7 +1968,7 @@ public class SimpleSortIT extends ESIntegTestCase {
                 .field("distance_type", "plane")
                 .endObject()
                 .endObject().endArray().string();
-        searchResponse = client().prepareSearch().setSource(geoSortRequest)
+        searchResponse = client().prepareSearch().setSource(new BytesArray(geoSortRequest))
                 .execute().actionGet();
         checkCorrectSortOrderForGeoSort(searchResponse);
 
@@ -1981,15 +1982,15 @@ public class SimpleSortIT extends ESIntegTestCase {
                 .field("distance_type", "plane")
                 .endObject()
                 .endObject().endArray().string();
-        searchResponse = client().prepareSearch().setSource(geoSortRequest)
+        searchResponse = client().prepareSearch().setSource(new BytesArray(geoSortRequest))
                 .execute().actionGet();
         checkCorrectSortOrderForGeoSort(searchResponse);
     }
 
     private void checkCorrectSortOrderForGeoSort(SearchResponse searchResponse) {
         assertOrderedSearchHits(searchResponse, "d2", "d1");
-        assertThat((Double) searchResponse.getHits().getAt(0).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(2, 2, 1, 2, DistanceUnit.KILOMETERS), 1.e-5));
-        assertThat((Double) searchResponse.getHits().getAt(1).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(2, 2, 1, 1, DistanceUnit.KILOMETERS), 1.e-5));
+        assertThat((Double) searchResponse.getHits().getAt(0).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(2, 2, 1, 2, DistanceUnit.KILOMETERS), 1.e-4));
+        assertThat((Double) searchResponse.getHits().getAt(1).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(2, 2, 1, 1, DistanceUnit.KILOMETERS), 1.e-4));
     }
 
     protected void createQPoints(List<String> qHashes, List<GeoPoint> qPoints) {
diff --git a/core/src/test/java/org/elasticsearch/search/suggest/ContextSuggestSearchIT.java b/core/src/test/java/org/elasticsearch/search/suggest/ContextSuggestSearchIT.java
index 596c675..48048b7 100644
--- a/core/src/test/java/org/elasticsearch/search/suggest/ContextSuggestSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/suggest/ContextSuggestSearchIT.java
@@ -19,11 +19,12 @@
 package org.elasticsearch.search.suggest;
 
 import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
+
+import org.apache.lucene.util.XGeoHashUtils;
 import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
 import org.elasticsearch.action.suggest.SuggestRequest;
 import org.elasticsearch.action.suggest.SuggestRequestBuilder;
 import org.elasticsearch.action.suggest.SuggestResponse;
-import org.elasticsearch.common.geo.GeoHashUtils;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.unit.Fuzziness;
 import org.elasticsearch.common.util.set.Sets;
@@ -587,7 +588,7 @@ public class ContextSuggestSearchIT extends ESIntegTestCase {
 
     @Test // issue 5525, default location didnt work with lat/lon map, and did not set default location appropriately
     public void testGeoContextDefaultMapping() throws Exception {
-        GeoPoint berlinAlexanderplatz = GeoHashUtils.decode("u33dc1");
+        GeoPoint berlinAlexanderplatz = GeoPoint.fromGeohash("u33dc1");
 
         XContentBuilder xContentBuilder = jsonBuilder().startObject()
             .startObject("poi").startObject("properties").startObject("suggest")
@@ -734,10 +735,10 @@ public class ContextSuggestSearchIT extends ESIntegTestCase {
 
         // lets create some locations by geohashes in different cells with the precision 4
         // this means, that poelchaustr is not a neighour to alexanderplatz, but they share the same prefix until the fourth char!
-        GeoPoint alexanderplatz = GeoHashUtils.decode("u33dc1");
-        GeoPoint poelchaustr = GeoHashUtils.decode("u33du5");
-        GeoPoint dahlem = GeoHashUtils.decode("u336q"); // berlin dahlem, should be included with that precision
-        GeoPoint middleOfNoWhere = GeoHashUtils.decode("u334"); // location for west from berlin, should not be included in any suggestions
+        GeoPoint alexanderplatz = GeoPoint.fromGeohash("u33dc1");
+        GeoPoint poelchaustr = GeoPoint.fromGeohash("u33du5");
+        GeoPoint dahlem = GeoPoint.fromGeohash("u336q"); // berlin dahlem, should be included with that precision
+        GeoPoint middleOfNoWhere = GeoPoint.fromGeohash("u334"); // location for west from berlin, should not be included in any suggestions
 
         index(INDEX, "item", "1", jsonBuilder().startObject().startObject("suggest").field("input", "Berlin Alexanderplatz").field("weight", 3).startObject("context").startObject("location").field("lat", alexanderplatz.lat()).field("lon", alexanderplatz.lon()).endObject().endObject().endObject().endObject());
         index(INDEX, "item", "2", jsonBuilder().startObject().startObject("suggest").field("input", "Berlin Poelchaustr.").field("weight", 2).startObject("context").startObject("location").field("lat", poelchaustr.lat()).field("lon", poelchaustr.lon()).endObject().endObject().endObject().endObject());
@@ -766,10 +767,10 @@ public class ContextSuggestSearchIT extends ESIntegTestCase {
         assertAcked(prepareCreate(INDEX).addMapping("item", xContentBuilder));
         ensureYellow();
 
-        GeoPoint alexanderplatz = GeoHashUtils.decode("u33dc1");
+        GeoPoint alexanderplatz = GeoPoint.fromGeohash("u33dc1");
         // does not look like it, but is a direct neighbor
         // this test would fail, if the precision was set 4, as then both cells would be the same, u33d
-        GeoPoint cellNeighbourOfAlexanderplatz = GeoHashUtils.decode("u33dbc");
+        GeoPoint cellNeighbourOfAlexanderplatz = GeoPoint.fromGeohash("u33dbc");
 
         index(INDEX, "item", "1", jsonBuilder().startObject().startObject("suggest").field("input", "Berlin Alexanderplatz").field("weight", 3).startObject("context").startObject("location").field("lat", alexanderplatz.lat()).field("lon", alexanderplatz.lon()).endObject().endObject().endObject().endObject());
         index(INDEX, "item", "2", jsonBuilder().startObject().startObject("suggest").field("input", "Berlin Hackescher Markt").field("weight", 2).startObject("context").startObject("location").field("lat", cellNeighbourOfAlexanderplatz.lat()).field("lon", cellNeighbourOfAlexanderplatz.lon()).endObject().endObject().endObject().endObject());
@@ -796,7 +797,7 @@ public class ContextSuggestSearchIT extends ESIntegTestCase {
         assertAcked(prepareCreate(INDEX).addMapping("item", xContentBuilder));
         ensureYellow();
 
-        GeoPoint alexanderplatz = GeoHashUtils.decode("u33dc1");
+        GeoPoint alexanderplatz = GeoPoint.fromGeohash("u33dc1");
         index(INDEX, "item", "1", jsonBuilder().startObject().startObject("suggest").field("input", "Berlin Alexanderplatz").endObject().startObject("loc").field("lat", alexanderplatz.lat()).field("lon", alexanderplatz.lon()).endObject().endObject());
         refresh();
 
@@ -836,7 +837,7 @@ public class ContextSuggestSearchIT extends ESIntegTestCase {
 
         double latitude = 52.22;
         double longitude = 4.53;
-        String geohash = GeoHashUtils.encode(latitude, longitude);
+        String geohash = XGeoHashUtils.stringEncode(longitude, latitude);
 
         XContentBuilder doc1 = jsonBuilder().startObject().startObject("suggest_geo").field("input", "Hotel Marriot in Amsterdam").startObject("context").startObject("location").field("lat", latitude).field("lon", longitude).endObject().endObject().endObject().endObject();
         index("test", "test", "1", doc1);
diff --git a/core/src/test/java/org/elasticsearch/search/suggest/CustomSuggesterSearchIT.java b/core/src/test/java/org/elasticsearch/search/suggest/CustomSuggesterSearchIT.java
index 9e51d25..9b97afc 100644
--- a/core/src/test/java/org/elasticsearch/search/suggest/CustomSuggesterSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/suggest/CustomSuggesterSearchIT.java
@@ -22,12 +22,14 @@ import org.elasticsearch.action.search.SearchRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.util.CollectionUtils;
+import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
 import org.junit.Test;
 
+import java.io.IOException;
 import java.util.Collection;
 import java.util.List;
 import java.util.Locale;
@@ -61,19 +63,16 @@ public class CustomSuggesterSearchIT extends ESIntegTestCase {
         String randomText = randomAsciiOfLength(10);
         String randomField = randomAsciiOfLength(10);
         String randomSuffix = randomAsciiOfLength(10);
-        SearchRequestBuilder searchRequestBuilder = client().prepareSearch("test").setTypes("test").setFrom(0).setSize(1);
-        XContentBuilder query = jsonBuilder().startObject()
-                .startObject("suggest")
-                .startObject("someName")
-                .field("text", randomText)
-                .startObject("custom")
-                .field("field", randomField)
-                .field("suffix", randomSuffix)
-                .endObject()
-                .endObject()
-                .endObject()
-                .endObject();
-        searchRequestBuilder.setExtraSource(query.bytes());
+        SearchRequestBuilder searchRequestBuilder = client().prepareSearch("test").setTypes("test").setFrom(0).setSize(1).addSuggestion(
+                new SuggestBuilder.SuggestionBuilder<SuggestBuilder.SuggestionBuilder>("someName", "custom") {
+                    @Override
+                    protected XContentBuilder innerToXContent(XContentBuilder builder, Params params) throws IOException {
+                        builder.field("field", randomField);
+                        builder.field("suffix", randomSuffix);
+                        return builder;
+                    }
+                }.text(randomText)
+        );
 
         SearchResponse searchResponse = searchRequestBuilder.execute().actionGet();
 
diff --git a/core/src/test/java/org/elasticsearch/search/suggest/completion/CompletionPostingsFormatTests.java b/core/src/test/java/org/elasticsearch/search/suggest/completion/CompletionPostingsFormatTests.java
index 672f8bb..ff672fb 100644
--- a/core/src/test/java/org/elasticsearch/search/suggest/completion/CompletionPostingsFormatTests.java
+++ b/core/src/test/java/org/elasticsearch/search/suggest/completion/CompletionPostingsFormatTests.java
@@ -244,11 +244,9 @@ public class CompletionPostingsFormatTests extends ESTestCase {
         fieldType.setProvider(currentProvider);
         final CompletionFieldMapper mapper = new CompletionFieldMapper("foo", fieldType, Integer.MAX_VALUE, indexSettings, FieldMapper.MultiFields.empty(), null);
         Lookup buildAnalyzingLookup = buildAnalyzingLookup(mapper, titles, titles, weights);
-        Field field = buildAnalyzingLookup.getClass().getDeclaredField("maxAnalyzedPathsForOneInput");
-        field.setAccessible(true);
-        Field refField = reference.getClass().getDeclaredField("maxAnalyzedPathsForOneInput");
-        refField.setAccessible(true);
-        assertThat(refField.get(reference), equalTo(field.get(buildAnalyzingLookup)));
+        if (buildAnalyzingLookup instanceof XAnalyzingSuggester) {
+            assertEquals(reference.getMaxAnalyzedPathsForOneInput(), ((XAnalyzingSuggester) buildAnalyzingLookup).getMaxAnalyzedPathsForOneInput());
+        }
 
         for (int i = 0; i < titles.length; i++) {
             int res = between(1, 10);
diff --git a/core/src/test/java/org/elasticsearch/search/suggest/context/GeoLocationContextMappingTests.java b/core/src/test/java/org/elasticsearch/search/suggest/context/GeoLocationContextMappingTests.java
index 60d2fd6..b525c4a 100644
--- a/core/src/test/java/org/elasticsearch/search/suggest/context/GeoLocationContextMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/search/suggest/context/GeoLocationContextMappingTests.java
@@ -18,8 +18,8 @@
  */
 package org.elasticsearch.search.suggest.context;
 
+import org.apache.lucene.util.XGeoHashUtils;
 import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.common.geo.GeoHashUtils;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentParser;
@@ -57,7 +57,7 @@ public class GeoLocationContextMappingTests extends ESTestCase {
         XContentParser parser = XContentHelper.createParser(builder.bytes());
         parser.nextToken();
 
-        String geohash = GeoHashUtils.encode(randomIntBetween(-90, +90), randomIntBetween(-180, +180));
+        String geohash = XGeoHashUtils.stringEncode(randomIntBetween(-180, +180), randomIntBetween(-90, +90));
         HashMap<String, Object> config = new HashMap<>();
         config.put("precision", 12);
         config.put("default", geohash);
@@ -182,8 +182,8 @@ public class GeoLocationContextMappingTests extends ESTestCase {
     }
 
     public void testUseWithMultiGeoHashGeoContext() throws Exception {
-        String geohash1 = GeoHashUtils.encode(randomIntBetween(-90, +90), randomIntBetween(-180, +180));
-        String geohash2 = GeoHashUtils.encode(randomIntBetween(-90, +90), randomIntBetween(-180, +180));
+        String geohash1 = XGeoHashUtils.stringEncode(randomIntBetween(-180, +180), randomIntBetween(-90, +90));
+        String geohash2 = XGeoHashUtils.stringEncode(randomIntBetween(-180, +180), randomIntBetween(-90, +90));
         XContentBuilder builder = jsonBuilder().startObject().startArray("location").value(geohash1).value(geohash2).endArray().endObject();
         XContentParser parser = XContentHelper.createParser(builder.bytes());
         parser.nextToken(); // start of object
diff --git a/core/src/test/java/org/elasticsearch/snapshots/BlobStoreFormatIT.java b/core/src/test/java/org/elasticsearch/snapshots/BlobStoreFormatIT.java
index a805aa3..9047c71 100644
--- a/core/src/test/java/org/elasticsearch/snapshots/BlobStoreFormatIT.java
+++ b/core/src/test/java/org/elasticsearch/snapshots/BlobStoreFormatIT.java
@@ -27,6 +27,7 @@ import org.elasticsearch.common.blobstore.BlobMetaData;
 import org.elasticsearch.common.blobstore.BlobPath;
 import org.elasticsearch.common.blobstore.BlobStore;
 import org.elasticsearch.common.blobstore.fs.FsBlobStore;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.compress.CompressorFactory;
 import org.elasticsearch.common.io.Streams;
@@ -42,7 +43,6 @@ import org.junit.Test;
 import java.io.EOFException;
 import java.io.IOException;
 import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.Map;
 import java.util.concurrent.*;
 
@@ -122,9 +122,7 @@ public class BlobStoreFormatIT extends AbstractSnapshotIntegTestCase {
 
         public void write(T obj, BlobContainer blobContainer, String blobName) throws IOException {
             BytesReference bytes = write(obj);
-            try (OutputStream outputStream = blobContainer.createOutput(blobName)) {
-                bytes.writeTo(outputStream);
-            }
+            blobContainer.writeBlob(blobName, bytes);
         }
 
         private BytesReference write(T obj) throws IOException {
@@ -272,16 +270,14 @@ public class BlobStoreFormatIT extends AbstractSnapshotIntegTestCase {
     protected void randomCorruption(BlobContainer blobContainer, String blobName) throws IOException {
         byte[] buffer = new byte[(int) blobContainer.listBlobsByPrefix(blobName).get(blobName).length()];
         long originalChecksum = checksum(buffer);
-        try (InputStream inputStream = blobContainer.openInput(blobName)) {
+        try (InputStream inputStream = blobContainer.readBlob(blobName)) {
             Streams.readFully(inputStream, buffer);
         }
         do {
             int location = randomIntBetween(0, buffer.length - 1);
             buffer[location] = (byte) (buffer[location] ^ 42);
         } while (originalChecksum == checksum(buffer));
-        try (OutputStream outputStream = blobContainer.createOutput(blobName)) {
-            Streams.copy(buffer, outputStream);
-        }
+        blobContainer.writeBlob(blobName, new BytesArray(buffer));
     }
 
     private long checksum(byte[] buffer) throws IOException {
diff --git a/core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java b/core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java
index 96c563c..6249232 100644
--- a/core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java
+++ b/core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java
@@ -21,7 +21,6 @@ package org.elasticsearch.snapshots;
 
 import com.carrotsearch.hppc.IntHashSet;
 import com.carrotsearch.hppc.IntSet;
-import com.google.common.util.concurrent.ListenableFuture;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.action.ListenableActionFuture;
 import org.elasticsearch.action.admin.cluster.repositories.put.PutRepositoryResponse;
@@ -723,7 +722,7 @@ public class DedicatedClusterSnapshotRestoreIT extends AbstractSnapshotIntegTest
 
         int asyncNodes = between(0, 5);
         logger.info("--> start {} additional nodes asynchronously", asyncNodes);
-        ListenableFuture<List<String>> asyncNodesFuture = internalCluster().startNodesAsync(asyncNodes, settings);
+        InternalTestCluster.Async<List<String>> asyncNodesFuture = internalCluster().startNodesAsync(asyncNodes, settings);
 
         int asyncIndices = between(0, 10);
         logger.info("--> create {} additional indices asynchronously", asyncIndices);
diff --git a/core/src/test/java/org/elasticsearch/snapshots/SnapshotRequestsTests.java b/core/src/test/java/org/elasticsearch/snapshots/SnapshotRequestsTests.java
new file mode 100644
index 0000000..7cf56bf
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/snapshots/SnapshotRequestsTests.java
@@ -0,0 +1,154 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.snapshots;
+
+import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotRequest;
+import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotRequest;
+import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.io.IOException;
+
+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
+
+public class SnapshotRequestsTests extends ESTestCase {
+    @Test
+    public void testRestoreSnapshotRequestParsing() throws IOException {
+
+        RestoreSnapshotRequest request = new RestoreSnapshotRequest("test-repo", "test-snap");
+
+        XContentBuilder builder = jsonBuilder().startObject();
+
+        if(randomBoolean()) {
+            builder.field("indices", "foo,bar,baz");
+        } else {
+            builder.startArray("indices");
+            builder.value("foo");
+            builder.value("bar");
+            builder.value("baz");
+            builder.endArray();
+        }
+
+        IndicesOptions indicesOptions = IndicesOptions.fromOptions(randomBoolean(), randomBoolean(), randomBoolean(), randomBoolean());
+        if (indicesOptions.expandWildcardsClosed()) {
+            if (indicesOptions.expandWildcardsOpen()) {
+                builder.field("expand_wildcards", "all");
+            } else {
+                builder.field("expand_wildcards", "closed");
+            }
+        } else {
+            if (indicesOptions.expandWildcardsOpen()) {
+                builder.field("expand_wildcards", "open");
+            } else {
+                builder.field("expand_wildcards", "none");
+            }
+        }
+        builder.field("allow_no_indices", indicesOptions.allowNoIndices());
+        builder.field("rename_pattern", "rename-from");
+        builder.field("rename_replacement", "rename-to");
+        boolean partial = randomBoolean();
+        builder.field("partial", partial);
+        builder.startObject("settings").field("set1", "val1").endObject();
+        builder.startObject("index_settings").field("set1", "val2").endObject();
+        if (randomBoolean()) {
+            builder.field("ignore_index_settings", "set2,set3");
+        } else {
+            builder.startArray("ignore_index_settings");
+            builder.value("set2");
+            builder.value("set3");
+            builder.endArray();
+        }
+
+        byte[] bytes = builder.endObject().bytes().toBytes();
+
+
+        request.source(bytes);
+
+        assertEquals("test-repo", request.repository());
+        assertEquals("test-snap", request.snapshot());
+        assertArrayEquals(request.indices(), new String[]{"foo", "bar", "baz"});
+        assertEquals("rename-from", request.renamePattern());
+        assertEquals("rename-to", request.renameReplacement());
+        assertEquals(partial, request.partial());
+        assertEquals("val1", request.settings().get("set1"));
+        assertArrayEquals(request.ignoreIndexSettings(), new String[]{"set2", "set3"});
+
+    }
+
+    @Test
+    public void testCreateSnapshotRequestParsing() throws IOException {
+
+        CreateSnapshotRequest request = new CreateSnapshotRequest("test-repo", "test-snap");
+
+        XContentBuilder builder = jsonBuilder().startObject();
+
+        if(randomBoolean()) {
+            builder.field("indices", "foo,bar,baz");
+        } else {
+            builder.startArray("indices");
+            builder.value("foo");
+            builder.value("bar");
+            builder.value("baz");
+            builder.endArray();
+        }
+
+        IndicesOptions indicesOptions = IndicesOptions.fromOptions(randomBoolean(), randomBoolean(), randomBoolean(), randomBoolean());
+        if (indicesOptions.expandWildcardsClosed()) {
+            if (indicesOptions.expandWildcardsOpen()) {
+                builder.field("expand_wildcards", "all");
+            } else {
+                builder.field("expand_wildcards", "closed");
+            }
+        } else {
+            if (indicesOptions.expandWildcardsOpen()) {
+                builder.field("expand_wildcards", "open");
+            } else {
+                builder.field("expand_wildcards", "none");
+            }
+        }
+        builder.field("allow_no_indices", indicesOptions.allowNoIndices());
+        boolean partial = randomBoolean();
+        builder.field("partial", partial);
+        builder.startObject("settings").field("set1", "val1").endObject();
+        builder.startObject("index_settings").field("set1", "val2").endObject();
+        if (randomBoolean()) {
+            builder.field("ignore_index_settings", "set2,set3");
+        } else {
+            builder.startArray("ignore_index_settings");
+            builder.value("set2");
+            builder.value("set3");
+            builder.endArray();
+        }
+
+        byte[] bytes = builder.endObject().bytes().toBytes();
+
+
+        request.source(bytes);
+
+        assertEquals("test-repo", request.repository());
+        assertEquals("test-snap", request.snapshot());
+        assertArrayEquals(request.indices(), new String[]{"foo", "bar", "baz"});
+        assertEquals(partial, request.partial());
+        assertEquals("val1", request.settings().get("set1"));
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/snapshots/mockstore/BlobContainerWrapper.java b/core/src/test/java/org/elasticsearch/snapshots/mockstore/BlobContainerWrapper.java
index e45d416..dd1bc63 100644
--- a/core/src/test/java/org/elasticsearch/snapshots/mockstore/BlobContainerWrapper.java
+++ b/core/src/test/java/org/elasticsearch/snapshots/mockstore/BlobContainerWrapper.java
@@ -18,14 +18,13 @@
  */
 package org.elasticsearch.snapshots.mockstore;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.blobstore.BlobContainer;
 import org.elasticsearch.common.blobstore.BlobMetaData;
 import org.elasticsearch.common.blobstore.BlobPath;
+import org.elasticsearch.common.bytes.BytesReference;
 
 import java.io.IOException;
 import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.Collection;
 import java.util.Map;
 
@@ -50,13 +49,18 @@ public class BlobContainerWrapper implements BlobContainer {
     }
 
     @Override
-    public InputStream openInput(String name) throws IOException {
-        return delegate.openInput(name);
+    public InputStream readBlob(String name) throws IOException {
+        return delegate.readBlob(name);
     }
 
     @Override
-    public OutputStream createOutput(String blobName) throws IOException {
-        return delegate.createOutput(blobName);
+    public void writeBlob(String blobName, InputStream inputStream, long blobSize) throws IOException {
+        delegate.writeBlob(blobName, inputStream, blobSize);
+    }
+
+    @Override
+    public void writeBlob(String blobName, BytesReference bytes) throws IOException {
+        delegate.writeBlob(blobName, bytes);
     }
 
     @Override
diff --git a/core/src/test/java/org/elasticsearch/snapshots/mockstore/MockRepository.java b/core/src/test/java/org/elasticsearch/snapshots/mockstore/MockRepository.java
index be10f6b..f667d86 100644
--- a/core/src/test/java/org/elasticsearch/snapshots/mockstore/MockRepository.java
+++ b/core/src/test/java/org/elasticsearch/snapshots/mockstore/MockRepository.java
@@ -27,6 +27,7 @@ import org.elasticsearch.common.blobstore.BlobContainer;
 import org.elasticsearch.common.blobstore.BlobMetaData;
 import org.elasticsearch.common.blobstore.BlobPath;
 import org.elasticsearch.common.blobstore.BlobStore;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.inject.AbstractModule;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.inject.Module;
@@ -43,7 +44,6 @@ import org.elasticsearch.repositories.fs.FsRepository;
 
 import java.io.IOException;
 import java.io.InputStream;
-import java.io.OutputStream;
 import java.io.UnsupportedEncodingException;
 import java.nio.file.Path;
 import java.security.MessageDigest;
@@ -317,9 +317,9 @@ public class MockRepository extends FsRepository {
             }
 
             @Override
-            public InputStream openInput(String name) throws IOException {
+            public InputStream readBlob(String name) throws IOException {
                 maybeIOExceptionOrBlock(name);
-                return super.openInput(name);
+                return super.readBlob(name);
             }
 
             @Override
@@ -353,9 +353,15 @@ public class MockRepository extends FsRepository {
             }
 
             @Override
-            public OutputStream createOutput(String blobName) throws IOException {
+            public void writeBlob(String blobName, BytesReference bytes) throws IOException {
                 maybeIOExceptionOrBlock(blobName);
-                return super.createOutput(blobName);
+                super.writeBlob(blobName, bytes);
+            }
+
+            @Override
+            public void writeBlob(String blobName, InputStream inputStream, long blobSize) throws IOException {
+                maybeIOExceptionOrBlock(blobName);
+                super.writeBlob(blobName, inputStream, blobSize);
             }
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java b/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java
index 95c48e8..73b6bf7 100644
--- a/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java
+++ b/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java
@@ -91,8 +91,6 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.json.JsonXContent;
 import org.elasticsearch.common.xcontent.support.XContentMapValues;
-import org.elasticsearch.discovery.Discovery;
-import org.elasticsearch.discovery.zen.ZenDiscovery;
 import org.elasticsearch.discovery.zen.elect.ElectMasterService;
 import org.elasticsearch.env.Environment;
 import org.elasticsearch.index.IndexService;
@@ -130,15 +128,33 @@ import org.junit.BeforeClass;
 
 import java.io.IOException;
 import java.io.InputStream;
-import java.lang.annotation.*;
+import java.lang.annotation.Annotation;
+import java.lang.annotation.ElementType;
+import java.lang.annotation.Inherited;
+import java.lang.annotation.Retention;
+import java.lang.annotation.RetentionPolicy;
+import java.lang.annotation.Target;
 import java.net.InetAddress;
 import java.net.InetSocketAddress;
 import java.net.UnknownHostException;
 import java.nio.file.DirectoryStream;
 import java.nio.file.Files;
 import java.nio.file.Path;
-import java.util.*;
-import java.util.concurrent.*;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.IdentityHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Random;
+import java.util.Set;
+import java.util.concurrent.Callable;
+import java.util.concurrent.CopyOnWriteArrayList;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicLong;
 import java.util.function.BooleanSupplier;
@@ -150,8 +166,14 @@ import static org.elasticsearch.common.util.CollectionUtils.eagerPartition;
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
 import static org.elasticsearch.test.XContentTestUtils.convertToMap;
 import static org.elasticsearch.test.XContentTestUtils.differenceBetweenMapsIgnoringArrayOrder;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
-import static org.hamcrest.Matchers.*;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoTimeout;
+import static org.hamcrest.Matchers.emptyIterable;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.is;
+import static org.hamcrest.Matchers.notNullValue;
+import static org.hamcrest.Matchers.startsWith;
 
 /**
  * {@link ESIntegTestCase} is an abstract base class to run integration
@@ -582,21 +604,6 @@ public abstract class ESIntegTestCase extends ESTestCase {
                     }
                     ensureClusterSizeConsistency();
                     ensureClusterStateConsistency();
-                    if (isInternalCluster()) {
-                        // check no pending cluster states are leaked
-                        for (Discovery discovery : internalCluster().getInstances(Discovery.class)) {
-                            if (discovery instanceof ZenDiscovery) {
-                                final ZenDiscovery zenDiscovery = (ZenDiscovery) discovery;
-                                assertBusy(new Runnable() {
-                                    @Override
-                                    public void run() {
-                                        assertThat("still having pending states: " + Strings.arrayToDelimitedString(zenDiscovery.pendingClusterStates(), "\n"),
-                                                zenDiscovery.pendingClusterStates(), emptyArray());
-                                    }
-                                });
-                            }
-                        }
-                    }
                     beforeIndexDeletion();
                     cluster().wipe(); // wipe after to make sure we fail in the test that didn't ack the delete
                     if (afterClass || currentClusterScope == Scope.TEST) {
@@ -871,7 +878,7 @@ public abstract class ESIntegTestCase extends ESTestCase {
             String failMsg = sb.toString();
             for (SearchHit hit : searchResponse.getHits().getHits()) {
                 sb.append("\n-> _index: [").append(hit.getIndex()).append("] type [").append(hit.getType())
-                        .append("] id [").append(hit.id()).append("]");
+                    .append("] id [").append(hit.id()).append("]");
             }
             logger.warn(sb.toString());
             fail(failMsg);
@@ -1635,6 +1642,7 @@ public abstract class ESIntegTestCase extends ESTestCase {
     }
 
 
+
     private Scope getCurrentClusterScope() {
         return getCurrentClusterScope(this.getClass());
     }
@@ -1769,17 +1777,14 @@ public abstract class ESIntegTestCase extends ESTestCase {
                 return Settings.builder().put(Node.HTTP_ENABLED, false).
                         put(ESIntegTestCase.this.nodeSettings(nodeOrdinal)).build();
             }
-
             @Override
             public Collection<Class<? extends Plugin>> nodePlugins() {
                 return ESIntegTestCase.this.nodePlugins();
             }
-
             @Override
             public Settings transportClientSettings() {
                 return ESIntegTestCase.this.transportClientSettings();
             }
-
             @Override
             public Collection<Class<? extends Plugin>> transportClientPlugins() {
                 return ESIntegTestCase.this.transportClientPlugins();
@@ -1800,7 +1805,7 @@ public abstract class ESIntegTestCase extends ESTestCase {
         String nodeMode = InternalTestCluster.configuredNodeMode();
         if (noLocal != null && noNetwork != null) {
             throw new IllegalStateException("Can't suppress both network and local mode");
-        } else if (noLocal != null) {
+        } else if (noLocal != null){
             nodeMode = "network";
         } else if (noNetwork != null) {
             nodeMode = "local";
@@ -2105,15 +2110,13 @@ public abstract class ESIntegTestCase extends ESTestCase {
      */
     @Retention(RetentionPolicy.RUNTIME)
     @Inherited
-    public @interface SuppressLocalMode {
-    }
+    public @interface SuppressLocalMode {}
 
     /**
      * If used the test will never run in network mode
      */
     @Retention(RetentionPolicy.RUNTIME)
     @Inherited
-    public @interface SuppressNetworkMode {
-    }
+    public @interface SuppressNetworkMode {}
 
 }
diff --git a/core/src/test/java/org/elasticsearch/test/ESTestCase.java b/core/src/test/java/org/elasticsearch/test/ESTestCase.java
index 2916cbb..dd60e96 100644
--- a/core/src/test/java/org/elasticsearch/test/ESTestCase.java
+++ b/core/src/test/java/org/elasticsearch/test/ESTestCase.java
@@ -29,9 +29,11 @@ import com.carrotsearch.randomizedtesting.generators.RandomInts;
 import com.carrotsearch.randomizedtesting.generators.RandomPicks;
 import com.carrotsearch.randomizedtesting.generators.RandomStrings;
 import com.carrotsearch.randomizedtesting.rules.TestRuleAdapter;
+
 import org.apache.lucene.uninverting.UninvertingReader;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
+import org.apache.lucene.util.TestRuleMarkFailure;
 import org.apache.lucene.util.TestUtil;
 import org.apache.lucene.util.TimeUnits;
 import org.elasticsearch.Version;
@@ -41,6 +43,7 @@ import org.elasticsearch.client.Requests;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.routing.DjbHashFunction;
 import org.elasticsearch.common.io.PathUtils;
+import org.elasticsearch.common.io.PathUtilsForTesting;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
 import org.elasticsearch.common.settings.Settings;
@@ -62,9 +65,7 @@ import org.junit.Rule;
 import org.junit.rules.RuleChain;
 
 import java.io.IOException;
-import java.lang.reflect.Field;
 import java.nio.file.DirectoryStream;
-import java.nio.file.FileSystem;
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.util.*;
@@ -137,20 +138,12 @@ public abstract class ESTestCase extends LuceneTestCase {
 
     @BeforeClass
     public static void setFileSystem() throws Exception {
-        Field field = PathUtils.class.getDeclaredField("DEFAULT");
-        field.setAccessible(true);
-        FileSystem mock = LuceneTestCase.getBaseTempDirForTestClass().getFileSystem();
-        field.set(null, mock);
-        assertEquals(mock, PathUtils.getDefaultFileSystem());
+        PathUtilsForTesting.setup();
     }
 
     @AfterClass
     public static void restoreFileSystem() throws Exception {
-        Field field1 = PathUtils.class.getDeclaredField("ACTUAL_DEFAULT");
-        field1.setAccessible(true);
-        Field field2 = PathUtils.class.getDeclaredField("DEFAULT");
-        field2.setAccessible(true);
-        field2.set(null, field1.get(null));
+        PathUtilsForTesting.teardown();
     }
 
     // setup a default exception handler which knows when and how to print a stacktrace
@@ -651,4 +644,9 @@ public abstract class ESTestCase extends LuceneTestCase {
         sb.append("]");
         assertThat(count + " files exist that should have been cleaned:\n" + sb.toString(), count, equalTo(0));
     }
+    
+    /** Returns the suite failure marker: internal use only! */
+    public static TestRuleMarkFailure getSuiteFailureMarker() {
+        return suiteFailureMarker;
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/test/InternalTestCluster.java b/core/src/test/java/org/elasticsearch/test/InternalTestCluster.java
index 012ce95..53733bc 100644
--- a/core/src/test/java/org/elasticsearch/test/InternalTestCluster.java
+++ b/core/src/test/java/org/elasticsearch/test/InternalTestCluster.java
@@ -24,10 +24,6 @@ import com.carrotsearch.randomizedtesting.SysGlobals;
 import com.carrotsearch.randomizedtesting.generators.RandomInts;
 import com.carrotsearch.randomizedtesting.generators.RandomPicks;
 import com.carrotsearch.randomizedtesting.generators.RandomStrings;
-import com.google.common.collect.Iterators;
-import com.google.common.util.concurrent.Futures;
-import com.google.common.util.concurrent.ListenableFuture;
-import com.google.common.util.concurrent.SettableFuture;
 import org.apache.lucene.store.StoreRateLimiting;
 import org.apache.lucene.util.IOUtils;
 import org.elasticsearch.ElasticsearchException;
@@ -64,7 +60,6 @@ import org.elasticsearch.common.unit.ByteSizeUnit;
 import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.EsExecutors;
-import org.elasticsearch.common.util.set.Sets;
 import org.elasticsearch.env.NodeEnvironment;
 import org.elasticsearch.http.HttpServerTransport;
 import org.elasticsearch.index.IndexService;
@@ -118,8 +113,7 @@ import java.util.NavigableMap;
 import java.util.Random;
 import java.util.Set;
 import java.util.TreeMap;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.TimeUnit;
+import java.util.concurrent.*;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.function.Predicate;
@@ -558,20 +552,22 @@ public final class InternalTestCluster extends TestCluster {
      * stop any of the running nodes.
      */
     public void ensureAtLeastNumDataNodes(int n) {
-        List<ListenableFuture<String>> futures = new ArrayList<>();
+        final List<Async<String>> asyncs = new ArrayList<>();
         synchronized (this) {
             int size = numDataNodes();
             for (int i = size; i < n; i++) {
                 logger.info("increasing cluster size from {} to {}", size, n);
-                futures.add(startNodeAsync());
+                asyncs.add(startNodeAsync());
             }
         }
         try {
-            Futures.allAsList(futures).get();
+            for (Async<String> async : asyncs) {
+                async.get();
+            }
         } catch (Exception e) {
             throw new ElasticsearchException("failed to start nodes", e);
         }
-        if (!futures.isEmpty()) {
+        if (!asyncs.isEmpty()) {
             synchronized (this) {
                 assertNoTimeout(client().admin().cluster().prepareHealth().setWaitForNodes(Integer.toString(nodes.size())).get());
             }
@@ -593,11 +589,11 @@ public final class InternalTestCluster extends TestCluster {
                 n == 0 ? nodes.values().stream() : nodes.values().stream().filter(new DataNodePredicate().and(new MasterNodePredicate(getMasterName()).negate()));
         final Iterator<NodeAndClient> values = collection.iterator();
 
-        final Iterator<NodeAndClient> limit = Iterators.limit(values, size - n);
         logger.info("changing cluster size from {} to {}, {} data nodes", size(), n + numSharedClientNodes, n);
         Set<NodeAndClient> nodesToRemove = new HashSet<>();
-        while (limit.hasNext()) {
-            NodeAndClient next = limit.next();
+        int numNodesAndClients = 0;
+        while (values.hasNext() && numNodesAndClients++ < size-n) {
+            NodeAndClient next = values.next();
             nodesToRemove.add(next);
             removeDisruptionSchemeFromNode(next);
             next.close();
@@ -1429,7 +1425,13 @@ public final class InternalTestCluster extends TestCluster {
                         .stream()
                         .filter(new EntryNodePredicate(new DataNodePredicate()))
                         .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));
-        return Sets.newHashSet(Iterators.limit(dataNodes.keySet().iterator(), numNodes));
+        final HashSet<String> set = new HashSet<>();
+        final Iterator<String> iterator = dataNodes.keySet().iterator();
+        for (int i = 0; i < numNodes; i++) {
+            assert iterator.hasNext();
+            set.add(iterator.next());
+        }
+        return set;
     }
 
     /**
@@ -1489,29 +1491,29 @@ public final class InternalTestCluster extends TestCluster {
         return buildNode.name;
     }
 
-    public synchronized ListenableFuture<List<String>> startMasterOnlyNodesAsync(int numNodes) {
+    public synchronized Async<List<String>> startMasterOnlyNodesAsync(int numNodes) {
         return startMasterOnlyNodesAsync(numNodes, Settings.EMPTY);
     }
 
-    public synchronized ListenableFuture<List<String>> startMasterOnlyNodesAsync(int numNodes, Settings settings) {
+    public synchronized Async<List<String>> startMasterOnlyNodesAsync(int numNodes, Settings settings) {
         Settings settings1 = Settings.builder().put(settings).put("node.master", true).put("node.data", false).build();
         return startNodesAsync(numNodes, settings1, Version.CURRENT);
     }
 
-    public synchronized ListenableFuture<List<String>> startDataOnlyNodesAsync(int numNodes) {
+    public synchronized Async<List<String>> startDataOnlyNodesAsync(int numNodes) {
         return startDataOnlyNodesAsync(numNodes, Settings.EMPTY);
     }
 
-    public synchronized ListenableFuture<List<String>> startDataOnlyNodesAsync(int numNodes, Settings settings) {
+    public synchronized Async<List<String>> startDataOnlyNodesAsync(int numNodes, Settings settings) {
         Settings settings1 = Settings.builder().put(settings).put("node.master", false).put("node.data", true).build();
         return startNodesAsync(numNodes, settings1, Version.CURRENT);
     }
 
-    public synchronized ListenableFuture<String> startMasterOnlyNodeAsync() {
+    public synchronized Async<String> startMasterOnlyNodeAsync() {
         return startMasterOnlyNodeAsync(Settings.EMPTY);
     }
 
-    public synchronized ListenableFuture<String> startMasterOnlyNodeAsync(Settings settings) {
+    public synchronized Async<String> startMasterOnlyNodeAsync(Settings settings) {
         Settings settings1 = Settings.builder().put(settings).put("node.master", true).put("node.data", false).build();
         return startNodeAsync(settings1, Version.CURRENT);
     }
@@ -1521,11 +1523,11 @@ public final class InternalTestCluster extends TestCluster {
         return startNode(settings1, Version.CURRENT);
     }
 
-    public synchronized ListenableFuture<String> startDataOnlyNodeAsync() {
+    public synchronized Async<String> startDataOnlyNodeAsync() {
         return startDataOnlyNodeAsync(Settings.EMPTY);
     }
 
-    public synchronized ListenableFuture<String> startDataOnlyNodeAsync(Settings settings) {
+    public synchronized Async<String> startDataOnlyNodeAsync(Settings settings) {
         Settings settings1 = Settings.builder().put(settings).put("node.master", false).put("node.data", true).build();
         return startNodeAsync(settings1, Version.CURRENT);
     }
@@ -1538,74 +1540,78 @@ public final class InternalTestCluster extends TestCluster {
     /**
      * Starts a node in an async manner with the given settings and returns future with its name.
      */
-    public synchronized ListenableFuture<String> startNodeAsync() {
+    public synchronized Async<String> startNodeAsync() {
         return startNodeAsync(Settings.EMPTY, Version.CURRENT);
     }
 
     /**
      * Starts a node in an async manner with the given settings and returns future with its name.
      */
-    public synchronized ListenableFuture<String> startNodeAsync(final Settings settings) {
+    public synchronized Async<String> startNodeAsync(final Settings settings) {
         return startNodeAsync(settings, Version.CURRENT);
     }
 
     /**
      * Starts a node in an async manner with the given settings and version and returns future with its name.
      */
-    public synchronized ListenableFuture<String> startNodeAsync(final Settings settings, final Version version) {
-        final SettableFuture<String> future = SettableFuture.create();
+    public synchronized Async<String> startNodeAsync(final Settings settings, final Version version) {
         final NodeAndClient buildNode = buildNode(settings, version);
-        Runnable startNode = new Runnable() {
-            @Override
-            public void run() {
-                try {
-                    buildNode.node().start();
-                    publishNode(buildNode);
-                    future.set(buildNode.name);
-                } catch (Throwable t) {
-                    future.setException(t);
-                }
-            }
-        };
-        executor.execute(startNode);
-        return future;
+        final Future<String> submit = executor.submit(() -> {
+            buildNode.node().start();
+            publishNode(buildNode);
+            return buildNode.name;
+        });
+        return () -> submit.get();
     }
 
     /**
      * Starts multiple nodes in an async manner and returns future with its name.
      */
-    public synchronized ListenableFuture<List<String>> startNodesAsync(final int numNodes) {
+    public synchronized Async<List<String>> startNodesAsync(final int numNodes) {
         return startNodesAsync(numNodes, Settings.EMPTY, Version.CURRENT);
     }
 
     /**
      * Starts multiple nodes in an async manner with the given settings and returns future with its name.
      */
-    public synchronized ListenableFuture<List<String>> startNodesAsync(final int numNodes, final Settings settings) {
+    public synchronized Async<List<String>> startNodesAsync(final int numNodes, final Settings settings) {
         return startNodesAsync(numNodes, settings, Version.CURRENT);
     }
 
     /**
      * Starts multiple nodes in an async manner with the given settings and version and returns future with its name.
      */
-    public synchronized ListenableFuture<List<String>> startNodesAsync(final int numNodes, final Settings settings, final Version version) {
-        List<ListenableFuture<String>> futures = new ArrayList<>();
+    public synchronized Async<List<String>> startNodesAsync(final int numNodes, final Settings settings, final Version version) {
+        final List<Async<String>> asyncs = new ArrayList<>();
         for (int i = 0; i < numNodes; i++) {
-            futures.add(startNodeAsync(settings, version));
+            asyncs.add(startNodeAsync(settings, version));
         }
-        return Futures.allAsList(futures);
+        
+        return () -> {
+            List<String> ids = new ArrayList<>();
+            for (Async<String> async : asyncs) {
+                ids.add(async.get());
+            }
+            return ids;
+        };
     }
 
     /**
      * Starts multiple nodes (based on the number of settings provided) in an async manner, with explicit settings for each node.
      * The order of the node names returned matches the order of the settings provided.
      */
-    public synchronized ListenableFuture<List<String>> startNodesAsync(final Settings... settings) {
-        List<ListenableFuture<String>> futures = new ArrayList<>();
+    public synchronized Async<List<String>> startNodesAsync(final Settings... settings) {
+        List<Async<String>> asyncs = new ArrayList<>();
         for (Settings setting : settings) {
-            futures.add(startNodeAsync(setting, Version.CURRENT));
+            asyncs.add(startNodeAsync(setting, Version.CURRENT));
         }
-        return Futures.allAsList(futures);
+        return () -> {
+            List<String> ids = new ArrayList<>();
+            for (Async<String> async : asyncs) {
+                ids.add(async.get());
+            }
+            return ids;
+        };
     }
 
     private synchronized void publishNode(NodeAndClient nodeAndClient) {
@@ -1898,4 +1904,12 @@ public final class InternalTestCluster extends TestCluster {
         }
     }
 
+    /**
+     * Simple interface that allows to wait for an async operation to finish
+     * @param <T> the result of the async execution
+     */
+    public interface Async<T> {
+        T get() throws ExecutionException, InterruptedException;
+    }
+
 }
diff --git a/core/src/test/java/org/elasticsearch/test/disruption/NetworkDelaysPartition.java b/core/src/test/java/org/elasticsearch/test/disruption/NetworkDelaysPartition.java
index 8439f6e..9eb9930 100644
--- a/core/src/test/java/org/elasticsearch/test/disruption/NetworkDelaysPartition.java
+++ b/core/src/test/java/org/elasticsearch/test/disruption/NetworkDelaysPartition.java
@@ -60,10 +60,6 @@ public class NetworkDelaysPartition extends NetworkPartition {
         this(nodesSideOne, nodesSideTwo, DEFAULT_DELAY_MIN, DEFAULT_DELAY_MAX, random);
     }
 
-    public NetworkDelaysPartition(Set<String> nodesSideOne, Set<String> nodesSideTwo, long delay, Random random) {
-        this(nodesSideOne, nodesSideTwo, delay, delay, random);
-    }
-
     public NetworkDelaysPartition(Set<String> nodesSideOne, Set<String> nodesSideTwo, long delayMin, long delayMax, Random random) {
         super(nodesSideOne, nodesSideTwo, random);
         this.delayMin = delayMin;
@@ -73,7 +69,7 @@ public class NetworkDelaysPartition extends NetworkPartition {
 
     @Override
     public synchronized void startDisrupting() {
-        duration = new TimeValue(delayMin == delayMax ? delayMin : delayMin + random.nextInt((int) (delayMax - delayMin)));
+        duration = new TimeValue(delayMin + random.nextInt((int) (delayMax - delayMin)));
         super.startDisrupting();
     }
 
diff --git a/core/src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java b/core/src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
index 6a4c883..5772543 100644
--- a/core/src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
+++ b/core/src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
@@ -688,7 +688,7 @@ public class ElasticsearchAssertions {
         ElasticsearchAssertions.assertVersionSerializable(version, new ThrowableWrapper(t));
     }
 
-    private static final class ThrowableWrapper implements Streamable {
+    public static final class ThrowableWrapper implements Streamable {
         Throwable throwable;
         public ThrowableWrapper(Throwable t) {
             throwable = t;
@@ -716,7 +716,6 @@ public class ElasticsearchAssertions {
             Class<? extends Streamable> clazz = streamable.getClass();
             Constructor<? extends Streamable> constructor = clazz.getDeclaredConstructor();
             assertThat(constructor, Matchers.notNullValue());
-            constructor.setAccessible(true);
             Streamable newInstance = constructor.newInstance();
             return newInstance;
         } catch (Throwable e) {
diff --git a/core/src/test/java/org/elasticsearch/test/store/MockFSDirectoryService.java b/core/src/test/java/org/elasticsearch/test/store/MockFSDirectoryService.java
index 9af67d4..86efa38 100644
--- a/core/src/test/java/org/elasticsearch/test/store/MockFSDirectoryService.java
+++ b/core/src/test/java/org/elasticsearch/test/store/MockFSDirectoryService.java
@@ -49,7 +49,6 @@ import org.junit.Assert;
 import java.io.Closeable;
 import java.io.IOException;
 import java.io.PrintStream;
-import java.lang.reflect.Field;
 import java.nio.file.Path;
 import java.util.*;
 
@@ -217,52 +216,10 @@ public class MockFSDirectoryService extends FsDirectoryService {
     public static final class ElasticsearchMockDirectoryWrapper extends MockDirectoryWrapper {
 
         private final boolean crash;
-        private final Set<String> superUnSyncedFiles;
-        private final Random superRandomState;
 
         public ElasticsearchMockDirectoryWrapper(Random random, Directory delegate, boolean crash) {
             super(random, delegate);
             this.crash = crash;
-
-            // TODO: remove all this and cutover to MockFS (DisableFsyncFS) instead
-            try {
-                Field field = MockDirectoryWrapper.class.getDeclaredField("unSyncedFiles");
-                field.setAccessible(true);
-                superUnSyncedFiles = (Set<String>) field.get(this);
-
-                field = MockDirectoryWrapper.class.getDeclaredField("randomState");
-                field.setAccessible(true);
-                superRandomState = (Random) field.get(this);
-            } catch (ReflectiveOperationException roe) {
-                throw new RuntimeException(roe);
-            }
-        }
-
-        /**
-         * Returns true if {@link #in} must sync its files.
-         * Currently, only {@link org.apache.lucene.store.NRTCachingDirectory} requires sync'ing its files
-         * because otherwise they are cached in an internal {@link org.apache.lucene.store.RAMDirectory}. If
-         * other directories require that too, they should be added to this method.
-         */
-        private boolean mustSync() {
-            Directory delegate = in;
-            while (delegate instanceof FilterDirectory) {
-                if (delegate instanceof NRTCachingDirectory) {
-                    return true;
-                }
-                delegate = ((FilterDirectory) delegate).getDelegate();
-            }
-            return delegate instanceof NRTCachingDirectory;
-        }
-
-        @Override
-        public synchronized void sync(Collection<String> names) throws IOException {
-            // don't wear out our hardware so much in tests.
-            if (superRandomState.nextInt(100) == 0 || mustSync()) {
-                super.sync(names);
-            } else {
-                superUnSyncedFiles.removeAll(names);
-            }
         }
 
         @Override
@@ -279,13 +236,7 @@ public class MockFSDirectoryService extends FsDirectoryService {
 
         public CloseableDirectory(BaseDirectoryWrapper dir) {
             this.dir = dir;
-            try {
-                final Field suiteFailureMarker = LuceneTestCase.class.getDeclaredField("suiteFailureMarker");
-                suiteFailureMarker.setAccessible(true);
-                this.failureMarker = (TestRuleMarkFailure) suiteFailureMarker.get(LuceneTestCase.class);
-            } catch (Throwable e) {
-                throw new ElasticsearchException("foo", e);
-            }
+            this.failureMarker = ESTestCase.getSuiteFailureMarker();
         }
 
         @Override
diff --git a/core/src/test/java/org/elasticsearch/test/test/InternalTestClusterTests.java b/core/src/test/java/org/elasticsearch/test/test/InternalTestClusterTests.java
index dcdec27..4d167d7 100644
--- a/core/src/test/java/org/elasticsearch/test/test/InternalTestClusterTests.java
+++ b/core/src/test/java/org/elasticsearch/test/test/InternalTestClusterTests.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.test.test;
 
-import com.google.common.collect.ImmutableSet;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LuceneTestCase;
 import org.elasticsearch.client.Client;
@@ -84,7 +83,7 @@ public class InternalTestClusterTests extends ESTestCase {
     }
 
     public static void assertSettings(Settings left, Settings right, boolean checkClusterUniqueSettings) {
-        ImmutableSet<Map.Entry<String, String>> entries0 = left.getAsMap().entrySet();
+        Set<Map.Entry<String, String>> entries0 = left.getAsMap().entrySet();
         Map<String, String> entries1 = right.getAsMap();
         assertThat(entries0.size(), equalTo(entries1.size()));
         for (Map.Entry<String, String> entry : entries0) {
diff --git a/core/src/test/java/org/elasticsearch/threadpool/UpdateThreadPoolSettingsTests.java b/core/src/test/java/org/elasticsearch/threadpool/UpdateThreadPoolSettingsTests.java
index 9330f71..359ebdd 100644
--- a/core/src/test/java/org/elasticsearch/threadpool/UpdateThreadPoolSettingsTests.java
+++ b/core/src/test/java/org/elasticsearch/threadpool/UpdateThreadPoolSettingsTests.java
@@ -62,7 +62,7 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
         // Replace with different type
         threadPool.updateSettings(settingsBuilder().put("threadpool.search.type", "same").build());
         assertThat(info(threadPool, Names.SEARCH).getType(), equalTo("same"));
-        assertThat(threadPool.executor(Names.SEARCH), instanceOf(MoreExecutors.directExecutor().getClass()));
+        assertThat(threadPool.executor(Names.SEARCH), is(ThreadPool.DIRECT_EXECUTOR));
 
         // Replace with different type again
         threadPool.updateSettings(settingsBuilder()
diff --git a/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java b/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java
index 4499d92..85315be 100644
--- a/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java
+++ b/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java
@@ -742,7 +742,7 @@ public abstract class AbstractSimpleTransportTestCase extends ESTestCase {
     }
 
 
-    static class StringMessageRequest extends TransportRequest {
+    public static class StringMessageRequest extends TransportRequest {
 
         private String message;
         private long timeout;
@@ -752,7 +752,7 @@ public abstract class AbstractSimpleTransportTestCase extends ESTestCase {
             this.timeout = timeout;
         }
 
-        StringMessageRequest() {
+        public StringMessageRequest() {
         }
 
         public StringMessageRequest(String message) {
@@ -803,7 +803,7 @@ public abstract class AbstractSimpleTransportTestCase extends ESTestCase {
     }
 
 
-    static class Version0Request extends TransportRequest {
+    public static class Version0Request extends TransportRequest {
 
         int value1;
 
@@ -821,7 +821,7 @@ public abstract class AbstractSimpleTransportTestCase extends ESTestCase {
         }
     }
 
-    static class Version1Request extends Version0Request {
+    public static class Version1Request extends Version0Request {
 
         int value2;
 
@@ -1213,7 +1213,7 @@ public abstract class AbstractSimpleTransportTestCase extends ESTestCase {
         assertTrue(nodeB.address().sameHost(addressB.get()));
     }
 
-    private static class TestRequest extends TransportRequest {
+    public static class TestRequest extends TransportRequest {
     }
 
     private static class TestResponse extends TransportResponse {
diff --git a/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java b/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java
index 9df8092..2810c09 100644
--- a/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java
+++ b/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java
@@ -358,10 +358,7 @@ public class ContextAndHeaderTransportIT extends ESIntegTestCase {
         Map<String, Object> params = new HashMap<>();
         params.put("query_string", "star wars");
 
-        SearchResponse searchResponse = transportClient().prepareSearch(queryIndex)
-                .setTemplateName("the_template")
-                .setTemplateParams(params)
-                .setTemplateType(ScriptService.ScriptType.INDEXED)
+        SearchResponse searchResponse = transportClient().prepareSearch(queryIndex).setTemplate(new Template("the_template", ScriptType.INDEXED, MustacheScriptEngineService.NAME, null, params))
                 .get();
 
         assertNoFailures(searchResponse);
diff --git a/core/src/test/java/org/elasticsearch/tribe/TribeIT.java b/core/src/test/java/org/elasticsearch/tribe/TribeIT.java
index 1f8b7f1..eb48d65 100644
--- a/core/src/test/java/org/elasticsearch/tribe/TribeIT.java
+++ b/core/src/test/java/org/elasticsearch/tribe/TribeIT.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.tribe;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.util.LuceneTestCase;
 import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
 import org.elasticsearch.action.admin.cluster.health.ClusterHealthStatus;
@@ -52,9 +51,7 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Map;
 
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.notNullValue;
 
@@ -109,7 +106,7 @@ public class TribeIT extends ESIntegTestCase {
     }
 
     private void setupTribeNode(Settings settings) {
-        ImmutableMap<String,String> asMap = internalCluster().getDefaultSettings().getAsMap();
+        Map<String,String> asMap = internalCluster().getDefaultSettings().getAsMap();
         Settings.Builder tribe1Defaults = Settings.builder();
         Settings.Builder tribe2Defaults = Settings.builder();
         for (Map.Entry<String, String> entry : asMap.entrySet()) {
@@ -442,4 +439,4 @@ public class TribeIT extends ESIntegTestCase {
         }
         return unicastHosts.toArray(new String[unicastHosts.size()]);
     }
-}
\ No newline at end of file
+}
diff --git a/core/src/test/resources/org/elasticsearch/index/query/function-filter-score-query.json b/core/src/test/resources/org/elasticsearch/index/query/function-filter-score-query.json
index e78c549..a7b4790 100644
--- a/core/src/test/resources/org/elasticsearch/index/query/function-filter-score-query.json
+++ b/core/src/test/resources/org/elasticsearch/index/query/function-filter-score-query.json
@@ -9,18 +9,12 @@
         },
         "functions":  [
             {
-                "boost_factor": 3,
+                "weight": 3,
                 "filter": {
-                    term:{
+                    "term":{
                         "name.last":"banon"
                     }
                 }
-            },
-            {
-                "boost_factor": 3
-            },
-            {
-                "boost_factor": 3
             }
         ],
         "boost" : 3,
diff --git a/dev-tools/src/main/resources/forbidden/all-signatures.txt b/dev-tools/src/main/resources/forbidden/all-signatures.txt
index 00d4871..443f627 100644
--- a/dev-tools/src/main/resources/forbidden/all-signatures.txt
+++ b/dev-tools/src/main/resources/forbidden/all-signatures.txt
@@ -86,3 +86,28 @@ java.net.InetAddress#getHostName()
 java.net.InetAddress#getCanonicalHostName()
 
 java.net.InetSocketAddress#getHostName() @ Use getHostString() instead, which avoids a DNS lookup
+
+@defaultMessage avoid adding additional dependencies on Guava
+com.google.common.collect.Lists
+com.google.common.collect.ImmutableList
+com.google.common.base.Objects
+com.google.common.base.Predicate
+com.google.common.base.Predicates
+com.google.common.base.Strings
+com.google.common.base.Throwables
+com.google.common.collect.Maps
+com.google.common.collect.Sets
+com.google.common.base.Preconditions#checkNotNull(java.lang.Object)
+com.google.common.base.Preconditions#checkNotNull(java.lang.Object, java.lang.Object)
+com.google.common.base.Preconditions#checkNotNull(java.lang.Object, java.lang.String, java.lang.Object[])
+com.google.common.collect.ImmutableSortedSet
+com.google.common.collect.Queues
+com.google.common.util.concurrent.ListenableFuture
+com.google.common.util.concurrent.SettableFuture
+com.google.common.util.concurrent.Futures
+com.google.common.util.concurrent.MoreExecutors
+com.google.common.collect.ImmutableSortedMap
+
+@defaultMessage Do not violate java's access system
+java.lang.reflect.AccessibleObject#setAccessible(boolean)
+java.lang.reflect.AccessibleObject#setAccessible(java.lang.reflect.AccessibleObject[], boolean)
diff --git a/dev-tools/src/main/resources/forbidden/core-signatures.txt b/dev-tools/src/main/resources/forbidden/core-signatures.txt
index 92792ca..3a925e6 100644
--- a/dev-tools/src/main/resources/forbidden/core-signatures.txt
+++ b/dev-tools/src/main/resources/forbidden/core-signatures.txt
@@ -83,19 +83,3 @@ java.util.concurrent.Future#cancel(boolean)
 @defaultMessage Don't try reading from paths that are not configured in Environment, resolve from Environment instead
 org.elasticsearch.common.io.PathUtils#get(java.lang.String, java.lang.String[])
 org.elasticsearch.common.io.PathUtils#get(java.net.URI)
-
-@defaultMessage avoid adding additional dependencies on Guava
-com.google.common.collect.Lists
-com.google.common.collect.ImmutableList
-com.google.common.base.Objects
-com.google.common.base.Predicate
-com.google.common.base.Predicates
-com.google.common.base.Strings
-com.google.common.base.Throwables
-com.google.common.collect.Maps
-com.google.common.collect.Sets
-com.google.common.base.Preconditions#checkNotNull(java.lang.Object)
-com.google.common.base.Preconditions#checkNotNull(java.lang.Object, java.lang.Object)
-com.google.common.base.Preconditions#checkNotNull(java.lang.Object, java.lang.String, java.lang.Object[])
-com.google.common.collect.ImmutableSortedSet
-com.google.common.collect.Queues
diff --git a/dev-tools/update_lucene.sh b/dev-tools/update_lucene.sh
new file mode 100644
index 0000000..4918c12
--- /dev/null
+++ b/dev-tools/update_lucene.sh
@@ -0,0 +1,14 @@
+#!/bin/sh
+mvn install -DskipTests
+perl dev-tools/src/main/resources/license-check/check_license_and_sha.pl \
+     --update distribution/licenses/ distribution/zip/target/releases/elasticsearch-3.0.0-SNAPSHOT.zip elasticsearch-3.0.0-SNAPSHOT
+perl dev-tools/src/main/resources/license-check/check_license_and_sha.pl \
+     --update plugins/analysis-icu/licenses/ plugins/analysis-icu/target/releases/analysis-icu-3.0.0-SNAPSHOT.zip analysis-icu-3.0.0-SNAPSHOT
+perl dev-tools/src/main/resources/license-check/check_license_and_sha.pl \
+     --update plugins/analysis-kuromoji/licenses/ plugins/analysis-kuromoji/target/releases/analysis-kuromoji-3.0.0-SNAPSHOT.zip analysis-kuromoji-3.0.0-SNAPSHOT
+perl dev-tools/src/main/resources/license-check/check_license_and_sha.pl \
+     --update plugins/analysis-phonetic/licenses/ plugins/analysis-phonetic/target/releases/analysis-phonetic-3.0.0-SNAPSHOT.zip analysis-phonetic-3.0.0-SNAPSHOT
+perl dev-tools/src/main/resources/license-check/check_license_and_sha.pl \
+     --update plugins/analysis-smartcn/licenses/ plugins/analysis-smartcn/target/releases/analysis-smartcn-3.0.0-SNAPSHOT.zip analysis-smartcn-3.0.0-SNAPSHOT
+perl dev-tools/src/main/resources/license-check/check_license_and_sha.pl \
+     --update plugins/analysis-stempel/licenses/ plugins/analysis-stempel/target/releases/analysis-stempel-3.0.0-SNAPSHOT.zip analysis-stempel-3.0.0-SNAPSHOT
diff --git a/distribution/licenses/lucene-analyzers-common-5.4.0-snapshot-1701068.jar.sha1 b/distribution/licenses/lucene-analyzers-common-5.4.0-snapshot-1701068.jar.sha1
deleted file mode 100644
index 3eff5a7..0000000
--- a/distribution/licenses/lucene-analyzers-common-5.4.0-snapshot-1701068.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-35fca29c4597a15ce4d4eb7dc73a517038684a27
diff --git a/distribution/licenses/lucene-analyzers-common-5.4.0-snapshot-1702265.jar.sha1 b/distribution/licenses/lucene-analyzers-common-5.4.0-snapshot-1702265.jar.sha1
new file mode 100644
index 0000000..41febf7
--- /dev/null
+++ b/distribution/licenses/lucene-analyzers-common-5.4.0-snapshot-1702265.jar.sha1
@@ -0,0 +1 @@
+8243b938b75818e86aa8d270d8d99529e1847578
diff --git a/distribution/licenses/lucene-backward-codecs-5.4.0-snapshot-1701068.jar.sha1 b/distribution/licenses/lucene-backward-codecs-5.4.0-snapshot-1701068.jar.sha1
deleted file mode 100644
index 6fe7609..0000000
--- a/distribution/licenses/lucene-backward-codecs-5.4.0-snapshot-1701068.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-e4769b5c05fad8339f4eaf9cfa9e850cbeaa10ec
diff --git a/distribution/licenses/lucene-backward-codecs-5.4.0-snapshot-1702265.jar.sha1 b/distribution/licenses/lucene-backward-codecs-5.4.0-snapshot-1702265.jar.sha1
new file mode 100644
index 0000000..17f6316
--- /dev/null
+++ b/distribution/licenses/lucene-backward-codecs-5.4.0-snapshot-1702265.jar.sha1
@@ -0,0 +1 @@
+ba85c6e5e77e1f76c52c31d34a59558afa135d47
diff --git a/distribution/licenses/lucene-core-5.4.0-snapshot-1701068.jar.sha1 b/distribution/licenses/lucene-core-5.4.0-snapshot-1701068.jar.sha1
deleted file mode 100644
index fb5b2dd..0000000
--- a/distribution/licenses/lucene-core-5.4.0-snapshot-1701068.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-3bbab9d7a395bd0b6cc8b5bee26287105c8659e8
diff --git a/distribution/licenses/lucene-core-5.4.0-snapshot-1702265.jar.sha1 b/distribution/licenses/lucene-core-5.4.0-snapshot-1702265.jar.sha1
new file mode 100644
index 0000000..e6198d4
--- /dev/null
+++ b/distribution/licenses/lucene-core-5.4.0-snapshot-1702265.jar.sha1
@@ -0,0 +1 @@
+f8a38658b6393015c9b33c16b1b4122167b526b2
diff --git a/distribution/licenses/lucene-expressions-5.4.0-snapshot-1701068.jar.sha1 b/distribution/licenses/lucene-expressions-5.4.0-snapshot-1701068.jar.sha1
deleted file mode 100644
index 000759a..0000000
--- a/distribution/licenses/lucene-expressions-5.4.0-snapshot-1701068.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-d60476428e7d3d8a68fe491d42dbda0d4024f589
diff --git a/distribution/licenses/lucene-expressions-5.4.0-snapshot-1702265.jar.sha1 b/distribution/licenses/lucene-expressions-5.4.0-snapshot-1702265.jar.sha1
new file mode 100644
index 0000000..ff8fd36
--- /dev/null
+++ b/distribution/licenses/lucene-expressions-5.4.0-snapshot-1702265.jar.sha1
@@ -0,0 +1 @@
+fa5d27ecadbe346caaf5a571ba71944b51761acf
diff --git a/distribution/licenses/lucene-grouping-5.4.0-snapshot-1701068.jar.sha1 b/distribution/licenses/lucene-grouping-5.4.0-snapshot-1701068.jar.sha1
deleted file mode 100644
index 1688910..0000000
--- a/distribution/licenses/lucene-grouping-5.4.0-snapshot-1701068.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-8618da3f400f0a4b140f196bbbecb0686fe754db
diff --git a/distribution/licenses/lucene-grouping-5.4.0-snapshot-1702265.jar.sha1 b/distribution/licenses/lucene-grouping-5.4.0-snapshot-1702265.jar.sha1
new file mode 100644
index 0000000..7acdc72
--- /dev/null
+++ b/distribution/licenses/lucene-grouping-5.4.0-snapshot-1702265.jar.sha1
@@ -0,0 +1 @@
+2c1464fcf6ede7819f8ba434b9bc7c79f5968407
diff --git a/distribution/licenses/lucene-highlighter-5.4.0-snapshot-1701068.jar.sha1 b/distribution/licenses/lucene-highlighter-5.4.0-snapshot-1701068.jar.sha1
deleted file mode 100644
index 5b6a48e..0000000
--- a/distribution/licenses/lucene-highlighter-5.4.0-snapshot-1701068.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-c7db4fe5587d08ab23b253c622566462aab6796a
diff --git a/distribution/licenses/lucene-highlighter-5.4.0-snapshot-1702265.jar.sha1 b/distribution/licenses/lucene-highlighter-5.4.0-snapshot-1702265.jar.sha1
new file mode 100644
index 0000000..9f46054
--- /dev/null
+++ b/distribution/licenses/lucene-highlighter-5.4.0-snapshot-1702265.jar.sha1
@@ -0,0 +1 @@
+a40f9a3ef224bc042ef2ad1b713e318911b6057a
diff --git a/distribution/licenses/lucene-join-5.4.0-snapshot-1701068.jar.sha1 b/distribution/licenses/lucene-join-5.4.0-snapshot-1701068.jar.sha1
deleted file mode 100644
index 9dbe328..0000000
--- a/distribution/licenses/lucene-join-5.4.0-snapshot-1701068.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-f9c8d435d3e1d553b0dca05c99b1fa377568eed0
diff --git a/distribution/licenses/lucene-join-5.4.0-snapshot-1702265.jar.sha1 b/distribution/licenses/lucene-join-5.4.0-snapshot-1702265.jar.sha1
new file mode 100644
index 0000000..01a2d77
--- /dev/null
+++ b/distribution/licenses/lucene-join-5.4.0-snapshot-1702265.jar.sha1
@@ -0,0 +1 @@
+0a7642c9b98cb3d9013fb33be5c0751baf9f0b31
diff --git a/distribution/licenses/lucene-memory-5.4.0-snapshot-1701068.jar.sha1 b/distribution/licenses/lucene-memory-5.4.0-snapshot-1701068.jar.sha1
deleted file mode 100644
index 1c0f2f5..0000000
--- a/distribution/licenses/lucene-memory-5.4.0-snapshot-1701068.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-571dd2e4363f0a0410de04b3f3f4bbf66e782c31
diff --git a/distribution/licenses/lucene-memory-5.4.0-snapshot-1702265.jar.sha1 b/distribution/licenses/lucene-memory-5.4.0-snapshot-1702265.jar.sha1
new file mode 100644
index 0000000..6f9b8b4
--- /dev/null
+++ b/distribution/licenses/lucene-memory-5.4.0-snapshot-1702265.jar.sha1
@@ -0,0 +1 @@
+a0d6461ab9cda93ea530560b0c074a28fe0dd717
diff --git a/distribution/licenses/lucene-misc-5.4.0-snapshot-1701068.jar.sha1 b/distribution/licenses/lucene-misc-5.4.0-snapshot-1701068.jar.sha1
deleted file mode 100644
index 0d44482..0000000
--- a/distribution/licenses/lucene-misc-5.4.0-snapshot-1701068.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-423264f839aace3b9159a0dd54f56c250458fd46
diff --git a/distribution/licenses/lucene-misc-5.4.0-snapshot-1702265.jar.sha1 b/distribution/licenses/lucene-misc-5.4.0-snapshot-1702265.jar.sha1
new file mode 100644
index 0000000..f274d12
--- /dev/null
+++ b/distribution/licenses/lucene-misc-5.4.0-snapshot-1702265.jar.sha1
@@ -0,0 +1 @@
+85c5c7b78715c50157700c90ffd101537446533d
diff --git a/distribution/licenses/lucene-queries-5.4.0-snapshot-1701068.jar.sha1 b/distribution/licenses/lucene-queries-5.4.0-snapshot-1701068.jar.sha1
deleted file mode 100644
index a4391c6..0000000
--- a/distribution/licenses/lucene-queries-5.4.0-snapshot-1701068.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-872530eeac156faa0989eb87145bbef74a72e66f
diff --git a/distribution/licenses/lucene-queries-5.4.0-snapshot-1702265.jar.sha1 b/distribution/licenses/lucene-queries-5.4.0-snapshot-1702265.jar.sha1
new file mode 100644
index 0000000..295a311
--- /dev/null
+++ b/distribution/licenses/lucene-queries-5.4.0-snapshot-1702265.jar.sha1
@@ -0,0 +1 @@
+70ca782d6ed458b5f777141353e09600083ed4fe
diff --git a/distribution/licenses/lucene-queryparser-5.4.0-snapshot-1701068.jar.sha1 b/distribution/licenses/lucene-queryparser-5.4.0-snapshot-1701068.jar.sha1
deleted file mode 100644
index 29c85e8..0000000
--- a/distribution/licenses/lucene-queryparser-5.4.0-snapshot-1701068.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-6f6b6a024ca96017252efea6d2fc7dc97c69febd
diff --git a/distribution/licenses/lucene-queryparser-5.4.0-snapshot-1702265.jar.sha1 b/distribution/licenses/lucene-queryparser-5.4.0-snapshot-1702265.jar.sha1
new file mode 100644
index 0000000..88c548a
--- /dev/null
+++ b/distribution/licenses/lucene-queryparser-5.4.0-snapshot-1702265.jar.sha1
@@ -0,0 +1 @@
+b4832cdfe7a6cc7c586a3e28d7cd530acb182232
diff --git a/distribution/licenses/lucene-sandbox-5.4.0-snapshot-1701068.jar.sha1 b/distribution/licenses/lucene-sandbox-5.4.0-snapshot-1701068.jar.sha1
deleted file mode 100644
index dbc3ec8..0000000
--- a/distribution/licenses/lucene-sandbox-5.4.0-snapshot-1701068.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-a6f5a5c84b165ebde104cdcde46fa9c5948650f0
diff --git a/distribution/licenses/lucene-sandbox-5.4.0-snapshot-1702265.jar.sha1 b/distribution/licenses/lucene-sandbox-5.4.0-snapshot-1702265.jar.sha1
new file mode 100644
index 0000000..54f5944
--- /dev/null
+++ b/distribution/licenses/lucene-sandbox-5.4.0-snapshot-1702265.jar.sha1
@@ -0,0 +1 @@
+bde73ae2b2324e1576c5789a7e6dd88b6543b939
diff --git a/distribution/licenses/lucene-spatial-5.4.0-snapshot-1701068.jar.sha1 b/distribution/licenses/lucene-spatial-5.4.0-snapshot-1701068.jar.sha1
deleted file mode 100644
index 1e2c1dc..0000000
--- a/distribution/licenses/lucene-spatial-5.4.0-snapshot-1701068.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-a305601f93b6cb02444816c96276a74f91ac7d40
diff --git a/distribution/licenses/lucene-spatial-5.4.0-snapshot-1702265.jar.sha1 b/distribution/licenses/lucene-spatial-5.4.0-snapshot-1702265.jar.sha1
new file mode 100644
index 0000000..f3aa363
--- /dev/null
+++ b/distribution/licenses/lucene-spatial-5.4.0-snapshot-1702265.jar.sha1
@@ -0,0 +1 @@
+8d261ff1c2333ce1e040c3aefca9784d1ae71acc
diff --git a/distribution/licenses/lucene-spatial3d-5.4.0-snapshot-1701068.jar.sha1 b/distribution/licenses/lucene-spatial3d-5.4.0-snapshot-1701068.jar.sha1
deleted file mode 100644
index ab2be14..0000000
--- a/distribution/licenses/lucene-spatial3d-5.4.0-snapshot-1701068.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-ef1fcaa5b6663dd9382719a1ad40d86fc962c690
diff --git a/distribution/licenses/lucene-spatial3d-5.4.0-snapshot-1702265.jar.sha1 b/distribution/licenses/lucene-spatial3d-5.4.0-snapshot-1702265.jar.sha1
new file mode 100644
index 0000000..d23a271
--- /dev/null
+++ b/distribution/licenses/lucene-spatial3d-5.4.0-snapshot-1702265.jar.sha1
@@ -0,0 +1 @@
+ee041e52dfcdb33a1aa6fab112042b5f33fc0c0c
diff --git a/distribution/licenses/lucene-suggest-5.4.0-snapshot-1701068.jar.sha1 b/distribution/licenses/lucene-suggest-5.4.0-snapshot-1701068.jar.sha1
deleted file mode 100644
index dd69c53..0000000
--- a/distribution/licenses/lucene-suggest-5.4.0-snapshot-1701068.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-3698e0623f45e181d2ceead46e48a6dd8c2867dd
diff --git a/distribution/licenses/lucene-suggest-5.4.0-snapshot-1702265.jar.sha1 b/distribution/licenses/lucene-suggest-5.4.0-snapshot-1702265.jar.sha1
new file mode 100644
index 0000000..f2307a3
--- /dev/null
+++ b/distribution/licenses/lucene-suggest-5.4.0-snapshot-1702265.jar.sha1
@@ -0,0 +1 @@
+a8ceb11b26e53612eee9a265ff454351f6dc99f2
diff --git a/docs/reference/aggregations/bucket/histogram-aggregation.asciidoc b/docs/reference/aggregations/bucket/histogram-aggregation.asciidoc
index bc8a6e1..e01a067 100644
--- a/docs/reference/aggregations/bucket/histogram-aggregation.asciidoc
+++ b/docs/reference/aggregations/bucket/histogram-aggregation.asciidoc
@@ -148,7 +148,7 @@ Example:
 --------------------------------------------------
 {
     "query" : {
-        "filtered" : { "filter": { "range" : { "price" : { "to" : "500" } } } }
+        "constant_score" : { "filter": { "range" : { "price" : { "to" : "500" } } } }
     },
     "aggs" : {
         "prices" : {
diff --git a/docs/reference/aggregations/metrics/sum-aggregation.asciidoc b/docs/reference/aggregations/metrics/sum-aggregation.asciidoc
index 98286e9..d55fcd0 100644
--- a/docs/reference/aggregations/metrics/sum-aggregation.asciidoc
+++ b/docs/reference/aggregations/metrics/sum-aggregation.asciidoc
@@ -9,8 +9,7 @@ Assuming the data consists of documents representing stock ticks, where each tic
 --------------------------------------------------
 {
     "query" : {
-        "filtered" : {
-            "query" : { "match_all" : {}},
+        "constant_score" : {
             "filter" : {
                 "range" : { "timestamp" : { "from" : "now/1d+9.5h", "to" : "now/1d+16h" }}
             }
diff --git a/docs/reference/getting-started.asciidoc b/docs/reference/getting-started.asciidoc
index e69cfb4..473287d 100755
--- a/docs/reference/getting-started.asciidoc
+++ b/docs/reference/getting-started.asciidoc
@@ -862,17 +862,17 @@ In the previous section, we skipped over a little detail called the document sco
 
 But queries do not always need to produce scores, in particular when they are only used for "filtering" the document set. Elasticsearch detects these situations and automatically optimizes query execution in order not to compute useless scores.
 
-To understand filters, let's first introduce the <<query-dsl-filtered-query,`filtered` query>>, which allows you to combine a query (like `match_all`, `match`, `bool`, etc.) together with another query which is only used for filtering. As an example, let's introduce the <<query-dsl-range-query,`range` query>>, which allows us to filter documents by a range of values. This is generally used for numeric or date filtering.
+The <<query-dsl-range-query,`range` query>> that we introduced in the previous section also supports `filter` clauses which allow to use a query to restrict the documents that will be matched by other clauses, without changing how scores are computed. As an example, let's introduce the <<query-dsl-range-query,`range` query>>, which allows us to filter documents by a range of values. This is generally used for numeric or date filtering.
 
-This example uses a filtered query to return all accounts with balances between 20000 and 30000, inclusive. In other words, we want to find accounts with a balance that is greater than or equal to 20000 and less than or equal to 30000.
+This example uses a bool query to return all accounts with balances between 20000 and 30000, inclusive. In other words, we want to find accounts with a balance that is greater than or equal to 20000 and less than or equal to 30000.
 
 [source,sh]
 --------------------------------------------------
 curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
 {
   "query": {
-    "filtered": {
-      "query": { "match_all": {} },
+    "bool": {
+      "must": { "match_all": {} },
       "filter": {
         "range": {
           "balance": {
@@ -886,9 +886,9 @@ curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
 }'
 --------------------------------------------------
 
-Dissecting the above, the filtered query contains a `match_all` query (the query part) and a `range` query (the filter part). We can substitute any other queries into the query and the filter parts. In the above case, the range query makes perfect sense since documents falling into the range all match "equally", i.e., no document is more relevant than another.
+Dissecting the above, the bool query contains a `match_all` query (the query part) and a `range` query (the filter part). We can substitute any other queries into the query and the filter parts. In the above case, the range query makes perfect sense since documents falling into the range all match "equally", i.e., no document is more relevant than another.
 
-In addition to the `match_all`, `match`, `bool`, `filtered`, and `range` queries, there are a lot of other query types that are available and we won't go into them here. Since we already have a basic understanding of how they work, it shouldn't be too difficult to apply this knowledge in learning and experimenting with the other query types.
+In addition to the `match_all`, `match`, `bool`, and `range` queries, there are a lot of other query types that are available and we won't go into them here. Since we already have a basic understanding of how they work, it shouldn't be too difficult to apply this knowledge in learning and experimenting with the other query types.
 
 === Executing Aggregations
 
diff --git a/docs/reference/mapping/fields/source-field.asciidoc b/docs/reference/mapping/fields/source-field.asciidoc
index 767b6af..ec40f66 100644
--- a/docs/reference/mapping/fields/source-field.asciidoc
+++ b/docs/reference/mapping/fields/source-field.asciidoc
@@ -15,10 +15,11 @@ within the index. For this reason, it can be disabled as follows:
 --------------------------------------------------
 PUT tweets
 {
-  "mappings": {},
-  "tweet": {
-    "_source": {
-      "enabled": false
+  "mappings": {
+    "tweet": {
+      "_source": {
+        "enabled": false
+      }
     }
   }
 }
diff --git a/docs/reference/migration/migrate_1_0.asciidoc b/docs/reference/migration/migrate_1_0.asciidoc
index 66b1245..c8750d1 100644
--- a/docs/reference/migration/migrate_1_0.asciidoc
+++ b/docs/reference/migration/migrate_1_0.asciidoc
@@ -188,7 +188,7 @@ GET /_count
 Also, the top-level `filter` parameter in search has been renamed to
 <<search-request-post-filter,`post_filter`>>, to indicate that it should not
 be used as the primary way to filter search results (use a
-<<query-dsl-filtered-query,`filtered` query>> instead), but only to filter
+<<query-dsl-bool-query,`bool` query>> instead), but only to filter
 results AFTER aggregations have been calculated.
 
 This example counts the top colors in all matching docs, but only returns docs
diff --git a/docs/reference/modules/discovery/zen.asciidoc b/docs/reference/modules/discovery/zen.asciidoc
index 2ad7133..fa5ad6a 100644
--- a/docs/reference/modules/discovery/zen.asciidoc
+++ b/docs/reference/modules/discovery/zen.asciidoc
@@ -108,18 +108,12 @@ considered failed. Defaults to `3`.
 The master node is the only node in a cluster that can make changes to the
 cluster state. The master node processes one cluster state update at a time,
 applies the required changes and publishes the updated cluster state to all
-the other nodes in the cluster. Each node receives the publish message, acknowledges
-it, but does *not* yet apply it. If the master does not receive acknowledgement from
-at least `discovery.zen.minimum_master_nodes` nodes within a certain time (controlled by
-the `discovery.zen.commit_timeout` setting and defaults to 30 seconds) the cluster state
-change is rejected.
-
-Once enough nodes have responded, the cluster state is committed and a message will
-be sent to all the nodes. The nodes then proceed to apply the new cluster state to their
-internal state. The master node waits for all nodes to respond, up to a timeout, before
-going ahead processing the next updates in the queue. The `discovery.zen.publish_timeout` is
-set by default to 30 seconds and is measured from the moment the publishing started. Both
-timeout settings can be changed dynamically through the <<cluster-update-settings,cluster update settings api>>
+the other nodes in the cluster. Each node receives the publish message,
+updates its own cluster state and replies to the master node, which waits for
+all nodes to respond, up to a timeout, before going ahead processing the next
+updates in the queue. The `discovery.zen.publish_timeout` is set by default
+to 30 seconds and can be changed dynamically through the
+<<cluster-update-settings,cluster update settings api>>
 
 [float]
 [[no-master-block]]
diff --git a/docs/reference/query-dsl/and-query.asciidoc b/docs/reference/query-dsl/and-query.asciidoc
deleted file mode 100644
index 5ef23af..0000000
--- a/docs/reference/query-dsl/and-query.asciidoc
+++ /dev/null
@@ -1,34 +0,0 @@
-[[query-dsl-and-query]]
-=== And Query
-
-deprecated[2.0.0-beta1, Use the `bool` query instead]
-
-A query that matches documents using the `AND` boolean operator on other
-queries.
-
-[source,js]
---------------------------------------------------
-{
-    "filtered" : {
-        "query" : {
-            "term" : { "name.first" : "shay" }
-        },
-        "filter" : {
-            "and" : [
-                {
-                    "range" : { 
-                        "postDate" : { 
-                            "from" : "2010-03-01",
-                            "to" : "2010-04-01"
-                        }
-                    }
-                },
-                {
-                    "prefix" : { "name.second" : "ba" }
-                }
-            ]
-        }
-    }
-}
---------------------------------------------------
-
diff --git a/docs/reference/query-dsl/compound-queries.asciidoc b/docs/reference/query-dsl/compound-queries.asciidoc
index 0228ddd..c6a1f0f 100644
--- a/docs/reference/query-dsl/compound-queries.asciidoc
+++ b/docs/reference/query-dsl/compound-queries.asciidoc
@@ -41,18 +41,6 @@ documents which also match a `negative` query.
 
 Execute one query for the specified indices, and another for other indices.
 
-<<query-dsl-and-query,`and`>>, <<query-dsl-or-query,`or`>>, <<query-dsl-not-query,`not`>>::
-
-Synonyms for the `bool` query.
-
-<<query-dsl-filtered-query,`filtered` query>>::
-
-Combine a query clause in query context with another in filter context. deprecated[2.0.0-beta1,Use the `bool` query instead]
-
-<<query-dsl-limit-query,`limit` query>>::
-
-Limits the number of documents examined per shard.
-
 
 include::constant-score-query.asciidoc[]
 include::bool-query.asciidoc[]
@@ -60,10 +48,5 @@ include::dis-max-query.asciidoc[]
 include::function-score-query.asciidoc[]
 include::boosting-query.asciidoc[]
 include::indices-query.asciidoc[]
-include::and-query.asciidoc[]
 include::not-query.asciidoc[]
-include::or-query.asciidoc[]
-include::filtered-query.asciidoc[]
-include::limit-query.asciidoc[]
-
 
diff --git a/docs/reference/query-dsl/filtered-query.asciidoc b/docs/reference/query-dsl/filtered-query.asciidoc
deleted file mode 100644
index 5d399d0..0000000
--- a/docs/reference/query-dsl/filtered-query.asciidoc
+++ /dev/null
@@ -1,96 +0,0 @@
-[[query-dsl-filtered-query]]
-=== Filtered Query
-
-deprecated[2.0.0-beta1, Use the `bool` query instead with a `must` clause for the query and a `filter` clause for the filter]
-
-The `filtered` query is used to combine a query which will be used for
-scoring with another query which will only be used for filtering the result
-set.
-
-TIP: Exclude as many document as you can with a filter, then query just the
-documents that remain.
-
-[source,js]
---------------------------------------------------
-{
-  "filtered": {
-    "query": {
-      "match": { "tweet": "full text search" }
-    },
-    "filter": {
-      "range": { "created": { "gte": "now-1d/d" }}
-    }
-  }
-}
---------------------------------------------------
-
-The `filtered` query can be used wherever a `query` is expected, for instance,
-to use the above example in search request:
-
-[source,js]
---------------------------------------------------
-curl -XGET localhost:9200/_search -d '
-{
-  "query": {
-    "filtered": { <1>
-      "query": {
-        "match": { "tweet": "full text search" }
-      },
-      "filter": {
-        "range": { "created": { "gte": "now-1d/d" }}
-      }
-    }
-  }
-}
-'
---------------------------------------------------
-<1> The `filtered` query is passed as the value of the `query`
-    parameter in the search request.
-
-==== Filtering without a query
-
-If a `query` is not specified, it defaults to the
-<<query-dsl-match-all-query,`match_all` query>>.  This means that the
-`filtered` query can be used to wrap just a filter, so that it can be used
-wherever a query is expected.
-
-[source,js]
---------------------------------------------------
-curl -XGET localhost:9200/_search -d '
-{
-  "query": {
-    "filtered": { <1>
-      "filter": {
-        "range": { "created": { "gte": "now-1d/d" }}
-      }
-    }
-  }
-}
-'
---------------------------------------------------
-<1> No `query` has been specified, so this request applies just the filter,
-   returning all documents created since yesterday.
-
-===== Multiple filters
-
-Multiple filters can be applied by wrapping them in a
-<<query-dsl-bool-query,`bool` query>>, for example:
-
-[source,js]
---------------------------------------------------
-{
-  "filtered": {
-    "query": { "match": { "tweet": "full text search" }},
-    "filter": {
-      "bool": {
-        "must": { "range": { "created": { "gte": "now-1d/d" }}},
-        "should": [
-          { "term": { "featured": true }},
-          { "term": { "starred":  true }}
-        ],
-        "must_not": { "term": { "deleted": false }}
-      }
-    }
-  }
-}
---------------------------------------------------
diff --git a/docs/reference/query-dsl/geo-bounding-box-query.asciidoc b/docs/reference/query-dsl/geo-bounding-box-query.asciidoc
index 60aab15..6710461 100644
--- a/docs/reference/query-dsl/geo-bounding-box-query.asciidoc
+++ b/docs/reference/query-dsl/geo-bounding-box-query.asciidoc
@@ -22,8 +22,8 @@ Then the following simple query can be executed with a
 [source,js]
 --------------------------------------------------
 {
-    "filtered" : {
-        "query" : {
+    "bool" : {
+        "must" : {
             "match_all" : {}
         },
         "filter" : {
@@ -75,8 +75,8 @@ representation of the geo point, the filter can accept it as well:
 [source,js]
 --------------------------------------------------
 {
-    "filtered" : {
-        "query" : {
+    "boost" : {
+        "must" : {
             "match_all" : {}
         },
         "filter" : {
@@ -106,8 +106,8 @@ conform with http://geojson.org/[GeoJSON].
 [source,js]
 --------------------------------------------------
 {
-    "filtered" : {
-        "query" : {
+    "bool" : {
+        "must" : {
             "match_all" : {}
         },
         "filter" : {
@@ -130,8 +130,8 @@ Format in `lat,lon`.
 [source,js]
 --------------------------------------------------
 {
-    "filtered" : {
-        "query" : {
+    "bool" : {
+        "must" : {
             "match_all" : {}
         },
         "filter" : {
@@ -152,8 +152,8 @@ Format in `lat,lon`.
 [source,js]
 --------------------------------------------------
 {
-    "filtered" : {
-        "query" : {
+    "bool" : {
+        "must" : {
             "match_all" : {}
         },
         "filter" : {
@@ -181,8 +181,8 @@ values separately.
 [source,js]
 --------------------------------------------------
 {
-    "filtered" : {
-        "query" : {
+    "bool" : {
+        "must" : {
             "match_all" : {}
         },
         "filter" : {
@@ -227,8 +227,8 @@ are not supported. Here is an example:
 [source,js]
 --------------------------------------------------
 {
-    "filtered" : {
-        "query" : {
+    "bool" : {
+        "must" : {
             "match_all" : {}
         },
         "filter" : {
diff --git a/docs/reference/query-dsl/geo-distance-query.asciidoc b/docs/reference/query-dsl/geo-distance-query.asciidoc
index 130319d..c5b6029 100644
--- a/docs/reference/query-dsl/geo-distance-query.asciidoc
+++ b/docs/reference/query-dsl/geo-distance-query.asciidoc
@@ -22,8 +22,8 @@ filter:
 [source,js]
 --------------------------------------------------
 {
-    "filtered" : {
-        "query" : {
+    "bool" : {
+        "must" : {
             "match_all" : {}
         },
         "filter" : {
@@ -51,8 +51,8 @@ representation of the geo point, the filter can accept it as well:
 [source,js]
 --------------------------------------------------
 {
-    "filtered" : {
-        "query" : {
+    "bool" : {
+        "must" : {
             "match_all" : {}
         },
         "filter" : {
@@ -77,8 +77,8 @@ conform with http://geojson.org/[GeoJSON].
 [source,js]
 --------------------------------------------------
 {
-    "filtered" : {
-        "query" : {
+    "bool" : {
+        "must" : {
             "match_all" : {}
         },
         "filter" : {
@@ -99,8 +99,8 @@ Format in `lat,lon`.
 [source,js]
 --------------------------------------------------
 {
-    "filtered" : {
-        "query" : {
+    "bool" : {
+        "must" : {
             "match_all" : {}
         },
         "filter" : {
@@ -119,8 +119,8 @@ Format in `lat,lon`.
 [source,js]
 --------------------------------------------------
 {
-    "filtered" : {
-        "query" : {
+    "bool" : {
+        "must" : {
             "match_all" : {}
         },
         "filter" : {
diff --git a/docs/reference/query-dsl/geo-distance-range-query.asciidoc b/docs/reference/query-dsl/geo-distance-range-query.asciidoc
index cacf0a7..901cca0 100644
--- a/docs/reference/query-dsl/geo-distance-range-query.asciidoc
+++ b/docs/reference/query-dsl/geo-distance-range-query.asciidoc
@@ -6,8 +6,8 @@ Filters documents that exists within a range from a specific point:
 [source,js]
 --------------------------------------------------
 {
-    "filtered" : {
-        "query" : {
+    "boost" : {
+        "must" : {
             "match_all" : {}
         },
         "filter" : {
diff --git a/docs/reference/query-dsl/geo-polygon-query.asciidoc b/docs/reference/query-dsl/geo-polygon-query.asciidoc
index ff53a35..306b2dd 100644
--- a/docs/reference/query-dsl/geo-polygon-query.asciidoc
+++ b/docs/reference/query-dsl/geo-polygon-query.asciidoc
@@ -7,7 +7,7 @@ points. Here is an example:
 [source,js]
 --------------------------------------------------
 {
-    "filtered" : {
+    "bool" : {
         "query" : {
             "match_all" : {}
         },
@@ -53,8 +53,8 @@ conform with http://geojson.org/[GeoJSON].
 [source,js]
 --------------------------------------------------
 {
-    "filtered" : {
-        "query" : {
+    "bool" : {
+        "must" : {
             "match_all" : {}
         },
         "filter" : {
@@ -80,8 +80,8 @@ Format in `lat,lon`.
 [source,js]
 --------------------------------------------------
 {
-    "filtered" : {
-        "query" : {
+    "bool" : {
+        "must" : {
             "match_all" : {}
         },
         "filter" : {
@@ -105,8 +105,8 @@ Format in `lat,lon`.
 [source,js]
 --------------------------------------------------
 {
-    "filtered" : {
-        "query" : {
+    "bool" : {
+        "must" : {
             "match_all" : {}
         },
         "filter" : {
diff --git a/docs/reference/query-dsl/geo-shape-query.asciidoc b/docs/reference/query-dsl/geo-shape-query.asciidoc
index 8ab2f13..77deabc 100644
--- a/docs/reference/query-dsl/geo-shape-query.asciidoc
+++ b/docs/reference/query-dsl/geo-shape-query.asciidoc
@@ -40,8 +40,8 @@ The following query will find the point using the Elasticsearch's
 --------------------------------------------------
 {
     "query":{
-        "filtered": {
-            "query": {
+        "bool": {
+            "must": {
                 "match_all": {}
             },
             "filter": {
@@ -81,8 +81,8 @@ shape:
 [source,js]
 --------------------------------------------------
 {
-    "filtered": {
-        "query": {
+    "bool": {
+        "must": {
             "match_all": {}
         },
         "filter": {
diff --git a/docs/reference/query-dsl/geohash-cell-query.asciidoc b/docs/reference/query-dsl/geohash-cell-query.asciidoc
index c3c83de..807e867 100644
--- a/docs/reference/query-dsl/geohash-cell-query.asciidoc
+++ b/docs/reference/query-dsl/geohash-cell-query.asciidoc
@@ -43,8 +43,8 @@ next to the given cell.
 [source,js]
 --------------------------------------------------
 {
-    "filtered" : {
-        "query" : {
+    "bool" : {
+        "must" : {
             "match_all" : {}
         },
         "filter" : {
diff --git a/docs/reference/query-dsl/limit-query.asciidoc b/docs/reference/query-dsl/limit-query.asciidoc
deleted file mode 100644
index 198ad78..0000000
--- a/docs/reference/query-dsl/limit-query.asciidoc
+++ /dev/null
@@ -1,19 +0,0 @@
-[[query-dsl-limit-query]]
-=== Limit Query
-
-A limit query limits the number of documents (per shard) to execute on.
-For example:
-
-[source,js]
---------------------------------------------------
-{
-    "filtered" : {
-        "filter" : {
-             "limit" : {"value" : 100}
-         },
-         "query" : {
-            "term" : { "name.first" : "shay" }
-        }
-    }
-}
---------------------------------------------------
diff --git a/docs/reference/query-dsl/not-query.asciidoc b/docs/reference/query-dsl/not-query.asciidoc
index a74a0f1..7854ee9 100644
--- a/docs/reference/query-dsl/not-query.asciidoc
+++ b/docs/reference/query-dsl/not-query.asciidoc
@@ -6,8 +6,8 @@ A query that filters out matched documents using a query. For example:
 [source,js]
 --------------------------------------------------
 {
-    "filtered" : {
-        "query" : {
+    "bool" : {
+        "must" : {
             "term" : { "name.first" : "shay" }
         },
         "filter" : {
@@ -29,8 +29,8 @@ Or, in a longer form with a `filter` element:
 [source,js]
 --------------------------------------------------
 {
-    "filtered" : {
-        "query" : {
+    "bool" : {
+        "must" : {
             "term" : { "name.first" : "shay" }
         },
         "filter" : {
diff --git a/docs/reference/query-dsl/or-query.asciidoc b/docs/reference/query-dsl/or-query.asciidoc
deleted file mode 100644
index 46005dc..0000000
--- a/docs/reference/query-dsl/or-query.asciidoc
+++ /dev/null
@@ -1,29 +0,0 @@
-[[query-dsl-or-query]]
-=== Or Query
-
-deprecated[2.0.0-beta1, Use the `bool` query instead]
-
-A query that matches documents using the `OR` boolean operator on other
-queries.
-
-[source,js]
---------------------------------------------------
-{
-    "filtered" : {
-        "query" : {
-            "term" : { "name.first" : "shay" }
-        },
-        "filter" : {
-            "or" : [
-                {
-                    "term" : { "name.second" : "banon" }
-                },
-                {
-                    "term" : { "name.nick" : "kimchy" }
-                }
-            ]
-        }
-    }
-}
---------------------------------------------------
-
diff --git a/docs/reference/query-dsl/script-query.asciidoc b/docs/reference/query-dsl/script-query.asciidoc
index c14e814..223460f 100644
--- a/docs/reference/query-dsl/script-query.asciidoc
+++ b/docs/reference/query-dsl/script-query.asciidoc
@@ -2,13 +2,13 @@
 === Script Query
 
 A query allowing to define
-<<modules-scripting,scripts>> as filters. For
-example:
+<<modules-scripting,scripts>> as queries. They are typically used in a filter
+context, for example:
 
 [source,js]
 ----------------------------------------------
-"filtered" : {
-    "query" : {
+"bool" : {
+    "must" : {
         ...
     }, 
     "filter" : {
@@ -28,8 +28,8 @@ to use the ability to pass parameters to the script itself, for example:
 
 [source,js]
 ----------------------------------------------
-"filtered" : {
-    "query" : {
+"bool" : {
+    "must" : {
         ...
     }, 
     "filter" : {
diff --git a/docs/reference/query-dsl/terms-query.asciidoc b/docs/reference/query-dsl/terms-query.asciidoc
index 58b0ba5..16cfd8c 100644
--- a/docs/reference/query-dsl/terms-query.asciidoc
+++ b/docs/reference/query-dsl/terms-query.asciidoc
@@ -79,16 +79,12 @@ curl -XPUT localhost:9200/tweets/tweet/1 -d '{
 # search on all the tweets that match the followers of user 2
 curl -XGET localhost:9200/tweets/_search -d '{
   "query" : {
-    "filtered" : {
-      "filter" : {
-        "terms" : {
-          "user" : {
-            "index" : "users",
-            "type" : "user",
-            "id" : "2",
-            "path" : "followers"
-          }
-        }
+    "terms" : {
+      "user" : {
+        "index" : "users",
+        "type" : "user",
+        "id" : "2",
+        "path" : "followers"
       }
     }
   }
diff --git a/docs/reference/redirects.asciidoc b/docs/reference/redirects.asciidoc
index 9c6a9b4..922216a 100644
--- a/docs/reference/redirects.asciidoc
+++ b/docs/reference/redirects.asciidoc
@@ -77,20 +77,6 @@ in ``query context'' and as a filter in ``filter context'' (see <<query-dsl>>).
 Queries and filters have been merged.  Any query clause can now be used as a query
 in ``query context'' and as a filter in ``filter context'' (see <<query-dsl>>).
 
-[role="exclude",id="query-dsl-and-filter"]
-=== And Filter
-
-The `and` filter has been replaced by the <<query-dsl-and-query>>.  It behaves
-as a query in ``query context'' and as a filter in ``filter context'' (see
-<<query-dsl>>).
-
-[role="exclude",id="query-dsl-or-filter"]
-=== Or Filter
-
-The `or` filter has been replaced by the <<query-dsl-or-query>>.  It behaves
-as a query in ``query context'' and as a filter in ``filter context'' (see
-<<query-dsl>>).
-
 [role="exclude",id="query-dsl-not-filter"]
 === Not Filter
 
@@ -195,13 +181,6 @@ The `indices` filter has been replaced by the <<query-dsl-indices-query>>.  It b
 as a query in ``query context'' and as a filter in ``filter context'' (see
 <<query-dsl>>).
 
-[role="exclude",id="query-dsl-limit-filter"]
-=== Limit Filter
-
-The `limit` filter has been replaced by the <<query-dsl-limit-query>>.
-It behaves as a query in ``query context'' and as a filter in ``filter
-context'' (see <<query-dsl>>).
-
 [role="exclude",id="query-dsl-match-all-filter"]
 === Match All Filter
 
@@ -381,3 +360,86 @@ The shard query cache has been renamed <<shard-request-cache>>.
 === Query cache
 
 The filter cache has been renamed <<query-cache>>.
+
+[role="exclude",id="query-dsl-filtered-query"]
+=== Filtered query
+
+The `filtered` query is replaced in favour of the <<query-dsl-bool-query,bool>> query. Instead of
+the following:
+
+[source,js]
+-------------------------
+GET _search
+{
+  "query": {
+    "filtered": {
+      "query": {
+        "match": {
+          "text": "quick brown fox"
+        }
+      },
+      "filter": {
+        "term": {
+          "status": "published"
+        }
+      }
+    }
+  }
+}
+-------------------------
+
+move the query and filter to the `must` and `filter` parameters in the `bool`
+query:
+
+[source,js]
+-------------------------
+GET _search
+{
+  "query": {
+    "bool": {
+      "must": {
+        "match": {
+          "text": "quick brown fox"
+        }
+      },
+      "filter": {
+        "term": {
+          "status": "published"
+        }
+      }
+    }
+  }
+}
+-------------------------
+
+[role="exclude",id="query-dsl-or-query"]
+=== Or query
+
+The `or` query is replaced in favour of the <<query-dsl-bool-query,bool>> query.
+
+[role="exclude",id="query-dsl-or-filter"]
+=== Or filter
+
+The `or` filter is replaced in favour of the <<query-dsl-bool-query,bool>> query.
+
+[role="exclude",id="query-dsl-and-query"]
+=== And query
+
+The `and` query is replaced in favour of the <<query-dsl-bool-query,bool>> query.
+
+[role="exclude",id="query-dsl-and-filter"]
+=== And filter
+
+The `and` filter is replaced in favour of the <<query-dsl-bool-query,bool>> query.
+
+[role="exclude",id="query-dsl-limit-query"]
+=== Limit query
+
+The `limit` query is replaced in favour of the <<search-request-body,terminate_after>>
+parameter of search requests.
+
+[role="exclude",id="query-dsl-limit-filter"]
+=== Limit filter
+
+The `limit` filter is replaced in favour of the <<search-request-body,terminate_after>>
+parameter of search requests.
diff --git a/docs/reference/search.asciidoc b/docs/reference/search.asciidoc
index 9a0de10..f59444d 100644
--- a/docs/reference/search.asciidoc
+++ b/docs/reference/search.asciidoc
@@ -34,8 +34,8 @@ only the relevant shard:
 --------------------------------------------------
 $ curl -XGET 'http://localhost:9200/twitter/tweet/_search?routing=kimchy' -d '{
     "query": {
-        "filtered" : {
-            "query" : {
+        "bool" : {
+            "must" : {
                 "query_string" : {
                     "query" : "some query string here"
                 }
diff --git a/docs/reference/search/request/named-queries-and-filters.asciidoc b/docs/reference/search/request/named-queries-and-filters.asciidoc
index 183f0e7..96d7c13 100644
--- a/docs/reference/search/request/named-queries-and-filters.asciidoc
+++ b/docs/reference/search/request/named-queries-and-filters.asciidoc
@@ -1,20 +1,16 @@
 [[search-request-named-queries-and-filters]]
-=== Named Queries and Filters
+=== Named Queries
 
 Each filter and query can accept a `_name` in its top level definition.
 
 [source,js]
 --------------------------------------------------
 {
-    "filtered" : {
-        "query" : {
-            "bool" : {
-                "should" : [
-                    {"match" : { "name.first" : {"query" : "shay", "_name" : "first"} }},
-                    {"match" : { "name.last" : {"query" : "banon", "_name" : "last"} }}
-                ]
-            }
-        },
+    "bool" : {
+        "should" : [
+            {"match" : { "name.first" : {"query" : "shay", "_name" : "first"} }},
+            {"match" : { "name.last" : {"query" : "banon", "_name" : "last"} }}
+        ],
         "filter" : {
             "terms" : {
                 "name.last" : ["banon", "kimchy"],
@@ -26,32 +22,5 @@ Each filter and query can accept a `_name` in its top level definition.
 --------------------------------------------------
 
 The search response will include for each hit the `matched_queries` it matched on. The tagging of queries and filters
-only make sense for compound queries and filters (such as `bool` query and filter, `or` and `and` filter, `filtered` query etc.).
-
-Note, the query filter had to be enhanced in order to support this. In
-order to set a name, the `fquery` filter should be used, which wraps a
-query (just so there will be a place to set a name for it), for example:
-
-[source,js]
---------------------------------------------------
-{
-    "filtered" : {
-        "query" : {
-            "term" : { "name.first" : "shay" }
-        },
-        "filter" : {
-            "fquery" : {
-                "query" : {
-                    "term" : { "name.last" : "banon" }
-                },
-                "_name" : "test"
-            }
-        }
-    }
-}
---------------------------------------------------
-
-==== Named queries
+only make sense for the `bool` query.
 
-The support for the `_name` option on queries is available from version `0.90.4` and the support on filters is available
-also in versions before `0.90.4`.
diff --git a/docs/reference/search/request/post-filter.asciidoc b/docs/reference/search/request/post-filter.asciidoc
index 274d14b..7c352e9 100644
--- a/docs/reference/search/request/post-filter.asciidoc
+++ b/docs/reference/search/request/post-filter.asciidoc
@@ -2,28 +2,24 @@
 === Post filter
 
 The `post_filter` is applied to the search `hits` at the very end of a search
-request,  after aggregations have already been calculated. It's purpose is
+request,  after aggregations have already been calculated. Its purpose is
 best explained by example:
 
 Imagine that you are selling shirts, and the user has specified two filters:
 `color:red` and `brand:gucci`.  You only want to show them red shirts made by
 Gucci in the search results.  Normally you would do this with a 
-<<query-dsl-filtered-query,`filtered` query>>:
+<<query-dsl-bool-query,`bool` query>>:
 
 [source,js]
 --------------------------------------------------
 curl -XGET localhost:9200/shirts/_search -d '
 {
   "query": {
-    "filtered": {
-      "filter": {
-        "bool": {
-          "must": [
-            { "term": { "color": "red"   }},
-            { "term": { "brand": "gucci" }}
-          ]
-        }
-      }
+    "bool": {
+      "filter": [
+        { "term": { "color": "red"   }},
+        { "term": { "brand": "gucci" }}
+      ]
     }
   }
 }
@@ -43,15 +39,11 @@ This can be done with a
 curl -XGET localhost:9200/shirts/_search -d '
 {
   "query": {
-    "filtered": {
-      "filter": {
-        "bool": {
-          "must": [
-            { "term": { "color": "red"   }},
-            { "term": { "brand": "gucci" }}
-          ]
-        }
-      }
+    "bool": {
+      "filter": [
+        { "term": { "color": "red"   }},
+        { "term": { "brand": "gucci" }}
+      ]
     }
   },
   "aggs": {
@@ -78,7 +70,7 @@ the `post_filter`:
 curl -XGET localhost:9200/shirts/_search -d '
 {
   "query": {
-    "filtered": {
+    "bool": {
       "filter": {
         { "term": { "brand": "gucci" }} <1>
       }
diff --git a/docs/reference/search/search-template.asciidoc b/docs/reference/search/search-template.asciidoc
index bce9528..77670ac 100644
--- a/docs/reference/search/search-template.asciidoc
+++ b/docs/reference/search/search-template.asciidoc
@@ -169,8 +169,8 @@ We could write the query as:
 ------------------------------------------
 {
   "query": {
-    "filtered": {
-      "query": {
+    "bool": {
+      "must": {
         "match": {
           "line": "{{text}}" <1>
         }
@@ -212,7 +212,7 @@ via the REST API, should be written as a string:
 
 [source,js]
 --------------------
-"inline": "{\"query\":{\"filtered\":{\"query\":{\"match\":{\"line\":\"{{text}}\"}},\"filter\":{{{#line_no}}\"range\":{\"line_no\":{{{#start}}\"gte\":\"{{start}}\"{{#end}},{{/end}}{{/start}}{{#end}}\"lte\":\"{{end}}\"{{/end}}}}{{/line_no}}}}}}"
+"inline": "{\"query\":{\"bool\":{\"must\":{\"match\":{\"line\":\"{{text}}\"}},\"filter\":{{{#line_no}}\"range\":{\"line_no\":{{{#start}}\"gte\":\"{{start}}\"{{#end}},{{/end}}{{/start}}{{#end}}\"lte\":\"{{end}}\"{{/end}}}}{{/line_no}}}}}}"
 --------------------
 
 ==================================
diff --git a/docs/reference/search/suggesters/completion-suggest.asciidoc b/docs/reference/search/suggesters/completion-suggest.asciidoc
index ee8969b..af93ea1 100644
--- a/docs/reference/search/suggesters/completion-suggest.asciidoc
+++ b/docs/reference/search/suggesters/completion-suggest.asciidoc
@@ -15,11 +15,12 @@ suggestion is, why you should use it at all, if you have prefix queries
 already. The answer is simple: Prefix suggestions are fast.
 
 The data structures are internally backed by Lucenes
-`AnalyzingSuggester`, which uses FSTs to execute suggestions. Usually
-these data structures are costly to create, stored in-memory and need to
-be rebuilt every now and then to reflect changes in your indexed
-documents. The `completion` suggester circumvents this by storing the
-FST as part of your index during index time. This allows for really fast
+`AnalyzingSuggester`, which uses FSTs (finite state transducers) to
+execute suggestions. Usually these data structures are costly to
+create, stored in-memory and need to be rebuilt every now and then to
+reflect changes in your indexed documents. The `completion` suggester
+circumvents this by storing the FST (finite state transducer) as part
+of your index during index time. This allows for really fast
 loads and executions.
 
 [[completion-suggester-mapping]]
diff --git a/docs/reference/search/validate.asciidoc b/docs/reference/search/validate.asciidoc
index b47f63e..a08e183 100644
--- a/docs/reference/search/validate.asciidoc
+++ b/docs/reference/search/validate.asciidoc
@@ -55,8 +55,8 @@ Or, with a request body:
 --------------------------------------------------
 curl -XGET 'http://localhost:9200/twitter/tweet/_validate/query' -d '{
   "query" : {
-    "filtered" : {
-      "query" : {
+    "bool" : {
+      "must" : {
         "query_string" : {
           "query" : "*:*"
         }
@@ -99,7 +99,7 @@ curl -XGET 'http://localhost:9200/twitter/tweet/_validate/query?q=post_date:foo&
   "explanations" : [ {
     "index" : "twitter",
     "valid" : false,
-    "error" : "org.elasticsearch.index.query.QueryParsingException: [twitter] Failed to parse; org.elasticsearch.ElasticsearchParseException: failed to parse date field [foo], tried both date format [dateOptionalTime], and timestamp number; java.lang.IllegalArgumentException: Invalid format: \"foo\""
+    "error" : "[twitter] QueryParsingException[Failed to parse]; nested: IllegalArgumentException[Invalid format: \"foo\"];; java.lang.IllegalArgumentException: Invalid format: \"foo\""
   } ]
 }
 --------------------------------------------------
@@ -112,14 +112,14 @@ For Fuzzy Queries:
 
 [source,js]
 --------------------------------------------------
-curl -XGET 'http://localhost:9200/imdb/movies/_validate/query?rewrite=true'
+curl -XGET 'http://localhost:9200/imdb/movies/_validate/query?rewrite=true' -d '
 {
   "query": {
     "fuzzy": {
       "actors": "kyle"
     }
   }
-}
+}'
 --------------------------------------------------
 
 Response:
@@ -137,7 +137,7 @@ Response:
       {
          "index": "imdb",
          "valid": true,
-         "explanation": "filtered(plot:kyle plot:kylie^0.75 plot:kyne^0.75 plot:lyle^0.75 plot:pyle^0.75)->cache(_type:movies)"
+         "explanation": "plot:kyle plot:kylie^0.75 plot:kyne^0.75 plot:lyle^0.75 plot:pyle^0.75 #_type:movies"
       }
    ]
 }
@@ -175,7 +175,7 @@ Response:
       {
          "index": "imdb",
          "valid": true,
-         "explanation": "filtered(((title:terminator^3.71334 plot:future^2.763601 plot:human^2.8415773 plot:sarah^3.4193945 plot:kyle^3.8244398 plot:cyborg^3.9177752 plot:connor^4.040236 plot:reese^4.7133346 ... )~6) -ConstantScore(_uid:movies#88247))->cache(_type:movies)"
+         "explanation": "((title:terminator^3.71334 plot:future^2.763601 plot:human^2.8415773 plot:sarah^3.4193945 plot:kyle^3.8244398 plot:cyborg^3.9177752 plot:connor^4.040236 plot:reese^4.7133346 ... )~6) -ConstantScore(_uid:movies#88247) #_type:movies"
       }
    ]
 }
diff --git a/docs/reference/setup/backup.asciidoc b/docs/reference/setup/backup.asciidoc
index 44a0a11..f4282aa 100644
--- a/docs/reference/setup/backup.asciidoc
+++ b/docs/reference/setup/backup.asciidoc
@@ -50,7 +50,7 @@ while the backup is in process:
 PUT /_cluster/settings
 {
   "transient": {
-    "cluster.routing.allocation.disable_allocation": "true"
+    "cluster.routing.allocation.enable": "none"
   }
 }
 -----------------------------------
@@ -79,7 +79,7 @@ PUT /_all/_settings
 PUT /_cluster/settings
 {
   "transient": {
-    "cluster.routing.allocation.disable_allocation": "false"
+    "cluster.routing.allocation.enable": "all"
   }
 }
 -----------------------------------
diff --git a/docs/resiliency/index.asciidoc b/docs/resiliency/index.asciidoc
index 7ca7cf9..2a05561 100644
--- a/docs/resiliency/index.asciidoc
+++ b/docs/resiliency/index.asciidoc
@@ -56,21 +56,6 @@ If you encounter an issue, https://github.com/elasticsearch/elasticsearch/issues
 We are committed to tracking down and fixing all the issues that are posted.
 
 [float]
-=== Use two phase commit for Cluster State publishing (STATUS: ONGOING)
-
-A master node in Elasticsearch continuously https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-zen.html#fault-detection[monitors the cluster nodes]
-and removes any node from the cluster that doesn't respond to its pings in a timely
-fashion. If the master is left with fewer nodes than the `discovery.zen.minimum_master_nodes`
-settings, it will step down and a new master election will start.
-
-When a network partition causes a master node to lose many followers, there is a short window
-in time until the node loss is detected and the master steps down. During that window, the
-master may erroneously accept and acknowledge cluster state changes. To avoid this, we introduce
-a new phase to cluster state publishing where the proposed cluster state is sent to all nodes
-but is not yet committed. Only once enough nodes (`discovery.zen.minimum_master_nodes`) actively acknowledge
-the change, it is committed and commit messages are sent to the nodes. See {GIT}13062[#13062].
-
-[float]
 === Make index creation more user friendly (STATUS: ONGOING)
 
 Today, Elasticsearch returns as soon as a create-index request has been processed,
diff --git a/plugins/analysis-icu/licenses/lucene-analyzers-icu-5.4.0-snapshot-1701068.jar.sha1 b/plugins/analysis-icu/licenses/lucene-analyzers-icu-5.4.0-snapshot-1701068.jar.sha1
deleted file mode 100644
index c1a1ec2..0000000
--- a/plugins/analysis-icu/licenses/lucene-analyzers-icu-5.4.0-snapshot-1701068.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-b7f57ef60f302b30e88196d4f0d11f789c5cfabd
diff --git a/plugins/analysis-icu/licenses/lucene-analyzers-icu-5.4.0-snapshot-1702265.jar.sha1 b/plugins/analysis-icu/licenses/lucene-analyzers-icu-5.4.0-snapshot-1702265.jar.sha1
new file mode 100644
index 0000000..54c8e96
--- /dev/null
+++ b/plugins/analysis-icu/licenses/lucene-analyzers-icu-5.4.0-snapshot-1702265.jar.sha1
@@ -0,0 +1 @@
+1f92d0376ca9219b0bf96fe5bd9a913089608d6a
diff --git a/plugins/analysis-kuromoji/licenses/lucene-analyzers-kuromoji-5.4.0-snapshot-1701068.jar.sha1 b/plugins/analysis-kuromoji/licenses/lucene-analyzers-kuromoji-5.4.0-snapshot-1701068.jar.sha1
deleted file mode 100644
index 60ea23d..0000000
--- a/plugins/analysis-kuromoji/licenses/lucene-analyzers-kuromoji-5.4.0-snapshot-1701068.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-5d1023fc3f28a42357d44d3a330ac0df1df4bf42
diff --git a/plugins/analysis-kuromoji/licenses/lucene-analyzers-kuromoji-5.4.0-snapshot-1702265.jar.sha1 b/plugins/analysis-kuromoji/licenses/lucene-analyzers-kuromoji-5.4.0-snapshot-1702265.jar.sha1
new file mode 100644
index 0000000..a3885bf
--- /dev/null
+++ b/plugins/analysis-kuromoji/licenses/lucene-analyzers-kuromoji-5.4.0-snapshot-1702265.jar.sha1
@@ -0,0 +1 @@
+60ee5bc1ac8ec102434e7064141a1f40281918b5
diff --git a/plugins/analysis-phonetic/licenses/lucene-analyzers-phonetic-5.4.0-snapshot-1701068.jar.sha1 b/plugins/analysis-phonetic/licenses/lucene-analyzers-phonetic-5.4.0-snapshot-1701068.jar.sha1
deleted file mode 100644
index 92243ae..0000000
--- a/plugins/analysis-phonetic/licenses/lucene-analyzers-phonetic-5.4.0-snapshot-1701068.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-654c3e345ffdd74605582d1320c51c1c550a5cca
diff --git a/plugins/analysis-phonetic/licenses/lucene-analyzers-phonetic-5.4.0-snapshot-1702265.jar.sha1 b/plugins/analysis-phonetic/licenses/lucene-analyzers-phonetic-5.4.0-snapshot-1702265.jar.sha1
new file mode 100644
index 0000000..44ac92c
--- /dev/null
+++ b/plugins/analysis-phonetic/licenses/lucene-analyzers-phonetic-5.4.0-snapshot-1702265.jar.sha1
@@ -0,0 +1 @@
+977aa506485d358b40602347c11238b0f912fe2c
diff --git a/plugins/analysis-smartcn/licenses/lucene-analyzers-smartcn-5.4.0-snapshot-1701068.jar.sha1 b/plugins/analysis-smartcn/licenses/lucene-analyzers-smartcn-5.4.0-snapshot-1701068.jar.sha1
deleted file mode 100644
index a9159eb..0000000
--- a/plugins/analysis-smartcn/licenses/lucene-analyzers-smartcn-5.4.0-snapshot-1701068.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-80c09e367abf2ad936c86cf74a16ae2b4e805b81
diff --git a/plugins/analysis-smartcn/licenses/lucene-analyzers-smartcn-5.4.0-snapshot-1702265.jar.sha1 b/plugins/analysis-smartcn/licenses/lucene-analyzers-smartcn-5.4.0-snapshot-1702265.jar.sha1
new file mode 100644
index 0000000..6f7d174
--- /dev/null
+++ b/plugins/analysis-smartcn/licenses/lucene-analyzers-smartcn-5.4.0-snapshot-1702265.jar.sha1
@@ -0,0 +1 @@
+61911b8400160bd206ea6ea46ba08fd9ba09e72b
diff --git a/plugins/analysis-stempel/licenses/lucene-analyzers-stempel-5.4.0-snapshot-1701068.jar.sha1 b/plugins/analysis-stempel/licenses/lucene-analyzers-stempel-5.4.0-snapshot-1701068.jar.sha1
deleted file mode 100644
index 390511f..0000000
--- a/plugins/analysis-stempel/licenses/lucene-analyzers-stempel-5.4.0-snapshot-1701068.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-7c6ae4fc7e8e1d39c155068fea67b7fabb12c444
diff --git a/plugins/analysis-stempel/licenses/lucene-analyzers-stempel-5.4.0-snapshot-1702265.jar.sha1 b/plugins/analysis-stempel/licenses/lucene-analyzers-stempel-5.4.0-snapshot-1702265.jar.sha1
new file mode 100644
index 0000000..cf50fb6
--- /dev/null
+++ b/plugins/analysis-stempel/licenses/lucene-analyzers-stempel-5.4.0-snapshot-1702265.jar.sha1
@@ -0,0 +1 @@
+5a9bdf48b63562bf1ac8a73c1c6bdb4cc450439e
diff --git a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureBlobContainer.java b/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureBlobContainer.java
index 0423132..a7c980c 100644
--- a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureBlobContainer.java
+++ b/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureBlobContainer.java
@@ -23,7 +23,7 @@ import com.microsoft.azure.storage.StorageException;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.blobstore.BlobMetaData;
 import org.elasticsearch.common.blobstore.BlobPath;
-import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;
+import org.elasticsearch.common.blobstore.support.AbstractLegacyBlobContainer;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
 import org.elasticsearch.repositories.RepositoryException;
@@ -39,7 +39,7 @@ import java.util.Map;
 /**
  *
  */
-public class AzureBlobContainer extends AbstractBlobContainer {
+public class AzureBlobContainer extends AbstractLegacyBlobContainer {
 
     protected final ESLogger logger = Loggers.getLogger(AzureBlobContainer.class);
     protected final AzureBlobStore blobStore;
diff --git a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/management/AzureComputeSettingsFilter.java b/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/management/AzureComputeSettingsFilter.java
index e584cef..c4a1837 100644
--- a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/management/AzureComputeSettingsFilter.java
+++ b/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/management/AzureComputeSettingsFilter.java
@@ -29,7 +29,7 @@ import static org.elasticsearch.cloud.azure.management.AzureComputeService.Manag
 public class AzureComputeSettingsFilter extends AbstractComponent {
 
     @Inject
-    protected AzureComputeSettingsFilter(Settings settings, SettingsFilter settingsFilter) {
+    public AzureComputeSettingsFilter(Settings settings, SettingsFilter settingsFilter) {
         super(settings);
         // Cloud management API settings we need to hide
         settingsFilter.addFilter(KEYSTORE_PATH);
diff --git a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageSettingsFilter.java b/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageSettingsFilter.java
index ac23f0c..da3aa8c 100644
--- a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageSettingsFilter.java
+++ b/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageSettingsFilter.java
@@ -29,7 +29,7 @@ import static org.elasticsearch.cloud.azure.storage.AzureStorageService.Storage.
 public class AzureStorageSettingsFilter extends AbstractComponent {
 
     @Inject
-    protected AzureStorageSettingsFilter(Settings settings, SettingsFilter settingsFilter) {
+    public AzureStorageSettingsFilter(Settings settings, SettingsFilter settingsFilter) {
         super(settings);
         // Cloud storage API settings needed to be hidden
         settingsFilter.addFilter(ACCOUNT);
diff --git a/plugins/cloud-azure/src/main/java/org/elasticsearch/plugin/cloud/azure/CloudAzurePlugin.java b/plugins/cloud-azure/src/main/java/org/elasticsearch/plugin/cloud/azure/CloudAzurePlugin.java
index 6667380..fa896fd 100644
--- a/plugins/cloud-azure/src/main/java/org/elasticsearch/plugin/cloud/azure/CloudAzurePlugin.java
+++ b/plugins/cloud-azure/src/main/java/org/elasticsearch/plugin/cloud/azure/CloudAzurePlugin.java
@@ -26,6 +26,7 @@ import org.elasticsearch.common.logging.Loggers;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.discovery.DiscoveryModule;
 import org.elasticsearch.discovery.azure.AzureDiscovery;
+import org.elasticsearch.discovery.azure.AzureUnicastHostsProvider;
 import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository;
 import org.elasticsearch.index.store.IndexStoreModule;
 import org.elasticsearch.index.store.smbmmapfs.SmbMmapFsIndexStore;
@@ -79,7 +80,10 @@ public class CloudAzurePlugin extends Plugin {
     }
 
     public void onModule(DiscoveryModule discoveryModule) {
-        discoveryModule.addDiscoveryType("azure", AzureDiscovery.class);
+        if (AzureModule.isDiscoveryReady(settings, logger)) {
+            discoveryModule.addDiscoveryType("azure", AzureDiscovery.class);
+            discoveryModule.addUnicastHostProvider(AzureUnicastHostsProvider.class);
+        }
     }
 
     public void onModule(IndexStoreModule storeModule) {
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AzureComputeServiceSimpleMock.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AzureComputeServiceSimpleMock.java
index 997a8aa..e303315 100644
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AzureComputeServiceSimpleMock.java
+++ b/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AzureComputeServiceSimpleMock.java
@@ -52,7 +52,7 @@ public class AzureComputeServiceSimpleMock extends AzureComputeServiceAbstractMo
     }
 
     @Inject
-    protected AzureComputeServiceSimpleMock(Settings settings) {
+    public AzureComputeServiceSimpleMock(Settings settings) {
         super(settings);
     }
 
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceMock.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceMock.java
index 96ee895..8fe1923 100644
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceMock.java
+++ b/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceMock.java
@@ -46,7 +46,7 @@ public class AzureStorageServiceMock extends AbstractLifecycleComponent<AzureSto
     protected Map<String, ByteArrayOutputStream> blobs = new ConcurrentHashMap<>();
 
     @Inject
-    protected AzureStorageServiceMock(Settings settings) {
+    public AzureStorageServiceMock(Settings settings) {
         super(settings);
     }
 
diff --git a/plugins/cloud-gce/src/main/java/org/elasticsearch/plugin/cloud/gce/CloudGcePlugin.java b/plugins/cloud-gce/src/main/java/org/elasticsearch/plugin/cloud/gce/CloudGcePlugin.java
index 8b2a3d2..5384f2c 100644
--- a/plugins/cloud-gce/src/main/java/org/elasticsearch/plugin/cloud/gce/CloudGcePlugin.java
+++ b/plugins/cloud-gce/src/main/java/org/elasticsearch/plugin/cloud/gce/CloudGcePlugin.java
@@ -25,6 +25,7 @@ import org.elasticsearch.common.inject.Module;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.discovery.DiscoveryModule;
 import org.elasticsearch.discovery.gce.GceDiscovery;
+import org.elasticsearch.discovery.gce.GceUnicastHostsProvider;
 import org.elasticsearch.plugins.Plugin;
 
 import java.util.ArrayList;
@@ -72,6 +73,7 @@ public class CloudGcePlugin extends Plugin {
 
     public void onModule(DiscoveryModule discoveryModule) {
         discoveryModule.addDiscoveryType("gce", GceDiscovery.class);
+        discoveryModule.addUnicastHostProvider(GceUnicastHostsProvider.class);
     }
 
 }
diff --git a/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java b/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java
index 7f06ce6..890a60c 100644
--- a/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java
+++ b/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java
@@ -57,7 +57,7 @@ public class TransportDeleteByQueryAction extends HandledTransportAction<DeleteB
     private final Client client;
 
     @Inject
-    protected TransportDeleteByQueryAction(Settings settings, ThreadPool threadPool, Client client,
+    public TransportDeleteByQueryAction(Settings settings, ThreadPool threadPool, Client client,
                                            TransportSearchAction transportSearchAction,
                                            TransportSearchScrollAction transportSearchScrollAction,
                                            TransportService transportService, ActionFilters actionFilters,
diff --git a/plugins/discovery-ec2/src/main/java/org/elasticsearch/plugin/discovery/ec2/Ec2DiscoveryPlugin.java b/plugins/discovery-ec2/src/main/java/org/elasticsearch/plugin/discovery/ec2/Ec2DiscoveryPlugin.java
index 180581b..38c7042 100644
--- a/plugins/discovery-ec2/src/main/java/org/elasticsearch/plugin/discovery/ec2/Ec2DiscoveryPlugin.java
+++ b/plugins/discovery-ec2/src/main/java/org/elasticsearch/plugin/discovery/ec2/Ec2DiscoveryPlugin.java
@@ -24,6 +24,7 @@ import org.elasticsearch.cloud.aws.Ec2Module;
 import org.elasticsearch.common.component.LifecycleComponent;
 import org.elasticsearch.common.inject.Module;
 import org.elasticsearch.discovery.DiscoveryModule;
+import org.elasticsearch.discovery.ec2.AwsEc2UnicastHostsProvider;
 import org.elasticsearch.discovery.ec2.Ec2Discovery;
 import org.elasticsearch.plugins.Plugin;
 
@@ -61,5 +62,6 @@ public class Ec2DiscoveryPlugin extends Plugin {
 
     public void onModule(DiscoveryModule discoveryModule) {
         discoveryModule.addDiscoveryType("ec2", Ec2Discovery.class);
+        discoveryModule.addUnicastHostProvider(AwsEc2UnicastHostsProvider.class);
     }
 }
diff --git a/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastZenPing.java b/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastZenPing.java
index 1abac08..f86466d 100644
--- a/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastZenPing.java
+++ b/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastZenPing.java
@@ -352,13 +352,13 @@ public class MulticastZenPing extends AbstractLifecycleComponent<ZenPing> implem
         }
     }
 
-    static class MulticastPingResponse extends TransportRequest {
+    public static class MulticastPingResponse extends TransportRequest {
 
         int id;
 
         PingResponse pingResponse;
 
-        MulticastPingResponse() {
+        public MulticastPingResponse() {
         }
 
         @Override
diff --git a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobContainer.java b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobContainer.java
index 1c0b23e..09194b2 100644
--- a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobContainer.java
+++ b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobContainer.java
@@ -25,7 +25,7 @@ import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.blobstore.BlobMetaData;
 import org.elasticsearch.common.blobstore.BlobPath;
 import org.elasticsearch.common.blobstore.BlobStoreException;
-import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;
+import org.elasticsearch.common.blobstore.support.AbstractLegacyBlobContainer;
 import org.elasticsearch.common.blobstore.support.PlainBlobMetaData;
 import org.elasticsearch.common.collect.MapBuilder;
 
@@ -38,7 +38,7 @@ import java.util.Map;
 /**
  *
  */
-public class S3BlobContainer extends AbstractBlobContainer {
+public class S3BlobContainer extends AbstractLegacyBlobContainer {
 
     protected final S3BlobStore blobStore;
 
diff --git a/pom.xml b/pom.xml
index cb4de2a..ceef389 100644
--- a/pom.xml
+++ b/pom.xml
@@ -45,7 +45,7 @@
 
         <!-- libraries -->
         <lucene.version>5.4.0</lucene.version>
-        <lucene.snapshot.revision>1701068</lucene.snapshot.revision>
+        <lucene.snapshot.revision>1702265</lucene.snapshot.revision>
         <lucene.maven.version>5.4.0-snapshot-${lucene.snapshot.revision}</lucene.maven.version>
         <testframework.version>2.1.16</testframework.version>
         <jackson.version>2.5.3</jackson.version>
@@ -115,8 +115,8 @@
         <tests.ifNoTests>fail</tests.ifNoTests>
         <skip.unit.tests>${skipTests}</skip.unit.tests>
         <skip.integ.tests>${skipTests}</skip.integ.tests>
-        <integ.scratch>${project.build.directory}/integ-tests</integ.scratch>
-        <integ.deps>${project.build.directory}/integ-deps</integ.deps>
+        <integ.scratch>${project.build.directory}/integ tests</integ.scratch>
+        <integ.deps>${project.build.directory}/integ deps</integ.deps>
         <integ.temp>${integ.scratch}/temp</integ.temp>
         <integ.http.port>9400</integ.http.port>
         <integ.transport.port>9500</integ.transport.port>
@@ -582,7 +582,7 @@
                             <arg>-XDignore.symbol.file</arg>
                             <arg>-Xlint:all</arg>
                             <arg>${xlint.options}</arg>
-                            <arg>-Werror</arg>
+                            <!-- DISABLED: incompatible with java 9 <arg>-Werror</arg> -->
                         </compilerArgs>
                     </configuration>
                 </plugin>
diff --git a/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java b/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java
index e18ca34..327bd2c 100644
--- a/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java
+++ b/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java
@@ -69,7 +69,7 @@ public abstract class ESSmokeClientTestCase extends LuceneTestCase {
      */
     public static final String TESTS_CLUSTER_DEFAULT = "localhost:9300";
 
-    protected static ESLogger logger = ESLoggerFactory.getLogger(ESSmokeClientTestCase.class.getName());
+    protected static final ESLogger logger = ESLoggerFactory.getLogger(ESSmokeClientTestCase.class.getName());
 
     private static final AtomicInteger counter = new AtomicInteger();
     private static Client client;
