diff --git a/.gitignore b/.gitignore
index eb18986..82ed94b 100644
--- a/.gitignore
+++ b/.gitignore
@@ -14,7 +14,7 @@ docs/html/
 docs/build.log
 /tmp/
 backwards/
-
+html_docs
 ## eclipse ignores (use 'mvn eclipse:eclipse' to build eclipse projects)
 ## All files (.project, .classpath, .settings/*) should be generated through Maven which
 ## will correctly set the classpath based on the declared dependencies and write settings 
diff --git a/core/src/main/java/org/apache/lucene/queries/BlendedTermQuery.java b/core/src/main/java/org/apache/lucene/queries/BlendedTermQuery.java
index dd68ba1..17485d0 100644
--- a/core/src/main/java/org/apache/lucene/queries/BlendedTermQuery.java
+++ b/core/src/main/java/org/apache/lucene/queries/BlendedTermQuery.java
@@ -33,6 +33,7 @@ import org.apache.lucene.search.Query;
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.util.ArrayUtil;
 import org.apache.lucene.util.InPlaceMergeSorter;
+import org.apache.lucene.util.ToStringUtils;
 
 import java.io.IOException;
 import java.util.Arrays;
@@ -62,13 +63,17 @@ import java.util.List;
 public abstract class BlendedTermQuery extends Query {
 
     private final Term[] terms;
+    private final float[] boosts;
 
-
-    public BlendedTermQuery(Term[] terms) {
+    public BlendedTermQuery(Term[] terms, float[] boosts) {
         if (terms == null || terms.length == 0) {
             throw new IllegalArgumentException("terms must not be null or empty");
         }
+        if (boosts != null && boosts.length != terms.length) {
+            throw new IllegalArgumentException("boosts must have the same size as terms");
+        }
         this.terms = terms;
+        this.boosts = boosts;
     }
 
     @Override
@@ -231,8 +236,22 @@ public abstract class BlendedTermQuery extends Query {
 
     @Override
     public String toString(String field) {
-        return "blended(terms: " + Arrays.toString(terms) + ")";
-
+        StringBuilder builder = new StringBuilder("blended(terms:[");
+        for (int i = 0; i < terms.length; ++i) {
+            builder.append(terms[i]);
+            float boost = 1f;
+            if (boosts != null) {
+                boost = boosts[i];
+            }
+            builder.append(ToStringUtils.boost(boost));
+            builder.append(", ");
+        }
+        if (terms.length > 0) {
+            builder.setLength(builder.length() - 2);
+        }
+        builder.append("])");
+        builder.append(ToStringUtils.boost(getBoost()));
+        return builder.toString();
     }
 
     private volatile Term[] equalTerms = null;
@@ -277,7 +296,7 @@ public abstract class BlendedTermQuery extends Query {
     }
 
     public static BlendedTermQuery booleanBlendedQuery(Term[] terms, final float[] boosts, final boolean disableCoord) {
-        return new BlendedTermQuery(terms) {
+        return new BlendedTermQuery(terms, boosts) {
             @Override
             protected Query topLevelQuery(Term[] terms, TermContext[] ctx, int[] docFreqs, int maxDoc) {
                 BooleanQuery query = new BooleanQuery(disableCoord);
@@ -294,7 +313,7 @@ public abstract class BlendedTermQuery extends Query {
     }
 
     public static BlendedTermQuery commonTermsBlendedQuery(Term[] terms, final float[] boosts, final boolean disableCoord, final float maxTermFrequency) {
-        return new BlendedTermQuery(terms) {
+        return new BlendedTermQuery(terms, boosts) {
             @Override
             protected Query topLevelQuery(Term[] terms, TermContext[] ctx, int[] docFreqs, int maxDoc) {
                 BooleanQuery query = new BooleanQuery(true);
@@ -334,7 +353,7 @@ public abstract class BlendedTermQuery extends Query {
     }
 
     public static BlendedTermQuery dismaxBlendedQuery(Term[] terms, final float[] boosts, final float tieBreakerMultiplier) {
-        return new BlendedTermQuery(terms) {
+        return new BlendedTermQuery(terms, boosts) {
             @Override
             protected Query topLevelQuery(Term[] terms, TermContext[] ctx, int[] docFreqs, int maxDoc) {
                 DisjunctionMaxQuery query = new DisjunctionMaxQuery(tieBreakerMultiplier);
diff --git a/core/src/main/java/org/apache/lucene/queryparser/classic/ExistsFieldQueryExtension.java b/core/src/main/java/org/apache/lucene/queryparser/classic/ExistsFieldQueryExtension.java
index cb4bee3..6cac629 100644
--- a/core/src/main/java/org/apache/lucene/queryparser/classic/ExistsFieldQueryExtension.java
+++ b/core/src/main/java/org/apache/lucene/queryparser/classic/ExistsFieldQueryExtension.java
@@ -21,8 +21,8 @@ package org.apache.lucene.queryparser.classic;
 
 import org.apache.lucene.search.ConstantScoreQuery;
 import org.apache.lucene.search.Query;
-import org.elasticsearch.index.query.ExistsQueryBuilder;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.ExistsQueryParser;
+import org.elasticsearch.index.query.QueryParseContext;
 
 /**
  *
@@ -32,7 +32,7 @@ public class ExistsFieldQueryExtension implements FieldQueryExtension {
     public static final String NAME = "_exists_";
 
     @Override
-    public Query query(QueryShardContext context, String queryText) {
-        return new ConstantScoreQuery(ExistsQueryBuilder.newFilter(context, queryText));
+    public Query query(QueryParseContext parseContext, String queryText) {
+        return new ConstantScoreQuery(ExistsQueryParser.newFilter(parseContext, queryText, null));
     }
 }
diff --git a/core/src/main/java/org/apache/lucene/queryparser/classic/FieldQueryExtension.java b/core/src/main/java/org/apache/lucene/queryparser/classic/FieldQueryExtension.java
index 299a37a..003ff18 100644
--- a/core/src/main/java/org/apache/lucene/queryparser/classic/FieldQueryExtension.java
+++ b/core/src/main/java/org/apache/lucene/queryparser/classic/FieldQueryExtension.java
@@ -20,12 +20,12 @@
 package org.apache.lucene.queryparser.classic;
 
 import org.apache.lucene.search.Query;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 
 /**
  *
  */
 public interface FieldQueryExtension {
 
-    Query query(QueryShardContext context, String queryText);
+    Query query(QueryParseContext parseContext, String queryText);
 }
diff --git a/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java b/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java
index 6974dc0..4cfdc25 100644
--- a/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java
+++ b/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java
@@ -39,7 +39,7 @@ import org.elasticsearch.common.unit.Fuzziness;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.core.DateFieldMapper;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.support.QueryParsers;
 
 import com.google.common.base.Objects;
@@ -70,7 +70,7 @@ public class MapperQueryParser extends QueryParser {
                 .build();
     }
 
-    private final QueryShardContext context;
+    private final QueryParseContext parseContext;
 
     private QueryParserSettings settings;
 
@@ -85,9 +85,15 @@ public class MapperQueryParser extends QueryParser {
 
     private String quoteFieldSuffix;
 
-    public MapperQueryParser(QueryShardContext context) {
+    public MapperQueryParser(QueryParseContext parseContext) {
         super(null, null);
-        this.context = context;
+        this.parseContext = parseContext;
+    }
+
+    public MapperQueryParser(QueryParserSettings settings, QueryParseContext parseContext) {
+        super(settings.defaultField(), settings.defaultAnalyzer());
+        this.parseContext = parseContext;
+        reset(settings);
     }
 
     public void reset(QueryParserSettings settings) {
@@ -162,7 +168,7 @@ public class MapperQueryParser extends QueryParser {
     public Query getFieldQuery(String field, String queryText, boolean quoted) throws ParseException {
         FieldQueryExtension fieldQueryExtension = fieldQueryExtensions.get(field);
         if (fieldQueryExtension != null) {
-            return fieldQueryExtension.query(context, queryText);
+            return fieldQueryExtension.query(parseContext, queryText);
         }
         Collection<String> fields = extractMultiFields(field);
         if (fields != null) {
@@ -226,27 +232,27 @@ public class MapperQueryParser extends QueryParser {
             if (quoted) {
                 setAnalyzer(quoteAnalyzer);
                 if (quoteFieldSuffix != null) {
-                    currentFieldType = context.fieldMapper(field + quoteFieldSuffix);
+                    currentFieldType = parseContext.fieldMapper(field + quoteFieldSuffix);
                 }
             }
             if (currentFieldType == null) {
-                currentFieldType = context.fieldMapper(field);
+                currentFieldType = parseContext.fieldMapper(field);
             }
             if (currentFieldType != null) {
                 if (quoted) {
                     if (!forcedQuoteAnalyzer) {
-                        setAnalyzer(context.getSearchQuoteAnalyzer(currentFieldType));
+                        setAnalyzer(parseContext.getSearchQuoteAnalyzer(currentFieldType));
                     }
                 } else {
                     if (!forcedAnalyzer) {
-                        setAnalyzer(context.getSearchAnalyzer(currentFieldType));
+                        setAnalyzer(parseContext.getSearchAnalyzer(currentFieldType));
                     }
                 }
                 if (currentFieldType != null) {
                     Query query = null;
                     if (currentFieldType.useTermQueryWithQueryString()) {
                         try {
-                            query = currentFieldType.termQuery(queryText, context);
+                            query = currentFieldType.termQuery(queryText, parseContext);
                         } catch (RuntimeException e) {
                             if (settings.lenient()) {
                                 return null;
@@ -357,7 +363,7 @@ public class MapperQueryParser extends QueryParser {
     }
 
     private Query getRangeQuerySingle(String field, String part1, String part2, boolean startInclusive, boolean endInclusive) {
-        currentFieldType = context.fieldMapper(field);
+        currentFieldType = parseContext.fieldMapper(field);
         if (currentFieldType != null) {
             if (lowercaseExpandedTerms && !currentFieldType.isNumeric()) {
                 part1 = part1 == null ? null : part1.toLowerCase(locale);
@@ -422,7 +428,7 @@ public class MapperQueryParser extends QueryParser {
     }
 
     private Query getFuzzyQuerySingle(String field, String termStr, String minSimilarity) throws ParseException {
-        currentFieldType = context.fieldMapper(field);
+        currentFieldType = parseContext.fieldMapper(field);
         if (currentFieldType != null) {
             try {
                 return currentFieldType.fuzzyQuery(termStr, Fuzziness.build(minSimilarity), fuzzyPrefixLength, settings.fuzzyMaxExpansions(), FuzzyQuery.defaultTranspositions);
@@ -492,14 +498,14 @@ public class MapperQueryParser extends QueryParser {
         currentFieldType = null;
         Analyzer oldAnalyzer = getAnalyzer();
         try {
-            currentFieldType = context.fieldMapper(field);
+            currentFieldType = parseContext.fieldMapper(field);
             if (currentFieldType != null) {
                 if (!forcedAnalyzer) {
-                    setAnalyzer(context.getSearchAnalyzer(currentFieldType));
+                    setAnalyzer(parseContext.getSearchAnalyzer(currentFieldType));
                 }
                 Query query = null;
                 if (currentFieldType.useTermQueryWithQueryString()) {
-                    query = currentFieldType.prefixQuery(termStr, multiTermRewriteMethod, context);
+                    query = currentFieldType.prefixQuery(termStr, multiTermRewriteMethod, parseContext);
                 }
                 if (query == null) {
                     query = getPossiblyAnalyzedPrefixQuery(currentFieldType.names().indexName(), termStr);
@@ -584,7 +590,7 @@ public class MapperQueryParser extends QueryParser {
                     return newMatchAllDocsQuery();
                 }
                 // effectively, we check if a field exists or not
-                return fieldQueryExtensions.get(ExistsFieldQueryExtension.NAME).query(context, actualField);
+                return fieldQueryExtensions.get(ExistsFieldQueryExtension.NAME).query(parseContext, actualField);
             }
         }
         if (lowercaseExpandedTerms) {
@@ -633,10 +639,10 @@ public class MapperQueryParser extends QueryParser {
         currentFieldType = null;
         Analyzer oldAnalyzer = getAnalyzer();
         try {
-            currentFieldType = context.fieldMapper(field);
+            currentFieldType = parseContext.fieldMapper(field);
             if (currentFieldType != null) {
                 if (!forcedAnalyzer) {
-                    setAnalyzer(context.getSearchAnalyzer(currentFieldType));
+                    setAnalyzer(parseContext.getSearchAnalyzer(currentFieldType));
                 }
                 indexedNameField = currentFieldType.names().indexName();
                 return getPossiblyAnalyzedWildcardQuery(indexedNameField, termStr);
@@ -774,14 +780,14 @@ public class MapperQueryParser extends QueryParser {
         currentFieldType = null;
         Analyzer oldAnalyzer = getAnalyzer();
         try {
-            currentFieldType = context.fieldMapper(field);
+            currentFieldType = parseContext.fieldMapper(field);
             if (currentFieldType != null) {
                 if (!forcedAnalyzer) {
-                    setAnalyzer(context.getSearchAnalyzer(currentFieldType));
+                    setAnalyzer(parseContext.getSearchAnalyzer(currentFieldType));
                 }
                 Query query = null;
                 if (currentFieldType.useTermQueryWithQueryString()) {
-                    query = currentFieldType.regexpQuery(termStr, RegExp.ALL, maxDeterminizedStates, multiTermRewriteMethod, context);
+                    query = currentFieldType.regexpQuery(termStr, RegExp.ALL, maxDeterminizedStates, multiTermRewriteMethod, parseContext);
                 }
                 if (query == null) {
                     query = super.getRegexpQuery(field, termStr);
@@ -829,7 +835,7 @@ public class MapperQueryParser extends QueryParser {
     private Collection<String> extractMultiFields(String field) {
         Collection<String> fields = null;
         if (field != null) {
-            fields = context.simpleMatchToIndexNames(field);
+            fields = parseContext.simpleMatchToIndexNames(field);
         } else {
             fields = settings.fields();
         }
diff --git a/core/src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java b/core/src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java
index f9fc8c9..ed1b704 100644
--- a/core/src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java
+++ b/core/src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java
@@ -21,8 +21,8 @@ package org.apache.lucene.queryparser.classic;
 
 import org.apache.lucene.search.ConstantScoreQuery;
 import org.apache.lucene.search.Query;
-import org.elasticsearch.index.query.MissingQueryBuilder;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.MissingQueryParser;
+import org.elasticsearch.index.query.QueryParseContext;
 
 /**
  *
@@ -32,11 +32,8 @@ public class MissingFieldQueryExtension implements FieldQueryExtension {
     public static final String NAME = "_missing_";
 
     @Override
-    public Query query(QueryShardContext context, String queryText) {
-        Query query = MissingQueryBuilder.newFilter(context, queryText, MissingQueryBuilder.DEFAULT_EXISTENCE_VALUE, MissingQueryBuilder.DEFAULT_NULL_VALUE);
-        if (query != null) {
-            return new ConstantScoreQuery(query);
-        }
-        return null;
+    public Query query(QueryParseContext parseContext, String queryText) {
+        return new ConstantScoreQuery(MissingQueryParser.newFilter(parseContext, queryText,
+                MissingQueryParser.DEFAULT_EXISTENCE_VALUE, MissingQueryParser.DEFAULT_NULL_VALUE, null));
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/ElasticsearchException.java b/core/src/main/java/org/elasticsearch/ElasticsearchException.java
index e7fe791..83be9f4 100644
--- a/core/src/main/java/org/elasticsearch/ElasticsearchException.java
+++ b/core/src/main/java/org/elasticsearch/ElasticsearchException.java
@@ -572,7 +572,6 @@ public class ElasticsearchException extends RuntimeException implements ToXConte
                 org.elasticsearch.index.engine.RecoveryEngineException.class,
                 org.elasticsearch.common.blobstore.BlobStoreException.class,
                 org.elasticsearch.index.snapshots.IndexShardRestoreException.class,
-                org.elasticsearch.index.query.QueryShardException.class,
                 org.elasticsearch.index.query.QueryParsingException.class,
                 org.elasticsearch.action.support.replication.TransportReplicationAction.RetryOnPrimaryException.class,
                 org.elasticsearch.index.engine.DeleteByQueryFailedEngineException.class,
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java
index 06d8d63..2d99769 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java
@@ -272,13 +272,13 @@ public class TransportClusterHealthAction extends TransportMasterNodeReadAction<
         } catch (IndexNotFoundException e) {
             // one of the specified indices is not there - treat it as RED.
             ClusterHealthResponse response = new ClusterHealthResponse(clusterName.value(), Strings.EMPTY_ARRAY, clusterState,
-                    numberOfPendingTasks, numberOfInFlightFetch, UnassignedInfo.getNumberOfDelayedUnassigned(settings, clusterState),
+                    numberOfPendingTasks, numberOfInFlightFetch, UnassignedInfo.getNumberOfDelayedUnassigned(System.currentTimeMillis(), settings, clusterState),
                     pendingTaskTimeInQueue);
             response.status = ClusterHealthStatus.RED;
             return response;
         }
 
         return new ClusterHealthResponse(clusterName.value(), concreteIndices, clusterState, numberOfPendingTasks,
-                numberOfInFlightFetch, UnassignedInfo.getNumberOfDelayedUnassigned(settings, clusterState), pendingTaskTimeInQueue);
+                numberOfInFlightFetch, UnassignedInfo.getNumberOfDelayedUnassigned(System.currentTimeMillis(), settings, clusterState), pendingTaskTimeInQueue);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/shards/TransportIndicesShardStoresAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/shards/TransportIndicesShardStoresAction.java
index 0167a33..b783ce1 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/shards/TransportIndicesShardStoresAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/shards/TransportIndicesShardStoresAction.java
@@ -79,7 +79,7 @@ public class TransportIndicesShardStoresAction extends TransportMasterNodeReadAc
     @Override
     protected void masterOperation(IndicesShardStoresRequest request, ClusterState state, ActionListener<IndicesShardStoresResponse> listener) {
         final RoutingTable routingTables = state.routingTable();
-        final RoutingNodes routingNodes = state.routingNodes();
+        final RoutingNodes routingNodes = state.getRoutingNodes();
         final String[] concreteIndices = indexNameExpressionResolver.concreteIndices(state, request);
         final Set<ShardId> shardIdsToFetch = new HashSet<>();
 
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
index 807a638..4437e80 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
@@ -42,7 +42,6 @@ import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.engine.Engine;
 import org.elasticsearch.index.query.IndexQueryParserService;
-import org.elasticsearch.index.query.QueryShardException;
 import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.indices.IndicesService;
@@ -190,8 +189,8 @@ public class TransportValidateQueryAction extends TransportBroadcastAction<Valid
             }
             if (request.rewrite()) {
                 explanation = getRewrittenQuery(searcher.searcher(), searchContext.query());
-            }
-        } catch (QueryShardException|QueryParsingException e) {
+            }   
+        } catch (QueryParsingException e) {
             valid = false;
             error = e.getDetailedMessage();
         } catch (AssertionError|IOException e) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequestBuilder.java
index 515ecd1..4acdfdc 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequestBuilder.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.admin.indices.validate.query;
 
+import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.action.support.broadcast.BroadcastOperationRequestBuilder;
 import org.elasticsearch.client.ElasticsearchClient;
diff --git a/core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java b/core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java
index fa2fd31..9d51904 100644
--- a/core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java
+++ b/core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java
@@ -41,7 +41,7 @@ import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.index.IndexService;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.script.ScriptService;
@@ -166,10 +166,10 @@ public class TransportExistsAction extends TransportBroadcastAction<ExistsReques
             BytesReference source = request.querySource();
             if (source != null && source.length() > 0) {
                 try {
-                    QueryShardContext.setTypes(request.types());
+                    QueryParseContext.setTypes(request.types());
                     context.parsedQuery(indexService.queryParserService().parseQuery(source));
                 } finally {
-                    QueryShardContext.removeTypes();
+                    QueryParseContext.removeTypes();
                 }
             }
             context.preProcess();
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/JVMCheck.java b/core/src/main/java/org/elasticsearch/bootstrap/JVMCheck.java
index 9d12293..db9e2ba 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/JVMCheck.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/JVMCheck.java
@@ -59,7 +59,7 @@ final class JVMCheck {
         /** Returns an error message to the user for a broken version */
         String getErrorMessage() {
             StringBuilder sb = new StringBuilder();
-            sb.append("Java version: ").append(Constants.JAVA_VERSION);
+            sb.append("Java version: ").append(fullVersion());
             sb.append(" suffers from critical bug ").append(bugUrl);
             sb.append(" which can cause data corruption.");
             sb.append(System.lineSeparator());
@@ -111,7 +111,7 @@ final class JVMCheck {
      */
     static void check() {
         if (Boolean.parseBoolean(System.getProperty(JVM_BYPASS))) {
-            Loggers.getLogger(JVMCheck.class).warn("bypassing jvm version check for version [{}], this can result in data corruption!", Constants.JAVA_VERSION);
+            Loggers.getLogger(JVMCheck.class).warn("bypassing jvm version check for version [{}], this can result in data corruption!", fullVersion());
         } else if ("Oracle Corporation".equals(Constants.JVM_VENDOR)) {
             HotspotBug bug = JVM_BROKEN_HOTSPOT_VERSIONS.get(Constants.JVM_VERSION);
             if (bug != null) {
@@ -135,11 +135,28 @@ final class JVMCheck {
                 StringBuilder sb = new StringBuilder();
                 sb.append("IBM J9 runtimes < 2.8 suffer from several bugs which can cause data corruption.");
                 sb.append(System.lineSeparator());
-                sb.append("Your version: " + Constants.JVM_VERSION);
+                sb.append("Your version: " + fullVersion());
                 sb.append(System.lineSeparator());
                 sb.append("Please upgrade the JVM to a recent IBM JDK");
                 throw new RuntimeException(sb.toString());
             }
         }
     }
+    
+    /** 
+     * Returns java + jvm version, looks like this:
+     * {@code Oracle Corporation 1.8.0_45 [Java HotSpot(TM) 64-Bit Server VM 25.45-b02]}
+     */
+    static String fullVersion() {
+        StringBuilder sb = new StringBuilder();
+        sb.append(Constants.JAVA_VENDOR);
+        sb.append(" ");
+        sb.append(Constants.JAVA_VERSION);
+        sb.append(" [");
+        sb.append(Constants.JVM_NAME);
+        sb.append(" ");
+        sb.append(Constants.JVM_VERSION);
+        sb.append("]");
+        return sb.toString();
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/cluster/ClusterState.java b/core/src/main/java/org/elasticsearch/cluster/ClusterState.java
index 8fb8667..2bae507 100644
--- a/core/src/main/java/org/elasticsearch/cluster/ClusterState.java
+++ b/core/src/main/java/org/elasticsearch/cluster/ClusterState.java
@@ -231,14 +231,6 @@ public class ClusterState implements ToXContent, Diffable<ClusterState> {
         return routingTable();
     }
 
-    public RoutingNodes routingNodes() {
-        return routingTable.routingNodes(this);
-    }
-
-    public RoutingNodes getRoutingNodes() {
-        return readOnlyRoutingNodes();
-    }
-
     public ClusterBlocks blocks() {
         return this.blocks;
     }
@@ -269,14 +261,13 @@ public class ClusterState implements ToXContent, Diffable<ClusterState> {
     }
 
     /**
-     * Returns a built (on demand) routing nodes view of the routing table. <b>NOTE, the routing nodes
-     * are mutable, use them just for read operations</b>
+     * Returns a built (on demand) routing nodes view of the routing table.
      */
-    public RoutingNodes readOnlyRoutingNodes() {
+    public RoutingNodes getRoutingNodes() {
         if (routingNodes != null) {
             return routingNodes;
         }
-        routingNodes = routingTable.routingNodes(this);
+        routingNodes = new RoutingNodes(this);
         return routingNodes;
     }
 
@@ -288,7 +279,7 @@ public class ClusterState implements ToXContent, Diffable<ClusterState> {
         sb.append("meta data version: ").append(metaData.version()).append("\n");
         sb.append(nodes().prettyPrint());
         sb.append(routingTable().prettyPrint());
-        sb.append(readOnlyRoutingNodes().prettyPrint());
+        sb.append(getRoutingNodes().prettyPrint());
         return sb.toString();
     }
 
@@ -509,13 +500,13 @@ public class ClusterState implements ToXContent, Diffable<ClusterState> {
         if (metrics.contains(Metric.ROUTING_NODES)) {
             builder.startObject("routing_nodes");
             builder.startArray("unassigned");
-            for (ShardRouting shardRouting : readOnlyRoutingNodes().unassigned()) {
+            for (ShardRouting shardRouting : getRoutingNodes().unassigned()) {
                 shardRouting.toXContent(builder, params);
             }
             builder.endArray();
 
             builder.startObject("nodes");
-            for (RoutingNode routingNode : readOnlyRoutingNodes()) {
+            for (RoutingNode routingNode : getRoutingNodes()) {
                 builder.startArray(routingNode.nodeId() == null ? "null" : routingNode.nodeId(), XContentBuilder.FieldCaseConversion.NONE);
                 for (ShardRouting shardRouting : routingNode) {
                     shardRouting.toXContent(builder, params);
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/AliasValidator.java b/core/src/main/java/org/elasticsearch/cluster/metadata/AliasValidator.java
index d5b398c..f12824d 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/AliasValidator.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/AliasValidator.java
@@ -28,7 +28,7 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.Index;
 import org.elasticsearch.index.query.IndexQueryParserService;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.indices.InvalidAliasNameException;
 
 import java.io.IOException;
@@ -142,10 +142,10 @@ public class AliasValidator extends AbstractComponent {
     }
 
     private void validateAliasFilter(XContentParser parser, IndexQueryParserService indexQueryParserService) throws IOException {
-        QueryShardContext context = indexQueryParserService.getShardContext();
+        QueryParseContext context = indexQueryParserService.getParseContext();
         try {
             context.reset(parser);
-            context.parseContext().parseInnerFilter();
+            context.parseInnerFilter();
         } finally {
             context.reset(null);
             parser.close();
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java b/core/src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java
index ac5a648..85a14e2 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java
@@ -498,7 +498,6 @@ public class IndexRoutingTable extends AbstractDiffable<IndexRoutingTable> imple
                     for (ShardRouting shardRouting : indexShard) {
                         if (!shardRouting.primary()) {
                             builder.removeShard(shardRouting);
-                            removed = true;
                             break;
                         }
                     }
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java
index dcb44e5..7acedd2 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java
@@ -58,6 +58,8 @@ public class RoutingNodes implements Iterable<RoutingNode> {
 
     private final ImmutableOpenMap<String, ClusterState.Custom> customs;
 
+    private final boolean readOnly;
+
     private int inactivePrimaryCount = 0;
 
     private int inactiveShardCount = 0;
@@ -67,6 +69,11 @@ public class RoutingNodes implements Iterable<RoutingNode> {
     private final Map<String, ObjectIntHashMap<String>> nodesPerAttributeNames = new HashMap<>();
 
     public RoutingNodes(ClusterState clusterState) {
+        this(clusterState, true);
+    }
+
+    public RoutingNodes(ClusterState clusterState, boolean readOnly) {
+        this.readOnly = readOnly;
         this.metaData = clusterState.metaData();
         this.blocks = clusterState.blocks();
         this.routingTable = clusterState.routingTable();
@@ -93,7 +100,7 @@ public class RoutingNodes implements Iterable<RoutingNode> {
                             entries = newArrayList();
                             nodesToShards.put(shard.currentNodeId(), entries);
                         }
-                        ShardRouting sr = new ShardRouting(shard);
+                        final ShardRouting sr = getRouting(shard, readOnly);
                         entries.add(sr);
                         assignedShardsAdd(sr);
                         if (shard.relocating()) {
@@ -105,9 +112,12 @@ public class RoutingNodes implements Iterable<RoutingNode> {
                             }
                             // add the counterpart shard with relocatingNodeId reflecting the source from which
                             // it's relocating from.
-                            sr = shard.buildTargetRelocatingShard();
-                            entries.add(sr);
-                            assignedShardsAdd(sr);
+                            ShardRouting targetShardRouting = shard.buildTargetRelocatingShard();
+                            if (readOnly) {
+                                targetShardRouting.freeze();
+                            }
+                            entries.add(targetShardRouting);
+                            assignedShardsAdd(targetShardRouting);
                         } else if (!shard.active()) { // shards that are initializing without being relocated
                             if (shard.primary()) {
                                 inactivePrimaryCount++;
@@ -115,7 +125,7 @@ public class RoutingNodes implements Iterable<RoutingNode> {
                             inactiveShardCount++;
                         }
                     } else {
-                        ShardRouting sr = new ShardRouting(shard);
+                        final ShardRouting sr =  getRouting(shard, readOnly);
                         assignedShardsAdd(sr);
                         unassignedShards.add(sr);
                     }
@@ -128,6 +138,15 @@ public class RoutingNodes implements Iterable<RoutingNode> {
         }
     }
 
+    private static ShardRouting getRouting(ShardRouting src, boolean readOnly) {
+        if (readOnly) {
+            src.freeze(); // we just freeze and reuse this instance if we are read only
+        } else {
+            src = new ShardRouting(src);
+        }
+        return src;
+    }
+
     @Override
     public Iterator<RoutingNode> iterator() {
         return Iterators.unmodifiableIterator(nodesToShards.values().iterator());
@@ -165,18 +184,6 @@ public class RoutingNodes implements Iterable<RoutingNode> {
         return (T) customs.get(type);
     }
 
-    public int requiredAverageNumberOfShardsPerNode() {
-        int totalNumberOfShards = 0;
-        // we need to recompute to take closed shards into account
-        for (ObjectCursor<IndexMetaData> cursor : metaData.indices().values()) {
-            IndexMetaData indexMetaData = cursor.value;
-            if (indexMetaData.state() == IndexMetaData.State.OPEN) {
-                totalNumberOfShards += indexMetaData.totalNumberOfShards();
-            }
-        }
-        return totalNumberOfShards / nodesToShards.size();
-    }
-
     public boolean hasUnassigned() {
         return !unassignedShards.isEmpty();
     }
@@ -339,6 +346,7 @@ public class RoutingNodes implements Iterable<RoutingNode> {
      * Moves a shard from unassigned to initialize state
      */
     public void initialize(ShardRouting shard, String nodeId) {
+        ensureMutable();
         assert shard.unassigned() : shard;
         shard.initialize(nodeId);
         node(nodeId).add(shard);
@@ -355,6 +363,7 @@ public class RoutingNodes implements Iterable<RoutingNode> {
      * shard.
      */
     public ShardRouting relocate(ShardRouting shard, String nodeId) {
+        ensureMutable();
         relocatingShards++;
         shard.relocate(nodeId);
         ShardRouting target = shard.buildTargetRelocatingShard();
@@ -367,6 +376,7 @@ public class RoutingNodes implements Iterable<RoutingNode> {
      * Mark a shard as started and adjusts internal statistics.
      */
     public void started(ShardRouting shard) {
+        ensureMutable();
         assert !shard.active() : "expected an intializing shard " + shard;
         if (shard.relocatingNodeId() == null) {
             // if this is not a target shard for relocation, we need to update statistics
@@ -382,6 +392,7 @@ public class RoutingNodes implements Iterable<RoutingNode> {
      * Cancels a relocation of a shard that shard must relocating.
      */
     public void cancelRelocation(ShardRouting shard) {
+        ensureMutable();
         relocatingShards--;
         shard.cancelRelocation();
     }
@@ -392,6 +403,7 @@ public class RoutingNodes implements Iterable<RoutingNode> {
      * @param shards the shard to have its primary status swapped.
      */
     public void swapPrimaryFlag(ShardRouting... shards) {
+        ensureMutable();
         for (ShardRouting shard : shards) {
             if (shard.primary()) {
                 shard.moveFromPrimary();
@@ -420,6 +432,7 @@ public class RoutingNodes implements Iterable<RoutingNode> {
      * @param shard
      */
     private void remove(ShardRouting shard) {
+        ensureMutable();
         if (!shard.active() && shard.relocatingNodeId() == null) {
             inactiveShardCount--;
             assert inactiveShardCount >= 0;
@@ -454,6 +467,7 @@ public class RoutingNodes implements Iterable<RoutingNode> {
     }
 
     private void assignedShardsRemove(ShardRouting shard) {
+        ensureMutable();
         final List<ShardRouting> replicaSet = assignedShards.get(shard.shardId());
         if (replicaSet != null) {
             final Iterator<ShardRouting> iterator = replicaSet.iterator();
@@ -473,6 +487,7 @@ public class RoutingNodes implements Iterable<RoutingNode> {
     }
 
     public void addNode(DiscoveryNode node) {
+        ensureMutable();
         RoutingNode routingNode = new RoutingNode(node.id(), node);
         nodesToShards.put(routingNode.nodeId(), routingNode);
     }
@@ -490,6 +505,7 @@ public class RoutingNodes implements Iterable<RoutingNode> {
     }
 
     public void reinitShadowPrimary(ShardRouting candidate) {
+        ensureMutable();
         if (candidate.relocating()) {
             cancelRelocation(candidate);
         }
@@ -623,6 +639,7 @@ public class RoutingNodes implements Iterable<RoutingNode> {
             }
 
             private void innerRemove() {
+                nodes.ensureMutable();
                 iterator.remove();
                 if (current.primary()) {
                     primaries--;
@@ -810,6 +827,7 @@ public class RoutingNodes implements Iterable<RoutingNode> {
 
         @Override
         public void remove() {
+            ensureMutable();
             delegate.remove();
             RoutingNodes.this.remove(shard);
             removed = true;
@@ -827,6 +845,7 @@ public class RoutingNodes implements Iterable<RoutingNode> {
         }
 
         public void moveToUnassigned(UnassignedInfo unassignedInfo) {
+            ensureMutable();
             if (isRemoved() == false) {
                 remove();
             }
@@ -839,4 +858,10 @@ public class RoutingNodes implements Iterable<RoutingNode> {
             return shard;
         }
     }
+
+    private void ensureMutable() {
+        if (readOnly) {
+            throw new IllegalStateException("can't modify RoutingNodes - readonly");
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingService.java b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingService.java
index bfc1d93..a3aa9b2 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingService.java
@@ -57,6 +57,7 @@ public class RoutingService extends AbstractLifecycleComponent<RoutingService> i
     private AtomicBoolean rerouting = new AtomicBoolean();
     private volatile long registeredNextDelaySetting = Long.MAX_VALUE;
     private volatile ScheduledFuture registeredNextDelayFuture;
+    private volatile long unassignedShardsAllocatedTimestamp = 0;
 
     @Inject
     public RoutingService(Settings settings, ThreadPool threadPool, ClusterService clusterService, AllocationService allocationService) {
@@ -88,6 +89,19 @@ public class RoutingService extends AbstractLifecycleComponent<RoutingService> i
     }
 
     /**
+     * Update the last time the allocator tried to assign unassigned shards
+     *
+     * This is used so that both the GatewayAllocator and RoutingService use a
+     * consistent timestamp for comparing which shards have been delayed to
+     * avoid a race condition where GatewayAllocator thinks the shard should
+     * be delayed and the RoutingService thinks it has already passed the delay
+     * and that the GatewayAllocator has/will handle it.
+     */
+    public void setUnassignedShardsAllocatedTimestamp(long timeInMillis) {
+        this.unassignedShardsAllocatedTimestamp = timeInMillis;
+    }
+
+    /**
      * Initiates a reroute.
      */
     public final void reroute(String reason) {
@@ -108,20 +122,29 @@ public class RoutingService extends AbstractLifecycleComponent<RoutingService> i
             if (nextDelaySetting > 0 && nextDelaySetting < registeredNextDelaySetting) {
                 FutureUtils.cancel(registeredNextDelayFuture);
                 registeredNextDelaySetting = nextDelaySetting;
-                TimeValue nextDelay = TimeValue.timeValueMillis(UnassignedInfo.findNextDelayedAllocationIn(settings, event.state()));
-                logger.info("delaying allocation for [{}] unassigned shards, next check in [{}]", UnassignedInfo.getNumberOfDelayedUnassigned(settings, event.state()), nextDelay);
-                registeredNextDelayFuture = threadPool.schedule(nextDelay, ThreadPool.Names.SAME, new AbstractRunnable() {
-                    @Override
-                    protected void doRun() throws Exception {
-                        registeredNextDelaySetting = Long.MAX_VALUE;
-                        reroute("assign delayed unassigned shards");
-                    }
-
-                    @Override
-                    public void onFailure(Throwable t) {
-                        logger.warn("failed to schedule/execute reroute post unassigned shard", t);
-                    }
-                });
+                // We use System.currentTimeMillis here because we want the
+                // next delay from the "now" perspective, rather than the
+                // delay from the last time the GatewayAllocator tried to
+                // assign/delay the shard
+                TimeValue nextDelay = TimeValue.timeValueMillis(UnassignedInfo.findNextDelayedAllocationIn(System.currentTimeMillis(), settings, event.state()));
+                int unassignedDelayedShards = UnassignedInfo.getNumberOfDelayedUnassigned(unassignedShardsAllocatedTimestamp, settings, event.state());
+                if (unassignedDelayedShards > 0) {
+                    logger.info("delaying allocation for [{}] unassigned shards, next check in [{}]",
+                            unassignedDelayedShards, nextDelay);
+                    registeredNextDelayFuture = threadPool.schedule(nextDelay, ThreadPool.Names.SAME, new AbstractRunnable() {
+                        @Override
+                        protected void doRun() throws Exception {
+                            registeredNextDelaySetting = Long.MAX_VALUE;
+                            reroute("assign delayed unassigned shards");
+                        }
+
+                        @Override
+                        public void onFailure(Throwable t) {
+                            logger.warn("failed to schedule/execute reroute post unassigned shard", t);
+                            registeredNextDelaySetting = Long.MAX_VALUE;
+                        }
+                    });
+                }
             } else {
                 logger.trace("no need to schedule reroute due to delayed unassigned, next_delay_setting [{}], registered [{}]", nextDelaySetting, registeredNextDelaySetting);
             }
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java
index 8b4f75d..449867b 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java
@@ -90,10 +90,6 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
         return indicesRouting();
     }
 
-    public RoutingNodes routingNodes(ClusterState state) {
-        return new RoutingNodes(state);
-    }
-
     public RoutingTable validateRaiseException(MetaData metaData) throws RoutingValidationException {
         RoutingTableValidation validation = validate(metaData);
         if (!validation.valid()) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/UnassignedInfo.java b/core/src/main/java/org/elasticsearch/cluster/routing/UnassignedInfo.java
index a09b034..1c60249 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/UnassignedInfo.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/UnassignedInfo.java
@@ -199,12 +199,12 @@ public class UnassignedInfo implements ToXContent, Writeable<UnassignedInfo> {
     /**
      * The time in millisecond until this unassigned shard can be reassigned.
      */
-    public long getDelayAllocationExpirationIn(Settings settings, Settings indexSettings) {
+    public long getDelayAllocationExpirationIn(long unassignedShardsAllocatedTimestamp, Settings settings, Settings indexSettings) {
         long delayTimeout = getAllocationDelayTimeoutSetting(settings, indexSettings);
         if (delayTimeout == 0) {
             return 0;
         }
-        long delta = System.currentTimeMillis() - timestamp;
+        long delta = unassignedShardsAllocatedTimestamp - timestamp;
         // account for time drift, treat it as no timeout
         if (delta < 0) {
             return 0;
@@ -216,12 +216,12 @@ public class UnassignedInfo implements ToXContent, Writeable<UnassignedInfo> {
     /**
      * Returns the number of shards that are unassigned and currently being delayed.
      */
-    public static int getNumberOfDelayedUnassigned(Settings settings, ClusterState state) {
+    public static int getNumberOfDelayedUnassigned(long unassignedShardsAllocatedTimestamp, Settings settings, ClusterState state) {
         int count = 0;
         for (ShardRouting shard : state.routingTable().shardsWithState(ShardRoutingState.UNASSIGNED)) {
             if (shard.primary() == false) {
                 IndexMetaData indexMetaData = state.metaData().index(shard.getIndex());
-                long delay = shard.unassignedInfo().getDelayAllocationExpirationIn(settings, indexMetaData.getSettings());
+                long delay = shard.unassignedInfo().getDelayAllocationExpirationIn(unassignedShardsAllocatedTimestamp, settings, indexMetaData.getSettings());
                 if (delay > 0) {
                     count++;
                 }
@@ -251,12 +251,12 @@ public class UnassignedInfo implements ToXContent, Writeable<UnassignedInfo> {
     /**
      * Finds the next (closest) delay expiration of an unassigned shard. Returns 0 if there are none.
      */
-    public static long findNextDelayedAllocationIn(Settings settings, ClusterState state) {
+    public static long findNextDelayedAllocationIn(long unassignedShardsAllocatedTimestamp, Settings settings, ClusterState state) {
         long nextDelay = Long.MAX_VALUE;
         for (ShardRouting shard : state.routingTable().shardsWithState(ShardRoutingState.UNASSIGNED)) {
             if (shard.primary() == false) {
                 IndexMetaData indexMetaData = state.metaData().index(shard.getIndex());
-                long nextShardDelay = shard.unassignedInfo().getDelayAllocationExpirationIn(settings, indexMetaData.getSettings());
+                long nextShardDelay = shard.unassignedInfo().getDelayAllocationExpirationIn(unassignedShardsAllocatedTimestamp, settings, indexMetaData.getSettings());
                 if (nextShardDelay > 0 && nextShardDelay < nextDelay) {
                     nextDelay = nextShardDelay;
                 }
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java
index a2408a6..e5a4685 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java
@@ -70,7 +70,7 @@ public class AllocationService extends AbstractComponent {
     }
 
     public RoutingAllocation.Result applyStartedShards(ClusterState clusterState, List<? extends ShardRouting> startedShards, boolean withReroute) {
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = getMutableRoutingNodes(clusterState);
         // shuffle the unassigned nodes, just so we won't have things like poison failed shards
         routingNodes.unassigned().shuffle();
         StartedRerouteAllocation allocation = new StartedRerouteAllocation(allocationDeciders, routingNodes, clusterState.nodes(), startedShards, clusterInfoService.getClusterInfo());
@@ -95,7 +95,7 @@ public class AllocationService extends AbstractComponent {
      * <p>If the same instance of the routing table is returned, then no change has been made.</p>
      */
     public RoutingAllocation.Result applyFailedShards(ClusterState clusterState, List<FailedRerouteAllocation.FailedShard> failedShards) {
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = getMutableRoutingNodes(clusterState);
         // shuffle the unassigned nodes, just so we won't have things like poison failed shards
         routingNodes.unassigned().shuffle();
         FailedRerouteAllocation allocation = new FailedRerouteAllocation(allocationDeciders, routingNodes, clusterState.nodes(), failedShards, clusterInfoService.getClusterInfo());
@@ -116,7 +116,7 @@ public class AllocationService extends AbstractComponent {
     }
 
     public RoutingAllocation.Result reroute(ClusterState clusterState, AllocationCommands commands, boolean explain) {
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = getMutableRoutingNodes(clusterState);
         // we don't shuffle the unassigned shards here, to try and get as close as possible to
         // a consistent result of the effect the commands have on the routing
         // this allows systems to dry run the commands, see the resulting cluster state, and act on it
@@ -149,7 +149,7 @@ public class AllocationService extends AbstractComponent {
      * <p>If the same instance of the routing table is returned, then no change has been made.
      */
     public RoutingAllocation.Result reroute(ClusterState clusterState, boolean debug) {
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = getMutableRoutingNodes(clusterState);
         // shuffle the unassigned nodes, just so we won't have things like poison failed shards
         routingNodes.unassigned().shuffle();
         RoutingAllocation allocation = new RoutingAllocation(allocationDeciders, routingNodes, clusterState.nodes(), clusterInfoService.getClusterInfo());
@@ -461,4 +461,9 @@ public class AllocationService extends AbstractComponent {
         assert matchedNode.isRemoved() : "failedShard " + failedShard + " was matched but wasn't removed";
         return true;
     }
+
+    private RoutingNodes getMutableRoutingNodes(ClusterState clusterState) {
+        RoutingNodes routingNodes = new RoutingNodes(clusterState, false); // this is a costly operation - only call this once!
+        return routingNodes;
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java b/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java
index 70e9932..1fea2ed 100644
--- a/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java
@@ -389,7 +389,7 @@ public class InternalClusterService extends AbstractLifecycleComponent<ClusterSe
                     StringBuilder sb = new StringBuilder("failed to execute cluster state update in ").append(executionTime).append(", state:\nversion [").append(previousClusterState.version()).append("], source [").append(source).append("]\n");
                     sb.append(previousClusterState.nodes().prettyPrint());
                     sb.append(previousClusterState.routingTable().prettyPrint());
-                    sb.append(previousClusterState.readOnlyRoutingNodes().prettyPrint());
+                    sb.append(previousClusterState.getRoutingNodes().prettyPrint());
                     logger.trace(sb.toString(), e);
                 }
                 warnAboutSlowTaskIfNeeded(executionTime, source);
@@ -533,7 +533,7 @@ public class InternalClusterService extends AbstractLifecycleComponent<ClusterSe
                 StringBuilder sb = new StringBuilder("failed to apply updated cluster state in ").append(executionTime).append(":\nversion [").append(newClusterState.version()).append("], uuid [").append(newClusterState.stateUUID()).append("], source [").append(source).append("]\n");
                 sb.append(newClusterState.nodes().prettyPrint());
                 sb.append(newClusterState.routingTable().prettyPrint());
-                sb.append(newClusterState.readOnlyRoutingNodes().prettyPrint());
+                sb.append(newClusterState.getRoutingNodes().prettyPrint());
                 logger.warn(sb.toString(), t);
                 // TODO: do we want to call updateTask.onFailure here?
             }
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/FilterStreamInput.java b/core/src/main/java/org/elasticsearch/common/io/stream/FilterStreamInput.java
index 5f3bd01..0dac786 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/FilterStreamInput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/FilterStreamInput.java
@@ -68,4 +68,4 @@ public abstract class FilterStreamInput extends StreamInput {
     public void setVersion(Version version) {
         delegate.setVersion(version);
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java b/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
index ecef804..cb42a9f 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
@@ -33,7 +33,6 @@ import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.text.StringAndBytesText;
 import org.elasticsearch.common.text.Text;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
 
@@ -46,18 +45,8 @@ import static org.elasticsearch.ElasticsearchException.readStackTrace;
 
 public abstract class StreamInput extends InputStream {
 
-    private final NamedWriteableRegistry namedWriteableRegistry;
-
     private Version version = Version.CURRENT;
 
-    protected StreamInput() {
-        this.namedWriteableRegistry = new NamedWriteableRegistry();
-    }
-
-    protected StreamInput(NamedWriteableRegistry namedWriteableRegistry) {
-        this.namedWriteableRegistry = namedWriteableRegistry;
-    }
-
     public Version getVersion() {
         return this.version;
     }
@@ -570,13 +559,6 @@ public abstract class StreamInput extends InputStream {
         throw new UnsupportedOperationException();
     }
 
-    /**
-     * Reads a {@link QueryBuilder} from the current stream
-     */
-    public QueryBuilder readQuery() throws IOException {
-        return readNamedWriteable(QueryBuilder.class);
-    }
-
     public static StreamInput wrap(BytesReference reference) {
         if (reference.hasArray() == false) {
             reference = reference.toBytesArray();
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java b/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
index 84cd303..afd6073 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
@@ -32,7 +32,6 @@ import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.regex.Regex;
 import org.elasticsearch.common.text.Text;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.joda.time.ReadableInstant;
 
 import java.io.EOFException;
@@ -622,11 +621,4 @@ public abstract class StreamOutput extends OutputStream {
         writeString(namedWriteable.getWriteableName());
         namedWriteable.writeTo(this);
     }
-
-    /**
-     * Writes a {@link QueryBuilder} to the current stream
-     */
-    public void writeQuery(QueryBuilder queryBuilder) throws IOException {
-        writeNamedWriteable(queryBuilder);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/search/function/BoostScoreFunction.java b/core/src/main/java/org/elasticsearch/common/lucene/search/function/BoostScoreFunction.java
index 58d438a..13b4526 100644
--- a/core/src/main/java/org/elasticsearch/common/lucene/search/function/BoostScoreFunction.java
+++ b/core/src/main/java/org/elasticsearch/common/lucene/search/function/BoostScoreFunction.java
@@ -58,6 +58,11 @@ public class BoostScoreFunction extends ScoreFunction {
     }
 
     @Override
+    public boolean needsScores() {
+        return false;
+    }
+
+    @Override
     public String toString() {
         return "boost[" + boost + "]";
     }
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/search/function/FieldValueFactorFunction.java b/core/src/main/java/org/elasticsearch/common/lucene/search/function/FieldValueFactorFunction.java
index 8aa4897a..488c5fe 100644
--- a/core/src/main/java/org/elasticsearch/common/lucene/search/function/FieldValueFactorFunction.java
+++ b/core/src/main/java/org/elasticsearch/common/lucene/search/function/FieldValueFactorFunction.java
@@ -91,6 +91,11 @@ public class FieldValueFactorFunction extends ScoreFunction {
         };
     }
 
+    @Override
+    public boolean needsScores() {
+        return false;
+    }
+
     /**
      * The Type class encapsulates the modification types that can be applied
      * to the score/value product.
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java b/core/src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java
index 0b96e0b..91e93be 100644
--- a/core/src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java
+++ b/core/src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java
@@ -124,25 +124,31 @@ public class FiltersFunctionScoreQuery extends Query {
 
     @Override
     public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {
-        // TODO: needsScores
-        // if we dont need scores, just return the underlying Weight?
-        Weight subQueryWeight = subQuery.createWeight(searcher, needsScores);
+        if (needsScores == false) {
+            return subQuery.createWeight(searcher, needsScores);
+        }
+
+        boolean subQueryNeedsScores = combineFunction != CombineFunction.REPLACE;
         Weight[] filterWeights = new Weight[filterFunctions.length];
         for (int i = 0; i < filterFunctions.length; ++i) {
+            subQueryNeedsScores |= filterFunctions[i].function.needsScores();
             filterWeights[i] = searcher.createNormalizedWeight(filterFunctions[i].filter, false);
         }
-        return new CustomBoostFactorWeight(this, subQueryWeight, filterWeights);
+        Weight subQueryWeight = subQuery.createWeight(searcher, subQueryNeedsScores);
+        return new CustomBoostFactorWeight(this, subQueryWeight, filterWeights, subQueryNeedsScores);
     }
 
     class CustomBoostFactorWeight extends Weight {
 
         final Weight subQueryWeight;
         final Weight[] filterWeights;
+        final boolean needsScores;
 
-        public CustomBoostFactorWeight(Query parent, Weight subQueryWeight, Weight[] filterWeights) throws IOException {
+        public CustomBoostFactorWeight(Query parent, Weight subQueryWeight, Weight[] filterWeights, boolean needsScores) throws IOException {
             super(parent);
             this.subQueryWeight = subQueryWeight;
             this.filterWeights = filterWeights;
+            this.needsScores = needsScores;
         }
 
         @Override
@@ -179,7 +185,7 @@ public class FiltersFunctionScoreQuery extends Query {
                 Scorer filterScorer = filterWeights[i].scorer(context, null); // no need to apply accepted docs
                 docSets[i] = Lucene.asSequentialAccessBits(context.reader().maxDoc(), filterScorer);
             }
-            return new FiltersFunctionFactorScorer(this, subQueryScorer, scoreMode, filterFunctions, maxBoost, functions, docSets, combineFunction, minScore);
+            return new FiltersFunctionFactorScorer(this, subQueryScorer, scoreMode, filterFunctions, maxBoost, functions, docSets, combineFunction, minScore, needsScores);
         }
 
         @Override
@@ -269,21 +275,26 @@ public class FiltersFunctionScoreQuery extends Query {
         private final ScoreMode scoreMode;
         private final LeafScoreFunction[] functions;
         private final Bits[] docSets;
+        private final boolean needsScores;
 
         private FiltersFunctionFactorScorer(CustomBoostFactorWeight w, Scorer scorer, ScoreMode scoreMode, FilterFunction[] filterFunctions,
-                                            float maxBoost, LeafScoreFunction[] functions, Bits[] docSets, CombineFunction scoreCombiner, Float minScore) throws IOException {
+                                            float maxBoost, LeafScoreFunction[] functions, Bits[] docSets, CombineFunction scoreCombiner, Float minScore, boolean needsScores) throws IOException {
             super(w, scorer, maxBoost, scoreCombiner, minScore);
             this.scoreMode = scoreMode;
             this.filterFunctions = filterFunctions;
             this.functions = functions;
             this.docSets = docSets;
+            this.needsScores = needsScores;
         }
 
         @Override
         public float innerScore() throws IOException {
             int docId = scorer.docID();
             double factor = 1.0f;
-            float subQueryScore = scorer.score();
+            // Even if the weight is created with needsScores=false, it might
+            // be costly to call score(), so we explicitly check if scores
+            // are needed
+            float subQueryScore = needsScores ? scorer.score() : 0f;
             if (scoreMode == ScoreMode.First) {
                 for (int i = 0; i < filterFunctions.length; i++) {
                     if (docSets[i].get(docId)) {
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/search/function/FunctionScoreQuery.java b/core/src/main/java/org/elasticsearch/common/lucene/search/function/FunctionScoreQuery.java
index 1f8eb8e..b3ad83e 100644
--- a/core/src/main/java/org/elasticsearch/common/lucene/search/function/FunctionScoreQuery.java
+++ b/core/src/main/java/org/elasticsearch/common/lucene/search/function/FunctionScoreQuery.java
@@ -44,7 +44,7 @@ public class FunctionScoreQuery extends Query {
     public FunctionScoreQuery(Query subQuery, ScoreFunction function, Float minScore) {
         this.subQuery = subQuery;
         this.function = function;
-        this.combineFunction = function == null? combineFunction.MULT : function.getDefaultScoreCombiner();
+        this.combineFunction = function == null? CombineFunction.MULT : function.getDefaultScoreCombiner();
         this.minScore = minScore;
     }
 
@@ -87,19 +87,27 @@ public class FunctionScoreQuery extends Query {
 
     @Override
     public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {
-        // TODO: needsScores
-        // if we don't need scores, just return the underlying weight?
-        Weight subQueryWeight = subQuery.createWeight(searcher, needsScores);
-        return new CustomBoostFactorWeight(this, subQueryWeight);
+        if (needsScores == false) {
+            return subQuery.createWeight(searcher, needsScores);
+        }
+
+        boolean subQueryNeedsScores =
+                combineFunction != CombineFunction.REPLACE // if we don't replace we need the original score
+                || function == null // when the function is null, we just multiply the score, so we need it
+                || function.needsScores(); // some scripts can replace with a script that returns eg. 1/_score
+        Weight subQueryWeight = subQuery.createWeight(searcher, subQueryNeedsScores);
+        return new CustomBoostFactorWeight(this, subQueryWeight, subQueryNeedsScores);
     }
 
     class CustomBoostFactorWeight extends Weight {
 
         final Weight subQueryWeight;
+        final boolean needsScores;
 
-        public CustomBoostFactorWeight(Query parent, Weight subQueryWeight) throws IOException {
+        public CustomBoostFactorWeight(Query parent, Weight subQueryWeight, boolean needsScores) throws IOException {
             super(parent);
             this.subQueryWeight = subQueryWeight;
+            this.needsScores = needsScores;
         }
 
         @Override
@@ -129,7 +137,7 @@ public class FunctionScoreQuery extends Query {
             if (function != null) {
                 leafFunction = function.getLeafScoreFunction(context);
             }
-            return new FunctionFactorScorer(this, subQueryScorer, leafFunction, maxBoost, combineFunction, minScore);
+            return new FunctionFactorScorer(this, subQueryScorer, leafFunction, maxBoost, combineFunction, minScore, needsScores);
         }
 
         @Override
@@ -150,16 +158,21 @@ public class FunctionScoreQuery extends Query {
     static class FunctionFactorScorer extends CustomBoostFactorScorer {
 
         private final LeafScoreFunction function;
+        private final boolean needsScores;
 
-        private FunctionFactorScorer(CustomBoostFactorWeight w, Scorer scorer, LeafScoreFunction function, float maxBoost, CombineFunction scoreCombiner, Float minScore)
+        private FunctionFactorScorer(CustomBoostFactorWeight w, Scorer scorer, LeafScoreFunction function, float maxBoost, CombineFunction scoreCombiner, Float minScore, boolean needsScores)
                 throws IOException {
             super(w, scorer, maxBoost, scoreCombiner, minScore);
             this.function = function;
+            this.needsScores = needsScores;
         }
 
         @Override
         public float innerScore() throws IOException {
-            float score = scorer.score();
+            // Even if the weight is created with needsScores=false, it might
+            // be costly to call score(), so we explicitly check if scores
+            // are needed
+            float score = needsScores ? scorer.score() : 0f;
             if (function == null) {
                 return subQueryBoost * score;
             } else {
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/search/function/RandomScoreFunction.java b/core/src/main/java/org/elasticsearch/common/lucene/search/function/RandomScoreFunction.java
index 934640e..a95f24e 100644
--- a/core/src/main/java/org/elasticsearch/common/lucene/search/function/RandomScoreFunction.java
+++ b/core/src/main/java/org/elasticsearch/common/lucene/search/function/RandomScoreFunction.java
@@ -81,4 +81,8 @@ public class RandomScoreFunction extends ScoreFunction {
         };
     }
 
+    @Override
+    public boolean needsScores() {
+        return false;
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/search/function/ScoreFunction.java b/core/src/main/java/org/elasticsearch/common/lucene/search/function/ScoreFunction.java
index babefd2..1f12336 100644
--- a/core/src/main/java/org/elasticsearch/common/lucene/search/function/ScoreFunction.java
+++ b/core/src/main/java/org/elasticsearch/common/lucene/search/function/ScoreFunction.java
@@ -39,4 +39,11 @@ public abstract class ScoreFunction {
     }
 
     public abstract LeafScoreFunction getLeafScoreFunction(LeafReaderContext ctx) throws IOException;
+
+    /**
+     * Indicates if document scores are needed by this function.
+     * 
+     * @return {@code true} if scores are needed.
+     */
+    public abstract boolean needsScores();
 }
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/search/function/ScriptScoreFunction.java b/core/src/main/java/org/elasticsearch/common/lucene/search/function/ScriptScoreFunction.java
index d1ee646..1ff3cdd 100644
--- a/core/src/main/java/org/elasticsearch/common/lucene/search/function/ScriptScoreFunction.java
+++ b/core/src/main/java/org/elasticsearch/common/lucene/search/function/ScriptScoreFunction.java
@@ -127,6 +127,11 @@ public class ScriptScoreFunction extends ScoreFunction {
     }
 
     @Override
+    public boolean needsScores() {
+        return script.needsScores();
+    }
+
+    @Override
     public String toString() {
         return "script" + sScript.toString();
     }
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/search/function/WeightFactorFunction.java b/core/src/main/java/org/elasticsearch/common/lucene/search/function/WeightFactorFunction.java
index db651ab..2def071 100644
--- a/core/src/main/java/org/elasticsearch/common/lucene/search/function/WeightFactorFunction.java
+++ b/core/src/main/java/org/elasticsearch/common/lucene/search/function/WeightFactorFunction.java
@@ -71,6 +71,11 @@ public class WeightFactorFunction extends ScoreFunction {
         };
     }
 
+    @Override
+    public boolean needsScores() {
+        return false;
+    }
+
     public Explanation explainWeight() {
         return Explanation.match(getWeight(), "weight");
     }
@@ -99,5 +104,10 @@ public class WeightFactorFunction extends ScoreFunction {
                 }
             };
         }
+
+        @Override
+        public boolean needsScores() {
+            return false;
+        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/gateway/GatewayAllocator.java b/core/src/main/java/org/elasticsearch/gateway/GatewayAllocator.java
index 7b6da4d..449bd67 100644
--- a/core/src/main/java/org/elasticsearch/gateway/GatewayAllocator.java
+++ b/core/src/main/java/org/elasticsearch/gateway/GatewayAllocator.java
@@ -113,6 +113,10 @@ public class GatewayAllocator extends AbstractComponent {
     }
 
     public boolean allocateUnassigned(final RoutingAllocation allocation) {
+        // Take a snapshot of the current time and tell the RoutingService
+        // about it, so it will use a consistent timestamp for delays
+        long lastAllocateUnassignedRun = System.currentTimeMillis();
+        this.routingService.setUnassignedShardsAllocatedTimestamp(lastAllocateUnassignedRun);
         boolean changed = false;
 
         RoutingNodes.UnassignedShards unassigned = allocation.routingNodes().unassigned();
@@ -127,7 +131,7 @@ public class GatewayAllocator extends AbstractComponent {
 
         changed |= primaryShardAllocator.allocateUnassigned(allocation);
         changed |= replicaShardAllocator.processExistingRecoveries(allocation);
-        changed |= replicaShardAllocator.allocateUnassigned(allocation);
+        changed |= replicaShardAllocator.allocateUnassigned(allocation, lastAllocateUnassignedRun);
         return changed;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/gateway/ReplicaShardAllocator.java b/core/src/main/java/org/elasticsearch/gateway/ReplicaShardAllocator.java
index 6ab016b..9af9a4e 100644
--- a/core/src/main/java/org/elasticsearch/gateway/ReplicaShardAllocator.java
+++ b/core/src/main/java/org/elasticsearch/gateway/ReplicaShardAllocator.java
@@ -111,6 +111,10 @@ public abstract class ReplicaShardAllocator extends AbstractComponent {
     }
 
     public boolean allocateUnassigned(RoutingAllocation allocation) {
+        return allocateUnassigned(allocation, System.currentTimeMillis());
+    }
+
+    public boolean allocateUnassigned(RoutingAllocation allocation, long allocateUnassignedTimestapm) {
         boolean changed = false;
         final RoutingNodes routingNodes = allocation.routingNodes();
         final RoutingNodes.UnassignedShards.UnassignedIterator unassignedIterator = routingNodes.unassigned().iterator();
@@ -174,7 +178,7 @@ public abstract class ReplicaShardAllocator extends AbstractComponent {
                 //       will anyhow wait to find an existing copy of the shard to be allocated
                 // note: the other side of the equation is scheduling a reroute in a timely manner, which happens in the RoutingService
                 IndexMetaData indexMetaData = allocation.metaData().index(shard.getIndex());
-                long delay = shard.unassignedInfo().getDelayAllocationExpirationIn(settings, indexMetaData.getSettings());
+                long delay = shard.unassignedInfo().getDelayAllocationExpirationIn(allocateUnassignedTimestapm, settings, indexMetaData.getSettings());
                 if (delay > 0) {
                     logger.debug("[{}][{}]: delaying allocation of [{}] for [{}]", shard.index(), shard.id(), shard, TimeValue.timeValueMillis(delay));
                     /**
diff --git a/core/src/main/java/org/elasticsearch/index/get/ShardGetService.java b/core/src/main/java/org/elasticsearch/index/get/ShardGetService.java
index c17ab75..632284d 100644
--- a/core/src/main/java/org/elasticsearch/index/get/ShardGetService.java
+++ b/core/src/main/java/org/elasticsearch/index/get/ShardGetService.java
@@ -228,8 +228,6 @@ public final class ShardGetService extends AbstractIndexShardComponent {
                         if (source.ttl > 0) {
                             value = docMapper.TTLFieldMapper().valueForSearch(source.timestamp + source.ttl);
                         }
-                    } else if (field.equals(SizeFieldMapper.NAME) && docMapper.rootMapper(SizeFieldMapper.class).fieldType().stored()) {
-                        value = source.source.length();
                     } else {
                         if (searchLookup == null) {
                             searchLookup = new SearchLookup(mapperService, null, new String[]{type});
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java
index 1fbd0f2..80f2603 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java
@@ -48,7 +48,6 @@ import org.elasticsearch.index.mapper.internal.IdFieldMapper;
 import org.elasticsearch.index.mapper.internal.IndexFieldMapper;
 import org.elasticsearch.index.mapper.internal.ParentFieldMapper;
 import org.elasticsearch.index.mapper.internal.RoutingFieldMapper;
-import org.elasticsearch.index.mapper.internal.SizeFieldMapper;
 import org.elasticsearch.index.mapper.internal.SourceFieldMapper;
 import org.elasticsearch.index.mapper.internal.TTLFieldMapper;
 import org.elasticsearch.index.mapper.internal.TimestampFieldMapper;
@@ -106,7 +105,6 @@ public class DocumentMapper implements ToXContent {
             this.rootMappers.put(IdFieldMapper.class, new IdFieldMapper(indexSettings, mapperService.fullName(IdFieldMapper.NAME)));
             this.rootMappers.put(RoutingFieldMapper.class, new RoutingFieldMapper(indexSettings, mapperService.fullName(RoutingFieldMapper.NAME)));
             // add default mappers, order is important (for example analyzer should come before the rest to set context.analyzer)
-            this.rootMappers.put(SizeFieldMapper.class, new SizeFieldMapper(indexSettings, mapperService.fullName(SizeFieldMapper.NAME)));
             this.rootMappers.put(IndexFieldMapper.class, new IndexFieldMapper(indexSettings, mapperService.fullName(IndexFieldMapper.NAME)));
             this.rootMappers.put(SourceFieldMapper.class, new SourceFieldMapper(indexSettings));
             this.rootMappers.put(TypeFieldMapper.class, new TypeFieldMapper(indexSettings, mapperService.fullName(TypeFieldMapper.NAME)));
@@ -283,10 +281,6 @@ public class DocumentMapper implements ToXContent {
         return rootMapper(ParentFieldMapper.class);
     }
 
-    public SizeFieldMapper sizeFieldMapper() {
-        return rootMapper(SizeFieldMapper.class);
-    }
-
     public TimestampFieldMapper timestampFieldMapper() {
         return rootMapper(TimestampFieldMapper.class);
     }
@@ -299,10 +293,6 @@ public class DocumentMapper implements ToXContent {
         return rootMapper(IndexFieldMapper.class);
     }
 
-    public SizeFieldMapper SizeFieldMapper() {
-        return rootMapper(SizeFieldMapper.class);
-    }
-
     public Query typeFilter() {
         return typeMapper().fieldType().termQuery(type, null);
     }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java b/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java
index 7b572be..5c6a106 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java
@@ -20,7 +20,9 @@
 package org.elasticsearch.index.mapper;
 
 import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.ImmutableSortedMap;
 import com.google.common.collect.Maps;
+
 import org.elasticsearch.Version;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseFieldMatcher;
@@ -35,8 +37,6 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.AbstractIndexComponent;
-import org.elasticsearch.index.Index;
 import org.elasticsearch.index.analysis.AnalysisService;
 import org.elasticsearch.index.mapper.core.*;
 import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
@@ -50,6 +50,7 @@ import org.elasticsearch.index.similarity.SimilarityLookupService;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptService;
 
+import java.util.Collections;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
@@ -73,6 +74,7 @@ public class DocumentMapperParser {
 
     private volatile ImmutableMap<String, Mapper.TypeParser> typeParsers;
     private volatile ImmutableMap<String, Mapper.TypeParser> rootTypeParsers;
+    private volatile ImmutableMap<String, Mapper.TypeParser> additionalRootMappers;
 
     public DocumentMapperParser(@IndexSettings Settings indexSettings, MapperService mapperService, AnalysisService analysisService,
                                 SimilarityLookupService similarityLookupService, ScriptService scriptService) {
@@ -109,7 +111,6 @@ public class DocumentMapperParser {
         typeParsers = typeParsersBuilder.immutableMap();
 
         rootTypeParsers = new MapBuilder<String, Mapper.TypeParser>()
-                .put(SizeFieldMapper.NAME, new SizeFieldMapper.TypeParser())
                 .put(IndexFieldMapper.NAME, new IndexFieldMapper.TypeParser())
                 .put(SourceFieldMapper.NAME, new SourceFieldMapper.TypeParser())
                 .put(TypeFieldMapper.NAME, new TypeFieldMapper.TypeParser())
@@ -123,6 +124,7 @@ public class DocumentMapperParser {
                 .put(IdFieldMapper.NAME, new IdFieldMapper.TypeParser())
                 .put(FieldNamesFieldMapper.NAME, new FieldNamesFieldMapper.TypeParser())
                 .immutableMap();
+        additionalRootMappers = ImmutableSortedMap.<String, Mapper.TypeParser>of();
         indexVersionCreated = Version.indexCreated(indexSettings);
     }
 
@@ -139,6 +141,10 @@ public class DocumentMapperParser {
             rootTypeParsers = new MapBuilder<>(rootTypeParsers)
                     .put(type, typeParser)
                     .immutableMap();
+            additionalRootMappers = ImmutableSortedMap.<String, Mapper.TypeParser>naturalOrder()
+                    .putAll(additionalRootMappers)
+                    .put(type, typeParser)
+                    .build();
         }
     }
 
@@ -204,6 +210,10 @@ public class DocumentMapperParser {
         Mapper.TypeParser.ParserContext parserContext = parserContext();
         // parse RootObjectMapper
         DocumentMapper.Builder docBuilder = doc(indexSettings, (RootObjectMapper.Builder) rootObjectTypeParser.parse(type, mapping, parserContext), mapperService);
+        // Add default mapping for the plugged-in meta mappers
+        for (Map.Entry<String, Mapper.TypeParser> entry : additionalRootMappers.entrySet()) {
+            docBuilder.put((MetadataFieldMapper.Builder<?, ?>) entry.getValue().parse(entry.getKey(), Collections.<String, Object>emptyMap(), parserContext));
+        }
         Iterator<Map.Entry<String, Object>> iterator = mapping.entrySet().iterator();
         // parse DocumentMapper
         while(iterator.hasNext()) {
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java b/core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java
index 638cdce..7dc78dd 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java
@@ -33,7 +33,7 @@ import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.unit.Fuzziness;
 import org.elasticsearch.index.analysis.NamedAnalyzer;
 import org.elasticsearch.index.fielddata.FieldDataType;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.similarity.SimilarityProvider;
 
 import java.io.IOException;
@@ -425,7 +425,7 @@ public abstract class MappedFieldType extends FieldType {
     }
 
     /**
-     * Should the field query {@link #termQuery(Object, org.elasticsearch.index.query.QueryShardContext)}  be used when detecting this
+     * Should the field query {@link #termQuery(Object, org.elasticsearch.index.query.QueryParseContext)}  be used when detecting this
      * field in query string.
      */
     public boolean useTermQueryWithQueryString() {
@@ -437,11 +437,11 @@ public abstract class MappedFieldType extends FieldType {
         return new Term(names().indexName(), indexedValueForSearch(value));
     }
 
-    public Query termQuery(Object value, @Nullable QueryShardContext context) {
+    public Query termQuery(Object value, @Nullable QueryParseContext context) {
         return new TermQuery(createTerm(value));
     }
 
-    public Query termsQuery(List values, @Nullable QueryShardContext context) {
+    public Query termsQuery(List values, @Nullable QueryParseContext context) {
         BytesRef[] bytesRefs = new BytesRef[values.size()];
         for (int i = 0; i < bytesRefs.length; i++) {
             bytesRefs[i] = indexedValueForSearch(values.get(i));
@@ -460,7 +460,7 @@ public abstract class MappedFieldType extends FieldType {
         return new FuzzyQuery(createTerm(value), fuzziness.asDistance(BytesRefs.toString(value)), prefixLength, maxExpansions, transpositions);
     }
 
-    public Query prefixQuery(String value, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryShardContext context) {
+    public Query prefixQuery(String value, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryParseContext context) {
         PrefixQuery query = new PrefixQuery(createTerm(value));
         if (method != null) {
             query.setRewriteMethod(method);
@@ -468,7 +468,7 @@ public abstract class MappedFieldType extends FieldType {
         return query;
     }
 
-    public Query regexpQuery(String value, int flags, int maxDeterminizedStates, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryShardContext context) {
+    public Query regexpQuery(String value, int flags, int maxDeterminizedStates, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryParseContext context) {
         RegexpQuery query = new RegexpQuery(createTerm(value), flags, maxDeterminizedStates);
         if (method != null) {
             query.setRewriteMethod(method);
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java
index e538a00..f872207 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java
@@ -40,7 +40,7 @@ import org.elasticsearch.index.mapper.MergeMappingException;
 import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.MetadataFieldMapper;
 import org.elasticsearch.index.mapper.ParseContext;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.similarity.SimilarityLookupService;
 
 import java.io.IOException;
@@ -186,7 +186,7 @@ public class AllFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query termQuery(Object value, QueryShardContext context) {
+        public Query termQuery(Object value, QueryParseContext context) {
             return queryStringTermQuery(createTerm(value));
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java
index f6e09b2..63fa41f 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java
@@ -49,7 +49,7 @@ import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.MetadataFieldMapper;
 import org.elasticsearch.index.mapper.ParseContext;
 import org.elasticsearch.index.mapper.Uid;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 
 import java.io.IOException;
 import java.util.Collection;
@@ -167,7 +167,7 @@ public class IdFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query termQuery(Object value, @Nullable QueryShardContext context) {
+        public Query termQuery(Object value, @Nullable QueryParseContext context) {
             if (indexOptions() != IndexOptions.NONE || context == null) {
                 return super.termQuery(value, context);
             }
@@ -176,7 +176,7 @@ public class IdFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query termsQuery(List values, @Nullable QueryShardContext context) {
+        public Query termsQuery(List values, @Nullable QueryParseContext context) {
             if (indexOptions() != IndexOptions.NONE || context == null) {
                 return super.termsQuery(values, context);
             }
@@ -184,7 +184,7 @@ public class IdFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query prefixQuery(String value, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryShardContext context) {
+        public Query prefixQuery(String value, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryParseContext context) {
             if (indexOptions() != IndexOptions.NONE || context == null) {
                 return super.prefixQuery(value, method, context);
             }
@@ -201,7 +201,7 @@ public class IdFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query regexpQuery(String value, int flags, int maxDeterminizedStates, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryShardContext context) {
+        public Query regexpQuery(String value, int flags, int maxDeterminizedStates, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryParseContext context) {
             if (indexOptions() != IndexOptions.NONE || context == null) {
                 return super.regexpQuery(value, flags, maxDeterminizedStates, method, context);
             }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java
index 1b7168a..3f395a8 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java
@@ -38,7 +38,7 @@ import org.elasticsearch.index.mapper.MergeMappingException;
 import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.MetadataFieldMapper;
 import org.elasticsearch.index.mapper.ParseContext;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 
 import java.io.IOException;
 import java.util.Iterator;
@@ -157,7 +157,7 @@ public class IndexFieldMapper extends MetadataFieldMapper {
          * indices
          */
         @Override
-        public Query termQuery(Object value, @Nullable QueryShardContext context) {
+        public Query termQuery(Object value, @Nullable QueryParseContext context) {
             if (context == null) {
                 return super.termQuery(value, context);
             }
@@ -171,7 +171,7 @@ public class IndexFieldMapper extends MetadataFieldMapper {
         
 
         @Override
-        public Query termsQuery(List values, QueryShardContext context) {
+        public Query termsQuery(List values, QueryParseContext context) {
             if (context == null) {
                 return super.termsQuery(values, context);
             }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java
index 7cd4ac0..5fcd10c 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java
@@ -43,7 +43,7 @@ import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.MetadataFieldMapper;
 import org.elasticsearch.index.mapper.ParseContext;
 import org.elasticsearch.index.mapper.Uid;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -189,12 +189,12 @@ public class ParentFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query termQuery(Object value, @Nullable QueryShardContext context) {
+        public Query termQuery(Object value, @Nullable QueryParseContext context) {
             return termsQuery(Collections.singletonList(value), context);
         }
 
         @Override
-        public Query termsQuery(List values, @Nullable QueryShardContext context) {
+        public Query termsQuery(List values, @Nullable QueryParseContext context) {
             if (context == null) {
                 return super.termsQuery(values, context);
             }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java
deleted file mode 100644
index a955ae8..0000000
--- a/core/src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java
+++ /dev/null
@@ -1,183 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.mapper.internal;
-
-import org.apache.lucene.document.Field;
-import org.elasticsearch.Version;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.analysis.NumericIntegerAnalyzer;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.Mapper;
-import org.elasticsearch.index.mapper.MapperParsingException;
-import org.elasticsearch.index.mapper.MergeMappingException;
-import org.elasticsearch.index.mapper.MergeResult;
-import org.elasticsearch.index.mapper.MetadataFieldMapper;
-import org.elasticsearch.index.mapper.ParseContext;
-import org.elasticsearch.index.mapper.core.IntegerFieldMapper;
-
-import java.io.IOException;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
-import static org.elasticsearch.common.xcontent.support.XContentMapValues.nodeBooleanValue;
-import static org.elasticsearch.index.mapper.core.TypeParsers.parseStore;
-
-public class SizeFieldMapper extends MetadataFieldMapper {
-
-    public static final String NAME = "_size";
-    public static final String CONTENT_TYPE = "_size";
-
-    public static class Defaults extends IntegerFieldMapper.Defaults {
-        public static final EnabledAttributeMapper ENABLED_STATE = EnabledAttributeMapper.UNSET_DISABLED;
-
-        public static final MappedFieldType SIZE_FIELD_TYPE = IntegerFieldMapper.Defaults.FIELD_TYPE.clone();
-
-        static {
-            SIZE_FIELD_TYPE.setStored(true);
-            SIZE_FIELD_TYPE.setNumericPrecisionStep(Defaults.PRECISION_STEP_32_BIT);
-            SIZE_FIELD_TYPE.setNames(new MappedFieldType.Names(NAME));
-            SIZE_FIELD_TYPE.setIndexAnalyzer(NumericIntegerAnalyzer.buildNamedAnalyzer(Defaults.PRECISION_STEP_32_BIT));
-            SIZE_FIELD_TYPE.setSearchAnalyzer(NumericIntegerAnalyzer.buildNamedAnalyzer(Integer.MAX_VALUE));
-            SIZE_FIELD_TYPE.freeze();
-        }
-    }
-
-    public static class Builder extends MetadataFieldMapper.Builder<Builder, SizeFieldMapper> {
-
-        protected EnabledAttributeMapper enabledState = EnabledAttributeMapper.UNSET_DISABLED;
-
-        public Builder(MappedFieldType existing) {
-            super(NAME, existing == null ? Defaults.SIZE_FIELD_TYPE : existing);
-            builder = this;
-        }
-
-        public Builder enabled(EnabledAttributeMapper enabled) {
-            this.enabledState = enabled;
-            return builder;
-        }
-
-        @Override
-        public SizeFieldMapper build(BuilderContext context) {
-            setupFieldType(context);
-            fieldType.setHasDocValues(false);
-            return new SizeFieldMapper(enabledState, fieldType, context.indexSettings());
-        }
-    }
-
-    public static class TypeParser implements Mapper.TypeParser {
-        @Override
-        public Mapper.Builder parse(String name, Map<String, Object> node, ParserContext parserContext) throws MapperParsingException {
-            Builder builder = new Builder(parserContext.mapperService().fullName(NAME));
-            for (Iterator<Map.Entry<String, Object>> iterator = node.entrySet().iterator(); iterator.hasNext();) {
-                Map.Entry<String, Object> entry = iterator.next();
-                String fieldName = Strings.toUnderscoreCase(entry.getKey());
-                Object fieldNode = entry.getValue();
-                if (fieldName.equals("enabled")) {
-                    builder.enabled(nodeBooleanValue(fieldNode) ? EnabledAttributeMapper.ENABLED : EnabledAttributeMapper.DISABLED);
-                    iterator.remove();
-                } else if (fieldName.equals("store") && parserContext.indexVersionCreated().before(Version.V_2_0_0_beta1)) {
-                    builder.store(parseStore(fieldName, fieldNode.toString()));
-                    iterator.remove();
-                }
-            }
-            return builder;
-        }
-    }
-
-    private EnabledAttributeMapper enabledState;
-
-    public SizeFieldMapper(Settings indexSettings, MappedFieldType existing) {
-        this(Defaults.ENABLED_STATE, existing == null ? Defaults.SIZE_FIELD_TYPE.clone() : existing.clone(), indexSettings);
-    }
-
-    public SizeFieldMapper(EnabledAttributeMapper enabled, MappedFieldType fieldType, Settings indexSettings) {
-        super(NAME, fieldType, Defaults.SIZE_FIELD_TYPE, indexSettings);
-        this.enabledState = enabled;
-
-    }
-
-    @Override
-    protected String contentType() {
-        return NAME;
-    }
-
-    public boolean enabled() {
-        return this.enabledState.enabled;
-    }
-
-    @Override
-    public void preParse(ParseContext context) throws IOException {
-    }
-
-    @Override
-    public void postParse(ParseContext context) throws IOException {
-        // we post parse it so we get the size stored, possibly compressed (source will be preParse)
-        super.parse(context);
-    }
-
-    @Override
-    public Mapper parse(ParseContext context) throws IOException {
-        // nothing to do here, we call the parent in postParse
-        return null;
-    }
-
-    @Override
-    protected void parseCreateField(ParseContext context, List<Field> fields) throws IOException {
-        if (!enabledState.enabled) {
-            return;
-        }
-        if (context.flyweight()) {
-            return;
-        }
-        fields.add(new IntegerFieldMapper.CustomIntegerNumericField(context.source().length(), fieldType()));
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        boolean includeDefaults = params.paramAsBoolean("include_defaults", false);
-
-        // all are defaults, no need to write it at all
-        if (!includeDefaults && enabledState == Defaults.ENABLED_STATE && (indexCreatedBefore2x == false || fieldType().stored() == false)) {
-            return builder;
-        }
-        builder.startObject(contentType());
-        if (includeDefaults || enabledState != Defaults.ENABLED_STATE) {
-            builder.field("enabled", enabledState.enabled);
-        }
-        if (indexCreatedBefore2x && (includeDefaults || fieldType().stored() == true)) {
-            builder.field("store", fieldType().stored());
-        }
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public void merge(Mapper mergeWith, MergeResult mergeResult) throws MergeMappingException {
-        SizeFieldMapper sizeFieldMapperMergeWith = (SizeFieldMapper) mergeWith;
-        if (!mergeResult.simulate()) {
-            if (sizeFieldMapperMergeWith.enabledState != enabledState && !sizeFieldMapperMergeWith.enabledState.unset()) {
-                this.enabledState = sizeFieldMapperMergeWith.enabledState;
-            }
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java
index 12e40de..480d2a4 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java
@@ -43,7 +43,7 @@ import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.MetadataFieldMapper;
 import org.elasticsearch.index.mapper.ParseContext;
 import org.elasticsearch.index.mapper.Uid;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 
 import java.io.IOException;
 import java.util.List;
@@ -137,7 +137,7 @@ public class TypeFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query termQuery(Object value, @Nullable QueryShardContext context) {
+        public Query termQuery(Object value, @Nullable QueryParseContext context) {
             if (indexOptions() == IndexOptions.NONE) {
                 return new ConstantScoreQuery(new PrefixQuery(new Term(UidFieldMapper.NAME, Uid.typePrefixAsBytes(BytesRefs.toBytesRef(value)))));
             }
diff --git a/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java b/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java
index 29e83f6..3a230d1 100644
--- a/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java
+++ b/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java
@@ -41,7 +41,7 @@ import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
 import org.elasticsearch.index.percolator.stats.ShardPercolateService;
 import org.elasticsearch.index.query.IndexQueryParserService;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.settings.IndexSettings;
 import org.elasticsearch.index.shard.AbstractIndexShardComponent;
@@ -184,13 +184,12 @@ public class PercolatorQueriesRegistry extends AbstractIndexShardComponent imple
         }
     }
 
-    //norelease this method parses from xcontent to lucene query, need to re-investigate how to split context here
     private Query parseQuery(String type, XContentParser parser) {
         String[] previousTypes = null;
         if (type != null) {
-            QueryShardContext.setTypesWithPrevious(new String[]{type});
+            QueryParseContext.setTypesWithPrevious(new String[]{type});
         }
-        QueryShardContext context = queryParserService.getShardContext();
+        QueryParseContext context = queryParserService.getParseContext();
         try {
             context.reset(parser);
             // This means that fields in the query need to exist in the mapping prior to registering this query
@@ -209,10 +208,10 @@ public class PercolatorQueriesRegistry extends AbstractIndexShardComponent imple
             context.setMapUnmappedFieldAsString(mapUnmappedFieldsAsString ? true : false);
             return queryParserService.parseInnerQuery(context);
         } catch (IOException e) {
-            throw new QueryParsingException(context.parseContext(), "Failed to parse", e);
+            throw new QueryParsingException(context, "Failed to parse", e);
         } finally {
             if (type != null) {
-                QueryShardContext.setTypes(previousTypes);
+                QueryParseContext.setTypes(previousTypes);
             }
             context.reset(null);
         }
diff --git a/core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java
deleted file mode 100644
index e54b484..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java
+++ /dev/null
@@ -1,298 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.action.support.ToXContentToBytes;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.BytesRefs;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentType;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.List;
-import java.util.Objects;
-
-/**
- * Base class for all classes producing lucene queries.
- * Supports conversion to BytesReference and creation of lucene Query objects.
- */
-public abstract class AbstractQueryBuilder<QB extends AbstractQueryBuilder> extends ToXContentToBytes implements QueryBuilder<QB> {
-
-    /** Default for boost to apply to resulting Lucene query. Defaults to 1.0*/
-    public static final float DEFAULT_BOOST = 1.0f;
-
-    protected String queryName;
-    protected float boost = DEFAULT_BOOST;
-
-    protected AbstractQueryBuilder() {
-        super(XContentType.JSON);
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        doXContent(builder, params);
-        builder.endObject();
-        return builder;
-    }
-
-    protected abstract void doXContent(XContentBuilder builder, Params params) throws IOException;
-
-    protected void printBoostAndQueryName(XContentBuilder builder) throws IOException {
-        builder.field("boost", boost);
-        if (queryName != null) {
-            builder.field("_name", queryName);
-        }
-    }
-
-    @Override
-    public final Query toQuery(QueryShardContext context) throws IOException {
-        Query query = doToQuery(context);
-        if (query != null) {
-            query.setBoost(boost);
-            if (queryName != null) {
-                context.addNamedQuery(queryName, query);
-            }
-        }
-        return query;
-    }
-
-    //norelease to be made abstract once all query builders override doToQuery providing their own specific implementation.
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return context.indexQueryParserService().queryParser(getName()).parse(context);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        // default impl does not validate, subclasses should override.
-        //norelease to be possibly made abstract once all queries support validation
-        return null;
-    }
-
-    /**
-     * Returns the query name for the query.
-     */
-    @SuppressWarnings("unchecked")
-    @Override
-    public final QB queryName(String queryName) {
-        this.queryName = queryName;
-        return (QB) this;
-    }
-
-    /**
-     * Sets the query name for the query.
-     */
-    @Override
-    public final String queryName() {
-        return queryName;
-    }
-
-    /**
-     * Returns the boost for this query.
-     */
-    @Override
-    public final float boost() {
-        return this.boost;
-    }
-
-    /**
-     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
-     * weightings) have their score multiplied by the boost provided.
-     */
-    @SuppressWarnings("unchecked")
-    @Override
-    public final QB boost(float boost) {
-        this.boost = boost;
-        return (QB) this;
-    }
-
-    @Override
-    public final QB readFrom(StreamInput in) throws IOException {
-        QB queryBuilder = doReadFrom(in);
-        queryBuilder.boost = in.readFloat();
-        queryBuilder.queryName = in.readOptionalString();
-        return queryBuilder;
-    }
-
-    //norelease make this abstract once all builders implement doReadFrom themselves
-    protected QB doReadFrom(StreamInput in) throws IOException {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public final void writeTo(StreamOutput out) throws IOException {
-        doWriteTo(out);
-        out.writeFloat(boost);
-        out.writeOptionalString(queryName);
-    }
-
-    //norelease make this abstract once all builders implement doWriteTo themselves
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        throw new UnsupportedOperationException();
-    }
-
-    protected final QueryValidationException addValidationError(String validationError, QueryValidationException validationException) {
-        return QueryValidationException.addValidationError(getName(), validationError, validationException);
-    }
-
-    @Override
-    public final boolean equals(Object obj) {
-        if (this == obj) {
-            return true;
-        }
-        if (obj == null || getClass() != obj.getClass()) {
-            return false;
-        }
-        @SuppressWarnings("unchecked")
-        QB other = (QB) obj;
-        return Objects.equals(queryName, other.queryName) &&
-                Objects.equals(boost, other.boost) &&
-                doEquals(other);
-    }
-
-    /**
-     * Indicates whether some other {@link QueryBuilder} object of the same type is "equal to" this one.
-     */
-    //norelease to be made abstract once all queries are refactored
-    protected boolean doEquals(QB other) {
-        return super.equals(other);
-    }
-
-    @Override
-    public final int hashCode() {
-        return 31 * Objects.hash(getClass(), queryName, boost) + doHashCode();
-    }
-
-    //norelease to be made abstract once all queries are refactored
-    protected int doHashCode() {
-        return super.hashCode();
-    }
-
-    /**
-     * This helper method checks if the object passed in is a string, if so it
-     * converts it to a {@link BytesRef}.
-     * @param obj the input object
-     * @return the same input object or a {@link BytesRef} representation if input was of type string
-     */
-    protected static Object convertToBytesRefIfString(Object obj) {
-        if (obj instanceof String) {
-            return BytesRefs.toBytesRef(obj);
-        }
-        return obj;
-    }
-
-    /**
-     * This helper method checks if the object passed in is a {@link BytesRef}, if so it
-     * converts it to a utf8 string.
-     * @param obj the input object
-     * @return the same input object or a utf8 string if input was of type {@link BytesRef}
-     */
-    protected static Object convertToStringIfBytesRef(Object obj) {
-        if (obj instanceof BytesRef) {
-            return ((BytesRef) obj).utf8ToString();
-        }
-        return obj;
-    }
-
-    /**
-     * Helper method to convert collection of {@link QueryBuilder} instances to lucene
-     * {@link Query} instances. {@link QueryBuilder} that return <tt>null</tt> calling
-     * their {@link QueryBuilder#toQuery(QueryShardContext)} method are not added to the
-     * resulting collection.
-     *
-     * @throws IOException
-     * @throws QueryShardException
-     */
-    protected static Collection<Query> toQueries(Collection<QueryBuilder> queryBuilders, QueryShardContext context) throws QueryShardException,
-            IOException {
-        List<Query> queries = new ArrayList<>(queryBuilders.size());
-        for (QueryBuilder queryBuilder : queryBuilders) {
-            Query query = queryBuilder.toQuery(context);
-            if (query != null) {
-                queries.add(query);
-            }
-        }
-        return queries;
-    }
-
-    protected QueryValidationException validateInnerQueries(List<QueryBuilder> queryBuilders, QueryValidationException initialValidationException) {
-        QueryValidationException validationException = initialValidationException;
-        for (QueryBuilder queryBuilder : queryBuilders) {
-            validationException = validateInnerQuery(queryBuilder, validationException);
-        }
-        return validationException;
-    }
-
-    protected QueryValidationException validateInnerQuery(QueryBuilder queryBuilder, QueryValidationException initialValidationException) {
-        QueryValidationException validationException = initialValidationException;
-        if (queryBuilder != null) {
-            QueryValidationException queryValidationException = queryBuilder.validate();
-            if (queryValidationException != null) {
-                validationException = QueryValidationException.addValidationErrors(queryValidationException.validationErrors(), validationException);
-            }
-        } else {
-            validationException = addValidationError("inner query cannot be null", validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    public String getName() {
-        //default impl returns the same as writeable name, but we keep the distinction between the two just to make sure
-        return getWriteableName();
-    }
-
-    protected final void writeQueries(StreamOutput out, List<? extends QueryBuilder> queries) throws IOException {
-        out.writeVInt(queries.size());
-        for (QueryBuilder query : queries) {
-            out.writeQuery(query);
-        }
-    }
-
-    protected final List<QueryBuilder> readQueries(StreamInput in) throws IOException {
-        List<QueryBuilder> queries = new ArrayList<>();
-        int size = in.readVInt();
-        for (int i = 0; i < size; i++) {
-            queries.add(in.readQuery());
-        }
-        return queries;
-    }
-
-    protected final void writeOptionalQuery(StreamOutput out, QueryBuilder query) throws IOException {
-        if (query == null) {
-            out.writeBoolean(false);
-        } else {
-            out.writeBoolean(true);
-            out.writeQuery(query);
-        }
-    }
-
-    protected final QueryBuilder readOptionalQuery(StreamInput in) throws IOException {
-        if (in.readBoolean()) {
-            return in.readQuery();
-        }
-        return null;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java
index 05ef359..e0ecafc 100644
--- a/core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java
@@ -20,35 +20,22 @@
 package org.elasticsearch.index.query;
 
 import com.google.common.collect.Lists;
-
-import org.apache.lucene.search.BooleanClause.Occur;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.List;
-import java.util.Objects;
 
 /**
  * A filter that matches documents matching boolean combinations of other filters.
  * @deprecated Use {@link BoolQueryBuilder} instead
  */
 @Deprecated
-public class AndQueryBuilder extends AbstractQueryBuilder<AndQueryBuilder> {
-
-    public static final String NAME = "and";
+public class AndQueryBuilder extends QueryBuilder {
 
-    private final ArrayList<QueryBuilder> filters = Lists.newArrayList();
+    private ArrayList<QueryBuilder> filters = Lists.newArrayList();
 
-    static final AndQueryBuilder PROTOTYPE = new AndQueryBuilder();
+    private String queryName;
 
-    /**
-     * @param filters nested filters, no <tt>null</tt> values are allowed
-     */
     public AndQueryBuilder(QueryBuilder... filters) {
         for (QueryBuilder filter : filters) {
             this.filters.add(filter);
@@ -57,7 +44,6 @@ public class AndQueryBuilder extends AbstractQueryBuilder<AndQueryBuilder> {
 
     /**
      * Adds a filter to the list of filters to "and".
-     * @param filterBuilder nested filter, no <tt>null</tt> value allowed
      */
     public AndQueryBuilder add(QueryBuilder filterBuilder) {
         filters.add(filterBuilder);
@@ -65,79 +51,24 @@ public class AndQueryBuilder extends AbstractQueryBuilder<AndQueryBuilder> {
     }
 
     /**
-     * @return the list of filters added to "and".
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
      */
-    public List<QueryBuilder> filters() {
-        return this.filters;
+    public AndQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(AndQueryParser.NAME);
         builder.startArray("filters");
         for (QueryBuilder filter : filters) {
             filter.toXContent(builder, params);
         }
         builder.endArray();
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        if (filters.isEmpty()) {
-            // no filters provided, this should be ignored upstream
-            return null;
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-
-        BooleanQuery query = new BooleanQuery();
-        for (QueryBuilder f : filters) {
-            Query innerQuery = f.toQuery(context);
-            // ignore queries that are null
-            if (innerQuery != null) {
-                query.add(innerQuery, Occur.MUST);
-            }
-        }
-        if (query.clauses().isEmpty()) {
-            // no inner lucene query exists, ignore upstream
-            return null;
-        }
-        return query;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        return validateInnerQueries(filters, null);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(filters);
-    }
-
-    @Override
-    protected boolean doEquals(AndQueryBuilder other) {
-        return Objects.equals(filters, other.filters);
-    }
-
-    @Override
-    protected AndQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        AndQueryBuilder andQueryBuilder = new AndQueryBuilder();
-        List<QueryBuilder> queryBuilders = readQueries(in);
-        for (QueryBuilder queryBuilder : queryBuilders) {
-            andQueryBuilder.add(queryBuilder);
-        }
-        return andQueryBuilder;
-
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        writeQueries(out, filters);
+        builder.endObject();
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/AndQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/AndQueryParser.java
index 4f5c24d..8c1969d 100644
--- a/core/src/main/java/org/elasticsearch/index/query/AndQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/AndQueryParser.java
@@ -19,6 +19,9 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
 
@@ -27,8 +30,13 @@ import java.util.ArrayList;
 
 import static com.google.common.collect.Lists.newArrayList;
 
+/**
+ *
+ */
 @Deprecated
-public class AndQueryParser extends BaseQueryParser {
+public class AndQueryParser implements QueryParser {
+
+    public static final String NAME = "and";
 
     @Inject
     public AndQueryParser() {
@@ -36,25 +44,26 @@ public class AndQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{AndQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        final ArrayList<QueryBuilder> queries = newArrayList();
+        ArrayList<Query> queries = newArrayList();
         boolean queriesFound = false;
 
         String queryName = null;
         String currentFieldName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         XContentParser.Token token = parser.currentToken();
         if (token == XContentParser.Token.START_ARRAY) {
             while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                 queriesFound = true;
-                QueryBuilder filter = parseContext.parseInnerFilterToQueryBuilder();
-                queries.add(filter);
+                Query filter = parseContext.parseInnerFilter();
+                if (filter != null) {
+                    queries.add(filter);
+                }
             }
         } else {
             while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -66,21 +75,23 @@ public class AndQueryParser extends BaseQueryParser {
                     if ("filters".equals(currentFieldName)) {
                         queriesFound = true;
                         while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                            QueryBuilder filter = parseContext.parseInnerFilterToQueryBuilder();
-                            queries.add(filter);
+                            Query filter = parseContext.parseInnerFilter();
+                            if (filter != null) {
+                                queries.add(filter);
+                            }
                         }
                     } else {
                         queriesFound = true;
                         while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                            QueryBuilder filter = parseContext.parseInnerFilterToQueryBuilder();
-                            queries.add(filter);
+                            Query filter = parseContext.parseInnerFilter();
+                            if (filter != null) {
+                                queries.add(filter);
+                            }
                         }
                     }
                 } else if (token.isValue()) {
                     if ("_name".equals(currentFieldName)) {
                         queryName = parser.text();
-                    } else if ("boost".equals(currentFieldName)) {
-                        boost = parser.floatValue();
                     } else {
                         throw new QueryParsingException(parseContext, "[and] query does not support [" + currentFieldName + "]");
                     }
@@ -92,17 +103,18 @@ public class AndQueryParser extends BaseQueryParser {
             throw new QueryParsingException(parseContext, "[and] query requires 'filters' to be set on it'");
         }
 
-        AndQueryBuilder andQuery = new AndQueryBuilder();
-        for (QueryBuilder query : queries) {
-            andQuery.add(query);
+        if (queries.isEmpty()) {
+            // no filters provided, this should be ignored upstream
+            return null;
         }
-        andQuery.queryName(queryName);
-        andQuery.boost(boost);
-        return andQuery;
-    }
 
-    @Override
-    public AndQueryBuilder getBuilderPrototype() {
-        return AndQueryBuilder.PROTOTYPE;
+        BooleanQuery query = new BooleanQuery();
+        for (Query f : queries) {
+            query.add(f, Occur.MUST);
+        }
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/BaseQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/BaseQueryParser.java
deleted file mode 100644
index c3f0440..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/BaseQueryParser.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-
-import java.io.IOException;
-
-/**
- * Class used during the query parsers refactoring. Will be removed once we can parse search requests on the coordinating node.
- * All query parsers that have a refactored "fromXContent" method can be changed to extend this instead of {@link BaseQueryParserTemp}.
- * Keeps old {@link QueryParser#parse(QueryShardContext)} method as a stub delegating to
- * {@link QueryParser#fromXContent(QueryShardContext)} and {@link QueryBuilder#toQuery(QueryShardContext)}}
- */
-//norelease needs to be removed once we parse search requests on the coordinating node, as the parse method is not needed anymore at that point.
-public abstract class BaseQueryParser implements QueryParser {
-
-    @Override
-    public final Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        return fromXContent(context.parseContext()).toQuery(context);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/BaseQueryParserTemp.java b/core/src/main/java/org/elasticsearch/index/query/BaseQueryParserTemp.java
deleted file mode 100644
index 4dc3eae..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/BaseQueryParserTemp.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-
-import java.io.IOException;
-
-/**
- * This class with method impl is an intermediate step in the query parsers refactoring.
- * Provides a fromXContent default implementation for query parsers that don't have yet a
- * specific fromXContent implementation that returns a QueryBuilder.
- */
-//norelease to be removed once all queries are moved over to extend BaseQueryParser
-public abstract class BaseQueryParserTemp implements QueryParser {
-
-    @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
-        Query query = parse(parseContext.shardContext());
-        return new QueryWrappingQueryBuilder(query);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java
deleted file mode 100644
index 6444184..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java
+++ /dev/null
@@ -1,171 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-
-import java.io.IOException;
-import java.util.Objects;
-
-public abstract class BaseTermQueryBuilder<QB extends BaseTermQueryBuilder<QB>> extends AbstractQueryBuilder<QB> {
-
-    /** Name of field to match against. */
-    protected final String fieldName;
-
-    /** Value to find matches for. */
-    protected final Object value;
-
-    /**
-     * Constructs a new base term query.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, String value) {
-        this(fieldName, (Object) value);
-    }
-
-    /**
-     * Constructs a new base term query.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, int value) {
-        this(fieldName, (Object) value);
-    }
-
-    /**
-     * Constructs a new base term query.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, long value) {
-        this(fieldName, (Object) value);
-    }
-
-    /**
-     * Constructs a new base term query.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, float value) {
-        this(fieldName, (Object) value);
-    }
-
-    /**
-     * Constructs a new base term query.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, double value) {
-        this(fieldName, (Object) value);
-    }
-
-    /**
-     * Constructs a new base term query.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, boolean value) {
-        this(fieldName, (Object) value);
-    }
-
-    /**
-     * Constructs a new base term query.
-     * In case value is assigned to a string, we internally convert it to a {@link BytesRef}
-     * because in {@link TermQueryParser} and {@link SpanTermQueryParser} string values are parsed to {@link BytesRef}
-     * and we want internal representation of query to be equal regardless of whether it was created from XContent or via Java API.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, Object value) {
-        this.fieldName = fieldName;
-        this.value = convertToBytesRefIfString(value);
-    }
-
-    /** Returns the field name used in this query. */
-    public String fieldName() {
-        return this.fieldName;
-    }
-
-    /**
-     *  Returns the value used in this query.
-     *  If necessary, converts internal {@link BytesRef} representation back to string.
-     */
-    public Object value() {
-        return convertToStringIfBytesRef(this.value);
-    }
-
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(getName());
-        builder.startObject(fieldName);
-        builder.field("value", convertToStringIfBytesRef(this.value));
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
-    }
-
-    /** Returns a {@link QueryValidationException} if fieldName is null or empty, or if value is null. */
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (fieldName == null || fieldName.isEmpty()) {
-            validationException = addValidationError("field name cannot be null or empty.", validationException);
-        }
-        if (value == null) {
-            validationException = addValidationError("value cannot be null.", validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    protected final int doHashCode() {
-        return Objects.hash(fieldName, value);
-    }
-
-    @Override
-    protected final boolean doEquals(BaseTermQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-               Objects.equals(value, other.value);
-    }
-
-    @Override
-    protected final QB doReadFrom(StreamInput in) throws IOException {
-        return createBuilder(in.readString(), in.readGenericValue());
-    }
-
-    protected abstract QB createBuilder(String fieldName, Object value);
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-        out.writeGenericValue(value);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java
index 9ce5a39..c377667 100644
--- a/core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java
@@ -19,35 +19,17 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanClause.Occur;
-import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
-import java.util.Objects;
-
-import static org.elasticsearch.common.lucene.search.Queries.fixNegativeQueryIfNeeded;
 
 /**
  * A Query that matches documents matching boolean combinations of other queries.
  */
-public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
-
-    public static final String NAME = "bool";
-
-    public static final boolean ADJUST_PURE_NEGATIVE_DEFAULT = true;
-
-    public static final boolean DISABLE_COORD_DEFAULT = false;
-
-    static final BoolQueryBuilder PROTOTYPE = new BoolQueryBuilder();
+public class BoolQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<BoolQueryBuilder> {
 
     private final List<QueryBuilder> mustClauses = new ArrayList<>();
 
@@ -57,15 +39,19 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
 
     private final List<QueryBuilder> shouldClauses = new ArrayList<>();
 
-    private boolean disableCoord = DISABLE_COORD_DEFAULT;
+    private float boost = -1;
 
-    private boolean adjustPureNegative = ADJUST_PURE_NEGATIVE_DEFAULT;
+    private Boolean disableCoord;
 
     private String minimumShouldMatch;
+    
+    private Boolean adjustPureNegative;
+
+    private String queryName;
 
     /**
      * Adds a query that <b>must</b> appear in the matching documents and will
-     * contribute to scoring. No <tt>null</tt> value allowed.
+     * contribute to scoring.
      */
     public BoolQueryBuilder must(QueryBuilder queryBuilder) {
         mustClauses.add(queryBuilder);
@@ -73,15 +59,8 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
     }
 
     /**
-     * Gets the queries that <b>must</b> appear in the matching documents.
-     */
-    public List<QueryBuilder> must() {
-        return this.mustClauses;
-    }
-
-    /**
      * Adds a query that <b>must</b> appear in the matching documents but will
-     * not contribute to scoring. No <tt>null</tt> value allowed.
+     * not contribute to scoring.
      */
     public BoolQueryBuilder filter(QueryBuilder queryBuilder) {
         filterClauses.add(queryBuilder);
@@ -89,15 +68,8 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
     }
 
     /**
-     * Gets the queries that <b>must</b> appear in the matching documents but don't conntribute to scoring
-     */
-    public List<QueryBuilder> filter() {
-        return this.filterClauses;
-    }
-
-    /**
-     * Adds a query that <b>must not</b> appear in the matching documents.
-     * No <tt>null</tt> value allowed.
+     * Adds a query that <b>must not</b> appear in the matching documents and
+     * will not contribute to scoring.
      */
     public BoolQueryBuilder mustNot(QueryBuilder queryBuilder) {
         mustNotClauses.add(queryBuilder);
@@ -105,16 +77,9 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
     }
 
     /**
-     * Gets the queries that <b>must not</b> appear in the matching documents.
-     */
-    public List<QueryBuilder> mustNot() {
-        return this.mustNotClauses;
-    }
-
-    /**
-     * Adds a clause that <i>should</i> be matched by the returned documents. For a boolean query with no
+     * Adds a query that <i>should</i> appear in the matching documents. For a boolean query with no
      * <tt>MUST</tt> clauses one or more <code>SHOULD</code> clauses must match a document
-     * for the BooleanQuery to match. No <tt>null</tt> value allowed.
+     * for the BooleanQuery to match.
      *
      * @see #minimumNumberShouldMatch(int)
      */
@@ -124,13 +89,13 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
     }
 
     /**
-     * Gets the list of clauses that <b>should</b> be matched by the returned documents.
-     *
-     * @see #should(QueryBuilder)
-     *  @see #minimumNumberShouldMatch(int)
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
      */
-    public List<QueryBuilder> should() {
-        return this.shouldClauses;
+    @Override
+    public BoolQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
@@ -142,13 +107,6 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
     }
 
     /**
-     * @return whether the <tt>Similarity#coord(int,int)</tt> in scoring are disabled. Defaults to <tt>false</tt>.
-     */
-    public boolean disableCoord() {
-        return this.disableCoord;
-    }
-
-    /**
      * Specifies a minimum number of the optional (should) boolean clauses which must be satisfied.
      * <p/>
      * <p>By default no optional clauses are necessary for a match
@@ -166,23 +124,6 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
         return this;
     }
 
-
-    /**
-     * Specifies a minimum number of the optional (should) boolean clauses which must be satisfied.
-     * @see BoolQueryBuilder#minimumNumberShouldMatch(int)
-     */
-    public BoolQueryBuilder minimumNumberShouldMatch(String minimumNumberShouldMatch) {
-        this.minimumShouldMatch = minimumNumberShouldMatch;
-        return this;
-    }
-
-    /**
-     * @return the string representation of the minimumShouldMatch settings for this query
-     */
-    public String minimumNumberShouldMatch() {
-        return this.minimumShouldMatch;
-    }
-
     /**
      * Sets the minimum should match using the special syntax (for example, supporting percentage).
      */
@@ -198,7 +139,7 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
     public boolean hasClauses() {
         return !(mustClauses.isEmpty() && shouldClauses.isEmpty() && mustNotClauses.isEmpty() && filterClauses.isEmpty());
     }
-
+    
     /**
      * If a boolean query contains only negative ("must not") clauses should the
      * BooleanQuery be enhanced with a {@link MatchAllDocsQuery} in order to act
@@ -210,29 +151,39 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
     }
 
     /**
-     * @return the setting for the adjust_pure_negative setting in this query
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public boolean adjustPureNegative() {
-        return this.adjustPureNegative;
+    public BoolQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject("bool");
         doXArrayContent("must", mustClauses, builder, params);
         doXArrayContent("filter", filterClauses, builder, params);
         doXArrayContent("must_not", mustNotClauses, builder, params);
         doXArrayContent("should", shouldClauses, builder, params);
-        builder.field("disable_coord", disableCoord);
-        builder.field("adjust_pure_negative", adjustPureNegative);
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
+        if (disableCoord != null) {
+            builder.field("disable_coord", disableCoord);
+        }
         if (minimumShouldMatch != null) {
             builder.field("minimum_should_match", minimumShouldMatch);
         }
-        printBoostAndQueryName(builder);
+        if (adjustPureNegative != null) {
+            builder.field("adjust_pure_negative", adjustPureNegative);
+        }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         builder.endObject();
     }
 
-    private static void doXArrayContent(String field, List<QueryBuilder> clauses, XContentBuilder builder, Params params) throws IOException {
+    private void doXArrayContent(String field, List<QueryBuilder> clauses, XContentBuilder builder, Params params) throws IOException {
         if (clauses.isEmpty()) {
             return;
         }
@@ -248,89 +199,4 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
         }
     }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        BooleanQuery booleanQuery = new BooleanQuery(disableCoord);
-        addBooleanClauses(context, booleanQuery, mustClauses, BooleanClause.Occur.MUST);
-        addBooleanClauses(context, booleanQuery, mustNotClauses, BooleanClause.Occur.MUST_NOT);
-        addBooleanClauses(context, booleanQuery, shouldClauses, BooleanClause.Occur.SHOULD);
-        addBooleanClauses(context, booleanQuery, filterClauses, BooleanClause.Occur.FILTER);
-
-        if (booleanQuery.clauses().isEmpty()) {
-            return new MatchAllDocsQuery();
-        }
-
-        Queries.applyMinimumShouldMatch(booleanQuery, minimumShouldMatch);
-        return adjustPureNegative ? fixNegativeQueryIfNeeded(booleanQuery) : booleanQuery;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        validationException = validateInnerQueries(mustClauses, validationException);
-        validationException = validateInnerQueries(shouldClauses, validationException);
-        validationException = validateInnerQueries(mustNotClauses, validationException);
-        validationException = validateInnerQueries(filterClauses, validationException);
-        return validationException;
-    }
-
-    private static void addBooleanClauses(QueryShardContext context, BooleanQuery booleanQuery, List<QueryBuilder> clauses, Occur occurs) throws IOException {
-        for (QueryBuilder query : clauses) {
-            Query luceneQuery = query.toQuery(context);
-            if (luceneQuery != null) {
-                booleanQuery.add(new BooleanClause(luceneQuery, occurs));
-            }
-        }
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(adjustPureNegative, disableCoord,
-                minimumShouldMatch, mustClauses, shouldClauses, mustNotClauses, filterClauses);
-    }
-
-    @Override
-    protected boolean doEquals(BoolQueryBuilder other) {
-        return Objects.equals(adjustPureNegative, other.adjustPureNegative) &&
-                Objects.equals(disableCoord, other.disableCoord) &&
-                Objects.equals(minimumShouldMatch, other.minimumShouldMatch) &&
-                Objects.equals(mustClauses, other.mustClauses) &&
-                Objects.equals(shouldClauses, other.shouldClauses) &&
-                Objects.equals(mustNotClauses, other.mustNotClauses) &&
-                Objects.equals(filterClauses, other.filterClauses);
-    }
-
-    @Override
-    protected BoolQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        BoolQueryBuilder boolQueryBuilder = new BoolQueryBuilder();
-        List<QueryBuilder> queryBuilders = readQueries(in);
-        boolQueryBuilder.mustClauses.addAll(queryBuilders);
-        queryBuilders = readQueries(in);
-        boolQueryBuilder.mustNotClauses.addAll(queryBuilders);
-        queryBuilders = readQueries(in);
-        boolQueryBuilder.shouldClauses.addAll(queryBuilders);
-        queryBuilders = readQueries(in);
-        boolQueryBuilder.filterClauses.addAll(queryBuilders);
-        boolQueryBuilder.adjustPureNegative = in.readBoolean();
-        boolQueryBuilder.disableCoord = in.readBoolean();
-        boolQueryBuilder.minimumShouldMatch = in.readOptionalString();
-        return boolQueryBuilder;
-
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        writeQueries(out, mustClauses);
-        writeQueries(out, mustNotClauses);
-        writeQueries(out, shouldClauses);
-        writeQueries(out, filterClauses);
-        out.writeBoolean(adjustPureNegative);
-        out.writeBoolean(disableCoord);
-        out.writeOptionalString(minimumShouldMatch);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java
index ad919de..6fa7faa 100644
--- a/core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java
@@ -19,8 +19,12 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentParser;
 
@@ -28,11 +32,14 @@ import java.io.IOException;
 import java.util.List;
 
 import static com.google.common.collect.Lists.newArrayList;
+import static org.elasticsearch.common.lucene.search.Queries.fixNegativeQueryIfNeeded;
 
 /**
- * Parser for the {@link BoolQueryBuilder}
+ *
  */
-public class BoolQueryParser extends BaseQueryParser {
+public class BoolQueryParser implements QueryParser {
+
+    public static final String NAME = "bool";
 
     @Inject
     public BoolQueryParser(Settings settings) {
@@ -41,27 +48,23 @@ public class BoolQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{BoolQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        boolean disableCoord = BoolQueryBuilder.DISABLE_COORD_DEFAULT;
-        boolean adjustPureNegative = BoolQueryBuilder.ADJUST_PURE_NEGATIVE_DEFAULT;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        boolean disableCoord = false;
+        float boost = 1.0f;
         String minimumShouldMatch = null;
 
-        final List<QueryBuilder> mustClauses = newArrayList();
-        final List<QueryBuilder> mustNotClauses = newArrayList();
-        final List<QueryBuilder> shouldClauses = newArrayList();
-        final List<QueryBuilder> filterClauses = newArrayList();
+        List<BooleanClause> clauses = newArrayList();
+        boolean adjustPureNegative = true;
         String queryName = null;
-
+        
         String currentFieldName = null;
         XContentParser.Token token;
-        QueryBuilder query;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
@@ -70,27 +73,32 @@ public class BoolQueryParser extends BaseQueryParser {
             } else if (token == XContentParser.Token.START_OBJECT) {
                 switch (currentFieldName) {
                 case "must":
-                    query = parseContext.parseInnerQueryBuilder();
-                    mustClauses.add(query);
+                    Query query = parseContext.parseInnerQuery();
+                    if (query != null) {
+                        clauses.add(new BooleanClause(query, BooleanClause.Occur.MUST));
+                    }
                     break;
                 case "should":
-                    query = parseContext.parseInnerQueryBuilder();
-                    shouldClauses.add(query);
-                    // EmptyQueryBuilder does not add lucene query later, skip setting minuminShouldMatch
-                    if (query != EmptyQueryBuilder.PROTOTYPE) {
+                    query = parseContext.parseInnerQuery();
+                    if (query != null) {
+                        clauses.add(new BooleanClause(query, BooleanClause.Occur.SHOULD));
                         if (parseContext.isFilter() && minimumShouldMatch == null) {
                             minimumShouldMatch = "1";
                         }
                     }
                     break;
                 case "filter":
-                    query = parseContext.parseInnerFilterToQueryBuilder();
-                    filterClauses.add(query);
+                    query = parseContext.parseInnerFilter();
+                    if (query != null) {
+                        clauses.add(new BooleanClause(query, BooleanClause.Occur.FILTER));
+                    }
                     break;
                 case "must_not":
                 case "mustNot":
-                    query = parseContext.parseInnerFilterToQueryBuilder();
-                    mustNotClauses.add(query);
+                    query = parseContext.parseInnerFilter();
+                    if (query != null) {
+                        clauses.add(new BooleanClause(query, BooleanClause.Occur.MUST_NOT));
+                    }
                     break;
                 default:
                     throw new QueryParsingException(parseContext, "[bool] query does not support [" + currentFieldName + "]");
@@ -99,27 +107,32 @@ public class BoolQueryParser extends BaseQueryParser {
                 while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                     switch (currentFieldName) {
                     case "must":
-                        query = parseContext.parseInnerQueryBuilder();
-                        mustClauses.add(query);
+                        Query query = parseContext.parseInnerQuery();
+                        if (query != null) {
+                            clauses.add(new BooleanClause(query, BooleanClause.Occur.MUST));
+                        }
                         break;
                     case "should":
-                        query = parseContext.parseInnerQueryBuilder();
-                        shouldClauses.add(query);
-                        // EmptyQueryBuilder does not add lucene query later, skip setting minuminShouldMatch
-                        if (query != EmptyQueryBuilder.PROTOTYPE) {
+                        query = parseContext.parseInnerQuery();
+                        if (query != null) {
+                            clauses.add(new BooleanClause(query, BooleanClause.Occur.SHOULD));
                             if (parseContext.isFilter() && minimumShouldMatch == null) {
                                 minimumShouldMatch = "1";
                             }
                         }
                         break;
                     case "filter":
-                        query = parseContext.parseInnerFilterToQueryBuilder();
-                        filterClauses.add(query);
+                        query = parseContext.parseInnerFilter();
+                        if (query != null) {
+                            clauses.add(new BooleanClause(query, BooleanClause.Occur.FILTER));
+                        }
                         break;
                     case "must_not":
                     case "mustNot":
-                        query = parseContext.parseInnerFilterToQueryBuilder();
-                        mustNotClauses.add(query);
+                        query = parseContext.parseInnerFilter();
+                        if (query != null) {
+                            clauses.add(new BooleanClause(query, BooleanClause.Occur.MUST_NOT));
+                        }
                         break;
                     default:
                         throw new QueryParsingException(parseContext, "bool query does not support [" + currentFieldName + "]");
@@ -143,29 +156,21 @@ public class BoolQueryParser extends BaseQueryParser {
                 }
             }
         }
-        BoolQueryBuilder boolQuery = new BoolQueryBuilder();
-        for (QueryBuilder queryBuilder : mustClauses) {
-            boolQuery.must(queryBuilder);
-        }
-        for (QueryBuilder queryBuilder : mustNotClauses) {
-            boolQuery.mustNot(queryBuilder);
+
+        if (clauses.isEmpty()) {
+            return new MatchAllDocsQuery();
         }
-        for (QueryBuilder queryBuilder : shouldClauses) {
-            boolQuery.should(queryBuilder);
+
+        BooleanQuery booleanQuery = new BooleanQuery(disableCoord);
+        for (BooleanClause clause : clauses) {
+            booleanQuery.add(clause);
         }
-        for (QueryBuilder queryBuilder : filterClauses) {
-            boolQuery.filter(queryBuilder);
+        booleanQuery.setBoost(boost);
+        Queries.applyMinimumShouldMatch(booleanQuery, minimumShouldMatch);
+        Query query = adjustPureNegative ? fixNegativeQueryIfNeeded(booleanQuery) : booleanQuery;
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
         }
-        boolQuery.boost(boost);
-        boolQuery.disableCoord(disableCoord);
-        boolQuery.adjustPureNegative(adjustPureNegative);
-        boolQuery.minimumNumberShouldMatch(minimumShouldMatch);
-        boolQuery.queryName(queryName);
-        return boolQuery;
-    }
-
-    @Override
-    public BoolQueryBuilder getBuilderPrototype() {
-        return BoolQueryBuilder.PROTOTYPE;
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/BoostableQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/BoostableQueryBuilder.java
new file mode 100644
index 0000000..31572ce
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/index/query/BoostableQueryBuilder.java
@@ -0,0 +1,33 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.query;
+
+/**
+ * Query builder which allow setting some boost
+ */
+public interface BoostableQueryBuilder<B extends BoostableQueryBuilder<B>> {
+
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
+    B boost(float boost);
+
+}
diff --git a/core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java
index a9cab5f..9d67469 100644
--- a/core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java
@@ -19,14 +19,9 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.queries.BoostingQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * The BoostingQuery class can be used to effectively demote results that match a given query.
@@ -40,132 +35,63 @@ import java.util.Objects;
  * multiplied by the supplied "boost" parameter, so this should be less than 1 to achieve a
  * demoting effect
  */
-public class BoostingQueryBuilder extends AbstractQueryBuilder<BoostingQueryBuilder> {
+public class BoostingQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<BoostingQueryBuilder> {
 
-    public static final String NAME = "boosting";
+    private QueryBuilder positiveQuery;
 
-    private final QueryBuilder positiveQuery;
-
-    private final QueryBuilder negativeQuery;
+    private QueryBuilder negativeQuery;
 
     private float negativeBoost = -1;
 
-    static final BoostingQueryBuilder PROTOTYPE = new BoostingQueryBuilder(null, null);
+    private float boost = -1;
+
+    public BoostingQueryBuilder() {
 
-    /**
-     * Create a new {@link BoostingQueryBuilder}
-     *
-     * @param positiveQuery the positive query for this boosting query.
-     * @param negativeQuery the negative query for this boosting query.
-     */
-    public BoostingQueryBuilder(QueryBuilder positiveQuery, QueryBuilder negativeQuery) {
-        this.positiveQuery = positiveQuery;
-        this.negativeQuery = negativeQuery;
     }
 
-    /**
-     * Get the positive query for this boosting query.
-     */
-    public QueryBuilder positive() {
-        return this.positiveQuery;
+    public BoostingQueryBuilder positive(QueryBuilder positiveQuery) {
+        this.positiveQuery = positiveQuery;
+        return this;
     }
 
-    /**
-     * Get the negative query for this boosting query.
-     */
-    public QueryBuilder negative() {
-        return this.negativeQuery;
+    public BoostingQueryBuilder negative(QueryBuilder negativeQuery) {
+        this.negativeQuery = negativeQuery;
+        return this;
     }
 
-    /**
-     * Set the negative boost factor.
-     */
     public BoostingQueryBuilder negativeBoost(float negativeBoost) {
         this.negativeBoost = negativeBoost;
         return this;
     }
 
-    /**
-     * Get the negative boost factor.
-     */
-    public float negativeBoost() {
-        return this.negativeBoost;
-    }
-
     @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("positive");
-        positiveQuery.toXContent(builder, params);
-        builder.field("negative");
-        negativeQuery.toXContent(builder, params);
-        builder.field("negative_boost", negativeBoost);
-        printBoostAndQueryName(builder);
-        builder.endObject();
+    public BoostingQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (negativeBoost < 0) {
-            validationException = addValidationError("query requires negativeBoost to be set to positive value", validationException);
+    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
+        if (positiveQuery == null) {
+            throw new IllegalArgumentException("boosting query requires positive query to be set");
         }
         if (negativeQuery == null) {
-            validationException = addValidationError("inner clause [negative] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(negativeQuery, validationException);
+            throw new IllegalArgumentException("boosting query requires negative query to be set");
         }
-        if (positiveQuery == null) {
-            validationException = addValidationError("inner clause [positive] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(positiveQuery, validationException);
+        if (negativeBoost == -1) {
+            throw new IllegalArgumentException("boosting query requires negativeBoost to be set");
         }
-        return validationException;
-    }
+        builder.startObject(BoostingQueryParser.NAME);
+        builder.field("positive");
+        positiveQuery.toXContent(builder, params);
+        builder.field("negative");
+        negativeQuery.toXContent(builder, params);
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
+        builder.field("negative_boost", negativeBoost);
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query positive = positiveQuery.toQuery(context);
-        Query negative = negativeQuery.toQuery(context);
-        // make upstream queries ignore this query by returning `null`
-        // if either inner query builder returns null
-        if (positive == null || negative == null) {
-            return null;
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-
-        return new BoostingQuery(positive, negative, negativeBoost);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(negativeBoost, positiveQuery, negativeQuery);
-    }
-
-    @Override
-    protected boolean doEquals(BoostingQueryBuilder other) {
-        return Objects.equals(negativeBoost, other.negativeBoost) &&
-                Objects.equals(positiveQuery, other.positiveQuery) &&
-                Objects.equals(negativeQuery, other.negativeQuery);
-    }
-
-    @Override
-    protected BoostingQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryBuilder positiveQuery = in.readQuery();
-        QueryBuilder negativeQuery = in.readQuery();
-        BoostingQueryBuilder boostingQuery = new BoostingQueryBuilder(positiveQuery, negativeQuery);
-        boostingQuery.negativeBoost = in.readFloat();
-        return boostingQuery;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(positiveQuery);
-        out.writeQuery(negativeQuery);
-        out.writeFloat(negativeBoost);
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java
index 9cb5d20..c160b2f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.queries.BoostingQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
 
@@ -27,7 +29,9 @@ import java.io.IOException;
 /**
  *
  */
-public class BoostingQueryParser extends BaseQueryParser {
+public class BoostingQueryParser implements QueryParser {
+
+    public static final String NAME = "boosting";
 
     @Inject
     public BoostingQueryParser() {
@@ -35,20 +39,19 @@ public class BoostingQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{BoostingQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        QueryBuilder positiveQuery = null;
+        Query positiveQuery = null;
         boolean positiveQueryFound = false;
-        QueryBuilder negativeQuery = null;
+        Query negativeQuery = null;
         boolean negativeQueryFound = false;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = -1;
         float negativeBoost = -1;
-        String queryName = null;
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -57,10 +60,10 @@ public class BoostingQueryParser extends BaseQueryParser {
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("positive".equals(currentFieldName)) {
-                    positiveQuery = parseContext.parseInnerQueryBuilder();
+                    positiveQuery = parseContext.parseInnerQuery();
                     positiveQueryFound = true;
                 } else if ("negative".equals(currentFieldName)) {
-                    negativeQuery = parseContext.parseInnerQueryBuilder();
+                    negativeQuery = parseContext.parseInnerQuery();
                     negativeQueryFound = true;
                 } else {
                     throw new QueryParsingException(parseContext, "[boosting] query does not support [" + currentFieldName + "]");
@@ -68,8 +71,6 @@ public class BoostingQueryParser extends BaseQueryParser {
             } else if (token.isValue()) {
                 if ("negative_boost".equals(currentFieldName) || "negativeBoost".equals(currentFieldName)) {
                     negativeBoost = parser.floatValue();
-                } else if ("_name".equals(currentFieldName)) {
-                    queryName = parser.text();
                 } else if ("boost".equals(currentFieldName)) {
                     boost = parser.floatValue();
                 } else {
@@ -78,25 +79,25 @@ public class BoostingQueryParser extends BaseQueryParser {
             }
         }
 
-        if (!positiveQueryFound) {
+        if (positiveQuery == null && !positiveQueryFound) {
             throw new QueryParsingException(parseContext, "[boosting] query requires 'positive' query to be set'");
         }
-        if (!negativeQueryFound) {
+        if (negativeQuery == null && !negativeQueryFound) {
             throw new QueryParsingException(parseContext, "[boosting] query requires 'negative' query to be set'");
         }
-        if (negativeBoost < 0) {
-            throw new QueryParsingException(parseContext, "[boosting] query requires 'negative_boost' to be set to be a positive value'");
+        if (negativeBoost == -1) {
+            throw new QueryParsingException(parseContext, "[boosting] query requires 'negative_boost' to be set'");
         }
 
-        BoostingQueryBuilder boostingQuery = new BoostingQueryBuilder(positiveQuery, negativeQuery);
-        boostingQuery.negativeBoost(negativeBoost);
-        boostingQuery.boost(boost);
-        boostingQuery.queryName(queryName);
-        return boostingQuery;
-    }
+        // parsers returned null
+        if (positiveQuery == null || negativeQuery == null) {
+            return null;
+        }
 
-    @Override
-    public BoostingQueryBuilder getBuilderPrototype() {
-        return BoostingQueryBuilder.PROTOTYPE;
+        BoostingQuery boostingQuery = new BoostingQuery(positiveQuery, negativeQuery, negativeBoost);
+        if (boost != -1) {
+            boostingQuery.setBoost(boost);
+        }
+        return boostingQuery;
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java
index f907877..ae9c10d 100644
--- a/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java
@@ -19,31 +19,18 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.queries.ExtendedCommonTermsQuery;
-import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
 import org.apache.lucene.search.similarities.Similarity;
-import org.apache.lucene.util.BytesRefBuilder;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * CommonTermsQuery query is a query that executes high-frequency terms in a
  * optional sub-query to prevent slow queries due to "common" terms like
- * stopwords. This query basically builds 2 queries off the
- * {@link org.apache.lucene.queries.CommonTermsQuery#add(Term) added} terms
- * where low-frequency terms are added to a required boolean clause
+ * stopwords. This query basically builds 2 queries off the {@link #add(Term)
+ * added} terms where low-frequency terms are added to a required boolean clause
  * and high-frequency terms are added to an optional boolean clause. The
  * optional clause is only executed if the required "low-frequency' clause
  * matches. Scores produced by this query will be slightly different to plain
@@ -55,52 +42,46 @@ import java.util.Objects;
  * execution times significantly if applicable.
  * <p>
  */
-public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQueryBuilder> {
+public class CommonTermsQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<CommonTermsQueryBuilder> {
 
-    public static final String NAME = "common";
-
-    public static final float DEFAULT_CUTOFF_FREQ = 0.01f;
-
-    public static final Operator DEFAULT_HIGH_FREQ_OCCUR = Operator.OR;
-
-    public static final Operator DEFAULT_LOW_FREQ_OCCUR = Operator.OR;
-
-    public static final boolean DEFAULT_DISABLE_COORD = true;
+    public static enum Operator {
+        OR, AND
+    }
 
-    private final String fieldName;
+    private final String name;
 
     private final Object text;
 
-    private Operator highFreqOperator = DEFAULT_HIGH_FREQ_OCCUR;
+    private Operator highFreqOperator = null;
 
-    private Operator lowFreqOperator = DEFAULT_LOW_FREQ_OCCUR;
+    private Operator lowFreqOperator = null;
 
     private String analyzer = null;
 
+    private Float boost = null;
+
     private String lowFreqMinimumShouldMatch = null;
 
     private String highFreqMinimumShouldMatch = null;
 
-    private boolean disableCoord = DEFAULT_DISABLE_COORD;
+    private Boolean disableCoord = null;
 
-    private float cutoffFrequency = DEFAULT_CUTOFF_FREQ;
+    private Float cutoffFrequency = null;
 
-    static final CommonTermsQueryBuilder PROTOTYPE = new CommonTermsQueryBuilder(null, null);
+    private String queryName;
 
     /**
      * Constructs a new common terms query.
      */
-    public CommonTermsQueryBuilder(String fieldName, Object text) {
-        this.fieldName = fieldName;
+    public CommonTermsQueryBuilder(String name, Object text) {
+        if (name == null) {
+            throw new IllegalArgumentException("Field name must not be null");
+        }
+        if (text == null) {
+            throw new IllegalArgumentException("Query must not be null");
+        }
         this.text = text;
-    }
-
-    public String fieldName() {
-        return this.fieldName;
-    }
-
-    public Object text() {
-        return this.text;
+        this.name = name;
     }
 
     /**
@@ -109,27 +90,19 @@ public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQue
      * <tt>AND</tt>.
      */
     public CommonTermsQueryBuilder highFreqOperator(Operator operator) {
-        this.highFreqOperator = (operator == null) ? DEFAULT_HIGH_FREQ_OCCUR : operator;
+        this.highFreqOperator = operator;
         return this;
     }
 
-    public Operator highFreqOperator() {
-        return highFreqOperator;
-    }
-
     /**
      * Sets the operator to use for terms with a low document frequency (less
      * than {@link #cutoffFrequency(float)}. Defaults to <tt>AND</tt>.
      */
     public CommonTermsQueryBuilder lowFreqOperator(Operator operator) {
-        this.lowFreqOperator = (operator == null) ? DEFAULT_LOW_FREQ_OCCUR : operator;
+        this.lowFreqOperator = operator;
         return this;
     }
 
-    public Operator lowFreqOperator() {
-        return lowFreqOperator;
-    }
-
     /**
      * Explicitly set the analyzer to use. Defaults to use explicit mapping
      * config for the field, or, if not set, the default search analyzer.
@@ -139,8 +112,13 @@ public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQue
         return this;
     }
 
-    public String analyzer() {
-        return this.analyzer;
+    /**
+     * Set the boost to apply to the query.
+     */
+    @Override
+    public CommonTermsQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
@@ -148,17 +126,13 @@ public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQue
      * in [0..1] (or absolute number >=1) representing the maximum threshold of
      * a terms document frequency to be considered a low frequency term.
      * Defaults to
-     * <tt>{@value #DEFAULT_CUTOFF_FREQ}</tt>
+     * <tt>{@value CommonTermsQueryParser#DEFAULT_MAX_TERM_DOC_FREQ}</tt>
      */
     public CommonTermsQueryBuilder cutoffFrequency(float cutoffFrequency) {
         this.cutoffFrequency = cutoffFrequency;
         return this;
     }
 
-    public float cutoffFrequency() {
-        return this.cutoffFrequency;
-    }
-
     /**
      * Sets the minimum number of high frequent query terms that need to match in order to
      * produce a hit when there are no low frequen terms.
@@ -168,10 +142,6 @@ public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQue
         return this;
     }
 
-    public String highFreqMinimumShouldMatch() {
-        return this.highFreqMinimumShouldMatch;
-    }
-
     /**
      * Sets the minimum number of low frequent query terms that need to match in order to
      * produce a hit.
@@ -180,33 +150,44 @@ public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQue
         this.lowFreqMinimumShouldMatch = lowFreqMinimumShouldMatch;
         return this;
     }
-
-    public String lowFreqMinimumShouldMatch() {
-        return this.lowFreqMinimumShouldMatch;
-    }
-
+    
     public CommonTermsQueryBuilder disableCoord(boolean disableCoord) {
         this.disableCoord = disableCoord;
         return this;
     }
 
-    public boolean disableCoord() {
-        return this.disableCoord;
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public CommonTermsQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.startObject(fieldName);
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(CommonTermsQueryParser.NAME);
+        builder.startObject(name);
 
         builder.field("query", text);
-        builder.field("disable_coord", disableCoord);
-        builder.field("high_freq_operator", highFreqOperator.toString());
-        builder.field("low_freq_operator", lowFreqOperator.toString());
+        if (disableCoord != null) {
+            builder.field("disable_coord", disableCoord);
+        }
+        if (highFreqOperator != null) {
+            builder.field("high_freq_operator", highFreqOperator.toString());
+        }
+        if (lowFreqOperator != null) {
+            builder.field("low_freq_operator", lowFreqOperator.toString());
+        }
         if (analyzer != null) {
             builder.field("analyzer", analyzer);
         }
-        builder.field("cutoff_frequency", cutoffFrequency);
+        if (boost != null) {
+            builder.field("boost", boost);
+        }
+        if (cutoffFrequency != null) {
+            builder.field("cutoff_frequency", cutoffFrequency);
+        }
         if (lowFreqMinimumShouldMatch != null || highFreqMinimumShouldMatch != null) {
             builder.startObject("minimum_should_match");
             if (lowFreqMinimumShouldMatch != null) {
@@ -217,125 +198,11 @@ public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQue
             }
             builder.endObject();
         }
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        String field;
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType != null) {
-            field = fieldType.names().indexName();
-        } else {
-            field = fieldName;
-        }
-
-        Analyzer analyzerObj;
-        if (analyzer == null) {
-            if (fieldType != null) {
-                analyzerObj = context.getSearchAnalyzer(fieldType);
-            } else {
-                analyzerObj = context.mapperService().searchAnalyzer();
-            }
-        } else {
-            analyzerObj = context.mapperService().analysisService().analyzer(analyzer);
-            if (analyzerObj == null) {
-                throw new QueryShardException(context, "[common] analyzer [" + analyzer + "] not found");
-            }
-        }
-
-        Occur highFreqOccur = highFreqOperator.toBooleanClauseOccur();
-        Occur lowFreqOccur = lowFreqOperator.toBooleanClauseOccur();
-
-        ExtendedCommonTermsQuery commonsQuery = new ExtendedCommonTermsQuery(highFreqOccur, lowFreqOccur, cutoffFrequency, disableCoord, fieldType);
-        return parseQueryString(commonsQuery, text, field, analyzerObj, lowFreqMinimumShouldMatch, highFreqMinimumShouldMatch);
-    }
-
-    static Query parseQueryString(ExtendedCommonTermsQuery query, Object queryString, String field, Analyzer analyzer,
-                                         String lowFreqMinimumShouldMatch, String highFreqMinimumShouldMatch) throws IOException {
-        // Logic similar to QueryParser#getFieldQuery
-        int count = 0;
-        try (TokenStream source = analyzer.tokenStream(field, queryString.toString())) {
-            source.reset();
-            CharTermAttribute termAtt = source.addAttribute(CharTermAttribute.class);
-            BytesRefBuilder builder = new BytesRefBuilder();
-            while (source.incrementToken()) {
-                // UTF-8
-                builder.copyChars(termAtt);
-                query.add(new Term(field, builder.toBytesRef()));
-                count++;
-            }
-        }
-
-        if (count == 0) {
-            return null;
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        query.setLowFreqMinimumNumberShouldMatch(lowFreqMinimumShouldMatch);
-        query.setHighFreqMinimumNumberShouldMatch(highFreqMinimumShouldMatch);
-        return query;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (Strings.isEmpty(this.fieldName)) {
-            validationException = addValidationError("field name cannot be null or empty.", validationException);
-        }
-        if (this.text == null) {
-            validationException = addValidationError("query text cannot be null", validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    protected CommonTermsQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        CommonTermsQueryBuilder commonTermsQueryBuilder = new CommonTermsQueryBuilder(in.readString(), in.readGenericValue());
-        commonTermsQueryBuilder.highFreqOperator = Operator.readOperatorFrom(in);
-        commonTermsQueryBuilder.lowFreqOperator = Operator.readOperatorFrom(in);
-        commonTermsQueryBuilder.analyzer = in.readOptionalString();
-        commonTermsQueryBuilder.lowFreqMinimumShouldMatch = in.readOptionalString();
-        commonTermsQueryBuilder.highFreqMinimumShouldMatch = in.readOptionalString();
-        commonTermsQueryBuilder.disableCoord = in.readBoolean();
-        commonTermsQueryBuilder.cutoffFrequency = in.readFloat();
-        return commonTermsQueryBuilder;
-    }
 
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(this.fieldName);
-        out.writeGenericValue(this.text);
-        highFreqOperator.writeTo(out);
-        lowFreqOperator.writeTo(out);
-        out.writeOptionalString(analyzer);
-        out.writeOptionalString(lowFreqMinimumShouldMatch);
-        out.writeOptionalString(highFreqMinimumShouldMatch);
-        out.writeBoolean(disableCoord);
-        out.writeFloat(cutoffFrequency);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldName, text, highFreqOperator, lowFreqOperator, analyzer,
-                lowFreqMinimumShouldMatch, highFreqMinimumShouldMatch, disableCoord, cutoffFrequency);
-    }
-
-    @Override
-    protected boolean doEquals(CommonTermsQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-                Objects.equals(text, other.text) &&
-                Objects.equals(highFreqOperator, other.highFreqOperator) &&
-                Objects.equals(lowFreqOperator, other.lowFreqOperator) &&
-                Objects.equals(analyzer, other.analyzer) &&
-                Objects.equals(lowFreqMinimumShouldMatch, other.lowFreqMinimumShouldMatch) &&
-                Objects.equals(highFreqMinimumShouldMatch, other.highFreqMinimumShouldMatch) &&
-                Objects.equals(disableCoord, other.disableCoord) &&
-                Objects.equals(cutoffFrequency, other.cutoffFrequency);
+        builder.endObject();
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java
index fc6b9ca..c18229e 100644
--- a/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java
@@ -19,15 +19,36 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.queries.ExtendedCommonTermsQuery;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.util.BytesRefBuilder;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
 
 import java.io.IOException;
 
 /**
  *
  */
-public class CommonTermsQueryParser extends BaseQueryParser {
+public class CommonTermsQueryParser implements QueryParser {
+
+    public static final String NAME = "common";
+
+    static final float DEFAULT_MAX_TERM_DOC_FREQ = 0.01f;
+
+    static final Occur DEFAULT_HIGH_FREQ_OCCUR = Occur.SHOULD;
+
+    static final Occur DEFAULT_LOW_FREQ_OCCUR = Occur.SHOULD;
+
+    static final boolean DEFAULT_DISABLE_COORD = true;
+
 
     @Inject
     public CommonTermsQueryParser() {
@@ -35,26 +56,26 @@ public class CommonTermsQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[] { CommonTermsQueryBuilder.NAME };
+        return new String[] { NAME };
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
         XContentParser.Token token = parser.nextToken();
         if (token != XContentParser.Token.FIELD_NAME) {
             throw new QueryParsingException(parseContext, "[common] query malformed, no field");
         }
         String fieldName = parser.currentName();
-        Object text = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        String analyzer = null;
+        Object value = null;
+        float boost = 1.0f;
+        String queryAnalyzer = null;
         String lowFreqMinimumShouldMatch = null;
         String highFreqMinimumShouldMatch = null;
-        boolean disableCoord = CommonTermsQueryBuilder.DEFAULT_DISABLE_COORD;
-        Operator highFreqOperator = CommonTermsQueryBuilder.DEFAULT_HIGH_FREQ_OCCUR;
-        Operator lowFreqOperator = CommonTermsQueryBuilder.DEFAULT_LOW_FREQ_OCCUR;
-        float cutoffFrequency = CommonTermsQueryBuilder.DEFAULT_CUTOFF_FREQ;
+        boolean disableCoord = DEFAULT_DISABLE_COORD;
+        Occur highFreqOccur = DEFAULT_HIGH_FREQ_OCCUR;
+        Occur lowFreqOccur = DEFAULT_LOW_FREQ_OCCUR;
+        float maxTermFrequency = DEFAULT_MAX_TERM_DOC_FREQ;
         String queryName = null;
         token = parser.nextToken();
         if (token == XContentParser.Token.START_OBJECT) {
@@ -84,21 +105,41 @@ public class CommonTermsQueryParser extends BaseQueryParser {
                     }
                 } else if (token.isValue()) {
                     if ("query".equals(currentFieldName)) {
-                        text = parser.objectText();
+                        value = parser.objectText();
                     } else if ("analyzer".equals(currentFieldName)) {
-                        analyzer = parser.text();
+                        String analyzer = parser.text();
+                        if (parseContext.analysisService().analyzer(analyzer) == null) {
+                            throw new QueryParsingException(parseContext, "[common] analyzer [" + parser.text() + "] not found");
+                        }
+                        queryAnalyzer = analyzer;
                     } else if ("disable_coord".equals(currentFieldName) || "disableCoord".equals(currentFieldName)) {
                         disableCoord = parser.booleanValue();
                     } else if ("boost".equals(currentFieldName)) {
                         boost = parser.floatValue();
                     } else if ("high_freq_operator".equals(currentFieldName) || "highFreqOperator".equals(currentFieldName)) {
-                        highFreqOperator = Operator.fromString(parser.text());
+                        String op = parser.text();
+                        if ("or".equalsIgnoreCase(op)) {
+                            highFreqOccur = BooleanClause.Occur.SHOULD;
+                        } else if ("and".equalsIgnoreCase(op)) {
+                            highFreqOccur = BooleanClause.Occur.MUST;
+                        } else {
+                            throw new QueryParsingException(parseContext,
+                                    "[common] query requires operator to be either 'and' or 'or', not [" + op + "]");
+                        }
                     } else if ("low_freq_operator".equals(currentFieldName) || "lowFreqOperator".equals(currentFieldName)) {
-                        lowFreqOperator = Operator.fromString(parser.text());
+                        String op = parser.text();
+                        if ("or".equalsIgnoreCase(op)) {
+                            lowFreqOccur = BooleanClause.Occur.SHOULD;
+                        } else if ("and".equalsIgnoreCase(op)) {
+                            lowFreqOccur = BooleanClause.Occur.MUST;
+                        } else {
+                            throw new QueryParsingException(parseContext,
+                                    "[common] query requires operator to be either 'and' or 'or', not [" + op + "]");
+                        }
                     } else if ("minimum_should_match".equals(currentFieldName) || "minimumShouldMatch".equals(currentFieldName)) {
                         lowFreqMinimumShouldMatch = parser.text();
                     } else if ("cutoff_frequency".equals(currentFieldName)) {
-                        cutoffFrequency = parser.floatValue();
+                        maxTermFrequency = parser.floatValue();
                     } else if ("_name".equals(currentFieldName)) {
                         queryName = parser.text();
                     } else {
@@ -108,7 +149,7 @@ public class CommonTermsQueryParser extends BaseQueryParser {
             }
             parser.nextToken();
         } else {
-            text = parser.objectText();
+            value = parser.objectText();
             // move to the next token
             token = parser.nextToken();
             if (token != XContentParser.Token.END_OBJECT) {
@@ -118,23 +159,66 @@ public class CommonTermsQueryParser extends BaseQueryParser {
             }
         }
 
-        if (text == null) {
+        if (value == null) {
             throw new QueryParsingException(parseContext, "No text specified for text query");
         }
-        return new CommonTermsQueryBuilder(fieldName, text)
-                .lowFreqMinimumShouldMatch(lowFreqMinimumShouldMatch)
-                .highFreqMinimumShouldMatch(highFreqMinimumShouldMatch)
-                .analyzer(analyzer)
-                .highFreqOperator(highFreqOperator)
-                .lowFreqOperator(lowFreqOperator)
-                .disableCoord(disableCoord)
-                .cutoffFrequency(cutoffFrequency)
-                .boost(boost)
-                .queryName(queryName);
+        String field;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            field = fieldType.names().indexName();
+        } else {
+            field = fieldName;
+        }
+
+        Analyzer analyzer = null;
+        if (queryAnalyzer == null) {
+            if (fieldType != null) {
+                analyzer = fieldType.searchAnalyzer();
+            }
+            if (analyzer == null && fieldType != null) {
+                analyzer = parseContext.getSearchAnalyzer(fieldType);
+            }
+            if (analyzer == null) {
+                analyzer = parseContext.mapperService().searchAnalyzer();
+            }
+        } else {
+            analyzer = parseContext.mapperService().analysisService().analyzer(queryAnalyzer);
+            if (analyzer == null) {
+                throw new IllegalArgumentException("No analyzer found for [" + queryAnalyzer + "]");
+            }
+        }
+
+        ExtendedCommonTermsQuery commonsQuery = new ExtendedCommonTermsQuery(highFreqOccur, lowFreqOccur, maxTermFrequency, disableCoord, fieldType);
+        commonsQuery.setBoost(boost);
+        Query query = parseQueryString(commonsQuery, value.toString(), field, parseContext, analyzer, lowFreqMinimumShouldMatch, highFreqMinimumShouldMatch);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 
-    @Override
-    public CommonTermsQueryBuilder getBuilderPrototype() {
-        return CommonTermsQueryBuilder.PROTOTYPE;
+
+    private final Query parseQueryString(ExtendedCommonTermsQuery query, String queryString, String field, QueryParseContext parseContext,
+            Analyzer analyzer, String lowFreqMinimumShouldMatch, String highFreqMinimumShouldMatch) throws IOException {
+        // Logic similar to QueryParser#getFieldQuery
+        int count = 0;
+        try (TokenStream source = analyzer.tokenStream(field, queryString.toString())) {
+            source.reset();
+            CharTermAttribute termAtt = source.addAttribute(CharTermAttribute.class);
+            BytesRefBuilder builder = new BytesRefBuilder();
+            while (source.incrementToken()) {
+                // UTF-8
+                builder.copyChars(termAtt);
+                query.add(new Term(field, builder.toBytesRef()));
+                count++;
+            }
+        }
+
+        if (count == 0) {
+            return null;
+        }
+        query.setLowFreqMinimumNumberShouldMatch(lowFreqMinimumShouldMatch);
+        query.setHighFreqMinimumNumberShouldMatch(highFreqMinimumShouldMatch);
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java
index 9648db8..bdcbe9c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java
@@ -19,10 +19,6 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
@@ -32,84 +28,41 @@ import java.util.Objects;
  * A query that wraps a filter and simply returns a constant score equal to the
  * query boost for every document in the filter.
  */
-public class ConstantScoreQueryBuilder extends AbstractQueryBuilder<ConstantScoreQueryBuilder> {
-
-    public static final String NAME = "constant_score";
+public class ConstantScoreQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<ConstantScoreQueryBuilder> {
 
     private final QueryBuilder filterBuilder;
 
-    static final ConstantScoreQueryBuilder PROTOTYPE = new ConstantScoreQueryBuilder(null);
+    private float boost = -1;
 
     /**
-     * A query that wraps another query and simply returns a constant score equal to the
+     * A query that wraps a query and simply returns a constant score equal to the
      * query boost for every document in the query.
      *
      * @param filterBuilder The query to wrap in a constant score query
      */
     public ConstantScoreQueryBuilder(QueryBuilder filterBuilder) {
-        this.filterBuilder = filterBuilder;
+        this.filterBuilder = Objects.requireNonNull(filterBuilder);
     }
 
     /**
-     * @return the query that was wrapped in this constant score query
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
      */
-    public QueryBuilder query() {
-        return this.filterBuilder;
+    @Override
+    public ConstantScoreQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(ConstantScoreQueryParser.NAME);
         builder.field("filter");
         filterBuilder.toXContent(builder, params);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query innerFilter = filterBuilder.toQuery(context);
-        if (innerFilter == null ) {
-            // return null so that parent queries (e.g. bool) also ignore this
-            return null;
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        return new ConstantScoreQuery(filterBuilder.toQuery(context));
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (filterBuilder == null) {
-            validationException = addValidationError("inner clause [filter] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(filterBuilder, validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(filterBuilder);
-    }
-
-    @Override
-    protected boolean doEquals(ConstantScoreQueryBuilder other) {
-        return Objects.equals(filterBuilder, other.filterBuilder);
-    }
-
-    @Override
-    protected ConstantScoreQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryBuilder innerFilterBuilder = in.readQuery();
-        return new ConstantScoreQueryBuilder(innerFilterBuilder);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(filterBuilder);
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java
index 22c1da3..d8a34b9 100644
--- a/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
@@ -29,8 +31,9 @@ import java.io.IOException;
 /**
  *
  */
-public class ConstantScoreQueryParser extends BaseQueryParser {
+public class ConstantScoreQueryParser implements QueryParser {
 
+    public static final String NAME = "constant_score";
     private static final ParseField INNER_QUERY_FIELD = new ParseField("filter", "query");
 
     @Inject
@@ -39,17 +42,16 @@ public class ConstantScoreQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{ConstantScoreQueryBuilder.NAME, Strings.toCamelCase(ConstantScoreQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        QueryBuilder query = null;
+        Query filter = null;
         boolean queryFound = false;
-        String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -60,15 +62,13 @@ public class ConstantScoreQueryParser extends BaseQueryParser {
                 // skip
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if (parseContext.parseFieldMatcher().match(currentFieldName, INNER_QUERY_FIELD)) {
-                    query = parseContext.parseInnerFilterToQueryBuilder();
+                    filter = parseContext.parseInnerFilter();
                     queryFound = true;
                 } else {
                     throw new QueryParsingException(parseContext, "[constant_score] query does not support [" + currentFieldName + "]");
                 }
             } else if (token.isValue()) {
-                if ("_name".equals(currentFieldName)) {
-                    queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
+                if ("boost".equals(currentFieldName)) {
                     boost = parser.floatValue();
                 } else {
                     throw new QueryParsingException(parseContext, "[constant_score] query does not support [" + currentFieldName + "]");
@@ -79,14 +79,12 @@ public class ConstantScoreQueryParser extends BaseQueryParser {
             throw new QueryParsingException(parseContext, "[constant_score] requires a 'filter' element");
         }
 
-        ConstantScoreQueryBuilder constantScoreBuilder = new ConstantScoreQueryBuilder(query);
-        constantScoreBuilder.boost(boost);
-        constantScoreBuilder.queryName(queryName);
-        return constantScoreBuilder;
-    }
+        if (filter == null) {
+            return null;
+        }
 
-    @Override
-    public ConstantScoreQueryBuilder getBuilderPrototype() {
-        return ConstantScoreQueryBuilder.PROTOTYPE;
+        filter = new ConstantScoreQuery(filter);
+        filter.setBoost(boost);
+        return filter;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java
index df44ae2..ddf9d95 100644
--- a/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java
@@ -19,34 +19,27 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.DisjunctionMaxQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collection;
-import java.util.List;
-import java.util.Objects;
+
+import static com.google.common.collect.Lists.newArrayList;
 
 /**
  * A query that generates the union of documents produced by its sub-queries, and that scores each document
  * with the maximum score for that document as produced by any sub-query, plus a tie breaking increment for any
  * additional matching sub-queries.
  */
-public class DisMaxQueryBuilder extends AbstractQueryBuilder<DisMaxQueryBuilder> {
+public class DisMaxQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<DisMaxQueryBuilder> {
 
-    public static final String NAME = "dis_max";
+    private ArrayList<QueryBuilder> queries = newArrayList();
 
-    private final ArrayList<QueryBuilder> queries = new ArrayList<>();
+    private float boost = -1;
 
-    /** Default multiplication factor for breaking ties in document scores.*/
-    public static float DEFAULT_TIE_BREAKER = 0.0f;
-    private float tieBreaker = DEFAULT_TIE_BREAKER;
+    private float tieBreaker = -1;
 
-    static final DisMaxQueryBuilder PROTOTYPE = new DisMaxQueryBuilder();
+    private String queryName;
 
     /**
      * Add a sub-query to this disjunction.
@@ -57,10 +50,13 @@ public class DisMaxQueryBuilder extends AbstractQueryBuilder<DisMaxQueryBuilder>
     }
 
     /**
-     * @return an immutable list copy of the current sub-queries of this disjunction
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
      */
-    public List<QueryBuilder> queries() {
-        return this.queries;
+    @Override
+    public DisMaxQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
@@ -75,70 +71,30 @@ public class DisMaxQueryBuilder extends AbstractQueryBuilder<DisMaxQueryBuilder>
     }
 
     /**
-     * @return the tie breaker score
-     * @see DisMaxQueryBuilder#tieBreaker(float)
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public float tieBreaker() {
-        return this.tieBreaker;
+    public DisMaxQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("tie_breaker", tieBreaker);
+        builder.startObject(DisMaxQueryParser.NAME);
+        if (tieBreaker != -1) {
+            builder.field("tie_breaker", tieBreaker);
+        }
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         builder.startArray("queries");
         for (QueryBuilder queryBuilder : queries) {
             queryBuilder.toXContent(builder, params);
         }
         builder.endArray();
-        printBoostAndQueryName(builder);
         builder.endObject();
     }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        // return null if there are no queries at all
-        Collection<Query> luceneQueries = toQueries(queries, context);
-        if (luceneQueries.isEmpty()) {
-            return null;
-        }
-
-        return new DisjunctionMaxQuery(luceneQueries, tieBreaker);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        return validateInnerQueries(queries, null);
-    }
-
-    @Override
-    protected DisMaxQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        DisMaxQueryBuilder disMax = new DisMaxQueryBuilder();
-        List<QueryBuilder> queryBuilders = readQueries(in);
-        disMax.queries.addAll(queryBuilders);
-        disMax.tieBreaker = in.readFloat();
-        return disMax;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        writeQueries(out, queries);
-        out.writeFloat(tieBreaker);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(queries, tieBreaker);
-    }
-
-    @Override
-    protected boolean doEquals(DisMaxQueryBuilder other) {
-        return Objects.equals(queries, other.queries) &&
-               Objects.equals(tieBreaker, other.tieBreaker);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java
index fa3a4e0..2747387 100644
--- a/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.DisjunctionMaxQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
@@ -31,7 +33,9 @@ import static com.google.common.collect.Lists.newArrayList;
 /**
  *
  */
-public class DisMaxQueryParser extends BaseQueryParser {
+public class DisMaxQueryParser implements QueryParser {
+
+    public static final String NAME = "dis_max";
 
     @Inject
     public DisMaxQueryParser() {
@@ -39,17 +43,17 @@ public class DisMaxQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{DisMaxQueryBuilder.NAME, Strings.toCamelCase(DisMaxQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        float tieBreaker = DisMaxQueryBuilder.DEFAULT_TIE_BREAKER;
+        float boost = 1.0f;
+        float tieBreaker = 0.0f;
 
-        final List<QueryBuilder> queries = newArrayList();
+        List<Query> queries = newArrayList();
         boolean queriesFound = false;
         String queryName = null;
 
@@ -61,8 +65,10 @@ public class DisMaxQueryParser extends BaseQueryParser {
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("queries".equals(currentFieldName)) {
                     queriesFound = true;
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    queries.add(query);
+                    Query query = parseContext.parseInnerQuery();
+                    if (query != null) {
+                        queries.add(query);
+                    }
                 } else {
                     throw new QueryParsingException(parseContext, "[dis_max] query does not support [" + currentFieldName + "]");
                 }
@@ -70,8 +76,10 @@ public class DisMaxQueryParser extends BaseQueryParser {
                 if ("queries".equals(currentFieldName)) {
                     queriesFound = true;
                     while (token != XContentParser.Token.END_ARRAY) {
-                        QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                        queries.add(query);
+                        Query query = parseContext.parseInnerQuery();
+                        if (query != null) {
+                            queries.add(query);
+                        }
                         token = parser.nextToken();
                     }
                 } else {
@@ -94,18 +102,15 @@ public class DisMaxQueryParser extends BaseQueryParser {
             throw new QueryParsingException(parseContext, "[dis_max] requires 'queries' field");
         }
 
-        DisMaxQueryBuilder disMaxQuery = new DisMaxQueryBuilder();
-        disMaxQuery.tieBreaker(tieBreaker);
-        disMaxQuery.queryName(queryName);
-        disMaxQuery.boost(boost);
-        for (QueryBuilder query : queries) {
-            disMaxQuery.add(query);
+        if (queries.isEmpty()) {
+            return null;
         }
-        return disMaxQuery;
-    }
 
-    @Override
-    public DisMaxQueryBuilder getBuilderPrototype() {
-        return DisMaxQueryBuilder.PROTOTYPE;
+        DisjunctionMaxQuery query = new DisjunctionMaxQuery(queries, tieBreaker);
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/EmptyQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/EmptyQueryBuilder.java
deleted file mode 100644
index de5d5bb..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/EmptyQueryBuilder.java
+++ /dev/null
@@ -1,111 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.elasticsearch.action.support.ToXContentToBytes;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentType;
-
-import java.io.IOException;
-
-/**
- * A {@link QueryBuilder} that is a stand in replacement for an empty query clause in the DSL.
- * The current DSL allows parsing inner queries / filters like "{ }", in order to have a
- * valid non-null representation of these clauses that actually do nothing we can use this class.
- *
- * This builder has no corresponding parser and it is not registered under the query name. It is
- * intended to be used internally as a stand-in for nested queries that are left empty and should
- * be ignored upstream.
- */
-public class EmptyQueryBuilder extends ToXContentToBytes implements QueryBuilder<EmptyQueryBuilder> {
-
-    public static final String NAME = "empty_query";
-
-    /** the one and only empty query builder */
-    public static final EmptyQueryBuilder PROTOTYPE = new EmptyQueryBuilder();
-
-    // prevent instances other than prototype
-    private EmptyQueryBuilder() {
-        super(XContentType.JSON);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    public String getName() {
-        return getWriteableName();
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public Query toQuery(QueryShardContext context) throws IOException {
-        // empty
-        return null;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        // nothing to validate
-        return null;
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-    }
-
-    @Override
-    public EmptyQueryBuilder readFrom(StreamInput in) throws IOException {
-        return EmptyQueryBuilder.PROTOTYPE;
-    }
-
-    @Override
-    public EmptyQueryBuilder queryName(String queryName) {
-        //no-op
-        return this;
-    }
-
-    @Override
-    public String queryName() {
-        return null;
-    }
-
-    @Override
-    public float boost() {
-        return -1;
-    }
-
-    @Override
-    public EmptyQueryBuilder boost(float boost) {
-        //no-op
-        return this;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java
index 4e1f54e..9980d81 100644
--- a/core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java
@@ -19,126 +19,38 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.*;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.internal.FieldNamesFieldMapper;
-import org.elasticsearch.index.mapper.object.ObjectMapper;
 
 import java.io.IOException;
-import java.util.Collection;
-import java.util.Objects;
 
 /**
  * Constructs a query that only match on documents that the field has a value in them.
  */
-public class ExistsQueryBuilder extends AbstractQueryBuilder<ExistsQueryBuilder> {
+public class ExistsQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "exists";
+    private String name;
 
-    private final String name;
-
-    static final ExistsQueryBuilder PROTOTYPE = new ExistsQueryBuilder(null);
+    private String queryName;
 
     public ExistsQueryBuilder(String name) {
         this.name = name;
     }
 
     /**
-     * @return the field name that has to exist for this query to match
+     * Sets the query name for the query that can be used when searching for matched_queries per hit.
      */
-    public String name() {
-        return this.name;
+    public ExistsQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(ExistsQueryParser.NAME);
         builder.field("field", name);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return newFilter(context, name);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        // nothing to validate
-        return null;
-    }
-
-    public static Query newFilter(QueryShardContext context, String fieldPattern) {
-        final FieldNamesFieldMapper.FieldNamesFieldType fieldNamesFieldType = (FieldNamesFieldMapper.FieldNamesFieldType)context.mapperService().fullName(FieldNamesFieldMapper.NAME);
-        if (fieldNamesFieldType == null) {
-            // can only happen when no types exist, so no docs exist either
-            return Queries.newMatchNoDocsQuery();
-        }
-
-        ObjectMapper objectMapper = context.getObjectMapper(fieldPattern);
-        if (objectMapper != null) {
-            // automatic make the object mapper pattern
-            fieldPattern = fieldPattern + ".*";
-        }
-
-        Collection<String> fields = context.simpleMatchToIndexNames(fieldPattern);
-        if (fields.isEmpty()) {
-            // no fields exists, so we should not match anything
-            return Queries.newMatchNoDocsQuery();
-        }
-
-        BooleanQuery boolFilter = new BooleanQuery();
-        for (String field : fields) {
-            MappedFieldType fieldType = context.fieldMapper(field);
-            Query filter = null;
-            if (fieldNamesFieldType.isEnabled()) {
-                final String f;
-                if (fieldType != null) {
-                    f = fieldType.names().indexName();
-                } else {
-                    f = field;
-                }
-                filter = fieldNamesFieldType.termQuery(f, context);
-            }
-            // if _field_names are not indexed, we need to go the slow way
-            if (filter == null && fieldType != null) {
-                filter = fieldType.rangeQuery(null, null, true, true);
-            }
-            if (filter == null) {
-                filter = new TermRangeQuery(field, null, null, true, true);
-            }
-            boolFilter.add(filter, BooleanClause.Occur.SHOULD);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return new ConstantScoreQuery(boolFilter);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(name);
-    }
-
-    @Override
-    protected boolean doEquals(ExistsQueryBuilder other) {
-        return Objects.equals(name, other.name);
-    }
-
-    @Override
-    protected ExistsQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new ExistsQueryBuilder(in.readString());
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(name);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java
index 12cb7f3..0ce578c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java
@@ -19,15 +19,23 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.*;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.internal.FieldNamesFieldMapper;
+import org.elasticsearch.index.mapper.object.ObjectMapper;
 
 import java.io.IOException;
+import java.util.Collection;
 
 /**
  *
  */
-public class ExistsQueryParser extends BaseQueryParser {
+public class ExistsQueryParser implements QueryParser {
+
+    public static final String NAME = "exists";
 
     @Inject
     public ExistsQueryParser() {
@@ -35,16 +43,15 @@ public class ExistsQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{ExistsQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldPattern = null;
         String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
 
         XContentParser.Token token;
         String currentFieldName = null;
@@ -56,8 +63,6 @@ public class ExistsQueryParser extends BaseQueryParser {
                     fieldPattern = parser.text();
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else {
                     throw new QueryParsingException(parseContext, "[exists] query does not support [" + currentFieldName + "]");
                 }
@@ -68,14 +73,55 @@ public class ExistsQueryParser extends BaseQueryParser {
             throw new QueryParsingException(parseContext, "exists must be provided with a [field]");
         }
 
-        ExistsQueryBuilder builder = new ExistsQueryBuilder(fieldPattern);
-        builder.queryName(queryName);
-        builder.boost(boost);
-        return builder;
+        return newFilter(parseContext, fieldPattern, queryName);
     }
 
-    @Override
-    public ExistsQueryBuilder getBuilderPrototype() {
-        return ExistsQueryBuilder.PROTOTYPE;
+    public static Query newFilter(QueryParseContext parseContext, String fieldPattern, String queryName) {
+        final FieldNamesFieldMapper.FieldNamesFieldType fieldNamesFieldType = (FieldNamesFieldMapper.FieldNamesFieldType)parseContext.mapperService().fullName(FieldNamesFieldMapper.NAME);
+        if (fieldNamesFieldType == null) {
+            // can only happen when no types exist, so no docs exist either
+            return Queries.newMatchNoDocsQuery();
+        }
+
+        ObjectMapper objectMapper = parseContext.getObjectMapper(fieldPattern);
+        if (objectMapper != null) {
+            // automatic make the object mapper pattern
+            fieldPattern = fieldPattern + ".*";
+        }
+
+        Collection<String> fields = parseContext.simpleMatchToIndexNames(fieldPattern);
+        if (fields.isEmpty()) {
+            // no fields exists, so we should not match anything
+            return Queries.newMatchNoDocsQuery();
+        }
+
+        BooleanQuery boolFilter = new BooleanQuery();
+        for (String field : fields) {
+            MappedFieldType fieldType = parseContext.fieldMapper(field);
+            Query filter = null;
+            if (fieldNamesFieldType.isEnabled()) {
+                final String f;
+                if (fieldType != null) {
+                    f = fieldType.names().indexName();
+                } else {
+                    f = field;
+                }
+                filter = fieldNamesFieldType.termQuery(f, parseContext);
+            }
+            // if _field_names are not indexed, we need to go the slow way
+            if (filter == null && fieldType != null) {
+                filter = fieldType.rangeQuery(null, null, true, true);
+            }
+            if (filter == null) {
+                filter = new TermRangeQuery(field, null, null, true, true);
+            }
+            boolFilter.add(filter, BooleanClause.Occur.SHOULD);
+        }
+
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, boolFilter);
+        }
+        return new ConstantScoreQuery(boolFilter);
     }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/FQueryFilterBuilder.java b/core/src/main/java/org/elasticsearch/index/query/FQueryFilterBuilder.java
deleted file mode 100644
index 40511c4..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/FQueryFilterBuilder.java
+++ /dev/null
@@ -1,112 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-
-import java.io.IOException;
-import java.util.Objects;
-
-/**
- * A filter that simply wraps a query. Same as the {@link QueryFilterBuilder} except that it allows also to
- * associate a name with the query filter.
- * @deprecated Useless now that queries and filters are merged: pass the
- *             query as a filter directly.
- */
-@Deprecated
-public class FQueryFilterBuilder extends AbstractQueryBuilder<FQueryFilterBuilder> {
-
-    public static final String NAME = "fquery";
-
-    static final FQueryFilterBuilder PROTOTYPE = new FQueryFilterBuilder(null);
-
-    private final QueryBuilder queryBuilder;
-
-    /**
-     * A filter that simply wraps a query.
-     *
-     * @param queryBuilder The query to wrap as a filter
-     */
-    public FQueryFilterBuilder(QueryBuilder queryBuilder) {
-        this.queryBuilder = queryBuilder;
-    }
-
-    /**
-     * @return the query builder that is wrapped by this {@link FQueryFilterBuilder}
-     */
-    public QueryBuilder innerQuery() {
-        return this.queryBuilder;
-    }
-
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(FQueryFilterBuilder.NAME);
-        builder.field("query");
-        queryBuilder.toXContent(builder, params);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        // inner query builder can potentially be `null`, in that case we ignore it
-        Query innerQuery = this.queryBuilder.toQuery(context);
-        if (innerQuery == null) {
-            return null;
-        }
-        return new ConstantScoreQuery(innerQuery);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        return validateInnerQuery(queryBuilder, null);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(queryBuilder);
-    }
-
-    @Override
-    protected boolean doEquals(FQueryFilterBuilder other) {
-        return Objects.equals(queryBuilder, other.queryBuilder);
-    }
-
-    @Override
-    protected FQueryFilterBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryBuilder innerQueryBuilder = in.readQuery();
-        FQueryFilterBuilder fquery = new FQueryFilterBuilder(innerQueryBuilder);
-        return fquery;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(queryBuilder);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java b/core/src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java
index 180ecbf..4c0f782 100644
--- a/core/src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
 
@@ -29,7 +31,9 @@ import java.io.IOException;
  * associate a name with the query filter.
  */
 @Deprecated
-public class FQueryFilterParser extends BaseQueryParser {
+public class FQueryFilterParser implements QueryParser {
+
+    public static final String NAME = "fquery";
 
     @Inject
     public FQueryFilterParser() {
@@ -37,17 +41,16 @@ public class FQueryFilterParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{FQueryFilterBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        QueryBuilder wrappedQuery = null;
+        Query query = null;
         boolean queryFound = false;
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         String queryName = null;
         String currentFieldName = null;
         XContentParser.Token token;
@@ -59,15 +62,13 @@ public class FQueryFilterParser extends BaseQueryParser {
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("query".equals(currentFieldName)) {
                     queryFound = true;
-                    wrappedQuery = parseContext.parseInnerQueryBuilder();
+                    query = parseContext.parseInnerQuery();
                 } else {
                     throw new QueryParsingException(parseContext, "[fquery] query does not support [" + currentFieldName + "]");
                 }
             } else if (token.isValue()) {
                 if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else {
                     throw new QueryParsingException(parseContext, "[fquery] query does not support [" + currentFieldName + "]");
                 }
@@ -76,14 +77,13 @@ public class FQueryFilterParser extends BaseQueryParser {
         if (!queryFound) {
             throw new QueryParsingException(parseContext, "[fquery] requires 'query' element");
         }
-        FQueryFilterBuilder queryBuilder = new FQueryFilterBuilder(wrappedQuery);
-        queryBuilder.queryName(queryName);
-        queryBuilder.boost(boost);
-        return queryBuilder;
-    }
-
-    @Override
-    public FQueryFilterBuilder getBuilderPrototype() {
-        return FQueryFilterBuilder.PROTOTYPE;
+        if (query == null) {
+            return null;
+        }
+        query = new ConstantScoreQuery(query);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java
index a577225..c118416 100644
--- a/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java
@@ -19,113 +19,52 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.FieldMaskingSpanQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
 
 import java.io.IOException;
-import java.util.Objects;
 
-public class FieldMaskingSpanQueryBuilder extends AbstractQueryBuilder<FieldMaskingSpanQueryBuilder> implements SpanQueryBuilder<FieldMaskingSpanQueryBuilder>{
-
-    public static final String NAME = "field_masking_span";
+public class FieldMaskingSpanQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<FieldMaskingSpanQueryBuilder> {
 
     private final SpanQueryBuilder queryBuilder;
 
-    private final String fieldName;
+    private final String field;
 
-    static final FieldMaskingSpanQueryBuilder PROTOTYPE = new FieldMaskingSpanQueryBuilder(null, null);
+    private float boost = -1;
 
-    /**
-     * Constructs a new {@link FieldMaskingSpanQueryBuilder} given an inner {@link SpanQueryBuilder} for
-     * a given field
-     * @param queryBuilder inner {@link SpanQueryBuilder}
-     * @param fieldName the field name
-     */
-    public FieldMaskingSpanQueryBuilder(SpanQueryBuilder queryBuilder, String fieldName) {
+    private String queryName;
+
+
+    public FieldMaskingSpanQueryBuilder(SpanQueryBuilder queryBuilder, String field) {
         this.queryBuilder = queryBuilder;
-        this.fieldName = fieldName;
+        this.field = field;
     }
 
-    /**
-     * @return the field name for this query
-     */
-    public String fieldName() {
-        return this.fieldName;
+    @Override
+    public FieldMaskingSpanQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
-     * @return the inner {@link QueryBuilder}
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public SpanQueryBuilder innerQuery() {
-        return this.queryBuilder;
+    public FieldMaskingSpanQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(FieldMaskingSpanQueryParser.NAME);
         builder.field("query");
         queryBuilder.toXContent(builder, params);
-        builder.field("field", fieldName);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected SpanQuery doToQuery(QueryShardContext context) throws IOException {
-        String fieldInQuery = fieldName;
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType != null) {
-            fieldInQuery = fieldType.names().indexName();
-        }
-        Query innerQuery = queryBuilder.toQuery(context);
-        assert innerQuery instanceof SpanQuery;
-        return new FieldMaskingSpanQuery((SpanQuery)innerQuery, fieldInQuery);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (queryBuilder == null) {
-            validationException = addValidationError("inner clause [query] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(queryBuilder, validationException);
+        builder.field("field", field);
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        if (fieldName == null || fieldName.isEmpty()) {
-            validationException = addValidationError("field name is null or empty", validationException);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return validationException;
-    }
-
-    @Override
-    protected FieldMaskingSpanQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryBuilder innerQueryBuilder = in.readQuery();
-        return new FieldMaskingSpanQueryBuilder((SpanQueryBuilder) innerQueryBuilder, in.readString());
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(queryBuilder);
-        out.writeString(fieldName);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(queryBuilder, fieldName);
-    }
-
-    @Override
-    protected boolean doEquals(FieldMaskingSpanQueryBuilder other) {
-        return Objects.equals(queryBuilder, other.queryBuilder) &&
-               Objects.equals(fieldName, other.fieldName);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java
index 87bf183..2980be1 100644
--- a/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java
@@ -19,15 +19,23 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.FieldMaskingSpanQuery;
+import org.apache.lucene.search.spans.SpanQuery;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.FieldMapper;
+import org.elasticsearch.index.mapper.MappedFieldType;
+
 import java.io.IOException;
 
 /**
  *
  */
-public class FieldMaskingSpanQueryParser extends BaseQueryParser {
+public class FieldMaskingSpanQueryParser implements QueryParser {
+
+    public static final String NAME = "field_masking_span";
 
     @Inject
     public FieldMaskingSpanQueryParser() {
@@ -35,16 +43,16 @@ public class FieldMaskingSpanQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{FieldMaskingSpanQueryBuilder.NAME, Strings.toCamelCase(FieldMaskingSpanQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
 
-        SpanQueryBuilder inner = null;
+        SpanQuery inner = null;
         String field = null;
         String queryName = null;
 
@@ -55,11 +63,11 @@ public class FieldMaskingSpanQueryParser extends BaseQueryParser {
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("query".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (!(query instanceof SpanQueryBuilder)) {
-                        throw new QueryParsingException(parseContext, "[field_masking_span] query must be of type span query");
+                    Query query = parseContext.parseInnerQuery();
+                    if (!(query instanceof SpanQuery)) {
+                        throw new QueryParsingException(parseContext, "[field_masking_span] query] must be of type span query");
                     }
-                    inner = (SpanQueryBuilder) query;
+                    inner = (SpanQuery) query;
                 } else {
                     throw new QueryParsingException(parseContext, "[field_masking_span] query does not support ["
                             + currentFieldName + "]");
@@ -83,14 +91,16 @@ public class FieldMaskingSpanQueryParser extends BaseQueryParser {
             throw new QueryParsingException(parseContext, "field_masking_span must have [field] set for it");
         }
 
-        FieldMaskingSpanQueryBuilder queryBuilder = new FieldMaskingSpanQueryBuilder(inner, field);
-        queryBuilder.boost(boost);
-        queryBuilder.queryName(queryName);
-        return queryBuilder;
-    }
+        MappedFieldType fieldType = parseContext.fieldMapper(field);
+        if (fieldType != null) {
+            field = fieldType.names().indexName();
+        }
 
-    @Override
-    public FieldMaskingSpanQueryBuilder getBuilderPrototype() {
-        return FieldMaskingSpanQueryBuilder.PROTOTYPE;
+        FieldMaskingSpanQuery query = new FieldMaskingSpanQuery(inner, field);
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java
index b36e541..93507cf 100644
--- a/core/src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java
@@ -19,140 +19,72 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
+import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * A query that applies a filter to the results of another query.
  * @deprecated Use {@link BoolQueryBuilder} instead.
  */
 @Deprecated
-public class FilteredQueryBuilder extends AbstractQueryBuilder<FilteredQueryBuilder> {
+public class FilteredQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<FilteredQueryBuilder> {
 
-    /** Name of the query in the REST API. */
-    public static final String NAME = "filtered";
-    /** The query to filter. */
     private final QueryBuilder queryBuilder;
-    /** The filter to apply to the query. */
+
     private final QueryBuilder filterBuilder;
 
-    static final FilteredQueryBuilder PROTOTYPE = new FilteredQueryBuilder(null, null);
+    private float boost = -1;
 
-    /**
-     * Returns a {@link MatchAllQueryBuilder} instance that will be used as
-     * default queryBuilder if none is supplied by the user. Feel free to
-     * set queryName and boost on that instance - it's always a new one.
-     * */
-    private static QueryBuilder generateDefaultQuery() {
-        return new MatchAllQueryBuilder();
-    }
-
-    /**
-     * A query that applies a filter to the results of a match_all query.
-     * @param filterBuilder The filter to apply on the query (Can be null)
-     * */
-    public FilteredQueryBuilder(QueryBuilder filterBuilder) {
-        this(generateDefaultQuery(), filterBuilder);
-    }
+    private String queryName;
 
     /**
      * A query that applies a filter to the results of another query.
      *
-     * @param queryBuilder  The query to apply the filter to
+     * @param queryBuilder  The query to apply the filter to (Can be null)
      * @param filterBuilder The filter to apply on the query (Can be null)
      */
-    public FilteredQueryBuilder(QueryBuilder queryBuilder, QueryBuilder filterBuilder) {
-        this.queryBuilder = (queryBuilder != null) ? queryBuilder : generateDefaultQuery();
-        this.filterBuilder = (filterBuilder != null) ? filterBuilder : EmptyQueryBuilder.PROTOTYPE;
-    }
-
-    /** Returns the query to apply the filter to. */
-    public QueryBuilder query() {
-        return queryBuilder;
-    }
-
-    /** Returns the filter to apply to the query results. */
-    public QueryBuilder filter() {
-        return filterBuilder;
+    public FilteredQueryBuilder(@Nullable QueryBuilder queryBuilder, @Nullable QueryBuilder filterBuilder) {
+        this.queryBuilder = queryBuilder;
+        this.filterBuilder = filterBuilder;
     }
 
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
     @Override
-    protected boolean doEquals(FilteredQueryBuilder other) {
-        return Objects.equals(queryBuilder, other.queryBuilder) &&
-                Objects.equals(filterBuilder, other.filterBuilder);
+    public FilteredQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
-    @Override
-    public int doHashCode() {
-        return Objects.hash(queryBuilder, filterBuilder);
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public FilteredQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
-    public Query doToQuery(QueryShardContext context) throws QueryShardException, IOException {
-        Query query = queryBuilder.toQuery(context);
-        Query filter = filterBuilder.toQuery(context);
-
-        if (query == null) {
-            // Most likely this query was generated from the JSON query DSL - it parsed to an EmptyQueryBuilder so we ignore
-            // the whole filtered query as there is nothing to filter on. See FilteredQueryParser for an example.
-            return null;
+    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(FilteredQueryParser.NAME);
+        if (queryBuilder != null) {
+            builder.field("query");
+            queryBuilder.toXContent(builder, params);
         }
-
-        if (filter == null || Queries.isConstantMatchAllQuery(filter)) {
-            // no filter, or match all filter
-            return query;
-        } else if (Queries.isConstantMatchAllQuery(query)) {
-            // if its a match_all query, use constant_score
-            return new ConstantScoreQuery(filter);
+        if (filterBuilder != null) {
+            builder.field("filter");
+            filterBuilder.toXContent(builder, params);
+        }
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-
-        // use a BooleanQuery
-        return Queries.filtered(query, filter);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        validationException = validateInnerQuery(queryBuilder, validationException);
-        validationException = validateInnerQuery(filterBuilder, validationException);
-        return validationException;
-
-    }
-
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("query");
-        queryBuilder.toXContent(builder, params);
-        builder.field("filter");
-        filterBuilder.toXContent(builder, params);
-        printBoostAndQueryName(builder);
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    public FilteredQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryBuilder query = in.readQuery();
-        QueryBuilder filter = in.readQuery();
-        FilteredQueryBuilder qb = new FilteredQueryBuilder(query, filter);
-        return qb;
-    }
-
-    @Override
-    public void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(queryBuilder);
-        out.writeQuery(filterBuilder);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java
index 3653d23..774ff74 100644
--- a/core/src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java
@@ -19,17 +19,23 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
 
 /**
- * @deprecated Use {@link BoolQueryParser} instead.
+ *
  */
-
 @Deprecated
-public class FilteredQueryParser extends BaseQueryParser {
+public class FilteredQueryParser implements QueryParser {
+
+    public static final String NAME = "filtered";
 
     @Inject
     public FilteredQueryParser() {
@@ -37,16 +43,17 @@ public class FilteredQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{FilteredQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        QueryBuilder query = null;
-        QueryBuilder filter = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        Query query = Queries.newMatchAllQuery();
+        Query filter = null;
+        boolean filterFound = false;
+        float boost = 1.0f;
         String queryName = null;
 
         String currentFieldName = null;
@@ -59,9 +66,10 @@ public class FilteredQueryParser extends BaseQueryParser {
                 // skip
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("query".equals(currentFieldName)) {
-                    query = parseContext.parseInnerQueryBuilder();
+                    query = parseContext.parseInnerQuery();
                 } else if ("filter".equals(currentFieldName)) {
-                    filter = parseContext.parseInnerFilterToQueryBuilder();
+                    filterFound = true;
+                    filter = parseContext.parseInnerFilter();
                 } else {
                     throw new QueryParsingException(parseContext, "[filtered] query does not support [" + currentFieldName + "]");
                 }
@@ -78,15 +86,39 @@ public class FilteredQueryParser extends BaseQueryParser {
             }
         }
 
-        FilteredQueryBuilder qb = new FilteredQueryBuilder(query, filter);
-        qb.boost(boost);
-        qb.queryName(queryName);
-        return qb;
-    }
+        // parsed internally, but returned null during parsing...
+        if (query == null) {
+            return null;
+        }
 
-    @Override
-    public FilteredQueryBuilder getBuilderPrototype() {
-        return FilteredQueryBuilder.PROTOTYPE;
-    }
+        if (filter == null) {
+            if (!filterFound) {
+                // we allow for null filter, so it makes compositions on the client side to be simpler
+                return query;
+            } else {
+                // even if the filter is not found, and its null, we should simply ignore it, and go
+                // by the query
+                return query;
+            }
+        }
+        if (Queries.isConstantMatchAllQuery(filter)) {
+            // this is an instance of match all filter, just execute the query
+            return query;
+        }
+
+        // if its a match_all query, use constant_score
+        if (Queries.isConstantMatchAllQuery(query)) {
+            Query q = new ConstantScoreQuery(filter);
+            q.setBoost(boost);
+            return q;
+        }
 
+        BooleanQuery filteredQuery = Queries.filtered(query, filter);
+
+        filteredQuery.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, filteredQuery);
+        }
+        return filteredQuery;
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java
index 56ff392..23557b1 100644
--- a/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java
@@ -27,26 +27,26 @@ import java.io.IOException;
 /**
  * A Query that does fuzzy matching for a specific value.
  */
-public class FuzzyQueryBuilder extends AbstractQueryBuilder<FuzzyQueryBuilder> implements MultiTermQueryBuilder<FuzzyQueryBuilder> {
-
-    public static final String NAME = "fuzzy";
+public class FuzzyQueryBuilder extends MultiTermQueryBuilder implements BoostableQueryBuilder<FuzzyQueryBuilder> {
 
     private final String name;
 
     private final Object value;
 
+    private float boost = -1;
+
     private Fuzziness fuzziness;
 
     private Integer prefixLength;
 
     private Integer maxExpansions;
-
+    
     //LUCENE 4 UPGRADE  we need a testcase for this + documentation
     private Boolean transpositions;
 
     private String rewrite;
 
-    static final FuzzyQueryBuilder PROTOTYPE = new FuzzyQueryBuilder(null, null);
+    private String queryName;
 
     /**
      * Constructs a new fuzzy query.
@@ -120,6 +120,16 @@ public class FuzzyQueryBuilder extends AbstractQueryBuilder<FuzzyQueryBuilder> i
         this(name, (Object) value);
     }
 
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
+    @Override
+    public FuzzyQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
     public FuzzyQueryBuilder fuzziness(Fuzziness fuzziness) {
         this.fuzziness = fuzziness;
         return this;
@@ -134,7 +144,7 @@ public class FuzzyQueryBuilder extends AbstractQueryBuilder<FuzzyQueryBuilder> i
         this.maxExpansions = maxExpansions;
         return this;
     }
-
+    
     public FuzzyQueryBuilder transpositions(boolean transpositions) {
       this.transpositions = transpositions;
       return this;
@@ -145,11 +155,22 @@ public class FuzzyQueryBuilder extends AbstractQueryBuilder<FuzzyQueryBuilder> i
         return this;
     }
 
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public FuzzyQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
     @Override
     public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(FuzzyQueryParser.NAME);
         builder.startObject(name);
         builder.field("value", value);
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
         if (transpositions != null) {
             builder.field("transpositions", transpositions);
         }
@@ -165,13 +186,10 @@ public class FuzzyQueryBuilder extends AbstractQueryBuilder<FuzzyQueryBuilder> i
         if (rewrite != null) {
             builder.field("rewrite", rewrite);
         }
-        printBoostAndQueryName(builder);
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         builder.endObject();
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java
index ec757fd..aefdb4c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java
@@ -36,8 +36,9 @@ import java.io.IOException;
 /**
  *
  */
-public class FuzzyQueryParser extends BaseQueryParserTemp {
+public class FuzzyQueryParser implements QueryParser {
 
+    public static final String NAME = "fuzzy";
     private static final Fuzziness DEFAULT_FUZZINESS = Fuzziness.AUTO;
     private static final ParseField FUZZINESS = Fuzziness.FIELD.withDeprecation("min_similarity");
 
@@ -48,12 +49,11 @@ public class FuzzyQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{FuzzyQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         XContentParser.Token token = parser.nextToken();
@@ -63,7 +63,7 @@ public class FuzzyQueryParser extends BaseQueryParserTemp {
         String fieldName = parser.currentName();
 
         Object value = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         Fuzziness fuzziness = DEFAULT_FUZZINESS;
         int prefixLength = FuzzyQuery.defaultPrefixLength;
         int maxExpansions = FuzzyQuery.defaultMaxExpansions;
@@ -113,9 +113,9 @@ public class FuzzyQueryParser extends BaseQueryParserTemp {
         if (value == null) {
             throw new QueryParsingException(parseContext, "No value specified for fuzzy query");
         }
-
+        
         Query query = null;
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
         if (fieldType != null) {
             query = fieldType.fuzzyQuery(value, fuzziness, prefixLength, maxExpansions, transpositions);
         }
@@ -129,13 +129,8 @@ public class FuzzyQueryParser extends BaseQueryParserTemp {
         query.setBoost(boost);
 
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
         return query;
     }
-
-    @Override
-    public FuzzyQueryBuilder getBuilderPrototype() {
-        return FuzzyQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java
index 4e61528..9b376ca 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java
@@ -25,9 +25,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 
-public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBoundingBoxQueryBuilder> {
-
-    public static final String NAME = "geo_bbox";
+public class GeoBoundingBoxQueryBuilder extends QueryBuilder {
 
     public static final String TOP_LEFT = GeoBoundingBoxQueryParser.TOP_LEFT;
     public static final String BOTTOM_RIGHT = GeoBoundingBoxQueryParser.BOTTOM_RIGHT;
@@ -36,15 +34,14 @@ public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBounding
     private static final int LEFT = 1;
     private static final int BOTTOM = 2;
     private static final int RIGHT = 3;
-
+    
     private final String name;
 
     private double[] box = {Double.NaN, Double.NaN, Double.NaN, Double.NaN};
 
+    private String queryName;
     private String type;
 
-    static final GeoBoundingBoxQueryBuilder PROTOTYPE = new GeoBoundingBoxQueryBuilder(null);
-
     public GeoBoundingBoxQueryBuilder(String name) {
         this.name = name;
     }
@@ -108,7 +105,7 @@ public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBounding
     public GeoBoundingBoxQueryBuilder bottomLeft(String geohash) {
         return bottomLeft(GeoHashUtils.decode(geohash));
     }
-
+    
     /**
      * Adds top right point.
      *
@@ -130,6 +127,14 @@ public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBounding
     }
 
     /**
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public GeoBoundingBoxQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
+    /**
      * Sets the type of executing of the geo bounding box. Can be either `memory` or `indexed`. Defaults
      * to `memory`.
      */
@@ -150,25 +155,21 @@ public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBounding
         } else if(Double.isNaN(box[LEFT])) {
             throw new IllegalArgumentException("geo_bounding_box requires left longitude to be set");
         }
-
-        builder.startObject(NAME);
+                
+        builder.startObject(GeoBoundingBoxQueryParser.NAME);
 
         builder.startObject(name);
         builder.array(TOP_LEFT, box[LEFT], box[TOP]);
         builder.array(BOTTOM_RIGHT, box[RIGHT], box[BOTTOM]);
         builder.endObject();
 
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         if (type != null) {
             builder.field("type", type);
         }
 
-        printBoostAndQueryName(builder);
-
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java
index 6ec143d..8901257 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java
@@ -26,6 +26,7 @@ import org.elasticsearch.common.geo.GeoUtils;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
+import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
 import org.elasticsearch.index.search.geo.InMemoryGeoBoundingBoxQuery;
@@ -36,7 +37,7 @@ import java.io.IOException;
 /**
  *
  */
-public class GeoBoundingBoxQueryParser extends BaseQueryParserTemp {
+public class GeoBoundingBoxQueryParser implements QueryParser {
 
     public static final String NAME = "geo_bbox";
 
@@ -63,12 +64,11 @@ public class GeoBoundingBoxQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{GeoBoundingBoxQueryBuilder.NAME, "geoBbox", "geo_bounding_box", "geoBoundingBox"};
+        return new String[]{NAME, "geoBbox", "geo_bounding_box", "geoBoundingBox"};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldName = null;
@@ -77,15 +77,14 @@ public class GeoBoundingBoxQueryParser extends BaseQueryParserTemp {
         double bottom = Double.NaN;
         double left = Double.NaN;
         double right = Double.NaN;
-
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        
         String queryName = null;
         String currentFieldName = null;
         XContentParser.Token token;
         boolean normalize = true;
 
         GeoPoint sparse = new GeoPoint();
-
+        
         String type = "memory";
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -138,8 +137,6 @@ public class GeoBoundingBoxQueryParser extends BaseQueryParserTemp {
             } else if (token.isValue()) {
                 if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else if ("normalize".equals(currentFieldName)) {
                     normalize = parser.booleanValue();
                 } else if ("type".equals(currentFieldName)) {
@@ -154,7 +151,7 @@ public class GeoBoundingBoxQueryParser extends BaseQueryParserTemp {
         final GeoPoint bottomRight = new GeoPoint(bottom, right);
 
         if (normalize) {
-            // Special case: if the difference bettween the left and right is 360 and the right is greater than the left, we are asking for
+            // Special case: if the difference bettween the left and right is 360 and the right is greater than the left, we are asking for 
             // the complete longitude range so need to set longitude to the complete longditude range
             boolean completeLonRange = ((right - left) % 360 == 0 && right > left);
             GeoUtils.normalizePoint(topLeft, true, !completeLonRange);
@@ -165,7 +162,7 @@ public class GeoBoundingBoxQueryParser extends BaseQueryParserTemp {
             }
         }
 
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
         if (fieldType == null) {
             throw new QueryParsingException(parseContext, "failed to parse [{}] query. could not find [{}] field [{}]", NAME, GeoPointFieldMapper.CONTENT_TYPE, fieldName);
         }
@@ -178,22 +175,15 @@ public class GeoBoundingBoxQueryParser extends BaseQueryParserTemp {
         if ("indexed".equals(type)) {
             filter = IndexedGeoBoundingBoxQuery.create(topLeft, bottomRight, geoFieldType);
         } else if ("memory".equals(type)) {
-            IndexGeoPointFieldData indexFieldData = context.getForField(fieldType);
+            IndexGeoPointFieldData indexFieldData = parseContext.getForField(fieldType);
             filter = new InMemoryGeoBoundingBoxQuery(topLeft, bottomRight, indexFieldData);
         } else {
             throw new QueryParsingException(parseContext, "failed to parse [{}] query. geo bounding box type [{}] is not supported. either [indexed] or [memory] are allowed", NAME, type);
         }
-        if (filter != null) {
-            filter.setBoost(boost);
-        }
+
         if (queryName != null) {
-            context.addNamedQuery(queryName, filter);
+            parseContext.addNamedQuery(queryName, filter);
         }
         return filter;
-    }
-
-    @Override
-    public GeoBoundingBoxQueryBuilder getBuilderPrototype() {
-        return GeoBoundingBoxQueryBuilder.PROTOTYPE;
-    }
+    }    
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java
index 895679a..0995a5e 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java
@@ -26,9 +26,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import java.io.IOException;
 import java.util.Locale;
 
-public class GeoDistanceQueryBuilder extends AbstractQueryBuilder<GeoDistanceQueryBuilder> {
-
-    public static final String NAME = "geo_distance";
+public class GeoDistanceQueryBuilder extends QueryBuilder {
 
     private final String name;
 
@@ -44,7 +42,7 @@ public class GeoDistanceQueryBuilder extends AbstractQueryBuilder<GeoDistanceQue
 
     private String optimizeBbox;
 
-    static final GeoDistanceQueryBuilder PROTOTYPE = new GeoDistanceQueryBuilder(null);
+    private String queryName;
 
     public GeoDistanceQueryBuilder(String name) {
         this.name = name;
@@ -91,9 +89,17 @@ public class GeoDistanceQueryBuilder extends AbstractQueryBuilder<GeoDistanceQue
         return this;
     }
 
+    /**
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public GeoDistanceQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(GeoDistanceQueryParser.NAME);
         if (geohash != null) {
             builder.field(name, geohash);
         } else {
@@ -106,12 +112,9 @@ public class GeoDistanceQueryBuilder extends AbstractQueryBuilder<GeoDistanceQue
         if (optimizeBbox != null) {
             builder.field("optimize_bbox", optimizeBbox);
         }
-        printBoostAndQueryName(builder);
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java
index 77df31d..b785625 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java
@@ -28,6 +28,7 @@ import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.unit.DistanceUnit;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
+import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
 import org.elasticsearch.index.search.geo.GeoDistanceRangeQuery;
@@ -42,7 +43,9 @@ import java.io.IOException;
  * }
  * </pre>
  */
-public class GeoDistanceQueryParser extends BaseQueryParserTemp {
+public class GeoDistanceQueryParser implements QueryParser {
+
+    public static final String NAME = "geo_distance";
 
     @Inject
     public GeoDistanceQueryParser() {
@@ -50,17 +53,15 @@ public class GeoDistanceQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{GeoDistanceQueryBuilder.NAME, "geoDistance"};
+        return new String[]{NAME, "geoDistance"};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         XContentParser.Token token;
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         String queryName = null;
         String currentFieldName = null;
         GeoPoint point = new GeoPoint();
@@ -122,8 +123,6 @@ public class GeoDistanceQueryParser extends BaseQueryParserTemp {
                     fieldName = currentFieldName.substring(0, currentFieldName.length() - GeoPointFieldMapper.Names.GEOHASH_SUFFIX.length());
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else if ("optimize_bbox".equals(currentFieldName) || "optimizeBbox".equals(currentFieldName)) {
                     optimizeBbox = parser.textOrNull();
                 } else if ("normalize".equals(currentFieldName)) {
@@ -149,7 +148,7 @@ public class GeoDistanceQueryParser extends BaseQueryParserTemp {
             GeoUtils.normalizePoint(point, normalizeLat, normalizeLon);
         }
 
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
         if (fieldType == null) {
             throw new QueryParsingException(parseContext, "failed to find geo_point field [" + fieldName + "]");
         }
@@ -159,17 +158,11 @@ public class GeoDistanceQueryParser extends BaseQueryParserTemp {
         GeoPointFieldMapper.GeoPointFieldType geoFieldType = ((GeoPointFieldMapper.GeoPointFieldType) fieldType);
 
 
-        IndexGeoPointFieldData indexFieldData = context.getForField(fieldType);
+        IndexGeoPointFieldData indexFieldData = parseContext.getForField(fieldType);
         Query query = new GeoDistanceRangeQuery(point, null, distance, true, false, geoDistance, geoFieldType, indexFieldData, optimizeBbox);
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
-        query.setBoost(boost);
         return query;
     }
-
-    @Override
-    public GeoDistanceQueryBuilder getBuilderPrototype() {
-        return GeoDistanceQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java
index b97263a..d21810f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java
@@ -25,9 +25,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import java.io.IOException;
 import java.util.Locale;
 
-public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistanceRangeQueryBuilder> {
-
-    public static final String NAME = "geo_distance_range";
+public class GeoDistanceRangeQueryBuilder extends QueryBuilder {
 
     private final String name;
 
@@ -44,9 +42,9 @@ public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistan
 
     private GeoDistance geoDistance;
 
-    private String optimizeBbox;
+    private String queryName;
 
-    static final GeoDistanceRangeQueryBuilder PROTOTYPE = new GeoDistanceRangeQueryBuilder(null);
+    private String optimizeBbox;
 
     public GeoDistanceRangeQueryBuilder(String name) {
         this.name = name;
@@ -127,9 +125,17 @@ public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistan
         return this;
     }
 
+    /**
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public GeoDistanceRangeQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(GeoDistanceRangeQueryParser.NAME);
         if (geohash != null) {
             builder.field(name, geohash);
         } else {
@@ -145,12 +151,9 @@ public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistan
         if (optimizeBbox != null) {
             builder.field("optimize_bbox", optimizeBbox);
         }
-        printBoostAndQueryName(builder);
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java
index 63d11ff..6c8479b 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java
@@ -28,6 +28,7 @@ import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.unit.DistanceUnit;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
+import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
 import org.elasticsearch.index.search.geo.GeoDistanceRangeQuery;
@@ -42,7 +43,9 @@ import java.io.IOException;
  * }
  * </pre>
  */
-public class GeoDistanceRangeQueryParser extends BaseQueryParserTemp {
+public class GeoDistanceRangeQueryParser implements QueryParser {
+
+    public static final String NAME = "geo_distance_range";
 
     @Inject
     public GeoDistanceRangeQueryParser() {
@@ -50,17 +53,15 @@ public class GeoDistanceRangeQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{GeoDistanceRangeQueryBuilder.NAME, "geoDistanceRange"};
+        return new String[]{NAME, "geoDistanceRange"};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         XContentParser.Token token;
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         String queryName = null;
         String currentFieldName = null;
         GeoPoint point = new GeoPoint();
@@ -152,8 +153,6 @@ public class GeoDistanceRangeQueryParser extends BaseQueryParserTemp {
                     fieldName = currentFieldName.substring(0, currentFieldName.length() - GeoPointFieldMapper.Names.GEOHASH_SUFFIX.length());
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else if ("optimize_bbox".equals(currentFieldName) || "optimizeBbox".equals(currentFieldName)) {
                     optimizeBbox = parser.textOrNull();
                 } else if ("normalize".equals(currentFieldName)) {
@@ -189,7 +188,7 @@ public class GeoDistanceRangeQueryParser extends BaseQueryParserTemp {
             GeoUtils.normalizePoint(point, normalizeLat, normalizeLon);
         }
 
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
         if (fieldType == null) {
             throw new QueryParsingException(parseContext, "failed to find geo_point field [" + fieldName + "]");
         }
@@ -198,17 +197,11 @@ public class GeoDistanceRangeQueryParser extends BaseQueryParserTemp {
         }
         GeoPointFieldMapper.GeoPointFieldType geoFieldType = ((GeoPointFieldMapper.GeoPointFieldType) fieldType);
 
-        IndexGeoPointFieldData indexFieldData = context.getForField(fieldType);
+        IndexGeoPointFieldData indexFieldData = parseContext.getForField(fieldType);
         Query query = new GeoDistanceRangeQuery(point, from, to, includeLower, includeUpper, geoDistance, geoFieldType, indexFieldData, optimizeBbox);
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
-        query.setBoost(boost);
         return query;
     }
-
-    @Override
-    public GeoDistanceRangeQueryBuilder getBuilderPrototype() {
-        return GeoDistanceRangeQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java
index 3ce56bb..4fd2f41 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java
@@ -28,17 +28,15 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import java.io.IOException;
 import java.util.List;
 
-public class GeoPolygonQueryBuilder extends AbstractQueryBuilder<GeoPolygonQueryBuilder> {
-
-    public static final String NAME = "geo_polygon";
+public class GeoPolygonQueryBuilder extends QueryBuilder {
 
     public static final String POINTS = GeoPolygonQueryParser.POINTS;
-
+    
     private final String name;
 
     private final List<GeoPoint> shell = Lists.newArrayList();
 
-    static final GeoPolygonQueryBuilder PROTOTYPE = new GeoPolygonQueryBuilder(null);
+    private String queryName;
 
     public GeoPolygonQueryBuilder(String name) {
         this.name = name;
@@ -49,7 +47,7 @@ public class GeoPolygonQueryBuilder extends AbstractQueryBuilder<GeoPolygonQuery
      *
      * @param lat The latitude
      * @param lon The longitude
-     * @return the current builder
+     * @return
      */
     public GeoPolygonQueryBuilder addPoint(double lat, double lon) {
         return addPoint(new GeoPoint(lat, lon));
@@ -63,10 +61,18 @@ public class GeoPolygonQueryBuilder extends AbstractQueryBuilder<GeoPolygonQuery
         shell.add(point);
         return this;
     }
+    
+    /**
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public GeoPolygonQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(GeoPolygonQueryParser.NAME);
 
         builder.startObject(name);
         builder.startArray(POINTS);
@@ -76,13 +82,10 @@ public class GeoPolygonQueryBuilder extends AbstractQueryBuilder<GeoPolygonQuery
         builder.endArray();
         builder.endObject();
 
-        printBoostAndQueryName(builder);
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
 
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java
index f36c6d7..43d3686 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.index.query;
 
 import com.google.common.collect.Lists;
+
 import org.apache.lucene.search.Query;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.geo.GeoUtils;
@@ -27,6 +28,7 @@ import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.XContentParser.Token;
 import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
+import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
 import org.elasticsearch.index.search.geo.GeoPolygonQuery;
@@ -46,8 +48,9 @@ import java.util.List;
  * }
  * </pre>
  */
-public class GeoPolygonQueryParser extends BaseQueryParserTemp {
+public class GeoPolygonQueryParser implements QueryParser {
 
+    public static final String NAME = "geo_polygon";
     public static final String POINTS = "points";
 
     @Inject
@@ -56,12 +59,11 @@ public class GeoPolygonQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{GeoPolygonQueryBuilder.NAME, "geoPolygon"};
+        return new String[]{NAME, "geoPolygon"};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldName = null;
@@ -71,7 +73,6 @@ public class GeoPolygonQueryParser extends BaseQueryParserTemp {
         boolean normalizeLon = true;
         boolean normalizeLat = true;
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         String queryName = null;
         String currentFieldName = null;
         XContentParser.Token token;
@@ -107,8 +108,6 @@ public class GeoPolygonQueryParser extends BaseQueryParserTemp {
             } else if (token.isValue()) {
                 if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else if ("normalize".equals(currentFieldName)) {
                     normalizeLat = parser.booleanValue();
                     normalizeLon = parser.booleanValue();
@@ -141,7 +140,7 @@ public class GeoPolygonQueryParser extends BaseQueryParserTemp {
             }
         }
 
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
         if (fieldType == null) {
             throw new QueryParsingException(parseContext, "failed to find geo_point field [" + fieldName + "]");
         }
@@ -149,17 +148,11 @@ public class GeoPolygonQueryParser extends BaseQueryParserTemp {
             throw new QueryParsingException(parseContext, "field [" + fieldName + "] is not a geo_point field");
         }
 
-        IndexGeoPointFieldData indexFieldData = context.getForField(fieldType);
+        IndexGeoPointFieldData indexFieldData = parseContext.getForField(fieldType);
         Query query = new GeoPolygonQuery(indexFieldData, shell.toArray(new GeoPoint[shell.size()]));
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
-        query.setBoost(boost);
         return query;
     }
-
-    @Override
-    public GeoPolygonQueryBuilder getBuilderPrototype() {
-        return GeoPolygonQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java
index 9180d0e..3887874 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java
@@ -29,11 +29,7 @@ import java.io.IOException;
 /**
  * {@link QueryBuilder} that builds a GeoShape Filter
  */
-public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuilder> {
-
-    public static final String NAME = "geo_shape";
-
-    static final GeoShapeQueryBuilder PROTOTYPE = new GeoShapeQueryBuilder(null, null);
+public class GeoShapeQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<GeoShapeQueryBuilder> {
 
     private final String name;
 
@@ -41,6 +37,8 @@ public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuil
 
     private SpatialStrategy strategy = null;
 
+    private String queryName;
+
     private final String indexedShapeId;
     private final String indexedShapeType;
 
@@ -49,6 +47,8 @@ public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuil
 
     private ShapeRelation relation = null;
 
+    private float boost = -1;
+    
     /**
      * Creates a new GeoShapeQueryBuilder whose Filter will be against the
      * given field name using the given Shape
@@ -93,6 +93,17 @@ public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuil
     }
 
     /**
+     * Sets the name of the filter
+     *
+     * @param queryName Name of the filter
+     * @return this
+     */
+    public GeoShapeQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
+    /**
      * Defines which spatial strategy will be used for building the geo shape filter. When not set, the strategy that
      * will be used will be the one that is associated with the geo shape field in the mappings.
      *
@@ -138,8 +149,14 @@ public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuil
     }
 
     @Override
+    public GeoShapeQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
+    @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(GeoShapeQueryParser.NAME);
 
         builder.startObject(name);
 
@@ -168,13 +185,14 @@ public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuil
 
         builder.endObject();
 
-        printBoostAndQueryName(builder);
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
+
+        if (name != null) {
+            builder.field("_name", queryName);
+        }
 
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java
index 693db14..286fa1c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java
@@ -31,6 +31,7 @@ import org.elasticsearch.common.geo.ShapeRelation;
 import org.elasticsearch.common.geo.builders.ShapeBuilder;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.geo.GeoShapeFieldMapper;
 import org.elasticsearch.index.search.shape.ShapeFetchService;
@@ -38,7 +39,9 @@ import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
-public class GeoShapeQueryParser extends BaseQueryParserTemp {
+public class GeoShapeQueryParser implements QueryParser {
+
+    public static final String NAME = "geo_shape";
 
     private ShapeFetchService fetchService;
 
@@ -49,12 +52,11 @@ public class GeoShapeQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{GeoShapeQueryBuilder.NAME, Strings.toCamelCase(GeoShapeQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldName = null;
@@ -137,7 +139,7 @@ public class GeoShapeQueryParser extends BaseQueryParserTemp {
             throw new QueryParsingException(parseContext, "No Shape Relation defined");
         }
 
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
         if (fieldType == null) {
             throw new QueryParsingException(parseContext, "Failed to find geo_shape field [" + fieldName + "]");
         }
@@ -158,7 +160,7 @@ public class GeoShapeQueryParser extends BaseQueryParserTemp {
             // this strategy doesn't support disjoint anymore: but it did before, including creating lucene fieldcache (!)
             // in this case, execute disjoint as exists && !intersects
             BooleanQuery bool = new BooleanQuery();
-            Query exists = ExistsQueryBuilder.newFilter(context, fieldName);
+            Query exists = ExistsQueryParser.newFilter(parseContext, fieldName, null);
             Filter intersects = strategy.makeFilter(getArgs(shape, ShapeRelation.INTERSECTS));
             bool.add(exists, BooleanClause.Occur.MUST);
             bool.add(intersects, BooleanClause.Occur.MUST_NOT);
@@ -168,7 +170,7 @@ public class GeoShapeQueryParser extends BaseQueryParserTemp {
         }
         query.setBoost(boost);
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
         return query;
     }
@@ -188,11 +190,7 @@ public class GeoShapeQueryParser extends BaseQueryParserTemp {
             return new SpatialArgs(SpatialOperation.IsWithin, shape.build());
         default:
             throw new IllegalArgumentException("");
-        }
-    }
 
-    @Override
-    public GeoShapeQueryBuilder getBuilderPrototype() {
-        return GeoShapeQueryBuilder.PROTOTYPE;
+        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java b/core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java
index 5ea66b9..814aca4 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java
@@ -31,7 +31,9 @@ import org.elasticsearch.common.unit.DistanceUnit;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.XContentParser.Token;
+import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.core.StringFieldMapper;
 import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
 
 import java.io.IOException;
@@ -69,7 +71,7 @@ public class GeohashCellQuery {
      * @param geohashes   optional array of additional geohashes
      * @return a new GeoBoundinboxfilter
      */
-    public static Query create(QueryShardContext context, GeoPointFieldMapper.GeoPointFieldType fieldType, String geohash, @Nullable List<CharSequence> geohashes) {
+    public static Query create(QueryParseContext context, GeoPointFieldMapper.GeoPointFieldType fieldType, String geohash, @Nullable List<CharSequence> geohashes) {
         MappedFieldType geoHashMapper = fieldType.geohashFieldType();
         if (geoHashMapper == null) {
             throw new IllegalArgumentException("geohash filter needs geohash_prefix to be enabled");
@@ -88,7 +90,7 @@ public class GeohashCellQuery {
      * <code>geohash</code> to be set. the default for a neighbor filteing is
      * <code>false</code>.
      */
-    public static class Builder extends AbstractQueryBuilder<Builder> {
+    public static class Builder extends QueryBuilder {
         // we need to store the geohash rather than the corresponding point,
         // because a transformation from a geohash to a point an back to the
         // geohash will extend the accuracy of the hash to max precision
@@ -97,7 +99,6 @@ public class GeohashCellQuery {
         private String geohash;
         private int levels = -1;
         private boolean neighbors;
-        private static final Builder PROTOTYPE = new Builder(null);
 
 
         public Builder(String field) {
@@ -164,17 +165,12 @@ public class GeohashCellQuery {
                 builder.field(PRECISION, levels);
             }
             builder.field(field, geohash);
-            printBoostAndQueryName(builder);
-            builder.endObject();
-        }
 
-        @Override
-        public String getWriteableName() {
-            return NAME;
+            builder.endObject();
         }
     }
 
-    public static class Parser extends BaseQueryParserTemp {
+    public static class Parser implements QueryParser {
 
         @Inject
         public Parser() {
@@ -186,16 +182,14 @@ public class GeohashCellQuery {
         }
 
         @Override
-        public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-            QueryParseContext parseContext = context.parseContext();
+        public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
             XContentParser parser = parseContext.parser();
 
             String fieldName = null;
             String geohash = null;
             int levels = -1;
             boolean neighbors = false;
-            String queryName = null;
-            float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+
 
             XContentParser.Token token;
             if ((token = parser.currentToken()) != Token.START_OBJECT) {
@@ -219,17 +213,11 @@ public class GeohashCellQuery {
                     } else if (NEIGHBORS.equals(field)) {
                         parser.nextToken();
                         neighbors = parser.booleanValue();
-                    } else if ("_name".equals(field)) {
-                        parser.nextToken();
-                        queryName = parser.text();
-                    } else if ("boost".equals(field)) {
-                        parser.nextToken();
-                        boost = parser.floatValue();
                     } else {
                         fieldName = field;
                         token = parser.nextToken();
                         if(token == Token.VALUE_STRING) {
-                            // A string indicates either a geohash or a lat/lon string
+                            // A string indicates either a gehash or a lat/lon string
                             String location = parser.text();
                             if(location.indexOf(",")>0) {
                                 geohash = GeoUtils.parseGeoPoint(parser).geohash();
@@ -249,7 +237,7 @@ public class GeohashCellQuery {
                 throw new QueryParsingException(parseContext, "failed to parse [{}] query. missing geohash value", NAME);
             }
 
-            MappedFieldType fieldType = context.fieldMapper(fieldName);
+            MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
             if (fieldType == null) {
                 throw new QueryParsingException(parseContext, "failed to parse [{}] query. missing [{}] field [{}]", NAME, GeoPointFieldMapper.CONTENT_TYPE, fieldName);
             }
@@ -270,22 +258,12 @@ public class GeohashCellQuery {
 
             Query filter;
             if (neighbors) {
-                filter = create(context, geoFieldType, geohash, GeoHashUtils.addNeighbors(geohash, new ArrayList<CharSequence>(8)));
+                filter = create(parseContext, geoFieldType, geohash, GeoHashUtils.addNeighbors(geohash, new ArrayList<CharSequence>(8)));
             } else {
-                filter = create(context, geoFieldType, geohash, null);
+                filter = create(parseContext, geoFieldType, geohash, null);
             }
-            if (queryName != null) {
-                context.addNamedQuery(queryName, filter);
-            }
-            if (filter != null) {
-                filter.setBoost(boost);
-            }
-            return filter;
-        }
 
-        @Override
-        public GeohashCellQuery.Builder getBuilderPrototype() {
-            return Builder.PROTOTYPE;
+            return filter;
         }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java
index 64b852d..74a6a5c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java
@@ -23,14 +23,14 @@ import org.elasticsearch.index.query.support.QueryInnerHitBuilder;
 
 import java.io.IOException;
 
-public class HasChildQueryBuilder extends AbstractQueryBuilder<HasChildQueryBuilder> {
-
-    public static final String NAME = "has_child";
+public class HasChildQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<HasChildQueryBuilder> {
 
     private final QueryBuilder queryBuilder;
 
     private String childType;
 
+    private float boost = 1.0f;
+
     private String scoreType;
 
     private Integer minChildren;
@@ -39,9 +39,9 @@ public class HasChildQueryBuilder extends AbstractQueryBuilder<HasChildQueryBuil
 
     private Integer shortCircuitCutoff;
 
-    private QueryInnerHitBuilder innerHit = null;
+    private String queryName;
 
-    static final HasChildQueryBuilder PROTOTYPE = new HasChildQueryBuilder(null, null);
+    private QueryInnerHitBuilder innerHit = null;
 
     public HasChildQueryBuilder(String type, QueryBuilder queryBuilder) {
         this.childType = type;
@@ -49,6 +49,16 @@ public class HasChildQueryBuilder extends AbstractQueryBuilder<HasChildQueryBuil
     }
 
     /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
+    @Override
+    public HasChildQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
+    /**
      * Defines how the scores from the matching child documents are mapped into the parent document.
      */
     public HasChildQueryBuilder scoreType(String scoreType) {
@@ -82,6 +92,14 @@ public class HasChildQueryBuilder extends AbstractQueryBuilder<HasChildQueryBuil
     }
 
     /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public HasChildQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
+    /**
      * Sets inner hit definition in the scope of this query and reusing the defined type and query.
      */
     public HasChildQueryBuilder innerHit(QueryInnerHitBuilder innerHit) {
@@ -91,10 +109,13 @@ public class HasChildQueryBuilder extends AbstractQueryBuilder<HasChildQueryBuil
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(HasChildQueryParser.NAME);
         builder.field("query");
         queryBuilder.toXContent(builder, params);
         builder.field("child_type", childType);
+        if (boost != 1.0f) {
+            builder.field("boost", boost);
+        }
         if (scoreType != null) {
             builder.field("score_type", scoreType);
         }
@@ -107,7 +128,9 @@ public class HasChildQueryBuilder extends AbstractQueryBuilder<HasChildQueryBuil
         if (shortCircuitCutoff != null) {
             builder.field("short_circuit_cutoff", shortCircuitCutoff);
         }
-        printBoostAndQueryName(builder);
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         if (innerHit != null) {
             builder.startObject("inner_hits");
             builder.value(innerHit);
@@ -115,9 +138,4 @@ public class HasChildQueryBuilder extends AbstractQueryBuilder<HasChildQueryBuil
         }
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java
index ba2ef65..c5388b1 100644
--- a/core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java
@@ -22,6 +22,9 @@ package org.elasticsearch.index.query;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiDocValues;
 import org.apache.lucene.search.*;
+import org.apache.lucene.search.Filter;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.QueryWrapperFilter;
 import org.apache.lucene.search.join.BitDocIdSetFilter;
 import org.elasticsearch.common.ParseField;
 import org.apache.lucene.search.join.JoinUtil;
@@ -50,8 +53,9 @@ import java.io.IOException;
 /**
  *
  */
-public class HasChildQueryParser extends BaseQueryParserTemp {
+public class HasChildQueryParser implements QueryParser {
 
+    public static final String NAME = "has_child";
     private static final ParseField QUERY_FIELD = new ParseField("query", "filter");
 
     private final InnerHitsQueryParserHelper innerHitsQueryParserHelper;
@@ -63,16 +67,15 @@ public class HasChildQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[] { HasChildQueryBuilder.NAME, Strings.toCamelCase(HasChildQueryBuilder.NAME) };
+        return new String[] { NAME, Strings.toCamelCase(NAME) };
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         boolean queryFound = false;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         String childType = null;
         ScoreType scoreType = ScoreType.NONE;
         int minChildren = 0;
@@ -138,7 +141,7 @@ public class HasChildQueryParser extends BaseQueryParserTemp {
         }
         innerQuery.setBoost(boost);
 
-        DocumentMapper childDocMapper = context.mapperService().documentMapper(childType);
+        DocumentMapper childDocMapper = parseContext.mapperService().documentMapper(childType);
         if (childDocMapper == null) {
             throw new QueryParsingException(parseContext, "[has_child] No mapping for for type [" + childType + "]");
         }
@@ -148,14 +151,14 @@ public class HasChildQueryParser extends BaseQueryParserTemp {
         }
 
         if (innerHits != null) {
-            ParsedQuery parsedQuery = new ParsedQuery(innerQuery, context.copyNamedQueries());
-            InnerHitsContext.ParentChildInnerHits parentChildInnerHits = new InnerHitsContext.ParentChildInnerHits(innerHits.v2(), parsedQuery, null, context.mapperService(), childDocMapper);
+            ParsedQuery parsedQuery = new ParsedQuery(innerQuery, parseContext.copyNamedQueries());
+            InnerHitsContext.ParentChildInnerHits parentChildInnerHits = new InnerHitsContext.ParentChildInnerHits(innerHits.v2(), parsedQuery, null, parseContext.mapperService(), childDocMapper);
             String name = innerHits.v1() != null ? innerHits.v1() : childType;
-            context.addInnerHits(name, parentChildInnerHits);
+            parseContext.addInnerHits(name, parentChildInnerHits);
         }
 
         String parentType = parentFieldMapper.type();
-        DocumentMapper parentDocMapper = context.mapperService().documentMapper(parentType);
+        DocumentMapper parentDocMapper = parseContext.mapperService().documentMapper(parentType);
         if (parentDocMapper == null) {
             throw new QueryParsingException(parseContext, "[has_child]  Type [" + childType + "] points to a non existent parent type ["
                     + parentType + "]");
@@ -167,15 +170,15 @@ public class HasChildQueryParser extends BaseQueryParserTemp {
 
         BitDocIdSetFilter nonNestedDocsFilter = null;
         if (parentDocMapper.hasNestedObjects()) {
-            nonNestedDocsFilter = context.bitsetFilter(Queries.newNonNestedFilter());
+            nonNestedDocsFilter = parseContext.bitsetFilter(Queries.newNonNestedFilter());
         }
 
         // wrap the query with type query
         innerQuery = Queries.filtered(innerQuery, childDocMapper.typeFilter());
 
         final Query query;
-        final ParentChildIndexFieldData parentChildIndexFieldData = context.getForField(parentFieldMapper.fieldType());
-        if (context.indexVersionCreated().onOrAfter(Version.V_2_0_0_beta1)) {
+        final ParentChildIndexFieldData parentChildIndexFieldData = parseContext.getForField(parentFieldMapper.fieldType());
+        if (parseContext.indexVersionCreated().onOrAfter(Version.V_2_0_0_beta1)) {
             query = joinUtilHelper(parentType, parentChildIndexFieldData, parentDocMapper.typeFilter(), scoreType, innerQuery, minChildren, maxChildren);
         } else {
             // TODO: use the query API
@@ -189,7 +192,7 @@ public class HasChildQueryParser extends BaseQueryParserTemp {
             }
         }
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
         query.setBoost(boost);
         return query;
@@ -286,9 +289,4 @@ public class HasChildQueryParser extends BaseQueryParserTemp {
             return "LateParsingQuery {parentType=" + parentType + "}";
         }
     }
-
-    @Override
-    public HasChildQueryBuilder getBuilderPrototype() {
-        return HasChildQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java
index 67bfe07..743ad76 100644
--- a/core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java
@@ -26,14 +26,14 @@ import java.io.IOException;
 /**
  * Builder for the 'has_parent' query.
  */
-public class HasParentQueryBuilder extends AbstractQueryBuilder<HasParentQueryBuilder> {
+public class HasParentQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<HasParentQueryBuilder> {
 
-    public static final String NAME = "has_parent";
     private final QueryBuilder queryBuilder;
     private final String parentType;
     private String scoreType;
+    private float boost = 1.0f;
+    private String queryName;
     private QueryInnerHitBuilder innerHit = null;
-    static final HasParentQueryBuilder PROTOTYPE = new HasParentQueryBuilder(null, null);
 
     /**
      * @param parentType  The parent type
@@ -44,6 +44,12 @@ public class HasParentQueryBuilder extends AbstractQueryBuilder<HasParentQueryBu
         this.queryBuilder = parentQuery;
     }
 
+    @Override
+    public HasParentQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
     /**
      * Defines how the parent score is mapped into the child documents.
      */
@@ -53,6 +59,14 @@ public class HasParentQueryBuilder extends AbstractQueryBuilder<HasParentQueryBu
     }
 
     /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public HasParentQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
+    /**
      * Sets inner hit definition in the scope of this query and reusing the defined type and query.
      */
     public HasParentQueryBuilder innerHit(QueryInnerHitBuilder innerHit) {
@@ -62,14 +76,19 @@ public class HasParentQueryBuilder extends AbstractQueryBuilder<HasParentQueryBu
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(HasParentQueryParser.NAME);
         builder.field("query");
         queryBuilder.toXContent(builder, params);
         builder.field("parent_type", parentType);
         if (scoreType != null) {
             builder.field("score_type", scoreType);
         }
-        printBoostAndQueryName(builder);
+        if (boost != 1.0f) {
+            builder.field("boost", boost);
+        }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         if (innerHit != null) {
             builder.startObject("inner_hits");
             builder.value(innerHit);
@@ -77,10 +96,5 @@ public class HasParentQueryBuilder extends AbstractQueryBuilder<HasParentQueryBu
         }
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
 
diff --git a/core/src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java
index 00f8884..3d3a662 100644
--- a/core/src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java
@@ -47,8 +47,9 @@ import java.util.Set;
 
 import static org.elasticsearch.index.query.HasChildQueryParser.joinUtilHelper;
 
-public class HasParentQueryParser extends BaseQueryParserTemp {
+public class HasParentQueryParser implements QueryParser {
 
+    public static final String NAME = "has_parent";
     private static final ParseField QUERY_FIELD = new ParseField("query", "filter");
 
     private final InnerHitsQueryParserHelper innerHitsQueryParserHelper;
@@ -60,16 +61,15 @@ public class HasParentQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{HasParentQueryBuilder.NAME, Strings.toCamelCase(HasParentQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         boolean queryFound = false;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         String parentType = null;
         boolean score = false;
         String queryName = null;
@@ -134,40 +134,40 @@ public class HasParentQueryParser extends BaseQueryParserTemp {
         }
 
         innerQuery.setBoost(boost);
-        Query query = createParentQuery(innerQuery, parentType, score, context, innerHits);
+        Query query = createParentQuery(innerQuery, parentType, score, parseContext, innerHits);
         if (query == null) {
             return null;
         }
 
         query.setBoost(boost);
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
         return query;
     }
 
-    static Query createParentQuery(Query innerQuery, String parentType, boolean score, QueryShardContext context, Tuple<String, SubSearchContext> innerHits) throws IOException {
-        DocumentMapper parentDocMapper = context.mapperService().documentMapper(parentType);
+    static Query createParentQuery(Query innerQuery, String parentType, boolean score, QueryParseContext parseContext, Tuple<String, SubSearchContext> innerHits) throws IOException {
+        DocumentMapper parentDocMapper = parseContext.mapperService().documentMapper(parentType);
         if (parentDocMapper == null) {
-            throw new QueryParsingException(context.parseContext(), "[has_parent] query configured 'parent_type' [" + parentType
+            throw new QueryParsingException(parseContext, "[has_parent] query configured 'parent_type' [" + parentType
                     + "] is not a valid type");
         }
 
         if (innerHits != null) {
-            ParsedQuery parsedQuery = new ParsedQuery(innerQuery, context.copyNamedQueries());
-            InnerHitsContext.ParentChildInnerHits parentChildInnerHits = new InnerHitsContext.ParentChildInnerHits(innerHits.v2(), parsedQuery, null, context.mapperService(), parentDocMapper);
+            ParsedQuery parsedQuery = new ParsedQuery(innerQuery, parseContext.copyNamedQueries());
+            InnerHitsContext.ParentChildInnerHits parentChildInnerHits = new InnerHitsContext.ParentChildInnerHits(innerHits.v2(), parsedQuery, null, parseContext.mapperService(), parentDocMapper);
             String name = innerHits.v1() != null ? innerHits.v1() : parentType;
-            context.addInnerHits(name, parentChildInnerHits);
+            parseContext.addInnerHits(name, parentChildInnerHits);
         }
 
         Set<String> parentTypes = new HashSet<>(5);
         parentTypes.add(parentDocMapper.type());
         ParentChildIndexFieldData parentChildIndexFieldData = null;
-        for (DocumentMapper documentMapper : context.mapperService().docMappers(false)) {
+        for (DocumentMapper documentMapper : parseContext.mapperService().docMappers(false)) {
             ParentFieldMapper parentFieldMapper = documentMapper.parentFieldMapper();
             if (parentFieldMapper.active()) {
-                DocumentMapper parentTypeDocumentMapper = context.mapperService().documentMapper(parentFieldMapper.type());
-                parentChildIndexFieldData = context.getForField(parentFieldMapper.fieldType());
+                DocumentMapper parentTypeDocumentMapper = parseContext.mapperService().documentMapper(parentFieldMapper.type());
+                parentChildIndexFieldData = parseContext.getForField(parentFieldMapper.fieldType());
                 if (parentTypeDocumentMapper == null) {
                     // Only add this, if this parentFieldMapper (also a parent)  isn't a child of another parent.
                     parentTypes.add(parentFieldMapper.type());
@@ -175,19 +175,19 @@ public class HasParentQueryParser extends BaseQueryParserTemp {
             }
         }
         if (parentChildIndexFieldData == null) {
-            throw new QueryParsingException(context.parseContext(), "[has_parent] no _parent field configured");
+            throw new QueryParsingException(parseContext, "[has_parent] no _parent field configured");
         }
 
         Query parentFilter = null;
         if (parentTypes.size() == 1) {
-            DocumentMapper documentMapper = context.mapperService().documentMapper(parentTypes.iterator().next());
+            DocumentMapper documentMapper = parseContext.mapperService().documentMapper(parentTypes.iterator().next());
             if (documentMapper != null) {
                 parentFilter = documentMapper.typeFilter();
             }
         } else {
             BooleanQuery parentsFilter = new BooleanQuery();
             for (String parentTypeStr : parentTypes) {
-                DocumentMapper documentMapper = context.mapperService().documentMapper(parentTypeStr);
+                DocumentMapper documentMapper = parseContext.mapperService().documentMapper(parentTypeStr);
                 if (documentMapper != null) {
                     parentsFilter.add(documentMapper.typeFilter(), BooleanClause.Occur.SHOULD);
                 }
@@ -202,7 +202,7 @@ public class HasParentQueryParser extends BaseQueryParserTemp {
         // wrap the query with type query
         innerQuery = Queries.filtered(innerQuery, parentDocMapper.typeFilter());
         Filter childrenFilter = new QueryWrapperFilter(Queries.not(parentFilter));
-        if (context.indexVersionCreated().onOrAfter(Version.V_2_0_0_beta1)) {
+        if (parseContext.indexVersionCreated().onOrAfter(Version.V_2_0_0_beta1)) {
             ScoreType scoreMode = score ? ScoreType.MAX : ScoreType.NONE;
             return joinUtilHelper(parentType, parentChildIndexFieldData, childrenFilter, scoreMode, innerQuery, 0, Integer.MAX_VALUE);
         } else {
@@ -214,9 +214,4 @@ public class HasParentQueryParser extends BaseQueryParserTemp {
         }
     }
 
-    @Override
-    public HasParentQueryBuilder getBuilderPrototype() {
-        return HasParentQueryBuilder.PROTOTYPE;
-    }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java
index 461a800..02c2a17 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java
@@ -19,62 +19,44 @@
 
 package org.elasticsearch.index.query;
 
-import com.google.common.collect.Sets;
-
-import org.apache.lucene.queries.TermsQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.Uid;
-import org.elasticsearch.index.mapper.internal.UidFieldMapper;
 
 import java.io.IOException;
-import java.util.*;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.List;
 
 /**
  * A query that will return only documents matching specific ids (and a type).
  */
-public class IdsQueryBuilder extends AbstractQueryBuilder<IdsQueryBuilder> {
+public class IdsQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<IdsQueryBuilder> {
 
-    public static final String NAME = "ids";
+    private final List<String> types;
 
-    private final Set<String> ids = Sets.newHashSet();
+    private List<String> values = new ArrayList<>();
 
-    private final String[] types;
+    private float boost = -1;
 
-    static final IdsQueryBuilder PROTOTYPE = new IdsQueryBuilder();
+    private String queryName;
 
-    /**
-     * Creates a new IdsQueryBuilder by optionally providing the types of the documents to look for
-     */
-    public IdsQueryBuilder(@Nullable String... types) {
-        this.types = types;
-    }
-
-    /**
-     * Returns the types used in this query
-     */
-    public String[] types() {
-        return this.types;
+    public IdsQueryBuilder(String... types) {
+        this.types = types == null ? null : Arrays.asList(types);
     }
 
     /**
-     * Adds ids to the query.
+     * Adds ids to the filter.
      */
     public IdsQueryBuilder addIds(String... ids) {
-        Collections.addAll(this.ids, ids);
+        values.addAll(Arrays.asList(ids));
         return this;
     }
 
     /**
-     * Adds ids to the query.
+     * Adds ids to the filter.
      */
     public IdsQueryBuilder addIds(Collection<String> ids) {
-        this.ids.addAll(ids);
+        values.addAll(ids);
         return this;
     }
 
@@ -93,83 +75,48 @@ public class IdsQueryBuilder extends AbstractQueryBuilder<IdsQueryBuilder> {
     }
 
     /**
-     * Returns the ids for the query.
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
      */
-    public Set<String> ids() {
-        return this.ids;
+    @Override
+    public IdsQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public IdsQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(IdsQueryParser.NAME);
         if (types != null) {
-            if (types.length == 1) {
-                builder.field("type", types[0]);
+            if (types.size() == 1) {
+                builder.field("type", types.get(0));
             } else {
-                builder.array("types", types);
+                builder.startArray("types");
+                for (Object type : types) {
+                    builder.value(type);
+                }
+                builder.endArray();
             }
         }
         builder.startArray("values");
-        for (String value : ids) {
+        for (Object value : values) {
             builder.value(value);
         }
         builder.endArray();
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query query;
-        if (this.ids.isEmpty()) {
-             query = Queries.newMatchNoDocsQuery();
-        } else {
-            Collection<String> typesForQuery;
-            if (types == null || types.length == 0) {
-                typesForQuery = context.queryTypes();
-            } else if (types.length == 1 && MetaData.ALL.equals(types[0])) {
-                typesForQuery = context.mapperService().types();
-            } else {
-                typesForQuery = Sets.newHashSet(types);
-            }
-
-            query = new TermsQuery(UidFieldMapper.NAME, Uid.createUidsForTypesAndIds(typesForQuery, ids));
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        return query;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        // all fields can be empty or null
-        return null;
-    }
-
-    @Override
-    protected IdsQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        IdsQueryBuilder idsQueryBuilder = new IdsQueryBuilder(in.readStringArray());
-        idsQueryBuilder.addIds(in.readStringArray());
-        return idsQueryBuilder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeStringArray(types);
-        out.writeStringArray(ids.toArray(new String[ids.size()]));
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(ids, Arrays.hashCode(types));
-    }
-
-    @Override
-    protected boolean doEquals(IdsQueryBuilder other) {
-        return Objects.equals(ids, other.ids) &&
-               Arrays.equals(types, other.types);
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java
index 54cb8c5..340eb81 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java
@@ -20,17 +20,28 @@
 package org.elasticsearch.index.query;
 
 import com.google.common.collect.ImmutableList;
+import com.google.common.collect.Iterables;
+
+import org.apache.lucene.queries.TermsQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.Uid;
+import org.elasticsearch.index.mapper.internal.UidFieldMapper;
 
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.Collection;
 import java.util.List;
 
 /**
- * Parser for the IdsQuery.
+ *
  */
-public class IdsQueryParser extends BaseQueryParser {
+public class IdsQueryParser implements QueryParser {
+
+    public static final String NAME = "ids";
 
     @Inject
     public IdsQueryParser() {
@@ -38,21 +49,18 @@ public class IdsQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{IdsQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
-    /**
-     * @return a QueryBuilder representation of the query passed in as XContent in the parse context
-     */
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
-        List<String> ids = new ArrayList<>();
-        List<String> types = new ArrayList<>();
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        String queryName = null;
 
+        List<BytesRef> ids = new ArrayList<>();
+        Collection<String> types = null;
         String currentFieldName = null;
+        float boost = 1.0f;
+        String queryName = null;
         XContentParser.Token token;
         boolean idsProvided = false;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -64,17 +72,18 @@ public class IdsQueryParser extends BaseQueryParser {
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                         if ((token == XContentParser.Token.VALUE_STRING) ||
                                 (token == XContentParser.Token.VALUE_NUMBER)) {
-                            String id = parser.textOrNull();
-                            if (id == null) {
+                            BytesRef value = parser.utf8BytesOrNull();
+                            if (value == null) {
                                 throw new QueryParsingException(parseContext, "No value specified for term filter");
                             }
-                            ids.add(id);
+                            ids.add(value);
                         } else {
                             throw new QueryParsingException(parseContext, "Illegal value for id, expecting a string or number, got: "
                                     + token);
                         }
                     }
                 } else if ("types".equals(currentFieldName) || "type".equals(currentFieldName)) {
+                    types = new ArrayList<>();
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                         String value = parser.textOrNull();
                         if (value == null) {
@@ -97,18 +106,26 @@ public class IdsQueryParser extends BaseQueryParser {
                 }
             }
         }
+
         if (!idsProvided) {
             throw new QueryParsingException(parseContext, "[ids] query, no ids values provided");
         }
 
-        IdsQueryBuilder query = new IdsQueryBuilder(types.toArray(new String[types.size()]));
-        query.addIds(ids.toArray(new String[ids.size()]));
-        query.boost(boost).queryName(queryName);
-        return query;
-    }
+        if (ids.isEmpty()) {
+            return Queries.newMatchNoDocsQuery();
+        }
 
-    @Override
-    public IdsQueryBuilder getBuilderPrototype() {
-        return IdsQueryBuilder.PROTOTYPE;
+        if (types == null || types.isEmpty()) {
+            types = parseContext.queryTypes();
+        } else if (types.size() == 1 && Iterables.getFirst(types, null).equals("_all")) {
+            types = parseContext.mapperService().types();
+        }
+
+        TermsQuery query = new TermsQuery(UidFieldMapper.NAME, Uid.createUidsForTypesAndIds(types, ids));
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java b/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java
index 5be792b..810504a 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java
@@ -53,10 +53,10 @@ public class IndexQueryParserService extends AbstractIndexComponent {
     public static final String PARSE_STRICT = "index.query.parse.strict";
     public static final String ALLOW_UNMAPPED = "index.query.parse.allow_unmapped_fields";
 
-    private CloseableThreadLocal<QueryShardContext> cache = new CloseableThreadLocal<QueryShardContext>() {
+    private CloseableThreadLocal<QueryParseContext> cache = new CloseableThreadLocal<QueryParseContext>() {
         @Override
-        protected QueryShardContext initialValue() {
-            return new QueryShardContext(index, IndexQueryParserService.this);
+        protected QueryParseContext initialValue() {
+            return new QueryParseContext(index, IndexQueryParserService.this);
         }
     };
 
@@ -120,20 +120,16 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         return indicesQueriesRegistry.queryParsers().get(name);
     }
 
-    public IndicesQueriesRegistry indicesQueriesRegistry() {
-        return indicesQueriesRegistry;
-    }
-
     public ParsedQuery parse(QueryBuilder queryBuilder) {
         XContentParser parser = null;
         try {
             BytesReference bytes = queryBuilder.buildAsBytes();
             parser = XContentFactory.xContent(bytes).createParser(bytes);
             return parse(cache.get(), parser);
-        } catch (QueryShardException e) {
+        } catch (QueryParsingException e) {
             throw e;
         } catch (Exception e) {
-            throw new QueryParsingException(getShardContext().parseContext(), "Failed to parse", e);
+            throw new QueryParsingException(getParseContext(), "Failed to parse", e);
         } finally {
             if (parser != null) {
                 parser.close();
@@ -150,10 +146,10 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         try {
             parser = XContentFactory.xContent(source, offset, length).createParser(source, offset, length);
             return parse(cache.get(), parser);
-        } catch (QueryShardException e) {
+        } catch (QueryParsingException e) {
             throw e;
         } catch (Exception e) {
-            throw new QueryParsingException(getShardContext().parseContext(), "Failed to parse", e);
+            throw new QueryParsingException(getParseContext(), "Failed to parse", e);
         } finally {
             if (parser != null) {
                 parser.close();
@@ -165,8 +161,7 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         return parse(cache.get(), source);
     }
 
-    //norelease
-    public ParsedQuery parse(QueryShardContext context, BytesReference source) {
+    public ParsedQuery parse(QueryParseContext context, BytesReference source) {
         XContentParser parser = null;
         try {
             parser = XContentFactory.xContent(source).createParser(source);
@@ -174,7 +169,7 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         } catch (QueryParsingException e) {
             throw e;
         } catch (Exception e) {
-            throw new QueryParsingException(context.parseContext(), "Failed to parse", e);
+            throw new QueryParsingException(context, "Failed to parse", e);
         } finally {
             if (parser != null) {
                 parser.close();
@@ -182,15 +177,15 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         }
     }
 
-    public ParsedQuery parse(String source) throws QueryParsingException, QueryShardException {
+    public ParsedQuery parse(String source) throws QueryParsingException {
         XContentParser parser = null;
         try {
             parser = XContentFactory.xContent(source).createParser(source);
             return innerParse(cache.get(), parser);
-        } catch (QueryShardException|QueryParsingException e) {
+        } catch (QueryParsingException e) {
             throw e;
         } catch (Exception e) {
-            throw new QueryParsingException(getShardContext().parseContext(), "Failed to parse [" + source + "]", e);
+            throw new QueryParsingException(getParseContext(), "Failed to parse [" + source + "]", e);
         } finally {
             if (parser != null) {
                 parser.close();
@@ -202,12 +197,11 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         return parse(cache.get(), parser);
     }
 
-    //norelease
-    public ParsedQuery parse(QueryShardContext context, XContentParser parser) {
+    public ParsedQuery parse(QueryParseContext context, XContentParser parser) {
         try {
             return innerParse(context, parser);
         } catch (IOException e) {
-            throw new QueryParsingException(context.parseContext(), "Failed to parse", e);
+            throw new QueryParsingException(context, "Failed to parse", e);
         }
     }
 
@@ -215,12 +209,11 @@ public class IndexQueryParserService extends AbstractIndexComponent {
      * Parses an inner filter, returning null if the filter should be ignored.
      */
     @Nullable
-    //norelease
     public ParsedQuery parseInnerFilter(XContentParser parser) throws IOException {
-        QueryShardContext context = cache.get();
+        QueryParseContext context = cache.get();
         context.reset(parser);
         try {
-            Query filter = context.parseContext().parseInnerFilter();
+            Query filter = context.parseInnerFilter();
             if (filter == null) {
                 return null;
             }
@@ -231,23 +224,27 @@ public class IndexQueryParserService extends AbstractIndexComponent {
     }
 
     @Nullable
-    public QueryBuilder parseInnerQueryBuilder(QueryParseContext parseContext) throws IOException {
-        parseContext.parseFieldMatcher(parseFieldMatcher);
-        QueryBuilder query = parseContext.parseInnerQueryBuilder();
-        return query;
+    public Query parseInnerQuery(XContentParser parser) throws IOException {
+        QueryParseContext context = cache.get();
+        context.reset(parser);
+        try {
+            return context.parseInnerQuery();
+        } finally {
+            context.reset(null);
+        }
     }
 
     @Nullable
-    //norelease
-    public Query parseInnerQuery(QueryShardContext context) throws IOException {
-        Query query = context.parseContext().parseInnerQueryBuilder().toQuery(context);
+    public Query parseInnerQuery(QueryParseContext parseContext) throws IOException {
+        parseContext.parseFieldMatcher(parseFieldMatcher);
+        Query query = parseContext.parseInnerQuery();
         if (query == null) {
             query = Queries.newMatchNoDocsQuery();
         }
         return query;
     }
 
-    public QueryShardContext getShardContext() {
+    public QueryParseContext getParseContext() {
         return cache.get();
     }
 
@@ -279,39 +276,34 @@ public class IndexQueryParserService extends AbstractIndexComponent {
                         XContentParser qSourceParser = XContentFactory.xContent(querySource).createParser(querySource);
                         parsedQuery = parse(qSourceParser);
                     } else {
-                        throw new QueryParsingException(getShardContext().parseContext(), "request does not support [" + fieldName + "]");
+                        throw new QueryParsingException(getParseContext(), "request does not support [" + fieldName + "]");
                     }
                 }
             }
             if (parsedQuery != null) {
                 return parsedQuery;
             }
-        } catch (QueryShardException e) {
+        } catch (QueryParsingException e) {
             throw e;
         } catch (Throwable e) {
-            throw new QueryParsingException(getShardContext().parseContext(), "Failed to parse", e);
+            throw new QueryParsingException(getParseContext(), "Failed to parse", e);
         }
 
-        throw new QueryParsingException(getShardContext().parseContext(), "Required query is missing");
+        throw new QueryParsingException(getParseContext(), "Required query is missing");
     }
 
-    //norelease
-    private ParsedQuery innerParse(QueryShardContext context, XContentParser parser) throws IOException, QueryShardException {
-        context.reset(parser);
+    private ParsedQuery innerParse(QueryParseContext parseContext, XContentParser parser) throws IOException, QueryParsingException {
+        parseContext.reset(parser);
         try {
-            context.parseFieldMatcher(parseFieldMatcher);
-            return innerParse(context, context.parseContext().parseInnerQueryBuilder());
+            parseContext.parseFieldMatcher(parseFieldMatcher);
+            Query query = parseContext.parseInnerQuery();
+            if (query == null) {
+                query = Queries.newMatchNoDocsQuery();
+            }
+            return new ParsedQuery(query, parseContext.copyNamedQueries());
         } finally {
-            context.reset(null);
-        }
-    }
-
-    private static ParsedQuery innerParse(QueryShardContext context, QueryBuilder queryBuilder) throws IOException, QueryShardException {
-        Query query = queryBuilder.toQuery(context);
-        if (query == null) {
-            query = Queries.newMatchNoDocsQuery();
+            parseContext.reset(null);
         }
-        return new ParsedQuery(query, context.copyNamedQueries());
     }
 
     public ParseFieldMatcher parseFieldMatcher() {
diff --git a/core/src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java
index d7de7c1..7c2af81 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java
@@ -27,9 +27,7 @@ import java.io.IOException;
  * A query that will execute the wrapped query only for the specified indices, and "match_all" when
  * it does not match those indices (by default).
  */
-public class IndicesQueryBuilder extends AbstractQueryBuilder<IndicesQueryBuilder> {
-
-    public static final String NAME = "indices";
+public class IndicesQueryBuilder extends QueryBuilder {
 
     private final QueryBuilder queryBuilder;
 
@@ -38,7 +36,7 @@ public class IndicesQueryBuilder extends AbstractQueryBuilder<IndicesQueryBuilde
     private String sNoMatchQuery;
     private QueryBuilder noMatchQuery;
 
-    static final IndicesQueryBuilder PROTOTYPE = new IndicesQueryBuilder(null);
+    private String queryName;
 
     public IndicesQueryBuilder(QueryBuilder queryBuilder, String... indices) {
         this.queryBuilder = queryBuilder;
@@ -61,9 +59,17 @@ public class IndicesQueryBuilder extends AbstractQueryBuilder<IndicesQueryBuilde
         return this;
     }
 
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public IndicesQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(IndicesQueryParser.NAME);
         builder.field("indices", indices);
         builder.field("query");
         queryBuilder.toXContent(builder, params);
@@ -73,12 +79,9 @@ public class IndicesQueryBuilder extends AbstractQueryBuilder<IndicesQueryBuilde
         } else if (sNoMatchQuery != null) {
             builder.field("no_match_query", sNoMatchQuery);
         }
-        printBoostAndQueryName(builder);
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java
index d368d6b..a18c865 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java
@@ -37,8 +37,9 @@ import java.util.Collection;
 
 /**
  */
-public class IndicesQueryParser extends BaseQueryParserTemp {
+public class IndicesQueryParser implements QueryParser {
 
+    public static final String NAME = "indices";
     private static final ParseField QUERY_FIELD = new ParseField("query", "filter");
     private static final ParseField NO_MATCH_QUERY = new ParseField("no_match_query", "no_match_filter");
 
@@ -54,12 +55,11 @@ public class IndicesQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{IndicesQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         Query noMatchQuery = null;
@@ -67,7 +67,6 @@ public class IndicesQueryParser extends BaseQueryParserTemp {
         boolean indicesFound = false;
         boolean currentIndexMatchesIndices = false;
         String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -119,8 +118,6 @@ public class IndicesQueryParser extends BaseQueryParserTemp {
                     }
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else {
                     throw new QueryParsingException(parseContext, "[indices] query does not support [" + currentFieldName + "]");
                 }
@@ -150,9 +147,8 @@ public class IndicesQueryParser extends BaseQueryParserTemp {
             }
         }
         if (queryName != null) {
-            context.addNamedQuery(queryName, chosenQuery);
+            parseContext.addNamedQuery(queryName, chosenQuery);
         }
-        chosenQuery.setBoost(boost);
         return chosenQuery;
     }
 
@@ -165,9 +161,4 @@ public class IndicesQueryParser extends BaseQueryParserTemp {
         }
         return false;
     }
-
-    @Override
-    public IndicesQueryBuilder getBuilderPrototype() {
-        return IndicesQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java
index b217a5e..9d44f39 100644
--- a/core/src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java
@@ -19,11 +19,7 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
 import org.elasticsearch.action.search.SearchRequestBuilder;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
@@ -32,62 +28,18 @@ import java.io.IOException;
  * @deprecated Use {@link SearchRequestBuilder#setTerminateAfter(int)} instead.
  */
 @Deprecated
-public class LimitQueryBuilder extends AbstractQueryBuilder<LimitQueryBuilder> {
+public class LimitQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "limit";
     private final int limit;
-    static final LimitQueryBuilder PROTOTYPE = new LimitQueryBuilder(-1);
 
     public LimitQueryBuilder(int limit) {
         this.limit = limit;
     }
 
-    public int limit() {
-        return limit;
-    }
-
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(LimitQueryParser.NAME);
         builder.field("value", limit);
-        printBoostAndQueryName(builder);
         builder.endObject();
     }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        // this filter is deprecated and parses to a filter that matches everything
-        return Queries.newMatchAllQuery();
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        // nothing to validate
-        return null;
-    }
-
-    @Override
-    protected boolean doEquals(LimitQueryBuilder other) {
-        return Integer.compare(other.limit, limit) == 0;
-    }
-
-    @Override
-    protected int doHashCode() {
-        return this.limit;
-    }
-
-    @Override
-    protected LimitQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new LimitQueryBuilder(in.readInt());
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeInt(limit);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/LimitQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/LimitQueryParser.java
index 3b68382..3419f61 100644
--- a/core/src/main/java/org/elasticsearch/index/query/LimitQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/LimitQueryParser.java
@@ -19,13 +19,17 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
 
 @Deprecated
-public class LimitQueryParser extends BaseQueryParser {
+public class LimitQueryParser implements QueryParser {
+
+    public static final String NAME = "limit";
 
     @Inject
     public LimitQueryParser() {
@@ -33,16 +37,14 @@ public class LimitQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{LimitQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         int limit = -1;
-        String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         String currentFieldName = null;
         XContentParser.Token token;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -51,10 +53,6 @@ public class LimitQueryParser extends BaseQueryParser {
             } else if (token.isValue()) {
                 if ("value".equals(currentFieldName)) {
                     limit = parser.intValue();
-                } else if ("_name".equals(currentFieldName)) {
-                    queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else {
                     throw new QueryParsingException(parseContext, "[limit] query does not support [" + currentFieldName + "]");
                 }
@@ -65,11 +63,7 @@ public class LimitQueryParser extends BaseQueryParser {
             throw new QueryParsingException(parseContext, "No value specified for limit query");
         }
 
-        return new LimitQueryBuilder(limit).boost(boost).queryName(queryName);
-    }
-
-    @Override
-    public LimitQueryBuilder getBuilderPrototype() {
-        return LimitQueryBuilder.PROTOTYPE;
+        // this filter is deprecated and parses to a filter that matches everything
+        return Queries.newMatchAllQuery();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java
index 00c5019..b09bc9f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java
@@ -19,10 +19,6 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
@@ -30,52 +26,26 @@ import java.io.IOException;
 /**
  * A query that matches on all documents.
  */
-public class MatchAllQueryBuilder extends AbstractQueryBuilder<MatchAllQueryBuilder> {
+public class MatchAllQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<MatchAllQueryBuilder> {
 
-    public static final String NAME = "match_all";
-
-    static final MatchAllQueryBuilder PROTOTYPE = new MatchAllQueryBuilder();
-
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return Queries.newMatchAllQuery();
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        // nothing to validate
-        return null;
-    }
+    private float boost = -1;
 
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
     @Override
-    protected boolean doEquals(MatchAllQueryBuilder other) {
-        return true;
+    public MatchAllQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     @Override
-    protected int doHashCode() {
-        return 0;
-    }
-
-    @Override
-    protected MatchAllQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new MatchAllQueryBuilder();
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        //nothing to write really
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(MatchAllQueryParser.NAME);
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java
index fc2ea42..933d3d3 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java
@@ -19,16 +19,21 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
 
 /**
- * Parser code for MatchAllQuery
+ *
  */
-public class MatchAllQueryParser extends BaseQueryParser {
+public class MatchAllQueryParser implements QueryParser {
+
+    public static final String NAME = "match_all";
 
     @Inject
     public MatchAllQueryParser() {
@@ -36,38 +41,35 @@ public class MatchAllQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{MatchAllQueryBuilder.NAME, Strings.toCamelCase(MatchAllQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public MatchAllQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
+        float boost = 1.0f;
         String currentFieldName = null;
+
         XContentParser.Token token;
-        String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         while (((token = parser.nextToken()) != XContentParser.Token.END_OBJECT && token != XContentParser.Token.END_ARRAY)) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
             } else if (token.isValue()) {
-                if ("_name".equals(currentFieldName)) {
-                    queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
+                if ("boost".equals(currentFieldName)) {
                     boost = parser.floatValue();
                 } else {
                     throw new QueryParsingException(parseContext, "[match_all] query does not support [" + currentFieldName + "]");
                 }
             }
         }
-        MatchAllQueryBuilder queryBuilder = new MatchAllQueryBuilder();
-        queryBuilder.boost(boost);
-        queryBuilder.queryName(queryName);
-        return queryBuilder;
-    }
 
-    @Override
-    public MatchAllQueryBuilder getBuilderPrototype() {
-        return MatchAllQueryBuilder.PROTOTYPE;
+        if (boost == 1.0f) {
+            return Queries.newMatchAllQuery();
+        }
+
+        MatchAllDocsQuery query = new MatchAllDocsQuery();
+        query.setBoost(boost);
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java
index e31a8cf..e4a643f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java
@@ -29,9 +29,12 @@ import java.util.Locale;
  * Match query is a query that analyzes the text and constructs a query as the result of the analysis. It
  * can construct different queries based on the type provided.
  */
-public class MatchQueryBuilder extends AbstractQueryBuilder<MatchQueryBuilder> {
+public class MatchQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<MatchQueryBuilder> {
 
-    public static final String NAME = "match";
+    public enum Operator {
+        OR,
+        AND
+    }
 
     public enum Type {
         /**
@@ -63,6 +66,8 @@ public class MatchQueryBuilder extends AbstractQueryBuilder<MatchQueryBuilder> {
 
     private String analyzer;
 
+    private Float boost;
+
     private Integer slop;
 
     private Fuzziness fuzziness;
@@ -85,7 +90,7 @@ public class MatchQueryBuilder extends AbstractQueryBuilder<MatchQueryBuilder> {
 
     private Float cutoff_Frequency = null;
 
-    static final MatchQueryBuilder PROTOTYPE = new MatchQueryBuilder(null, null);
+    private String queryName;
 
     /**
      * Constructs a new text query.
@@ -121,6 +126,15 @@ public class MatchQueryBuilder extends AbstractQueryBuilder<MatchQueryBuilder> {
     }
 
     /**
+     * Set the boost to apply to the query.
+     */
+    @Override
+    public MatchQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
+    /**
      * Set the phrase slop if evaluated to a phrase query type.
      */
     public MatchQueryBuilder slop(int slop) {
@@ -194,9 +208,17 @@ public class MatchQueryBuilder extends AbstractQueryBuilder<MatchQueryBuilder> {
         return this;
     }
 
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public MatchQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
     @Override
     public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(MatchQueryParser.NAME);
         builder.startObject(name);
 
         builder.field("query", text);
@@ -209,6 +231,9 @@ public class MatchQueryBuilder extends AbstractQueryBuilder<MatchQueryBuilder> {
         if (analyzer != null) {
             builder.field("analyzer", analyzer);
         }
+        if (boost != null) {
+            builder.field("boost", boost);
+        }
         if (slop != null) {
             builder.field("slop", slop);
         }
@@ -243,13 +268,12 @@ public class MatchQueryBuilder extends AbstractQueryBuilder<MatchQueryBuilder> {
         if (cutoff_Frequency != null) {
             builder.field("cutoff_frequency", cutoff_Frequency);
         }
-        printBoostAndQueryName(builder);
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
+
+
         builder.endObject();
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java
index 5d9ba66..62177ab 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.index.query;
 
 import org.apache.lucene.queries.ExtendedCommonTermsQuery;
+import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
@@ -34,7 +35,9 @@ import java.io.IOException;
 /**
  *
  */
-public class MatchQueryParser extends BaseQueryParserTemp {
+public class MatchQueryParser implements QueryParser {
+
+    public static final String NAME = "match";
 
     @Inject
     public MatchQueryParser() {
@@ -43,13 +46,12 @@ public class MatchQueryParser extends BaseQueryParserTemp {
     @Override
     public String[] names() {
         return new String[]{
-                MatchQueryBuilder.NAME, "match_phrase", "matchPhrase", "match_phrase_prefix", "matchPhrasePrefix", "matchFuzzy", "match_fuzzy", "fuzzy_match"
+                NAME, "match_phrase", "matchPhrase", "match_phrase_prefix", "matchPhrasePrefix", "matchFuzzy", "match_fuzzy", "fuzzy_match"
         };
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         MatchQuery.Type type = MatchQuery.Type.BOOLEAN;
@@ -68,8 +70,8 @@ public class MatchQueryParser extends BaseQueryParserTemp {
         String fieldName = parser.currentName();
 
         Object value = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        MatchQuery matchQuery = new MatchQuery(context);
+        float boost = 1.0f;
+        MatchQuery matchQuery = new MatchQuery(parseContext);
         String minimumShouldMatch = null;
         String queryName = null;
 
@@ -95,7 +97,7 @@ public class MatchQueryParser extends BaseQueryParserTemp {
                         }
                     } else if ("analyzer".equals(currentFieldName)) {
                         String analyzer = parser.text();
-                        if (context.analysisService().analyzer(analyzer) == null) {
+                        if (parseContext.analysisService().analyzer(analyzer) == null) {
                             throw new QueryParsingException(parseContext, "[match] analyzer [" + parser.text() + "] not found");
                         }
                         matchQuery.setAnalyzer(analyzer);
@@ -110,7 +112,15 @@ public class MatchQueryParser extends BaseQueryParserTemp {
                     } else if ("max_expansions".equals(currentFieldName) || "maxExpansions".equals(currentFieldName)) {
                         matchQuery.setMaxExpansions(parser.intValue());
                     } else if ("operator".equals(currentFieldName)) {
-                        matchQuery.setOccur(Operator.fromString(parser.text()).toBooleanClauseOccur());
+                        String op = parser.text();
+                        if ("or".equalsIgnoreCase(op)) {
+                            matchQuery.setOccur(BooleanClause.Occur.SHOULD);
+                        } else if ("and".equalsIgnoreCase(op)) {
+                            matchQuery.setOccur(BooleanClause.Occur.MUST);
+                        } else {
+                            throw new QueryParsingException(parseContext, "text query requires operator to be either 'and' or 'or', not ["
+                                    + op + "]");
+                        }
                     } else if ("minimum_should_match".equals(currentFieldName) || "minimumShouldMatch".equals(currentFieldName)) {
                         minimumShouldMatch = parser.textOrNull();
                     } else if ("fuzzy_rewrite".equals(currentFieldName) || "fuzzyRewrite".equals(currentFieldName)) {
@@ -164,13 +174,8 @@ public class MatchQueryParser extends BaseQueryParserTemp {
         }
         query.setBoost(boost);
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
         return query;
     }
-
-    @Override
-    public MatchQueryBuilder getBuilderPrototype() {
-        return MatchQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java
index 253af16..ac3f279 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java
@@ -19,45 +19,25 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.*;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.internal.FieldNamesFieldMapper;
-import org.elasticsearch.index.mapper.object.ObjectMapper;
 
 import java.io.IOException;
-import java.util.Collection;
-import java.util.Objects;
 
 /**
- * Constructs a filter that have only null values or no value in the original field.
+ * Constructs a filter that only match on documents that the field has a value in them.
  */
-public class MissingQueryBuilder extends AbstractQueryBuilder<MissingQueryBuilder> {
+public class MissingQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "missing";
+    private String name;
 
-    public static final boolean DEFAULT_NULL_VALUE = false;
+    private String queryName;
 
-    public static final boolean DEFAULT_EXISTENCE_VALUE = true;
+    private Boolean nullValue;
 
-    private final String fieldPattern;
+    private Boolean existence;
 
-    private boolean nullValue = DEFAULT_NULL_VALUE;
-
-    private boolean existence = DEFAULT_EXISTENCE_VALUE;
-
-    static final MissingQueryBuilder PROTOTYPE = new MissingQueryBuilder(null);
-
-    public MissingQueryBuilder(String fieldPattern) {
-        this.fieldPattern = fieldPattern;
-    }
-
-    public String fieldPattern() {
-        return this.fieldPattern;
+    public MissingQueryBuilder(String name) {
+        this.name = name;
     }
 
     /**
@@ -70,15 +50,7 @@ public class MissingQueryBuilder extends AbstractQueryBuilder<MissingQueryBuilde
     }
 
     /**
-     * Returns true if the missing filter will include documents where the field contains a null value, otherwise
-     * these documents will not be included.
-     */
-    public boolean nullValue() {
-        return this.nullValue;
-    }
-
-    /**
-     * Should the missing filter include documents where the field doesn't exist in the docs.
+     * Should the missing filter include documents where the field doesn't exists in the docs.
      * Defaults to <tt>true</tt>.
      */
     public MissingQueryBuilder existence(boolean existence) {
@@ -87,157 +59,26 @@ public class MissingQueryBuilder extends AbstractQueryBuilder<MissingQueryBuilde
     }
 
     /**
-     * Returns true if the missing filter will include documents where the field has no values, otherwise
-     * these documents will not be included.
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
      */
-    public boolean existence() {
-        return this.existence;
+    public MissingQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("field", fieldPattern);
-        builder.field("null_value", nullValue);
-        builder.field("existence", existence);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return newFilter(context, fieldPattern, existence, nullValue);
-    }
-
-    public static Query newFilter(QueryShardContext context, String fieldPattern, boolean existence, boolean nullValue) {
-        if (!existence && !nullValue) {
-            throw new QueryShardException(context, "missing must have either existence, or null_value, or both set to true");
-        }
-
-        final FieldNamesFieldMapper.FieldNamesFieldType fieldNamesFieldType = (FieldNamesFieldMapper.FieldNamesFieldType) context.mapperService().fullName(FieldNamesFieldMapper.NAME);
-        if (fieldNamesFieldType == null) {
-            // can only happen when no types exist, so no docs exist either
-            return Queries.newMatchNoDocsQuery();
-        }
-
-        ObjectMapper objectMapper = context.getObjectMapper(fieldPattern);
-        if (objectMapper != null) {
-            // automatic make the object mapper pattern
-            fieldPattern = fieldPattern + ".*";
-        }
-
-        Collection<String> fields = context.simpleMatchToIndexNames(fieldPattern);
-        if (fields.isEmpty()) {
-            if (existence) {
-                // if we ask for existence of fields, and we found none, then we should match on all
-                return Queries.newMatchAllQuery();
-            }
-            return null;
-        }
-
-        Query existenceFilter = null;
-        Query nullFilter = null;
-
-        if (existence) {
-            BooleanQuery boolFilter = new BooleanQuery();
-            for (String field : fields) {
-                MappedFieldType fieldType = context.fieldMapper(field);
-                Query filter = null;
-                if (fieldNamesFieldType.isEnabled()) {
-                    final String f;
-                    if (fieldType != null) {
-                        f = fieldType.names().indexName();
-                    } else {
-                        f = field;
-                    }
-                    filter = fieldNamesFieldType.termQuery(f, context);
-                }
-                // if _field_names are not indexed, we need to go the slow way
-                if (filter == null && fieldType != null) {
-                    filter = fieldType.rangeQuery(null, null, true, true);
-                }
-                if (filter == null) {
-                    filter = new TermRangeQuery(field, null, null, true, true);
-                }
-                boolFilter.add(filter, BooleanClause.Occur.SHOULD);
-            }
-
-            existenceFilter = boolFilter;
-            existenceFilter = Queries.not(existenceFilter);;
-        }
-
-        if (nullValue) {
-            for (String field : fields) {
-                MappedFieldType fieldType = context.fieldMapper(field);
-                if (fieldType != null) {
-                    nullFilter = fieldType.nullValueQuery();
-                }
-            }
-        }
-
-        Query filter;
-        if (nullFilter != null) {
-            if (existenceFilter != null) {
-                BooleanQuery combined = new BooleanQuery();
-                combined.add(existenceFilter, BooleanClause.Occur.SHOULD);
-                combined.add(nullFilter, BooleanClause.Occur.SHOULD);
-                // cache the not filter as well, so it will be faster
-                filter = combined;
-            } else {
-                filter = nullFilter;
-            }
-        } else {
-            filter = existenceFilter;
+        builder.startObject(MissingQueryParser.NAME);
+        builder.field("field", name);
+        if (nullValue != null) {
+            builder.field("null_value", nullValue);
         }
-
-        if (filter == null) {
-            return null;
-        }
-
-        return new ConstantScoreQuery(filter);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (Strings.isEmpty(this.fieldPattern)) {
-            validationException = addValidationError("missing must be provided with a [field]", validationException);
+        if (existence != null) {
+            builder.field("existence", existence);
         }
-        if (!existence && !nullValue) {
-            validationException = addValidationError("missing must have either existence, or null_value, or both set to true", validationException);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return validationException;
-    }
-
-    @Override
-    protected MissingQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        MissingQueryBuilder missingQueryBuilder = new MissingQueryBuilder(in.readString());
-        missingQueryBuilder.nullValue = in.readBoolean();
-        missingQueryBuilder.existence = in.readBoolean();
-        return missingQueryBuilder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldPattern);
-        out.writeBoolean(nullValue);
-        out.writeBoolean(existence);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldPattern, nullValue, existence);
-    }
-
-    @Override
-    protected boolean doEquals(MissingQueryBuilder other) {
-        return Objects.equals(fieldPattern, other.fieldPattern) &&
-                Objects.equals(nullValue, other.nullValue) &&
-                Objects.equals(existence, other.existence);
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java
index bf00360..6ef19d7 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java
@@ -19,15 +19,29 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermRangeQuery;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.internal.FieldNamesFieldMapper;
+import org.elasticsearch.index.mapper.object.ObjectMapper;
 
 import java.io.IOException;
+import java.util.Collection;
 
 /**
  *
  */
-public class MissingQueryParser extends BaseQueryParser {
+public class MissingQueryParser implements QueryParser {
+
+    public static final String NAME = "missing";
+    public static final boolean DEFAULT_NULL_VALUE = false;
+    public static final boolean DEFAULT_EXISTENCE_VALUE = true;
 
     @Inject
     public MissingQueryParser() {
@@ -35,18 +49,17 @@ public class MissingQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{MissingQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldPattern = null;
         String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        boolean nullValue = MissingQueryBuilder.DEFAULT_NULL_VALUE;
-        boolean existence = MissingQueryBuilder.DEFAULT_EXISTENCE_VALUE;
+        boolean nullValue = DEFAULT_NULL_VALUE;
+        boolean existence = DEFAULT_EXISTENCE_VALUE;
 
         XContentParser.Token token;
         String currentFieldName = null;
@@ -62,8 +75,6 @@ public class MissingQueryParser extends BaseQueryParser {
                     existence = parser.booleanValue();
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else {
                     throw new QueryParsingException(parseContext, "[missing] query does not support [" + currentFieldName + "]");
                 }
@@ -73,15 +84,98 @@ public class MissingQueryParser extends BaseQueryParser {
         if (fieldPattern == null) {
             throw new QueryParsingException(parseContext, "missing must be provided with a [field]");
         }
-        return new MissingQueryBuilder(fieldPattern)
-                .nullValue(nullValue)
-                .existence(existence)
-                .boost(boost)
-                .queryName(queryName);
+
+        return newFilter(parseContext, fieldPattern, existence, nullValue, queryName);
     }
 
-    @Override
-    public MissingQueryBuilder getBuilderPrototype() {
-        return MissingQueryBuilder.PROTOTYPE;
+    public static Query newFilter(QueryParseContext parseContext, String fieldPattern, boolean existence, boolean nullValue, String queryName) {
+        if (!existence && !nullValue) {
+            throw new QueryParsingException(parseContext, "missing must have either existence, or null_value, or both set to true");
+        }
+
+        final FieldNamesFieldMapper.FieldNamesFieldType fieldNamesFieldType = (FieldNamesFieldMapper.FieldNamesFieldType)parseContext.mapperService().fullName(FieldNamesFieldMapper.NAME);
+        if (fieldNamesFieldType == null) {
+            // can only happen when no types exist, so no docs exist either
+            return Queries.newMatchNoDocsQuery();
+        }
+
+        ObjectMapper objectMapper = parseContext.getObjectMapper(fieldPattern);
+        if (objectMapper != null) {
+            // automatic make the object mapper pattern
+            fieldPattern = fieldPattern + ".*";
+        }
+
+        Collection<String> fields = parseContext.simpleMatchToIndexNames(fieldPattern);
+        if (fields.isEmpty()) {
+            if (existence) {
+                // if we ask for existence of fields, and we found none, then we should match on all
+                return Queries.newMatchAllQuery();
+            }
+            return null;
+        }
+
+        Query existenceFilter = null;
+        Query nullFilter = null;
+
+        if (existence) {
+            BooleanQuery boolFilter = new BooleanQuery();
+            for (String field : fields) {
+                MappedFieldType fieldType = parseContext.fieldMapper(field);
+                Query filter = null;
+                if (fieldNamesFieldType.isEnabled()) {
+                    final String f;
+                    if (fieldType != null) {
+                        f = fieldType.names().indexName();
+                    } else {
+                        f = field;
+                    }
+                    filter = fieldNamesFieldType.termQuery(f, parseContext);
+                }
+                // if _field_names are not indexed, we need to go the slow way
+                if (filter == null && fieldType != null) {
+                    filter = fieldType.rangeQuery(null, null, true, true);
+                }
+                if (filter == null) {
+                    filter = new TermRangeQuery(field, null, null, true, true);
+                }
+                boolFilter.add(filter, BooleanClause.Occur.SHOULD);
+            }
+
+            existenceFilter = boolFilter;
+            existenceFilter = Queries.not(existenceFilter);;
+        }
+
+        if (nullValue) {
+            for (String field : fields) {
+                MappedFieldType fieldType = parseContext.fieldMapper(field);
+                if (fieldType != null) {
+                    nullFilter = fieldType.nullValueQuery();
+                }
+            }
+        }
+
+        Query filter;
+        if (nullFilter != null) {
+            if (existenceFilter != null) {
+                BooleanQuery combined = new BooleanQuery();
+                combined.add(existenceFilter, BooleanClause.Occur.SHOULD);
+                combined.add(nullFilter, BooleanClause.Occur.SHOULD);
+                // cache the not filter as well, so it will be faster
+                filter = combined;
+            } else {
+                filter = nullFilter;
+            }
+        } else {
+            filter = existenceFilter;
+        }
+
+        if (filter == null) {
+            return null;
+        }
+
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, existenceFilter);
+        }
+        return new ConstantScoreQuery(filter);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java
index fbd13ea..19d65d9 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java
@@ -23,11 +23,7 @@ import org.elasticsearch.action.get.MultiGetRequest;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.lucene.uid.Versions;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentType;
+import org.elasticsearch.common.xcontent.*;
 import org.elasticsearch.index.VersionType;
 import org.elasticsearch.search.fetch.source.FetchSourceContext;
 
@@ -41,7 +37,7 @@ import java.util.Locale;
  * A more like this query that finds documents that are "like" the provided {@link #likeText(String)}
  * which is checked against the fields the query is constructed with.
  */
-public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQueryBuilder> {
+public class MoreLikeThisQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<MoreLikeThisQueryBuilder> {
 
     /**
      * A single get item. Pure delegate to multi get.
@@ -132,8 +128,6 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
         }
     }
 
-    public static final String NAME = "mlt";
-
     private final String[] fields;
     private List<Item> docs = new ArrayList<>();
     private List<Item> unlikeDocs = new ArrayList<>();
@@ -147,10 +141,10 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
     private int minWordLength = -1;
     private int maxWordLength = -1;
     private float boostTerms = -1;
+    private float boost = -1;
     private String analyzer;
     private Boolean failOnUnsupportedField;
-
-    static final MoreLikeThisQueryBuilder PROTOTYPE = new MoreLikeThisQueryBuilder();
+    private String queryName;
 
     /**
      * Constructs a new more like this query which uses the "_all" field.
@@ -346,6 +340,12 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
         return this;
     }
 
+    @Override
+    public MoreLikeThisQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
     /**
      * Whether to fail or return no result when this query is run against a field which is not supported such as binary/numeric fields.
      */
@@ -354,10 +354,18 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
         return this;
     }
 
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public MoreLikeThisQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
         String likeFieldName = MoreLikeThisQueryParser.Fields.LIKE.getPreferredName();
-        builder.startObject(NAME);
+        builder.startObject(MoreLikeThisQueryParser.NAME);
         if (fields != null) {
             builder.startArray("fields");
             for (String field : fields) {
@@ -404,21 +412,21 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
         if (boostTerms != -1) {
             builder.field(MoreLikeThisQueryParser.Fields.BOOST_TERMS.getPreferredName(), boostTerms);
         }
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
         if (analyzer != null) {
             builder.field("analyzer", analyzer);
         }
         if (failOnUnsupportedField != null) {
             builder.field(MoreLikeThisQueryParser.Fields.FAIL_ON_UNSUPPORTED_FIELD.getPreferredName(), failOnUnsupportedField);
         }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         if (include != null) {
             builder.field("include", include);
         }
-        printBoostAndQueryName(builder);
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java
index 6f505c5..d56ce40 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java
@@ -21,7 +21,6 @@ package org.elasticsearch.index.query;
 
 import com.google.common.collect.Lists;
 import com.google.common.collect.Sets;
-
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.queries.TermsQuery;
 import org.apache.lucene.search.BooleanClause;
@@ -54,8 +53,9 @@ import static org.elasticsearch.index.mapper.Uid.createUidAsBytes;
 /**
  *
  */
-public class MoreLikeThisQueryParser extends BaseQueryParserTemp {
+public class MoreLikeThisQueryParser implements QueryParser {
 
+    public static final String NAME = "mlt";
     private MoreLikeThisFetchService fetchService = null;
 
     public static class Fields {
@@ -88,16 +88,15 @@ public class MoreLikeThisQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{MoreLikeThisQueryBuilder.NAME, "more_like_this", "moreLikeThis"};
+        return new String[]{NAME, "more_like_this", "moreLikeThis"};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         MoreLikeThisQuery mltQuery = new MoreLikeThisQuery();
-        mltQuery.setSimilarity(context.searchSimilarity());
+        mltQuery.setSimilarity(parseContext.searchSimilarity());
         Analyzer analyzer = null;
         List<String> moreLikeFields = null;
         boolean failOnUnsupportedField = true;
@@ -144,7 +143,7 @@ public class MoreLikeThisQueryParser extends BaseQueryParserTemp {
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, Fields.MINIMUM_SHOULD_MATCH)) {
                     mltQuery.setMinimumShouldMatch(parser.text());
                 } else if ("analyzer".equals(currentFieldName)) {
-                    analyzer = context.analysisService().analyzer(parser.text());
+                    analyzer = parseContext.analysisService().analyzer(parser.text());
                 } else if ("boost".equals(currentFieldName)) {
                     mltQuery.setBoost(parser.floatValue());
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, Fields.FAIL_ON_UNSUPPORTED_FIELD)) {
@@ -167,7 +166,7 @@ public class MoreLikeThisQueryParser extends BaseQueryParserTemp {
                     moreLikeFields = Lists.newLinkedList();
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                         String field = parser.text();
-                        MappedFieldType fieldType = context.fieldMapper(field);
+                        MappedFieldType fieldType = parseContext.fieldMapper(field);
                         moreLikeFields.add(fieldType == null ? field : fieldType.names().indexName());
                     }
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, Fields.DOCUMENT_IDS)) {
@@ -216,14 +215,14 @@ public class MoreLikeThisQueryParser extends BaseQueryParserTemp {
 
         // set analyzer
         if (analyzer == null) {
-            analyzer = context.mapperService().searchAnalyzer();
+            analyzer = parseContext.mapperService().searchAnalyzer();
         }
         mltQuery.setAnalyzer(analyzer);
 
         // set like text fields
         boolean useDefaultField = (moreLikeFields == null);
         if (useDefaultField) {
-            moreLikeFields = Lists.newArrayList(context.defaultField());
+            moreLikeFields = Lists.newArrayList(parseContext.defaultField());
         }
         // possibly remove unsupported fields
         removeUnsupportedFields(moreLikeFields, analyzer, failOnUnsupportedField);
@@ -234,7 +233,7 @@ public class MoreLikeThisQueryParser extends BaseQueryParserTemp {
 
         // support for named query
         if (queryName != null) {
-            context.addNamedQuery(queryName, mltQuery);
+            parseContext.addNamedQuery(queryName, mltQuery);
         }
 
         // handle like texts
@@ -258,12 +257,12 @@ public class MoreLikeThisQueryParser extends BaseQueryParserTemp {
                     item.index(parseContext.index().name());
                 }
                 if (item.type() == null) {
-                    if (context.queryTypes().size() > 1) {
+                    if (parseContext.queryTypes().size() > 1) {
                         throw new QueryParsingException(parseContext,
                                     "ambiguous type for item with id: " + item.id()
                                 + " and index: " + item.index());
                     } else {
-                        item.type(context.queryTypes().iterator().next());
+                        item.type(parseContext.queryTypes().iterator().next());
                     }
                 }
                 // default fields if not present but don't override for artificial docs
@@ -356,9 +355,4 @@ public class MoreLikeThisQueryParser extends BaseQueryParserTemp {
             boolQuery.add(query, BooleanClause.Occur.MUST_NOT);
         }
     }
-
-    @Override
-    public MoreLikeThisQueryBuilder getBuilderPrototype() {
-        return MoreLikeThisQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java
index bdcd886..1f7f960 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java
@@ -21,7 +21,6 @@ package org.elasticsearch.index.query;
 
 import com.carrotsearch.hppc.ObjectFloatHashMap;
 import com.google.common.collect.Lists;
-
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
@@ -37,9 +36,7 @@ import java.util.Locale;
 /**
  * Same as {@link MatchQueryBuilder} but supports multiple fields.
  */
-public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQueryBuilder> {
-
-    public static final String NAME = "multi_match";
+public class MultiMatchQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<MultiMatchQueryBuilder> {
 
     private final Object text;
 
@@ -48,10 +45,12 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
 
     private MultiMatchQueryBuilder.Type type;
 
-    private Operator operator;
+    private MatchQueryBuilder.Operator operator;
 
     private String analyzer;
 
+    private Float boost;
+
     private Integer slop;
 
     private Fuzziness fuzziness;
@@ -76,7 +75,8 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
 
     private MatchQueryBuilder.ZeroTermsQuery zeroTermsQuery = null;
 
-    static final MultiMatchQueryBuilder PROTOTYPE = new MultiMatchQueryBuilder(null);
+    private String queryName;
+
 
     public enum Type {
 
@@ -143,7 +143,7 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
                 }
             }
             if (type == null) {
-                throw new ElasticsearchParseException("failed to parse [{}] query type [{}]. unknown type.", NAME, value);
+                throw new ElasticsearchParseException("failed to parse [{}] query type [{}]. unknown type.", MultiMatchQueryParser.NAME, value);
             }
             return type;
         }
@@ -197,7 +197,7 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
     /**
      * Sets the operator to use when using a boolean query. Defaults to <tt>OR</tt>.
      */
-    public MultiMatchQueryBuilder operator(Operator operator) {
+    public MultiMatchQueryBuilder operator(MatchQueryBuilder.Operator operator) {
         this.operator = operator;
         return this;
     }
@@ -212,6 +212,15 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
     }
 
     /**
+     * Set the boost to apply to the query.
+     */
+    @Override
+    public MultiMatchQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
+    /**
      * Set the phrase slop if evaluated to a phrase query type.
      */
     public MultiMatchQueryBuilder slop(int slop) {
@@ -308,9 +317,17 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
         return this;
     }
 
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public MultiMatchQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
     @Override
     public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(MultiMatchQueryParser.NAME);
 
         builder.field("query", text);
         builder.startArray("fields");
@@ -332,6 +349,9 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
         if (analyzer != null) {
             builder.field("analyzer", analyzer);
         }
+        if (boost != null) {
+            builder.field("boost", boost);
+        }
         if (slop != null) {
             builder.field("slop", slop);
         }
@@ -374,13 +394,11 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
             builder.field("zero_terms_query", zeroTermsQuery.toString());
         }
 
-        printBoostAndQueryName(builder);
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
 
         builder.endObject();
     }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java
index fcd79d8..5922f52 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java
@@ -21,6 +21,7 @@ package org.elasticsearch.index.query;
 
 import com.google.common.collect.Maps;
 
+import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.regex.Regex;
@@ -36,7 +37,9 @@ import java.util.Map;
 /**
  * Same as {@link MatchQueryParser} but has support for multiple fields.
  */
-public class MultiMatchQueryParser extends BaseQueryParserTemp {
+public class MultiMatchQueryParser implements QueryParser {
+
+    public static final String NAME = "multi_match";
 
     @Inject
     public MultiMatchQueryParser() {
@@ -45,20 +48,19 @@ public class MultiMatchQueryParser extends BaseQueryParserTemp {
     @Override
     public String[] names() {
         return new String[]{
-                MultiMatchQueryBuilder.NAME, "multiMatch"
+                NAME, "multiMatch"
         };
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         Object value = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         Float tieBreaker = null;
         MultiMatchQueryBuilder.Type type = null;
-        MultiMatchQuery multiMatchQuery = new MultiMatchQuery(context);
+        MultiMatchQuery multiMatchQuery = new MultiMatchQuery(parseContext);
         String minimumShouldMatch = null;
         Map<String, Float> fieldNameWithBoosts = Maps.newHashMap();
         String queryName = null;
@@ -71,12 +73,12 @@ public class MultiMatchQueryParser extends BaseQueryParserTemp {
             } else if ("fields".equals(currentFieldName)) {
                 if (token == XContentParser.Token.START_ARRAY) {
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        extractFieldAndBoost(context, parser, fieldNameWithBoosts);
+                        extractFieldAndBoost(parseContext, parser, fieldNameWithBoosts);
                     }
                 } else if (token.isValue()) {
-                    extractFieldAndBoost(context, parser, fieldNameWithBoosts);
+                    extractFieldAndBoost(parseContext, parser, fieldNameWithBoosts);
                 } else {
-                    throw new QueryParsingException(parseContext, "[" + MultiMatchQueryBuilder.NAME + "] query does not support [" + currentFieldName + "]");
+                    throw new QueryParsingException(parseContext, "[" + NAME + "] query does not support [" + currentFieldName + "]");
                 }
             } else if (token.isValue()) {
                 if ("query".equals(currentFieldName)) {
@@ -85,8 +87,8 @@ public class MultiMatchQueryParser extends BaseQueryParserTemp {
                     type = MultiMatchQueryBuilder.Type.parse(parser.text(), parseContext.parseFieldMatcher());
                 } else if ("analyzer".equals(currentFieldName)) {
                     String analyzer = parser.text();
-                    if (context.analysisService().analyzer(analyzer) == null) {
-                        throw new QueryParsingException(parseContext, "[" + MultiMatchQueryBuilder.NAME + "] analyzer [" + parser.text() + "] not found");
+                    if (parseContext.analysisService().analyzer(analyzer) == null) {
+                        throw new QueryParsingException(parseContext, "[" + NAME + "] analyzer [" + parser.text() + "] not found");
                     }
                     multiMatchQuery.setAnalyzer(analyzer);
                 } else if ("boost".equals(currentFieldName)) {
@@ -100,7 +102,15 @@ public class MultiMatchQueryParser extends BaseQueryParserTemp {
                 } else if ("max_expansions".equals(currentFieldName) || "maxExpansions".equals(currentFieldName)) {
                     multiMatchQuery.setMaxExpansions(parser.intValue());
                 } else if ("operator".equals(currentFieldName)) {
-                    multiMatchQuery.setOccur(Operator.fromString(parser.text()).toBooleanClauseOccur());
+                    String op = parser.text();
+                    if ("or".equalsIgnoreCase(op)) {
+                        multiMatchQuery.setOccur(BooleanClause.Occur.SHOULD);
+                    } else if ("and".equalsIgnoreCase(op)) {
+                        multiMatchQuery.setOccur(BooleanClause.Occur.MUST);
+                    } else {
+                        throw new QueryParsingException(parseContext, "text query requires operator to be either 'and' or 'or', not [" + op
+                                + "]");
+                    }
                 } else if ("minimum_should_match".equals(currentFieldName) || "minimumShouldMatch".equals(currentFieldName)) {
                     minimumShouldMatch = parser.textOrNull();
                 } else if ("fuzzy_rewrite".equals(currentFieldName) || "fuzzyRewrite".equals(currentFieldName)) {
@@ -157,12 +167,12 @@ public class MultiMatchQueryParser extends BaseQueryParserTemp {
 
         query.setBoost(boost);
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
         return query;
     }
 
-    private void extractFieldAndBoost(QueryShardContext context, XContentParser parser, Map<String, Float> fieldNameWithBoosts) throws IOException {
+    private void extractFieldAndBoost(QueryParseContext parseContext, XContentParser parser, Map<String, Float> fieldNameWithBoosts) throws IOException {
         String fField = null;
         Float fBoost = null;
         char[] fieldText = parser.textCharacters();
@@ -180,16 +190,11 @@ public class MultiMatchQueryParser extends BaseQueryParserTemp {
         }
 
         if (Regex.isSimpleMatchPattern(fField)) {
-            for (String field : context.mapperService().simpleMatchToIndexNames(fField)) {
+            for (String field : parseContext.mapperService().simpleMatchToIndexNames(fField)) {
                 fieldNameWithBoosts.put(field, fBoost);
             }
         } else {
             fieldNameWithBoosts.put(fField, fBoost);
         }
     }
-
-    @Override
-    public MultiMatchQueryBuilder getBuilderPrototype() {
-        return MultiMatchQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MultiTermQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MultiTermQueryBuilder.java
index 0e946d6..9c7383d 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MultiTermQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MultiTermQueryBuilder.java
@@ -18,6 +18,6 @@
  */
 package org.elasticsearch.index.query;
 
-public interface MultiTermQueryBuilder<QB extends MultiTermQueryBuilder<QB>> extends QueryBuilder<QB> {
+public abstract class MultiTermQueryBuilder extends QueryBuilder {
 
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java
index bb2e4b4..63b40dc 100644
--- a/core/src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java
@@ -25,9 +25,7 @@ import org.elasticsearch.index.query.support.QueryInnerHitBuilder;
 import java.io.IOException;
 import java.util.Objects;
 
-public class NestedQueryBuilder extends AbstractQueryBuilder<NestedQueryBuilder> {
-
-    public static final String NAME = "nested";
+public class NestedQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<NestedQueryBuilder> {
 
     private final QueryBuilder queryBuilder;
 
@@ -35,28 +33,39 @@ public class NestedQueryBuilder extends AbstractQueryBuilder<NestedQueryBuilder>
 
     private String scoreMode;
 
-    private QueryInnerHitBuilder innerHit;
+    private float boost = 1.0f;
 
-    static final NestedQueryBuilder PROTOTYPE = new NestedQueryBuilder();
+    private String queryName;
+
+    private QueryInnerHitBuilder innerHit;
 
     public NestedQueryBuilder(String path, QueryBuilder queryBuilder) {
         this.path = path;
         this.queryBuilder = Objects.requireNonNull(queryBuilder);
     }
+    /**
+     * The score mode.
+     */
+    public NestedQueryBuilder scoreMode(String scoreMode) {
+        this.scoreMode = scoreMode;
+        return this;
+    }
 
     /**
-     * private constructor only used internally
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
      */
-    private NestedQueryBuilder() {
-        this.path = null;
-        this.queryBuilder = null;
+    @Override
+    public NestedQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
-     * The score mode.
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public NestedQueryBuilder scoreMode(String scoreMode) {
-        this.scoreMode = scoreMode;
+    public NestedQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
         return this;
     }
 
@@ -70,14 +79,19 @@ public class NestedQueryBuilder extends AbstractQueryBuilder<NestedQueryBuilder>
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(NestedQueryParser.NAME);
         builder.field("query");
         queryBuilder.toXContent(builder, params);
         builder.field("path", path);
         if (scoreMode != null) {
             builder.field("score_mode", scoreMode);
         }
-        printBoostAndQueryName(builder);
+        if (boost != 1.0f) {
+            builder.field("boost", boost);
+        }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         if (innerHit != null) {
             builder.startObject("inner_hits");
             builder.value(innerHit);
@@ -86,8 +100,4 @@ public class NestedQueryBuilder extends AbstractQueryBuilder<NestedQueryBuilder>
         builder.endObject();
     }
 
-    @Override
-    public final String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java
index 7e4b376..4dc71f9 100644
--- a/core/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java
@@ -37,8 +37,9 @@ import org.elasticsearch.search.internal.SubSearchContext;
 
 import java.io.IOException;
 
-public class NestedQueryParser extends BaseQueryParserTemp {
+public class NestedQueryParser implements QueryParser {
 
+    public static final String NAME = "nested";
     private static final ParseField FILTER_FIELD = new ParseField("filter").withAllDeprecated("query");
 
     private final InnerHitsQueryParserHelper innerHitsQueryParserHelper;
@@ -50,16 +51,15 @@ public class NestedQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{NestedQueryBuilder.NAME, Strings.toCamelCase(NestedQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
-        final ToBlockJoinQueryBuilder builder = new ToBlockJoinQueryBuilder(context);
+        final ToBlockJoinQueryBuilder builder = new ToBlockJoinQueryBuilder(parseContext);
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         ScoreMode scoreMode = ScoreMode.Avg;
         String queryName = null;
 
@@ -111,7 +111,7 @@ public class NestedQueryParser extends BaseQueryParserTemp {
         if (joinQuery != null) {
             joinQuery.setBoost(boost);
             if (queryName != null) {
-                context.addNamedQuery(queryName, joinQuery);
+                parseContext.addNamedQuery(queryName, joinQuery);
             }
         }
         return joinQuery;
@@ -122,8 +122,8 @@ public class NestedQueryParser extends BaseQueryParserTemp {
         private ScoreMode scoreMode;
         private Tuple<String, SubSearchContext> innerHits;
 
-        public ToBlockJoinQueryBuilder(QueryShardContext context) throws IOException {
-            super(context);
+        public ToBlockJoinQueryBuilder(QueryParseContext parseContext) throws IOException {
+            super(parseContext);
         }
 
         public void setScoreMode(ScoreMode scoreMode) {
@@ -147,14 +147,14 @@ public class NestedQueryParser extends BaseQueryParserTemp {
                     innerQuery = null;
                 }
             } else {
-                throw new QueryShardException(shardContext, "[nested] requires either 'query' or 'filter' field");
+                throw new QueryParsingException(parseContext, "[nested] requires either 'query' or 'filter' field");
             }
 
             if (innerHits != null) {
-                ParsedQuery parsedQuery = new ParsedQuery(innerQuery, shardContext.copyNamedQueries());
+                ParsedQuery parsedQuery = new ParsedQuery(innerQuery, parseContext.copyNamedQueries());
                 InnerHitsContext.NestedInnerHits nestedInnerHits = new InnerHitsContext.NestedInnerHits(innerHits.v2(), parsedQuery, null, getParentObjectMapper(), nestedObjectMapper);
                 String name = innerHits.v1() != null ? innerHits.v1() : path;
-                shardContext.addInnerHits(name, nestedInnerHits);
+                parseContext.addInnerHits(name, nestedInnerHits);
             }
 
             if (innerQuery != null) {
@@ -165,9 +165,4 @@ public class NestedQueryParser extends BaseQueryParserTemp {
         }
 
     }
-
-    @Override
-    public NestedQueryBuilder getBuilderPrototype() {
-        return NestedQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java
index 70aec44..c16cf64 100644
--- a/core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java
@@ -19,10 +19,6 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
@@ -31,71 +27,29 @@ import java.util.Objects;
 /**
  * A filter that matches documents matching boolean combinations of other filters.
  */
-public class NotQueryBuilder extends AbstractQueryBuilder<NotQueryBuilder> {
-
-    public static final String NAME = "not";
+public class NotQueryBuilder extends QueryBuilder {
 
     private final QueryBuilder filter;
 
-    static final NotQueryBuilder PROTOTYPE = new NotQueryBuilder(null);
+    private String queryName;
 
     public NotQueryBuilder(QueryBuilder filter) {
-        this.filter = filter;
+        this.filter = Objects.requireNonNull(filter);
     }
 
-    /**
-     * @return the filter added to "not".
-     */
-    public QueryBuilder filter() {
-        return this.filter;
+    public NotQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(NotQueryParser.NAME);
         builder.field("query");
         filter.toXContent(builder, params);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query luceneQuery = filter.toQuery(context);
-        if (luceneQuery == null) {
-            return null;
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return Queries.not(luceneQuery);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        return validateInnerQuery(filter, null);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(filter);
-    }
-
-    @Override
-    protected boolean doEquals(NotQueryBuilder other) {
-        return Objects.equals(filter, other.filter);
-    }
-
-    @Override
-    protected NotQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryBuilder queryBuilder = in.readQuery();
-        return new NotQueryBuilder(queryBuilder);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(filter);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/NotQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/NotQueryParser.java
index 90d638e..68ffe43 100644
--- a/core/src/main/java/org/elasticsearch/index/query/NotQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/NotQueryParser.java
@@ -19,8 +19,10 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
@@ -28,8 +30,9 @@ import java.io.IOException;
 /**
  *
  */
-public class NotQueryParser extends BaseQueryParser {
+public class NotQueryParser implements QueryParser {
 
+    public static final String NAME = "not";
     private static final ParseField QUERY_FIELD = new ParseField("filter", "query");
 
     @Inject
@@ -38,19 +41,18 @@ public class NotQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{NotQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        QueryBuilder query = null;
+        Query query = null;
         boolean queryFound = false;
 
         String queryName = null;
         String currentFieldName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         XContentParser.Token token;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
@@ -59,22 +61,20 @@ public class NotQueryParser extends BaseQueryParser {
                 // skip
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if (parseContext.parseFieldMatcher().match(currentFieldName, QUERY_FIELD)) {
-                    query = parseContext.parseInnerFilterToQueryBuilder();
+                    query = parseContext.parseInnerFilter();
                     queryFound = true;
                 } else {
                     queryFound = true;
                     // its the filter, and the name is the field
-                    query = parseContext.parseInnerFilterToQueryBuilder(currentFieldName);
+                    query = parseContext.parseInnerFilter(currentFieldName);
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 queryFound = true;
                 // its the filter, and the name is the field
-                query = parseContext.parseInnerFilterToQueryBuilder(currentFieldName);
+                query = parseContext.parseInnerFilter(currentFieldName);
             } else if (token.isValue()) {
                 if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else {
                     throw new QueryParsingException(parseContext, "[not] query does not support [" + currentFieldName + "]");
                 }
@@ -85,14 +85,14 @@ public class NotQueryParser extends BaseQueryParser {
             throw new QueryParsingException(parseContext, "filter is required when using `not` query");
         }
 
-        NotQueryBuilder notQueryBuilder = new NotQueryBuilder(query);
-        notQueryBuilder.queryName(queryName);
-        notQueryBuilder.boost(boost);
-        return notQueryBuilder;
-    }
+        if (query == null) {
+            return null;
+        }
 
-    @Override
-    public NotQueryBuilder getBuilderPrototype() {
-        return NotQueryBuilder.PROTOTYPE;
+        Query notQuery = Queries.not(query);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, notQuery);
+        }
+        return notQuery;
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/Operator.java b/core/src/main/java/org/elasticsearch/index/query/Operator.java
deleted file mode 100644
index ce143eb..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/Operator.java
+++ /dev/null
@@ -1,85 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.query;
-
-import com.google.common.collect.Lists;
-import org.apache.lucene.search.BooleanClause;
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-
-import java.io.IOException;
-
-public enum Operator implements Writeable<Operator> {
-    OR(0), AND(1);
-
-    private final int ordinal;
-
-    private static final Operator PROTOTYPE = OR;
-
-    private Operator(int ordinal) {
-        this.ordinal = ordinal;
-    }
-
-    public BooleanClause.Occur toBooleanClauseOccur() {
-        switch (this) {
-            case OR:
-                return BooleanClause.Occur.SHOULD;
-            case AND:
-                return BooleanClause.Occur.MUST;
-            default:
-                throw Operator.newOperatorException(this.toString());
-        }
-    }
-
-    @Override
-    public Operator readFrom(StreamInput in) throws IOException {
-        int ord = in.readVInt();
-        for (Operator operator : Operator.values()) {
-            if (operator.ordinal == ord) {
-                return operator;
-            }
-        }
-        throw new ElasticsearchException("unknown serialized operator [" + ord + "]");
-    }
-
-    public static Operator readOperatorFrom(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeVInt(this.ordinal);
-    }
-
-    public static Operator fromString(String op) {
-        for (Operator operator : Operator.values()) {
-            if (operator.name().equalsIgnoreCase(op)) {
-                return operator;
-            }
-        }
-        throw Operator.newOperatorException(op);
-    }
-
-    private static IllegalArgumentException newOperatorException(String op) {
-        return new IllegalArgumentException("operator needs to be either " + Lists.newArrayList(Operator.values()) +
-                ", but not [" + op + "]");
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java
index 84452f6..19ad3e4 100644
--- a/core/src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java
@@ -20,121 +20,51 @@
 package org.elasticsearch.index.query;
 
 import com.google.common.collect.Lists;
-
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.BooleanClause.Occur;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.List;
-import java.util.Objects;
+import java.util.Collections;
 
 /**
  * A filter that matches documents matching boolean combinations of other filters.
  * @deprecated Use {@link BoolQueryBuilder} instead
  */
 @Deprecated
-public class OrQueryBuilder extends AbstractQueryBuilder<OrQueryBuilder> {
-
-    public static final String NAME = "or";
+public class OrQueryBuilder extends QueryBuilder {
 
-    private final ArrayList<QueryBuilder> filters = Lists.newArrayList();
+    private ArrayList<QueryBuilder> filters = Lists.newArrayList();
 
-    static final OrQueryBuilder PROTOTYPE = new OrQueryBuilder();
+    private String queryName;
 
     public OrQueryBuilder(QueryBuilder... filters) {
-        for (QueryBuilder filter : filters) {
-            this.filters.add(filter);
-        }
+        Collections.addAll(this.filters, filters);
     }
 
     /**
      * Adds a filter to the list of filters to "or".
-     * No <tt>null</tt> value allowed.
      */
     public OrQueryBuilder add(QueryBuilder filterBuilder) {
         filters.add(filterBuilder);
         return this;
     }
 
-    /**
-     * @return the list of filters added to "or".
-     */
-    public List<QueryBuilder> filters() {
-        return this.filters;
+    public OrQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(OrQueryParser.NAME);
         builder.startArray("filters");
         for (QueryBuilder filter : filters) {
             filter.toXContent(builder, params);
         }
         builder.endArray();
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        if (filters.isEmpty()) {
-            // no filters provided, this should be ignored upstream
-            return null;
-        }
-
-        BooleanQuery query = new BooleanQuery();
-        for (QueryBuilder f : filters) {
-            Query innerQuery = f.toQuery(context);
-            // ignore queries that are null
-            if (innerQuery != null) {
-                query.add(innerQuery, Occur.SHOULD);
-            }
-        }
-        if (query.clauses().isEmpty()) {
-            // no inner lucene query exists, ignore upstream
-            return null;
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return query;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        return validateInnerQueries(filters, null);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(filters);
-    }
-
-    @Override
-    protected boolean doEquals(OrQueryBuilder other) {
-        return Objects.equals(filters, other.filters);
-    }
-
-    @Override
-    protected OrQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        OrQueryBuilder orQueryBuilder = new OrQueryBuilder();
-        List<QueryBuilder> queryBuilders = readQueries(in);
-        for (QueryBuilder queryBuilder : queryBuilders) {
-            orQueryBuilder.add(queryBuilder);
-        }
-        return orQueryBuilder;
-
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        writeQueries(out, filters);
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/OrQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/OrQueryParser.java
index f5b5f95..acc55e5 100644
--- a/core/src/main/java/org/elasticsearch/index/query/OrQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/OrQueryParser.java
@@ -19,6 +19,9 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
 
@@ -27,8 +30,13 @@ import java.util.ArrayList;
 
 import static com.google.common.collect.Lists.newArrayList;
 
+/**
+ *
+ */
 @Deprecated
-public class OrQueryParser extends BaseQueryParser {
+public class OrQueryParser implements QueryParser {
+
+    public static final String NAME = "or";
 
     @Inject
     public OrQueryParser() {
@@ -36,24 +44,23 @@ public class OrQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{OrQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        final ArrayList<QueryBuilder> queries = newArrayList();
+        ArrayList<Query> queries = newArrayList();
         boolean queriesFound = false;
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         String queryName = null;
         String currentFieldName = null;
         XContentParser.Token token = parser.currentToken();
         if (token == XContentParser.Token.START_ARRAY) {
             while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                 queriesFound = true;
-                QueryBuilder filter = parseContext.parseInnerFilterToQueryBuilder();
+                Query filter = parseContext.parseInnerFilter();
                 if (filter != null) {
                     queries.add(filter);
                 }
@@ -66,7 +73,7 @@ public class OrQueryParser extends BaseQueryParser {
                     if ("filters".equals(currentFieldName)) {
                         queriesFound = true;
                         while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                            QueryBuilder filter = parseContext.parseInnerFilterToQueryBuilder();
+                            Query filter = parseContext.parseInnerFilter();
                             if (filter != null) {
                                 queries.add(filter);
                             }
@@ -74,7 +81,7 @@ public class OrQueryParser extends BaseQueryParser {
                     } else {
                         while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                             queriesFound = true;
-                            QueryBuilder filter = parseContext.parseInnerFilterToQueryBuilder();
+                            Query filter = parseContext.parseInnerFilter();
                             if (filter != null) {
                                 queries.add(filter);
                             }
@@ -83,8 +90,6 @@ public class OrQueryParser extends BaseQueryParser {
                 } else if (token.isValue()) {
                     if ("_name".equals(currentFieldName)) {
                         queryName = parser.text();
-                    } else if ("boost".equals(currentFieldName)) {
-                        boost = parser.floatValue();
                     } else {
                         throw new QueryParsingException(parseContext, "[or] query does not support [" + currentFieldName + "]");
                     }
@@ -96,17 +101,17 @@ public class OrQueryParser extends BaseQueryParser {
             throw new QueryParsingException(parseContext, "[or] query requires 'filters' to be set on it'");
         }
 
-        OrQueryBuilder orQuery = new OrQueryBuilder();
-        for (QueryBuilder query : queries) {
-            orQuery.add(query);
+        if (queries.isEmpty()) {
+            return null;
         }
-        orQuery.queryName(queryName);
-        orQuery.boost(boost);
-        return orQuery;
-    }
 
-    @Override
-    public OrQueryBuilder getBuilderPrototype() {
-        return OrQueryBuilder.PROTOTYPE;
+        BooleanQuery query = new BooleanQuery();
+        for (Query f : queries) {
+            query.add(f, Occur.SHOULD);
+        }
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java
index a49580c..e0e5b2f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java
@@ -19,53 +19,44 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.PrefixQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * A Query that matches documents containing terms with a specified prefix.
  */
-public class PrefixQueryBuilder extends AbstractQueryBuilder<PrefixQueryBuilder> implements MultiTermQueryBuilder<PrefixQueryBuilder> {
+public class PrefixQueryBuilder extends MultiTermQueryBuilder implements BoostableQueryBuilder<PrefixQueryBuilder> {
 
-    public static final String NAME = "prefix";
+    private final String name;
 
-    private final String fieldName;
+    private final String prefix;
 
-    private final String value;
+    private float boost = -1;
 
     private String rewrite;
 
-    static final PrefixQueryBuilder PROTOTYPE = new PrefixQueryBuilder(null, null);
+    private String queryName;
 
     /**
      * A Query that matches documents containing terms with a specified prefix.
      *
-     * @param fieldName The name of the field
-     * @param value The prefix query
+     * @param name   The name of the field
+     * @param prefix The prefix query
      */
-    public PrefixQueryBuilder(String fieldName, String value) {
-        this.fieldName = fieldName;
-        this.value = value;
+    public PrefixQueryBuilder(String name, String prefix) {
+        this.name = name;
+        this.prefix = prefix;
     }
 
-    public String fieldName() {
-        return this.fieldName;
-    }
-
-    public String value() {
-        return this.value;
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
+    @Override
+    public PrefixQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     public PrefixQueryBuilder rewrite(String rewrite) {
@@ -73,83 +64,33 @@ public class PrefixQueryBuilder extends AbstractQueryBuilder<PrefixQueryBuilder>
         return this;
     }
 
-    public String rewrite() {
-        return this.rewrite;
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public PrefixQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.startObject(fieldName);
-        builder.field("prefix", this.value);
-        if (rewrite != null) {
-            builder.field("rewrite", rewrite);
-        }
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        MultiTermQuery.RewriteMethod method = QueryParsers.parseRewriteMethod(context.parseFieldMatcher(), rewrite, null);
-
-        Query query = null;
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType != null) {
-            query = fieldType.prefixQuery(value, method, context);
-        }
-        if (query == null) {
-            PrefixQuery prefixQuery = new PrefixQuery(new Term(fieldName, BytesRefs.toBytesRef(value)));
-            if (method != null) {
-                prefixQuery.setRewriteMethod(method);
+        builder.startObject(PrefixQueryParser.NAME);
+        if (boost == -1 && rewrite == null && queryName == null) {
+            builder.field(name, prefix);
+        } else {
+            builder.startObject(name);
+            builder.field("prefix", prefix);
+            if (boost != -1) {
+                builder.field("boost", boost);
             }
-            query = prefixQuery;
-        }
-
-        return query;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (Strings.isEmpty(this.fieldName)) {
-            validationException = addValidationError("field name cannot be null or empty.", validationException);
-        }
-        if (this.value == null) {
-            validationException = addValidationError("query text cannot be null", validationException);
+            if (rewrite != null) {
+                builder.field("rewrite", rewrite);
+            }
+            if (queryName != null) {
+                builder.field("_name", queryName);
+            }
+            builder.endObject();
         }
-        return validationException;
-    }
-
-    @Override
-    protected PrefixQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        PrefixQueryBuilder prefixQueryBuilder = new PrefixQueryBuilder(in.readString(), in.readString());
-        prefixQueryBuilder.rewrite = in.readOptionalString();
-        return prefixQueryBuilder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-        out.writeString(value);
-        out.writeOptionalString(rewrite);
-    }
-
-    @Override
-    protected final int doHashCode() {
-        return Objects.hash(fieldName, value, rewrite);
-    }
-
-    @Override
-    protected boolean doEquals(PrefixQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-                Objects.equals(value, other.value) &&
-                Objects.equals(rewrite, other.rewrite);
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java
index 47d52fa..56387d9 100644
--- a/core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java
@@ -19,15 +19,24 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.MultiTermQuery;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
 
 /**
  *
  */
-public class PrefixQueryParser extends BaseQueryParser {
+public class PrefixQueryParser implements QueryParser {
+
+    public static final String NAME = "prefix";
 
     @Inject
     public PrefixQueryParser() {
@@ -35,19 +44,19 @@ public class PrefixQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{PrefixQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldName = parser.currentName();
-        String value = null;
-        String rewrite = null;
-
+        String rewriteMethod = null;
         String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+
+        String value = null;
+        float boost = 1.0f;
         String currentFieldName = null;
         XContentParser.Token token;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -68,7 +77,7 @@ public class PrefixQueryParser extends BaseQueryParser {
                         } else if ("boost".equals(currentFieldName)) {
                             boost = parser.floatValue();
                         } else if ("rewrite".equals(currentFieldName)) {
-                            rewrite = parser.textOrNull();
+                            rewriteMethod = parser.textOrNull();
                         } else {
                             throw new QueryParsingException(parseContext, "[regexp] query does not support [" + currentFieldName + "]");
                         }
@@ -87,14 +96,25 @@ public class PrefixQueryParser extends BaseQueryParser {
         if (value == null) {
             throw new QueryParsingException(parseContext, "No value specified for prefix query");
         }
-        return new PrefixQueryBuilder(fieldName, value)
-                .rewrite(rewrite)
-                .boost(boost)
-                .queryName(queryName);
-    }
 
-    @Override
-    public PrefixQueryBuilder getBuilderPrototype() {
-        return PrefixQueryBuilder.PROTOTYPE;
+        MultiTermQuery.RewriteMethod method = QueryParsers.parseRewriteMethod(parseContext.parseFieldMatcher(), rewriteMethod, null);
+
+        Query query = null;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            query = fieldType.prefixQuery(value, method, parseContext);
+        }
+        if (query == null) {
+            PrefixQuery prefixQuery = new PrefixQuery(new Term(fieldName, BytesRefs.toBytesRef(value)));
+            if (method != null) {
+                prefixQuery.setRewriteMethod(method);
+            }
+            query = prefixQuery;
+        }
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return  query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java
index 02bbfb6..fa11d32 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java
@@ -19,67 +19,25 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.NamedWriteable;
-import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.action.support.ToXContentToBytes;
+import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentType;
 
 import java.io.IOException;
 
-public interface QueryBuilder<QB extends QueryBuilder> extends NamedWriteable<QB>, ToXContent {
-    
-    /**
-     * Validate the query.
-     * @return a {@link QueryValidationException} containing error messages, {@code null} if query is valid.
-     * e.g. if fields that are needed to create the lucene query are missing.
-     */
-    QueryValidationException validate();
+public abstract class QueryBuilder extends ToXContentToBytes {
 
-    /**
-     * Converts this QueryBuilder to a lucene {@link Query}.
-     * Returns <tt>null</tt> if this query should be ignored in the context of
-     * parent queries.
-     *
-     * @param context additional information needed to construct the queries
-     * @return the {@link Query} or <tt>null</tt> if this query should be ignored upstream
-     * @throws QueryShardException
-     * @throws IOException
-     */
-    Query toQuery(QueryShardContext context) throws IOException;
+    protected QueryBuilder() {
+        super(XContentType.JSON);
+    }
 
-    /**
-     * Returns a {@link org.elasticsearch.common.bytes.BytesReference}
-     * containing the {@link ToXContent} output in binary format.
-     * Builds the request based on the default {@link XContentType}, either {@link Requests#CONTENT_TYPE} or provided as a constructor argument
-     */
-    //norelease once we move to serializing queries over the wire in Streamable format, this method shouldn't be needed anymore
-    BytesReference buildAsBytes();
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        doXContent(builder, params);
+        builder.endObject();
+        return builder;
+    }
 
-    /**
-     * Sets the arbitrary name to be assigned to the query (see named queries).
-     */
-    QB queryName(String queryName);
-
-    /**
-     * Returns the arbitrary name assigned to the query (see named queries).
-     */
-    String queryName();
-
-    /**
-     * Returns the boost for this query.
-     */
-    float boost();
-
-    /**
-     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
-     * weightings) have their score multiplied by the boost provided.
-     */
-    QB boost(float boost);
-
-    /**
-     * Returns the name that identifies uniquely the query
-     */
-    String getName();
+    protected abstract void doXContent(XContentBuilder builder, Params params) throws IOException;
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java b/core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java
index 75bc64b..fe2852d 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java
@@ -59,11 +59,11 @@ public abstract class QueryBuilders {
     /**
      * Creates a common query for the provided field name and text.
      *
-     * @param fieldName The field name.
+     * @param name The field name.
      * @param text The query text (to be analyzed).
      */
-    public static CommonTermsQueryBuilder commonTermsQuery(String fieldName, Object text) {
-        return new CommonTermsQueryBuilder(fieldName, text);
+    public static CommonTermsQueryBuilder commonTermsQuery(String name, Object text) {
+        return new CommonTermsQueryBuilder(name, text);
     }
 
     /**
@@ -277,8 +277,8 @@ public abstract class QueryBuilders {
      * Unlike the "NOT" clause, this still selects documents that contain undesirable terms,
      * but reduces their overall score:
      */
-    public static BoostingQueryBuilder boostingQuery(QueryBuilder positiveQuery, QueryBuilder negativeQuery) {
-        return new BoostingQueryBuilder(positiveQuery, negativeQuery);
+    public static BoostingQueryBuilder boostingQuery() {
+        return new BoostingQueryBuilder();
     }
 
     /**
@@ -312,33 +312,26 @@ public abstract class QueryBuilders {
         return new SpanFirstQueryBuilder(match, end);
     }
 
-    public static SpanNearQueryBuilder spanNearQuery(int slop) {
-        return new SpanNearQueryBuilder(slop);
+    public static SpanNearQueryBuilder spanNearQuery() {
+        return new SpanNearQueryBuilder();
     }
 
-    public static SpanNotQueryBuilder spanNotQuery(SpanQueryBuilder include, SpanQueryBuilder exclude) {
-        return new SpanNotQueryBuilder(include, exclude);
+    public static SpanNotQueryBuilder spanNotQuery() {
+        return new SpanNotQueryBuilder();
     }
 
     public static SpanOrQueryBuilder spanOrQuery() {
         return new SpanOrQueryBuilder();
     }
 
-    /** Creates a new {@code span_within} builder.
-    * @param big the big clause, it must enclose {@code little} for a match.
-    * @param little the little clause, it must be contained within {@code big} for a match.
-    */
-    public static SpanWithinQueryBuilder spanWithinQuery(SpanQueryBuilder big, SpanQueryBuilder little) {
-        return new SpanWithinQueryBuilder(big, little);
+    /** Creates a new {@code span_within} builder. */
+    public static SpanWithinQueryBuilder spanWithinQuery() {
+        return new SpanWithinQueryBuilder();
     }
 
-    /**
-     * Creates a new {@code span_containing} builder.
-     * @param big the big clause, it must enclose {@code little} for a match.
-     * @param little the little clause, it must be contained within {@code big} for a match.
-     */
-    public static SpanContainingQueryBuilder spanContainingQuery(SpanQueryBuilder big, SpanQueryBuilder little) {
-        return new SpanContainingQueryBuilder(big, little);
+    /** Creates a new {@code span_containing} builder. */
+    public static SpanContainingQueryBuilder spanContainingQuery() {
+        return new SpanContainingQueryBuilder();
     }
 
     /**
@@ -600,10 +593,11 @@ public abstract class QueryBuilders {
     }
 
     /**
-     * A terms query that can extract the terms from another doc in an index.
+     * A terms lookup filter for the provided field name. A lookup terms filter can
+     * extract the terms to filter by from another doc in an index.
      */
-    public static TermsQueryBuilder termsLookupQuery(String name) {
-        return new TermsQueryBuilder(name, (Object[]) null);
+    public static TermsLookupQueryBuilder termsLookupQuery(String name) {
+        return new TermsLookupQueryBuilder(name);
     }
 
     /**
@@ -690,7 +684,7 @@ public abstract class QueryBuilders {
     public static GeohashCellQuery.Builder geoHashCellQuery(String name, String geohash, boolean neighbors) {
         return new GeohashCellQuery.Builder(name, geohash, neighbors);
     }
-
+    
     /**
      * A filter to filter based on a polygon defined by a set of locations  / points.
      *
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java b/core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java
index 6dc2d39..936e466 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java
@@ -19,14 +19,9 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * A filter that simply wraps a query.
@@ -34,13 +29,11 @@ import java.util.Objects;
  *             query as a filter directly.
  */
 @Deprecated
-public class QueryFilterBuilder extends AbstractQueryBuilder<QueryFilterBuilder> {
-
-    public static final String NAME = "query";
+public class QueryFilterBuilder extends QueryBuilder {
 
     private final QueryBuilder queryBuilder;
 
-    static final QueryFilterBuilder PROTOTYPE = new QueryFilterBuilder(null);
+    private String queryName;
 
     /**
      * A filter that simply wraps a query.
@@ -52,56 +45,26 @@ public class QueryFilterBuilder extends AbstractQueryBuilder<QueryFilterBuilder>
     }
 
     /**
-     * @return the query builder that is wrapped by this {@link QueryFilterBuilder}
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public QueryBuilder innerQuery() {
-        return this.queryBuilder;
+    public QueryFilterBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.field(NAME);
-        queryBuilder.toXContent(builder, params);
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        // inner query builder can potentially be `null`, in that case we ignore it
-        Query innerQuery = this.queryBuilder.toQuery(context);
-        if (innerQuery == null) {
-            return null;
+        if (queryName == null) {
+            builder.field(QueryFilterParser.NAME);
+            queryBuilder.toXContent(builder, params);
+        } else {
+            builder.startObject(FQueryFilterParser.NAME);
+            builder.field("query");
+            queryBuilder.toXContent(builder, params);
+            if (queryName != null) {
+                builder.field("_name", queryName);
+            }
+            builder.endObject();
         }
-        return new ConstantScoreQuery(innerQuery);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        return validateInnerQuery(queryBuilder, null);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(queryBuilder);
-    }
-
-    @Override
-    protected boolean doEquals(QueryFilterBuilder other) {
-        return Objects.equals(queryBuilder, other.queryBuilder);
-    }
-
-    @Override
-    protected QueryFilterBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryBuilder innerQueryBuilder = in.readQuery();
-        return new QueryFilterBuilder(innerQueryBuilder);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(queryBuilder);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryFilterParser.java b/core/src/main/java/org/elasticsearch/index/query/QueryFilterParser.java
index 3a5d0ab..fdb9cb3 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryFilterParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryFilterParser.java
@@ -19,12 +19,16 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
 
 import java.io.IOException;
 
 @Deprecated
-public class QueryFilterParser extends BaseQueryParser {
+public class QueryFilterParser implements QueryParser {
+
+    public static final String NAME = "query";
 
     @Inject
     public QueryFilterParser() {
@@ -32,16 +36,11 @@ public class QueryFilterParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{QueryFilterBuilder.NAME};
-    }
-
-    @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
-        return new QueryFilterBuilder(parseContext.parseInnerQueryBuilder());
+        return new String[]{NAME};
     }
 
     @Override
-    public QueryFilterBuilder getBuilderPrototype() {
-        return QueryFilterBuilder.PROTOTYPE;
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
+        return new ConstantScoreQuery(parseContext.parseInnerQuery());
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java b/core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java
index de10877..4b12200 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java
@@ -19,118 +19,207 @@
 
 package org.elasticsearch.index.query;
 
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.Maps;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.queryparser.classic.MapperQueryParser;
+import org.apache.lucene.queryparser.classic.QueryParserSettings;
+import org.apache.lucene.search.Filter;
 import org.apache.lucene.search.Query;
+import org.apache.lucene.search.join.BitDocIdSetFilter;
+import org.apache.lucene.search.similarities.Similarity;
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
+import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.Index;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
+import org.elasticsearch.index.analysis.AnalysisService;
+import org.elasticsearch.index.fielddata.IndexFieldData;
+import org.elasticsearch.index.mapper.*;
+import org.elasticsearch.index.mapper.core.StringFieldMapper;
+import org.elasticsearch.index.mapper.object.ObjectMapper;
+import org.elasticsearch.index.query.support.NestedScope;
+import org.elasticsearch.index.similarity.SimilarityService;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.search.fetch.innerhits.InnerHitsContext;
+import org.elasticsearch.search.internal.SearchContext;
+import org.elasticsearch.search.lookup.SearchLookup;
 
 import java.io.IOException;
+import java.util.*;
 
 public class QueryParseContext {
 
     private static final ParseField CACHE = new ParseField("_cache").withAllDeprecated("Elasticsearch makes its own caching decisions");
     private static final ParseField CACHE_KEY = new ParseField("_cache_key").withAllDeprecated("Filters are always used as cache keys");
 
-    private XContentParser parser;
+    private static ThreadLocal<String[]> typesContext = new ThreadLocal<>();
+
+    public static void setTypes(String[] types) {
+        typesContext.set(types);
+    }
+
+    public static String[] getTypes() {
+        return typesContext.get();
+    }
+
+    public static String[] setTypesWithPrevious(String[] types) {
+        String[] old = typesContext.get();
+        setTypes(types);
+        return old;
+    }
+
+    public static void removeTypes() {
+        typesContext.remove();
+    }
+
     private final Index index;
-    //norelease this flag is also used in the QueryShardContext, we need to make sure we set it there correctly in doToQuery()
-    private boolean isFilter;
+
+    private final Version indexVersionCreated;
+
+    private final IndexQueryParserService indexQueryParser;
+
+    private final Map<String, Query> namedQueries = Maps.newHashMap();
+
+    private final MapperQueryParser queryParser = new MapperQueryParser(this);
+
+    private XContentParser parser;
+
     private ParseFieldMatcher parseFieldMatcher;
 
-    //norelease this can eventually be deleted when context() method goes away
-    private final QueryShardContext shardContext;
-    private IndicesQueriesRegistry indicesQueriesRegistry;
+    private boolean allowUnmappedFields;
+
+    private boolean mapUnmappedFieldAsString;
+
+    private NestedScope nestedScope;
+
+    private boolean isFilter;
 
-    public QueryParseContext(Index index, IndicesQueriesRegistry registry) {
+    public QueryParseContext(Index index, IndexQueryParserService indexQueryParser) {
         this.index = index;
-        this.indicesQueriesRegistry = registry;
-        this.shardContext = null;
+        this.indexVersionCreated = Version.indexCreated(indexQueryParser.indexSettings());
+        this.indexQueryParser = indexQueryParser;
     }
 
-    QueryParseContext(QueryShardContext context) {
-        this.shardContext = context;
-        this.index = context.index();
-        this.indicesQueriesRegistry = context.indexQueryParserService().indicesQueriesRegistry();
+    public void parseFieldMatcher(ParseFieldMatcher parseFieldMatcher) {
+        this.parseFieldMatcher = parseFieldMatcher;
+    }
+
+    public ParseFieldMatcher parseFieldMatcher() {
+        return parseFieldMatcher;
     }
 
     public void reset(XContentParser jp) {
+        allowUnmappedFields = indexQueryParser.defaultAllowUnmappedFields();
         this.parseFieldMatcher = ParseFieldMatcher.EMPTY;
+        this.lookup = null;
         this.parser = jp;
+        this.namedQueries.clear();
+        this.nestedScope = new NestedScope();
+        this.isFilter = false;
     }
 
-    //norelease this is still used in BaseQueryParserTemp and FunctionScoreQueryParse, remove if not needed there anymore
-    @Deprecated
-    public QueryShardContext shardContext() {
-        return this.shardContext;
+    public Index index() {
+        return this.index;
+    }
+
+    public void parser(XContentParser parser) {
+        this.parser = parser;
     }
 
     public XContentParser parser() {
-        return this.parser;
+        return parser;
+    }
+    
+    public IndexQueryParserService indexQueryParserService() {
+        return indexQueryParser;
     }
 
-    public void parseFieldMatcher(ParseFieldMatcher parseFieldMatcher) {
-        this.parseFieldMatcher = parseFieldMatcher;
+    public AnalysisService analysisService() {
+        return indexQueryParser.analysisService;
     }
 
-    public boolean isDeprecatedSetting(String setting) {
-        return parseFieldMatcher.match(setting, CACHE) || parseFieldMatcher.match(setting, CACHE_KEY);
+    public ScriptService scriptService() {
+        return indexQueryParser.scriptService;
     }
 
-    public Index index() {
-        return this.index;
+    public MapperService mapperService() {
+        return indexQueryParser.mapperService;
     }
 
-    /**
-     * @deprecated replaced by calls to parseInnerFilterToQueryBuilder(String queryName) for the resulting queries
-     */
     @Nullable
-    @Deprecated
-    //norelease should be possible to remove after refactoring all queries
-    public Query parseInnerFilter(String queryName) throws IOException, QueryShardException {
-        assert this.shardContext != null;
-        QueryBuilder builder = parseInnerFilterToQueryBuilder(queryName);
-        return (builder != null) ? builder.toQuery(this.shardContext) : null;
+    public SimilarityService similarityService() {
+        return indexQueryParser.similarityService;
     }
 
-    /**
-     * @deprecated replaced by calls to parseInnerFilterToQueryBuilder() for the resulting queries
-     */
-    @Nullable
-    @Deprecated
-    //norelease should be possible to remove after refactoring all queries
-    public Query parseInnerFilter() throws QueryShardException, IOException {
-        assert this.shardContext != null;
-        QueryBuilder builder = parseInnerFilterToQueryBuilder();
-        Query result = null;
-        if (builder != null) {
-            result = builder.toQuery(this.shardContext);
+    public Similarity searchSimilarity() {
+        return indexQueryParser.similarityService != null ? indexQueryParser.similarityService.similarity() : null;
+    }
+
+    public String defaultField() {
+        return indexQueryParser.defaultField();
+    }
+
+    public boolean queryStringLenient() {
+        return indexQueryParser.queryStringLenient();
+    }
+
+    public MapperQueryParser queryParser(QueryParserSettings settings) {
+        queryParser.reset(settings);
+        return queryParser;
+    }
+
+    public BitDocIdSetFilter bitsetFilter(Filter filter) {
+        return indexQueryParser.bitsetFilterCache.getBitDocIdSetFilter(filter);
+    }
+
+    public <IFD extends IndexFieldData<?>> IFD getForField(MappedFieldType mapper) {
+        return indexQueryParser.fieldDataService.getForField(mapper);
+    }
+
+    public void addNamedQuery(String name, Query query) {
+        if (query != null) {
+            namedQueries.put(name, query);
         }
-        return result;
+    }
+
+    public ImmutableMap<String, Query> copyNamedQueries() {
+        return ImmutableMap.copyOf(namedQueries);
+    }
+
+    public void combineNamedQueries(QueryParseContext context) {
+        namedQueries.putAll(context.namedQueries);
     }
 
     /**
-     * @deprecated replaced by calls to parseInnerQueryBuilder() for the resulting queries
+     * Return whether we are currently parsing a filter or a query.
      */
-    @Nullable
-    @Deprecated
-    //norelease should be possible to remove after refactoring all queries
-    public Query parseInnerQuery() throws IOException, QueryShardException {
-        QueryBuilder builder = parseInnerQueryBuilder();
-        Query result = null;
-        if (builder != null) {
-            result = builder.toQuery(this.shardContext);
+    public boolean isFilter() {
+        return isFilter;
+    }
+
+    public void addInnerHits(String name, InnerHitsContext.BaseInnerHits context) {
+        SearchContext sc = SearchContext.current();
+        if (sc == null) {
+            throw new QueryParsingException(this, "inner_hits unsupported");
         }
-        return result;
+
+        InnerHitsContext innerHitsContext;
+        if (sc.innerHits() == null) {
+            innerHitsContext = new InnerHitsContext(new HashMap<String, InnerHitsContext.BaseInnerHits>());
+            sc.innerHits(innerHitsContext);
+        } else {
+            innerHitsContext = sc.innerHits();
+        }
+        innerHitsContext.addInnerHitDefinition(name, context);
     }
 
-    /**
-     * @return a new QueryBuilder based on the current state of the parser
-     * @throws IOException
-     */
-    public QueryBuilder parseInnerQueryBuilder() throws IOException {
+    @Nullable
+    public Query parseInnerQuery() throws QueryParsingException, IOException {
         // move to START object
         XContentParser.Token token;
         if (parser.currentToken() != XContentParser.Token.START_OBJECT) {
@@ -142,7 +231,7 @@ public class QueryParseContext {
         token = parser.nextToken();
         if (token == XContentParser.Token.END_OBJECT) {
             // empty query
-            return EmptyQueryBuilder.PROTOTYPE;
+            return null;
         }
         if (token != XContentParser.Token.FIELD_NAME) {
             throw new QueryParsingException(this, "[_na] query malformed, no field after start_object");
@@ -154,11 +243,11 @@ public class QueryParseContext {
             throw new QueryParsingException(this, "[_na] query malformed, no field after start_object");
         }
 
-        QueryParser queryParser = queryParser(queryName);
+        QueryParser queryParser = indexQueryParser.queryParser(queryName);
         if (queryParser == null) {
             throw new QueryParsingException(this, "No query registered for [" + queryName + "]");
         }
-        QueryBuilder result = queryParser.fromXContent(this);
+        Query result = queryParser.parse(this);
         if (parser.currentToken() == XContentParser.Token.END_OBJECT || parser.currentToken() == XContentParser.Token.END_ARRAY) {
             // if we are at END_OBJECT, move to the next one...
             parser.nextToken();
@@ -166,49 +255,138 @@ public class QueryParseContext {
         return result;
     }
 
-    /**
-     * @return a new QueryBuilder based on the current state of the parser, but does so that the inner query
-     * is parsed to a filter
-     * @throws IOException
-     */
     @Nullable
-    public QueryBuilder parseInnerFilterToQueryBuilder() throws IOException {
+    public Query parseInnerFilter() throws QueryParsingException, IOException {
         final boolean originalIsFilter = isFilter;
         try {
             isFilter = true;
-            return parseInnerQueryBuilder();
+            return parseInnerQuery();
         } finally {
             isFilter = originalIsFilter;
         }
     }
 
-    QueryBuilder parseInnerFilterToQueryBuilder(String queryName) throws IOException, QueryParsingException {
+    public Query parseInnerFilter(String queryName) throws IOException, QueryParsingException {
         final boolean originalIsFilter = isFilter;
         try {
             isFilter = true;
-            QueryParser queryParser = queryParser(queryName);
+            QueryParser queryParser = indexQueryParser.queryParser(queryName);
             if (queryParser == null) {
                 throw new QueryParsingException(this, "No query registered for [" + queryName + "]");
             }
-            return queryParser.fromXContent(this);
+            return queryParser.parse(this);
         } finally {
             isFilter = originalIsFilter;
         }
     }
 
-    public boolean isFilter() {
-        return this.isFilter;
+    public Collection<String> simpleMatchToIndexNames(String pattern) {
+        return indexQueryParser.mapperService.simpleMatchToIndexNames(pattern, getTypes());
     }
 
-    public ParseFieldMatcher parseFieldMatcher() {
-        return parseFieldMatcher;
+    public MappedFieldType fieldMapper(String name) {
+        return failIfFieldMappingNotFound(name, indexQueryParser.mapperService.smartNameFieldType(name, getTypes()));
+    }
+
+    public ObjectMapper getObjectMapper(String name) {
+        return indexQueryParser.mapperService.getObjectMapper(name, getTypes());
+    }
+
+    /** Gets the search analyzer for the given field, or the default if there is none present for the field
+     * TODO: remove this by moving defaults into mappers themselves
+     */
+    public Analyzer getSearchAnalyzer(MappedFieldType fieldType) {
+        if (fieldType.searchAnalyzer() != null) {
+            return fieldType.searchAnalyzer();
+        }
+        return mapperService().searchAnalyzer();
+    }
+
+    /** Gets the search quote nalyzer for the given field, or the default if there is none present for the field
+     * TODO: remove this by moving defaults into mappers themselves
+     */
+    public Analyzer getSearchQuoteAnalyzer(MappedFieldType fieldType) {
+        if (fieldType.searchQuoteAnalyzer() != null) {
+            return fieldType.searchQuoteAnalyzer();
+        }
+        return mapperService().searchQuoteAnalyzer();
     }
 
-    public void parser(XContentParser innerParser) {
-        this.parser = innerParser;
+    public void setAllowUnmappedFields(boolean allowUnmappedFields) {
+        this.allowUnmappedFields = allowUnmappedFields;
     }
 
-    QueryParser queryParser(String name) {
-        return indicesQueriesRegistry.queryParsers().get(name);
+    public void setMapUnmappedFieldAsString(boolean mapUnmappedFieldAsString) {
+        this.mapUnmappedFieldAsString = mapUnmappedFieldAsString;
     }
+
+    private MappedFieldType failIfFieldMappingNotFound(String name, MappedFieldType fieldMapping) {
+        if (allowUnmappedFields) {
+            return fieldMapping;
+        } else if (mapUnmappedFieldAsString){
+            StringFieldMapper.Builder builder = MapperBuilders.stringField(name);
+            // it would be better to pass the real index settings, but they are not easily accessible from here...
+            Settings settings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, indexQueryParser.getIndexCreatedVersion()).build();
+            return builder.build(new Mapper.BuilderContext(settings, new ContentPath(1))).fieldType();
+        } else {
+            Version indexCreatedVersion = indexQueryParser.getIndexCreatedVersion();
+            if (fieldMapping == null && indexCreatedVersion.onOrAfter(Version.V_1_4_0_Beta1)) {
+                throw new QueryParsingException(this, "Strict field resolution and no field mapping can be found for the field with name ["
+                        + name + "]");
+            } else {
+                return fieldMapping;
+            }
+        }
+    }
+
+    /**
+     * Returns the narrowed down explicit types, or, if not set, all types.
+     */
+    public Collection<String> queryTypes() {
+        String[] types = getTypes();
+        if (types == null || types.length == 0) {
+            return mapperService().types();
+        }
+        if (types.length == 1 && types[0].equals("_all")) {
+            return mapperService().types();
+        }
+        return Arrays.asList(types);
+    }
+
+    private SearchLookup lookup = null;
+
+    public SearchLookup lookup() {
+        SearchContext current = SearchContext.current();
+        if (current != null) {
+            return current.lookup();
+        }
+        if (lookup == null) {
+            lookup = new SearchLookup(mapperService(), indexQueryParser.fieldDataService, null);
+        }
+        return lookup;
+    }
+
+    public long nowInMillis() {
+        SearchContext current = SearchContext.current();
+        if (current != null) {
+            return current.nowInMillis();
+        }
+        return System.currentTimeMillis();
+    }
+
+    public NestedScope nestedScope() {
+        return nestedScope;
+    }
+
+    /**
+     * Return whether the setting is deprecated.
+     */
+    public boolean isDeprecatedSetting(String setting) {
+        return parseFieldMatcher.match(setting, CACHE) || parseFieldMatcher.match(setting, CACHE_KEY);
+    }
+
+    public Version indexVersionCreated() {
+        return indexVersionCreated;
+    }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryParser.java b/core/src/main/java/org/elasticsearch/index/query/QueryParser.java
index f6a019a..eff585a 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryParser.java
@@ -35,33 +35,11 @@ public interface QueryParser {
     String[] names();
 
     /**
-     * Parses the into a query from the current parser location. Will be at
-     * "START_OBJECT" location, and should end when the token is at the matching
-     * "END_OBJECT".
+     * Parses the into a query from the current parser location. Will be at "START_OBJECT" location,
+     * and should end when the token is at the matching "END_OBJECT".
      * <p/>
-     * Returns <tt>null</tt> if this query should be ignored in the context of
-     * the DSL.
+     * Returns <tt>null</tt> if this query should be ignored in the context of the DSL.
      */
-    //norelease can be removed in favour of fromXContent once search requests can be parsed on the coordinating node
     @Nullable
-    Query parse(QueryShardContext context) throws IOException, QueryParsingException;
-
-    /**
-     * Creates a new {@link QueryBuilder} from the query held by the {@link QueryShardContext}
-     * in {@link org.elasticsearch.common.xcontent.XContent} format
-     *
-     * @param parseContext
-     *            the input parse context. The state on the parser contained in
-     *            this context will be changed as a side effect of this method
-     *            call
-     * @return the new QueryBuilder
-     * @throws IOException
-     * @throws QueryParsingException
-     */
-    QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException;
-
-    /**
-     * @return an empty {@link QueryBuilder} instance for this parser that can be used for deserialization
-     */
-    QueryBuilder<? extends QueryBuilder> getBuilderPrototype();
+    Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException;
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryParsingException.java b/core/src/main/java/org/elasticsearch/index/query/QueryParsingException.java
index 80acae7..c606953 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryParsingException.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryParsingException.java
@@ -31,8 +31,7 @@ import org.elasticsearch.rest.RestStatus;
 import java.io.IOException;
 
 /**
- * Exception that can be used when parsing queries with a given {@link QueryParseContext}.
- * Can contain information about location of the error.
+ *
  */
 public class QueryParsingException extends ElasticsearchException {
 
@@ -72,15 +71,9 @@ public class QueryParsingException extends ElasticsearchException {
         this.columnNumber = col;
     }
 
-    public QueryParsingException(StreamInput in) throws IOException{
-        super(in);
-        lineNumber = in.readInt();
-        columnNumber = in.readInt();
-    }
-
     /**
      * Line number of the location of the error
-     *
+     * 
      * @return the line number or -1 if unknown
      */
     public int getLineNumber() {
@@ -89,7 +82,7 @@ public class QueryParsingException extends ElasticsearchException {
 
     /**
      * Column number of the location of the error
-     *
+     * 
      * @return the column number or -1 if unknown
      */
     public int getColumnNumber() {
@@ -116,4 +109,11 @@ public class QueryParsingException extends ElasticsearchException {
         out.writeInt(lineNumber);
         out.writeInt(columnNumber);
     }
+
+    public QueryParsingException(StreamInput in) throws IOException{
+        super(in);
+        lineNumber = in.readInt();
+        columnNumber = in.readInt();
+    }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java b/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java
deleted file mode 100644
index abaf9a7..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java
+++ /dev/null
@@ -1,327 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import com.google.common.collect.ImmutableMap;
-import com.google.common.collect.Maps;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.queryparser.classic.MapperQueryParser;
-import org.apache.lucene.queryparser.classic.QueryParserSettings;
-import org.apache.lucene.search.Filter;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.join.BitDocIdSetFilter;
-import org.apache.lucene.search.similarities.Similarity;
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.analysis.AnalysisService;
-import org.elasticsearch.index.fielddata.IndexFieldData;
-import org.elasticsearch.index.mapper.ContentPath;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.Mapper;
-import org.elasticsearch.index.mapper.MapperBuilders;
-import org.elasticsearch.index.mapper.MapperService;
-import org.elasticsearch.index.mapper.core.StringFieldMapper;
-import org.elasticsearch.index.mapper.object.ObjectMapper;
-import org.elasticsearch.index.query.support.NestedScope;
-import org.elasticsearch.index.similarity.SimilarityService;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsContext;
-import org.elasticsearch.search.internal.SearchContext;
-import org.elasticsearch.search.lookup.SearchLookup;
-
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- * Context object used to create lucene queries on the shard level.
- */
-public class QueryShardContext {
-
-    private static ThreadLocal<String[]> typesContext = new ThreadLocal<>();
-
-    public static void setTypes(String[] types) {
-        typesContext.set(types);
-    }
-
-    public static String[] getTypes() {
-        return typesContext.get();
-    }
-
-    public static String[] setTypesWithPrevious(String[] types) {
-        String[] old = typesContext.get();
-        setTypes(types);
-        return old;
-    }
-
-    public static void removeTypes() {
-        typesContext.remove();
-    }
-
-    private final Index index;
-
-    private final Version indexVersionCreated;
-
-    private final IndexQueryParserService indexQueryParser;
-
-    private final Map<String, Query> namedQueries = Maps.newHashMap();
-
-    private final MapperQueryParser queryParser = new MapperQueryParser(this);
-
-    private ParseFieldMatcher parseFieldMatcher;
-
-    private boolean allowUnmappedFields;
-
-    private boolean mapUnmappedFieldAsString;
-
-    private NestedScope nestedScope;
-
-    //norelease this should be possible to remove once query context are completely separated
-    private QueryParseContext parseContext;
-
-    private boolean isFilter;
-
-    public QueryShardContext(Index index, IndexQueryParserService indexQueryParser) {
-        this.index = index;
-        this.indexVersionCreated = Version.indexCreated(indexQueryParser.indexSettings());
-        this.indexQueryParser = indexQueryParser;
-        this.parseContext = new QueryParseContext(this);
-    }
-
-    public void parseFieldMatcher(ParseFieldMatcher parseFieldMatcher) {
-        this.parseFieldMatcher = parseFieldMatcher;
-    }
-
-    public ParseFieldMatcher parseFieldMatcher() {
-        return parseFieldMatcher;
-    }
-
-    private void reset() {
-        allowUnmappedFields = indexQueryParser.defaultAllowUnmappedFields();
-        this.parseFieldMatcher = ParseFieldMatcher.EMPTY;
-        this.lookup = null;
-        this.namedQueries.clear();
-        this.nestedScope = new NestedScope();
-    }
-
-    //norelease remove parser argument once query contexts are separated
-    public void reset(XContentParser jp) {
-        this.reset();
-        this.parseContext.reset(jp);
-    }
-
-    public Index index() {
-        return this.index;
-    }
-
-    public IndexQueryParserService indexQueryParserService() {
-        return indexQueryParser;
-    }
-
-    public AnalysisService analysisService() {
-        return indexQueryParser.analysisService;
-    }
-
-    public ScriptService scriptService() {
-        return indexQueryParser.scriptService;
-    }
-
-    public MapperService mapperService() {
-        return indexQueryParser.mapperService;
-    }
-
-    @Nullable
-    public SimilarityService similarityService() {
-        return indexQueryParser.similarityService;
-    }
-
-    public Similarity searchSimilarity() {
-        return indexQueryParser.similarityService != null ? indexQueryParser.similarityService.similarity() : null;
-    }
-
-    public String defaultField() {
-        return indexQueryParser.defaultField();
-    }
-
-    public boolean queryStringLenient() {
-        return indexQueryParser.queryStringLenient();
-    }
-
-    public MapperQueryParser queryParser(QueryParserSettings settings) {
-        queryParser.reset(settings);
-        return queryParser;
-    }
-
-    public BitDocIdSetFilter bitsetFilter(Filter filter) {
-        return indexQueryParser.bitsetFilterCache.getBitDocIdSetFilter(filter);
-    }
-
-    public <IFD extends IndexFieldData<?>> IFD getForField(MappedFieldType mapper) {
-        return indexQueryParser.fieldDataService.getForField(mapper);
-    }
-
-    public void addNamedQuery(String name, Query query) {
-        namedQueries.put(name, query);
-    }
-
-    public ImmutableMap<String, Query> copyNamedQueries() {
-        return ImmutableMap.copyOf(namedQueries);
-    }
-
-    public void combineNamedQueries(QueryShardContext context) {
-        namedQueries.putAll(context.namedQueries);
-    }
-
-    /**
-     * Return whether we are currently parsing a filter or a query.
-     */
-    public boolean isFilter() {
-        return isFilter;
-    }
-
-    public void addInnerHits(String name, InnerHitsContext.BaseInnerHits context) {
-        SearchContext sc = SearchContext.current();
-        if (sc == null) {
-            throw new QueryShardException(this, "inner_hits unsupported");
-        }
-
-        InnerHitsContext innerHitsContext;
-        if (sc.innerHits() == null) {
-            innerHitsContext = new InnerHitsContext(new HashMap<String, InnerHitsContext.BaseInnerHits>());
-            sc.innerHits(innerHitsContext);
-        } else {
-            innerHitsContext = sc.innerHits();
-        }
-        innerHitsContext.addInnerHitDefinition(name, context);
-    }
-
-    public Collection<String> simpleMatchToIndexNames(String pattern) {
-        return indexQueryParser.mapperService.simpleMatchToIndexNames(pattern, getTypes());
-    }
-
-    public MappedFieldType fieldMapper(String name) {
-        return failIfFieldMappingNotFound(name, indexQueryParser.mapperService.smartNameFieldType(name, getTypes()));
-    }
-
-    public ObjectMapper getObjectMapper(String name) {
-        return indexQueryParser.mapperService.getObjectMapper(name, getTypes());
-    }
-
-    /** Gets the search analyzer for the given field, or the default if there is none present for the field
-     * TODO: remove this by moving defaults into mappers themselves
-     */
-    public Analyzer getSearchAnalyzer(MappedFieldType fieldType) {
-        if (fieldType.searchAnalyzer() != null) {
-            return fieldType.searchAnalyzer();
-        }
-        return mapperService().searchAnalyzer();
-    }
-
-    /** Gets the search quote nalyzer for the given field, or the default if there is none present for the field
-     * TODO: remove this by moving defaults into mappers themselves
-     */
-    public Analyzer getSearchQuoteAnalyzer(MappedFieldType fieldType) {
-        if (fieldType.searchQuoteAnalyzer() != null) {
-            return fieldType.searchQuoteAnalyzer();
-        }
-        return mapperService().searchQuoteAnalyzer();
-    }
-
-    public void setAllowUnmappedFields(boolean allowUnmappedFields) {
-        this.allowUnmappedFields = allowUnmappedFields;
-    }
-
-    public void setMapUnmappedFieldAsString(boolean mapUnmappedFieldAsString) {
-        this.mapUnmappedFieldAsString = mapUnmappedFieldAsString;
-    }
-
-    private MappedFieldType failIfFieldMappingNotFound(String name, MappedFieldType fieldMapping) {
-        if (allowUnmappedFields) {
-            return fieldMapping;
-        } else if (mapUnmappedFieldAsString){
-            StringFieldMapper.Builder builder = MapperBuilders.stringField(name);
-            // it would be better to pass the real index settings, but they are not easily accessible from here...
-            Settings settings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, indexQueryParser.getIndexCreatedVersion()).build();
-            return builder.build(new Mapper.BuilderContext(settings, new ContentPath(1))).fieldType();
-        } else {
-            Version indexCreatedVersion = indexQueryParser.getIndexCreatedVersion();
-            if (fieldMapping == null && indexCreatedVersion.onOrAfter(Version.V_1_4_0_Beta1)) {
-                throw new QueryShardException(this, "Strict field resolution and no field mapping can be found for the field with name ["
-                        + name + "]");
-            } else {
-                return fieldMapping;
-            }
-        }
-    }
-
-    /**
-     * Returns the narrowed down explicit types, or, if not set, all types.
-     */
-    public Collection<String> queryTypes() {
-        String[] types = getTypes();
-        if (types == null || types.length == 0) {
-            return mapperService().types();
-        }
-        if (types.length == 1 && types[0].equals("_all")) {
-            return mapperService().types();
-        }
-        return Arrays.asList(types);
-    }
-
-    private SearchLookup lookup = null;
-
-    public SearchLookup lookup() {
-        SearchContext current = SearchContext.current();
-        if (current != null) {
-            return current.lookup();
-        }
-        if (lookup == null) {
-            lookup = new SearchLookup(mapperService(), indexQueryParser.fieldDataService, null);
-        }
-        return lookup;
-    }
-
-    public long nowInMillis() {
-        SearchContext current = SearchContext.current();
-        if (current != null) {
-            return current.nowInMillis();
-        }
-        return System.currentTimeMillis();
-    }
-
-    public NestedScope nestedScope() {
-        return nestedScope;
-    }
-
-    public Version indexVersionCreated() {
-        return indexVersionCreated;
-    }
-
-    public QueryParseContext parseContext() {
-        return this.parseContext;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryShardException.java b/core/src/main/java/org/elasticsearch/index/query/QueryShardException.java
deleted file mode 100644
index da4c8c6..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/QueryShardException.java
+++ /dev/null
@@ -1,72 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.rest.RestStatus;
-
-import java.io.IOException;
-
-/**
- * Exception that is thrown when creating lucene queries on the shard
- */
-public class QueryShardException extends ElasticsearchException {
-
-    public QueryShardException(QueryShardContext context, String msg, Object... args) {
-        this(context, msg, null, args);
-    }
-
-    public QueryShardException(QueryShardContext context, String msg, Throwable cause, Object... args) {
-        super(msg, cause, args);
-        setIndex(context.index());
-    }
-
-    /**
-     * This constructor is provided for use in unit tests where a
-     * {@link QueryShardContext} may not be available
-     */
-    public QueryShardException(Index index, int line, int col, String msg, Throwable cause) {
-        super(msg, cause);
-        setIndex(index);
-    }
-
-    public QueryShardException(StreamInput in) throws IOException{
-        super(in);
-    }
-
-    @Override
-    public RestStatus status() {
-        return RestStatus.BAD_REQUEST;
-    }
-
-    @Override
-    protected void innerToXContent(XContentBuilder builder, Params params) throws IOException {
-        super.innerToXContent(builder, params);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java
index baff082..7965eec 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java
@@ -38,9 +38,12 @@ import static com.google.common.collect.Lists.newArrayList;
  * them either using DisMax or a plain boolean query (see {@link #useDisMax(boolean)}).
  * <p/>
  */
-public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQueryBuilder> {
+public class QueryStringQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<QueryStringQueryBuilder> {
 
-    public static final String NAME = "query_string";
+    public enum Operator {
+        OR,
+        AND
+    }
 
     private final String queryString;
 
@@ -65,6 +68,9 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
 
     private Locale locale;
 
+
+    private float boost = -1;
+
     private Fuzziness fuzziness;
     private int fuzzyPrefixLength = -1;
     private int fuzzyMaxExpansions = -1;
@@ -86,13 +92,13 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
 
     private Boolean lenient;
 
+    private String queryName;
+
     private String timeZone;
 
     /** To limit effort spent determinizing regexp queries. */
     private Integer maxDeterminizedStates;
 
-    static final QueryStringQueryBuilder PROTOTYPE = new QueryStringQueryBuilder(null);
-
     public QueryStringQueryBuilder(String queryString) {
         this.queryString = queryString;
     }
@@ -153,11 +159,11 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
     /**
      * Sets the boolean operator of the query parser used to parse the query string.
      * <p/>
-     * <p>In default mode ({@link Operator#OR}) terms without any modifiers
+     * <p>In default mode ({@link FieldQueryBuilder.Operator#OR}) terms without any modifiers
      * are considered optional: for example <code>capital of Hungary</code> is equal to
      * <code>capital OR of OR Hungary</code>.
      * <p/>
-     * <p>In {@link Operator#AND} mode terms are considered to be in conjunction: the
+     * <p>In {@link FieldQueryBuilder.Operator#AND} mode terms are considered to be in conjunction: the
      * above mentioned query is parsed as <code>capital AND of AND Hungary</code>
      */
     public QueryStringQueryBuilder defaultOperator(Operator defaultOperator) {
@@ -289,6 +295,16 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
     }
 
     /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
+    @Override
+    public QueryStringQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
+    /**
      * An optional field name suffix to automatically try and add to the field searched when using quoted text.
      */
     public QueryStringQueryBuilder quoteFieldSuffix(String quoteFieldSuffix) {
@@ -305,6 +321,14 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
         return this;
     }
 
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public QueryStringQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
     public QueryStringQueryBuilder locale(Locale locale) {
         this.locale = locale;
         return this;
@@ -320,7 +344,7 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(QueryStringQueryParser.NAME);
         builder.field("query", queryString);
         if (defaultField != null) {
             builder.field("default_field", defaultField);
@@ -368,6 +392,9 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
         if (fuzziness != null) {
             fuzziness.toXContent(builder, params);
         }
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
         if (fuzzyPrefixLength != -1) {
             builder.field("fuzzy_prefix_length", fuzzyPrefixLength);
         }
@@ -395,18 +422,15 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
         if (lenient != null) {
             builder.field("lenient", lenient);
         }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         if (locale != null) {
             builder.field("locale", locale.toString());
         }
         if (timeZone != null) {
             builder.field("time_zone", timeZone);
         }
-        printBoostAndQueryName(builder);
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java
index 20ab078..2a21d3d 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java
@@ -47,8 +47,9 @@ import static org.elasticsearch.common.lucene.search.Queries.fixNegativeQueryIfN
 /**
  *
  */
-public class QueryStringQueryParser extends BaseQueryParserTemp {
+public class QueryStringQueryParser implements QueryParser {
 
+    public static final String NAME = "query_string";
     private static final ParseField FUZZINESS = Fuzziness.FIELD.withDeprecation("fuzzy_min_sim");
 
     private final boolean defaultAnalyzeWildcard;
@@ -62,18 +63,17 @@ public class QueryStringQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{QueryStringQueryBuilder.NAME, Strings.toCamelCase(QueryStringQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String queryName = null;
         QueryParserSettings qpSettings = new QueryParserSettings();
-        qpSettings.defaultField(context.defaultField());
-        qpSettings.lenient(context.queryStringLenient());
+        qpSettings.defaultField(parseContext.defaultField());
+        qpSettings.lenient(parseContext.queryStringLenient());
         qpSettings.analyzeWildcard(defaultAnalyzeWildcard);
         qpSettings.allowLeadingWildcard(defaultAllowLeadingWildcard);
         qpSettings.locale(Locale.ROOT);
@@ -106,7 +106,7 @@ public class QueryStringQueryParser extends BaseQueryParserTemp {
                         }
 
                         if (Regex.isSimpleMatchPattern(fField)) {
-                            for (String field : context.mapperService().simpleMatchToIndexNames(fField)) {
+                            for (String field : parseContext.mapperService().simpleMatchToIndexNames(fField)) {
                                 qpSettings.fields().add(field);
                                 if (fBoost != -1) {
                                     if (qpSettings.boosts() == null) {
@@ -144,13 +144,13 @@ public class QueryStringQueryParser extends BaseQueryParserTemp {
                         throw new QueryParsingException(parseContext, "Query default operator [" + op + "] is not allowed");
                     }
                 } else if ("analyzer".equals(currentFieldName)) {
-                    NamedAnalyzer analyzer = context.analysisService().analyzer(parser.text());
+                    NamedAnalyzer analyzer = parseContext.analysisService().analyzer(parser.text());
                     if (analyzer == null) {
                         throw new QueryParsingException(parseContext, "[query_string] analyzer [" + parser.text() + "] not found");
                     }
                     qpSettings.forcedAnalyzer(analyzer);
                 } else if ("quote_analyzer".equals(currentFieldName) || "quoteAnalyzer".equals(currentFieldName)) {
-                    NamedAnalyzer analyzer = context.analysisService().analyzer(parser.text());
+                    NamedAnalyzer analyzer = parseContext.analysisService().analyzer(parser.text());
                     if (analyzer == null) {
                         throw new QueryParsingException(parseContext, "[query_string] quote_analyzer [" + parser.text()
                                 + "] not found");
@@ -215,16 +215,16 @@ public class QueryStringQueryParser extends BaseQueryParserTemp {
         if (qpSettings.queryString() == null) {
             throw new QueryParsingException(parseContext, "query_string must be provided with a [query]");
         }
-        qpSettings.defaultAnalyzer(context.mapperService().searchAnalyzer());
-        qpSettings.defaultQuoteAnalyzer(context.mapperService().searchQuoteAnalyzer());
+        qpSettings.defaultAnalyzer(parseContext.mapperService().searchAnalyzer());
+        qpSettings.defaultQuoteAnalyzer(parseContext.mapperService().searchQuoteAnalyzer());
 
         if (qpSettings.escape()) {
             qpSettings.queryString(org.apache.lucene.queryparser.classic.QueryParser.escape(qpSettings.queryString()));
         }
 
-        qpSettings.queryTypes(context.queryTypes());
+        qpSettings.queryTypes(parseContext.queryTypes());
 
-        MapperQueryParser queryParser = context.queryParser(qpSettings);
+        MapperQueryParser queryParser = parseContext.queryParser(qpSettings);
 
         try {
             Query query = queryParser.parse(qpSettings.queryString());
@@ -239,16 +239,11 @@ public class QueryStringQueryParser extends BaseQueryParserTemp {
                 Queries.applyMinimumShouldMatch((BooleanQuery) query, qpSettings.minimumShouldMatch());
             }
             if (queryName != null) {
-                context.addNamedQuery(queryName, query);
+                parseContext.addNamedQuery(queryName, query);
             }
             return query;
         } catch (org.apache.lucene.queryparser.classic.ParseException e) {
             throw new QueryParsingException(parseContext, "Failed to parse query [" + qpSettings.queryString() + "]", e);
         }
     }
-
-    @Override
-    public QueryStringQueryBuilder getBuilderPrototype() {
-        return QueryStringQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryValidationException.java b/core/src/main/java/org/elasticsearch/index/query/QueryValidationException.java
deleted file mode 100644
index 3761a82..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/QueryValidationException.java
+++ /dev/null
@@ -1,100 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import java.util.ArrayList;
-import java.util.List;
-
-/**
- * This exception can be used to indicate various reasons why validation of a query has failed.
- */
-public class QueryValidationException extends IllegalArgumentException {
-
-    private final List<String> validationErrors = new ArrayList<>();
-
-    public QueryValidationException(String error) {
-        super("query validation failed");
-        validationErrors.add(error);
-    }
-
-    public QueryValidationException(List<String> errors) {
-        super("query validation failed");
-        validationErrors.addAll(errors);
-    }
-
-    public void addValidationError(String error) {
-        validationErrors.add(error);
-    }
-
-    public void addValidationErrors(Iterable<String> errors) {
-        for (String error : errors) {
-            validationErrors.add(error);
-        }
-    }
-
-    public List<String> validationErrors() {
-        return validationErrors;
-    }
-
-    @Override
-    public String getMessage() {
-        StringBuilder sb = new StringBuilder();
-        sb.append("Validation Failed: ");
-        int index = 0;
-        for (String error : validationErrors) {
-            sb.append(++index).append(": ").append(error).append(";");
-        }
-        return sb.toString();
-    }
-
-    /**
-     * Helper method than can be used to add error messages to an existing {@link QueryValidationException}.
-     * When passing {@code null} as the initial exception, a new exception is created.
-     *
-     * @param queryId
-     * @param validationError the error message to add to an initial exception
-     * @param validationException an initial exception. Can be {@code null}, in which case a new exception is created.
-     * @return a {@link QueryValidationException} with added validation error message
-     */
-    public static QueryValidationException addValidationError(String queryId, String validationError, QueryValidationException validationException) {
-        if (validationException == null) {
-            validationException = new QueryValidationException("[" + queryId + "] " + validationError);
-        } else {
-            validationException.addValidationError(validationError);
-        }
-        return validationException;
-    }
-
-    /**
-     * Helper method than can be used to add error messages to an existing {@link QueryValidationException}.
-     * When passing {@code null} as the initial exception, a new exception is created.
-     * @param validationErrors the error messages to add to an initial exception
-     * @param validationException an initial exception. Can be {@code null}, in which case a new exception is created.
-     * @return a {@link QueryValidationException} with added validation error message
-     */
-    public static QueryValidationException addValidationErrors(List<String> validationErrors, QueryValidationException validationException) {
-        if (validationException == null) {
-            validationException = new QueryValidationException(validationErrors);
-        } else {
-            validationException.addValidationErrors(validationErrors);
-        }
-        return validationException;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryWrappingQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/QueryWrappingQueryBuilder.java
deleted file mode 100644
index d3be9da..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/QueryWrappingQueryBuilder.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-
-import java.io.IOException;
-
-/**
- * QueryBuilder implementation that  holds a lucene query, which can be returned by {@link QueryBuilder#toQuery(QueryShardContext)}.
- * Doesn't support conversion to {@link org.elasticsearch.common.xcontent.XContent} via {@link #doXContent(XContentBuilder, Params)}.
- */
-//norelease to be removed once all queries support separate fromXContent and toQuery methods. Make AbstractQueryBuilder#toQuery final as well then.
-public class QueryWrappingQueryBuilder extends AbstractQueryBuilder<QueryWrappingQueryBuilder> implements SpanQueryBuilder<QueryWrappingQueryBuilder>, MultiTermQueryBuilder<QueryWrappingQueryBuilder>{
-
-    private Query query;
-
-    public QueryWrappingQueryBuilder(Query query) {
-        this.query = query;
-        //hack to make sure that the boost from the wrapped query is used, otherwise it gets overwritten.
-        if (query != null) {
-            this.boost = query.getBoost();
-        }
-    }
-
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return query;
-    }
-
-    @Override
-    public String getWriteableName() {
-        // this should not be called since we overwrite BaseQueryBuilder#toQuery() in this class
-        throw new UnsupportedOperationException();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java
index 0db4152..da23698 100644
--- a/core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java
@@ -19,111 +19,187 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermRangeQuery;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.joda.DateMathParser;
-import org.elasticsearch.common.joda.Joda;
-import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.core.DateFieldMapper;
-import org.joda.time.DateTimeZone;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * A Query that matches documents within an range of terms.
  */
-public class RangeQueryBuilder extends AbstractQueryBuilder<RangeQueryBuilder> implements MultiTermQueryBuilder<RangeQueryBuilder> {
+public class RangeQueryBuilder extends MultiTermQueryBuilder implements BoostableQueryBuilder<RangeQueryBuilder> {
 
-    public static final boolean DEFAULT_INCLUDE_UPPER = true;
+    private final String name;
+    private Object from;
+    private Object to;
+    private String timeZone;
+    private boolean includeLower = true;
+    private boolean includeUpper = true;
+    private float boost = -1;
+    private String queryName;
+    private String format;
 
-    public static final boolean DEFAULT_INCLUDE_LOWER = true;
+    /**
+     * A Query that matches documents within an range of terms.
+     *
+     * @param name The field name
+     */
+    public RangeQueryBuilder(String name) {
+        this.name = name;
+    }
 
-    public static final String NAME = "range";
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder from(Object from) {
+        this.from = from;
+        return this;
+    }
 
-    private final String fieldName;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder from(String from) {
+        this.from = from;
+        return this;
+    }
 
-    private Object from;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder from(int from) {
+        this.from = from;
+        return this;
+    }
 
-    private Object to;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder from(long from) {
+        this.from = from;
+        return this;
+    }
 
-    private String timeZone;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder from(float from) {
+        this.from = from;
+        return this;
+    }
 
-    private boolean includeLower = DEFAULT_INCLUDE_LOWER;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder from(double from) {
+        this.from = from;
+        return this;
+    }
 
-    private boolean includeUpper = DEFAULT_INCLUDE_UPPER;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder gt(String from) {
+        this.from = from;
+        this.includeLower = false;
+        return this;
+    }
 
-    private String format;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder gt(Object from) {
+        this.from = from;
+        this.includeLower = false;
+        return this;
+    }
 
-    static final RangeQueryBuilder PROTOTYPE = new RangeQueryBuilder(null);
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder gt(int from) {
+        this.from = from;
+        this.includeLower = false;
+        return this;
+    }
 
     /**
-     * A Query that matches documents within an range of terms.
-     *
-     * @param fieldName The field name
+     * The from part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder(String fieldName) {
-        this.fieldName = fieldName;
+    public RangeQueryBuilder gt(long from) {
+        this.from = from;
+        this.includeLower = false;
+        return this;
     }
 
     /**
-     * Get the field name for this query.
+     * The from part of the range query. Null indicates unbounded.
      */
-    public String fieldName() {
-        return this.fieldName;
+    public RangeQueryBuilder gt(float from) {
+        this.from = from;
+        this.includeLower = false;
+        return this;
     }
 
     /**
      * The from part of the range query. Null indicates unbounded.
-     * In case lower bound is assigned to a string, we internally convert it to a {@link BytesRef} because
-     * in {@link RangeQueryParser} field are later parsed as {@link BytesRef} and we need internal representation
-     * of query to be equal regardless of whether it was created from XContent or via Java API.
      */
-    public RangeQueryBuilder from(Object from, boolean includeLower) {
-        this.from = convertToBytesRefIfString(from);
-        this.includeLower = includeLower;
+    public RangeQueryBuilder gt(double from) {
+        this.from = from;
+        this.includeLower = false;
         return this;
     }
 
     /**
      * The from part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder from(Object from) {
-        return from(from, this.includeLower);
+    public RangeQueryBuilder gte(String from) {
+        this.from = from;
+        this.includeLower = true;
+        return this;
     }
 
     /**
-     * Gets the lower range value for this query.
+     * The from part of the range query. Null indicates unbounded.
      */
-    public Object from() {
-        return convertToStringIfBytesRef(this.from);
+    public RangeQueryBuilder gte(Object from) {
+        this.from = from;
+        this.includeLower = true;
+        return this;
     }
 
     /**
      * The from part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder gt(Object from) {
-        return from(from, false);
+    public RangeQueryBuilder gte(int from) {
+        this.from = from;
+        this.includeLower = true;
+        return this;
     }
 
     /**
      * The from part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder gte(Object from) {
-        return from(from, true);
+    public RangeQueryBuilder gte(long from) {
+        this.from = from;
+        this.includeLower = true;
+        return this;
     }
 
     /**
-     * The to part of the range query. Null indicates unbounded.
+     * The from part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder to(Object to, boolean includeUpper) {
-        this.to = convertToBytesRefIfString(to);
-        this.includeUpper = includeUpper;
+    public RangeQueryBuilder gte(float from) {
+        this.from = from;
+        this.includeLower = true;
+        return this;
+    }
+
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder gte(double from) {
+        this.from = from;
+        this.includeLower = true;
         return this;
     }
 
@@ -131,214 +207,229 @@ public class RangeQueryBuilder extends AbstractQueryBuilder<RangeQueryBuilder> i
      * The to part of the range query. Null indicates unbounded.
      */
     public RangeQueryBuilder to(Object to) {
-        return to(to, this.includeUpper);
+        this.to = to;
+        return this;
     }
 
     /**
-     * Gets the upper range value for this query.
-     * In case upper bound is assigned to a string, we internally convert it to a {@link BytesRef} because
-     * in {@link RangeQueryParser} field are later parsed as {@link BytesRef} and we need internal representation
-     * of query to be equal regardless of whether it was created from XContent or via Java API.
+     * The to part of the range query. Null indicates unbounded.
      */
-    public Object to() {
-        return convertToStringIfBytesRef(this.to);
+    public RangeQueryBuilder to(String to) {
+        this.to = to;
+        return this;
     }
 
     /**
      * The to part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder lt(Object to) {
-        return to(to, false);
+    public RangeQueryBuilder to(int to) {
+        this.to = to;
+        return this;
     }
 
     /**
      * The to part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder lte(Object to) {
-        return to(to, true);
+    public RangeQueryBuilder to(long to) {
+        this.to = to;
+        return this;
     }
 
     /**
-     * Should the lower bound be included or not. Defaults to <tt>true</tt>.
+     * The to part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder includeLower(boolean includeLower) {
-        this.includeLower = includeLower;
+    public RangeQueryBuilder to(float to) {
+        this.to = to;
         return this;
     }
 
     /**
-     * Gets the includeLower flag for this query.
+     * The to part of the range query. Null indicates unbounded.
      */
-    public boolean includeLower() {
-        return this.includeLower;
+    public RangeQueryBuilder to(double to) {
+        this.to = to;
+        return this;
     }
 
     /**
-     * Should the upper bound be included or not. Defaults to <tt>true</tt>.
+     * The to part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder includeUpper(boolean includeUpper) {
-        this.includeUpper = includeUpper;
+    public RangeQueryBuilder lt(String to) {
+        this.to = to;
+        this.includeUpper = false;
         return this;
     }
 
     /**
-     * Gets the includeUpper flag for this query.
+     * The to part of the range query. Null indicates unbounded.
      */
-    public boolean includeUpper() {
-        return this.includeUpper;
+    public RangeQueryBuilder lt(Object to) {
+        this.to = to;
+        this.includeUpper = false;
+        return this;
     }
 
     /**
-     * In case of date field, we can adjust the from/to fields using a timezone
+     * The to part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder timeZone(String timezone) {
-        this.timeZone = timezone;
+    public RangeQueryBuilder lt(int to) {
+        this.to = to;
+        this.includeUpper = false;
         return this;
     }
 
     /**
-     * In case of date field, gets the from/to fields timezone adjustment
+     * The to part of the range query. Null indicates unbounded.
      */
-    public String timeZone() {
-        return this.timeZone;
+    public RangeQueryBuilder lt(long to) {
+        this.to = to;
+        this.includeUpper = false;
+        return this;
     }
 
     /**
-     * In case of format field, we can parse the from/to fields using this time format
+     * The to part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder format(String format) {
-        this.format = format;
+    public RangeQueryBuilder lt(float to) {
+        this.to = to;
+        this.includeUpper = false;
         return this;
     }
 
     /**
-     * Gets the format field to parse the from/to fields
+     * The to part of the range query. Null indicates unbounded.
      */
-    public String format() {
-        return this.format;
+    public RangeQueryBuilder lt(double to) {
+        this.to = to;
+        this.includeUpper = false;
+        return this;
     }
 
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.startObject(fieldName);
-        builder.field("from", convertToStringIfBytesRef(this.from));
-        builder.field("to", convertToStringIfBytesRef(this.to));
-        builder.field("include_lower", includeLower);
-        builder.field("include_upper", includeUpper);
-        if (timeZone != null) {
-            builder.field("time_zone", timeZone);
-        }
-        if (format != null) {
-            builder.field("format", format);
-        }
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
+    /**
+     * The to part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder lte(String to) {
+        this.to = to;
+        this.includeUpper = true;
+        return this;
     }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
+    /**
+     * The to part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder lte(Object to) {
+        this.to = to;
+        this.includeUpper = true;
+        return this;
     }
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query query = null;
-        MappedFieldType mapper = context.fieldMapper(this.fieldName);
-        if (mapper != null) {
-            if (mapper instanceof DateFieldMapper.DateFieldType) {
-                DateMathParser forcedDateParser = null;
-                if (this.format  != null) {
-                    forcedDateParser = new DateMathParser(Joda.forPattern(this.format));
-                }
-                DateTimeZone dateTimeZone = null;
-                if (this.timeZone != null) {
-                    dateTimeZone = DateTimeZone.forID(this.timeZone);
-                }
-                query = ((DateFieldMapper.DateFieldType) mapper).rangeQuery(from, to, includeLower, includeUpper, dateTimeZone, forcedDateParser);
-            } else  {
-                if (timeZone != null) {
-                    throw new QueryShardException(context, "[range] time_zone can not be applied to non date field ["
-                            + fieldName + "]");
-                }
-                //LUCENE 4 UPGRADE Mapper#rangeQuery should use bytesref as well?
-                query = mapper.rangeQuery(from, to, includeLower, includeUpper);
-            }
-        } else {
-            if (timeZone != null) {
-                throw new QueryShardException(context, "[range] time_zone can not be applied to non unmapped field ["
-                        + fieldName + "]");
-            }
-        }
+    /**
+     * The to part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder lte(int to) {
+        this.to = to;
+        this.includeUpper = true;
+        return this;
+    }
 
-        if (query == null) {
-            query = new TermRangeQuery(this.fieldName, BytesRefs.toBytesRef(from), BytesRefs.toBytesRef(to), includeLower, includeUpper);
-        }
-        return query;
+    /**
+     * The to part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder lte(long to) {
+        this.to = to;
+        this.includeUpper = true;
+        return this;
     }
 
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (this.fieldName == null || this.fieldName.isEmpty()) {
-            validationException = addValidationError("field name cannot be null or empty.", validationException);
-        }
-        if (this.timeZone != null) {
-            try {
-                DateTimeZone.forID(this.timeZone);
-            } catch (Exception e) {
-                validationException = addValidationError("error parsing timezone." + e.getMessage(),
-                        validationException);
-            }
-        }
-        if (this.format != null) {
-            try {
-                Joda.forPattern(this.format);
-            } catch (Exception e) {
-                validationException = addValidationError("error parsing format." + e.getMessage(),
-                        validationException);
-            }
-        }
-        return validationException;
+    /**
+     * The to part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder lte(float to) {
+        this.to = to;
+        this.includeUpper = true;
+        return this;
     }
 
-    @Override
-    protected RangeQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        RangeQueryBuilder rangeQueryBuilder = new RangeQueryBuilder(in.readString());
-        rangeQueryBuilder.from = in.readGenericValue();
-        rangeQueryBuilder.to = in.readGenericValue();
-        rangeQueryBuilder.includeLower = in.readBoolean();
-        rangeQueryBuilder.includeUpper = in.readBoolean();
-        rangeQueryBuilder.timeZone = in.readOptionalString();
-        rangeQueryBuilder.format = in.readOptionalString();
-        return rangeQueryBuilder;
+    /**
+     * The to part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder lte(double to) {
+        this.to = to;
+        this.includeUpper = true;
+        return this;
     }
 
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(this.fieldName);
-        out.writeGenericValue(this.from);
-        out.writeGenericValue(this.to);
-        out.writeBoolean(this.includeLower);
-        out.writeBoolean(this.includeUpper);
-        out.writeOptionalString(this.timeZone);
-        out.writeOptionalString(this.format);
+    /**
+     * Should the lower bound be included or not. Defaults to <tt>true</tt>.
+     */
+    public RangeQueryBuilder includeLower(boolean includeLower) {
+        this.includeLower = includeLower;
+        return this;
     }
 
+    /**
+     * Should the upper bound be included or not. Defaults to <tt>true</tt>.
+     */
+    public RangeQueryBuilder includeUpper(boolean includeUpper) {
+        this.includeUpper = includeUpper;
+        return this;
+    }
+
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
     @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldName, from, to, timeZone, includeLower, includeUpper, format);
+    public RangeQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public RangeQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
+    /**
+     * In case of date field, we can adjust the from/to fields using a timezone
+     */
+    public RangeQueryBuilder timeZone(String timezone) {
+        this.timeZone = timezone;
+        return this;
+    }
+
+    /**
+     * In case of date field, we can set the format to be used instead of the mapper format
+     */
+    public RangeQueryBuilder format(String format) {
+        this.format = format;
+        return this;
     }
 
     @Override
-    protected boolean doEquals(RangeQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-               Objects.equals(from, other.from) &&
-               Objects.equals(to, other.to) &&
-               Objects.equals(timeZone, other.timeZone) &&
-               Objects.equals(includeLower, other.includeLower) &&
-               Objects.equals(includeUpper, other.includeUpper) &&
-               Objects.equals(format, other.format);
+    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(RangeQueryParser.NAME);
+        builder.startObject(name);
+        builder.field("from", from);
+        builder.field("to", to);
+        if (timeZone != null) {
+            builder.field("time_zone", timeZone);
+        }
+        if (format != null) {
+            builder.field("format", format);
+        }
+        builder.field("include_lower", includeLower);
+        builder.field("include_upper", includeUpper);
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
+        builder.endObject();
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java
index 1df4ec7..6e4cd45 100644
--- a/core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java
@@ -19,17 +19,26 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermRangeQuery;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.joda.DateMathParser;
+import org.elasticsearch.common.joda.Joda;
+import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.core.DateFieldMapper;
+import org.joda.time.DateTimeZone;
 
 import java.io.IOException;
 
 /**
  *
  */
-public class RangeQueryParser extends BaseQueryParser {
+public class RangeQueryParser implements QueryParser {
 
+    public static final String NAME = "range";
     private static final ParseField FIELDDATA_FIELD = new ParseField("fielddata").withAllDeprecated("[no replacement]");
 
     @Inject
@@ -38,22 +47,22 @@ public class RangeQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{RangeQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public RangeQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldName = null;
         Object from = null;
         Object to = null;
-        boolean includeLower = RangeQueryBuilder.DEFAULT_INCLUDE_LOWER;
-        boolean includeUpper = RangeQueryBuilder.DEFAULT_INCLUDE_UPPER;
-        String timeZone = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        boolean includeLower = true;
+        boolean includeUpper = true;
+        DateTimeZone timeZone = null;
+        DateMathParser forcedDateParser = null;
+        float boost = 1.0f;
         String queryName = null;
-        String format = null;
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -91,11 +100,9 @@ public class RangeQueryParser extends BaseQueryParser {
                             to = parser.objectBytes();
                             includeUpper = true;
                         } else if ("time_zone".equals(currentFieldName) || "timeZone".equals(currentFieldName)) {
-                            timeZone = parser.text();
+                            timeZone = DateTimeZone.forID(parser.text());
                         } else if ("format".equals(currentFieldName)) {
-                            format = parser.text();
-                        } else if ("_name".equals(currentFieldName)) {
-                            queryName = parser.text();
+                            forcedDateParser = new DateMathParser(Joda.forPattern(parser.text()));
                         } else {
                             throw new QueryParsingException(parseContext, "[range] query does not support [" + currentFieldName + "]");
                         }
@@ -112,20 +119,27 @@ public class RangeQueryParser extends BaseQueryParser {
             }
         }
 
-        RangeQueryBuilder rangeQuery = new RangeQueryBuilder(fieldName);
-        rangeQuery.from(from);
-        rangeQuery.to(to);
-        rangeQuery.includeLower(includeLower);
-        rangeQuery.includeUpper(includeUpper);
-        rangeQuery.timeZone(timeZone);
-        rangeQuery.boost(boost);
-        rangeQuery.queryName(queryName);
-        rangeQuery.format(format);
-        return rangeQuery;
-    }
-
-    @Override
-    public RangeQueryBuilder getBuilderPrototype() {
-        return RangeQueryBuilder.PROTOTYPE;
+        Query query = null;
+        MappedFieldType mapper = parseContext.fieldMapper(fieldName);
+        if (mapper != null) {
+            if (mapper instanceof DateFieldMapper.DateFieldType) {
+                query = ((DateFieldMapper.DateFieldType) mapper).rangeQuery(from, to, includeLower, includeUpper, timeZone, forcedDateParser);
+            } else  {
+                if (timeZone != null) {
+                    throw new QueryParsingException(parseContext, "[range] time_zone can not be applied to non date field ["
+                            + fieldName + "]");
+                }
+                //LUCENE 4 UPGRADE Mapper#rangeQuery should use bytesref as well?
+                query = mapper.rangeQuery(from, to, includeLower, includeUpper);
+            }
+        }
+        if (query == null) {
+            query = new TermRangeQuery(fieldName, BytesRefs.toBytesRef(from), BytesRefs.toBytesRef(to), includeLower, includeUpper);
+        }
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java
index 6399089..ee143eb 100644
--- a/core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java
@@ -19,73 +19,48 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.RegexpQuery;
 import org.apache.lucene.util.automaton.Operations;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * A Query that does fuzzy matching for a specific value.
  */
-public class RegexpQueryBuilder extends AbstractQueryBuilder<RegexpQueryBuilder> implements MultiTermQueryBuilder<RegexpQueryBuilder> {
+public class RegexpQueryBuilder extends MultiTermQueryBuilder implements BoostableQueryBuilder<RegexpQueryBuilder> {
 
-    public static final String NAME = "regexp";
+    private final String name;
+    private final String regexp;
 
-    public static final int DEFAULT_FLAGS_VALUE = RegexpFlag.ALL.value();
-
-    public static final int DEFAULT_MAX_DETERMINIZED_STATES = Operations.DEFAULT_MAX_DETERMINIZED_STATES;
-
-    private final String fieldName;
-    
-    private final String value;
-    
-    private int flagsValue = DEFAULT_FLAGS_VALUE;
-    
-    private int maxDeterminizedStates = DEFAULT_MAX_DETERMINIZED_STATES;
-    
+    private int flags = RegexpQueryParser.DEFAULT_FLAGS_VALUE;
+    private float boost = -1;
     private String rewrite;
-    
-    static final RegexpQueryBuilder PROTOTYPE = new RegexpQueryBuilder(null, null);
+    private String queryName;
+    private int maxDeterminizedStates = Operations.DEFAULT_MAX_DETERMINIZED_STATES;
+    private boolean maxDetermizedStatesSet;
 
     /**
-     * Constructs a new regex query.
-     * 
-     * @param fieldName  The name of the field
-     * @param value The regular expression
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param regexp The regular expression
      */
-    public RegexpQueryBuilder(String fieldName, String value) {
-        this.fieldName = fieldName;
-        this.value = value;
-    }
-
-    /** Returns the field name used in this query. */
-    public String fieldName() {
-        return this.fieldName;
+    public RegexpQueryBuilder(String name, String regexp) {
+        this.name = name;
+        this.regexp = regexp;
     }
 
     /**
-     *  Returns the value used in this query.
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
      */
-    public String value() {
-        return this.value;
+    @Override
+    public RegexpQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     public RegexpQueryBuilder flags(RegexpFlag... flags) {
-        if (flags == null) {
-            this.flagsValue = DEFAULT_FLAGS_VALUE;
-            return this;
-        }
         int value = 0;
         if (flags.length == 0) {
             value = RegexpFlag.ALL.value;
@@ -94,120 +69,53 @@ public class RegexpQueryBuilder extends AbstractQueryBuilder<RegexpQueryBuilder>
                 value |= flag.value;
             }
         }
-        this.flagsValue = value;
-        return this;
-    }
-
-    public RegexpQueryBuilder flags(int flags) {
-        this.flagsValue = flags;
+        this.flags = value;
         return this;
     }
 
-    public int flags() {
-        return this.flagsValue;
-    }
-
     /**
      * Sets the regexp maxDeterminizedStates.
      */
     public RegexpQueryBuilder maxDeterminizedStates(int value) {
         this.maxDeterminizedStates = value;
+        this.maxDetermizedStatesSet = true;
         return this;
     }
-    
-    public int maxDeterminizedStates() {
-        return this.maxDeterminizedStates;
-    }
 
     public RegexpQueryBuilder rewrite(String rewrite) {
         this.rewrite = rewrite;
         return this;
     }
-    
-    public String rewrite() {
-        return this.rewrite;
+
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public RegexpQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.startObject(fieldName);
-        builder.field("value", this.value);
-        builder.field("flags_value", flagsValue);
-        builder.field("max_determinized_states", maxDeterminizedStates);
-        if (rewrite != null) {
-            builder.field("rewrite", rewrite);
+        builder.startObject(RegexpQueryParser.NAME);
+        builder.startObject(name);
+        builder.field("value", regexp);
+        if (flags != -1) {
+            builder.field("flags_value", flags);
         }
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    public Query doToQuery(QueryShardContext context) throws QueryShardException, IOException {
-        MultiTermQuery.RewriteMethod method = QueryParsers.parseRewriteMethod(context.parseFieldMatcher(), rewrite, null);
-
-        Query query = null;
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType != null) {
-            query = fieldType.regexpQuery(value, flagsValue, maxDeterminizedStates, method, context);
+        if (maxDetermizedStatesSet) {
+            builder.field("max_determinized_states", maxDeterminizedStates);
         }
-        if (query == null) {
-            RegexpQuery regexpQuery = new RegexpQuery(new Term(fieldName, BytesRefs.toBytesRef(value)), flagsValue, maxDeterminizedStates);
-            if (method != null) {
-                regexpQuery.setRewriteMethod(method);
-            }
-            query = regexpQuery;
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        return query;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (Strings.isEmpty(this.fieldName)) {
-            validationException = addValidationError("field name cannot be null or empty.", validationException);
+        if (rewrite != null) {
+            builder.field("rewrite", rewrite);
         }
-        if (this.value == null) {
-            validationException = addValidationError("query text cannot be null", validationException);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return validationException;
-    }
-
-    @Override
-    public RegexpQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        RegexpQueryBuilder regexpQueryBuilder = new RegexpQueryBuilder(in.readString(), in.readString());
-        regexpQueryBuilder.flagsValue = in.readVInt();
-        regexpQueryBuilder.maxDeterminizedStates = in.readVInt();
-        regexpQueryBuilder.rewrite = in.readOptionalString();
-        return regexpQueryBuilder;
-    }
-
-    @Override
-    public void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-        out.writeString(value);
-        out.writeVInt(flagsValue);
-        out.writeVInt(maxDeterminizedStates);
-        out.writeOptionalString(rewrite);
-    }
-
-    @Override
-    public int doHashCode() {
-        return Objects.hash(fieldName, value, flagsValue, maxDeterminizedStates, rewrite);
-    }
-
-    @Override
-    public boolean doEquals(RegexpQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-                Objects.equals(value, other.value) &&
-                Objects.equals(flagsValue, other.flagsValue) &&
-                Objects.equals(maxDeterminizedStates, other.maxDeterminizedStates) &&
-                Objects.equals(rewrite, other.rewrite);
+        builder.endObject();
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java
index c5b9f91..533e3cd 100644
--- a/core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java
@@ -19,12 +19,25 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.MultiTermQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.RegexpQuery;
+import org.apache.lucene.util.automaton.Operations;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
 
-public class RegexpQueryParser extends BaseQueryParser {
+/**
+ *
+ */
+public class RegexpQueryParser implements QueryParser {
+
+    public static final String NAME = "regexp";
 
     public static final int DEFAULT_FLAGS_VALUE = RegexpFlag.ALL.value();
 
@@ -34,20 +47,20 @@ public class RegexpQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{RegexpQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldName = parser.currentName();
-        String rewrite = null;
+        String rewriteMethod = null;
 
         String value = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        int flagsValue = RegexpQueryBuilder.DEFAULT_FLAGS_VALUE;
-        int maxDeterminizedStates = RegexpQueryBuilder.DEFAULT_MAX_DETERMINIZED_STATES;
+        float boost = 1.0f;
+        int flagsValue = DEFAULT_FLAGS_VALUE;
+        int maxDeterminizedStates = Operations.DEFAULT_MAX_DETERMINIZED_STATES;
         String queryName = null;
         String currentFieldName = null;
         XContentParser.Token token;
@@ -67,7 +80,7 @@ public class RegexpQueryParser extends BaseQueryParser {
                         } else if ("boost".equals(currentFieldName)) {
                             boost = parser.floatValue();
                         } else if ("rewrite".equals(currentFieldName)) {
-                            rewrite = parser.textOrNull();
+                            rewriteMethod = parser.textOrNull();
                         } else if ("flags".equals(currentFieldName)) {
                             String flags = parser.textOrNull();
                             flagsValue = RegexpFlag.resolveValue(flags);
@@ -95,16 +108,27 @@ public class RegexpQueryParser extends BaseQueryParser {
         if (value == null) {
             throw new QueryParsingException(parseContext, "No value specified for regexp query");
         }
-        return new RegexpQueryBuilder(fieldName, value)
-                .flags(flagsValue)
-                .maxDeterminizedStates(maxDeterminizedStates)
-                .rewrite(rewrite)
-                .boost(boost)
-                .queryName(queryName);
-    }
 
-    @Override
-    public RegexpQueryBuilder getBuilderPrototype() {
-        return RegexpQueryBuilder.PROTOTYPE;
+        MultiTermQuery.RewriteMethod method = QueryParsers.parseRewriteMethod(parseContext.parseFieldMatcher(), rewriteMethod, null);
+
+        Query query = null;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            query = fieldType.regexpQuery(value, flagsValue, maxDeterminizedStates, method, parseContext);
+        }
+        if (query == null) {
+            RegexpQuery regexpQuery = new RegexpQuery(new Term(fieldName, BytesRefs.toBytesRef(value)), flagsValue, maxDeterminizedStates);
+            if (method != null) {
+                regexpQuery.setRewriteMethod(method);
+            }
+            query = regexpQuery;
+        }
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
+
+
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java
index 519f065..a9a35ac 100644
--- a/core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java
@@ -19,155 +19,40 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.RandomAccessWeight;
-import org.apache.lucene.search.Weight;
-import org.apache.lucene.util.Bits;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.script.*;
+import org.elasticsearch.script.Script;
 import org.elasticsearch.script.Script.ScriptField;
-import org.elasticsearch.search.lookup.SearchLookup;
 
 import java.io.IOException;
-import java.util.Objects;
+import java.util.HashMap;
+import java.util.Map;
 
-public class ScriptQueryBuilder extends AbstractQueryBuilder<ScriptQueryBuilder> {
+public class ScriptQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "script";
+    private Script script;
 
-    static final ScriptQueryBuilder PROTOTYPE = new ScriptQueryBuilder(null);
-
-    private final Script script;
+    private String queryName;
 
     public ScriptQueryBuilder(Script script) {
         this.script = script;
     }
 
-    public Script script() {
-        return this.script;
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+    /**
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public ScriptQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params builderParams) throws IOException {
-        builder.startObject(NAME);
-        builder.field(ScriptField.SCRIPT.getPreferredName(), script);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return new ScriptQuery(script, context.scriptService(), context.lookup());
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (this.script == null) {
-            validationException = addValidationError("script cannot be null", validationException);
-        }
-        return validationException;
-    }
-
-    static class ScriptQuery extends Query {
-
-        private final Script script;
-
-        private final SearchScript searchScript;
-
-        public ScriptQuery(Script script, ScriptService scriptService, SearchLookup searchLookup) {
-            this.script = script;
-            this.searchScript = scriptService.search(searchLookup, script, ScriptContext.Standard.SEARCH);
-        }
-
-        @Override
-        public String toString(String field) {
-            StringBuilder buffer = new StringBuilder();
-            buffer.append("ScriptFilter(");
-            buffer.append(script);
-            buffer.append(")");
-            return buffer.toString();
-        }
 
-        @Override
-        public boolean equals(Object obj) {
-            if (this == obj)
-                return true;
-            if (!super.equals(obj))
-                return false;
-            ScriptQuery other = (ScriptQuery) obj;
-            return Objects.equals(script, other.script);
-        }
-
-        @Override
-        public int hashCode() {
-            final int prime = 31;
-            int result = super.hashCode();
-            result = prime * result + Objects.hashCode(script);
-            return result;
-        }
-
-        @Override
-        public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {
-            return new RandomAccessWeight(this) {
-                @Override
-                protected Bits getMatchingDocs(final LeafReaderContext context) throws IOException {
-                    final LeafSearchScript leafScript = searchScript.getLeafSearchScript(context);
-                    return new Bits() {
-
-                        @Override
-                        public boolean get(int doc) {
-                            leafScript.setDocument(doc);
-                            Object val = leafScript.run();
-                            if (val == null) {
-                                return false;
-                            }
-                            if (val instanceof Boolean) {
-                                return (Boolean) val;
-                            }
-                            if (val instanceof Number) {
-                                return ((Number) val).longValue() != 0;
-                            }
-                            throw new IllegalArgumentException("Can't handle type [" + val + "] in script filter");
-                        }
-
-                        @Override
-                        public int length() {
-                            return context.reader().maxDoc();
-                        }
-
-                    };
-                }
-            };
+        builder.startObject(ScriptQueryParser.NAME);
+        builder.field(ScriptField.SCRIPT.getPreferredName(), script);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-    }
-
-    @Override
-    protected ScriptQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new ScriptQueryBuilder(Script.readScript(in));
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        script.writeTo(out);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(script);
-    }
-
-    @Override
-    protected boolean doEquals(ScriptQueryBuilder other) {
-        return Objects.equals(script, other.script);
+        builder.endObject();
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java
index 734cd7f..62561f3 100644
--- a/core/src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java
@@ -19,12 +19,20 @@
 
 package org.elasticsearch.index.query;
 
+import com.google.common.base.Objects;
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.RandomAccessWeight;
+import org.apache.lucene.search.Weight;
+import org.apache.lucene.util.Bits;
+import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.script.Script;
+import org.elasticsearch.script.*;
 import org.elasticsearch.script.Script.ScriptField;
-import org.elasticsearch.script.ScriptParameterParser;
 import org.elasticsearch.script.ScriptParameterParser.ScriptParameterValue;
+import org.elasticsearch.search.lookup.SearchLookup;
 
 import java.io.IOException;
 import java.util.Map;
@@ -34,7 +42,9 @@ import static com.google.common.collect.Maps.newHashMap;
 /**
  *
  */
-public class ScriptQueryParser extends BaseQueryParser {
+public class ScriptQueryParser implements QueryParser {
+
+    public static final String NAME = "script";
 
     @Inject
     public ScriptQueryParser() {
@@ -42,23 +52,23 @@ public class ScriptQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{ScriptQueryBuilder.NAME};
+        return new String[] { NAME };
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
         ScriptParameterParser scriptParameterParser = new ScriptParameterParser();
-        
+
+        XContentParser.Token token;
+
         // also, when caching, since its isCacheable is false, will result in loading all bit set...
         Script script = null;
         Map<String, Object> params = null;
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         String queryName = null;
-
-        XContentParser.Token token;
         String currentFieldName = null;
+
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
@@ -75,8 +85,6 @@ public class ScriptQueryParser extends BaseQueryParser {
             } else if (token.isValue()) {
                 if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else if (!scriptParameterParser.token(currentFieldName, token, parser, parseContext.parseFieldMatcher())) {
                     throw new QueryParsingException(parseContext, "[script] query does not support [" + currentFieldName + "]");
                 }
@@ -99,13 +107,83 @@ public class ScriptQueryParser extends BaseQueryParser {
             throw new QueryParsingException(parseContext, "script must be provided with a [script] filter");
         }
 
-        return new ScriptQueryBuilder(script)
-                .boost(boost)
-                .queryName(queryName);
+        Query query = new ScriptQuery(script, parseContext.scriptService(), parseContext.lookup());
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 
-    @Override
-    public ScriptQueryBuilder getBuilderPrototype() {
-        return ScriptQueryBuilder.PROTOTYPE;
+    static class ScriptQuery extends Query {
+
+        private final Script script;
+
+        private final SearchScript searchScript;
+
+        public ScriptQuery(Script script, ScriptService scriptService, SearchLookup searchLookup) {
+            this.script = script;
+            this.searchScript = scriptService.search(searchLookup, script, ScriptContext.Standard.SEARCH);
+        }
+
+        @Override
+        public String toString(String field) {
+            StringBuilder buffer = new StringBuilder();
+            buffer.append("ScriptFilter(");
+            buffer.append(script);
+            buffer.append(")");
+            return buffer.toString();
+        }
+
+        @Override
+        public boolean equals(Object obj) {
+            if (this == obj)
+                return true;
+            if (!super.equals(obj))
+                return false;
+            ScriptQuery other = (ScriptQuery) obj;
+            return Objects.equal(script, other.script);
+        }
+
+        @Override
+        public int hashCode() {
+            final int prime = 31;
+            int result = super.hashCode();
+            result = prime * result + Objects.hashCode(script);
+            return result;
+        }
+
+        @Override
+        public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {
+            return new RandomAccessWeight(this) {
+                @Override
+                protected Bits getMatchingDocs(final LeafReaderContext context) throws IOException {
+                    final LeafSearchScript leafScript = searchScript.getLeafSearchScript(context);
+                    return new Bits() {
+
+                        @Override
+                        public boolean get(int doc) {
+                            leafScript.setDocument(doc);
+                            Object val = leafScript.run();
+                            if (val == null) {
+                                return false;
+                            }
+                            if (val instanceof Boolean) {
+                                return (Boolean) val;
+                            }
+                            if (val instanceof Number) {
+                                return ((Number) val).longValue() != 0;
+                            }
+                            throw new IllegalArgumentException("Can't handle type [" + val + "] in script filter");
+                        }
+
+                        @Override
+                        public int length() {
+                            return context.reader().maxDoc();
+                        }
+
+                    };
+                }
+            };
+        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryParser.java
index 06a3ccb..fc916f5 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryParser.java
@@ -29,7 +29,6 @@ import org.apache.lucene.util.BytesRef;
 import java.io.IOException;
 import java.util.Locale;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Wrapper class for Lucene's SimpleQueryParser that allows us to redefine
@@ -203,102 +202,51 @@ public class SimpleQueryParser extends org.apache.lucene.queryparser.simple.Simp
             return new PrefixQuery(new Term(field, termStr));
         }
     }
+
     /**
      * Class encapsulating the settings for the SimpleQueryString query, with
      * their default values
      */
-    static class Settings {
-        /** Locale to use for parsing. */
-        private Locale locale = SimpleQueryStringBuilder.DEFAULT_LOCALE;
-        /** Specifies whether parsed terms should be lowercased. */
-        private boolean lowercaseExpandedTerms = SimpleQueryStringBuilder.DEFAULT_LOWERCASE_EXPANDED_TERMS;
-        /** Specifies whether lenient query parsing should be used. */
-        private boolean lenient = SimpleQueryStringBuilder.DEFAULT_LENIENT;
-        /** Specifies whether wildcards should be analyzed. */
-        private boolean analyzeWildcard = SimpleQueryStringBuilder.DEFAULT_ANALYZE_WILDCARD;
+    public static class Settings {
+        private Locale locale = Locale.ROOT;
+        private boolean lowercaseExpandedTerms = true;
+        private boolean lenient = false;
+        private boolean analyzeWildcard = false;
 
-        /**
-         * Generates default {@link Settings} object (uses ROOT locale, does
-         * lowercase terms, no lenient parsing, no wildcard analysis).
-         * */
         public Settings() {
-        }
 
-        public Settings(Locale locale, Boolean lowercaseExpandedTerms, Boolean lenient, Boolean analyzeWildcard) {
-            this.locale = locale;
-            this.lowercaseExpandedTerms = lowercaseExpandedTerms;
-            this.lenient = lenient;
-            this.analyzeWildcard = analyzeWildcard;
         }
 
-        /** Specifies the locale to use for parsing, Locale.ROOT by default. */
         public void locale(Locale locale) {
-            this.locale = (locale != null) ? locale : SimpleQueryStringBuilder.DEFAULT_LOCALE;
+            this.locale = locale;
         }
 
-        /** Returns the locale to use for parsing. */
         public Locale locale() {
             return this.locale;
         }
 
-        /**
-         * Specifies whether to lowercase parse terms, defaults to true if
-         * unset.
-         */
         public void lowercaseExpandedTerms(boolean lowercaseExpandedTerms) {
             this.lowercaseExpandedTerms = lowercaseExpandedTerms;
         }
 
-        /** Returns whether to lowercase parse terms. */
         public boolean lowercaseExpandedTerms() {
             return this.lowercaseExpandedTerms;
         }
 
-        /** Specifies whether to use lenient parsing, defaults to false. */
         public void lenient(boolean lenient) {
             this.lenient = lenient;
         }
 
-        /** Returns whether to use lenient parsing. */
         public boolean lenient() {
             return this.lenient;
         }
 
-        /** Specifies whether to analyze wildcards. Defaults to false if unset. */
         public void analyzeWildcard(boolean analyzeWildcard) {
             this.analyzeWildcard = analyzeWildcard;
         }
 
-        /** Returns whether to analyze wildcards. */
         public boolean analyzeWildcard() {
             return analyzeWildcard;
         }
-
-        @Override
-        public int hashCode() {
-            // checking the return value of toLanguageTag() for locales only.
-            // For further reasoning see
-            // https://issues.apache.org/jira/browse/LUCENE-4021
-            return Objects.hash(locale.toLanguageTag(), lowercaseExpandedTerms, lenient, analyzeWildcard);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (this == obj) {
-                return true;
-            }
-            if (obj == null || getClass() != obj.getClass()) {
-                return false;
-            }
-            Settings other = (Settings) obj;
-
-            // checking the return value of toLanguageTag() for locales only.
-            // For further reasoning see
-            // https://issues.apache.org/jira/browse/LUCENE-4021
-            return (Objects.equals(locale.toLanguageTag(), other.locale.toLanguageTag())
-                    && Objects.equals(lowercaseExpandedTerms, other.lowercaseExpandedTerms) 
-                    && Objects.equals(lenient, other.lenient)
-                    && Objects.equals(analyzeWildcard, other.analyzeWildcard));
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java
index 087fedd..700ad41 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java
@@ -19,305 +19,150 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
-import org.elasticsearch.common.regex.Regex;
+import org.elasticsearch.common.xcontent.ToXContent.Params;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.SimpleQueryParser.Settings;
 
 import java.io.IOException;
 import java.util.HashMap;
 import java.util.Locale;
 import java.util.Map;
-import java.util.Objects;
-import java.util.TreeMap;
 
 /**
- * SimpleQuery is a query parser that acts similar to a query_string query, but
- * won't throw exceptions for any weird string syntax.
- *
- * For more detailed explanation of the query string syntax see also the <a
- * href=
- * "https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-simple-query-string-query.html"
- * > online documentation</a>.
+ * SimpleQuery is a query parser that acts similar to a query_string
+ * query, but won't throw exceptions for any weird string syntax.
  */
-public class SimpleQueryStringBuilder extends AbstractQueryBuilder<SimpleQueryStringBuilder> {
-    /** Default locale used for parsing.*/
-    public static final Locale DEFAULT_LOCALE = Locale.ROOT;
-    /** Default for lowercasing parsed terms.*/
-    public static final boolean DEFAULT_LOWERCASE_EXPANDED_TERMS = true;
-    /** Default for using lenient query parsing.*/
-    public static final boolean DEFAULT_LENIENT = false;
-    /** Default for wildcard analysis.*/
-    public static final boolean DEFAULT_ANALYZE_WILDCARD = false;
-    /** Default for default operator to use for linking boolean clauses.*/
-    public static final Operator DEFAULT_OPERATOR = Operator.OR;
-    /** Default for search flags to use. */
-    public static final int DEFAULT_FLAGS = SimpleQueryStringFlag.ALL.value;
-    /** Name for (de-)serialization. */
-    public static final String NAME = "simple_query_string";
-
-    /** Query text to parse. */
-    private final String queryText;
-    /**
-     * Fields to query against. If left empty will query default field,
-     * currently _ALL. Uses a TreeMap to hold the fields so boolean clauses are
-     * always sorted in same order for generated Lucene query for easier
-     * testing.
-     *
-     * Can be changed back to HashMap once https://issues.apache.org/jira/browse/LUCENE-6305 is fixed.
-     */
-    private final Map<String, Float> fieldsAndWeights = new TreeMap<>();
-    /** If specified, analyzer to use to parse the query text, defaults to registered default in toQuery. */
+public class SimpleQueryStringBuilder extends QueryBuilder implements BoostableQueryBuilder<SimpleQueryStringBuilder> {
+    private Map<String, Float> fields = new HashMap<>();
     private String analyzer;
-    /** Default operator to use for linking boolean clauses. Defaults to OR according to docs. */
-    private Operator defaultOperator = DEFAULT_OPERATOR;
-    /** If result is a boolean query, minimumShouldMatch parameter to apply. Ignored otherwise. */
+    private Operator operator;
+    private final String queryText;
+    private String queryName;
     private String minimumShouldMatch;
-    /** Any search flags to be used, ALL by default. */
-    private int flags = DEFAULT_FLAGS;
-
-    /** Further search settings needed by the ES specific query string parser only. */
-    private Settings settings = new Settings();
+    private int flags = -1;
+    private float boost = -1.0f;
+    private Boolean lowercaseExpandedTerms;
+    private Boolean lenient;
+    private Boolean analyzeWildcard;
+    private Locale locale;
 
-    static final SimpleQueryStringBuilder PROTOTYPE = new SimpleQueryStringBuilder(null);
+    /**
+     * Operators for the default_operator
+     */
+    public static enum Operator {
+        AND,
+        OR
+    }
 
-    /** Construct a new simple query with this query string. */
-    public SimpleQueryStringBuilder(String queryText) {
-        this.queryText = queryText;
+    /**
+     * Construct a new simple query with the given text
+     */
+    public SimpleQueryStringBuilder(String text) {
+        this.queryText = text;
     }
 
-    /** Returns the text to parse the query from. */
-    public String text() {
-        return this.queryText;
+    /** Set the boost of this query. */
+    @Override
+    public SimpleQueryStringBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+    
+    /** Returns the boost of this query. */
+    public float boost() {
+        return this.boost;
     }
 
-    /** Add a field to run the query against. */
+    /**
+     * Add a field to run the query against
+     */
     public SimpleQueryStringBuilder field(String field) {
-        if (Strings.isEmpty(field)) {
-            throw new IllegalArgumentException("supplied field is null or empty.");
-        }
-        this.fieldsAndWeights.put(field, AbstractQueryBuilder.DEFAULT_BOOST);
+        this.fields.put(field, null);
         return this;
     }
 
-    /** Add a field to run the query against with a specific boost. */
+    /**
+     * Add a field to run the query against with a specific boost
+     */
     public SimpleQueryStringBuilder field(String field, float boost) {
-        if (Strings.isEmpty(field)) {
-            throw new IllegalArgumentException("supplied field is null or empty.");
-        }
-        this.fieldsAndWeights.put(field, boost);
+        this.fields.put(field, boost);
         return this;
     }
 
-    /** Add several fields to run the query against with a specific boost. */
-    public SimpleQueryStringBuilder fields(Map<String, Float> fields) {
-        this.fieldsAndWeights.putAll(fields);
+    /**
+     * Specify a name for the query
+     */
+    public SimpleQueryStringBuilder queryName(String name) {
+        this.queryName = name;
         return this;
     }
 
-    /** Returns the fields including their respective boosts to run the query against. */
-    public Map<String, Float> fields() {
-        return this.fieldsAndWeights;
-    }
-
-    /** Specify an analyzer to use for the query. */
+    /**
+     * Specify an analyzer to use for the query
+     */
     public SimpleQueryStringBuilder analyzer(String analyzer) {
         this.analyzer = analyzer;
         return this;
     }
 
-    /** Returns the analyzer to use for the query. */
-    public String analyzer() {
-        return this.analyzer;
-    }
-
     /**
      * Specify the default operator for the query. Defaults to "OR" if no
-     * operator is specified.
+     * operator is specified
      */
     public SimpleQueryStringBuilder defaultOperator(Operator defaultOperator) {
-        this.defaultOperator = (defaultOperator != null) ? defaultOperator : DEFAULT_OPERATOR;
+        this.operator = defaultOperator;
         return this;
     }
 
-    /** Returns the default operator for the query. */
-    public Operator defaultOperator() {
-        return this.defaultOperator;
-    }
-
     /**
-     * Specify the enabled features of the SimpleQueryString. Defaults to ALL if
-     * none are specified.
+     * Specify the enabled features of the SimpleQueryString.
      */
     public SimpleQueryStringBuilder flags(SimpleQueryStringFlag... flags) {
-        if (flags != null && flags.length > 0) {
-            int value = 0;
+        int value = 0;
+        if (flags.length == 0) {
+            value = SimpleQueryStringFlag.ALL.value;
+        } else {
             for (SimpleQueryStringFlag flag : flags) {
                 value |= flag.value;
             }
-            this.flags = value;
-        } else {
-            this.flags = DEFAULT_FLAGS;
         }
-
+        this.flags = value;
         return this;
     }
 
-    /** For testing and serialisation only. */
-    SimpleQueryStringBuilder flags(int flags) {
-        this.flags = flags;
-        return this;
-    }
-
-    /** For testing only: Return the flags set for this query. */
-    int flags() {
-        return this.flags;
-    }
-
-    /**
-     * Specifies whether parsed terms for this query should be lower-cased.
-     * Defaults to true if not set.
-     */
     public SimpleQueryStringBuilder lowercaseExpandedTerms(boolean lowercaseExpandedTerms) {
-        this.settings.lowercaseExpandedTerms(lowercaseExpandedTerms);
+        this.lowercaseExpandedTerms = lowercaseExpandedTerms;
         return this;
     }
 
-    /** Returns whether parsed terms should be lower cased for this query. */
-    public boolean lowercaseExpandedTerms() {
-        return this.settings.lowercaseExpandedTerms();
-    }
-
-    /** Specifies the locale for parsing terms. Defaults to ROOT if none is set. */
     public SimpleQueryStringBuilder locale(Locale locale) {
-        this.settings.locale(locale);
+        this.locale = locale;
         return this;
     }
 
-    /** Returns the locale for parsing terms for this query. */
-    public Locale locale() {
-        return this.settings.locale();
-    }
-
-    /** Specifies whether query parsing should be lenient. Defaults to false. */
     public SimpleQueryStringBuilder lenient(boolean lenient) {
-        this.settings.lenient(lenient);
+        this.lenient = lenient;
         return this;
     }
 
-    /** Returns whether query parsing should be lenient. */
-    public boolean lenient() {
-        return this.settings.lenient();
-    }
-
-    /** Specifies whether wildcards should be analyzed. Defaults to false. */
     public SimpleQueryStringBuilder analyzeWildcard(boolean analyzeWildcard) {
-        this.settings.analyzeWildcard(analyzeWildcard);
+        this.analyzeWildcard = analyzeWildcard;
         return this;
     }
 
-    /** Returns whether wildcards should by analyzed. */
-    public boolean analyzeWildcard() {
-        return this.settings.analyzeWildcard();
-    }
-
-    /**
-     * Specifies the minimumShouldMatch to apply to the resulting query should
-     * that be a Boolean query.
-     */
     public SimpleQueryStringBuilder minimumShouldMatch(String minimumShouldMatch) {
         this.minimumShouldMatch = minimumShouldMatch;
         return this;
     }
 
-    /**
-     * Returns the minimumShouldMatch to apply to the resulting query should
-     * that be a Boolean query.
-     */
-    public String minimumShouldMatch() {
-        return minimumShouldMatch;
-    }
-
-    /**
-     * {@inheritDoc}
-     *
-     * Checks that mandatory queryText is neither null nor empty.
-     * */
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        // Query text is required
-        if (queryText == null) {
-            validationException = addValidationError("query text missing", validationException);
-        }
-
-        return validationException;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        // Use the default field (_all) if no fields specified
-        if (fieldsAndWeights.isEmpty()) {
-            String field = context.defaultField();
-            fieldsAndWeights.put(field, 1.0F);
-        }
-
-        // field names in builder can have wildcards etc, need to resolve them here
-        Map<String, Float> resolvedFieldsAndWeights = new TreeMap<>();
-        for (String fField : fieldsAndWeights.keySet()) {
-            if (Regex.isSimpleMatchPattern(fField)) {
-                for (String fieldName : context.mapperService().simpleMatchToIndexNames(fField)) {
-                    resolvedFieldsAndWeights.put(fieldName, fieldsAndWeights.get(fField));
-                }
-            } else {
-                MappedFieldType fieldType = context.fieldMapper(fField);
-                if (fieldType != null) {
-                    resolvedFieldsAndWeights.put(fieldType.names().indexName(), fieldsAndWeights.get(fField));
-                } else {
-                    resolvedFieldsAndWeights.put(fField, fieldsAndWeights.get(fField));
-                }
-            }
-        }
-
-        // Use standard analyzer by default if none specified
-        Analyzer luceneAnalyzer;
-        if (analyzer == null) {
-            luceneAnalyzer = context.mapperService().searchAnalyzer();
-        } else {
-            luceneAnalyzer = context.analysisService().analyzer(analyzer);
-            if (luceneAnalyzer == null) {
-                throw new QueryShardException(context, "[" + SimpleQueryStringBuilder.NAME + "] analyzer [" + analyzer
-                        + "] not found");
-            }
-
-        }
-
-        SimpleQueryParser sqp = new SimpleQueryParser(luceneAnalyzer, resolvedFieldsAndWeights, flags, settings);
-        sqp.setDefaultOperator(defaultOperator.toBooleanClauseOccur());
-
-        Query query = sqp.parse(queryText);
-        if (minimumShouldMatch != null && query instanceof BooleanQuery) {
-            Queries.applyMinimumShouldMatch((BooleanQuery) query, minimumShouldMatch);
-        }
-        return query;
-    }
-
     @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(SimpleQueryStringParser.NAME);
 
         builder.field("query", queryText);
 
-        if (fieldsAndWeights.size() > 0) {
+        if (fields.size() > 0) {
             builder.startArray("fields");
-            for (Map.Entry<String, Float> entry : fieldsAndWeights.entrySet()) {
+            for (Map.Entry<String, Float> entry : fields.entrySet()) {
                 String field = entry.getKey();
                 Float boost = entry.getValue();
                 if (boost != null) {
@@ -329,82 +174,47 @@ public class SimpleQueryStringBuilder extends AbstractQueryBuilder<SimpleQuerySt
             builder.endArray();
         }
 
+        if (flags != -1) {
+            builder.field("flags", flags);
+        }
+
         if (analyzer != null) {
             builder.field("analyzer", analyzer);
         }
 
-        builder.field("flags", flags);
-        builder.field("default_operator", defaultOperator.name().toLowerCase(Locale.ROOT));
-        builder.field("lowercase_expanded_terms", settings.lowercaseExpandedTerms());
-        builder.field("lenient", settings.lenient());
-        builder.field("analyze_wildcard", settings.analyzeWildcard());
-        builder.field("locale", (settings.locale().toLanguageTag()));
+        if (operator != null) {
+            builder.field("default_operator", operator.name().toLowerCase(Locale.ROOT));
+        }
 
-        if (minimumShouldMatch != null) {
-            builder.field("minimum_should_match", minimumShouldMatch);
+        if (lowercaseExpandedTerms != null) {
+            builder.field("lowercase_expanded_terms", lowercaseExpandedTerms);
         }
 
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
+        if (lenient != null) {
+            builder.field("lenient", lenient);
+        }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
+        if (analyzeWildcard != null) {
+            builder.field("analyze_wildcard", analyzeWildcard);
+        }
 
-    @Override
-    protected SimpleQueryStringBuilder doReadFrom(StreamInput in) throws IOException {
-        SimpleQueryStringBuilder result = new SimpleQueryStringBuilder(in.readString());
-        int size = in.readInt();
-        Map<String, Float> fields = new HashMap<>();
-        for (int i = 0; i < size; i++) {
-            String field = in.readString();
-            Float weight = in.readFloat();
-            fields.put(field, weight);
+        if (locale != null) {
+            builder.field("locale", locale.toString());
         }
-        result.fieldsAndWeights.putAll(fields);
-        result.flags = in.readInt();
-        result.analyzer = in.readOptionalString();
-        result.defaultOperator = Operator.readOperatorFrom(in);
-        result.settings.lowercaseExpandedTerms(in.readBoolean());
-        result.settings.lenient(in.readBoolean());
-        result.settings.analyzeWildcard(in.readBoolean());
-        String localeStr = in.readString();
-        result.settings.locale(Locale.forLanguageTag(localeStr));
-        result.minimumShouldMatch = in.readOptionalString();
-        return result;
-    }
 
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(queryText);
-        out.writeInt(fieldsAndWeights.size());
-        for (Map.Entry<String, Float> entry : fieldsAndWeights.entrySet()) {
-            out.writeString(entry.getKey());
-            out.writeFloat(entry.getValue());
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        out.writeInt(flags);
-        out.writeOptionalString(analyzer);
-        defaultOperator.writeTo(out);
-        out.writeBoolean(settings.lowercaseExpandedTerms());
-        out.writeBoolean(settings.lenient());
-        out.writeBoolean(settings.analyzeWildcard());
-        out.writeString(settings.locale().toLanguageTag());
-        out.writeOptionalString(minimumShouldMatch);
-    }
 
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldsAndWeights, analyzer, defaultOperator, queryText, minimumShouldMatch, settings, flags);
-    }
+        if (minimumShouldMatch != null) {
+            builder.field("minimum_should_match", minimumShouldMatch);
+        }
+        
+        if (boost != -1.0f) {
+            builder.field("boost", boost);
+        }
 
-    @Override
-    protected boolean doEquals(SimpleQueryStringBuilder other) {
-        return Objects.equals(fieldsAndWeights, other.fieldsAndWeights) && Objects.equals(analyzer, other.analyzer)
-                && Objects.equals(defaultOperator, other.defaultOperator) && Objects.equals(queryText, other.queryText)
-                && Objects.equals(minimumShouldMatch, other.minimumShouldMatch)
-                && Objects.equals(settings, other.settings) && (flags == other.flags);
+        builder.endObject();
     }
-}
 
+}
diff --git a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringFlag.java b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringFlag.java
index 68d19db..ce0ce88 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringFlag.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringFlag.java
@@ -71,7 +71,7 @@ public enum SimpleQueryStringFlag {
                         magic |= flag.value();
                 }
             } catch (IllegalArgumentException iae) {
-                throw new IllegalArgumentException("Unknown " + SimpleQueryStringBuilder.NAME + " flag [" + s + "]");
+                throw new IllegalArgumentException("Unknown " + SimpleQueryStringParser.NAME + " flag [" + s + "]");
             }
         }
         return magic;
diff --git a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java
index 58bf7bc..c696a75 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java
@@ -19,9 +19,22 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
+import org.elasticsearch.common.regex.Regex;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.util.LocaleUtils;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.FieldMapper;
+import org.elasticsearch.index.mapper.MappedFieldType;
+
 import java.io.IOException;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.Locale;
 import java.util.Map;
@@ -42,7 +55,7 @@ import java.util.Map;
  * <li>'{@code ~}N' at the end of phrases specifies near/slop query: <tt>"term1 term2"~5</tt>
  * </ul>
  * <p/>
- * See: {@link SimpleQueryParser} for more information.
+ * See: {@link XSimpleQueryParser} for more information.
  * <p/>
  * This query supports these options:
  * <p/>
@@ -57,31 +70,35 @@ import java.util.Map;
  * {@code fields} - fields to search, defaults to _all if not set, allows
  * boosting a field with ^n
  */
-public class SimpleQueryStringParser extends BaseQueryParser {
+public class SimpleQueryStringParser implements QueryParser {
+
+    public static final String NAME = "simple_query_string";
+
+    @Inject
+    public SimpleQueryStringParser(Settings settings) {
+
+    }
 
     @Override
     public String[] names() {
-        return new String[]{SimpleQueryStringBuilder.NAME, Strings.toCamelCase(SimpleQueryStringBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String currentFieldName = null;
         String queryBody = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f; 
         String queryName = null;
         String field = null;
         String minimumShouldMatch = null;
-        Map<String, Float> fieldsAndWeights = new HashMap<>();
-        Operator defaultOperator = null;
-        String analyzerName = null;
-        int flags = SimpleQueryStringFlag.ALL.value();
-        boolean lenient = SimpleQueryStringBuilder.DEFAULT_LENIENT;
-        boolean lowercaseExpandedTerms = SimpleQueryStringBuilder.DEFAULT_LOWERCASE_EXPANDED_TERMS;
-        boolean analyzeWildcard = SimpleQueryStringBuilder.DEFAULT_ANALYZE_WILDCARD;
-        Locale locale = null;
+        Map<String, Float> fieldsAndWeights = null;
+        BooleanClause.Occur defaultOperator = null;
+        Analyzer analyzer = null;
+        int flags = -1;
+        SimpleQueryParser.Settings sqsSettings = new SimpleQueryParser.Settings();
 
         XContentParser.Token token;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -105,11 +122,27 @@ public class SimpleQueryStringParser extends BaseQueryParser {
                         if (fField == null) {
                             fField = parser.text();
                         }
-                        fieldsAndWeights.put(fField, fBoost);
+
+                        if (fieldsAndWeights == null) {
+                            fieldsAndWeights = new HashMap<>();
+                        }
+
+                        if (Regex.isSimpleMatchPattern(fField)) {
+                            for (String fieldName : parseContext.mapperService().simpleMatchToIndexNames(fField)) {
+                                fieldsAndWeights.put(fieldName, fBoost);
+                            }
+                        } else {
+                            MappedFieldType fieldType = parseContext.fieldMapper(fField);
+                            if (fieldType != null) {
+                                fieldsAndWeights.put(fieldType.names().indexName(), fBoost);
+                            } else {
+                                fieldsAndWeights.put(fField, fBoost);
+                            }
+                        }
                     }
                 } else {
                     throw new QueryParsingException(parseContext,
- "[" + SimpleQueryStringBuilder.NAME + "] query does not support [" + currentFieldName
+ "[" + NAME + "] query does not support [" + currentFieldName
  + "]");
                 }
             } else if (token.isValue()) {
@@ -118,11 +151,21 @@ public class SimpleQueryStringParser extends BaseQueryParser {
                 } else if ("boost".equals(currentFieldName)) {
                     boost = parser.floatValue();
                 } else if ("analyzer".equals(currentFieldName)) {
-                    analyzerName = parser.text();
+                    analyzer = parseContext.analysisService().analyzer(parser.text());
+                    if (analyzer == null) {
+                        throw new QueryParsingException(parseContext, "[" + NAME + "] analyzer [" + parser.text() + "] not found");
+                    }
                 } else if ("field".equals(currentFieldName)) {
                     field = parser.text();
                 } else if ("default_operator".equals(currentFieldName) || "defaultOperator".equals(currentFieldName)) {
-                    defaultOperator = Operator.fromString(parser.text());
+                    String op = parser.text();
+                    if ("or".equalsIgnoreCase(op)) {
+                        defaultOperator = BooleanClause.Occur.SHOULD;
+                    } else if ("and".equalsIgnoreCase(op)) {
+                        defaultOperator = BooleanClause.Occur.MUST;
+                    } else {
+                        throw new QueryParsingException(parseContext, "[" + NAME + "] default operator [" + op + "] is not allowed");
+                    }
                 } else if ("flags".equals(currentFieldName)) {
                     if (parser.currentToken() != XContentParser.Token.VALUE_NUMBER) {
                         // Possible options are:
@@ -136,43 +179,66 @@ public class SimpleQueryStringParser extends BaseQueryParser {
                     }
                 } else if ("locale".equals(currentFieldName)) {
                     String localeStr = parser.text();
-                    locale = Locale.forLanguageTag(localeStr);
+                    Locale locale = LocaleUtils.parse(localeStr);
+                    sqsSettings.locale(locale);
                 } else if ("lowercase_expanded_terms".equals(currentFieldName)) {
-                    lowercaseExpandedTerms = parser.booleanValue();
+                    sqsSettings.lowercaseExpandedTerms(parser.booleanValue());
                 } else if ("lenient".equals(currentFieldName)) {
-                    lenient = parser.booleanValue();
+                    sqsSettings.lenient(parser.booleanValue());
                 } else if ("analyze_wildcard".equals(currentFieldName)) {
-                    analyzeWildcard = parser.booleanValue();
+                    sqsSettings.analyzeWildcard(parser.booleanValue());
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
                 } else if ("minimum_should_match".equals(currentFieldName)) {
                     minimumShouldMatch = parser.textOrNull();
                 } else {
-                    throw new QueryParsingException(parseContext, "[" + SimpleQueryStringBuilder.NAME + "] unsupported field [" + parser.currentName() + "]");
+                    throw new QueryParsingException(parseContext, "[" + NAME + "] unsupported field [" + parser.currentName() + "]");
                 }
             }
         }
 
         // Query text is required
         if (queryBody == null) {
-            throw new QueryParsingException(parseContext, "[" + SimpleQueryStringBuilder.NAME + "] query text missing");
+            throw new QueryParsingException(parseContext, "[" + NAME + "] query text missing");
         }
-
+        
         // Support specifying only a field instead of a map
         if (field == null) {
             field = currentFieldName;
         }
 
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder(queryBody);
-        qb.boost(boost).fields(fieldsAndWeights).analyzer(analyzerName).queryName(queryName).minimumShouldMatch(minimumShouldMatch);
-        qb.flags(flags).defaultOperator(defaultOperator).locale(locale).lowercaseExpandedTerms(lowercaseExpandedTerms);
-        qb.lenient(lenient).analyzeWildcard(analyzeWildcard).boost(boost);
+        // Use the default field (_all) if no fields specified
+        if (fieldsAndWeights == null) {
+            field = parseContext.defaultField();
+        }
 
-        return qb;
-    }
+        // Use standard analyzer by default
+        if (analyzer == null) {
+            analyzer = parseContext.mapperService().searchAnalyzer();
+        }
 
-    @Override
-    public SimpleQueryStringBuilder getBuilderPrototype() {
-        return SimpleQueryStringBuilder.PROTOTYPE;
+        if (fieldsAndWeights == null) {
+            fieldsAndWeights = Collections.singletonMap(field, 1.0F);
+        }
+        SimpleQueryParser sqp = new SimpleQueryParser(analyzer, fieldsAndWeights, flags, sqsSettings);
+
+        if (defaultOperator != null) {
+            sqp.setDefaultOperator(defaultOperator);
+        }
+
+        Query query = sqp.parse(queryBody);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+
+        if (minimumShouldMatch != null && query instanceof BooleanQuery) {
+            Queries.applyMinimumShouldMatch((BooleanQuery) query, minimumShouldMatch);
+        }
+
+        if (query != null) {
+            query.setBoost(boost);
+        }
+
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java
index dc683a8..0b7a3cd 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java
@@ -19,111 +19,74 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanContainingQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * Builder for {@link org.apache.lucene.search.spans.SpanContainingQuery}.
  */
-public class SpanContainingQueryBuilder extends AbstractQueryBuilder<SpanContainingQueryBuilder> implements SpanQueryBuilder<SpanContainingQueryBuilder> {
+public class SpanContainingQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanContainingQueryBuilder> {
 
-    public static final String NAME = "span_containing";
-    private final SpanQueryBuilder big;
-    private final SpanQueryBuilder little;
-    static final SpanContainingQueryBuilder PROTOTYPE = new SpanContainingQueryBuilder(null, null);
+    private SpanQueryBuilder big;
+    private SpanQueryBuilder little;
+    private float boost = -1;
+    private String queryName;
 
-    /**
-     * @param big the big clause, it must enclose {@code little} for a match.
-     * @param little the little clause, it must be contained within {@code big} for a match.
+    /** 
+     * Sets the little clause, it must be contained within {@code big} for a match.
      */
-    public SpanContainingQueryBuilder(SpanQueryBuilder big, SpanQueryBuilder little) {
-        this.little = little;
-        this.big = big;
+    public SpanContainingQueryBuilder little(SpanQueryBuilder clause) {
+        this.little = clause;
+        return this;
     }
 
-    /**
-     * @return the big clause, it must enclose {@code little} for a match.
+    /** 
+     * Sets the big clause, it must enclose {@code little} for a match.
      */
-    public SpanQueryBuilder big() {
-        return this.big;
-    }
-
-    /**
-     * @return the little clause, it must be contained within {@code big} for a match.
-     */
-    public SpanQueryBuilder little() {
-        return this.little;
+    public SpanContainingQueryBuilder big(SpanQueryBuilder clause) {
+        this.big = clause;
+        return this;
     }
 
     @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("big");
-        big.toXContent(builder, params);
-        builder.field("little");
-        little.toXContent(builder, params);
-        printBoostAndQueryName(builder);
-        builder.endObject();
+    public SpanContainingQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query innerBig = big.toQuery(context);
-        assert innerBig instanceof SpanQuery;
-        Query innerLittle = little.toQuery(context);
-        assert innerLittle instanceof SpanQuery;
-        return new SpanContainingQuery((SpanQuery) innerBig, (SpanQuery) innerLittle);
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public SpanContainingQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
+    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
         if (big == null) {
-            validationException = addValidationError("inner clause [big] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(big, validationException);
+            throw new IllegalArgumentException("Must specify big clause when building a span_containing query");
         }
         if (little == null) {
-            validationException = addValidationError("inner clause [little] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(little, validationException);
+            throw new IllegalArgumentException("Must specify little clause when building a span_containing query");
         }
-        return validationException;
-    }
+        builder.startObject(SpanContainingQueryParser.NAME);
 
-    @Override
-    protected SpanContainingQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        SpanQueryBuilder big = (SpanQueryBuilder)in.readQuery();
-        SpanQueryBuilder little = (SpanQueryBuilder)in.readQuery();
-        return new SpanContainingQueryBuilder(big, little);
-    }
+        builder.field("big");
+        big.toXContent(builder, params);
 
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(big);
-        out.writeQuery(little);
-    }
+        builder.field("little");
+        little.toXContent(builder, params);
 
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(big, little);
-    }
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
 
-    @Override
-    protected boolean doEquals(SpanContainingQueryBuilder other) {
-        return Objects.equals(big, other.big) &&
-               Objects.equals(little, other.little);
-    }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java
index c405251..63e312b 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java
@@ -19,7 +19,9 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
 import org.apache.lucene.search.spans.SpanContainingQuery;
+import org.apache.lucene.search.spans.SpanQuery;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
@@ -29,7 +31,9 @@ import java.io.IOException;
 /**
  * Parser for {@link SpanContainingQuery}
  */
-public class SpanContainingQueryParser extends BaseQueryParser {
+public class SpanContainingQueryParser implements QueryParser {
+
+    public static final String NAME = "span_containing";
 
     @Inject
     public SpanContainingQueryParser() {
@@ -37,16 +41,17 @@ public class SpanContainingQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{SpanContainingQueryBuilder.NAME, Strings.toCamelCase(SpanContainingQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+
+        float boost = 1.0f;
         String queryName = null;
-        SpanQueryBuilder<?> big = null;
-        SpanQueryBuilder<?> little = null;
+        SpanQuery big = null;
+        SpanQuery little = null;
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -55,17 +60,17 @@ public class SpanContainingQueryParser extends BaseQueryParser {
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("big".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (!(query instanceof SpanQueryBuilder<?>)) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (!(query instanceof SpanQuery)) {
                         throw new QueryParsingException(parseContext, "span_containing [big] must be of type span query");
                     }
-                    big = (SpanQueryBuilder<?>) query;
+                    big = (SpanQuery) query;
                 } else if ("little".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (!(query instanceof SpanQueryBuilder<?>)) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (!(query instanceof SpanQuery)) {
                         throw new QueryParsingException(parseContext, "span_containing [little] must be of type span query");
                     }
-                    little = (SpanQueryBuilder<?>) query;
+                    little = (SpanQuery) query;
                 } else {
                     throw new QueryParsingException(parseContext, "[span_containing] query does not support [" + currentFieldName + "]");
                 }
@@ -76,15 +81,20 @@ public class SpanContainingQueryParser extends BaseQueryParser {
             } else {
                 throw new QueryParsingException(parseContext, "[span_containing] query does not support [" + currentFieldName + "]");
             }
+        }        
+        
+        if (big == null) {
+            throw new QueryParsingException(parseContext, "span_containing must include [big]");
+        }
+        if (little == null) {
+            throw new QueryParsingException(parseContext, "span_containing must include [little]");
         }
 
-        SpanContainingQueryBuilder query = new SpanContainingQueryBuilder(big, little);
-        query.boost(boost).queryName(queryName);
+        Query query = new SpanContainingQuery(big, little);
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
         return query;
     }
-
-    @Override
-    public SpanContainingQueryBuilder getBuilderPrototype() {
-        return SpanContainingQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java
index 59a906d..f967a1c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java
@@ -19,109 +19,51 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanFirstQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
-public class SpanFirstQueryBuilder extends AbstractQueryBuilder<SpanFirstQueryBuilder> implements SpanQueryBuilder<SpanFirstQueryBuilder>{
-
-    public static final String NAME = "span_first";
+public class SpanFirstQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanFirstQueryBuilder> {
 
     private final SpanQueryBuilder matchBuilder;
 
     private final int end;
 
-    static final SpanFirstQueryBuilder SPAN_FIRST_QUERY_BUILDER = new SpanFirstQueryBuilder(null, -1);
+    private float boost = -1;
+
+    private String queryName;
 
-    /**
-     * Query that matches spans queries defined in <code>matchBuilder</code>
-     * whose end position is less than or equal to <code>end</code>.
-     * @param matchBuilder inner {@link SpanQueryBuilder}
-     * @param end maximum end position of the match, needs to be positive
-     * @throws IllegalArgumentException for negative <code>end</code> positions
-     */
     public SpanFirstQueryBuilder(SpanQueryBuilder matchBuilder, int end) {
         this.matchBuilder = matchBuilder;
         this.end = end;
     }
 
-    /**
-     * @return the inner {@link SpanQueryBuilder} defined in this query
-     */
-    public SpanQueryBuilder matchBuilder() {
-        return this.matchBuilder;
+    @Override
+    public SpanFirstQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
-     * @return maximum end position of the matching inner span query
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public int end() {
-        return this.end;
+    public SpanFirstQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(SpanFirstQueryParser.NAME);
         builder.field("match");
         matchBuilder.toXContent(builder, params);
         builder.field("end", end);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query innerSpanQuery = matchBuilder.toQuery(context);
-        assert innerSpanQuery instanceof SpanQuery;
-        return new SpanFirstQuery((SpanQuery) innerSpanQuery, end);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (matchBuilder == null) {
-            validationException = addValidationError("inner clause [match] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(matchBuilder, validationException);
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        if (end < 0) {
-            validationException = addValidationError("parameter [end] needs to be positive.", validationException);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return validationException;
-    }
-
-    @Override
-    protected SpanFirstQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        SpanQueryBuilder matchBuilder = (SpanQueryBuilder)in.readQuery();
-        int end = in.readInt();
-        return new SpanFirstQueryBuilder(matchBuilder, end);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(matchBuilder);
-        out.writeInt(end);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(matchBuilder, end);
-    }
-
-    @Override
-    protected boolean doEquals(SpanFirstQueryBuilder other) {
-        return Objects.equals(matchBuilder, other.matchBuilder) &&
-               Objects.equals(end, other.end);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java
index 262ad15..5a302eb 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java
@@ -19,6 +19,9 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanFirstQuery;
+import org.apache.lucene.search.spans.SpanQuery;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
@@ -28,7 +31,9 @@ import java.io.IOException;
 /**
  *
  */
-public class SpanFirstQueryParser extends BaseQueryParser {
+public class SpanFirstQueryParser implements QueryParser {
+
+    public static final String NAME = "span_first";
 
     @Inject
     public SpanFirstQueryParser() {
@@ -36,17 +41,17 @@ public class SpanFirstQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{SpanFirstQueryBuilder.NAME, Strings.toCamelCase(SpanFirstQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
 
-        SpanQueryBuilder match = null;
-        Integer end = null;
+        SpanQuery match = null;
+        int end = -1;
         String queryName = null;
 
         String currentFieldName = null;
@@ -56,11 +61,11 @@ public class SpanFirstQueryParser extends BaseQueryParser {
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("match".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (!(query instanceof SpanQueryBuilder)) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (!(query instanceof SpanQuery)) {
                         throw new QueryParsingException(parseContext, "spanFirst [match] must be of type span query");
                     }
-                    match = (SpanQueryBuilder) query;
+                    match = (SpanQuery) query;
                 } else {
                     throw new QueryParsingException(parseContext, "[span_first] query does not support [" + currentFieldName + "]");
                 }
@@ -79,16 +84,15 @@ public class SpanFirstQueryParser extends BaseQueryParser {
         if (match == null) {
             throw new QueryParsingException(parseContext, "spanFirst must have [match] span query clause");
         }
-        if (end == null) {
+        if (end == -1) {
             throw new QueryParsingException(parseContext, "spanFirst must have [end] set for it");
         }
-        SpanFirstQueryBuilder queryBuilder = new SpanFirstQueryBuilder(match, end);
-        queryBuilder.boost(boost).queryName(queryName);
-        return queryBuilder;
-    }
 
-    @Override
-    public SpanFirstQueryBuilder getBuilderPrototype() {
-        return SpanFirstQueryBuilder.SPAN_FIRST_QUERY_BUILDER;
+        SpanFirstQuery query = new SpanFirstQuery(match, end);
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java
index 7e27b2f..11b9897 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java
@@ -18,88 +18,25 @@
  */
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanMultiTermQueryWrapper;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
-/**
- * Query that allows wraping a {@link MultiTermQueryBuilder} (one of wildcard, fuzzy, prefix, term, range or regexp query)
- * as a {@link SpanQueryBuilder} so it can be nested.
- */
-public class SpanMultiTermQueryBuilder extends AbstractQueryBuilder<SpanMultiTermQueryBuilder> implements SpanQueryBuilder<SpanMultiTermQueryBuilder> {
+public class SpanMultiTermQueryBuilder extends SpanQueryBuilder {
 
-    public static final String NAME = "span_multi";
-    private final MultiTermQueryBuilder multiTermQueryBuilder;
-    static final SpanMultiTermQueryBuilder PROTOTYPE = new SpanMultiTermQueryBuilder(null);
+    private MultiTermQueryBuilder multiTermQueryBuilder;
 
     public SpanMultiTermQueryBuilder(MultiTermQueryBuilder multiTermQueryBuilder) {
         this.multiTermQueryBuilder = multiTermQueryBuilder;
     }
 
-    public MultiTermQueryBuilder multiTermQueryBuilder() {
-        return this.multiTermQueryBuilder;
-    }
-
     @Override
     protected void doXContent(XContentBuilder builder, Params params)
             throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(SpanMultiTermQueryParser.NAME);
         builder.field(SpanMultiTermQueryParser.MATCH_NAME);
         multiTermQueryBuilder.toXContent(builder, params);
-        printBoostAndQueryName(builder);
         builder.endObject();
     }
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query subQuery = multiTermQueryBuilder.toQuery(context);
-        if (subQuery instanceof MultiTermQuery == false) {
-            throw new UnsupportedOperationException("unsupported inner query, should be " + MultiTermQuery.class.getName() +" but was "
-                    + subQuery.getClass().getName());
-        }
-        return new SpanMultiTermQueryWrapper<>((MultiTermQuery) subQuery);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (multiTermQueryBuilder == null) {
-            validationException = addValidationError("inner clause ["+ SpanMultiTermQueryParser.MATCH_NAME +"] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(multiTermQueryBuilder, validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    protected SpanMultiTermQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        MultiTermQueryBuilder multiTermBuilder = (MultiTermQueryBuilder)in.readQuery();
-        return new SpanMultiTermQueryBuilder(multiTermBuilder);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(multiTermQueryBuilder);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(multiTermQueryBuilder);
-    }
-
-    @Override
-    protected boolean doEquals(SpanMultiTermQueryBuilder other) {
-        return Objects.equals(multiTermQueryBuilder, other.multiTermQueryBuilder);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryParser.java
index d0067ad..a44580a 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryParser.java
@@ -18,17 +18,22 @@
  */
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.MultiTermQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanMultiTermQueryWrapper;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.common.xcontent.XContentParser.Token;
 
 import java.io.IOException;
 
 /**
  *
  */
-public class SpanMultiTermQueryParser extends BaseQueryParser {
+public class SpanMultiTermQueryParser implements QueryParser {
 
+    public static final String NAME = "span_multi";
     public static final String MATCH_NAME = "match";
 
     @Inject
@@ -37,50 +42,29 @@ public class SpanMultiTermQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{SpanMultiTermQueryBuilder.NAME, Strings.toCamelCase(SpanMultiTermQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
-        String currentFieldName = null;
-        MultiTermQueryBuilder subQuery = null;
-        String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        XContentParser.Token token;
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                currentFieldName = parser.currentName();
-            } else if (token == XContentParser.Token.START_OBJECT) {
-                if (MATCH_NAME.equals(currentFieldName)) {
-                    QueryBuilder innerQuery = parseContext.parseInnerQueryBuilder();
-                    if (innerQuery instanceof MultiTermQueryBuilder == false) {
-                        throw new QueryParsingException(parseContext, "[span_multi] [" + MATCH_NAME + "] must be of type multi term query");
-                    }
-                    subQuery = (MultiTermQueryBuilder) innerQuery;
-                } else {
-                    throw new QueryParsingException(parseContext, "[span_multi] query does not support [" + currentFieldName + "]");
-                }
-            } else if (token.isValue()) {
-                if ("_name".equals(currentFieldName)) {
-                    queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
-                } else {
-                    throw new QueryParsingException(parseContext, "[span_multi] query does not support [" + currentFieldName + "]");
-                }
-            }
+
+        Token token = parser.nextToken();
+        if (!MATCH_NAME.equals(parser.currentName()) || token != XContentParser.Token.FIELD_NAME) {
+            throw new QueryParsingException(parseContext, "spanMultiTerm must have [" + MATCH_NAME + "] multi term query clause");
         }
 
-        if (subQuery == null) {
-            throw new QueryParsingException(parseContext, "[span_multi] must have [" + MATCH_NAME + "] multi term query clause");
+        token = parser.nextToken();
+        if (token != XContentParser.Token.START_OBJECT) {
+            throw new QueryParsingException(parseContext, "spanMultiTerm must have [" + MATCH_NAME + "] multi term query clause");
         }
 
-        return new SpanMultiTermQueryBuilder(subQuery).queryName(queryName).boost(boost);
-    }
+        Query subQuery = parseContext.parseInnerQuery();
+        if (!(subQuery instanceof MultiTermQuery)) {
+            throw new QueryParsingException(parseContext, "spanMultiTerm [" + MATCH_NAME + "] must be of type multi term query");
+        }
 
-    @Override
-    public SpanMultiTermQueryBuilder getBuilderPrototype() {
-        return SpanMultiTermQueryBuilder.PROTOTYPE;
+        parser.nextToken();
+        return new SpanMultiTermQueryWrapper<>((MultiTermQuery) subQuery);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java
index e00cc32..cb05e08 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java
@@ -19,179 +19,86 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanNearQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.List;
-import java.util.Objects;
 
-/**
- * Matches spans which are near one another. One can specify slop, the maximum number
- * of intervening unmatched positions, as well as whether matches are required to be in-order.
- * The span near query maps to Lucene {@link SpanNearQuery}.
- */
-public class SpanNearQueryBuilder extends AbstractQueryBuilder<SpanNearQueryBuilder> implements SpanQueryBuilder<SpanNearQueryBuilder> {
-
-    public static final String NAME = "span_near";
-
-    /** Default for flag controlling whether matches are required to be in-order */
-    public static boolean DEFAULT_IN_ORDER = true;
-
-    /** Default for flag controlling whether payloads are collected */
-    public static boolean DEFAULT_COLLECT_PAYLOADS = true;
+public class SpanNearQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanNearQueryBuilder> {
 
-    private final List<SpanQueryBuilder> clauses = new ArrayList<>();
+    private ArrayList<SpanQueryBuilder> clauses = new ArrayList<>();
 
-    private final int slop;
+    private Integer slop = null;
 
-    private boolean inOrder = DEFAULT_IN_ORDER;
+    private Boolean inOrder;
 
-    private boolean collectPayloads = DEFAULT_COLLECT_PAYLOADS;
+    private Boolean collectPayloads;
 
-    static final SpanNearQueryBuilder PROTOTYPE = new SpanNearQueryBuilder(0);
+    private float boost = -1;
 
-    /**
-     * @param slop controls the maximum number of intervening unmatched positions permitted
-     */
-    public SpanNearQueryBuilder(int slop) {
-        this.slop = slop;
-    }
-
-    /**
-     * @return the maximum number of intervening unmatched positions permitted
-     */
-    public int slop() {
-        return this.slop;
-    }
+    private String queryName;
 
     public SpanNearQueryBuilder clause(SpanQueryBuilder clause) {
         clauses.add(clause);
         return this;
     }
 
-    /**
-     * @return the {@link SpanQueryBuilder} clauses that were set for this query
-     */
-    public List<SpanQueryBuilder> clauses() {
-        return this.clauses;
+    public SpanNearQueryBuilder slop(int slop) {
+        this.slop = slop;
+        return this;
     }
 
-    /**
-     * When <code>inOrder</code> is true, the spans from each clause
-     * must be in the same order as in <code>clauses</code> and must be non-overlapping.
-     * Defaults to <code>true</code>
-     */
     public SpanNearQueryBuilder inOrder(boolean inOrder) {
         this.inOrder = inOrder;
         return this;
     }
 
-    /**
-     * @see SpanNearQueryBuilder#inOrder(boolean))
-     */
-    public boolean inOrder() {
-        return this.inOrder;
-    }
-
-    /**
-     * @param collectPayloads flag controlling whether payloads are collected
-     */
     public SpanNearQueryBuilder collectPayloads(boolean collectPayloads) {
         this.collectPayloads = collectPayloads;
         return this;
     }
 
+    @Override
+    public SpanNearQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
     /**
-     * @see SpanNearQueryBuilder#collectPayloads(boolean))
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public boolean collectPayloads() {
-        return this.collectPayloads;
+    public SpanNearQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        if (clauses.isEmpty()) {
+            throw new IllegalArgumentException("Must have at least one clause when building a spanNear query");
+        }
+        if (slop == null) {
+            throw new IllegalArgumentException("Must set the slop when building a spanNear query");
+        }
+        builder.startObject(SpanNearQueryParser.NAME);
         builder.startArray("clauses");
         for (SpanQueryBuilder clause : clauses) {
             clause.toXContent(builder, params);
         }
         builder.endArray();
-        builder.field("slop", slop);
-        builder.field("in_order", inOrder);
-        builder.field("collect_payloads", collectPayloads);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        SpanQuery[] spanQueries = new SpanQuery[clauses.size()];
-        for (int i = 0; i < clauses.size(); i++) {
-            Query query = clauses.get(i).toQuery(context);
-            assert query instanceof SpanQuery;
-            spanQueries[i] = (SpanQuery) query;
+        builder.field("slop", slop.intValue());
+        if (inOrder != null) {
+            builder.field("in_order", inOrder);
         }
-        return new SpanNearQuery(spanQueries, slop, inOrder, collectPayloads);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (clauses.isEmpty()) {
-            validationException =  addValidationError("query must include [clauses]", validationException);
+        if (collectPayloads != null) {
+            builder.field("collect_payloads", collectPayloads);
         }
-        for (SpanQueryBuilder innerClause : clauses) {
-            if (innerClause == null) {
-                validationException =  addValidationError("[clauses] contains null element", validationException);
-            } else {
-                validationException = validateInnerQuery(innerClause, validationException);
-            }
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        return validationException;
-    }
-
-    @Override
-    protected SpanNearQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        SpanNearQueryBuilder queryBuilder = new SpanNearQueryBuilder(in.readVInt());
-        List<QueryBuilder> clauses = readQueries(in);
-        for (QueryBuilder subClause : clauses) {
-            queryBuilder.clauses.add((SpanQueryBuilder)subClause);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        queryBuilder.collectPayloads = in.readBoolean();
-        queryBuilder.inOrder = in.readBoolean();
-        return queryBuilder;
-
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeVInt(slop);
-        writeQueries(out, clauses);
-        out.writeBoolean(collectPayloads);
-        out.writeBoolean(inOrder);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(clauses, slop, collectPayloads, inOrder);
-    }
-
-    @Override
-    protected boolean doEquals(SpanNearQueryBuilder other) {
-        return Objects.equals(clauses, other.clauses) &&
-               Objects.equals(slop, other.slop) &&
-               Objects.equals(collectPayloads, other.collectPayloads) &&
-               Objects.equals(inOrder, other.inOrder);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java
index 2297104..6ecf1b7 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java
@@ -19,6 +19,9 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanNearQuery;
+import org.apache.lucene.search.spans.SpanQuery;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
@@ -31,7 +34,9 @@ import static com.google.common.collect.Lists.newArrayList;
 /**
  *
  */
-public class SpanNearQueryParser extends BaseQueryParser {
+public class SpanNearQueryParser implements QueryParser {
+
+    public static final String NAME = "span_near";
 
     @Inject
     public SpanNearQueryParser() {
@@ -39,20 +44,20 @@ public class SpanNearQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{SpanNearQueryBuilder.NAME, Strings.toCamelCase(SpanNearQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         Integer slop = null;
-        boolean inOrder = SpanNearQueryBuilder.DEFAULT_IN_ORDER;
-        boolean collectPayloads = SpanNearQueryBuilder.DEFAULT_COLLECT_PAYLOADS;
+        boolean inOrder = true;
+        boolean collectPayloads = true;
         String queryName = null;
 
-        List<SpanQueryBuilder> clauses = newArrayList();
+        List<SpanQuery> clauses = newArrayList();
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -62,11 +67,11 @@ public class SpanNearQueryParser extends BaseQueryParser {
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if ("clauses".equals(currentFieldName)) {
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                        if (!(query instanceof SpanQueryBuilder)) {
+                        Query query = parseContext.parseInnerQuery();
+                        if (!(query instanceof SpanQuery)) {
                             throw new QueryParsingException(parseContext, "spanNear [clauses] must be of type span query");
                         }
-                        clauses.add((SpanQueryBuilder) query);
+                        clauses.add((SpanQuery) query);
                     }
                 } else {
                     throw new QueryParsingException(parseContext, "[span_near] query does not support [" + currentFieldName + "]");
@@ -89,24 +94,18 @@ public class SpanNearQueryParser extends BaseQueryParser {
                 throw new QueryParsingException(parseContext, "[span_near] query does not support [" + currentFieldName + "]");
             }
         }
-
+        if (clauses.isEmpty()) {
+            throw new QueryParsingException(parseContext, "span_near must include [clauses]");
+        }
         if (slop == null) {
             throw new QueryParsingException(parseContext, "span_near must include [slop]");
         }
 
-        SpanNearQueryBuilder queryBuilder = new SpanNearQueryBuilder(slop);
-        for (SpanQueryBuilder subQuery : clauses) {
-            queryBuilder.clause(subQuery);
+        SpanNearQuery query = new SpanNearQuery(clauses.toArray(new SpanQuery[clauses.size()]), slop.intValue(), inOrder, collectPayloads);
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
         }
-        queryBuilder.inOrder(inOrder);
-        queryBuilder.collectPayloads(collectPayloads);
-        queryBuilder.boost(boost);
-        queryBuilder.queryName(queryName);
-        return queryBuilder;
-    }
-
-    @Override
-    public SpanNearQueryBuilder getBuilderPrototype() {
-        return SpanNearQueryBuilder.PROTOTYPE;
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java
index 5d25feb..e37cd80 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java
@@ -19,177 +19,100 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanNotQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
-public class SpanNotQueryBuilder extends AbstractQueryBuilder<SpanNotQueryBuilder> implements SpanQueryBuilder<SpanNotQueryBuilder> {
+public class SpanNotQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanNotQueryBuilder> {
 
-    public static final String NAME = "span_not";
+    private SpanQueryBuilder include;
 
-    /** the default pre parameter size */
-    public static final int DEFAULT_PRE = 0;
-    /** the default post parameter size */
-    public static final int DEFAULT_POST = 0;
+    private SpanQueryBuilder exclude;
 
-    private final SpanQueryBuilder include;
+    private Integer dist;
 
-    private final SpanQueryBuilder exclude;
+    private Integer pre;
 
-    private int pre = DEFAULT_PRE;
+    private Integer post;
 
-    private int post = DEFAULT_POST;
+    private Float boost;
 
-    static final SpanNotQueryBuilder PROTOTYPE = new SpanNotQueryBuilder(null, null);
+    private String queryName;
 
-    /**
-     * Construct a span query matching spans from <code>include</code> which
-     * have no overlap with spans from <code>exclude</code>.
-     * @param include the span query whose matches are filtered
-     * @param exclude the span query whose matches must not overlap
-     */
-    public SpanNotQueryBuilder(SpanQueryBuilder include, SpanQueryBuilder exclude) {
+    public SpanNotQueryBuilder include(SpanQueryBuilder include) {
         this.include = include;
-        this.exclude = exclude;
-    }
-
-    /**
-     * @return the span query whose matches are filtered
-     */
-    public SpanQueryBuilder include() {
-        return this.include;
+        return this;
     }
 
-    /**
-     * @return the span query whose matches must not overlap
-     */
-    public SpanQueryBuilder exclude() {
-        return this.exclude;
+    public SpanNotQueryBuilder exclude(SpanQueryBuilder exclude) {
+        this.exclude = exclude;
+        return this;
     }
 
-    /**
-     * @param dist the amount of tokens from within the include span can’t have overlap with the exclude span.
-     * Equivalent to setting both pre and post parameter.
-     */
     public SpanNotQueryBuilder dist(int dist) {
-        pre(dist);
-        post(dist);
+        this.dist = dist;
         return this;
     }
 
-    /**
-     * @param pre the amount of tokens before the include span that can’t have overlap with the exclude span. Values
-     * smaller than 0 will be ignored and 0 used instead.
-     */
     public SpanNotQueryBuilder pre(int pre) {
-        this.pre = (pre >= 0) ? pre : 0;
+        this.pre = (pre >=0) ? pre : 0;
         return this;
     }
 
-    /**
-     * @return the amount of tokens before the include span that can’t have overlap with the exclude span.
-     * @see SpanNotQueryBuilder#pre(int)
-     */
-    public Integer pre() {
-        return this.pre;
-    }
-
-    /**
-     * @param post the amount of tokens after the include span that can’t have overlap with the exclude span.
-     */
     public SpanNotQueryBuilder post(int post) {
         this.post = (post >= 0) ? post : 0;
         return this;
     }
 
-    /**
-     * @return the amount of tokens after the include span that can’t have overlap with the exclude span.
-     * @see SpanNotQueryBuilder#post(int)
-     */
-    public Integer post() {
-        return this.post;
-    }
-
     @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("include");
-        include.toXContent(builder, params);
-        builder.field("exclude");
-        exclude.toXContent(builder, params);
-        builder.field("pre", pre);
-        builder.field("post", post);
-        printBoostAndQueryName(builder);
-        builder.endObject();
+    public SpanNotQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-
-        Query includeQuery = this.include.toQuery(context);
-        assert includeQuery instanceof SpanQuery;
-        Query excludeQuery = this.exclude.toQuery(context);
-        assert excludeQuery instanceof SpanQuery;
-
-        SpanNotQuery query = new SpanNotQuery((SpanQuery) includeQuery, (SpanQuery) excludeQuery, pre, post);
-        return query;
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     * @param queryName The query name
+     * @return this
+     */
+    public SpanNotQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
+    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
         if (include == null) {
-            validationException = addValidationError("inner clause [include] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(include, validationException);
+            throw new IllegalArgumentException("Must specify include when using spanNot query");
         }
         if (exclude == null) {
-            validationException = addValidationError("inner clause [exclude] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(exclude, validationException);
+            throw new IllegalArgumentException("Must specify exclude when using spanNot query");
         }
-        return validationException;
-    }
-
-    @Override
-    protected SpanNotQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        SpanQueryBuilder include = (SpanQueryBuilder)in.readQuery();
-        SpanQueryBuilder exclude = (SpanQueryBuilder)in.readQuery();
-        SpanNotQueryBuilder queryBuilder = new SpanNotQueryBuilder(include, exclude);
-        queryBuilder.pre(in.readVInt());
-        queryBuilder.post(in.readVInt());
-        return queryBuilder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(include);
-        out.writeQuery(exclude);
-        out.writeVInt(pre);
-        out.writeVInt(post);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(include, exclude, pre, post);
-    }
 
-    @Override
-    protected boolean doEquals(SpanNotQueryBuilder other) {
-        return Objects.equals(include, other.include) &&
-               Objects.equals(exclude, other.exclude) &&
-               (pre == other.pre) &&
-               (post == other.post);
-    }
+        if (dist != null && (pre != null || post != null)) {
+             throw new IllegalArgumentException("spanNot can either use [dist] or [pre] & [post] (or none)");
+        }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.startObject(SpanNotQueryParser.NAME);
+        builder.field("include");
+        include.toXContent(builder, params);
+        builder.field("exclude");
+        exclude.toXContent(builder, params);
+        if (dist != null) {
+            builder.field("dist", dist);
+        }
+        if (pre != null) {
+            builder.field("pre", pre);
+        }
+        if (post != null) {
+            builder.field("post", post);
+        }
+        if (boost != null) {
+            builder.field("boost", boost);
+        }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java
index 0f270bc..bcb62e7 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java
@@ -19,6 +19,9 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanNotQuery;
+import org.apache.lucene.search.spans.SpanQuery;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
@@ -28,7 +31,9 @@ import java.io.IOException;
 /**
  *
  */
-public class SpanNotQueryParser extends BaseQueryParser {
+public class SpanNotQueryParser implements QueryParser {
+
+    public static final String NAME = "span_not";
 
     @Inject
     public SpanNotQueryParser() {
@@ -36,17 +41,17 @@ public class SpanNotQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{SpanNotQueryBuilder.NAME, Strings.toCamelCase(SpanNotQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
 
-        SpanQueryBuilder include = null;
-        SpanQueryBuilder exclude = null;
+        SpanQuery include = null;
+        SpanQuery exclude = null;
 
         Integer dist = null;
         Integer pre  = null;
@@ -61,17 +66,17 @@ public class SpanNotQueryParser extends BaseQueryParser {
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("include".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (!(query instanceof SpanQueryBuilder)) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (!(query instanceof SpanQuery)) {
                         throw new QueryParsingException(parseContext, "spanNot [include] must be of type span query");
                     }
-                    include = (SpanQueryBuilder) query;
+                    include = (SpanQuery) query;
                 } else if ("exclude".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (!(query instanceof SpanQueryBuilder)) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (!(query instanceof SpanQuery)) {
                         throw new QueryParsingException(parseContext, "spanNot [exclude] must be of type span query");
                     }
-                    exclude = (SpanQueryBuilder) query;
+                    exclude = (SpanQuery) query;
                 } else {
                     throw new QueryParsingException(parseContext, "[span_not] query does not support [" + currentFieldName + "]");
                 }
@@ -101,23 +106,26 @@ public class SpanNotQueryParser extends BaseQueryParser {
             throw new QueryParsingException(parseContext, "spanNot can either use [dist] or [pre] & [post] (or none)");
         }
 
-        SpanNotQueryBuilder spanNotQuery = new SpanNotQueryBuilder(include, exclude);
-        if (dist != null) {
-            spanNotQuery.dist(dist);
-        }
-        if (pre != null) {
-            spanNotQuery.pre(pre);
+        // set appropriate defaults
+        if (pre != null && post == null) {
+            post = 0;
+        } else if (pre == null && post != null){
+            pre = 0;
         }
-        if (post != null) {
-            spanNotQuery.post(post);
+
+        SpanNotQuery query;
+        if (pre != null && post != null) {
+            query = new SpanNotQuery(include, exclude, pre, post);
+        } else if (dist != null) {
+            query = new SpanNotQuery(include, exclude, dist);
+        } else {
+            query = new SpanNotQuery(include, exclude);
         }
-        spanNotQuery.boost(boost);
-        spanNotQuery.queryName(queryName);
-        return spanNotQuery;
-    }
 
-    @Override
-    public SpanNotQueryBuilder getBuilderPrototype() {
-        return SpanNotQueryBuilder.PROTOTYPE;
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java
index 8e9b7ae..0042aa7 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java
@@ -19,108 +19,55 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanOrQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.List;
-import java.util.Objects;
 
-/**
- * Span query that matches the union of its clauses. Maps to {@link SpanOrQuery}.
- */
-public class SpanOrQueryBuilder extends AbstractQueryBuilder<SpanOrQueryBuilder> implements SpanQueryBuilder<SpanOrQueryBuilder> {
+public class SpanOrQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanOrQueryBuilder> {
 
-    public static final String NAME = "span_or";
+    private ArrayList<SpanQueryBuilder> clauses = new ArrayList<>();
 
-    private final List<SpanQueryBuilder> clauses = new ArrayList<>();
+    private float boost = -1;
 
-    static final SpanOrQueryBuilder PROTOTYPE = new SpanOrQueryBuilder();
+    private String queryName;
 
     public SpanOrQueryBuilder clause(SpanQueryBuilder clause) {
         clauses.add(clause);
         return this;
     }
 
+    @Override
+    public SpanOrQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
     /**
-     * @return the {@link SpanQueryBuilder} clauses that were set for this query
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public List<SpanQueryBuilder> clauses() {
-        return this.clauses;
+    public SpanOrQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        if (clauses.isEmpty()) {
+            throw new IllegalArgumentException("Must have at least one clause when building a spanOr query");
+        }
+        builder.startObject(SpanOrQueryParser.NAME);
         builder.startArray("clauses");
         for (SpanQueryBuilder clause : clauses) {
             clause.toXContent(builder, params);
         }
         builder.endArray();
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        SpanQuery[] spanQueries = new SpanQuery[clauses.size()];
-        for (int i = 0; i < clauses.size(); i++) {
-            Query query = clauses.get(i).toQuery(context);
-            assert query instanceof SpanQuery;
-            spanQueries[i] = (SpanQuery) query;
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        return new SpanOrQuery(spanQueries);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (clauses.isEmpty()) {
-            validationException =  addValidationError("query must include [clauses]", validationException);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        for (SpanQueryBuilder innerClause : clauses) {
-            if (innerClause == null) {
-                validationException =  addValidationError("[clauses] contains null element", validationException);
-            } else {
-                validationException = validateInnerQuery(innerClause, validationException);
-            }
-        }
-        return validationException;
-    }
-
-    @Override
-    protected SpanOrQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        SpanOrQueryBuilder queryBuilder = new SpanOrQueryBuilder();
-        List<QueryBuilder> clauses = readQueries(in);
-        for (QueryBuilder subClause : clauses) {
-            queryBuilder.clauses.add((SpanQueryBuilder)subClause);
-        }
-        return queryBuilder;
-
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        writeQueries(out, clauses);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(clauses);
-    }
-
-    @Override
-    protected boolean doEquals(SpanOrQueryBuilder other) {
-        return Objects.equals(clauses, other.clauses);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java
index 72b503a..db58d4c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java
@@ -19,7 +19,11 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanOrQuery;
+import org.apache.lucene.search.spans.SpanQuery;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
@@ -27,21 +31,30 @@ import java.util.List;
 
 import static com.google.common.collect.Lists.newArrayList;
 
-public class SpanOrQueryParser extends BaseQueryParser {
+/**
+ *
+ */
+public class SpanOrQueryParser implements QueryParser {
+
+    public static final String NAME = "span_or";
+
+    @Inject
+    public SpanOrQueryParser() {
+    }
 
     @Override
     public String[] names() {
-        return new String[]{SpanOrQueryBuilder.NAME, Strings.toCamelCase(SpanOrQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         String queryName = null;
 
-        List<SpanQueryBuilder> clauses = newArrayList();
+        List<SpanQuery> clauses = newArrayList();
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -51,11 +64,11 @@ public class SpanOrQueryParser extends BaseQueryParser {
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if ("clauses".equals(currentFieldName)) {
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                        if (!(query instanceof SpanQueryBuilder)) {
+                        Query query = parseContext.parseInnerQuery();
+                        if (!(query instanceof SpanQuery)) {
                             throw new QueryParsingException(parseContext, "spanOr [clauses] must be of type span query");
                         }
-                        clauses.add((SpanQueryBuilder) query);
+                        clauses.add((SpanQuery) query);
                     }
                 } else {
                     throw new QueryParsingException(parseContext, "[span_or] query does not support [" + currentFieldName + "]");
@@ -74,17 +87,11 @@ public class SpanOrQueryParser extends BaseQueryParser {
             throw new QueryParsingException(parseContext, "spanOr must include [clauses]");
         }
 
-        SpanOrQueryBuilder queryBuilder = new SpanOrQueryBuilder();
-        for (SpanQueryBuilder clause : clauses) {
-            queryBuilder.clause(clause);
+        SpanOrQuery query = new SpanOrQuery(clauses.toArray(new SpanQuery[clauses.size()]));
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
         }
-        queryBuilder.boost(boost);
-        queryBuilder.queryName(queryName);
-        return queryBuilder;
-    }
-
-    @Override
-    public SpanOrQueryBuilder getBuilderPrototype() {
-        return SpanOrQueryBuilder.PROTOTYPE;
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanQueryBuilder.java
index d35dcbc..4216f22 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanQueryBuilder.java
@@ -19,9 +19,6 @@
 
 package org.elasticsearch.index.query;
 
-/**
- * Marker interface for a specific type of {@link QueryBuilder} that allows to build span queries
- */
-public interface SpanQueryBuilder<QB extends SpanQueryBuilder> extends QueryBuilder<QB> {
+public abstract class SpanQueryBuilder extends QueryBuilder {
 
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java
index 24cd816..9d0176e 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java
@@ -19,76 +19,75 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.apache.lucene.search.spans.SpanTermQuery;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.lucene.BytesRefs;
-import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 
-/**
- * A Span Query that matches documents containing a term.
- * @see SpanTermQuery
- */
-public class SpanTermQueryBuilder extends BaseTermQueryBuilder<SpanTermQueryBuilder> implements SpanQueryBuilder<SpanTermQueryBuilder> {
+public class SpanTermQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanTermQueryBuilder> {
+
+    private final String name;
+
+    private final Object value;
+
+    private float boost = -1;
 
-    public static final String NAME = "span_term";
-    static final SpanTermQueryBuilder PROTOTYPE = new SpanTermQueryBuilder(null, null);
+    private String queryName;
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, String) */
     public SpanTermQueryBuilder(String name, String value) {
-        super(name, (Object) value);
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, int) */
     public SpanTermQueryBuilder(String name, int value) {
-        super(name, (Object) value);
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, long) */
     public SpanTermQueryBuilder(String name, long value) {
-        super(name, (Object) value);
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, float) */
     public SpanTermQueryBuilder(String name, float value) {
-        super(name, (Object) value);
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, double) */
     public SpanTermQueryBuilder(String name, double value) {
-        super(name, (Object) value);
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, Object) */
-    public SpanTermQueryBuilder(String name, Object value) {
-        super(name, value);
+    private SpanTermQueryBuilder(String name, Object value) {
+        this.name = name;
+        this.value = value;
     }
 
     @Override
-    public SpanQuery doToQuery(QueryShardContext context) throws IOException {
-        BytesRef valueBytes = null;
-        String fieldName = this.fieldName;
-        MappedFieldType mapper = context.fieldMapper(fieldName);
-        if (mapper != null) {
-            fieldName = mapper.names().indexName();
-            valueBytes = mapper.indexedValueForSearch(value);
-        }
-        if (valueBytes == null) {
-            valueBytes = BytesRefs.toBytesRef(this.value);
-        }
-        return new SpanTermQuery(new Term(fieldName, valueBytes));
+    public SpanTermQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
-    @Override
-    protected SpanTermQueryBuilder createBuilder(String fieldName, Object value) {
-        return new SpanTermQueryBuilder(fieldName, value);
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public SpanTermQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
-    public String getWriteableName() {
-        return NAME;
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(SpanTermQueryParser.NAME);
+        if (boost == -1 && queryName != null) {
+            builder.field(name, value);
+        } else {
+            builder.startObject(name);
+            builder.field("value", value);
+            if (boost != -1) {
+                builder.field("boost", boost);
+            }
+            if (queryName != null) {
+                builder.field("_name", queryName);
+            }
+            builder.endObject();
+        }
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java
index 2c29198..c4ff2ee 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java
@@ -19,17 +19,23 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanTermQuery;
+import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
 
 import java.io.IOException;
 
 /**
- * Parses the json representation of a spantermquery into the Elasticsearch internal
- * query builder representation.
+ *
  */
-public class SpanTermQueryParser extends BaseQueryParser {
+public class SpanTermQueryParser implements QueryParser {
+
+    public static final String NAME = "span_term";
 
     @Inject
     public SpanTermQueryParser() {
@@ -37,24 +43,23 @@ public class SpanTermQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{SpanTermQueryBuilder.NAME, Strings.toCamelCase(SpanTermQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         XContentParser.Token token = parser.currentToken();
         if (token == XContentParser.Token.START_OBJECT) {
             token = parser.nextToken();
         }
-
         assert token == XContentParser.Token.FIELD_NAME;
         String fieldName = parser.currentName();
 
 
-        Object value = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        String value = null;
+        float boost = 1.0f;
         String queryName = null;
         token = parser.nextToken();
         if (token == XContentParser.Token.START_OBJECT) {
@@ -64,9 +69,9 @@ public class SpanTermQueryParser extends BaseQueryParser {
                     currentFieldName = parser.currentName();
                 } else {
                     if ("term".equals(currentFieldName)) {
-                        value = parser.objectBytes();
+                        value = parser.text();
                     } else if ("value".equals(currentFieldName)) {
-                        value = parser.objectBytes();
+                        value = parser.text();
                     } else if ("boost".equals(currentFieldName)) {
                         boost = parser.floatValue();
                     } else if ("_name".equals(currentFieldName)) {
@@ -78,7 +83,7 @@ public class SpanTermQueryParser extends BaseQueryParser {
             }
             parser.nextToken();
         } else {
-            value = parser.objectBytes();
+            value = parser.text();
             // move to the next token
             parser.nextToken();
         }
@@ -87,13 +92,21 @@ public class SpanTermQueryParser extends BaseQueryParser {
             throw new QueryParsingException(parseContext, "No value specified for term query");
         }
 
-        SpanTermQueryBuilder result = new SpanTermQueryBuilder(fieldName, value);
-        result.boost(boost).queryName(queryName);
-        return result;
-    }
+        BytesRef valueBytes = null;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            fieldName = fieldType.names().indexName();
+            valueBytes = fieldType.indexedValueForSearch(value);
+        }
+        if (valueBytes == null) {
+            valueBytes = new BytesRef(value);
+        }
 
-    @Override
-    public SpanTermQueryBuilder getBuilderPrototype() {
-        return SpanTermQueryBuilder.PROTOTYPE;
+        SpanTermQuery query = new SpanTermQuery(new Term(fieldName, valueBytes));
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java
index a4f4f42..d2b2fdc 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java
@@ -19,53 +19,59 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.apache.lucene.search.spans.SpanWithinQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * Builder for {@link org.apache.lucene.search.spans.SpanWithinQuery}.
  */
-public class SpanWithinQueryBuilder extends AbstractQueryBuilder<SpanWithinQueryBuilder> implements SpanQueryBuilder<SpanWithinQueryBuilder> {
+public class SpanWithinQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanWithinQueryBuilder> {
 
-    public static final String NAME = "span_within";
-    private final SpanQueryBuilder big;
-    private final SpanQueryBuilder little;
-    static final SpanWithinQueryBuilder PROTOTYPE = new SpanWithinQueryBuilder(null, null);
+    private SpanQueryBuilder big;
+    private SpanQueryBuilder little;
+    private float boost = -1;
+    private String queryName;
 
-    /**
-     * Query that returns spans from <code>little</code> that are contained in a spans from <code>big</code>.
-     * @param big clause that must enclose {@code little} for a match.
-     * @param little the little clause, it must be contained within {@code big} for a match.
+    /** 
+     * Sets the little clause, it must be contained within {@code big} for a match.
      */
-    public SpanWithinQueryBuilder(SpanQueryBuilder big, SpanQueryBuilder little) {
-        this.little = little;
-        this.big = big;
+    public SpanWithinQueryBuilder little(SpanQueryBuilder clause) {
+        this.little = clause;
+        return this;
     }
 
-    /**
-     * @return the little clause, contained within {@code big} for a match.
+    /** 
+     * Sets the big clause, it must enclose {@code little} for a match.
      */
-    public SpanQueryBuilder little() {
-        return this.little;
+    public SpanWithinQueryBuilder big(SpanQueryBuilder clause) {
+        this.big = clause;
+        return this;
+    }
+
+    @Override
+    public SpanWithinQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
-     * @return the big clause that must enclose {@code little} for a match.
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public SpanQueryBuilder big() {
-        return this.big;
+    public SpanWithinQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        if (big == null) {
+            throw new IllegalArgumentException("Must specify big clause when building a span_within query");
+        }
+        if (little == null) {
+            throw new IllegalArgumentException("Must specify little clause when building a span_within query");
+        }
+        builder.startObject(SpanWithinQueryParser.NAME);
 
         builder.field("big");
         big.toXContent(builder, params);
@@ -73,62 +79,14 @@ public class SpanWithinQueryBuilder extends AbstractQueryBuilder<SpanWithinQuery
         builder.field("little");
         little.toXContent(builder, params);
 
-        printBoostAndQueryName(builder);
-
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query innerBig = big.toQuery(context);
-        assert innerBig instanceof SpanQuery;
-        Query innerLittle = little.toQuery(context);
-        assert innerLittle instanceof SpanQuery;
-        return new SpanWithinQuery((SpanQuery) innerBig, (SpanQuery) innerLittle);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (big == null) {
-            validationException = addValidationError("inner clause [big] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(big, validationException);
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        if (little == null) {
-            validationException = addValidationError("inner clause [little] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(little, validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    protected SpanWithinQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        SpanQueryBuilder big = (SpanQueryBuilder)in.readQuery();
-        SpanQueryBuilder little = (SpanQueryBuilder)in.readQuery();
-        return new SpanWithinQueryBuilder(big, little);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(big);
-        out.writeQuery(little);
-    }
 
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(big, little);
-    }
-
-    @Override
-    protected boolean doEquals(SpanWithinQueryBuilder other) {
-        return Objects.equals(big, other.big) &&
-               Objects.equals(little, other.little);
-    }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java
index d13ba97..9194cbd 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanQuery;
 import org.apache.lucene.search.spans.SpanWithinQuery;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
@@ -29,7 +31,9 @@ import java.io.IOException;
 /**
  * Parser for {@link SpanWithinQuery}
  */
-public class SpanWithinQueryParser extends BaseQueryParser {
+public class SpanWithinQueryParser implements QueryParser {
+
+    public static final String NAME = "span_within";
 
     @Inject
     public SpanWithinQueryParser() {
@@ -37,17 +41,17 @@ public class SpanWithinQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{SpanWithinQueryBuilder.NAME, Strings.toCamelCase(SpanWithinQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         String queryName = null;
-        SpanQueryBuilder big = null;
-        SpanQueryBuilder little = null;
+        SpanQuery big = null;
+        SpanQuery little = null;
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -56,17 +60,17 @@ public class SpanWithinQueryParser extends BaseQueryParser {
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("big".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (query instanceof SpanQueryBuilder == false) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (query instanceof SpanQuery == false) {
                         throw new QueryParsingException(parseContext, "span_within [big] must be of type span query");
                     }
-                    big = (SpanQueryBuilder) query;
+                    big = (SpanQuery) query;
                 } else if ("little".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (query instanceof SpanQueryBuilder == false) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (query instanceof SpanQuery == false) {
                         throw new QueryParsingException(parseContext, "span_within [little] must be of type span query");
                     }
-                    little = (SpanQueryBuilder) query;
+                    little = (SpanQuery) query;
                 } else {
                     throw new QueryParsingException(parseContext, "[span_within] query does not support [" + currentFieldName + "]");
                 }
@@ -77,8 +81,8 @@ public class SpanWithinQueryParser extends BaseQueryParser {
             } else {
                 throw new QueryParsingException(parseContext, "[span_within] query does not support [" + currentFieldName + "]");
             }
-        }
-
+        }        
+        
         if (big == null) {
             throw new QueryParsingException(parseContext, "span_within must include [big]");
         }
@@ -86,13 +90,11 @@ public class SpanWithinQueryParser extends BaseQueryParser {
             throw new QueryParsingException(parseContext, "span_within must include [little]");
         }
 
-        SpanWithinQueryBuilder query = new SpanWithinQueryBuilder(big, little);
-        query.boost(boost).queryName(queryName);
+        Query query = new SpanWithinQuery(big, little);
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
         return query;
     }
-
-    @Override
-    public SpanWithinQueryBuilder getBuilderPrototype() {
-        return SpanWithinQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java
index 63721c0..852977f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java
@@ -28,10 +28,7 @@ import java.util.Map;
 /**
  * Facilitates creating template query requests.
  * */
-public class TemplateQueryBuilder extends AbstractQueryBuilder<TemplateQueryBuilder> {
-
-    /** Name to reference this type of query. */
-    public static final String NAME = "template";
+public class TemplateQueryBuilder extends QueryBuilder {
 
     /** Template to fill. */
     private Template template;
@@ -42,8 +39,6 @@ public class TemplateQueryBuilder extends AbstractQueryBuilder<TemplateQueryBuil
 
     private ScriptService.ScriptType templateType;
 
-    static final TemplateQueryBuilder PROTOTYPE = new TemplateQueryBuilder(null, null);
-
     /**
      * @param template
      *            the template to use for that query.
@@ -82,16 +77,11 @@ public class TemplateQueryBuilder extends AbstractQueryBuilder<TemplateQueryBuil
 
     @Override
     protected void doXContent(XContentBuilder builder, Params builderParams) throws IOException {
-        builder.field(TemplateQueryBuilder.NAME);
+        builder.field(TemplateQueryParser.NAME);
         if (template == null) {
             new Template(templateString, templateType, null, null, this.vars).toXContent(builder, builderParams);
         } else {
             template.toXContent(builder, builderParams);
         }
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
index 1a75071..9b289f3 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
@@ -38,8 +38,10 @@ import java.util.Map;
  * In the simplest case, parse template string and variables from the request,
  * compile the template and execute the template against the given variables.
  * */
-public class TemplateQueryParser extends BaseQueryParserTemp {
+public class TemplateQueryParser implements QueryParser {
 
+    /** Name to reference this type of query. */
+    public static final String NAME = "template";
     /** Name of query parameter containing the template string. */
     public static final String QUERY = "query";
 
@@ -59,21 +61,20 @@ public class TemplateQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[] {TemplateQueryBuilder.NAME};
+        return new String[] { NAME };
     }
 
     /**
      * Parses the template query replacing template parameters with provided
      * values. Handles both submitting the template as part of the request as
      * well as referencing only the template name.
-     *
-     * @param context
+     * 
+     * @param parseContext
      *            parse context containing the templated query.
      */
     @Override
     @Nullable
-    public Query parse(QueryShardContext context) throws IOException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException {
         XContentParser parser = parseContext.parser();
         Template template = parse(parser, parseContext.parseFieldMatcher());
         ExecutableScript executable = this.scriptService.executable(template, ScriptContext.Standard.SEARCH);
@@ -81,9 +82,9 @@ public class TemplateQueryParser extends BaseQueryParserTemp {
         BytesReference querySource = (BytesReference) executable.run();
 
         try (XContentParser qSourceParser = XContentFactory.xContent(querySource).createParser(querySource)) {
-            final QueryShardContext contextCopy = new QueryShardContext(context.index(), context.indexQueryParserService());
-            contextCopy.reset(qSourceParser);
-            return contextCopy.parseContext().parseInnerQuery();
+            final QueryParseContext context = new QueryParseContext(parseContext.index(), parseContext.indexQueryParserService());
+            context.reset(qSourceParser);
+            return context.parseInnerQuery();
         }
     }
 
@@ -112,9 +113,4 @@ public class TemplateQueryParser extends BaseQueryParserTemp {
     public static Template parse(XContentParser parser, Map<String, ScriptService.ScriptType> parameterMap, ParseFieldMatcher parseFieldMatcher) throws IOException {
         return Template.parse(parser, parameterMap, parseFieldMatcher);
     }
-
-    @Override
-    public TemplateQueryBuilder getBuilderPrototype() {
-        return TemplateQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java
index 5c8bf3f..5bd911a 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java
@@ -19,77 +19,128 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.elasticsearch.common.lucene.BytesRefs;
-import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 
 /**
  * A Query that matches documents containing a term.
  */
-public class TermQueryBuilder extends BaseTermQueryBuilder<TermQueryBuilder> {
+public class TermQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<TermQueryBuilder> {
 
-    public static final String NAME = "term";
-    static final TermQueryBuilder PROTOTYPE = new TermQueryBuilder(null, null);
+    private final String name;
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, String) */
-    public TermQueryBuilder(String fieldName, String value) {
-        super(fieldName, (Object) value);
+    private final Object value;
+
+    private float boost = -1;
+
+    private String queryName;
+
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, String value) {
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, int) */
-    public TermQueryBuilder(String fieldName, int value) {
-        super(fieldName, (Object) value);
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, int value) {
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, long) */
-    public TermQueryBuilder(String fieldName, long value) {
-        super(fieldName, (Object) value);
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, long value) {
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, float) */
-    public TermQueryBuilder(String fieldName, float value) {
-        super(fieldName, (Object) value);
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, float value) {
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, double) */
-    public TermQueryBuilder(String fieldName, double value) {
-        super(fieldName, (Object) value);
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, double value) {
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, boolean) */
-    public TermQueryBuilder(String fieldName, boolean value) {
-        super(fieldName, (Object) value);
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, boolean value) {
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, Object) */
-    public TermQueryBuilder(String fieldName, Object value) {
-        super(fieldName, value);
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, Object value) {
+        this.name = name;
+        this.value = value;
     }
 
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
     @Override
-    public Query doToQuery(QueryShardContext context) throws IOException {
-        Query query = null;
-        MappedFieldType mapper = context.fieldMapper(this.fieldName);
-        if (mapper != null) {
-            query = mapper.termQuery(this.value, context);
-        }
-        if (query == null) {
-            query = new TermQuery(new Term(this.fieldName, BytesRefs.toBytesRef(this.value)));
-        }
-        return query;
+    public TermQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
-    @Override
-    protected TermQueryBuilder createBuilder(String fieldName, Object value) {
-        return new TermQueryBuilder(fieldName, value);
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public TermQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
-    public String getWriteableName() {
-        return NAME;
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(TermQueryParser.NAME);
+        if (boost == -1 && queryName == null) {
+            builder.field(name, value);
+        } else {
+            builder.startObject(name);
+            builder.field("value", value);
+            if (boost != -1) {
+                builder.field("boost", boost);
+            }
+            if (queryName != null) {
+                builder.field("_name", queryName);
+            }
+            builder.endObject();
+        }
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/TermQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/TermQueryParser.java
index 9127c01..be74053 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TermQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TermQueryParser.java
@@ -19,15 +19,23 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.FieldMapper;
+import org.elasticsearch.index.mapper.MappedFieldType;
 
 import java.io.IOException;
 
 /**
- * Parser for the TermQuery.
+ *
  */
-public class TermQueryParser extends BaseQueryParser {
+public class TermQueryParser implements QueryParser {
+
+    public static final String NAME = "term";
 
     @Inject
     public TermQueryParser() {
@@ -35,17 +43,17 @@ public class TermQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{TermQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String queryName = null;
         String fieldName = null;
         Object value = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         String currentFieldName = null;
         XContentParser.Token token;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -93,16 +101,22 @@ public class TermQueryParser extends BaseQueryParser {
             }
         }
 
-        TermQueryBuilder termQuery = new TermQueryBuilder(fieldName, value);
-        termQuery.boost(boost);
-        if (queryName != null) {
-            termQuery.queryName(queryName);
+        if (value == null) {
+            throw new QueryParsingException(parseContext, "No value specified for term query");
         }
-        return termQuery;
-    }
 
-    @Override
-    public TermQueryBuilder getBuilderPrototype() {
-        return TermQueryBuilder.PROTOTYPE;
+        Query query = null;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            query = fieldType.termQuery(value, parseContext);
+        }
+        if (query == null) {
+            query = new TermQuery(new Term(fieldName, BytesRefs.toBytesRef(value)));
+        }
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/TermsLookupQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/TermsLookupQueryBuilder.java
index 7e82f1f..e5daab5 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TermsLookupQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TermsLookupQueryBuilder.java
@@ -19,20 +19,102 @@
 
 package org.elasticsearch.index.query;
 
+import org.elasticsearch.common.xcontent.XContentBuilder;
+
+import java.io.IOException;
 
 /**
- * A filter for a field based on several terms matching on any of them.
- * @deprecated use {@link TermsQueryBuilder} instead.
+ * A filer for a field based on several terms matching on any of them.
  */
-@Deprecated
-public class TermsLookupQueryBuilder extends TermsQueryBuilder {
+public class TermsLookupQueryBuilder extends QueryBuilder {
+
+    private final String name;
+    private String lookupIndex;
+    private String lookupType;
+    private String lookupId;
+    private String lookupRouting;
+    private String lookupPath;
+    private Boolean lookupCache;
+
+    private String queryName;
 
     public TermsLookupQueryBuilder(String name) {
-        super(name, (Object[]) null);
+        this.name = name;
+    }
+
+    /**
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public TermsLookupQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
+    /**
+     * Sets the index name to lookup the terms from.
+     */
+    public TermsLookupQueryBuilder lookupIndex(String lookupIndex) {
+        this.lookupIndex = lookupIndex;
+        return this;
+    }
+
+    /**
+     * Sets the index type to lookup the terms from.
+     */
+    public TermsLookupQueryBuilder lookupType(String lookupType) {
+        this.lookupType = lookupType;
+        return this;
+    }
+
+    /**
+     * Sets the doc id to lookup the terms from.
+     */
+    public TermsLookupQueryBuilder lookupId(String lookupId) {
+        this.lookupId = lookupId;
+        return this;
+    }
+
+    /**
+     * Sets the path within the document to lookup the terms from.
+     */
+    public TermsLookupQueryBuilder lookupPath(String lookupPath) {
+        this.lookupPath = lookupPath;
+        return this;
+    }
+
+    public TermsLookupQueryBuilder lookupRouting(String lookupRouting) {
+        this.lookupRouting = lookupRouting;
+        return this;
+    }
+
+    public TermsLookupQueryBuilder lookupCache(boolean lookupCache) {
+        this.lookupCache = lookupCache;
+        return this;
     }
 
     @Override
-    public String getWriteableName() {
-        return TermsQueryBuilder.NAME;
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(TermsQueryParser.NAME);
+
+        builder.startObject(name);
+        if (lookupIndex != null) {
+            builder.field("index", lookupIndex);
+        }
+        builder.field("type", lookupType);
+        builder.field("id", lookupId);
+        if (lookupRouting != null) {
+            builder.field("routing", lookupRouting);
+        }
+        if (lookupCache != null) {
+            builder.field("cache", lookupCache);
+        }
+        builder.field("path", lookupPath);
+        builder.endObject();
+
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
+
+        builder.endObject();
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java
index 987e97a..5cc13c7 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java
@@ -24,29 +24,22 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import java.io.IOException;
 
 /**
- * A filter for a field based on several terms matching on any of them.
+ * A filer for a field based on several terms matching on any of them.
  */
-public class TermsQueryBuilder extends AbstractQueryBuilder<TermsQueryBuilder> {
-
-    public static final String NAME = "terms";
-
-    static final TermsQueryBuilder PROTOTYPE = new TermsQueryBuilder(null, (Object) null);
+public class TermsQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<TermsQueryBuilder> {
 
     private final String name;
 
     private final Object values;
 
+    private String queryName;
+
     private String execution;
 
-    private String lookupIndex;
-    private String lookupType;
-    private String lookupId;
-    private String lookupRouting;
-    private String lookupPath;
-    private Boolean lookupCache;
+    private float boost = -1;
 
     /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
      * @param name   The field name
      * @param values The terms
@@ -56,7 +49,7 @@ public class TermsQueryBuilder extends AbstractQueryBuilder<TermsQueryBuilder> {
     }
 
     /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
      * @param name   The field name
      * @param values The terms
@@ -67,7 +60,7 @@ public class TermsQueryBuilder extends AbstractQueryBuilder<TermsQueryBuilder> {
     }
 
     /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
      * @param name   The field name
      * @param values The terms
@@ -78,7 +71,7 @@ public class TermsQueryBuilder extends AbstractQueryBuilder<TermsQueryBuilder> {
     }
 
     /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
      * @param name   The field name
      * @param values The terms
@@ -89,7 +82,7 @@ public class TermsQueryBuilder extends AbstractQueryBuilder<TermsQueryBuilder> {
     }
 
     /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
      * @param name   The field name
      * @param values The terms
@@ -100,7 +93,7 @@ public class TermsQueryBuilder extends AbstractQueryBuilder<TermsQueryBuilder> {
     }
 
     /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
      * @param name   The field name
      * @param values The terms
@@ -111,7 +104,7 @@ public class TermsQueryBuilder extends AbstractQueryBuilder<TermsQueryBuilder> {
     }
 
     /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
      * @param name   The field name
      * @param values The terms
@@ -133,78 +126,36 @@ public class TermsQueryBuilder extends AbstractQueryBuilder<TermsQueryBuilder> {
     }
 
     /**
-     * Sets the index name to lookup the terms from.
-     */
-    public TermsQueryBuilder lookupIndex(String lookupIndex) {
-        this.lookupIndex = lookupIndex;
-        return this;
-    }
-
-    /**
-     * Sets the index type to lookup the terms from.
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
      */
-    public TermsQueryBuilder lookupType(String lookupType) {
-        this.lookupType = lookupType;
+    public TermsQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
         return this;
     }
 
-    /**
-     * Sets the doc id to lookup the terms from.
-     */
-    public TermsQueryBuilder lookupId(String lookupId) {
-        this.lookupId = lookupId;
-        return this;
-    }
-
-    /**
-     * Sets the path within the document to lookup the terms from.
-     */
-    public TermsQueryBuilder lookupPath(String lookupPath) {
-        this.lookupPath = lookupPath;
-        return this;
-    }
-
-    public TermsQueryBuilder lookupRouting(String lookupRouting) {
-        this.lookupRouting = lookupRouting;
-        return this;
-    }
-
-    public TermsQueryBuilder lookupCache(boolean lookupCache) {
-        this.lookupCache = lookupCache;
+    @Override
+    public TermsQueryBuilder boost(float boost) {
+        this.boost = boost;
         return this;
     }
 
     @Override
     public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        if (values == null) {
-            builder.startObject(name);
-            if (lookupIndex != null) {
-                builder.field("index", lookupIndex);
-            }
-            builder.field("type", lookupType);
-            builder.field("id", lookupId);
-            if (lookupRouting != null) {
-                builder.field("routing", lookupRouting);
-            }
-            if (lookupCache != null) {
-                builder.field("cache", lookupCache);
-            }
-            builder.field("path", lookupPath);
-            builder.endObject();
-        } else {
-            builder.field(name, values);
-        }
+        builder.startObject(TermsQueryParser.NAME);
+        builder.field(name, values);
+
         if (execution != null) {
             builder.field("execution", execution);
         }
 
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
+
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/TermsQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/TermsQueryParser.java
index fec398f..2930b94 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TermsQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TermsQueryParser.java
@@ -47,8 +47,9 @@ import java.util.List;
 /**
  *
  */
-public class TermsQueryParser extends BaseQueryParserTemp {
+public class TermsQueryParser implements QueryParser {
 
+    public static final String NAME = "terms";
     private static final ParseField MIN_SHOULD_MATCH_FIELD = new ParseField("min_match", "min_should_match").withAllDeprecated("Use [bool] query instead");
     private Client client;
 
@@ -61,7 +62,7 @@ public class TermsQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{TermsQueryBuilder.NAME, "in"};
+        return new String[]{NAME, "in"};
     }
 
     @Inject(optional = true)
@@ -70,8 +71,7 @@ public class TermsQueryParser extends BaseQueryParserTemp {
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String queryName = null;
@@ -142,7 +142,7 @@ public class TermsQueryParser extends BaseQueryParserTemp {
                     // ignore
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, MIN_SHOULD_MATCH_FIELD)) {
                     if (minShouldMatch != null) {
-                        throw new IllegalArgumentException("[" + currentFieldName + "] is not allowed in a filter context for the [" + TermsQueryBuilder.NAME + "] query");
+                        throw new IllegalArgumentException("[" + currentFieldName + "] is not allowed in a filter context for the [" + NAME + "] query");
                     }
                     minShouldMatch = parser.textOrNull();
                 } else if ("boost".equals(currentFieldName)) {
@@ -159,7 +159,7 @@ public class TermsQueryParser extends BaseQueryParserTemp {
             throw new QueryParsingException(parseContext, "terms query requires a field name, followed by array of terms");
         }
 
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
         if (fieldType != null) {
             fieldName = fieldType.names().indexName();
         }
@@ -182,7 +182,7 @@ public class TermsQueryParser extends BaseQueryParserTemp {
         Query query;
         if (parseContext.isFilter()) {
             if (fieldType != null) {
-                query = fieldType.termsQuery(terms, context);
+                query = fieldType.termsQuery(terms, parseContext);
             } else {
                 BytesRef[] filterValues = new BytesRef[terms.size()];
                 for (int i = 0; i < filterValues.length; i++) {
@@ -194,7 +194,7 @@ public class TermsQueryParser extends BaseQueryParserTemp {
             BooleanQuery bq = new BooleanQuery();
             for (Object term : terms) {
                 if (fieldType != null) {
-                    bq.add(fieldType.termQuery(term, context), Occur.SHOULD);
+                    bq.add(fieldType.termQuery(term, parseContext), Occur.SHOULD);
                 } else {
                     bq.add(new TermQuery(new Term(fieldName, BytesRefs.toBytesRef(term))), Occur.SHOULD);
                 }
@@ -205,13 +205,8 @@ public class TermsQueryParser extends BaseQueryParserTemp {
         query.setBoost(boost);
 
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
         return query;
     }
-
-    @Override
-    public TermsQueryBuilder getBuilderPrototype() {
-        return TermsQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java
index 173daf2..2a9a6c5 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java
@@ -19,92 +19,22 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.DocumentMapper;
-import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
 
 import java.io.IOException;
-import java.util.Objects;
 
-public class TypeQueryBuilder extends AbstractQueryBuilder<TypeQueryBuilder> {
+public class TypeQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "type";
-
-    private final BytesRef type;
-
-    static final TypeQueryBuilder PROTOTYPE = new TypeQueryBuilder((BytesRef) null);
+    private final String type;
 
     public TypeQueryBuilder(String type) {
-        this.type = BytesRefs.toBytesRef(type);
-    }
-
-    TypeQueryBuilder(BytesRef type) {
         this.type = type;
     }
 
-    public BytesRef type() {
-        return this.type;
-    }
-
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("value", type.utf8ToString());
-        printBoostAndQueryName(builder);
+        builder.startObject(TypeQueryParser.NAME);
+        builder.field("value", type);
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query filter;
-        //LUCENE 4 UPGRADE document mapper should use bytesref as well?
-        DocumentMapper documentMapper = context.mapperService().documentMapper(type.utf8ToString());
-        if (documentMapper == null) {
-            filter = new TermQuery(new Term(TypeFieldMapper.NAME, type));
-        } else {
-            filter = documentMapper.typeFilter();
-        }
-        return filter;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (type == null) {
-            validationException = addValidationError("[type] cannot be null", validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    protected TypeQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new TypeQueryBuilder(in.readBytesRef());
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeBytesRef(type);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(type);
-    }
-
-    @Override
-    protected boolean doEquals(TypeQueryBuilder other) {
-        return Objects.equals(type, other.type);
-    }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/TypeQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/TypeQueryParser.java
index 6fdb5ec..e4b7889 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TypeQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TypeQueryParser.java
@@ -19,13 +19,20 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.DocumentMapper;
+import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
 
 import java.io.IOException;
 
-public class TypeQueryParser extends BaseQueryParser {
+public class TypeQueryParser implements QueryParser {
+
+    public static final String NAME = "type";
 
     @Inject
     public TypeQueryParser() {
@@ -33,45 +40,37 @@ public class TypeQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{TypeQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
-        BytesRef type = null;
-
-        String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
 
-        String currentFieldName = null;
-        XContentParser.Token token;
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                currentFieldName = parser.currentName();
-            } else if (token.isValue()) {
-                if ("_name".equals(currentFieldName)) {
-                    queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
-                } else if ("value".equals(currentFieldName)) {
-                    type = parser.utf8Bytes();
-                }
-            } else {
-                throw new QueryParsingException(parseContext, "[type] filter doesn't support [" + currentFieldName + "]");
-            }
+        XContentParser.Token token = parser.nextToken();
+        if (token != XContentParser.Token.FIELD_NAME) {
+            throw new QueryParsingException(parseContext, "[type] filter should have a value field, and the type name");
         }
-
-        if (type == null) {
-            throw new QueryParsingException(parseContext, "[type] filter needs to be provided with a value for the type");
+        String fieldName = parser.currentName();
+        if (!fieldName.equals("value")) {
+            throw new QueryParsingException(parseContext, "[type] filter should have a value field, and the type name");
         }
-        return new TypeQueryBuilder(type)
-                .boost(boost)
-                .queryName(queryName);
-    }
+        token = parser.nextToken();
+        if (token != XContentParser.Token.VALUE_STRING) {
+            throw new QueryParsingException(parseContext, "[type] filter should have a value field, and the type name");
+        }
+        BytesRef type = parser.utf8Bytes();
+        // move to the next token
+        parser.nextToken();
 
-    @Override
-    public TypeQueryBuilder getBuilderPrototype() {
-        return TypeQueryBuilder.PROTOTYPE;
+        Query filter;
+        //LUCENE 4 UPGRADE document mapper should use bytesref as well? 
+        DocumentMapper documentMapper = parseContext.mapperService().documentMapper(type.utf8ToString());
+        if (documentMapper == null) {
+            filter = new TermQuery(new Term(TypeFieldMapper.NAME, type));
+        } else {
+            filter = documentMapper.typeFilter();
+        }
+        return filter;
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java
index 89b753e..654f14e 100644
--- a/core/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java
@@ -19,20 +19,9 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.WildcardQuery;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * Implements the wildcard search query. Supported wildcards are <tt>*</tt>, which
@@ -42,17 +31,17 @@ import java.util.Objects;
  * a Wildcard term should not start with one of the wildcards <tt>*</tt> or
  * <tt>?</tt>.
  */
-public class WildcardQueryBuilder extends AbstractQueryBuilder<WildcardQueryBuilder> implements MultiTermQueryBuilder<WildcardQueryBuilder> {
+public class WildcardQueryBuilder extends MultiTermQueryBuilder implements BoostableQueryBuilder<WildcardQueryBuilder> {
 
-    public static final String NAME = "wildcard";
+    private final String name;
 
-    private final String fieldName;
+    private final String wildcard;
 
-    private final String value;
+    private float boost = -1;
 
     private String rewrite;
 
-    static final WildcardQueryBuilder PROTOTYPE = new WildcardQueryBuilder(null, null);
+    private String queryName;
 
     /**
      * Implements the wildcard search query. Supported wildcards are <tt>*</tt>, which
@@ -62,20 +51,12 @@ public class WildcardQueryBuilder extends AbstractQueryBuilder<WildcardQueryBuil
      * a Wildcard term should not start with one of the wildcards <tt>*</tt> or
      * <tt>?</tt>.
      *
-     * @param fieldName The field name
-     * @param value The wildcard query string
+     * @param name     The field name
+     * @param wildcard The wildcard query string
      */
-    public WildcardQueryBuilder(String fieldName, String value) {
-        this.fieldName = fieldName;
-        this.value = value;
-    }
-
-    public String fieldName() {
-        return fieldName;
-    }
-
-    public String value() {
-        return value;
+    public WildcardQueryBuilder(String name, String wildcard) {
+        this.name = name;
+        this.wildcard = wildcard;
     }
 
     public WildcardQueryBuilder rewrite(String rewrite) {
@@ -83,83 +64,43 @@ public class WildcardQueryBuilder extends AbstractQueryBuilder<WildcardQueryBuil
         return this;
     }
 
-    public String rewrite() {
-        return this.rewrite;
-    }
-
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
     @Override
-    public String getWriteableName() {
-        return NAME;
+    public WildcardQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
-    @Override
-    public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.startObject(fieldName);
-        builder.field("wildcard", value);
-        if (rewrite != null) {
-            builder.field("rewrite", rewrite);
-        }
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public WildcardQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        String indexFieldName;
-        BytesRef valueBytes;
-
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType != null) {
-            indexFieldName = fieldType.names().indexName();
-            valueBytes = fieldType.indexedValueForSearch(value);
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(WildcardQueryParser.NAME);
+        if (boost == -1 && rewrite == null && queryName == null) {
+            builder.field(name, wildcard);
         } else {
-            indexFieldName = fieldName;
-            valueBytes = new BytesRef(value);
+            builder.startObject(name);
+            builder.field("wildcard", wildcard);
+            if (boost != -1) {
+                builder.field("boost", boost);
+            }
+            if (rewrite != null) {
+                builder.field("rewrite", rewrite);
+            }
+            if (queryName != null) {
+                builder.field("_name", queryName);
+            }
+            builder.endObject();
         }
-
-        WildcardQuery query = new WildcardQuery(new Term(indexFieldName, valueBytes));
-        MultiTermQuery.RewriteMethod rewriteMethod = QueryParsers.parseRewriteMethod(context.parseFieldMatcher(), rewrite, null);
-        QueryParsers.setRewriteMethod(query, rewriteMethod);
-        return query;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (Strings.isEmpty(this.fieldName)) {
-            validationException = addValidationError("field name cannot be null or empty.", validationException);
-        }
-        if (this.value == null) {
-            validationException = addValidationError("wildcard cannot be null", validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    protected WildcardQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        WildcardQueryBuilder wildcardQueryBuilder = new WildcardQueryBuilder(in.readString(), in.readString());
-        wildcardQueryBuilder.rewrite = in.readOptionalString();
-        return wildcardQueryBuilder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-        out.writeString(value);
-        out.writeOptionalString(rewrite);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldName, value, rewrite);
-    }
-
-    @Override
-    protected boolean doEquals(WildcardQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-                Objects.equals(value, other.value) &&
-                Objects.equals(rewrite, other.rewrite);
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java
index 72330ab..da92db4 100644
--- a/core/src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java
@@ -19,15 +19,23 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.WildcardQuery;
+import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
 
 /**
  *
  */
-public class WildcardQueryParser extends BaseQueryParser {
+public class WildcardQueryParser implements QueryParser {
+
+    public static final String NAME = "wildcard";
 
     @Inject
     public WildcardQueryParser() {
@@ -35,11 +43,11 @@ public class WildcardQueryParser extends BaseQueryParser {
 
     @Override
     public String[] names() {
-        return new String[]{WildcardQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         XContentParser.Token token = parser.nextToken();
@@ -47,10 +55,10 @@ public class WildcardQueryParser extends BaseQueryParser {
             throw new QueryParsingException(parseContext, "[wildcard] query malformed, no field");
         }
         String fieldName = parser.currentName();
-        String rewrite = null;
+        String rewriteMethod = null;
 
         String value = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         String queryName = null;
         token = parser.nextToken();
         if (token == XContentParser.Token.START_OBJECT) {
@@ -66,7 +74,7 @@ public class WildcardQueryParser extends BaseQueryParser {
                     } else if ("boost".equals(currentFieldName)) {
                         boost = parser.floatValue();
                     } else if ("rewrite".equals(currentFieldName)) {
-                        rewrite = parser.textOrNull();
+                        rewriteMethod = parser.textOrNull();
                     } else if ("_name".equals(currentFieldName)) {
                         queryName = parser.text();
                     } else {
@@ -83,14 +91,22 @@ public class WildcardQueryParser extends BaseQueryParser {
         if (value == null) {
             throw new QueryParsingException(parseContext, "No value specified for prefix query");
         }
-        return new WildcardQueryBuilder(fieldName, value)
-                .rewrite(rewrite)
-                .boost(boost)
-                .queryName(queryName);
-    }
 
-    @Override
-    public WildcardQueryBuilder getBuilderPrototype() {
-        return WildcardQueryBuilder.PROTOTYPE;
+        BytesRef valueBytes;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            fieldName = fieldType.names().indexName();
+            valueBytes = fieldType.indexedValueForSearch(value);
+        } else {
+            valueBytes = new BytesRef(value);
+        }
+
+        WildcardQuery wildcardQuery = new WildcardQuery(new Term(fieldName, valueBytes));
+        QueryParsers.setRewriteMethod(wildcardQuery, parseContext.parseFieldMatcher(), rewriteMethod);
+        wildcardQuery.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, wildcardQuery);
+        }
+        return wildcardQuery;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java
index aac8176..6fde3c7 100644
--- a/core/src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.index.query;
 
 import com.google.common.base.Charsets;
-
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
@@ -40,13 +39,11 @@ import java.io.IOException;
  * }
  * </pre>
  */
-public class WrapperQueryBuilder extends AbstractQueryBuilder<WrapperQueryBuilder> {
+public class WrapperQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "wrapper";
     private final byte[] source;
     private final int offset;
     private final int length;
-    static final WrapperQueryBuilder PROTOTYPE = new WrapperQueryBuilder(null, -1, -1);
 
     /**
      * Creates a query builder given a query provided as a string
@@ -77,13 +74,8 @@ public class WrapperQueryBuilder extends AbstractQueryBuilder<WrapperQueryBuilde
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(WrapperQueryParser.NAME);
         builder.field("query", source, offset, length);
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java
index 02309da..331ba78 100644
--- a/core/src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java
@@ -29,7 +29,9 @@ import java.io.IOException;
 /**
  * Query parser for JSON Queries.
  */
-public class WrapperQueryParser extends BaseQueryParserTemp {
+public class WrapperQueryParser implements QueryParser {
+
+    public static final String NAME = "wrapper";
 
     @Inject
     public WrapperQueryParser() {
@@ -37,12 +39,11 @@ public class WrapperQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{WrapperQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         XContentParser.Token token = parser.nextToken();
@@ -57,17 +58,12 @@ public class WrapperQueryParser extends BaseQueryParserTemp {
 
         byte[] querySource = parser.binaryValue();
         try (XContentParser qSourceParser = XContentFactory.xContent(querySource).createParser(querySource)) {
-            final QueryShardContext contextCopy = new QueryShardContext(context.index(), context.indexQueryParserService());
-            contextCopy.reset(qSourceParser);
-            Query result = contextCopy.parseContext().parseInnerQuery();
+            final QueryParseContext context = new QueryParseContext(parseContext.index(), parseContext.indexQueryParserService());
+            context.reset(qSourceParser);
+            Query result = context.parseInnerQuery();
             parser.nextToken();
-            context.combineNamedQueries(contextCopy);
+            parseContext.combineNamedQueries(context);
             return result;
         }
     }
-
-    @Override
-    public WrapperQueryBuilder getBuilderPrototype() {
-        return WrapperQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java
index cd18d18..3dc2427 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java
@@ -43,7 +43,7 @@ import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.core.DateFieldMapper;
 import org.elasticsearch.index.mapper.core.NumberFieldMapper;
 import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.query.functionscore.gauss.GaussDecayFunctionBuilder;
 import org.elasticsearch.index.query.functionscore.gauss.GaussDecayFunctionParser;
@@ -119,7 +119,7 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
      *
      * */
     @Override
-    public ScoreFunction parse(QueryShardContext context, XContentParser parser) throws IOException, QueryParsingException {
+    public ScoreFunction parse(QueryParseContext parseContext, XContentParser parser) throws IOException, QueryParsingException {
         String currentFieldName;
         XContentParser.Token token;
         AbstractDistanceScoreFunction scoreFunction;
@@ -132,7 +132,7 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
             if (token == XContentParser.Token.START_OBJECT) {
                 variableContent.copyCurrentStructure(parser);
                 fieldName = currentFieldName;
-            } else if (context.parseFieldMatcher().match(currentFieldName, MULTI_VALUE_MODE)) {
+            } else if (parseContext.parseFieldMatcher().match(currentFieldName, MULTI_VALUE_MODE)) {
                 multiValueMode = parser.text();
             } else {
                 throw new ElasticsearchParseException("malformed score function score parameters.");
@@ -142,34 +142,34 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
             throw new ElasticsearchParseException("malformed score function score parameters.");
         }
         XContentParser variableParser = XContentFactory.xContent(variableContent.string()).createParser(variableContent.string());
-        scoreFunction = parseVariable(fieldName, variableParser, context, MultiValueMode.fromString(multiValueMode.toUpperCase(Locale.ROOT)));
+        scoreFunction = parseVariable(fieldName, variableParser, parseContext, MultiValueMode.fromString(multiValueMode.toUpperCase(Locale.ROOT)));
         return scoreFunction;
     }
 
     // parses origin and scale parameter for field "fieldName"
-    private AbstractDistanceScoreFunction parseVariable(String fieldName, XContentParser parser, QueryShardContext context, MultiValueMode mode) throws IOException {
+    private AbstractDistanceScoreFunction parseVariable(String fieldName, XContentParser parser, QueryParseContext parseContext, MultiValueMode mode) throws IOException {
 
         // now, the field must exist, else we cannot read the value for
         // the doc later
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
         if (fieldType == null) {
-            throw new QueryParsingException(context.parseContext(), "unknown field [{}]", fieldName);
+            throw new QueryParsingException(parseContext, "unknown field [{}]", fieldName);
         }
 
         // dates and time need special handling
         parser.nextToken();
         if (fieldType instanceof DateFieldMapper.DateFieldType) {
-            return parseDateVariable(fieldName, parser, context, (DateFieldMapper.DateFieldType) fieldType, mode);
+            return parseDateVariable(fieldName, parser, parseContext, (DateFieldMapper.DateFieldType) fieldType, mode);
         } else if (fieldType instanceof GeoPointFieldMapper.GeoPointFieldType) {
-            return parseGeoVariable(fieldName, parser, context, (GeoPointFieldMapper.GeoPointFieldType) fieldType, mode);
+            return parseGeoVariable(fieldName, parser, parseContext, (GeoPointFieldMapper.GeoPointFieldType) fieldType, mode);
         } else if (fieldType instanceof NumberFieldMapper.NumberFieldType) {
-            return parseNumberVariable(fieldName, parser, context, (NumberFieldMapper.NumberFieldType) fieldType, mode);
+            return parseNumberVariable(fieldName, parser, parseContext, (NumberFieldMapper.NumberFieldType) fieldType, mode);
         } else {
-            throw new QueryParsingException(context.parseContext(), "field [{}] is of type [{}], but only numeric types are supported.", fieldName, fieldType);
+            throw new QueryParsingException(parseContext, "field [{}] is of type [{}], but only numeric types are supported.", fieldName, fieldType);
         }
     }
 
-    private AbstractDistanceScoreFunction parseNumberVariable(String fieldName, XContentParser parser, QueryShardContext context,
+    private AbstractDistanceScoreFunction parseNumberVariable(String fieldName, XContentParser parser, QueryParseContext parseContext,
             NumberFieldMapper.NumberFieldType fieldType, MultiValueMode mode) throws IOException {
         XContentParser.Token token;
         String parameterName = null;
@@ -199,11 +199,11 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
         if (!scaleFound || !refFound) {
             throw new ElasticsearchParseException("both [{}] and [{}] must be set for numeric fields.", DecayFunctionBuilder.SCALE, DecayFunctionBuilder.ORIGIN);
         }
-        IndexNumericFieldData numericFieldData = context.getForField(fieldType);
+        IndexNumericFieldData numericFieldData = parseContext.getForField(fieldType);
         return new NumericFieldDataScoreFunction(origin, scale, decay, offset, getDecayFunction(), numericFieldData, mode);
     }
 
-    private AbstractDistanceScoreFunction parseGeoVariable(String fieldName, XContentParser parser, QueryShardContext context,
+    private AbstractDistanceScoreFunction parseGeoVariable(String fieldName, XContentParser parser, QueryParseContext parseContext,
             GeoPointFieldMapper.GeoPointFieldType fieldType, MultiValueMode mode) throws IOException {
         XContentParser.Token token;
         String parameterName = null;
@@ -231,12 +231,12 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
         }
         double scale = DistanceUnit.DEFAULT.parse(scaleString, DistanceUnit.DEFAULT);
         double offset = DistanceUnit.DEFAULT.parse(offsetString, DistanceUnit.DEFAULT);
-        IndexGeoPointFieldData indexFieldData = context.getForField(fieldType);
+        IndexGeoPointFieldData indexFieldData = parseContext.getForField(fieldType);
         return new GeoFieldDataScoreFunction(origin, scale, decay, offset, getDecayFunction(), indexFieldData, mode);
 
     }
 
-    private AbstractDistanceScoreFunction parseDateVariable(String fieldName, XContentParser parser, QueryShardContext context,
+    private AbstractDistanceScoreFunction parseDateVariable(String fieldName, XContentParser parser, QueryParseContext parseContext,
             DateFieldMapper.DateFieldType dateFieldType, MultiValueMode mode) throws IOException {
         XContentParser.Token token;
         String parameterName = null;
@@ -271,7 +271,7 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
         double scale = val.getMillis();
         val = TimeValue.parseTimeValue(offsetString, TimeValue.timeValueHours(24), getClass().getSimpleName() + ".offset");
         double offset = val.getMillis();
-        IndexNumericFieldData numericFieldData = context.getForField(dateFieldType);
+        IndexNumericFieldData numericFieldData = parseContext.getForField(dateFieldType);
         return new NumericFieldDataScoreFunction(origin, scale, decay, offset, getDecayFunction(), numericFieldData, mode);
     }
 
@@ -290,6 +290,11 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
         }
 
         @Override
+        public boolean needsScores() {
+            return false;
+        }
+
+        @Override
         protected NumericDoubleValues distance(LeafReaderContext context) {
             final MultiGeoPointValues geoPointValues = fieldData.load(context).getGeoPointValues();
             return mode.select(new MultiValueMode.UnsortedNumericDoubleValues() {
@@ -353,6 +358,11 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
         }
 
         @Override
+        public boolean needsScores() {
+            return false;
+        }
+
+        @Override
         protected NumericDoubleValues distance(LeafReaderContext context) {
             final SortedNumericDoubleValues doubleValues = fieldData.load(context).getDoubleValues();
             return mode.select(new MultiValueMode.UnsortedNumericDoubleValues() {
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java
index 3880592..dc7571a 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java
@@ -21,7 +21,7 @@ package org.elasticsearch.index.query.functionscore;
 
 import org.elasticsearch.common.lucene.search.function.CombineFunction;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.AbstractQueryBuilder;
+import org.elasticsearch.index.query.BoostableQueryBuilder;
 import org.elasticsearch.index.query.QueryBuilder;
 
 import java.io.IOException;
@@ -31,12 +31,14 @@ import java.util.ArrayList;
  * A query that uses a filters with a script associated with them to compute the
  * score.
  */
-public class FunctionScoreQueryBuilder extends AbstractQueryBuilder<FunctionScoreQueryBuilder> {
+public class FunctionScoreQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<FunctionScoreQueryBuilder> {
 
     private final QueryBuilder queryBuilder;
 
     private final QueryBuilder filterBuilder;
 
+    private Float boost;
+
     private Float maxBoost;
 
     private String scoreMode;
@@ -47,8 +49,6 @@ public class FunctionScoreQueryBuilder extends AbstractQueryBuilder<FunctionScor
     private ArrayList<ScoreFunctionBuilder> scoreFunctions = new ArrayList<>();
     private Float minScore = null;
 
-    static final FunctionScoreQueryBuilder PROTOTYPE = new FunctionScoreQueryBuilder();
-
     /**
      * Creates a function_score query that executes on documents that match query a query.
      * Query and filter will be wrapped into a filtered_query.
@@ -143,6 +143,17 @@ public class FunctionScoreQueryBuilder extends AbstractQueryBuilder<FunctionScor
         return this;
     }
 
+    /**
+     * Sets the boost for this query. Documents matching this query will (in
+     * addition to the normal weightings) have their score multiplied by the
+     * boost provided.
+     */
+    @Override
+    public FunctionScoreQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
         builder.startObject(FunctionScoreQueryParser.NAME);
@@ -175,10 +186,13 @@ public class FunctionScoreQueryBuilder extends AbstractQueryBuilder<FunctionScor
         if (maxBoost != null) {
             builder.field("max_boost", maxBoost);
         }
+        if (boost != null) {
+            builder.field("boost", boost);
+        }
         if (minScore != null) {
             builder.field("min_score", minScore);
         }
-        printBoostAndQueryName(builder);
+
         builder.endObject();
     }
 
@@ -186,9 +200,4 @@ public class FunctionScoreQueryBuilder extends AbstractQueryBuilder<FunctionScor
         this.minScore = minScore;
         return this;
     }
-
-    @Override
-    public String getWriteableName() {
-        return FunctionScoreQueryParser.NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java
index ccb98ac..02fc425 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java
@@ -37,7 +37,9 @@ import org.elasticsearch.common.lucene.search.function.FunctionScoreQuery;
 import org.elasticsearch.common.lucene.search.function.ScoreFunction;
 import org.elasticsearch.common.lucene.search.function.WeightFactorFunction;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.*;
+import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.query.QueryParser;
+import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.query.functionscore.factor.FactorParser;
 
 import java.io.IOException;
@@ -82,14 +84,12 @@ public class FunctionScoreQueryParser implements QueryParser {
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         Query query = null;
         Query filter = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        String queryName = null;
+        float boost = 1.0f;
 
         FiltersFunctionScoreQuery.ScoreMode scoreMode = FiltersFunctionScoreQuery.ScoreMode.Multiply;
         ArrayList<FiltersFunctionScoreQuery.FilterFunction> filterFunctions = new ArrayList<>();
@@ -119,8 +119,6 @@ public class FunctionScoreQueryParser implements QueryParser {
                 maxBoost = parser.floatValue();
             } else if ("boost".equals(currentFieldName)) {
                 boost = parser.floatValue();
-            } else if ("_name".equals(currentFieldName)) {
-                queryName = parser.text();
             } else if ("min_score".equals(currentFieldName) || "minScore".equals(currentFieldName)) {
                 minScore = parser.floatValue();
             } else if ("functions".equals(currentFieldName)) {
@@ -128,7 +126,7 @@ public class FunctionScoreQueryParser implements QueryParser {
                     String errorString = "already found [" + singleFunctionName + "], now encountering [functions].";
                     handleMisplacedFunctionsDeclaration(errorString, singleFunctionName);
                 }
-                currentFieldName = parseFiltersAndFunctions(context, parser, filterFunctions, currentFieldName);
+                currentFieldName = parseFiltersAndFunctions(parseContext, parser, filterFunctions, currentFieldName);
                 functionArrayFound = true;
             } else {
                 ScoreFunction scoreFunction;
@@ -139,7 +137,7 @@ public class FunctionScoreQueryParser implements QueryParser {
                     // we try to parse a score function. If there is no score
                     // function for the current field name,
                     // functionParserMapper.get() will throw an Exception.
-                    scoreFunction = functionParserMapper.get(parseContext, currentFieldName).parse(context, parser);
+                    scoreFunction = functionParserMapper.get(parseContext, currentFieldName).parse(parseContext, parser);
                 }
                 if (functionArrayFound) {
                     String errorString = "already found [functions] array, now encountering [" + currentFieldName + "].";
@@ -170,7 +168,6 @@ public class FunctionScoreQueryParser implements QueryParser {
         if (maxBoost == null) {
             maxBoost = Float.MAX_VALUE;
         }
-        Query result;
         // handle cases where only one score function and no filter was
         // provided. In this case we create a FunctionScoreQuery.
         if (filterFunctions.size() == 0 || filterFunctions.size() == 1 && (filterFunctions.get(0).filter == null || Queries.isConstantMatchAllQuery(filterFunctions.get(0).filter))) {
@@ -179,8 +176,9 @@ public class FunctionScoreQueryParser implements QueryParser {
             if (combineFunction != null) {
                 theQuery.setCombineFunction(combineFunction);
             }
+            theQuery.setBoost(boost);
             theQuery.setMaxBoost(maxBoost);
-            result = theQuery;
+            return theQuery;
             // in all other cases we create a FiltersFunctionScoreQuery.
         } else {
             FiltersFunctionScoreQuery functionScoreQuery = new FiltersFunctionScoreQuery(query, scoreMode,
@@ -188,13 +186,9 @@ public class FunctionScoreQueryParser implements QueryParser {
             if (combineFunction != null) {
                 functionScoreQuery.setCombineFunction(combineFunction);
             }
-            result = functionScoreQuery;
+            functionScoreQuery.setBoost(boost);
+            return functionScoreQuery;
         }
-        result.setBoost(boost);
-        if (queryName != null) {
-            context.addNamedQuery(queryName, query);
-        }
-        return result;
     }
 
     private void handleMisplacedFunctionsDeclaration(String errorString, String functionName) {
@@ -205,9 +199,8 @@ public class FunctionScoreQueryParser implements QueryParser {
         throw new ElasticsearchParseException("failed to parse [{}] query. [{}]", NAME, errorString);
     }
 
-    private String parseFiltersAndFunctions(QueryShardContext context, XContentParser parser,
+    private String parseFiltersAndFunctions(QueryParseContext parseContext, XContentParser parser,
                                             ArrayList<FiltersFunctionScoreQuery.FilterFunction> filterFunctions, String currentFieldName) throws IOException {
-        QueryParseContext parseContext = context.parseContext();
         XContentParser.Token token;
         while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
             Query filter = null;
@@ -229,7 +222,7 @@ public class FunctionScoreQueryParser implements QueryParser {
                             // functionParserMapper throws exception if parser
                             // non-existent
                             ScoreFunctionParser functionParser = functionParserMapper.get(parseContext, currentFieldName);
-                            scoreFunction = functionParser.parse(context, parser);
+                            scoreFunction = functionParser.parse(parseContext, parser);
                         }
                     }
                 }
@@ -276,16 +269,4 @@ public class FunctionScoreQueryParser implements QueryParser {
         }
         return cf;
     }
-
-    //norelease to be removed once all queries are moved over to extend BaseQueryParser
-    @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
-        Query query = parse(parseContext.shardContext());
-        return new QueryWrappingQueryBuilder(query);
-    }
-
-    @Override
-    public FunctionScoreQueryBuilder getBuilderPrototype() {
-        return FunctionScoreQueryBuilder.PROTOTYPE;
-    }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionParser.java
index 4065f08..74c3d08 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionParser.java
@@ -21,14 +21,14 @@ package org.elasticsearch.index.query.functionscore;
 
 import org.elasticsearch.common.lucene.search.function.ScoreFunction;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParsingException;
 
 import java.io.IOException;
 
 public interface ScoreFunctionParser {
 
-    ScoreFunction parse(QueryShardContext context, XContentParser parser) throws IOException, QueryParsingException;
+    ScoreFunction parse(QueryParseContext parseContext, XContentParser parser) throws IOException, QueryParsingException;
 
     /**
      * Returns the name of the function, for example "linear", "gauss" etc. This
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/factor/FactorParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/factor/FactorParser.java
index 2635c2b..a1c8d20 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/factor/FactorParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/factor/FactorParser.java
@@ -19,13 +19,14 @@
 
 package org.elasticsearch.index.query.functionscore.factor;
 
+import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
+
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.lucene.search.function.BoostScoreFunction;
 import org.elasticsearch.common.lucene.search.function.ScoreFunction;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParsingException;
-import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
 
 import java.io.IOException;
 
@@ -42,7 +43,7 @@ public class FactorParser implements ScoreFunctionParser {
     }
 
     @Override
-    public ScoreFunction parse(QueryShardContext context, XContentParser parser) throws IOException, QueryParsingException {
+    public ScoreFunction parse(QueryParseContext parseContext, XContentParser parser) throws IOException, QueryParsingException {
         float boostFactor = parser.floatValue();
         return new BoostScoreFunction(boostFactor);
     }
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/fieldvaluefactor/FieldValueFactorFunctionParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/fieldvaluefactor/FieldValueFactorFunctionParser.java
index a91d954..e6a8f2d 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/fieldvaluefactor/FieldValueFactorFunctionParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/fieldvaluefactor/FieldValueFactorFunctionParser.java
@@ -24,8 +24,8 @@ import org.elasticsearch.common.lucene.search.function.FieldValueFactorFunction;
 import org.elasticsearch.common.lucene.search.function.ScoreFunction;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.fielddata.IndexNumericFieldData;
+import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.QueryShardContext;
 import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
@@ -52,8 +52,7 @@ public class FieldValueFactorFunctionParser implements ScoreFunctionParser {
     public static String[] NAMES = { "field_value_factor", "fieldValueFactor" };
 
     @Override
-    public ScoreFunction parse(QueryShardContext context, XContentParser parser) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public ScoreFunction parse(QueryParseContext parseContext, XContentParser parser) throws IOException, QueryParsingException {
 
         String currentFieldName = null;
         String field = null;
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionParser.java
index 20c2f55..124336c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionParser.java
@@ -27,8 +27,8 @@ import org.elasticsearch.common.lucene.search.function.RandomScoreFunction;
 import org.elasticsearch.common.lucene.search.function.ScoreFunction;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.fielddata.IndexFieldData;
+import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.QueryShardContext;
 import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
@@ -51,8 +51,8 @@ public class RandomScoreFunctionParser implements ScoreFunctionParser {
     }
 
     @Override
-    public ScoreFunction parse(QueryShardContext context, XContentParser parser) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public ScoreFunction parse(QueryParseContext parseContext, XContentParser parser) throws IOException, QueryParsingException {
+
         int seed = -1;
 
         String currentFieldName = null;
@@ -90,7 +90,7 @@ public class RandomScoreFunctionParser implements ScoreFunctionParser {
         }
 
         if (seed == -1) {
-            seed = Longs.hashCode(context.nowInMillis());
+            seed = Longs.hashCode(parseContext.nowInMillis());
         }
         final ShardId shardId = SearchContext.current().indexShard().shardId();
         final int salt = (shardId.index().name().hashCode() << 10) | shardId.id();
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/script/ScriptScoreFunctionParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/script/ScriptScoreFunctionParser.java
index 38a29f3..2cf066f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/script/ScriptScoreFunctionParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/script/ScriptScoreFunctionParser.java
@@ -21,11 +21,11 @@
 
 package org.elasticsearch.index.query.functionscore.script;
 
+import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.lucene.search.function.ScoreFunction;
 import org.elasticsearch.common.lucene.search.function.ScriptScoreFunction;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardContext;
 import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
@@ -58,8 +58,7 @@ public class ScriptScoreFunctionParser implements ScoreFunctionParser {
     }
 
     @Override
-    public ScoreFunction parse(QueryShardContext context, XContentParser parser) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public ScoreFunction parse(QueryParseContext parseContext, XContentParser parser) throws IOException, QueryParsingException {
         ScriptParameterParser scriptParameterParser = new ScriptParameterParser();
         Script script = null;
         Map<String, Object> vars = null;
@@ -101,7 +100,7 @@ public class ScriptScoreFunctionParser implements ScoreFunctionParser {
 
         SearchScript searchScript;
         try {
-            searchScript = context.scriptService().search(context.lookup(), script, ScriptContext.Standard.SEARCH);
+            searchScript = parseContext.scriptService().search(parseContext.lookup(), script, ScriptContext.Standard.SEARCH);
             return new ScriptScoreFunction(script, searchScript);
         } catch (Exception e) {
             throw new QueryParsingException(parseContext, NAMES[0] + " the script could not be loaded", e);
diff --git a/core/src/main/java/org/elasticsearch/index/query/support/InnerHitsQueryParserHelper.java b/core/src/main/java/org/elasticsearch/index/query/support/InnerHitsQueryParserHelper.java
index 6e59d01..ae839c4 100644
--- a/core/src/main/java/org/elasticsearch/index/query/support/InnerHitsQueryParserHelper.java
+++ b/core/src/main/java/org/elasticsearch/index/query/support/InnerHitsQueryParserHelper.java
@@ -22,7 +22,6 @@ package org.elasticsearch.index.query.support;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardException;
 import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsParseElement;
@@ -52,7 +51,7 @@ public class InnerHitsQueryParserHelper {
         this.fieldDataFieldsParseElement = fieldDataFieldsParseElement;
     }
 
-    public Tuple<String, SubSearchContext> parse(QueryParseContext parserContext) throws IOException, QueryShardException {
+    public Tuple<String, SubSearchContext> parse(QueryParseContext parserContext) throws IOException, QueryParsingException {
         String fieldName = null;
         XContentParser.Token token;
         String innerHitName = null;
diff --git a/core/src/main/java/org/elasticsearch/index/query/support/NestedInnerQueryParseSupport.java b/core/src/main/java/org/elasticsearch/index/query/support/NestedInnerQueryParseSupport.java
index 1e941bb..63da8a1 100644
--- a/core/src/main/java/org/elasticsearch/index/query/support/NestedInnerQueryParseSupport.java
+++ b/core/src/main/java/org/elasticsearch/index/query/support/NestedInnerQueryParseSupport.java
@@ -26,10 +26,10 @@ import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
-import org.elasticsearch.index.query.QueryShardContext;
-import org.elasticsearch.index.query.QueryShardException;
 import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
@@ -41,7 +41,6 @@ import java.io.IOException;
  */
 public class NestedInnerQueryParseSupport {
 
-    protected final QueryShardContext shardContext;
     protected final QueryParseContext parseContext;
 
     private BytesReference source;
@@ -61,15 +60,12 @@ public class NestedInnerQueryParseSupport {
     private ObjectMapper parentObjectMapper;
 
     public NestedInnerQueryParseSupport(XContentParser parser, SearchContext searchContext) {
-        parseContext = searchContext.queryParserService().getShardContext().parseContext();
-        shardContext = searchContext.queryParserService().getShardContext();
-        shardContext.reset(parser);
-
+        parseContext = searchContext.queryParserService().getParseContext();
+        parseContext.reset(parser);
     }
 
-    public NestedInnerQueryParseSupport(QueryShardContext context) {
-        this.parseContext = context.parseContext();
-        this.shardContext = context;
+    public NestedInnerQueryParseSupport(QueryParseContext parseContext) {
+        this.parseContext = parseContext;
     }
 
     public void query() throws IOException {
@@ -107,10 +103,10 @@ public class NestedInnerQueryParseSupport {
             return innerQuery;
         } else {
             if (path == null) {
-                throw new QueryShardException(shardContext, "[nested] requires 'path' field");
+                throw new QueryParsingException(parseContext, "[nested] requires 'path' field");
             }
             if (!queryFound) {
-                throw new QueryShardException(shardContext, "[nested] requires either 'query' or 'filter' field");
+                throw new QueryParsingException(parseContext, "[nested] requires either 'query' or 'filter' field");
             }
 
             XContentParser old = parseContext.parser();
@@ -136,10 +132,10 @@ public class NestedInnerQueryParseSupport {
             return innerFilter;
         } else {
             if (path == null) {
-                throw new QueryShardException(shardContext, "[nested] requires 'path' field");
+                throw new QueryParsingException(parseContext, "[nested] requires 'path' field");
             }
             if (!filterFound) {
-                throw new QueryShardException(shardContext, "[nested] requires either 'query' or 'filter' field");
+                throw new QueryParsingException(parseContext, "[nested] requires either 'query' or 'filter' field");
             }
 
             setPathLevel();
@@ -159,12 +155,12 @@ public class NestedInnerQueryParseSupport {
 
     public void setPath(String path) {
         this.path = path;
-        nestedObjectMapper = shardContext.getObjectMapper(path);
+        nestedObjectMapper = parseContext.getObjectMapper(path);
         if (nestedObjectMapper == null) {
-            throw new QueryShardException(shardContext, "[nested] failed to find nested object under path [" + path + "]");
+            throw new QueryParsingException(parseContext, "[nested] failed to find nested object under path [" + path + "]");
         }
         if (!nestedObjectMapper.nested().isNested()) {
-            throw new QueryShardException(shardContext, "[nested] nested object under path [" + path + "] is not of nested type");
+            throw new QueryParsingException(parseContext, "[nested] nested object under path [" + path + "] is not of nested type");
         }
     }
 
@@ -189,18 +185,18 @@ public class NestedInnerQueryParseSupport {
     }
 
     private void setPathLevel() {
-        ObjectMapper objectMapper = shardContext.nestedScope().getObjectMapper();
+        ObjectMapper objectMapper = parseContext.nestedScope().getObjectMapper();
         if (objectMapper == null) {
-            parentFilter = shardContext.bitsetFilter(Queries.newNonNestedFilter());
+            parentFilter = parseContext.bitsetFilter(Queries.newNonNestedFilter());
         } else {
-            parentFilter = shardContext.bitsetFilter(objectMapper.nestedTypeFilter());
+            parentFilter = parseContext.bitsetFilter(objectMapper.nestedTypeFilter());
         }
-        childFilter = shardContext.bitsetFilter(nestedObjectMapper.nestedTypeFilter());
-        parentObjectMapper = shardContext.nestedScope().nextLevel(nestedObjectMapper);
+        childFilter = parseContext.bitsetFilter(nestedObjectMapper.nestedTypeFilter());
+        parentObjectMapper = parseContext.nestedScope().nextLevel(nestedObjectMapper);
     }
 
     private void resetPathLevel() {
-        shardContext.nestedScope().previousLevel();
+        parseContext.nestedScope().previousLevel();
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/support/QueryParsers.java b/core/src/main/java/org/elasticsearch/index/query/support/QueryParsers.java
index a500393..1a12c74 100644
--- a/core/src/main/java/org/elasticsearch/index/query/support/QueryParsers.java
+++ b/core/src/main/java/org/elasticsearch/index/query/support/QueryParsers.java
@@ -29,12 +29,12 @@ import org.elasticsearch.common.ParseFieldMatcher;
  */
 public final class QueryParsers {
 
-    public static final ParseField CONSTANT_SCORE = new ParseField("constant_score", "constant_score_auto", "constant_score_filter");
-    public static final ParseField SCORING_BOOLEAN = new ParseField("scoring_boolean");
-    public static final ParseField CONSTANT_SCORE_BOOLEAN = new ParseField("constant_score_boolean");
-    public static final ParseField TOP_TERMS = new ParseField("top_terms_");
-    public static final ParseField TOP_TERMS_BOOST = new ParseField("top_terms_boost_");
-    public static final ParseField TOP_TERMS_BLENDED_FREQS = new ParseField("top_terms_blended_freqs_");
+    private static final ParseField CONSTANT_SCORE = new ParseField("constant_score", "constant_score_auto", "constant_score_filter");
+    private static final ParseField SCORING_BOOLEAN = new ParseField("scoring_boolean");
+    private static final ParseField CONSTANT_SCORE_BOOLEAN = new ParseField("constant_score_boolean");
+    private static final ParseField TOP_TERMS = new ParseField("top_terms_");
+    private static final ParseField TOP_TERMS_BOOST = new ParseField("top_terms_boost_");
+    private static final ParseField TOP_TERMS_BLENDED_FREQS = new ParseField("top_terms_blended_freqs_");
 
     private QueryParsers() {
 
diff --git a/core/src/main/java/org/elasticsearch/index/query/support/XContentStructure.java b/core/src/main/java/org/elasticsearch/index/query/support/XContentStructure.java
index cd8fd27..37716d1 100644
--- a/core/src/main/java/org/elasticsearch/index/query/support/XContentStructure.java
+++ b/core/src/main/java/org/elasticsearch/index/query/support/XContentStructure.java
@@ -25,7 +25,6 @@ import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardContext;
 import org.elasticsearch.index.query.QueryParseContext;
 
 import java.io.IOException;
@@ -85,14 +84,14 @@ public abstract class XContentStructure {
         BytesReference br = this.bytes();
         assert br != null : "innerBytes must be set with .bytes(bytes) or .freeze() before parsing";
         XContentParser innerParser = XContentHelper.createParser(br);
-        String[] origTypes = QueryShardContext.setTypesWithPrevious(types);
+        String[] origTypes = QueryParseContext.setTypesWithPrevious(types);
         XContentParser old = parseContext.parser();
         parseContext.parser(innerParser);
         try {
             return parseContext.parseInnerQuery();
         } finally {
             parseContext.parser(old);
-            QueryShardContext.setTypes(origTypes);
+            QueryParseContext.setTypes(origTypes);
         }
     }
 
@@ -107,12 +106,12 @@ public abstract class XContentStructure {
         public InnerQuery(QueryParseContext parseContext1, @Nullable String... types) throws IOException {
             super(parseContext1);
             if (types != null) {
-                String[] origTypes = QueryShardContext.setTypesWithPrevious(types);
+                String[] origTypes = QueryParseContext.setTypesWithPrevious(types);
                 try {
                     query = parseContext1.parseInnerQuery();
                     queryParsed = true;
                 } finally {
-                    QueryShardContext.setTypes(origTypes);
+                    QueryParseContext.setTypes(origTypes);
                 }
             } else {
                 BytesReference innerBytes = XContentFactory.smileBuilder().copyCurrentStructure(parseContext1.parser()).bytes();
diff --git a/core/src/main/java/org/elasticsearch/index/search/MatchQuery.java b/core/src/main/java/org/elasticsearch/index/search/MatchQuery.java
index 0b5dae6..fb5fff8 100644
--- a/core/src/main/java/org/elasticsearch/index/search/MatchQuery.java
+++ b/core/src/main/java/org/elasticsearch/index/search/MatchQuery.java
@@ -30,7 +30,7 @@ import org.elasticsearch.common.lucene.search.MultiPhrasePrefixQuery;
 import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.unit.Fuzziness;
 import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
@@ -49,7 +49,7 @@ public class MatchQuery {
         ALL
     }
 
-    protected final QueryShardContext context;
+    protected final QueryParseContext parseContext;
 
     protected String analyzer;
 
@@ -60,9 +60,9 @@ public class MatchQuery {
     protected int phraseSlop = 0;
 
     protected Fuzziness fuzziness = null;
-
+    
     protected int fuzzyPrefixLength = FuzzyQuery.defaultPrefixLength;
-
+    
     protected int maxExpansions = FuzzyQuery.defaultMaxExpansions;
 
     protected boolean transpositions = FuzzyQuery.defaultTranspositions;
@@ -72,11 +72,11 @@ public class MatchQuery {
     protected boolean lenient;
 
     protected ZeroTermsQuery zeroTermsQuery = ZeroTermsQuery.NONE;
-
+    
     protected Float commonTermsCutoff = null;
-
-    public MatchQuery(QueryShardContext context) {
-        this.context = context;
+    
+    public MatchQuery(QueryParseContext parseContext) {
+        this.parseContext = parseContext;
     }
 
     public void setAnalyzer(String analyzer) {
@@ -86,7 +86,7 @@ public class MatchQuery {
     public void setOccur(BooleanClause.Occur occur) {
         this.occur = occur;
     }
-
+    
     public void setCommonTermsCutoff(float cutoff) {
         this.commonTermsCutoff = Float.valueOf(cutoff);
     }
@@ -134,11 +134,11 @@ public class MatchQuery {
     protected Analyzer getAnalyzer(MappedFieldType fieldType) {
         if (this.analyzer == null) {
             if (fieldType != null) {
-                return context.getSearchAnalyzer(fieldType);
+                return parseContext.getSearchAnalyzer(fieldType);
             }
-            return context.mapperService().searchAnalyzer();
+            return parseContext.mapperService().searchAnalyzer();
         } else {
-            Analyzer analyzer = context.mapperService().analysisService().analyzer(this.analyzer);
+            Analyzer analyzer = parseContext.mapperService().analysisService().analyzer(this.analyzer);
             if (analyzer == null) {
                 throw new IllegalArgumentException("No analyzer found for [" + this.analyzer + "]");
             }
@@ -148,7 +148,7 @@ public class MatchQuery {
 
     public Query parse(Type type, String fieldName, Object value) throws IOException {
         final String field;
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
         if (fieldType != null) {
             field = fieldType.names().indexName();
         } else {
@@ -157,14 +157,14 @@ public class MatchQuery {
 
         if (fieldType != null && fieldType.useTermQueryWithQueryString() && !forceAnalyzeQueryString()) {
             try {
-                return fieldType.termQuery(value, context);
+                return fieldType.termQuery(value, parseContext);
             } catch (RuntimeException e) {
                 if (lenient) {
                     return null;
                 }
                 throw e;
             }
-
+            
         }
         Analyzer analyzer = getAnalyzer(fieldType);
         assert analyzer != null;
diff --git a/core/src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java b/core/src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java
index 2cac148..621e7d0 100644
--- a/core/src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java
+++ b/core/src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java
@@ -29,9 +29,10 @@ import org.apache.lucene.search.Query;
 import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.lucene.search.Queries;
+import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.query.MultiMatchQueryBuilder;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -47,10 +48,10 @@ public class MultiMatchQuery extends MatchQuery {
         this.groupTieBreaker = tieBreaker;
     }
 
-    public MultiMatchQuery(QueryShardContext context) {
-        super(context);
+    public MultiMatchQuery(QueryParseContext parseContext) {
+        super(parseContext);
     }
-
+    
     private Query parseAndApply(Type type, String fieldName, Object value, String minimumShouldMatch, Float boostValue) throws IOException {
         Query query = parse(type, fieldName, value);
         if (query instanceof BooleanQuery) {
@@ -84,7 +85,7 @@ public class MultiMatchQuery extends MatchQuery {
                 throw new IllegalStateException("No such type: " + type);
         }
         final List<? extends Query> queries = queryBuilder.buildGroupedQueries(type, fieldNames, value, minimumShouldMatch);
-        return queryBuilder.conbineGrouped(queries);
+        return queryBuilder.combineGrouped(queries);
     }
 
     private QueryBuilder queryBuilder;
@@ -118,7 +119,7 @@ public class MultiMatchQuery extends MatchQuery {
             return parseAndApply(type, field, value, minimumShouldMatch, boostValue);
         }
 
-        public Query conbineGrouped(List<? extends Query> groupQuery) {
+        public Query combineGrouped(List<? extends Query> groupQuery) {
             if (groupQuery == null || groupQuery.isEmpty()) {
                 return null;
             }
@@ -162,7 +163,7 @@ public class MultiMatchQuery extends MatchQuery {
             List<Tuple<String, Float>> missing = new ArrayList<>();
             for (Map.Entry<String, Float> entry : fieldNames.entrySet()) {
                 String name = entry.getKey();
-                MappedFieldType fieldType = context.fieldMapper(name);
+                MappedFieldType fieldType = parseContext.fieldMapper(name);
                 if (fieldType != null) {
                     Analyzer actualAnalyzer = getAnalyzer(fieldType);
                     name = fieldType.names().indexName();
@@ -195,7 +196,7 @@ public class MultiMatchQuery extends MatchQuery {
                     blendedFields = null;
                 }
                 final FieldAndFieldType fieldAndFieldType = group.get(0);
-                Query q = parseGroup(type.matchQueryType(), fieldAndFieldType.field, fieldAndFieldType.boost, value, minimumShouldMatch);
+                Query q = parseGroup(type.matchQueryType(), fieldAndFieldType.field, 1f, value, minimumShouldMatch);
                 if (q != null) {
                     queries.add(q);
                 }
diff --git a/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java b/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java
index d8bea76..46c03de 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java
@@ -35,12 +35,7 @@ import org.elasticsearch.index.aliases.IndexAliasesService;
 import org.elasticsearch.index.cache.IndexCache;
 import org.elasticsearch.index.engine.Engine;
 import org.elasticsearch.index.engine.IgnoreOnRecoveryEngineException;
-import org.elasticsearch.index.mapper.DocumentMapper;
-import org.elasticsearch.index.mapper.MapperException;
-import org.elasticsearch.index.mapper.MapperService;
-import org.elasticsearch.index.mapper.MapperUtils;
-import org.elasticsearch.index.mapper.Mapping;
-import org.elasticsearch.index.mapper.Uid;
+import org.elasticsearch.index.mapper.*;
 import org.elasticsearch.index.query.IndexQueryParserService;
 import org.elasticsearch.index.query.ParsedQuery;
 import org.elasticsearch.index.query.QueryParsingException;
diff --git a/core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java b/core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java
index fd94546..d8e8139 100644
--- a/core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java
+++ b/core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java
@@ -244,7 +244,7 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic
     }
 
     private void applyDeletedShards(final ClusterChangedEvent event) {
-        RoutingNodes.RoutingNodeIterator routingNode = event.state().readOnlyRoutingNodes().routingNodeIter(event.state().nodes().localNodeId());
+        RoutingNodes.RoutingNodeIterator routingNode = event.state().getRoutingNodes().routingNodeIter(event.state().nodes().localNodeId());
         if (routingNode == null) {
             return;
         }
@@ -284,7 +284,7 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic
 
     private void applyNewIndices(final ClusterChangedEvent event) {
         // we only create indices for shards that are allocated
-        RoutingNodes.RoutingNodeIterator routingNode = event.state().readOnlyRoutingNodes().routingNodeIter(event.state().nodes().localNodeId());
+        RoutingNodes.RoutingNodeIterator routingNode = event.state().getRoutingNodes().routingNodeIter(event.state().nodes().localNodeId());
         if (routingNode == null) {
             return;
         }
@@ -465,7 +465,7 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic
         }
 
         RoutingTable routingTable = event.state().routingTable();
-        RoutingNodes.RoutingNodeIterator routingNode = event.state().readOnlyRoutingNodes().routingNodeIter(event.state().nodes().localNodeId());
+        RoutingNodes.RoutingNodeIterator routingNode = event.state().getRoutingNodes().routingNodeIter(event.state().nodes().localNodeId());
 
         if (routingNode == null) {
             failedShards.clear();
@@ -545,7 +545,7 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic
 
     private void cleanFailedShards(final ClusterChangedEvent event) {
         RoutingTable routingTable = event.state().routingTable();
-        RoutingNodes.RoutingNodeIterator routingNode = event.state().readOnlyRoutingNodes().routingNodeIter(event.state().nodes().localNodeId());
+        RoutingNodes.RoutingNodeIterator routingNode = event.state().getRoutingNodes().routingNodeIter(event.state().nodes().localNodeId());
         if (routingNode == null) {
             failedShards.clear();
             return;
diff --git a/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesModule.java b/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesModule.java
index 665ddf9..fb7ca17 100644
--- a/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesModule.java
+++ b/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesModule.java
@@ -32,9 +32,6 @@ public class IndicesQueriesModule extends AbstractModule {
 
     private Set<Class<? extends QueryParser>> queryParsersClasses = Sets.newHashSet();
 
-    /**
-     * Registers a {@link QueryParser} given its class
-     */
     public synchronized IndicesQueriesModule addQuery(Class<? extends QueryParser> queryParser) {
         queryParsersClasses.add(queryParser);
         return this;
@@ -86,6 +83,7 @@ public class IndicesQueriesModule extends AbstractModule {
         qpBinders.addBinding().to(TemplateQueryParser.class).asEagerSingleton();
         qpBinders.addBinding().to(TypeQueryParser.class).asEagerSingleton();
         qpBinders.addBinding().to(LimitQueryParser.class).asEagerSingleton();
+        qpBinders.addBinding().to(TermsQueryParser.class).asEagerSingleton();
         qpBinders.addBinding().to(ScriptQueryParser.class).asEagerSingleton();
         qpBinders.addBinding().to(GeoDistanceQueryParser.class).asEagerSingleton();
         qpBinders.addBinding().to(GeoDistanceRangeQueryParser.class).asEagerSingleton();
diff --git a/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java b/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java
index 925eaa8..7d13fe0 100644
--- a/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java
+++ b/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java
@@ -24,10 +24,7 @@ import com.google.common.collect.Maps;
 
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.query.EmptyQueryBuilder;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.index.query.QueryParser;
 
 import java.util.Map;
@@ -38,18 +35,14 @@ public class IndicesQueriesRegistry extends AbstractComponent {
     private ImmutableMap<String, QueryParser> queryParsers;
 
     @Inject
-    public IndicesQueriesRegistry(Settings settings, Set<QueryParser> injectedQueryParsers, NamedWriteableRegistry namedWriteableRegistry) {
+    public IndicesQueriesRegistry(Settings settings, Set<QueryParser> injectedQueryParsers) {
         super(settings);
         Map<String, QueryParser> queryParsers = Maps.newHashMap();
         for (QueryParser queryParser : injectedQueryParsers) {
             for (String name : queryParser.names()) {
                 queryParsers.put(name, queryParser);
             }
-            namedWriteableRegistry.registerPrototype(QueryBuilder.class, queryParser.getBuilderPrototype());
         }
-        // EmptyQueryBuilder is not registered as query parser but used internally.
-        // We need to register it with the NamedWriteableRegistry in order to serialize it
-        namedWriteableRegistry.registerPrototype(QueryBuilder.class, EmptyQueryBuilder.PROTOTYPE);
         this.queryParsers = ImmutableMap.copyOf(queryParsers);
     }
 
diff --git a/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java b/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java
index 80b319d..f5c6be5 100644
--- a/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java
+++ b/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java
@@ -93,7 +93,7 @@ public class RecoverySource extends AbstractComponent {
 
         // starting recovery from that our (the source) shard state is marking the shard to be in recovery mode as well, otherwise
         // the index operations will not be routed to it properly
-        RoutingNode node = clusterService.state().readOnlyRoutingNodes().node(request.targetNode().id());
+        RoutingNode node = clusterService.state().getRoutingNodes().node(request.targetNode().id());
         if (node == null) {
             logger.debug("delaying recovery of {} as source node {} is unknown", request.shardId(), request.targetNode());
             throw new DelayRecoveryException("source node does not have the node [" + request.targetNode() + "] in its state yet..");
diff --git a/core/src/main/java/org/elasticsearch/rest/action/explain/RestExplainAction.java b/core/src/main/java/org/elasticsearch/rest/action/explain/RestExplainAction.java
index 7c01fdd..ce306c6 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/explain/RestExplainAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/explain/RestExplainAction.java
@@ -30,7 +30,6 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.index.get.GetResult;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.QueryStringQueryBuilder;
 import org.elasticsearch.rest.*;
@@ -75,7 +74,13 @@ public class RestExplainAction extends BaseRestHandler {
             queryStringBuilder.lenient(request.paramAsBoolean("lenient", null));
             String defaultOperator = request.param("default_operator");
             if (defaultOperator != null) {
-                queryStringBuilder.defaultOperator(Operator.fromString(defaultOperator));
+                if ("OR".equals(defaultOperator)) {
+                    queryStringBuilder.defaultOperator(QueryStringQueryBuilder.Operator.OR);
+                } else if ("AND".equals(defaultOperator)) {
+                    queryStringBuilder.defaultOperator(QueryStringQueryBuilder.Operator.AND);
+                } else {
+                    throw new IllegalArgumentException("Unsupported defaultOperator [" + defaultOperator + "], can either be [OR] or [AND]");
+                }
             }
 
             QuerySourceBuilder querySourceBuilder = new QuerySourceBuilder();
diff --git a/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java b/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java
index 674aa69..bd17c1d 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java
@@ -27,7 +27,6 @@ import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.lucene.uid.Versions;
 import org.elasticsearch.common.xcontent.*;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.QueryStringQueryBuilder;
 import org.elasticsearch.rest.RestRequest;
@@ -98,7 +97,13 @@ public class RestActions {
         queryBuilder.lenient(request.paramAsBoolean("lenient", null));
         String defaultOperator = request.param("default_operator");
         if (defaultOperator != null) {
-            queryBuilder.defaultOperator(Operator.fromString(defaultOperator));
+            if ("OR".equals(defaultOperator)) {
+                queryBuilder.defaultOperator(QueryStringQueryBuilder.Operator.OR);
+            } else if ("AND".equals(defaultOperator)) {
+                queryBuilder.defaultOperator(QueryStringQueryBuilder.Operator.AND);
+            } else {
+                throw new IllegalArgumentException("Unsupported defaultOperator [" + defaultOperator + "], can either be [OR] or [AND]");
+            }
         }
         return new QuerySourceBuilder().setQuery(queryBuilder);
     }
diff --git a/core/src/main/java/org/elasticsearch/script/NativeScriptEngineService.java b/core/src/main/java/org/elasticsearch/script/NativeScriptEngineService.java
index b46bc73..548c1b3 100644
--- a/core/src/main/java/org/elasticsearch/script/NativeScriptEngineService.java
+++ b/core/src/main/java/org/elasticsearch/script/NativeScriptEngineService.java
@@ -86,6 +86,10 @@ public class NativeScriptEngineService extends AbstractComponent implements Scri
                 script.setLookup(lookup.getLeafSearchLookup(context));
                 return script;
             }
+            @Override
+            public boolean needsScores() {
+                return scriptFactory.needsScores();
+            }
         };
     }
 
diff --git a/core/src/main/java/org/elasticsearch/script/NativeScriptFactory.java b/core/src/main/java/org/elasticsearch/script/NativeScriptFactory.java
index 016103c..1625afe 100644
--- a/core/src/main/java/org/elasticsearch/script/NativeScriptFactory.java
+++ b/core/src/main/java/org/elasticsearch/script/NativeScriptFactory.java
@@ -41,4 +41,11 @@ public interface NativeScriptFactory {
      * @param params The parameters passed to the script. Can be <tt>null</tt>.
      */
     ExecutableScript newScript(@Nullable Map<String, Object> params);
+
+    /**
+     * Indicates if document scores may be needed by the produced scripts.
+     * 
+     * @return {@code true} if scores are needed.
+     */
+    boolean needsScores();
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/script/SearchScript.java b/core/src/main/java/org/elasticsearch/script/SearchScript.java
index 3169052..5d5f218 100644
--- a/core/src/main/java/org/elasticsearch/script/SearchScript.java
+++ b/core/src/main/java/org/elasticsearch/script/SearchScript.java
@@ -29,4 +29,11 @@ public interface SearchScript {
 
     LeafSearchScript getLeafSearchScript(LeafReaderContext context) throws IOException;
 
+    /**
+     * Indicates if document scores may be needed by this {@link SearchScript}.
+     * 
+     * @return {@code true} if scores are needed.
+     */
+    boolean needsScores();
+
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/script/expression/ExpressionScriptEngineService.java b/core/src/main/java/org/elasticsearch/script/expression/ExpressionScriptEngineService.java
index d9dd34c..9cd7da3 100644
--- a/core/src/main/java/org/elasticsearch/script/expression/ExpressionScriptEngineService.java
+++ b/core/src/main/java/org/elasticsearch/script/expression/ExpressionScriptEngineService.java
@@ -112,7 +112,6 @@ public class ExpressionScriptEngineService extends AbstractComponent implements
             for (String variable : expr.variables) {
                 if (variable.equals("_score")) {
                     bindings.add(new SortField("_score", SortField.Type.SCORE));
-
                 } else if (variable.equals("_value")) {
                     specialValue = new ReplaceableConstValueSource();
                     bindings.add("_value", specialValue);
@@ -173,7 +172,8 @@ public class ExpressionScriptEngineService extends AbstractComponent implements
                 }
             }
 
-            return new ExpressionSearchScript(compiledScript, bindings, specialValue);
+            final boolean needsScores = expr.getSortField(bindings, false).needsScores();
+            return new ExpressionSearchScript(compiledScript, bindings, specialValue, needsScores);
         } catch (Exception exception) {
             throw new ScriptException("Error during search with " + compiledScript, exception);
         }
diff --git a/core/src/main/java/org/elasticsearch/script/expression/ExpressionSearchScript.java b/core/src/main/java/org/elasticsearch/script/expression/ExpressionSearchScript.java
index 948b8d3..2825e55 100644
--- a/core/src/main/java/org/elasticsearch/script/expression/ExpressionSearchScript.java
+++ b/core/src/main/java/org/elasticsearch/script/expression/ExpressionSearchScript.java
@@ -46,14 +46,21 @@ class ExpressionSearchScript implements SearchScript {
     final SimpleBindings bindings;
     final ValueSource source;
     final ReplaceableConstValueSource specialValue; // _value
+    final boolean needsScores;
     Scorer scorer;
     int docid;
 
-    ExpressionSearchScript(CompiledScript c, SimpleBindings b, ReplaceableConstValueSource v) {
+    ExpressionSearchScript(CompiledScript c, SimpleBindings b, ReplaceableConstValueSource v, boolean needsScores) {
         compiledScript = c;
         bindings = b;
         source = ((Expression)compiledScript.compiled()).getValueSource(bindings);
         specialValue = v;
+        this.needsScores = needsScores;
+    }
+
+    @Override
+    public boolean needsScores() {
+        return needsScores;
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/script/groovy/GroovyScriptEngineService.java b/core/src/main/java/org/elasticsearch/script/groovy/GroovyScriptEngineService.java
index 17c4284..93f196f 100644
--- a/core/src/main/java/org/elasticsearch/script/groovy/GroovyScriptEngineService.java
+++ b/core/src/main/java/org/elasticsearch/script/groovy/GroovyScriptEngineService.java
@@ -168,6 +168,12 @@ public class GroovyScriptEngineService extends AbstractComponent implements Scri
                 }
                 return new GroovyScript(compiledScript, scriptObject, leafLookup, logger);
             }
+
+            @Override
+            public boolean needsScores() {
+                // TODO: can we reliably know if a groovy script makes use of _score
+                return true;
+            }
         };
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java
index f85bd80..0ac3b1d 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java
@@ -28,7 +28,7 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.index.query.QueryParsingException;
 
 import java.io.IOException;
 
@@ -115,7 +115,7 @@ public class GND extends NXYSignificanceHeuristic {
         }
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException, QueryShardException {
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException, QueryParsingException {
             String givenName = parser.currentName();
             boolean backgroundIsSuperset = true;
             XContentParser.Token token = parser.nextToken();
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java
index 5c9794a..78f1573 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java
@@ -27,7 +27,7 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.index.query.QueryParsingException;
 
 import java.io.IOException;
 
@@ -108,7 +108,7 @@ public class JLHScore extends SignificanceHeuristic {
     public static class JLHScoreParser implements SignificanceHeuristicParser {
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException, QueryShardException {
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException, QueryParsingException {
             // move to the closing bracket
             if (!parser.nextToken().equals(XContentParser.Token.END_OBJECT)) {
                 throw new ElasticsearchParseException("failed to parse [jhl] significance heuristic. expected an empty object, but found [{}] instead", parser.currentToken());
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java
index d21b319..cc684c8 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java
@@ -27,7 +27,7 @@ import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.index.query.QueryParsingException;
 
 import java.io.IOException;
 
@@ -138,7 +138,7 @@ public abstract class NXYSignificanceHeuristic extends SignificanceHeuristic {
     public static abstract class NXYParser implements SignificanceHeuristicParser {
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException, QueryShardException {
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException, QueryParsingException {
             String givenName = parser.currentName();
             boolean includeNegatives = false;
             boolean backgroundIsSuperset = true;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java
index 25556c9..1587a8f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java
@@ -27,7 +27,7 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.index.query.QueryParsingException;
 
 import java.io.IOException;
 
@@ -77,7 +77,7 @@ public class PercentageScore extends SignificanceHeuristic {
     public static class PercentageScoreParser implements SignificanceHeuristicParser {
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException, QueryShardException {
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException, QueryParsingException {
             // move to the closing bracket
             if (!parser.nextToken().equals(XContentParser.Token.END_OBJECT)) {
                 throw new ElasticsearchParseException("failed to parse [percentage] significance heuristic. expected an empty object, but got [{}] instead", parser.currentToken());
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java
index d0a26b6..59acd2d 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java
@@ -30,7 +30,7 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.logging.ESLoggerFactory;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.script.*;
 import org.elasticsearch.script.Script.ScriptField;
 import org.elasticsearch.script.ScriptParameterParser.ScriptParameterValue;
@@ -130,7 +130,7 @@ public class ScriptHeuristic extends SignificanceHeuristic {
         }
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException, QueryShardException {
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException, QueryParsingException {
             String heuristicName = parser.currentName();
             Script script = null;
             XContentParser.Token token;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSource.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSource.java
index 577b3b7..ab6648d 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSource.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSource.java
@@ -216,8 +216,7 @@ public abstract class ValuesSource {
 
             @Override
             public boolean needsScores() {
-                // TODO: add a way to know whether scripts are using scores
-                return true;
+                return script.needsScores();
             }
         }
 
@@ -295,8 +294,7 @@ public abstract class ValuesSource {
 
             @Override
             public boolean needsScores() {
-                // TODO: add a way to know whether scripts are using scores
-                return true;
+                return script.needsScores();
             }
 
             @Override
@@ -431,8 +429,7 @@ public abstract class ValuesSource {
 
             @Override
             public boolean needsScores() {
-                // TODO: add a way to know whether scripts are using scores
-                return true;
+                return script.needsScores();
             }
         }
 
@@ -451,8 +448,7 @@ public abstract class ValuesSource {
 
         @Override
         public boolean needsScores() {
-            // TODO: add a way to know whether scripts are using scores
-            return true;
+            return script.needsScores();
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java b/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
index 2df22bb..0eb4c5c 100644
--- a/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
@@ -37,6 +37,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.script.Script;
+import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
 import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
 import org.elasticsearch.search.fetch.source.FetchSourceContext;
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsParseElement.java b/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsParseElement.java
index ac6dc18..c02e2c6 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsParseElement.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsParseElement.java
@@ -24,7 +24,7 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.index.query.ParsedQuery;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.search.SearchParseElement;
 import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsParseElement;
 import org.elasticsearch.search.fetch.script.ScriptFieldsParseElement;
@@ -59,15 +59,15 @@ public class InnerHitsParseElement implements SearchParseElement {
 
     @Override
     public void parse(XContentParser parser, SearchContext searchContext) throws Exception {
-        QueryShardContext context = searchContext.queryParserService().getShardContext();
-        context.reset(parser);
-        Map<String, InnerHitsContext.BaseInnerHits> innerHitsMap = parseInnerHits(parser, context, searchContext);
+        QueryParseContext parseContext = searchContext.queryParserService().getParseContext();
+        parseContext.reset(parser);
+        Map<String, InnerHitsContext.BaseInnerHits> innerHitsMap = parseInnerHits(parser, parseContext, searchContext);
         if (innerHitsMap != null) {
             searchContext.innerHits(new InnerHitsContext(innerHitsMap));
         }
     }
 
-    private Map<String, InnerHitsContext.BaseInnerHits> parseInnerHits(XContentParser parser, QueryShardContext context, SearchContext searchContext) throws Exception {
+    private Map<String, InnerHitsContext.BaseInnerHits> parseInnerHits(XContentParser parser, QueryParseContext parseContext, SearchContext searchContext) throws Exception {
         XContentParser.Token token;
         Map<String, InnerHitsContext.BaseInnerHits> innerHitsMap = null;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -79,7 +79,7 @@ public class InnerHitsParseElement implements SearchParseElement {
             if (token != XContentParser.Token.START_OBJECT) {
                 throw new IllegalArgumentException("Inner hit definition for [" + innerHitName + " starts with a [" + token + "], expected a [" + XContentParser.Token.START_OBJECT + "].");
             }
-            InnerHitsContext.BaseInnerHits innerHits = parseInnerHit(parser, context, searchContext, innerHitName);
+            InnerHitsContext.BaseInnerHits innerHits = parseInnerHit(parser, parseContext, searchContext, innerHitName);
             if (innerHitsMap == null) {
                 innerHitsMap = new HashMap<>();
             }
@@ -88,7 +88,7 @@ public class InnerHitsParseElement implements SearchParseElement {
         return innerHitsMap;
     }
 
-    private InnerHitsContext.BaseInnerHits parseInnerHit(XContentParser parser, QueryShardContext context, SearchContext searchContext, String innerHitName) throws Exception {
+    private InnerHitsContext.BaseInnerHits parseInnerHit(XContentParser parser, QueryParseContext parseContext, SearchContext searchContext, String innerHitName) throws Exception {
         XContentParser.Token token = parser.nextToken();
         if (token != XContentParser.Token.FIELD_NAME) {
             throw new IllegalArgumentException("Unexpected token " + token + " inside inner hit definition. Either specify [path] or [type] object");
@@ -123,9 +123,9 @@ public class InnerHitsParseElement implements SearchParseElement {
 
         final InnerHitsContext.BaseInnerHits innerHits;
         if (nestedPath != null) {
-            innerHits = parseNested(parser, context, searchContext, fieldName);
+            innerHits = parseNested(parser, parseContext, searchContext, fieldName);
         } else if (type != null) {
-            innerHits = parseParentChild(parser, context, searchContext, fieldName);
+            innerHits = parseParentChild(parser, parseContext, searchContext, fieldName);
         } else {
             throw new IllegalArgumentException("Either [path] or [type] must be defined");
         }
@@ -143,16 +143,16 @@ public class InnerHitsParseElement implements SearchParseElement {
         return innerHits;
     }
 
-    private InnerHitsContext.ParentChildInnerHits parseParentChild(XContentParser parser, QueryShardContext context, SearchContext searchContext, String type) throws Exception {
-        ParseResult parseResult = parseSubSearchContext(searchContext, context, parser);
+    private InnerHitsContext.ParentChildInnerHits parseParentChild(XContentParser parser, QueryParseContext parseContext, SearchContext searchContext, String type) throws Exception {
+        ParseResult parseResult = parseSubSearchContext(searchContext, parseContext, parser);
         DocumentMapper documentMapper = searchContext.mapperService().documentMapper(type);
         if (documentMapper == null) {
             throw new IllegalArgumentException("type [" + type + "] doesn't exist");
         }
-        return new InnerHitsContext.ParentChildInnerHits(parseResult.context(), parseResult.query(), parseResult.childInnerHits(), context.mapperService(), documentMapper);
+        return new InnerHitsContext.ParentChildInnerHits(parseResult.context(), parseResult.query(), parseResult.childInnerHits(), parseContext.mapperService(), documentMapper);
     }
 
-    private InnerHitsContext.NestedInnerHits parseNested(XContentParser parser, QueryShardContext context, SearchContext searchContext, String nestedPath) throws Exception {
+    private InnerHitsContext.NestedInnerHits parseNested(XContentParser parser, QueryParseContext parseContext, SearchContext searchContext, String nestedPath) throws Exception {
         ObjectMapper objectMapper = searchContext.getObjectMapper(nestedPath);
         if (objectMapper == null) {
             throw new IllegalArgumentException("path [" + nestedPath +"] doesn't exist");
@@ -160,14 +160,14 @@ public class InnerHitsParseElement implements SearchParseElement {
         if (objectMapper.nested().isNested() == false) {
             throw new IllegalArgumentException("path [" + nestedPath +"] isn't nested");
         }
-        ObjectMapper parentObjectMapper = context.nestedScope().nextLevel(objectMapper);
-        ParseResult parseResult = parseSubSearchContext(searchContext, context, parser);
-        context.nestedScope().previousLevel();
+        ObjectMapper parentObjectMapper = parseContext.nestedScope().nextLevel(objectMapper);
+        ParseResult parseResult = parseSubSearchContext(searchContext, parseContext, parser);
+        parseContext.nestedScope().previousLevel();
 
         return new InnerHitsContext.NestedInnerHits(parseResult.context(), parseResult.query(), parseResult.childInnerHits(), parentObjectMapper, objectMapper);
     }
 
-    private ParseResult parseSubSearchContext(SearchContext searchContext, QueryShardContext context, XContentParser parser) throws Exception {
+    private ParseResult parseSubSearchContext(SearchContext searchContext, QueryParseContext parseContext, XContentParser parser) throws Exception {
         ParsedQuery query = null;
         Map<String, InnerHitsContext.BaseInnerHits> childInnerHits = null;
         SubSearchContext subSearchContext = new SubSearchContext(searchContext);
@@ -178,10 +178,10 @@ public class InnerHitsParseElement implements SearchParseElement {
                 fieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("query".equals(fieldName)) {
-                    Query q = searchContext.queryParserService().parseInnerQuery(context);
-                    query = new ParsedQuery(q, context.copyNamedQueries());
+                    Query q = searchContext.queryParserService().parseInnerQuery(parseContext);
+                    query = new ParsedQuery(q, parseContext.copyNamedQueries());
                 } else if ("inner_hits".equals(fieldName)) {
-                    childInnerHits = parseInnerHits(parser, context, searchContext);
+                    childInnerHits = parseInnerHits(parser, parseContext, searchContext);
                 } else {
                     parseCommonInnerHitOptions(parser, token, fieldName, subSearchContext, sortParseElement, sourceParseElement, highlighterParseElement, scriptFieldsParseElement, fieldDataFieldsParseElement);
                 }
diff --git a/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java b/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
index 1c90a2b..901b721 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
@@ -41,7 +41,7 @@ import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.index.query.IndexQueryParserService;
 import org.elasticsearch.index.query.ParsedQuery;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.script.ScriptService;
@@ -74,12 +74,12 @@ public abstract class SearchContext implements Releasable, HasContextAndHeaders
 
     public static void setCurrent(SearchContext value) {
         current.set(value);
-        QueryShardContext.setTypes(value.types());
+        QueryParseContext.setTypes(value.types());
     }
 
     public static void removeCurrent() {
         current.remove();
-        QueryShardContext.removeTypes();
+        QueryParseContext.removeTypes();
     }
 
     public static SearchContext current() {
diff --git a/core/src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java b/core/src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java
index 8cd88df..e7941a4 100644
--- a/core/src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java
+++ b/core/src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java
@@ -42,6 +42,7 @@ import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
 import org.elasticsearch.index.fielddata.MultiGeoPointValues;
 import org.elasticsearch.index.fielddata.NumericDoubleValues;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
+import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.index.query.support.NestedInnerQueryParseSupport;
@@ -155,7 +156,7 @@ public class GeoDistanceSortParser implements SortParser {
             ObjectMapper objectMapper = context.mapperService().resolveClosestNestedObjectMapper(fieldName);
             if (objectMapper != null && objectMapper.nested().isNested()) {
                 if (nestedHelper == null) {
-                    nestedHelper = new NestedInnerQueryParseSupport(context.queryParserService().getShardContext());
+                    nestedHelper = new NestedInnerQueryParseSupport(context.queryParserService().getParseContext());
                 }
                 nestedHelper.setPath(objectMapper.fullPath());
             }
@@ -163,7 +164,7 @@ public class GeoDistanceSortParser implements SortParser {
 
         final Nested nested;
         if (nestedHelper != null && nestedHelper.getPath() != null) {
-
+            
             BitDocIdSetFilter rootDocumentsFilter = context.bitsetFilterCache().getBitDocIdSetFilter(Queries.newNonNestedFilter());
             Filter innerDocumentsFilter;
             if (nestedHelper.filterFound()) {
diff --git a/core/src/main/java/org/elasticsearch/search/sort/SortParseElement.java b/core/src/main/java/org/elasticsearch/search/sort/SortParseElement.java
index d248514..39da982 100644
--- a/core/src/main/java/org/elasticsearch/search/sort/SortParseElement.java
+++ b/core/src/main/java/org/elasticsearch/search/sort/SortParseElement.java
@@ -244,7 +244,7 @@ public class SortParseElement implements SearchParseElement {
                     ObjectMapper objectMapper = context.mapperService().resolveClosestNestedObjectMapper(fieldName);
                     if (objectMapper != null && objectMapper.nested().isNested()) {
                         if (nestedHelper == null) {
-                            nestedHelper = new NestedInnerQueryParseSupport(context.queryParserService().getShardContext());
+                            nestedHelper = new NestedInnerQueryParseSupport(context.queryParserService().getParseContext());
                         }
                         nestedHelper.setPath(objectMapper.fullPath());
                     }
diff --git a/core/src/main/java/org/elasticsearch/transport/netty/MessageChannelHandler.java b/core/src/main/java/org/elasticsearch/transport/netty/MessageChannelHandler.java
index df860ec..3bf4fa6 100644
--- a/core/src/main/java/org/elasticsearch/transport/netty/MessageChannelHandler.java
+++ b/core/src/main/java/org/elasticsearch/transport/netty/MessageChannelHandler.java
@@ -25,8 +25,6 @@ import org.elasticsearch.common.component.Lifecycle;
 import org.elasticsearch.common.compress.Compressor;
 import org.elasticsearch.common.compress.CompressorFactory;
 import org.elasticsearch.common.compress.NotCompressedException;
-import org.elasticsearch.common.io.stream.FilterStreamInput;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
 import org.elasticsearch.common.io.stream.NamedWriteableAwareStreamInput;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.logging.ESLogger;
@@ -52,19 +50,13 @@ public class MessageChannelHandler extends SimpleChannelUpstreamHandler {
     protected final TransportServiceAdapter transportServiceAdapter;
     protected final NettyTransport transport;
     protected final String profileName;
-    private final NamedWriteableRegistry namedWriteableRegistry;
 
     public MessageChannelHandler(NettyTransport transport, ESLogger logger, String profileName) {
-        this(transport, logger, profileName, new NamedWriteableRegistry());
-    }
-
-    public MessageChannelHandler(NettyTransport transport, ESLogger logger, String profileName, NamedWriteableRegistry namedWriteableRegistry) {
         this.threadPool = transport.threadPool();
         this.transportServiceAdapter = transport.transportServiceAdapter();
         this.transport = transport;
         this.logger = logger;
         this.profileName = profileName;
-        this.namedWriteableRegistry = namedWriteableRegistry;
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java b/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
index 7bb7322..0b85bb6 100644
--- a/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
+++ b/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
@@ -1001,7 +1001,7 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
     }
 
     public ChannelPipelineFactory configureServerChannelPipelineFactory(String name, Settings settings) {
-        return new ServerChannelPipelineFactory(this, name, settings, namedWriteableRegistry);
+        return new ServerChannelPipelineFactory(this, name, settings);
     }
 
     protected static class ServerChannelPipelineFactory implements ChannelPipelineFactory {
@@ -1009,13 +1009,11 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
         protected final NettyTransport nettyTransport;
         protected final String name;
         protected final Settings settings;
-        protected final NamedWriteableRegistry namedWriteableRegistry;
 
-        public ServerChannelPipelineFactory(NettyTransport nettyTransport, String name, Settings settings, NamedWriteableRegistry namedWriteableRegistry) {
+        public ServerChannelPipelineFactory(NettyTransport nettyTransport, String name, Settings settings) {
             this.nettyTransport = nettyTransport;
             this.name = name;
             this.settings = settings;
-            this.namedWriteableRegistry = namedWriteableRegistry;
         }
 
         @Override
@@ -1034,7 +1032,7 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
                 sizeHeader.setMaxCumulationBufferComponents(nettyTransport.maxCompositeBufferComponents);
             }
             channelPipeline.addLast("size", sizeHeader);
-            channelPipeline.addLast("dispatcher", new MessageChannelHandler(nettyTransport, nettyTransport.logger, name, namedWriteableRegistry));
+            channelPipeline.addLast("dispatcher", new MessageChannelHandler(nettyTransport, nettyTransport.logger, name));
             return channelPipeline;
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/ESExceptionTests.java b/core/src/test/java/org/elasticsearch/ESExceptionTests.java
index 80d847e..a4a6d28 100644
--- a/core/src/test/java/org/elasticsearch/ESExceptionTests.java
+++ b/core/src/test/java/org/elasticsearch/ESExceptionTests.java
@@ -35,7 +35,6 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentLocation;
 import org.elasticsearch.index.Index;
 import org.elasticsearch.index.IndexNotFoundException;
-import org.elasticsearch.index.query.QueryShardException;
 import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.query.TestQueryParsingException;
 import org.elasticsearch.rest.RestStatus;
@@ -308,7 +307,7 @@ public class ESExceptionTests extends ESTestCase {
                 new OutOfMemoryError("no memory left"),
                 new AlreadyClosedException("closed!!", new NullPointerException()),
                 new LockObtainFailedException("can't lock directory", new NullPointerException()),
-                new Throwable("this exception is unknown", new QueryShardException(new Index("foo"), 1, 2, "foobar", null) ), // somethin unknown
+                new Throwable("this exception is unknown", new QueryParsingException(new Index("foo"), 1, 2, "foobar", null) ), // somethin unknown
         };
         for (Throwable t : causes) {
             BytesStreamOutput out = new BytesStreamOutput();
diff --git a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
index e2b58bd..69d83b7 100644
--- a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
+++ b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
@@ -22,7 +22,6 @@ import com.fasterxml.jackson.core.JsonLocation;
 import com.fasterxml.jackson.core.JsonParseException;
 import com.google.common.collect.ImmutableSet;
 import com.google.common.collect.Sets;
-
 import org.codehaus.groovy.runtime.typehandling.GroovyCastException;
 import org.elasticsearch.action.FailedNodeException;
 import org.elasticsearch.action.RoutingMissingException;
@@ -32,24 +31,13 @@ import org.elasticsearch.action.search.ShardSearchFailure;
 import org.elasticsearch.cluster.block.ClusterBlockException;
 import org.elasticsearch.cluster.metadata.SnapshotId;
 import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.cluster.routing.IllegalShardRoutingStateException;
-import org.elasticsearch.cluster.routing.RoutingTableValidation;
-import org.elasticsearch.cluster.routing.RoutingValidationException;
-import org.elasticsearch.cluster.routing.ShardRouting;
-import org.elasticsearch.cluster.routing.ShardRoutingState;
-import org.elasticsearch.cluster.routing.TestShardRouting;
+import org.elasticsearch.cluster.routing.*;
 import org.elasticsearch.common.breaker.CircuitBreakingException;
 import org.elasticsearch.common.io.PathUtils;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.NotSerializableExceptionWrapper;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.io.stream.*;
 import org.elasticsearch.common.transport.LocalTransportAddress;
 import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentLocation;
+import org.elasticsearch.common.xcontent.*;
 import org.elasticsearch.discovery.DiscoverySettings;
 import org.elasticsearch.index.AlreadyExpiredException;
 import org.elasticsearch.index.Index;
@@ -58,10 +46,7 @@ import org.elasticsearch.index.engine.IndexFailedEngineException;
 import org.elasticsearch.index.engine.RecoveryEngineException;
 import org.elasticsearch.index.mapper.MergeMappingException;
 import org.elasticsearch.index.query.QueryParsingException;
-import org.elasticsearch.index.shard.IllegalIndexShardStateException;
-import org.elasticsearch.index.shard.IndexShardState;
-import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.index.shard.TranslogRecoveryPerformer;
+import org.elasticsearch.index.shard.*;
 import org.elasticsearch.indices.IndexTemplateAlreadyExistsException;
 import org.elasticsearch.indices.IndexTemplateMissingException;
 import org.elasticsearch.indices.InvalidIndexTemplateException;
diff --git a/core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java b/core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java
index 80f4c45..8f8759e 100644
--- a/core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java
+++ b/core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java
@@ -151,7 +151,7 @@ public class IndexAliasesIT extends ESIntegTestCase {
         logger.info("--> making sure that filter was stored with alias [alias1] and filter [user:kimchy]");
         ClusterState clusterState = admin().cluster().prepareState().get().getState();
         IndexMetaData indexMd = clusterState.metaData().index("test");
-        assertThat(indexMd.aliases().get("alias1").filter().string(), equalTo("{\"term\":{\"user\":{\"value\":\"kimchy\",\"boost\":1.0}}}"));
+        assertThat(indexMd.aliases().get("alias1").filter().string(), equalTo("{\"term\":{\"user\":\"kimchy\"}}"));
 
     }
 
@@ -413,8 +413,8 @@ public class IndexAliasesIT extends ESIntegTestCase {
         assertThat(client().prepareCount("bars").setQuery(QueryBuilders.matchAllQuery()).get().getCount(), equalTo(1L));
     }
 
-
-
+    
+    
     @Test
     public void testDeleteAliases() throws Exception {
         logger.info("--> creating index [test1] and [test2]");
@@ -434,17 +434,17 @@ public class IndexAliasesIT extends ESIntegTestCase {
                 .addAlias("test2", "aliasToTests")
                 .addAlias("test2", "foos", termQuery("name", "foo"))
                 .addAlias("test2", "tests", termQuery("name", "test")));
-
-        String[] indices = {"test1", "test2"};
+        
+        String[] indices = {"test1", "test2"}; 
         String[] aliases = {"aliasToTest1", "foos", "bars", "tests", "aliasToTest2", "aliasToTests"};
-
+        
         admin().indices().prepareAliases().removeAlias(indices, aliases).get();
-
+        
         AliasesExistResponse response = admin().indices().prepareAliasesExist(aliases).get();
         assertThat(response.exists(), equalTo(false));
     }
 
-
+    
     @Test
     public void testWaitForAliasCreationMultipleShards() throws Exception {
         logger.info("--> creating index [test]");
@@ -532,16 +532,16 @@ public class IndexAliasesIT extends ESIntegTestCase {
 
         logger.info("--> verify that filter was updated");
         AliasMetaData aliasMetaData = ((AliasOrIndex.Alias) internalCluster().clusterService().state().metaData().getAliasAndIndexLookup().get("alias1")).getFirstAliasMetaData();
-        assertThat(aliasMetaData.getFilter().toString(), equalTo("{\"term\":{\"name\":{\"value\":\"bar\",\"boost\":1.0}}}"));
+        assertThat(aliasMetaData.getFilter().toString(), equalTo("{\"term\":{\"name\":\"bar\"}}"));
 
         logger.info("--> deleting alias1");
         stopWatch.start();
         assertAcked((admin().indices().prepareAliases().removeAlias("test", "alias1").setTimeout(timeout)));
         assertThat(stopWatch.stop().lastTaskTime().millis(), lessThan(timeout.millis()));
 
-
+        
     }
-
+    
     @Test(expected = AliasesNotFoundException.class)
     public void testIndicesRemoveNonExistingAliasResponds404() throws Exception {
         logger.info("--> creating index [test]");
diff --git a/core/src/test/java/org/elasticsearch/benchmark/cluster/ClusterAllocationRerouteBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/cluster/ClusterAllocationRerouteBenchmark.java
index ef7afbe..9ec74c9 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/cluster/ClusterAllocationRerouteBenchmark.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/cluster/ClusterAllocationRerouteBenchmark.java
@@ -73,9 +73,9 @@ public class ClusterAllocationRerouteBenchmark {
             logger.info("[{}] starting... ", i);
             long runStart = System.currentTimeMillis();
             ClusterState clusterState = initialClusterState;
-            while (clusterState.readOnlyRoutingNodes().hasUnassignedShards()) {
-                logger.info("[{}] remaining unassigned {}", i, clusterState.readOnlyRoutingNodes().unassigned().size());
-                RoutingAllocation.Result result = strategy.applyStartedShards(clusterState, clusterState.readOnlyRoutingNodes().shardsWithState(INITIALIZING));
+            while (clusterState.getRoutingNodes().hasUnassignedShards()) {
+                logger.info("[{}] remaining unassigned {}", i, clusterState.getRoutingNodes().unassigned().size());
+                RoutingAllocation.Result result = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));
                 clusterState = ClusterState.builder(clusterState).routingResult(result).build();
                 result = strategy.reroute(clusterState);
                 clusterState = ClusterState.builder(clusterState).routingResult(result).build();
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript1.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript1.java
index d4b7a0d..6f666b3 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript1.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript1.java
@@ -34,6 +34,11 @@ public class NativeScript1 extends AbstractSearchScript {
         public ExecutableScript newScript(@Nullable Map<String, Object> params) {
             return new NativeScript1();
         }
+
+        @Override
+        public boolean needsScores() {
+            return false;
+        }
     }
 
     public static final String NATIVE_SCRIPT_1 = "native_1";
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript2.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript2.java
index acb374b..585d7a5 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript2.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript2.java
@@ -34,6 +34,11 @@ public class NativeScript2 extends AbstractSearchScript {
         public ExecutableScript newScript(@Nullable Map<String, Object> params) {
             return new NativeScript2();
         }
+
+        @Override
+        public boolean needsScores() {
+            return false;
+        }
     }
 
     public static final String NATIVE_SCRIPT_2 = "native_2";
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript3.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript3.java
index b57cde7..c2d50fe 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript3.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript3.java
@@ -34,6 +34,11 @@ public class NativeScript3 extends AbstractSearchScript {
         public ExecutableScript newScript(@Nullable Map<String, Object> params) {
             return new NativeScript3();
         }
+
+        @Override
+        public boolean needsScores() {
+            return false;
+        }
     }
 
     public static final String NATIVE_SCRIPT_3 = "native_3";
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript4.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript4.java
index d87d1de..2bda86e 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript4.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript4.java
@@ -34,6 +34,11 @@ public class NativeScript4 extends AbstractSearchScript {
         public ExecutableScript newScript(@Nullable Map<String, Object> params) {
             return new NativeScript4();
         }
+
+        @Override
+        public boolean needsScores() {
+            return false;
+        }
     }
 
     public static final String NATIVE_SCRIPT_4 = "native_4";
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeConstantForLoopScoreScript.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeConstantForLoopScoreScript.java
index c61a40d..fee0a7e 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeConstantForLoopScoreScript.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeConstantForLoopScoreScript.java
@@ -36,6 +36,11 @@ public class NativeConstantForLoopScoreScript extends AbstractSearchScript {
         public ExecutableScript newScript(@Nullable Map<String, Object> params) {
             return new NativeConstantForLoopScoreScript(params);
         }
+
+        @Override
+        public boolean needsScores() {
+            return false;
+        }
     }
 
     private NativeConstantForLoopScoreScript(Map<String, Object> params) {
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeConstantScoreScript.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeConstantScoreScript.java
index 6d07242..17220cd 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeConstantScoreScript.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeConstantScoreScript.java
@@ -36,6 +36,11 @@ public class NativeConstantScoreScript extends AbstractSearchScript {
         public ExecutableScript newScript(@Nullable Map<String, Object> params) {
             return new NativeConstantScoreScript();
         }
+
+        @Override
+        public boolean needsScores() {
+            return false;
+        }
     }
 
     private NativeConstantScoreScript() {
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeNaiveTFIDFScoreScript.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeNaiveTFIDFScoreScript.java
index 1f88e66..9d6a1cd 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeNaiveTFIDFScoreScript.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeNaiveTFIDFScoreScript.java
@@ -42,6 +42,11 @@ public class NativeNaiveTFIDFScoreScript extends AbstractSearchScript {
         public ExecutableScript newScript(@Nullable Map<String, Object> params) {
             return new NativeNaiveTFIDFScoreScript(params);
         }
+
+        @Override
+        public boolean needsScores() {
+            return false;
+        }
     }
 
     private NativeNaiveTFIDFScoreScript(Map<String, Object> params) {
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativePayloadSumNoRecordScoreScript.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativePayloadSumNoRecordScoreScript.java
index 825b31e..7570426 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativePayloadSumNoRecordScoreScript.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativePayloadSumNoRecordScoreScript.java
@@ -44,6 +44,11 @@ public class NativePayloadSumNoRecordScoreScript extends AbstractSearchScript {
         public ExecutableScript newScript(@Nullable Map<String, Object> params) {
             return new NativePayloadSumNoRecordScoreScript(params);
         }
+
+        @Override
+        public boolean needsScores() {
+            return false;
+        }
     }
 
     private NativePayloadSumNoRecordScoreScript(Map<String, Object> params) {
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativePayloadSumScoreScript.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativePayloadSumScoreScript.java
index 0172c56..1522b3a 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativePayloadSumScoreScript.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativePayloadSumScoreScript.java
@@ -44,6 +44,11 @@ public class NativePayloadSumScoreScript extends AbstractSearchScript {
         public ExecutableScript newScript(@Nullable Map<String, Object> params) {
             return new NativePayloadSumScoreScript(params);
         }
+
+        @Override
+        public boolean needsScores() {
+            return false;
+        }
     }
 
     private NativePayloadSumScoreScript(Map<String, Object> params) {
diff --git a/core/src/test/java/org/elasticsearch/cluster/ack/AckClusterUpdateSettingsIT.java b/core/src/test/java/org/elasticsearch/cluster/ack/AckClusterUpdateSettingsIT.java
index 9e3a112..d6e4abb 100644
--- a/core/src/test/java/org/elasticsearch/cluster/ack/AckClusterUpdateSettingsIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/ack/AckClusterUpdateSettingsIT.java
@@ -96,7 +96,7 @@ public class AckClusterUpdateSettingsIT extends ESIntegTestCase {
 
         for (Client client : clients()) {
             ClusterState clusterState = getLocalClusterState(client);
-            assertThat(clusterState.routingNodes().metaData().transientSettings().get("cluster.routing.allocation.exclude._id"), equalTo(excludedNodeId));
+            assertThat(clusterState.getRoutingNodes().metaData().transientSettings().get("cluster.routing.allocation.exclude._id"), equalTo(excludedNodeId));
             for (IndexRoutingTable indexRoutingTable : clusterState.routingTable()) {
                 for (IndexShardRoutingTable indexShardRoutingTable : indexRoutingTable) {
                     for (ShardRouting shardRouting : indexShardRoutingTable) {
diff --git a/core/src/test/java/org/elasticsearch/cluster/ack/AckIT.java b/core/src/test/java/org/elasticsearch/cluster/ack/AckIT.java
index de05f40..84b31e3 100644
--- a/core/src/test/java/org/elasticsearch/cluster/ack/AckIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/ack/AckIT.java
@@ -190,7 +190,7 @@ public class AckIT extends ESIntegTestCase {
 
         for (Client client : clients()) {
             ClusterState clusterState = getLocalClusterState(client);
-            for (ShardRouting shardRouting : clusterState.routingNodes().routingNodeIter(moveAllocationCommand.fromNode())) {
+            for (ShardRouting shardRouting : clusterState.getRoutingNodes().routingNodeIter(moveAllocationCommand.fromNode())) {
                 //if the shard that we wanted to move is still on the same node, it must be relocating
                 if (shardRouting.shardId().equals(moveAllocationCommand.shardId())) {
                     assertThat(shardRouting.relocating(), equalTo(true));
@@ -199,7 +199,7 @@ public class AckIT extends ESIntegTestCase {
             }
 
             boolean found = false;
-            for (ShardRouting shardRouting : clusterState.routingNodes().routingNodeIter(moveAllocationCommand.toNode())) {
+            for (ShardRouting shardRouting : clusterState.getRoutingNodes().routingNodeIter(moveAllocationCommand.toNode())) {
                 if (shardRouting.shardId().equals(moveAllocationCommand.shardId())) {
                     assertThat(shardRouting.state(), anyOf(equalTo(ShardRoutingState.INITIALIZING), equalTo(ShardRoutingState.STARTED)));
                     found = true;
@@ -240,7 +240,7 @@ public class AckIT extends ESIntegTestCase {
         //all nodes hold the same cluster state version. We only know there was no need to change anything, thus no need for ack on this update.
         ClusterStateResponse clusterStateResponse = client().admin().cluster().prepareState().get();
         boolean found = false;
-        for (ShardRouting shardRouting : clusterStateResponse.getState().routingNodes().routingNodeIter(moveAllocationCommand.fromNode())) {
+        for (ShardRouting shardRouting : clusterStateResponse.getState().getRoutingNodes().routingNodeIter(moveAllocationCommand.fromNode())) {
             //the shard that we wanted to move is still on the same node, as we had dryRun flag
             if (shardRouting.shardId().equals(moveAllocationCommand.shardId())) {
                 assertThat(shardRouting.started(), equalTo(true));
@@ -250,7 +250,7 @@ public class AckIT extends ESIntegTestCase {
         }
         assertThat(found, equalTo(true));
 
-        for (ShardRouting shardRouting : clusterStateResponse.getState().routingNodes().routingNodeIter(moveAllocationCommand.toNode())) {
+        for (ShardRouting shardRouting : clusterStateResponse.getState().getRoutingNodes().routingNodeIter(moveAllocationCommand.toNode())) {
             if (shardRouting.shardId().equals(moveAllocationCommand.shardId())) {
                 fail("shard [" + shardRouting + "] shouldn't be on node [" + moveAllocationCommand.toString() + "]");
             }
@@ -277,7 +277,7 @@ public class AckIT extends ESIntegTestCase {
         String toNodeId = null;
         ShardRouting shardToBeMoved = null;
         ClusterStateResponse clusterStateResponse = client().admin().cluster().prepareState().get();
-        for (RoutingNode routingNode : clusterStateResponse.getState().routingNodes()) {
+        for (RoutingNode routingNode : clusterStateResponse.getState().getRoutingNodes()) {
             if (routingNode.node().isDataNode()) {
                 if (fromNodeId == null && routingNode.numberOfOwningShards() > 0) {
                     fromNodeId = routingNode.nodeId();
diff --git a/core/src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteIT.java b/core/src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteIT.java
index c9643d8..c623552 100644
--- a/core/src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteIT.java
@@ -34,15 +34,18 @@ import org.elasticsearch.cluster.routing.allocation.command.MoveAllocationComman
 import org.elasticsearch.cluster.routing.allocation.decider.Decision;
 import org.elasticsearch.cluster.routing.allocation.decider.DisableAllocationDecider;
 import org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider.Allocation;
+import org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider;
 import org.elasticsearch.common.Priority;
 import org.elasticsearch.common.io.FileSystemUtils;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.env.NodeEnvironment;
 import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
+import org.elasticsearch.test.InternalTestCluster;
 import org.junit.Test;
 
 import java.nio.file.Path;
@@ -93,7 +96,7 @@ public class ClusterRerouteIT extends ESIntegTestCase {
                 .execute().actionGet();
 
         ClusterState state = client().admin().cluster().prepareState().execute().actionGet().getState();
-        assertThat(state.routingNodes().unassigned().size(), equalTo(2));
+        assertThat(state.getRoutingNodes().unassigned().size(), equalTo(2));
 
         logger.info("--> explicitly allocate shard 1, *under dry_run*");
         state = client().admin().cluster().prepareReroute()
@@ -101,28 +104,28 @@ public class ClusterRerouteIT extends ESIntegTestCase {
                 .add(new AllocateAllocationCommand(new ShardId("test", 0), node_1, true))
                 .setDryRun(true)
                 .execute().actionGet().getState();
-        assertThat(state.routingNodes().unassigned().size(), equalTo(1));
-        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_1).id()).get(0).state(), equalTo(ShardRoutingState.INITIALIZING));
+        assertThat(state.getRoutingNodes().unassigned().size(), equalTo(1));
+        assertThat(state.getRoutingNodes().node(state.nodes().resolveNode(node_1).id()).get(0).state(), equalTo(ShardRoutingState.INITIALIZING));
 
         logger.info("--> get the state, verify nothing changed because of the dry run");
         state = client().admin().cluster().prepareState().execute().actionGet().getState();
-        assertThat(state.routingNodes().unassigned().size(), equalTo(2));
+        assertThat(state.getRoutingNodes().unassigned().size(), equalTo(2));
 
         logger.info("--> explicitly allocate shard 1, actually allocating, no dry run");
         state = client().admin().cluster().prepareReroute()
                 .setExplain(randomBoolean())
                 .add(new AllocateAllocationCommand(new ShardId("test", 0), node_1, true))
                 .execute().actionGet().getState();
-        assertThat(state.routingNodes().unassigned().size(), equalTo(1));
-        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_1).id()).get(0).state(), equalTo(ShardRoutingState.INITIALIZING));
+        assertThat(state.getRoutingNodes().unassigned().size(), equalTo(1));
+        assertThat(state.getRoutingNodes().node(state.nodes().resolveNode(node_1).id()).get(0).state(), equalTo(ShardRoutingState.INITIALIZING));
 
         ClusterHealthResponse healthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForYellowStatus().execute().actionGet();
         assertThat(healthResponse.isTimedOut(), equalTo(false));
 
         logger.info("--> get the state, verify shard 1 primary allocated");
         state = client().admin().cluster().prepareState().execute().actionGet().getState();
-        assertThat(state.routingNodes().unassigned().size(), equalTo(1));
-        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_1).id()).get(0).state(), equalTo(ShardRoutingState.STARTED));
+        assertThat(state.getRoutingNodes().unassigned().size(), equalTo(1));
+        assertThat(state.getRoutingNodes().node(state.nodes().resolveNode(node_1).id()).get(0).state(), equalTo(ShardRoutingState.STARTED));
 
         logger.info("--> move shard 1 primary from node1 to node2");
         state = client().admin().cluster().prepareReroute()
@@ -130,8 +133,8 @@ public class ClusterRerouteIT extends ESIntegTestCase {
                 .add(new MoveAllocationCommand(new ShardId("test", 0), node_1, node_2))
                 .execute().actionGet().getState();
 
-        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_1).id()).get(0).state(), equalTo(ShardRoutingState.RELOCATING));
-        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_2).id()).get(0).state(), equalTo(ShardRoutingState.INITIALIZING));
+        assertThat(state.getRoutingNodes().node(state.nodes().resolveNode(node_1).id()).get(0).state(), equalTo(ShardRoutingState.RELOCATING));
+        assertThat(state.getRoutingNodes().node(state.nodes().resolveNode(node_2).id()).get(0).state(), equalTo(ShardRoutingState.INITIALIZING));
 
 
         healthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForYellowStatus().setWaitForRelocatingShards(0).execute().actionGet();
@@ -139,8 +142,8 @@ public class ClusterRerouteIT extends ESIntegTestCase {
 
         logger.info("--> get the state, verify shard 1 primary moved from node1 to node2");
         state = client().admin().cluster().prepareState().execute().actionGet().getState();
-        assertThat(state.routingNodes().unassigned().size(), equalTo(1));
-        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_2).id()).get(0).state(), equalTo(ShardRoutingState.STARTED));
+        assertThat(state.getRoutingNodes().unassigned().size(), equalTo(1));
+        assertThat(state.getRoutingNodes().node(state.nodes().resolveNode(node_2).id()).get(0).state(), equalTo(ShardRoutingState.STARTED));
     }
 
     @Test
@@ -160,6 +163,40 @@ public class ClusterRerouteIT extends ESIntegTestCase {
         rerouteWithAllocateLocalGateway(commonSettings);
     }
 
+    @Test
+    public void testDelayWithALargeAmountOfShards() throws Exception {
+        Settings commonSettings = settingsBuilder()
+                .put("gateway.type", "local")
+                .put(ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_CONCURRENT_RECOVERIES, 1)
+                .build();
+        logger.info("--> starting 4 nodes");
+        String node_1 = internalCluster().startNode(commonSettings);
+        internalCluster().startNode(commonSettings);
+        internalCluster().startNode(commonSettings);
+        internalCluster().startNode(commonSettings);
+
+        assertThat(cluster().size(), equalTo(4));
+        ClusterHealthResponse healthResponse = client().admin().cluster().prepareHealth().setWaitForNodes("4").execute().actionGet();
+        assertThat(healthResponse.isTimedOut(), equalTo(false));
+
+        logger.info("--> create indices");
+        for (int i = 0; i < 25; i++) {
+            client().admin().indices().prepareCreate("test" + i)
+                    .setSettings(settingsBuilder()
+                            .put("index.number_of_shards", 5).put("index.number_of_replicas", 1)
+                            .put("index.unassigned.node_left.delayed_timeout", randomIntBetween(250, 1000) + "ms"))
+                    .execute().actionGet();
+        }
+
+        ensureGreen(TimeValue.timeValueMinutes(1));
+
+        logger.info("--> stopping node1");
+        internalCluster().stopRandomNode(InternalTestCluster.nameFilter(node_1));
+
+        // This might run slowly on older hardware
+        ensureGreen(TimeValue.timeValueMinutes(2));
+    }
+
     private void rerouteWithAllocateLocalGateway(Settings commonSettings) throws Exception {
         logger.info("--> starting 2 nodes");
         String node_1 = internalCluster().startNode(commonSettings);
@@ -174,23 +211,23 @@ public class ClusterRerouteIT extends ESIntegTestCase {
                 .execute().actionGet();
 
         ClusterState state = client().admin().cluster().prepareState().execute().actionGet().getState();
-        assertThat(state.routingNodes().unassigned().size(), equalTo(2));
+        assertThat(state.getRoutingNodes().unassigned().size(), equalTo(2));
 
         logger.info("--> explicitly allocate shard 1, actually allocating, no dry run");
         state = client().admin().cluster().prepareReroute()
                 .setExplain(randomBoolean())
                 .add(new AllocateAllocationCommand(new ShardId("test", 0), node_1, true))
                 .execute().actionGet().getState();
-        assertThat(state.routingNodes().unassigned().size(), equalTo(1));
-        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_1).id()).get(0).state(), equalTo(ShardRoutingState.INITIALIZING));
+        assertThat(state.getRoutingNodes().unassigned().size(), equalTo(1));
+        assertThat(state.getRoutingNodes().node(state.nodes().resolveNode(node_1).id()).get(0).state(), equalTo(ShardRoutingState.INITIALIZING));
 
         healthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForYellowStatus().execute().actionGet();
         assertThat(healthResponse.isTimedOut(), equalTo(false));
 
         logger.info("--> get the state, verify shard 1 primary allocated");
         state = client().admin().cluster().prepareState().execute().actionGet().getState();
-        assertThat(state.routingNodes().unassigned().size(), equalTo(1));
-        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_1).id()).get(0).state(), equalTo(ShardRoutingState.STARTED));
+        assertThat(state.getRoutingNodes().unassigned().size(), equalTo(1));
+        assertThat(state.getRoutingNodes().node(state.nodes().resolveNode(node_1).id()).get(0).state(), equalTo(ShardRoutingState.STARTED));
 
         client().prepareIndex("test", "type", "1").setSource("field", "value").setRefresh(true).execute().actionGet();
 
@@ -215,16 +252,16 @@ public class ClusterRerouteIT extends ESIntegTestCase {
                 .setExplain(randomBoolean())
                 .add(new AllocateAllocationCommand(new ShardId("test", 0), node_1, true))
                 .execute().actionGet().getState();
-        assertThat(state.routingNodes().unassigned().size(), equalTo(1));
-        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_1).id()).get(0).state(), equalTo(ShardRoutingState.INITIALIZING));
+        assertThat(state.getRoutingNodes().unassigned().size(), equalTo(1));
+        assertThat(state.getRoutingNodes().node(state.nodes().resolveNode(node_1).id()).get(0).state(), equalTo(ShardRoutingState.INITIALIZING));
 
         healthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForYellowStatus().execute().actionGet();
         assertThat(healthResponse.isTimedOut(), equalTo(false));
 
         logger.info("--> get the state, verify shard 1 primary allocated");
         state = client().admin().cluster().prepareState().execute().actionGet().getState();
-        assertThat(state.routingNodes().unassigned().size(), equalTo(1));
-        assertThat(state.routingNodes().node(state.nodes().resolveNode(node_1).id()).get(0).state(), equalTo(ShardRoutingState.STARTED));
+        assertThat(state.getRoutingNodes().unassigned().size(), equalTo(1));
+        assertThat(state.getRoutingNodes().node(state.nodes().resolveNode(node_1).id()).get(0).state(), equalTo(ShardRoutingState.STARTED));
 
     }
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/allocation/SimpleAllocationIT.java b/core/src/test/java/org/elasticsearch/cluster/allocation/SimpleAllocationIT.java
index 5b07daa..27b221d 100644
--- a/core/src/test/java/org/elasticsearch/cluster/allocation/SimpleAllocationIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/allocation/SimpleAllocationIT.java
@@ -50,8 +50,8 @@ public class SimpleAllocationIT extends ESIntegTestCase {
         ensureGreen();
 
         ClusterState state = client().admin().cluster().prepareState().execute().actionGet().getState();
-        assertThat(state.routingNodes().unassigned().size(), equalTo(0));
-        for (RoutingNode node : state.routingNodes()) {
+        assertThat(state.getRoutingNodes().unassigned().size(), equalTo(0));
+        for (RoutingNode node : state.getRoutingNodes()) {
             if (!node.isEmpty()) {
                 assertThat(node.size(), equalTo(2));
             }
@@ -60,8 +60,8 @@ public class SimpleAllocationIT extends ESIntegTestCase {
         ensureGreen();
         state = client().admin().cluster().prepareState().execute().actionGet().getState();
 
-        assertThat(state.routingNodes().unassigned().size(), equalTo(0));
-        for (RoutingNode node : state.routingNodes()) {
+        assertThat(state.getRoutingNodes().unassigned().size(), equalTo(0));
+        for (RoutingNode node : state.getRoutingNodes()) {
             if (!node.isEmpty()) {
                 assertThat(node.size(), equalTo(1));
             }
@@ -75,8 +75,8 @@ public class SimpleAllocationIT extends ESIntegTestCase {
         ensureGreen();
         state = client().admin().cluster().prepareState().execute().actionGet().getState();
 
-        assertThat(state.routingNodes().unassigned().size(), equalTo(0));
-        for (RoutingNode node : state.routingNodes()) {
+        assertThat(state.getRoutingNodes().unassigned().size(), equalTo(0));
+        for (RoutingNode node : state.getRoutingNodes()) {
             if (!node.isEmpty()) {
                 assertThat(node.size(), equalTo(4));
             }
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/DelayedAllocationIT.java b/core/src/test/java/org/elasticsearch/cluster/routing/DelayedAllocationIT.java
index 26ba7e1..c6e9f09 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/DelayedAllocationIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/DelayedAllocationIT.java
@@ -76,7 +76,7 @@ public class DelayedAllocationIT extends ESIntegTestCase {
         assertBusy(new Runnable() {
             @Override
             public void run() {
-                assertThat(client().admin().cluster().prepareState().all().get().getState().routingNodes().hasUnassigned(), equalTo(true));
+                assertThat(client().admin().cluster().prepareState().all().get().getState().getRoutingNodes().hasUnassigned(), equalTo(true));
             }
         });
         assertThat(client().admin().cluster().prepareHealth().get().getDelayedUnassignedShards(), equalTo(1));
@@ -124,7 +124,7 @@ public class DelayedAllocationIT extends ESIntegTestCase {
         assertBusy(new Runnable() {
             @Override
             public void run() {
-                assertThat(client().admin().cluster().prepareState().all().get().getState().routingNodes().hasUnassigned(), equalTo(true));
+                assertThat(client().admin().cluster().prepareState().all().get().getState().getRoutingNodes().hasUnassigned(), equalTo(true));
             }
         });
         assertThat(client().admin().cluster().prepareHealth().get().getDelayedUnassignedShards(), equalTo(1));
@@ -151,7 +151,7 @@ public class DelayedAllocationIT extends ESIntegTestCase {
         assertBusy(new Runnable() {
             @Override
             public void run() {
-                assertThat(client().admin().cluster().prepareState().all().get().getState().routingNodes().hasUnassigned(), equalTo(true));
+                assertThat(client().admin().cluster().prepareState().all().get().getState().getRoutingNodes().hasUnassigned(), equalTo(true));
             }
         });
         assertThat(client().admin().cluster().prepareHealth().get().getDelayedUnassignedShards(), equalTo(1));
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingServiceTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingServiceTests.java
index 94c9f19..6b2e972 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingServiceTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingServiceTests.java
@@ -28,6 +28,7 @@ import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.cluster.routing.allocation.AllocationService;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.test.ESAllocationTestCase;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.junit.After;
@@ -75,10 +76,10 @@ public class RoutingServiceTests extends ESAllocationTestCase {
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1")).put(newNode("node2")).localNodeId("node1").masterNodeId("node1")).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
         // starting primaries
-        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING))).build();
+        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING))).build();
         // starting replicas
-        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING))).build();
-        assertThat(clusterState.routingNodes().hasUnassigned(), equalTo(false));
+        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING))).build();
+        assertThat(clusterState.getRoutingNodes().hasUnassigned(), equalTo(false));
         // remove node2 and reroute
         ClusterState prevState = clusterState;
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes()).remove("node2")).build();
@@ -104,14 +105,18 @@ public class RoutingServiceTests extends ESAllocationTestCase {
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1")).put(newNode("node2")).localNodeId("node1").masterNodeId("node1")).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
         // starting primaries
-        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING))).build();
+        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING))).build();
         // starting replicas
-        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING))).build();
-        assertThat(clusterState.routingNodes().hasUnassigned(), equalTo(false));
+        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING))).build();
+        assertThat(clusterState.getRoutingNodes().hasUnassigned(), equalTo(false));
         // remove node2 and reroute
         ClusterState prevState = clusterState;
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes()).remove("node2")).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
+        // We need to update the routing service's last attempted run to
+        // signal that the GatewayAllocator tried to allocated it but
+        // it was delayed
+        routingService.setUnassignedShardsAllocatedTimestamp(System.currentTimeMillis());
         ClusterState newState = clusterState;
 
         routingService.clusterChanged(new ClusterChangedEvent("test", newState, prevState));
@@ -125,6 +130,44 @@ public class RoutingServiceTests extends ESAllocationTestCase {
         assertThat(routingService.getRegisteredNextDelaySetting(), equalTo(Long.MAX_VALUE));
     }
 
+    @Test
+    public void testDelayedUnassignedDoesNotRerouteForNegativeDelays() throws Exception {
+        AllocationService allocation = createAllocationService();
+        MetaData metaData = MetaData.builder()
+                .put(IndexMetaData.builder("test").settings(settings(Version.CURRENT).put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING, "100ms"))
+                        .numberOfShards(1).numberOfReplicas(1))
+                .build();
+        ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
+                .metaData(metaData)
+                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test"))).build();
+        clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1")).put(newNode("node2")).localNodeId("node1").masterNodeId("node1")).build();
+        clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
+        // starting primaries
+        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING))).build();
+        // starting replicas
+        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING))).build();
+        assertThat(clusterState.getRoutingNodes().hasUnassigned(), equalTo(false));
+        // remove node2 and reroute
+        ClusterState prevState = clusterState;
+        clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes()).remove("node2")).build();
+        clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
+        // Set it in the future so the delay will be negative
+        routingService.setUnassignedShardsAllocatedTimestamp(System.currentTimeMillis() + TimeValue.timeValueMinutes(1).millis());
+
+        ClusterState newState = clusterState;
+
+        routingService.clusterChanged(new ClusterChangedEvent("test", newState, prevState));
+        assertBusy(new Runnable() {
+            @Override
+            public void run() {
+                assertThat(routingService.hasReroutedAndClear(), equalTo(false));
+
+                // verify the registration has been updated
+                assertThat(routingService.getRegisteredNextDelaySetting(), equalTo(100L));
+            }
+        });
+    }
+
     private class TestRoutingService extends RoutingService {
 
         private AtomicBoolean rerouted = new AtomicBoolean();
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingTableTest.java b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingTableTest.java
index 0aa9c4c..2cbc364 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingTableTest.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingTableTest.java
@@ -95,7 +95,7 @@ public class RoutingTableTest extends ESAllocationTestCase {
     private void startInitializingShards(String index) {
         this.clusterState = ClusterState.builder(clusterState).routingTable(this.testRoutingTable).build();
         logger.info("start primary shards for index " + index);
-        RoutingAllocation.Result rerouteResult = ALLOCATION_SERVICE.applyStartedShards(this.clusterState, this.clusterState.routingNodes().shardsWithState(index, INITIALIZING));
+        RoutingAllocation.Result rerouteResult = ALLOCATION_SERVICE.applyStartedShards(this.clusterState, this.clusterState.getRoutingNodes().shardsWithState(index, INITIALIZING));
         this.clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
         this.testRoutingTable = rerouteResult.routingTable();
     }
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java
index 21c86c8..657d33f 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java
@@ -90,7 +90,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
                 .routingTable(RoutingTable.builder().addAsNew(metaData.index("test"))).build();
-        for (ShardRouting shard : clusterState.routingNodes().shardsWithState(UNASSIGNED)) {
+        for (ShardRouting shard : clusterState.getRoutingNodes().shardsWithState(UNASSIGNED)) {
             assertThat(shard.unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.INDEX_CREATED));
         }
     }
@@ -103,7 +103,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
                 .routingTable(RoutingTable.builder().addAsRecovery(metaData.index("test"))).build();
-        for (ShardRouting shard : clusterState.routingNodes().shardsWithState(UNASSIGNED)) {
+        for (ShardRouting shard : clusterState.getRoutingNodes().shardsWithState(UNASSIGNED)) {
             assertThat(shard.unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.CLUSTER_RECOVERED));
         }
     }
@@ -116,7 +116,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
                 .routingTable(RoutingTable.builder().addAsFromCloseToOpen(metaData.index("test"))).build();
-        for (ShardRouting shard : clusterState.routingNodes().shardsWithState(UNASSIGNED)) {
+        for (ShardRouting shard : clusterState.getRoutingNodes().shardsWithState(UNASSIGNED)) {
             assertThat(shard.unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.INDEX_REOPENED));
         }
     }
@@ -129,7 +129,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
                 .routingTable(RoutingTable.builder().addAsNewRestore(metaData.index("test"), new RestoreSource(new SnapshotId("rep1", "snp1"), Version.CURRENT, "test"), new IntHashSet())).build();
-        for (ShardRouting shard : clusterState.routingNodes().shardsWithState(UNASSIGNED)) {
+        for (ShardRouting shard : clusterState.getRoutingNodes().shardsWithState(UNASSIGNED)) {
             assertThat(shard.unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.NEW_INDEX_RESTORED));
         }
     }
@@ -142,7 +142,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
                 .routingTable(RoutingTable.builder().addAsRestore(metaData.index("test"), new RestoreSource(new SnapshotId("rep1", "snp1"), Version.CURRENT, "test"))).build();
-        for (ShardRouting shard : clusterState.routingNodes().shardsWithState(UNASSIGNED)) {
+        for (ShardRouting shard : clusterState.getRoutingNodes().shardsWithState(UNASSIGNED)) {
             assertThat(shard.unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.EXISTING_INDEX_RESTORED));
         }
     }
@@ -155,7 +155,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
                 .routingTable(RoutingTable.builder().addAsFromDangling(metaData.index("test"))).build();
-        for (ShardRouting shard : clusterState.routingNodes().shardsWithState(UNASSIGNED)) {
+        for (ShardRouting shard : clusterState.getRoutingNodes().shardsWithState(UNASSIGNED)) {
             assertThat(shard.unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.DANGLING_INDEX_IMPORTED));
         }
     }
@@ -172,16 +172,16 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1"))).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
         // starting primaries
-        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING))).build();
+        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING))).build();
         IndexRoutingTable.Builder builder = IndexRoutingTable.builder("test");
         for (IndexShardRoutingTable indexShardRoutingTable : clusterState.routingTable().index("test")) {
             builder.addIndexShard(indexShardRoutingTable);
         }
         builder.addReplica();
         clusterState = ClusterState.builder(clusterState).routingTable(RoutingTable.builder(clusterState.routingTable()).add(builder)).build();
-        assertThat(clusterState.routingNodes().shardsWithState(UNASSIGNED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo(), notNullValue());
-        assertThat(clusterState.routingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.REPLICA_ADDED));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo(), notNullValue());
+        assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.REPLICA_ADDED));
     }
 
     /**
@@ -215,19 +215,19 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1")).put(newNode("node2"))).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
         // starting primaries
-        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING))).build();
+        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING))).build();
         // starting replicas
-        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING))).build();
-        assertThat(clusterState.routingNodes().hasUnassigned(), equalTo(false));
+        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING))).build();
+        assertThat(clusterState.getRoutingNodes().hasUnassigned(), equalTo(false));
         // remove node2 and reroute
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes()).remove("node2")).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
         // verify that NODE_LEAVE is the reason for meta
-        assertThat(clusterState.routingNodes().hasUnassigned(), equalTo(true));
-        assertThat(clusterState.routingNodes().shardsWithState(UNASSIGNED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo(), notNullValue());
-        assertThat(clusterState.routingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.NODE_LEFT));
-        assertThat(clusterState.routingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo().getTimestampInMillis(), greaterThan(0l));
+        assertThat(clusterState.getRoutingNodes().hasUnassigned(), equalTo(true));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo(), notNullValue());
+        assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.NODE_LEFT));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo().getTimestampInMillis(), greaterThan(0l));
     }
 
     /**
@@ -245,21 +245,21 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1")).put(newNode("node2"))).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
         // starting primaries
-        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING))).build();
+        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING))).build();
         // starting replicas
-        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING))).build();
-        assertThat(clusterState.routingNodes().hasUnassigned(), equalTo(false));
+        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING))).build();
+        assertThat(clusterState.getRoutingNodes().hasUnassigned(), equalTo(false));
         // fail shard
-        ShardRouting shardToFail = clusterState.routingNodes().shardsWithState(STARTED).get(0);
+        ShardRouting shardToFail = clusterState.getRoutingNodes().shardsWithState(STARTED).get(0);
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyFailedShards(clusterState, ImmutableList.of(new FailedRerouteAllocation.FailedShard(shardToFail, "test fail", null)))).build();
         // verify the reason and details
-        assertThat(clusterState.routingNodes().hasUnassigned(), equalTo(true));
-        assertThat(clusterState.routingNodes().shardsWithState(UNASSIGNED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo(), notNullValue());
-        assertThat(clusterState.routingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.ALLOCATION_FAILED));
-        assertThat(clusterState.routingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo().getMessage(), equalTo("test fail"));
-        assertThat(clusterState.routingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo().getDetails(), equalTo("test fail"));
-        assertThat(clusterState.routingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo().getTimestampInMillis(), greaterThan(0l));
+        assertThat(clusterState.getRoutingNodes().hasUnassigned(), equalTo(true));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo(), notNullValue());
+        assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.ALLOCATION_FAILED));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo().getMessage(), equalTo("test fail"));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo().getDetails(), equalTo("test fail"));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo().getTimestampInMillis(), greaterThan(0l));
     }
 
     /**
@@ -273,7 +273,8 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
         assertBusy(new Runnable() {
             @Override
             public void run() {
-                long delay = unassignedInfo.getDelayAllocationExpirationIn(Settings.builder().put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING, "10h").build(), Settings.EMPTY);
+                long delay = unassignedInfo.getDelayAllocationExpirationIn(System.currentTimeMillis(),
+                        Settings.builder().put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING, "10h").build(), Settings.EMPTY);
                 assertThat(delay, greaterThan(0l));
                 assertThat(delay, lessThan(TimeValue.timeValueHours(10).millis()));
             }
@@ -290,7 +291,8 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
         UnassignedInfo unassignedInfo = new UnassignedInfo(RandomPicks.randomFrom(getRandom(), reasons), null);
         long delay = unassignedInfo.getAllocationDelayTimeoutSetting(Settings.builder().put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING, "10h").build(), Settings.EMPTY);
         assertThat(delay, equalTo(0l));
-        delay = unassignedInfo.getDelayAllocationExpirationIn(Settings.builder().put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING, "10h").build(), Settings.EMPTY);
+        delay = unassignedInfo.getDelayAllocationExpirationIn(System.currentTimeMillis(),
+                Settings.builder().put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING, "10h").build(), Settings.EMPTY);
         assertThat(delay, equalTo(0l));
     }
 
@@ -306,16 +308,18 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
                 .routingTable(RoutingTable.builder().addAsNew(metaData.index("test1")).addAsNew(metaData.index("test2"))).build();
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1")).put(newNode("node2"))).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
-        assertThat(UnassignedInfo.getNumberOfDelayedUnassigned(Settings.builder().put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING, "10h").build(), clusterState), equalTo(0));
+        assertThat(UnassignedInfo.getNumberOfDelayedUnassigned(System.currentTimeMillis(),
+                Settings.builder().put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING, "10h").build(), clusterState), equalTo(0));
         // starting primaries
-        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING))).build();
+        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING))).build();
         // starting replicas
-        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING))).build();
-        assertThat(clusterState.routingNodes().hasUnassigned(), equalTo(false));
+        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING))).build();
+        assertThat(clusterState.getRoutingNodes().hasUnassigned(), equalTo(false));
         // remove node2 and reroute
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes()).remove("node2")).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
-        assertThat(clusterState.prettyPrint(), UnassignedInfo.getNumberOfDelayedUnassigned(Settings.builder().put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING, "10h").build(), clusterState), equalTo(2));
+        assertThat(clusterState.prettyPrint(), UnassignedInfo.getNumberOfDelayedUnassigned(System.currentTimeMillis(),
+                Settings.builder().put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING, "10h").build(), clusterState), equalTo(2));
     }
 
     @Test
@@ -330,12 +334,13 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
                 .routingTable(RoutingTable.builder().addAsNew(metaData.index("test1")).addAsNew(metaData.index("test2"))).build();
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1")).put(newNode("node2"))).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
-        assertThat(UnassignedInfo.getNumberOfDelayedUnassigned(Settings.builder().put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING, "10h").build(), clusterState), equalTo(0));
+        assertThat(UnassignedInfo.getNumberOfDelayedUnassigned(System.currentTimeMillis(),
+                Settings.builder().put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING, "10h").build(), clusterState), equalTo(0));
         // starting primaries
-        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING))).build();
+        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING))).build();
         // starting replicas
-        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING))).build();
-        assertThat(clusterState.routingNodes().hasUnassigned(), equalTo(false));
+        clusterState = ClusterState.builder(clusterState).routingResult(allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING))).build();
+        assertThat(clusterState.getRoutingNodes().hasUnassigned(), equalTo(false));
         // remove node2 and reroute
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes()).remove("node2")).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
@@ -343,7 +348,8 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
         long nextDelaySetting = UnassignedInfo.findSmallestDelayedAllocationSetting(Settings.builder().put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING, "10h").build(), clusterState);
         assertThat(nextDelaySetting, equalTo(TimeValue.timeValueHours(10).millis()));
 
-        long nextDelay = UnassignedInfo.findNextDelayedAllocationIn(Settings.builder().put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING, "10h").build(), clusterState);
+        long nextDelay = UnassignedInfo.findNextDelayedAllocationIn(System.currentTimeMillis(),
+                Settings.builder().put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING, "10h").build(), clusterState);
         assertThat(nextDelay, greaterThan(TimeValue.timeValueHours(9).millis()));
         assertThat(nextDelay, lessThanOrEqualTo(TimeValue.timeValueHours(10).millis()));
     }
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AddIncrementallyTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AddIncrementallyTests.java
index d774cf9..b247441 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AddIncrementallyTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AddIncrementallyTests.java
@@ -56,13 +56,13 @@ public class AddIncrementallyTests extends ESAllocationTestCase {
         AllocationService service = createAllocationService(settings.build());
 
         ClusterState clusterState = initCluster(service, 1, 3, 3, 1);
-        assertThat(clusterState.routingNodes().node("node0").shardsWithState(STARTED).size(), Matchers.equalTo(9));
-        assertThat(clusterState.routingNodes().unassigned().size(), Matchers.equalTo(9));
+        assertThat(clusterState.getRoutingNodes().node("node0").shardsWithState(STARTED).size(), Matchers.equalTo(9));
+        assertThat(clusterState.getRoutingNodes().unassigned().size(), Matchers.equalTo(9));
         int nodeOffset = 1;
         clusterState = addNodes(clusterState, service, 1, nodeOffset++);
-        assertThat(clusterState.routingNodes().node("node0").shardsWithState(STARTED).size(), Matchers.equalTo(9));
-        assertThat(clusterState.routingNodes().node("node1").shardsWithState(STARTED).size(), Matchers.equalTo(9));
-        assertThat(clusterState.routingNodes().unassigned().size(), Matchers.equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node0").shardsWithState(STARTED).size(), Matchers.equalTo(9));
+        assertThat(clusterState.getRoutingNodes().node("node1").shardsWithState(STARTED).size(), Matchers.equalTo(9));
+        assertThat(clusterState.getRoutingNodes().unassigned().size(), Matchers.equalTo(0));
         assertNumIndexShardsPerNode(clusterState, Matchers.equalTo(3));
         clusterState = addNodes(clusterState, service, 1, nodeOffset++);
         assertNumIndexShardsPerNode(clusterState, Matchers.equalTo(2));
@@ -73,23 +73,23 @@ public class AddIncrementallyTests extends ESAllocationTestCase {
         assertNumIndexShardsPerNode(clusterState, Matchers.equalTo(2));
 
         clusterState = addIndex(clusterState, service, 3, 2, 3);
-        assertThat(clusterState.routingNodes().unassigned().size(), Matchers.equalTo(2));
+        assertThat(clusterState.getRoutingNodes().unassigned().size(), Matchers.equalTo(2));
         assertNumIndexShardsPerNode(clusterState, "test3", Matchers.equalTo(2));
         assertNumIndexShardsPerNode(clusterState, Matchers.lessThanOrEqualTo(2));
 
         clusterState = addIndex(clusterState, service, 4, 2, 3);
-        assertThat(clusterState.routingNodes().unassigned().size(), Matchers.equalTo(4));
+        assertThat(clusterState.getRoutingNodes().unassigned().size(), Matchers.equalTo(4));
         assertNumIndexShardsPerNode(clusterState, "test4", Matchers.equalTo(2));
         assertNumIndexShardsPerNode(clusterState, Matchers.lessThanOrEqualTo(2));
         clusterState = addNodes(clusterState, service, 1, nodeOffset++);
         assertNumIndexShardsPerNode(clusterState, Matchers.lessThanOrEqualTo(2));
-        assertThat(clusterState.routingNodes().unassigned().size(), Matchers.equalTo(0));
+        assertThat(clusterState.getRoutingNodes().unassigned().size(), Matchers.equalTo(0));
         clusterState = removeNodes(clusterState, service, 1);
-        assertThat(clusterState.routingNodes().unassigned().size(), Matchers.equalTo(4));
+        assertThat(clusterState.getRoutingNodes().unassigned().size(), Matchers.equalTo(4));
         assertNumIndexShardsPerNode(clusterState, Matchers.lessThanOrEqualTo(2));
         clusterState = addNodes(clusterState, service, 1, nodeOffset++);
         assertNumIndexShardsPerNode(clusterState, Matchers.lessThanOrEqualTo(2));
-        assertThat(clusterState.routingNodes().unassigned().size(), Matchers.equalTo(0));
+        assertThat(clusterState.getRoutingNodes().unassigned().size(), Matchers.equalTo(0));
         logger.debug("ClusterState: {}", clusterState.getRoutingNodes().prettyPrint());
     }
 
@@ -101,13 +101,13 @@ public class AddIncrementallyTests extends ESAllocationTestCase {
         AllocationService service = createAllocationService(settings.build());
 
         ClusterState clusterState = initCluster(service, 1, 3, 3, 1);
-        assertThat(clusterState.routingNodes().node("node0").shardsWithState(STARTED).size(), Matchers.equalTo(9));
-        assertThat(clusterState.routingNodes().unassigned().size(), Matchers.equalTo(9));
+        assertThat(clusterState.getRoutingNodes().node("node0").shardsWithState(STARTED).size(), Matchers.equalTo(9));
+        assertThat(clusterState.getRoutingNodes().unassigned().size(), Matchers.equalTo(9));
         int nodeOffset = 1;
         clusterState = addNodes(clusterState, service, 1, nodeOffset++);
-        assertThat(clusterState.routingNodes().node("node0").shardsWithState(STARTED).size(), Matchers.equalTo(9));
-        assertThat(clusterState.routingNodes().node("node1").shardsWithState(STARTED).size(), Matchers.equalTo(9));
-        assertThat(clusterState.routingNodes().unassigned().size(), Matchers.equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node0").shardsWithState(STARTED).size(), Matchers.equalTo(9));
+        assertThat(clusterState.getRoutingNodes().node("node1").shardsWithState(STARTED).size(), Matchers.equalTo(9));
+        assertThat(clusterState.getRoutingNodes().unassigned().size(), Matchers.equalTo(0));
         assertNumIndexShardsPerNode(clusterState, Matchers.equalTo(3));
 
         logger.info("now, start one more node, check that rebalancing will happen because we set it to always");
@@ -117,47 +117,46 @@ public class AddIncrementallyTests extends ESAllocationTestCase {
 
         RoutingTable routingTable = service.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
 
-        assertThat(clusterState.routingNodes().node("node2").shardsWithState(INITIALIZING).size(), Matchers.equalTo(2));
-        assertThat(clusterState.routingNodes().node("node0").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
-        assertThat(clusterState.routingNodes().node("node1").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node2").shardsWithState(INITIALIZING).size(), Matchers.equalTo(2));
+        assertThat(clusterState.getRoutingNodes().node("node0").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node1").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
 
         RoutingTable prev = routingTable;
         routingTable = service.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         assertThat(prev, Matchers.not(Matchers.sameInstance(routingTable)));
-        assertThat(clusterState.routingNodes().node("node2").shardsWithState(STARTED).size(), Matchers.equalTo(2));
-        assertThat(clusterState.routingNodes().node("node2").shardsWithState(INITIALIZING).size(), Matchers.equalTo(2));
-        assertThat(clusterState.routingNodes().node("node0").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
-        assertThat(clusterState.routingNodes().node("node1").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node2").shardsWithState(STARTED).size(), Matchers.equalTo(2));
+        assertThat(clusterState.getRoutingNodes().node("node2").shardsWithState(INITIALIZING).size(), Matchers.equalTo(2));
+        assertThat(clusterState.getRoutingNodes().node("node0").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node1").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
         assertThat(prev, Matchers.not(Matchers.sameInstance(routingTable)));
 
         prev = routingTable;
         routingTable = service.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
-        assertThat(clusterState.routingNodes().node("node2").shardsWithState(STARTED).size(), Matchers.equalTo(4));
-        assertThat(clusterState.routingNodes().node("node2").shardsWithState(INITIALIZING).size(), Matchers.equalTo(2));
-        assertThat(clusterState.routingNodes().node("node0").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
-        assertThat(clusterState.routingNodes().node("node1").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
+        routingNodes = clusterState.getRoutingNodes();
+        assertThat(clusterState.getRoutingNodes().node("node2").shardsWithState(STARTED).size(), Matchers.equalTo(4));
+        assertThat(clusterState.getRoutingNodes().node("node2").shardsWithState(INITIALIZING).size(), Matchers.equalTo(2));
+        assertThat(clusterState.getRoutingNodes().node("node0").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node1").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
         assertThat(prev, Matchers.not(Matchers.sameInstance(routingTable)));
 
         prev = routingTable;
         routingTable = service.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
-        assertThat(clusterState.routingNodes().node("node2").shardsWithState(STARTED).size(), Matchers.equalTo(6));
-        assertThat(clusterState.routingNodes().node("node2").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
-        assertThat(clusterState.routingNodes().node("node0").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
-        assertThat(clusterState.routingNodes().node("node1").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
+        routingNodes = clusterState.getRoutingNodes();
+        assertThat(clusterState.getRoutingNodes().node("node2").shardsWithState(STARTED).size(), Matchers.equalTo(6));
+        assertThat(clusterState.getRoutingNodes().node("node2").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node0").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node1").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
         assertThat(prev, Matchers.not(Matchers.sameInstance(routingTable)));
 
         prev = routingTable;
         routingTable = service.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
         assertThat(prev, Matchers.sameInstance(routingTable));
         assertNumIndexShardsPerNode(clusterState, Matchers.equalTo(2));
         logger.debug("ClusterState: {}", clusterState.getRoutingNodes().prettyPrint());
@@ -172,13 +171,13 @@ public class AddIncrementallyTests extends ESAllocationTestCase {
         AllocationService service = createAllocationService(settings.build());
 
         ClusterState clusterState = initCluster(service, 1, 3, 3, 1);
-        assertThat(clusterState.routingNodes().node("node0").shardsWithState(STARTED).size(), Matchers.equalTo(9));
-        assertThat(clusterState.routingNodes().unassigned().size(), Matchers.equalTo(9));
+        assertThat(clusterState.getRoutingNodes().node("node0").shardsWithState(STARTED).size(), Matchers.equalTo(9));
+        assertThat(clusterState.getRoutingNodes().unassigned().size(), Matchers.equalTo(9));
         int nodeOffset = 1;
         clusterState = addNodes(clusterState, service, 1, nodeOffset++);
-        assertThat(clusterState.routingNodes().node("node0").shardsWithState(STARTED).size(), Matchers.equalTo(9));
-        assertThat(clusterState.routingNodes().node("node1").shardsWithState(STARTED).size(), Matchers.equalTo(9));
-        assertThat(clusterState.routingNodes().unassigned().size(), Matchers.equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node0").shardsWithState(STARTED).size(), Matchers.equalTo(9));
+        assertThat(clusterState.getRoutingNodes().node("node1").shardsWithState(STARTED).size(), Matchers.equalTo(9));
+        assertThat(clusterState.getRoutingNodes().unassigned().size(), Matchers.equalTo(0));
         assertNumIndexShardsPerNode(clusterState, Matchers.equalTo(3));
 
         logger.info("now, start one more node, check that rebalancing will happen because we set it to always");
@@ -188,47 +187,47 @@ public class AddIncrementallyTests extends ESAllocationTestCase {
 
         RoutingTable routingTable = service.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
 
-        assertThat(clusterState.routingNodes().node("node2").shardsWithState(INITIALIZING).size(), Matchers.equalTo(2));
-        assertThat(clusterState.routingNodes().node("node0").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
-        assertThat(clusterState.routingNodes().node("node1").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node2").shardsWithState(INITIALIZING).size(), Matchers.equalTo(2));
+        assertThat(clusterState.getRoutingNodes().node("node0").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node1").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
 
         RoutingTable prev = routingTable;
         routingTable = service.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         assertThat(prev, Matchers.not(Matchers.sameInstance(routingTable)));
-        assertThat(clusterState.routingNodes().node("node2").shardsWithState(STARTED).size(), Matchers.equalTo(2));
-        assertThat(clusterState.routingNodes().node("node2").shardsWithState(INITIALIZING).size(), Matchers.equalTo(2));
-        assertThat(clusterState.routingNodes().node("node0").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
-        assertThat(clusterState.routingNodes().node("node1").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node2").shardsWithState(STARTED).size(), Matchers.equalTo(2));
+        assertThat(clusterState.getRoutingNodes().node("node2").shardsWithState(INITIALIZING).size(), Matchers.equalTo(2));
+        assertThat(clusterState.getRoutingNodes().node("node0").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node1").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
         assertThat(prev, Matchers.not(Matchers.sameInstance(routingTable)));
 
         prev = routingTable;
         routingTable = service.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
-        assertThat(clusterState.routingNodes().node("node2").shardsWithState(STARTED).size(), Matchers.equalTo(4));
-        assertThat(clusterState.routingNodes().node("node2").shardsWithState(INITIALIZING).size(), Matchers.equalTo(2));
-        assertThat(clusterState.routingNodes().node("node0").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
-        assertThat(clusterState.routingNodes().node("node1").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
+        routingNodes = clusterState.getRoutingNodes();
+        assertThat(clusterState.getRoutingNodes().node("node2").shardsWithState(STARTED).size(), Matchers.equalTo(4));
+        assertThat(clusterState.getRoutingNodes().node("node2").shardsWithState(INITIALIZING).size(), Matchers.equalTo(2));
+        assertThat(clusterState.getRoutingNodes().node("node0").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node1").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
         assertThat(prev, Matchers.not(Matchers.sameInstance(routingTable)));
 
         prev = routingTable;
         routingTable = service.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
-        assertThat(clusterState.routingNodes().node("node2").shardsWithState(STARTED).size(), Matchers.equalTo(6));
-        assertThat(clusterState.routingNodes().node("node2").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
-        assertThat(clusterState.routingNodes().node("node0").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
-        assertThat(clusterState.routingNodes().node("node1").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
+        routingNodes = clusterState.getRoutingNodes();
+        assertThat(clusterState.getRoutingNodes().node("node2").shardsWithState(STARTED).size(), Matchers.equalTo(6));
+        assertThat(clusterState.getRoutingNodes().node("node2").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node0").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node1").shardsWithState(INITIALIZING).size(), Matchers.equalTo(0));
         assertThat(prev, Matchers.not(Matchers.sameInstance(routingTable)));
 
         prev = routingTable;
         routingTable = service.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         assertThat(prev, Matchers.sameInstance(routingTable));
         assertNumIndexShardsPerNode(clusterState, Matchers.equalTo(2));
         logger.debug("ClusterState: {}", clusterState.getRoutingNodes().prettyPrint());
@@ -242,7 +241,7 @@ public class AddIncrementallyTests extends ESAllocationTestCase {
     }
 
     private void assertNumIndexShardsPerNode(ClusterState state, String index, Matcher<Integer> matcher) {
-        for (RoutingNode node : state.routingNodes()) {
+        for (RoutingNode node : state.getRoutingNodes()) {
             assertThat(node.shardsWithState(index, STARTED).size(), matcher);
         }
     }
@@ -251,7 +250,7 @@ public class AddIncrementallyTests extends ESAllocationTestCase {
     private void assertAtLeastOneIndexShardPerNode(ClusterState state) {
         for (String index : state.routingTable().indicesRouting().keySet()) {
 
-            for (RoutingNode node : state.routingNodes()) {
+            for (RoutingNode node : state.getRoutingNodes()) {
                 assertThat(node.shardsWithState(index, STARTED).size(), Matchers.greaterThanOrEqualTo(1));
             }
         }
@@ -269,7 +268,7 @@ public class AddIncrementallyTests extends ESAllocationTestCase {
 
         RoutingTable routingTable = service.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
 
         // move initializing to started
 
@@ -278,7 +277,7 @@ public class AddIncrementallyTests extends ESAllocationTestCase {
             logger.debug("ClusterState: {}", clusterState.getRoutingNodes().prettyPrint());
             routingTable = service.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
             clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-            routingNodes = clusterState.routingNodes();
+            routingNodes = clusterState.getRoutingNodes();
             if (routingTable == prev)
                 break;
             prev = routingTable;
@@ -314,19 +313,19 @@ public class AddIncrementallyTests extends ESAllocationTestCase {
         ClusterState clusterState = ClusterState.builder(org.elasticsearch.cluster.ClusterName.DEFAULT).nodes(nodes).metaData(metaData).routingTable(routingTable).build();
         routingTable = service.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
 
         logger.info("restart all the primary shards, replicas will start initializing");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         routingTable = service.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         logger.info("start the replica shards");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         routingTable = service.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         logger.info("complete rebalancing");
         RoutingTable prev = routingTable;
@@ -334,7 +333,7 @@ public class AddIncrementallyTests extends ESAllocationTestCase {
             logger.debug("ClusterState: {}", clusterState.getRoutingNodes().prettyPrint());
             routingTable = service.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
             clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-            routingNodes = clusterState.routingNodes();
+            routingNodes = clusterState.getRoutingNodes();
             if (routingTable == prev)
                 break;
             prev = routingTable;
@@ -359,19 +358,19 @@ public class AddIncrementallyTests extends ESAllocationTestCase {
         clusterState = ClusterState.builder(clusterState).metaData(metaData).routingTable(routingTable).build();
         routingTable = service.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
 
         logger.info("restart all the primary shards, replicas will start initializing");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         routingTable = service.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         logger.info("start the replica shards");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         routingTable = service.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         logger.info("complete rebalancing");
         RoutingTable prev = routingTable;
@@ -379,7 +378,7 @@ public class AddIncrementallyTests extends ESAllocationTestCase {
             logger.debug("ClusterState: {}", clusterState.getRoutingNodes().prettyPrint());
             routingTable = service.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
             clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-            routingNodes = clusterState.routingNodes();
+            routingNodes = clusterState.getRoutingNodes();
             if (routingTable == prev)
                 break;
             prev = routingTable;
@@ -402,22 +401,22 @@ public class AddIncrementallyTests extends ESAllocationTestCase {
         }
 
         clusterState = ClusterState.builder(clusterState).nodes(nodes.build()).build();
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
 
         logger.info("start all the primary shards, replicas will start initializing");
         RoutingTable routingTable = service.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         logger.info("start the replica shards");
         routingTable = service.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         logger.info("rebalancing");
         routingTable = service.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         logger.info("complete rebalancing");
         RoutingTable prev = routingTable;
@@ -425,7 +424,7 @@ public class AddIncrementallyTests extends ESAllocationTestCase {
             logger.debug("ClusterState: {}", clusterState.getRoutingNodes().prettyPrint());
             routingTable = service.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
             clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-            routingNodes = clusterState.routingNodes();
+            routingNodes = clusterState.getRoutingNodes();
             if (routingTable == prev)
                 break;
             prev = routingTable;
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java
index 8e3b92b..21e4d29 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java
@@ -72,7 +72,7 @@ public class AllocationCommandsTests extends ESAllocationTestCase {
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
 
         logger.info("start primary shard");
-        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING));
+        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
 
         logger.info("move the shard");
@@ -86,15 +86,15 @@ public class AllocationCommandsTests extends ESAllocationTestCase {
         rerouteResult = allocation.reroute(clusterState, new AllocationCommands(new MoveAllocationCommand(new ShardId("test", 0), existingNodeId, toNodeId)));
         assertThat(rerouteResult.changed(), equalTo(true));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
-        assertThat(clusterState.routingNodes().node(existingNodeId).get(0).state(), equalTo(ShardRoutingState.RELOCATING));
-        assertThat(clusterState.routingNodes().node(toNodeId).get(0).state(), equalTo(ShardRoutingState.INITIALIZING));
+        assertThat(clusterState.getRoutingNodes().node(existingNodeId).get(0).state(), equalTo(ShardRoutingState.RELOCATING));
+        assertThat(clusterState.getRoutingNodes().node(toNodeId).get(0).state(), equalTo(ShardRoutingState.INITIALIZING));
 
         logger.info("finish moving the shard");
-        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING));
+        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
 
-        assertThat(clusterState.routingNodes().node(existingNodeId).isEmpty(), equalTo(true));
-        assertThat(clusterState.routingNodes().node(toNodeId).get(0).state(), equalTo(ShardRoutingState.STARTED));
+        assertThat(clusterState.getRoutingNodes().node(existingNodeId).isEmpty(), equalTo(true));
+        assertThat(clusterState.getRoutingNodes().node(toNodeId).get(0).state(), equalTo(ShardRoutingState.STARTED));
     }
 
     @Test
@@ -122,7 +122,7 @@ public class AllocationCommandsTests extends ESAllocationTestCase {
         ).build();
         RoutingAllocation.Result rerouteResult = allocation.reroute(clusterState);
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
 
         logger.info("--> allocating with primary flag set to false, should fail");
         try {
@@ -142,16 +142,16 @@ public class AllocationCommandsTests extends ESAllocationTestCase {
         rerouteResult = allocation.reroute(clusterState, new AllocationCommands(new AllocateAllocationCommand(new ShardId("test", 0), "node1", true)));
         assertThat(rerouteResult.changed(), equalTo(true));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
-        assertThat(clusterState.routingNodes().node("node1").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node1").shardsWithState(INITIALIZING).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").shardsWithState(INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(0));
 
         logger.info("--> start the primary shard");
-        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING));
+        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
-        assertThat(clusterState.routingNodes().node("node1").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node1").shardsWithState(STARTED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").shardsWithState(STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(0));
 
         logger.info("--> allocate the replica shard on the primary shard node, should fail");
         try {
@@ -164,19 +164,19 @@ public class AllocationCommandsTests extends ESAllocationTestCase {
         rerouteResult = allocation.reroute(clusterState, new AllocationCommands(new AllocateAllocationCommand(new ShardId("test", 0), "node2", false)));
         assertThat(rerouteResult.changed(), equalTo(true));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
-        assertThat(clusterState.routingNodes().node("node1").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node1").shardsWithState(STARTED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").shardsWithState(INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").shardsWithState(STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").shardsWithState(INITIALIZING).size(), equalTo(1));
 
 
         logger.info("--> start the replica shard");
-        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING));
+        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
-        assertThat(clusterState.routingNodes().node("node1").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node1").shardsWithState(STARTED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").shardsWithState(STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").shardsWithState(STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").shardsWithState(STARTED).size(), equalTo(1));
 
         logger.info("--> verify that we fail when there are no unassigned shards");
         try {
@@ -210,15 +210,15 @@ public class AllocationCommandsTests extends ESAllocationTestCase {
         ).build();
         RoutingAllocation.Result rerouteResult = allocation.reroute(clusterState);
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
 
         logger.info("--> allocating with primary flag set to true");
         rerouteResult = allocation.reroute(clusterState, new AllocationCommands(new AllocateAllocationCommand(new ShardId("test", 0), "node1", true)));
         assertThat(rerouteResult.changed(), equalTo(true));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
-        assertThat(clusterState.routingNodes().node("node1").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node1").shardsWithState(INITIALIZING).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").shardsWithState(INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(0));
 
         logger.info("--> cancel primary allocation, make sure it fails...");
         try {
@@ -228,11 +228,11 @@ public class AllocationCommandsTests extends ESAllocationTestCase {
         }
 
         logger.info("--> start the primary shard");
-        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING));
+        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
-        assertThat(clusterState.routingNodes().node("node1").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node1").shardsWithState(STARTED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").shardsWithState(STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(0));
 
         logger.info("--> cancel primary allocation, make sure it fails...");
         try {
@@ -245,28 +245,28 @@ public class AllocationCommandsTests extends ESAllocationTestCase {
         rerouteResult = allocation.reroute(clusterState, new AllocationCommands(new AllocateAllocationCommand(new ShardId("test", 0), "node2", false)));
         assertThat(rerouteResult.changed(), equalTo(true));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
-        assertThat(clusterState.routingNodes().node("node1").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node1").shardsWithState(STARTED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").shardsWithState(INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").shardsWithState(STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").shardsWithState(INITIALIZING).size(), equalTo(1));
 
         logger.info("--> cancel the relocation allocation");
         rerouteResult = allocation.reroute(clusterState, new AllocationCommands(new CancelAllocationCommand(new ShardId("test", 0), "node2", false)));
         assertThat(rerouteResult.changed(), equalTo(true));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
-        assertThat(clusterState.routingNodes().node("node1").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node1").shardsWithState(STARTED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").size(), equalTo(0));
-        assertThat(clusterState.routingNodes().node("node3").size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").shardsWithState(STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node3").size(), equalTo(0));
 
         logger.info("--> allocate the replica shard on on the second node");
         rerouteResult = allocation.reroute(clusterState, new AllocationCommands(new AllocateAllocationCommand(new ShardId("test", 0), "node2", false)));
         assertThat(rerouteResult.changed(), equalTo(true));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
-        assertThat(clusterState.routingNodes().node("node1").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node1").shardsWithState(STARTED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").shardsWithState(INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").shardsWithState(STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").shardsWithState(INITIALIZING).size(), equalTo(1));
 
         logger.info("--> cancel the primary being replicated, make sure it fails");
         try {
@@ -276,64 +276,64 @@ public class AllocationCommandsTests extends ESAllocationTestCase {
         }
 
         logger.info("--> start the replica shard");
-        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING));
+        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
-        assertThat(clusterState.routingNodes().node("node1").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node1").shardsWithState(STARTED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").shardsWithState(STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").shardsWithState(STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").shardsWithState(STARTED).size(), equalTo(1));
 
         logger.info("--> cancel allocation of the replica shard");
         rerouteResult = allocation.reroute(clusterState, new AllocationCommands(new CancelAllocationCommand(new ShardId("test", 0), "node2", false)));
         assertThat(rerouteResult.changed(), equalTo(true));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
-        assertThat(clusterState.routingNodes().node("node1").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node1").shardsWithState(STARTED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").size(), equalTo(0));
-        assertThat(clusterState.routingNodes().node("node3").size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").shardsWithState(STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node3").size(), equalTo(0));
 
         logger.info("--> allocate the replica shard on on the second node");
         rerouteResult = allocation.reroute(clusterState, new AllocationCommands(new AllocateAllocationCommand(new ShardId("test", 0), "node2", false)));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
         assertThat(rerouteResult.changed(), equalTo(true));
-        assertThat(clusterState.routingNodes().node("node1").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node1").shardsWithState(STARTED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").shardsWithState(INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").shardsWithState(STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").shardsWithState(INITIALIZING).size(), equalTo(1));
         logger.info("--> start the replica shard");
-        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING));
+        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
-        assertThat(clusterState.routingNodes().node("node1").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node1").shardsWithState(STARTED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").shardsWithState(STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").shardsWithState(STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").shardsWithState(STARTED).size(), equalTo(1));
 
         logger.info("--> move the replica shard");
         rerouteResult = allocation.reroute(clusterState, new AllocationCommands(new MoveAllocationCommand(new ShardId("test", 0), "node2", "node3")));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
-        assertThat(clusterState.routingNodes().node("node1").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node1").shardsWithState(STARTED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").shardsWithState(RELOCATING).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node3").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node3").shardsWithState(INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").shardsWithState(STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").shardsWithState(RELOCATING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node3").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node3").shardsWithState(INITIALIZING).size(), equalTo(1));
 
         logger.info("--> cancel the move of the replica shard");
         rerouteResult = allocation.reroute(clusterState, new AllocationCommands(new CancelAllocationCommand(new ShardId("test", 0), "node3", false)));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
-        assertThat(clusterState.routingNodes().node("node1").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node1").shardsWithState(STARTED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").shardsWithState(STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").shardsWithState(STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").shardsWithState(STARTED).size(), equalTo(1));
 
 
         logger.info("--> cancel the primary allocation (with allow_primary set to true)");
         rerouteResult = allocation.reroute(clusterState, new AllocationCommands(new CancelAllocationCommand(new ShardId("test", 0), "node1", true)));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
         assertThat(rerouteResult.changed(), equalTo(true));
-        assertThat(clusterState.routingNodes().node("node2").shardsWithState(STARTED).get(0).primary(), equalTo(true));
-        assertThat(clusterState.routingNodes().node("node1").size(), equalTo(0));
-        assertThat(clusterState.routingNodes().node("node3").size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node2").shardsWithState(STARTED).get(0).primary(), equalTo(true));
+        assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node3").size(), equalTo(0));
     }
 
     @Test
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java
index f8895a7..f1f29d5 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java
@@ -71,17 +71,17 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
 
         logger.info("--> start the shards (primaries)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("--> start the shards (replicas)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(2));
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
@@ -90,15 +90,15 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.RELOCATING).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.RELOCATING).get(0).relocatingNodeId(), equalTo("node3"));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.RELOCATING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.RELOCATING).get(0).relocatingNodeId(), equalTo("node3"));
 
         logger.info("--> complete relocation");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(2));
 
         logger.info("--> do another reroute, make sure nothing moves");
         assertThat(strategy.reroute(clusterState).routingTable(), sameInstance(clusterState.routingTable()));
@@ -110,7 +110,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
         assertThat(routingTable, sameInstance(clusterState.routingTable()));
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(2));
     }
 
     @Test
@@ -141,17 +141,17 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
 
         logger.info("--> start the shards (primaries)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("--> start the shards (replicas)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(2));
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
@@ -160,15 +160,15 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.RELOCATING).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.RELOCATING).get(0).relocatingNodeId(), equalTo("node4"));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.RELOCATING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.RELOCATING).get(0).relocatingNodeId(), equalTo("node4"));
 
         logger.info("--> complete relocation");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(2));
 
         logger.info("--> do another reroute, make sure nothing moves");
         assertThat(strategy.reroute(clusterState).routingTable(), sameInstance(clusterState.routingTable()));
@@ -180,7 +180,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
         assertThat(routingTable, sameInstance(clusterState.routingTable()));
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(2));
     }
 
     @Test
@@ -216,30 +216,30 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        for (ShardRouting shard : clusterState.routingNodes().shardsWithState(INITIALIZING)) {
+        for (ShardRouting shard : clusterState.getRoutingNodes().shardsWithState(INITIALIZING)) {
             logger.info(shard.toString());
         }
-        for (ShardRouting shard : clusterState.routingNodes().shardsWithState(STARTED)) {
+        for (ShardRouting shard : clusterState.getRoutingNodes().shardsWithState(STARTED)) {
             logger.info(shard.toString());
         }
-        for (ShardRouting shard : clusterState.routingNodes().shardsWithState(RELOCATING)) {
+        for (ShardRouting shard : clusterState.getRoutingNodes().shardsWithState(RELOCATING)) {
             logger.info(shard.toString());
         }
-        for (ShardRouting shard : clusterState.routingNodes().shardsWithState(UNASSIGNED)) {
+        for (ShardRouting shard : clusterState.getRoutingNodes().shardsWithState(UNASSIGNED)) {
             logger.info(shard.toString());
         }
 
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(5));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(5));
 
         logger.info("--> start the shards (primaries)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("--> start the shards (replicas)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(10));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(10));
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
@@ -248,20 +248,20 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(5));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.RELOCATING).size(), equalTo(5));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(5));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.RELOCATING).get(0).relocatingNodeId(), equalTo("node3"));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(5));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.RELOCATING).size(), equalTo(5));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(5));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.RELOCATING).get(0).relocatingNodeId(), equalTo("node3"));
 
         logger.info("--> complete initializing");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("--> run it again, since we still might have relocation");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(10));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(10));
 
         logger.info("--> do another reroute, make sure nothing moves");
         assertThat(strategy.reroute(clusterState).routingTable(), sameInstance(clusterState.routingTable()));
@@ -272,13 +272,13 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(RELOCATING).size(), greaterThan(0));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(RELOCATING).size(), greaterThan(0));
 
         logger.info("--> complete relocation");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(10));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(10));
 
         logger.info("--> do another reroute, make sure nothing moves");
         assertThat(strategy.reroute(clusterState).routingTable(), sameInstance(clusterState.routingTable()));
@@ -315,17 +315,17 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(10));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(10));
 
         logger.info("--> start the shards (primaries)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("--> start the shards (replicas)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(20));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(20));
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
@@ -334,21 +334,21 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(10));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.RELOCATING).size(), equalTo(10));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(10));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.RELOCATING).get(0).relocatingNodeId(), equalTo("node3"));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(10));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.RELOCATING).size(), equalTo(10));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(10));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.RELOCATING).get(0).relocatingNodeId(), equalTo("node3"));
 
         logger.info("--> complete initializing");
         for (int i = 0; i < 2; i++) {
             logger.info("--> complete initializing round: [{}]", i);
-            routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+            routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
             clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
         }
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(20));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(20));
         assertThat(clusterState.getRoutingNodes().node("node3").size(), equalTo(10));
         assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(5));
         assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(5));
@@ -362,15 +362,15 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(RELOCATING).size(), greaterThan(0));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(RELOCATING).size(), greaterThan(0));
 
         logger.info("--> complete relocation");
         for (int i = 0; i < 2; i++) {
             logger.info("--> complete initializing round: [{}]", i);
-            routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+            routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
             clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
         }
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(20));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(20));
         assertThat(clusterState.getRoutingNodes().node("node3").size(), equalTo(5));
         assertThat(clusterState.getRoutingNodes().node("node4").size(), equalTo(5));
         assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(5));
@@ -407,17 +407,17 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
 
         logger.info("--> start the shards (primaries)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("--> start the shards (replicas)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(2));
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
@@ -426,15 +426,15 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(2));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).get(0).currentNodeId(), equalTo("node3"));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.INITIALIZING).get(0).currentNodeId(), equalTo("node3"));
 
         logger.info("--> complete relocation");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(3));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(3));
 
         logger.info("--> do another reroute, make sure nothing moves");
         assertThat(strategy.reroute(clusterState).routingTable(), sameInstance(clusterState.routingTable()));
@@ -445,15 +445,15 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(2));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.RELOCATING).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.RELOCATING).get(0).relocatingNodeId(), equalTo("node4"));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.RELOCATING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.RELOCATING).get(0).relocatingNodeId(), equalTo("node4"));
 
         logger.info("--> complete relocation");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(3));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(3));
 
         logger.info("--> make sure another reroute does not move things");
         assertThat(strategy.reroute(clusterState).routingTable(), sameInstance(clusterState.routingTable()));
@@ -488,17 +488,17 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
 
         logger.info("--> start the shards (primaries)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("--> start the shards (replicas)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(4));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(4));
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
@@ -507,15 +507,15 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(3));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.RELOCATING).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.RELOCATING).get(0).relocatingNodeId(), equalTo("node5"));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(3));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.RELOCATING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.RELOCATING).get(0).relocatingNodeId(), equalTo("node5"));
 
         logger.info("--> complete relocation");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(4));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(4));
 
         logger.info("--> do another reroute, make sure nothing moves");
         assertThat(strategy.reroute(clusterState).routingTable(), sameInstance(clusterState.routingTable()));
@@ -526,15 +526,15 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(3));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.RELOCATING).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.RELOCATING).get(0).relocatingNodeId(), equalTo("node6"));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(3));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.RELOCATING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.RELOCATING).get(0).relocatingNodeId(), equalTo("node6"));
 
         logger.info("--> complete relocation");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(4));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(4));
 
         logger.info("--> make sure another reroute does not move things");
         assertThat(strategy.reroute(clusterState).routingTable(), sameInstance(clusterState.routingTable()));
@@ -568,15 +568,15 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
 
         logger.info("--> start the shards (primaries)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("--> replica will not start because we have only one rack value");
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
@@ -585,15 +585,15 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).get(0).currentNodeId(), equalTo("node3"));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.INITIALIZING).get(0).currentNodeId(), equalTo("node3"));
 
         logger.info("--> complete relocation");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(2));
 
         logger.info("--> do another reroute, make sure nothing moves");
         assertThat(strategy.reroute(clusterState).routingTable(), sameInstance(clusterState.routingTable()));
@@ -605,7 +605,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
         assertThat(routingTable, sameInstance(clusterState.routingTable()));
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(2));
     }
 
     @Test
@@ -637,15 +637,15 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
 
         logger.info("--> start the shards (primaries)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("--> replica will not start because we have only one rack value");
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
@@ -654,15 +654,15 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).get(0).currentNodeId(), equalTo("node4"));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.INITIALIZING).get(0).currentNodeId(), equalTo("node4"));
 
         logger.info("--> complete relocation");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(2));
 
         logger.info("--> do another reroute, make sure nothing moves");
         assertThat(strategy.reroute(clusterState).routingTable(), sameInstance(clusterState.routingTable()));
@@ -674,7 +674,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
         assertThat(routingTable, sameInstance(clusterState.routingTable()));
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(2));
     }
 
     @Test
@@ -712,13 +712,13 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(10));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(10));
 
         logger.info("--> start the shards (primaries)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(10));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(10));
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
@@ -727,19 +727,19 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(10));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(10));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).get(0).currentNodeId(), equalTo("node3"));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(10));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(10));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.INITIALIZING).get(0).currentNodeId(), equalTo("node3"));
 
         logger.info("--> complete initializing");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("--> run it again, since we still might have relocation");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(20));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(20));
 
         logger.info("--> do another reroute, make sure nothing moves");
         assertThat(strategy.reroute(clusterState).routingTable(), sameInstance(clusterState.routingTable()));
@@ -750,13 +750,13 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(RELOCATING).size(), greaterThan(0));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(RELOCATING).size(), greaterThan(0));
 
         logger.info("--> complete relocation");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(20));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(20));
 
         logger.info("--> do another reroute, make sure nothing moves");
         assertThat(strategy.reroute(clusterState).routingTable(), sameInstance(clusterState.routingTable()));
@@ -792,20 +792,20 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(0));
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(5));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(5));
 
         logger.info("--> start the shards (primaries)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(5));
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(5));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(5));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(5));
 
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
         logger.info("--> all replicas are allocated and started since we have on node in each zone");
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(10));
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(10));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
 
         logger.info("--> add a new node in zone 'a' and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
@@ -813,15 +813,15 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(8));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(2));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).get(0).currentNodeId(), equalTo("A-1"));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(8));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.INITIALIZING).get(0).currentNodeId(), equalTo("A-1"));
         logger.info("--> starting initializing shards on the new node");
 
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(10));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(10));
         assertThat(clusterState.getRoutingNodes().node("A-1").size(), equalTo(2));
         assertThat(clusterState.getRoutingNodes().node("A-0").size(), equalTo(3));
         assertThat(clusterState.getRoutingNodes().node("B-0").size(), equalTo(5));
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java
index c5c96b0..f9306ad 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java
@@ -21,7 +21,6 @@ package org.elasticsearch.cluster.routing.allocation;
 
 import com.carrotsearch.hppc.cursors.ObjectCursor;
 import org.elasticsearch.Version;
-import org.elasticsearch.cluster.ClusterInfoService;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.EmptyClusterInfoService;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
@@ -133,26 +132,26 @@ public class BalanceConfigurationTests extends ESAllocationTestCase {
         ClusterState clusterState = ClusterState.builder(org.elasticsearch.cluster.ClusterName.DEFAULT).nodes(nodes).metaData(metaData).routingTable(routingTable).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
 
         logger.info("restart all the primary shards, replicas will start initializing");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         logger.info("start the replica shards");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         logger.info("complete rebalancing");
         RoutingTable prev = routingTable;
         while (true) {
             routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
             clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-            routingNodes = clusterState.routingNodes();
+            routingNodes = clusterState.getRoutingNodes();
             if (routingTable == prev)
                 break;
             prev = routingTable;
@@ -169,7 +168,7 @@ public class BalanceConfigurationTests extends ESAllocationTestCase {
 
         RoutingTable routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
 
         // move initializing to started
 
@@ -177,7 +176,7 @@ public class BalanceConfigurationTests extends ESAllocationTestCase {
         while (true) {
             routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
             clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-            routingNodes = clusterState.routingNodes();
+            routingNodes = clusterState.getRoutingNodes();
             if (routingTable == prev)
                 break;
             prev = routingTable;
@@ -195,29 +194,29 @@ public class BalanceConfigurationTests extends ESAllocationTestCase {
         }
 
         clusterState = ClusterState.builder(clusterState).nodes(nodes.build()).build();
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
 
         logger.info("start all the primary shards, replicas will start initializing");
         RoutingTable routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         logger.info("start the replica shards");
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         logger.info("rebalancing");
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         logger.info("complete rebalancing");
         RoutingTable prev = routingTable;
         while (true) {
             routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
             clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-            routingNodes = clusterState.routingNodes();
+            routingNodes = clusterState.getRoutingNodes();
             if (routingTable == prev)
                 break;
             prev = routingTable;
@@ -428,7 +427,7 @@ public class BalanceConfigurationTests extends ESAllocationTestCase {
         ClusterState clusterState = ClusterState.builder(org.elasticsearch.cluster.ClusterName.DEFAULT).nodes(nodes).metaData(metaData).routingTable(routingTable).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
 
         for (RoutingNode routingNode : routingNodes) {
             for (ShardRouting shardRouting : routingNode) {
@@ -438,10 +437,10 @@ public class BalanceConfigurationTests extends ESAllocationTestCase {
         strategy = createAllocationService(settings.build());
 
         logger.info("use the new allocator and check if it moves shards");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         for (RoutingNode routingNode : routingNodes) {
             for (ShardRouting shardRouting : routingNode) {
                 assertThat(shardRouting.state(), Matchers.equalTo(ShardRoutingState.STARTED));
@@ -451,7 +450,7 @@ public class BalanceConfigurationTests extends ESAllocationTestCase {
         logger.info("start the replica shards");
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (RoutingNode routingNode : routingNodes) {
             for (ShardRouting shardRouting : routingNode) {
@@ -462,7 +461,7 @@ public class BalanceConfigurationTests extends ESAllocationTestCase {
         logger.info("rebalancing");
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (RoutingNode routingNode : routingNodes) {
             for (ShardRouting shardRouting : routingNode) {
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ClusterRebalanceRoutingTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ClusterRebalanceRoutingTests.java
index 1361d27..59e7529 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ClusterRebalanceRoutingTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ClusterRebalanceRoutingTests.java
@@ -77,11 +77,11 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
         }
 
         logger.info("start all the primary shards for test1, replicas will start initializing");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState("test1", INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (int i = 0; i < routingTable.index("test1").shards().size(); i++) {
             assertThat(routingTable.index("test1").shard(i).shards().size(), equalTo(2));
@@ -96,11 +96,11 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
         }
 
         logger.info("start the test1 replica shards");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState("test1", INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (int i = 0; i < routingTable.index("test1").shards().size(); i++) {
             assertThat(routingTable.index("test1").shard(i).shards().size(), equalTo(2));
@@ -121,7 +121,7 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
         prevRoutingTable = routingTable;
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(routingNodes.node("node3").size(), equalTo(1));
         assertThat(routingNodes.node("node3").get(0).shardId().index().name(), equalTo("test1"));
@@ -164,11 +164,11 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
         }
 
         logger.info("start all the primary shards for test1, replicas will start initializing");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState("test1", INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (int i = 0; i < routingTable.index("test1").shards().size(); i++) {
             assertThat(routingTable.index("test1").shard(i).size(), equalTo(2));
@@ -183,11 +183,11 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
         }
 
         logger.info("start the test1 replica shards");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState("test1", INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (int i = 0; i < routingTable.index("test1").shards().size(); i++) {
             assertThat(routingTable.index("test1").shard(i).size(), equalTo(2));
@@ -202,11 +202,11 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
         }
 
         logger.info("start all the primary shards for test2, replicas will start initializing");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState("test2", INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (int i = 0; i < routingTable.index("test1").shards().size(); i++) {
             assertThat(routingTable.index("test1").shard(i).size(), equalTo(2));
@@ -227,7 +227,7 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
         prevRoutingTable = routingTable;
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(routingNodes.node("node3").size(), equalTo(1));
         assertThat(routingNodes.node("node3").get(0).shardId().index().name(), equalTo("test1"));
@@ -269,11 +269,11 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
         }
 
         logger.info("start all the primary shards for test1, replicas will start initializing");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState("test1", INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (int i = 0; i < routingTable.index("test1").shards().size(); i++) {
             assertThat(routingTable.index("test1").shard(i).shards().size(), equalTo(2));
@@ -288,11 +288,11 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
         }
 
         logger.info("start the test1 replica shards");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState("test1", INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (int i = 0; i < routingTable.index("test1").shards().size(); i++) {
             assertThat(routingTable.index("test1").shard(i).shards().size(), equalTo(2));
@@ -313,7 +313,7 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
         prevRoutingTable = routingTable;
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(routingNodes.node("node3").isEmpty(), equalTo(true));
     }
@@ -354,11 +354,11 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
         }
 
         logger.info("start all the primary shards for test1, replicas will start initializing");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState("test1", INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (int i = 0; i < routingTable.index("test1").shards().size(); i++) {
             assertThat(routingTable.index("test1").shard(i).shards().size(), equalTo(2));
@@ -373,11 +373,11 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
         }
 
         logger.info("start the test1 replica shards");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState("test1", INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (int i = 0; i < routingTable.index("test1").shards().size(); i++) {
             assertThat(routingTable.index("test1").shard(i).shards().size(), equalTo(2));
@@ -392,11 +392,11 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
         }
 
         logger.info("start all the primary shards for test2, replicas will start initializing");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState("test2", INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (int i = 0; i < routingTable.index("test1").shards().size(); i++) {
             assertThat(routingTable.index("test1").shard(i).shards().size(), equalTo(2));
@@ -411,11 +411,11 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
         }
 
         logger.info("start the test2 replica shards");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState("test2", INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (int i = 0; i < routingTable.index("test1").shards().size(); i++) {
             assertThat(routingTable.index("test1").shard(i).shards().size(), equalTo(2));
@@ -436,7 +436,7 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
         prevRoutingTable = routingTable;
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(routingNodes.node("node3").size(), equalTo(1));
         assertThat(routingNodes.node("node3").get(0).shardId().index().name(), anyOf(equalTo("test1"), equalTo("test2")));
@@ -478,11 +478,11 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
         }
 
         logger.info("start all the primary shards for test1, replicas will start initializing");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState("test1", INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (int i = 0; i < routingTable.index("test1").shards().size(); i++) {
             assertThat(routingTable.index("test1").shard(i).shards().size(), equalTo(2));
@@ -497,11 +497,11 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
         }
 
         logger.info("start the test1 replica shards");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState("test1", INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (int i = 0; i < routingTable.index("test1").shards().size(); i++) {
             assertThat(routingTable.index("test1").shard(i).shards().size(), equalTo(2));
@@ -522,7 +522,7 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
         prevRoutingTable = routingTable;
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(routingNodes.node("node3").isEmpty(), equalTo(true));
     }
@@ -563,11 +563,11 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
         }
 
         logger.info("start all the primary shards for test1, replicas will start initializing");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState("test1", INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (int i = 0; i < routingTable.index("test1").shards().size(); i++) {
             assertThat(routingTable.index("test1").shard(i).shards().size(), equalTo(2));
@@ -582,11 +582,11 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
         }
 
         logger.info("start the test1 replica shards");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState("test1", INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (int i = 0; i < routingTable.index("test1").shards().size(); i++) {
             assertThat(routingTable.index("test1").shard(i).shards().size(), equalTo(2));
@@ -601,11 +601,11 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
         }
 
         logger.info("start all the primary shards for test2, replicas will start initializing");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState("test2", INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (int i = 0; i < routingTable.index("test1").shards().size(); i++) {
             assertThat(routingTable.index("test1").shard(i).shards().size(), equalTo(2));
@@ -626,7 +626,7 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
         prevRoutingTable = routingTable;
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(routingNodes.node("node3").isEmpty(), equalTo(true));
     }
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ConcurrentRebalanceRoutingTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ConcurrentRebalanceRoutingTests.java
index 688a281..7fe26d0 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ConcurrentRebalanceRoutingTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ConcurrentRebalanceRoutingTests.java
@@ -81,11 +81,11 @@ public class ConcurrentRebalanceRoutingTests extends ESAllocationTestCase {
         }
 
         logger.info("start all the primary shards, replicas will start initializing");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (int i = 0; i < routingTable.index("test").shards().size(); i++) {
             assertThat(routingTable.index("test").shard(i).shards().size(), equalTo(2));
@@ -100,7 +100,7 @@ public class ConcurrentRebalanceRoutingTests extends ESAllocationTestCase {
         prevRoutingTable = routingTable;
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (int i = 0; i < routingTable.index("test").shards().size(); i++) {
             assertThat(routingTable.index("test").shard(i).shards().size(), equalTo(2));
@@ -109,44 +109,44 @@ public class ConcurrentRebalanceRoutingTests extends ESAllocationTestCase {
         }
 
         logger.info("start the replica shards, rebalancing should start, but, only 3 should be rebalancing");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         // we only allow one relocation at a time
         assertThat(routingTable.shardsWithState(STARTED).size(), equalTo(7));
         assertThat(routingTable.shardsWithState(RELOCATING).size(), equalTo(3));
 
         logger.info("finalize this session relocation, 3 more should relocate now");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         // we only allow one relocation at a time
         assertThat(routingTable.shardsWithState(STARTED).size(), equalTo(7));
         assertThat(routingTable.shardsWithState(RELOCATING).size(), equalTo(3));
 
         logger.info("finalize this session relocation, 2 more should relocate now");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         // we only allow one relocation at a time
         assertThat(routingTable.shardsWithState(STARTED).size(), equalTo(8));
         assertThat(routingTable.shardsWithState(RELOCATING).size(), equalTo(2));
 
         logger.info("finalize this session relocation, no more relocation");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         // we only allow one relocation at a time
         assertThat(routingTable.shardsWithState(STARTED).size(), equalTo(10));
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/DeadNodesAllocationTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/DeadNodesAllocationTests.java
index 7697e5b..43f8479 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/DeadNodesAllocationTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/DeadNodesAllocationTests.java
@@ -69,17 +69,17 @@ public class DeadNodesAllocationTests extends ESAllocationTestCase {
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
 
         // starting primaries
-        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING));
+        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
         // starting replicas
-        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING));
+        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
 
         logger.info("--> verifying all is allocated");
-        assertThat(clusterState.routingNodes().node("node1").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node1").get(0).state(), equalTo(STARTED));
-        assertThat(clusterState.routingNodes().node("node2").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").get(0).state(), equalTo(STARTED));
+        assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").get(0).state(), equalTo(STARTED));
+        assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").get(0).state(), equalTo(STARTED));
 
         logger.info("--> fail node with primary");
         String nodeIdToFail = clusterState.routingTable().index("test").shard(0).primaryShard().currentNodeId();
@@ -91,8 +91,8 @@ public class DeadNodesAllocationTests extends ESAllocationTestCase {
         rerouteResult = allocation.reroute(clusterState);
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
 
-        assertThat(clusterState.routingNodes().node(nodeIdRemaining).get(0).primary(), equalTo(true));
-        assertThat(clusterState.routingNodes().node(nodeIdRemaining).get(0).state(), equalTo(STARTED));
+        assertThat(clusterState.getRoutingNodes().node(nodeIdRemaining).get(0).primary(), equalTo(true));
+        assertThat(clusterState.getRoutingNodes().node(nodeIdRemaining).get(0).state(), equalTo(STARTED));
     }
 
     @Test
@@ -121,17 +121,17 @@ public class DeadNodesAllocationTests extends ESAllocationTestCase {
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
 
         // starting primaries
-        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING));
+        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
         // starting replicas
-        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING));
+        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
 
         logger.info("--> verifying all is allocated");
-        assertThat(clusterState.routingNodes().node("node1").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node1").get(0).state(), equalTo(STARTED));
-        assertThat(clusterState.routingNodes().node("node2").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").get(0).state(), equalTo(STARTED));
+        assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").get(0).state(), equalTo(STARTED));
+        assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").get(0).state(), equalTo(STARTED));
 
         logger.info("--> adding additional node");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
@@ -140,11 +140,11 @@ public class DeadNodesAllocationTests extends ESAllocationTestCase {
         rerouteResult = allocation.reroute(clusterState);
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
 
-        assertThat(clusterState.routingNodes().node("node1").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node1").get(0).state(), equalTo(STARTED));
-        assertThat(clusterState.routingNodes().node("node2").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").get(0).state(), equalTo(STARTED));
-        assertThat(clusterState.routingNodes().node("node3").size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").get(0).state(), equalTo(STARTED));
+        assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").get(0).state(), equalTo(STARTED));
+        assertThat(clusterState.getRoutingNodes().node("node3").size(), equalTo(0));
 
         String origPrimaryNodeId = clusterState.routingTable().index("test").shard(0).primaryShard().currentNodeId();
         String origReplicaNodeId = clusterState.routingTable().index("test").shard(0).replicaShards().get(0).currentNodeId();
@@ -155,8 +155,8 @@ public class DeadNodesAllocationTests extends ESAllocationTestCase {
         );
         assertThat(rerouteResult.changed(), equalTo(true));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
-        assertThat(clusterState.routingNodes().node(origPrimaryNodeId).get(0).state(), equalTo(RELOCATING));
-        assertThat(clusterState.routingNodes().node("node3").get(0).state(), equalTo(INITIALIZING));
+        assertThat(clusterState.getRoutingNodes().node(origPrimaryNodeId).get(0).state(), equalTo(RELOCATING));
+        assertThat(clusterState.getRoutingNodes().node("node3").get(0).state(), equalTo(INITIALIZING));
 
         logger.info("--> fail primary shard recovering instance on node3 being initialized by killing node3");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
@@ -166,8 +166,8 @@ public class DeadNodesAllocationTests extends ESAllocationTestCase {
         rerouteResult = allocation.reroute(clusterState);
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
 
-        assertThat(clusterState.routingNodes().node(origPrimaryNodeId).get(0).state(), equalTo(STARTED));
-        assertThat(clusterState.routingNodes().node(origReplicaNodeId).get(0).state(), equalTo(STARTED));
+        assertThat(clusterState.getRoutingNodes().node(origPrimaryNodeId).get(0).state(), equalTo(STARTED));
+        assertThat(clusterState.getRoutingNodes().node(origReplicaNodeId).get(0).state(), equalTo(STARTED));
     }
 
     @Test
@@ -196,17 +196,17 @@ public class DeadNodesAllocationTests extends ESAllocationTestCase {
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
 
         // starting primaries
-        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING));
+        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
         // starting replicas
-        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING));
+        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
 
         logger.info("--> verifying all is allocated");
-        assertThat(clusterState.routingNodes().node("node1").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node1").get(0).state(), equalTo(STARTED));
-        assertThat(clusterState.routingNodes().node("node2").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").get(0).state(), equalTo(STARTED));
+        assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").get(0).state(), equalTo(STARTED));
+        assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").get(0).state(), equalTo(STARTED));
 
         logger.info("--> adding additional node");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
@@ -215,11 +215,11 @@ public class DeadNodesAllocationTests extends ESAllocationTestCase {
         rerouteResult = allocation.reroute(clusterState);
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
 
-        assertThat(clusterState.routingNodes().node("node1").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node1").get(0).state(), equalTo(STARTED));
-        assertThat(clusterState.routingNodes().node("node2").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").get(0).state(), equalTo(STARTED));
-        assertThat(clusterState.routingNodes().node("node3").size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").get(0).state(), equalTo(STARTED));
+        assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").get(0).state(), equalTo(STARTED));
+        assertThat(clusterState.getRoutingNodes().node("node3").size(), equalTo(0));
 
         String origPrimaryNodeId = clusterState.routingTable().index("test").shard(0).primaryShard().currentNodeId();
         String origReplicaNodeId = clusterState.routingTable().index("test").shard(0).replicaShards().get(0).currentNodeId();
@@ -230,8 +230,8 @@ public class DeadNodesAllocationTests extends ESAllocationTestCase {
         );
         assertThat(rerouteResult.changed(), equalTo(true));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
-        assertThat(clusterState.routingNodes().node(origPrimaryNodeId).get(0).state(), equalTo(RELOCATING));
-        assertThat(clusterState.routingNodes().node("node3").get(0).state(), equalTo(INITIALIZING));
+        assertThat(clusterState.getRoutingNodes().node(origPrimaryNodeId).get(0).state(), equalTo(RELOCATING));
+        assertThat(clusterState.getRoutingNodes().node("node3").get(0).state(), equalTo(INITIALIZING));
 
         logger.info("--> fail primary shard recovering instance on 'origPrimaryNodeId' being relocated");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
@@ -241,7 +241,7 @@ public class DeadNodesAllocationTests extends ESAllocationTestCase {
         rerouteResult = allocation.reroute(clusterState);
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
 
-        assertThat(clusterState.routingNodes().node(origReplicaNodeId).get(0).state(), equalTo(STARTED));
-        assertThat(clusterState.routingNodes().node("node3").get(0).state(), equalTo(INITIALIZING));
+        assertThat(clusterState.getRoutingNodes().node(origReplicaNodeId).get(0).state(), equalTo(STARTED));
+        assertThat(clusterState.getRoutingNodes().node("node3").get(0).state(), equalTo(INITIALIZING));
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/DisableAllocationTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/DisableAllocationTests.java
index 2b66bee..b074e97 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/DisableAllocationTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/DisableAllocationTests.java
@@ -68,7 +68,7 @@ public class DisableAllocationTests extends ESAllocationTestCase {
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
 
     }
 
@@ -97,13 +97,13 @@ public class DisableAllocationTests extends ESAllocationTestCase {
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
 
         logger.info("--> start the shards (primaries)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
     }
 
     @Test
@@ -130,17 +130,17 @@ public class DisableAllocationTests extends ESAllocationTestCase {
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
         logger.info("--> start the shards (primaries)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
         logger.info("--> start the shards (replicas)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("--> verify only enabled index has been routed");
-        assertThat(clusterState.readOnlyRoutingNodes().shardsWithState("enabled", STARTED).size(), equalTo(2));
-        assertThat(clusterState.readOnlyRoutingNodes().shardsWithState("disabled", STARTED).size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().shardsWithState("enabled", STARTED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState("disabled", STARTED).size(), equalTo(0));
     }
 
 }
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ElectReplicaAsPrimaryDuringRelocationTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ElectReplicaAsPrimaryDuringRelocationTests.java
index e776ea5..d2ece37 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ElectReplicaAsPrimaryDuringRelocationTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ElectReplicaAsPrimaryDuringRelocationTests.java
@@ -67,17 +67,17 @@ public class ElectReplicaAsPrimaryDuringRelocationTests extends ESAllocationTest
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("Start the primary shards");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("Start the replica shards");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(prevRoutingTable != routingTable, equalTo(true));
         assertThat(routingTable.index("test").shards().size(), equalTo(2));
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedNodeRoutingTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedNodeRoutingTests.java
index 2240e2e..70b056b 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedNodeRoutingTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedNodeRoutingTests.java
@@ -66,18 +66,18 @@ public class FailedNodeRoutingTests extends ESAllocationTestCase {
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("start all the primary shards, replicas will start initializing");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         logger.info("start the replica shards");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(routingNodes.node("node1").numberOfShardsWithState(STARTED), equalTo(1));
         assertThat(routingNodes.node("node2").numberOfShardsWithState(STARTED), equalTo(1));
@@ -95,7 +95,7 @@ public class FailedNodeRoutingTests extends ESAllocationTestCase {
         prevRoutingTable = routingTable;
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (RoutingNode routingNode : routingNodes) {
             assertThat(routingNode.numberOfShardsWithState(STARTED), equalTo(1));
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java
index f45b42c..78083a6 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java
@@ -73,17 +73,17 @@ public class FailedShardsRoutingTests extends ESAllocationTestCase {
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
 
         // starting primaries
-        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING));
+        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
         // starting replicas
-        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING));
+        rerouteResult = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
 
         logger.info("--> verifying all is allocated");
-        assertThat(clusterState.routingNodes().node("node1").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node1").get(0).state(), equalTo(STARTED));
-        assertThat(clusterState.routingNodes().node("node2").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").get(0).state(), equalTo(STARTED));
+        assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").get(0).state(), equalTo(STARTED));
+        assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").get(0).state(), equalTo(STARTED));
 
         logger.info("--> adding additional node");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
@@ -92,11 +92,11 @@ public class FailedShardsRoutingTests extends ESAllocationTestCase {
         rerouteResult = allocation.reroute(clusterState);
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
 
-        assertThat(clusterState.routingNodes().node("node1").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node1").get(0).state(), equalTo(STARTED));
-        assertThat(clusterState.routingNodes().node("node2").size(), equalTo(1));
-        assertThat(clusterState.routingNodes().node("node2").get(0).state(), equalTo(STARTED));
-        assertThat(clusterState.routingNodes().node("node3").size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node1").get(0).state(), equalTo(STARTED));
+        assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().node("node2").get(0).state(), equalTo(STARTED));
+        assertThat(clusterState.getRoutingNodes().node("node3").size(), equalTo(0));
 
         String origPrimaryNodeId = clusterState.routingTable().index("test").shard(0).primaryShard().currentNodeId();
         String origReplicaNodeId = clusterState.routingTable().index("test").shard(0).replicaShards().get(0).currentNodeId();
@@ -107,15 +107,15 @@ public class FailedShardsRoutingTests extends ESAllocationTestCase {
         );
         assertThat(rerouteResult.changed(), equalTo(true));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
-        assertThat(clusterState.routingNodes().node(origPrimaryNodeId).get(0).state(), equalTo(RELOCATING));
-        assertThat(clusterState.routingNodes().node("node3").get(0).state(), equalTo(INITIALIZING));
+        assertThat(clusterState.getRoutingNodes().node(origPrimaryNodeId).get(0).state(), equalTo(RELOCATING));
+        assertThat(clusterState.getRoutingNodes().node("node3").get(0).state(), equalTo(INITIALIZING));
 
         logger.info("--> fail primary shard recovering instance on node3 being initialized");
-        rerouteResult = allocation.applyFailedShard(clusterState, new ShardRouting(clusterState.routingNodes().node("node3").get(0)));
+        rerouteResult = allocation.applyFailedShard(clusterState, new ShardRouting(clusterState.getRoutingNodes().node("node3").get(0)));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
 
-        assertThat(clusterState.routingNodes().node(origPrimaryNodeId).get(0).state(), equalTo(STARTED));
-        assertThat(clusterState.routingNodes().node("node3").size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node(origPrimaryNodeId).get(0).state(), equalTo(STARTED));
+        assertThat(clusterState.getRoutingNodes().node("node3").size(), equalTo(0));
 
         logger.info("--> moving primary shard to node3");
         rerouteResult = allocation.reroute(clusterState, new AllocationCommands(
@@ -123,15 +123,15 @@ public class FailedShardsRoutingTests extends ESAllocationTestCase {
         );
         assertThat(rerouteResult.changed(), equalTo(true));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
-        assertThat(clusterState.routingNodes().node(origPrimaryNodeId).get(0).state(), equalTo(RELOCATING));
-        assertThat(clusterState.routingNodes().node("node3").get(0).state(), equalTo(INITIALIZING));
+        assertThat(clusterState.getRoutingNodes().node(origPrimaryNodeId).get(0).state(), equalTo(RELOCATING));
+        assertThat(clusterState.getRoutingNodes().node("node3").get(0).state(), equalTo(INITIALIZING));
 
         logger.info("--> fail primary shard recovering instance on node1 being relocated");
-        rerouteResult = allocation.applyFailedShard(clusterState, new ShardRouting(clusterState.routingNodes().node(origPrimaryNodeId).get(0)));
+        rerouteResult = allocation.applyFailedShard(clusterState, new ShardRouting(clusterState.getRoutingNodes().node(origPrimaryNodeId).get(0)));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
 
         // check promotion of replica to primary
-        assertThat(clusterState.routingNodes().node(origReplicaNodeId).get(0).state(), equalTo(STARTED));
+        assertThat(clusterState.getRoutingNodes().node(origReplicaNodeId).get(0).state(), equalTo(STARTED));
         assertThat(clusterState.routingTable().index("test").shard(0).primaryShard().currentNodeId(), equalTo(origReplicaNodeId));
         assertThat(clusterState.routingTable().index("test").shard(0).replicaShards().get(0).currentNodeId(), anyOf(equalTo(origPrimaryNodeId), equalTo("node3")));
     }
@@ -162,7 +162,7 @@ public class FailedShardsRoutingTests extends ESAllocationTestCase {
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("Start the shards (primaries)");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
 
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
@@ -181,7 +181,7 @@ public class FailedShardsRoutingTests extends ESAllocationTestCase {
         }
 
         logger.info("Start the shards (backups)");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -203,7 +203,7 @@ public class FailedShardsRoutingTests extends ESAllocationTestCase {
         prevRoutingTable = routingTable;
         routingTable = strategy.applyFailedShard(clusterState, shardToFail).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(prevRoutingTable != routingTable, equalTo(true));
         assertThat(routingTable.index("test").shard(0).size(), equalTo(2));
@@ -256,7 +256,7 @@ public class FailedShardsRoutingTests extends ESAllocationTestCase {
 
         logger.info("fail the first shard, will have no place to be rerouted to (single node), so stays unassigned");
         prevRoutingTable = routingTable;
-        ShardRouting firstShard = clusterState.routingNodes().node("node1").get(0);
+        ShardRouting firstShard = clusterState.getRoutingNodes().node("node1").get(0);
         routingTable = strategy.applyFailedShard(clusterState, firstShard).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
@@ -313,7 +313,7 @@ public class FailedShardsRoutingTests extends ESAllocationTestCase {
 
         int shardsToFail = randomIntBetween(1, numberOfReplicas);
         ArrayList<FailedRerouteAllocation.FailedShard> failedShards = new ArrayList<>();
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         for (int i = 0; i < shardsToFail; i++) {
             String n = "node" + Integer.toString(randomInt(numberOfReplicas));
             logger.info("failing shard on node [{}]", n);
@@ -324,7 +324,7 @@ public class FailedShardsRoutingTests extends ESAllocationTestCase {
         routingTable = strategy.applyFailedShards(clusterState, failedShards).routingTable();
 
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         for (FailedRerouteAllocation.FailedShard failedShard : failedShards) {
             if (!routingNodes.node(failedShard.shard.currentNodeId()).isEmpty()) {
                 fail("shard " + failedShard + " was re-assigned to it's node");
@@ -371,7 +371,7 @@ public class FailedShardsRoutingTests extends ESAllocationTestCase {
 
         logger.info("fail the first shard, will start INITIALIZING on the second node");
         prevRoutingTable = routingTable;
-        final ShardRouting firstShard = clusterState.routingNodes().node(nodeHoldingPrimary).get(0);
+        final ShardRouting firstShard = clusterState.getRoutingNodes().node(nodeHoldingPrimary).get(0);
         routingTable = strategy.applyFailedShard(clusterState, firstShard).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
         assertThat(prevRoutingTable != routingTable, equalTo(true));
@@ -419,7 +419,7 @@ public class FailedShardsRoutingTests extends ESAllocationTestCase {
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("Start the shards (primaries)");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -437,7 +437,7 @@ public class FailedShardsRoutingTests extends ESAllocationTestCase {
         }
 
         logger.info("Start the shards (backups)");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -459,7 +459,7 @@ public class FailedShardsRoutingTests extends ESAllocationTestCase {
         prevRoutingTable = routingTable;
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(prevRoutingTable != routingTable, equalTo(true));
         assertThat(routingTable.index("test").shards().size(), equalTo(2));
@@ -472,11 +472,11 @@ public class FailedShardsRoutingTests extends ESAllocationTestCase {
 
         logger.info("Fail the shards on node 3");
         ShardRouting shardToFail = routingNodes.node("node3").get(0);
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyFailedShard(clusterState, new ShardRouting(shardToFail)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(prevRoutingTable != routingTable, equalTo(true));
         assertThat(routingTable.index("test").shards().size(), equalTo(2));
@@ -507,12 +507,12 @@ public class FailedShardsRoutingTests extends ESAllocationTestCase {
         // add 4 nodes
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1")).put(newNode("node2")).put(newNode("node3")).put(newNode("node4"))).build();
         clusterState = ClusterState.builder(clusterState).routingTable(allocation.reroute(clusterState).routingTable()).build();
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(UNASSIGNED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).size(), equalTo(2));
         // start primary shards
-        clusterState = ClusterState.builder(clusterState).routingTable(allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable()).build();
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(2));
+        clusterState = ClusterState.builder(clusterState).routingTable(allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable()).build();
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(2));
 
         // fail the primary shard, check replicas get removed as well...
         ShardRouting primaryShardToFail = clusterState.routingTable().index("test").shard(0).primaryShard();
@@ -520,16 +520,16 @@ public class FailedShardsRoutingTests extends ESAllocationTestCase {
         assertThat(routingResult.changed(), equalTo(true));
         clusterState = ClusterState.builder(clusterState).routingTable(routingResult.routingTable()).build();
         // the primary gets allocated on another node, replicas are unassigned
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(UNASSIGNED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).size(), equalTo(2));
 
         ShardRouting newPrimaryShard = clusterState.routingTable().index("test").shard(0).primaryShard();
         assertThat(newPrimaryShard, not(equalTo(primaryShardToFail)));
 
         // start the primary shard
-        clusterState = ClusterState.builder(clusterState).routingTable(allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable()).build();
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(2));
+        clusterState = ClusterState.builder(clusterState).routingTable(allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable()).build();
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(2));
 
         // simulate another failure coming in, with the "old" shard routing, verify that nothing changes, and we ignore it
         routingResult = allocation.applyFailedShard(clusterState, primaryShardToFail);
@@ -554,25 +554,25 @@ public class FailedShardsRoutingTests extends ESAllocationTestCase {
         // add 4 nodes
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1")).put(newNode("node2")).put(newNode("node3")).put(newNode("node4"))).build();
         clusterState = ClusterState.builder(clusterState).routingTable(allocation.reroute(clusterState).routingTable()).build();
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(UNASSIGNED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).size(), equalTo(2));
         // start primary shards
-        clusterState = ClusterState.builder(clusterState).routingTable(allocation.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable()).build();
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(2));
+        clusterState = ClusterState.builder(clusterState).routingTable(allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable()).build();
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(2));
 
         // start another replica shard, while keep one initializing
-        clusterState = ClusterState.builder(clusterState).routingTable(allocation.applyStartedShards(clusterState, ImmutableList.of(clusterState.routingNodes().shardsWithState(INITIALIZING).get(0))).routingTable()).build();
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(2));
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
+        clusterState = ClusterState.builder(clusterState).routingTable(allocation.applyStartedShards(clusterState, ImmutableList.of(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).get(0))).routingTable()).build();
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
 
         // fail the primary shard, check one replica gets elected to primary, others become INITIALIZING (from it)
         ShardRouting primaryShardToFail = clusterState.routingTable().index("test").shard(0).primaryShard();
         RoutingAllocation.Result routingResult = allocation.applyFailedShard(clusterState, primaryShardToFail);
         assertThat(routingResult.changed(), equalTo(true));
         clusterState = ClusterState.builder(clusterState).routingTable(routingResult.routingTable()).build();
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(2));
 
         ShardRouting newPrimaryShard = clusterState.routingTable().index("test").shard(0).primaryShard();
         assertThat(newPrimaryShard, not(equalTo(primaryShardToFail)));
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FilterRoutingTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FilterRoutingTests.java
index 037daec..e9a905d 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FilterRoutingTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FilterRoutingTests.java
@@ -74,18 +74,18 @@ public class FilterRoutingTests extends ESAllocationTestCase {
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(2));
 
         logger.info("--> start the shards (primaries)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("--> start the shards (replicas)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("--> make sure shards are only allocated on tag1 with value1 and value2");
-        List<ShardRouting> startedShards = clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED);
+        List<ShardRouting> startedShards = clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED);
         assertThat(startedShards.size(), equalTo(4));
         for (ShardRouting startedShard : startedShards) {
             assertThat(startedShard.currentNodeId(), Matchers.anyOf(equalTo("node1"), equalTo("node2")));
@@ -123,18 +123,18 @@ public class FilterRoutingTests extends ESAllocationTestCase {
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(2));
 
         logger.info("--> start the shards (primaries)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("--> start the shards (replicas)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("--> make sure shards are only allocated on tag1 with value1 and value2");
-        List<ShardRouting> startedShards = clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED);
+        List<ShardRouting> startedShards = clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED);
         assertThat(startedShards.size(), equalTo(4));
         for (ShardRouting startedShard : startedShards) {
             assertThat(startedShard.currentNodeId(), Matchers.anyOf(equalTo("node1"), equalTo("node2")));
@@ -153,14 +153,14 @@ public class FilterRoutingTests extends ESAllocationTestCase {
         clusterState = ClusterState.builder(clusterState).metaData(metaData).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(2));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.RELOCATING).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.RELOCATING).size(), equalTo(2));
 
         logger.info("--> finish relocation");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        startedShards = clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED);
+        startedShards = clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED);
         assertThat(startedShards.size(), equalTo(4));
         for (ShardRouting startedShard : startedShards) {
             assertThat(startedShard.currentNodeId(), Matchers.anyOf(equalTo("node1"), equalTo("node4")));
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/IndexBalanceTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/IndexBalanceTests.java
index c48cdda..aece576 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/IndexBalanceTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/IndexBalanceTests.java
@@ -108,7 +108,7 @@ public class IndexBalanceTests extends ESAllocationTestCase {
 
         assertThat(prevRoutingTable == routingTable, equalTo(true));
 
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -132,11 +132,11 @@ public class IndexBalanceTests extends ESAllocationTestCase {
         assertThat(prevRoutingTable == routingTable, equalTo(true));
 
         logger.info("Start the more shards");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(prevRoutingTable != routingTable, equalTo(true));
         assertThat(routingTable.index("test").shards().size(), equalTo(3));
@@ -150,7 +150,7 @@ public class IndexBalanceTests extends ESAllocationTestCase {
 
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(prevRoutingTable != routingTable, equalTo(true));
         assertThat(routingTable.index("test1").shards().size(), equalTo(3));
@@ -240,7 +240,7 @@ public class IndexBalanceTests extends ESAllocationTestCase {
         assertThat(prevRoutingTable == routingTable, equalTo(true));
 
         logger.info("Start the primary shard");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -264,11 +264,11 @@ public class IndexBalanceTests extends ESAllocationTestCase {
         assertThat(prevRoutingTable == routingTable, equalTo(true));
 
         logger.info("Start the backup shard");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(prevRoutingTable != routingTable, equalTo(true));
         assertThat(routingTable.index("test").shards().size(), equalTo(3));
@@ -282,7 +282,7 @@ public class IndexBalanceTests extends ESAllocationTestCase {
 
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(prevRoutingTable != routingTable, equalTo(true));
         assertThat(routingTable.index("test1").shards().size(), equalTo(3));
@@ -309,18 +309,18 @@ public class IndexBalanceTests extends ESAllocationTestCase {
         assertThat(prevRoutingTable == routingTable, equalTo(true));
 
         logger.info("Start the backup shard");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(prevRoutingTable != routingTable, equalTo(true));
         assertThat(routingTable.index("test").shards().size(), equalTo(3));
 
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(prevRoutingTable != routingTable, equalTo(true));
         assertThat(routingTable.index("test1").shards().size(), equalTo(3));
@@ -391,7 +391,7 @@ public class IndexBalanceTests extends ESAllocationTestCase {
 
         assertThat(prevRoutingTable == routingTable, equalTo(true));
 
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -415,11 +415,11 @@ public class IndexBalanceTests extends ESAllocationTestCase {
         assertThat(prevRoutingTable == routingTable, equalTo(true));
 
         logger.info("Start the more shards");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         assertThat(prevRoutingTable != routingTable, equalTo(true));
         assertThat(routingTable.index("test").shards().size(), equalTo(3));
         for (int i = 0; i < routingTable.index("test").shards().size(); i++) {
@@ -431,7 +431,7 @@ public class IndexBalanceTests extends ESAllocationTestCase {
 
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(prevRoutingTable != routingTable, equalTo(true));
 
@@ -483,7 +483,7 @@ public class IndexBalanceTests extends ESAllocationTestCase {
 
         assertThat(prevRoutingTable == routingTable, equalTo(true));
 
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -507,11 +507,11 @@ public class IndexBalanceTests extends ESAllocationTestCase {
         assertThat(prevRoutingTable == routingTable, equalTo(true));
 
         logger.info("Start the more shards");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         assertThat(prevRoutingTable != routingTable, equalTo(true));
         assertThat(routingTable.index("test1").shards().size(), equalTo(3));
         for (int i = 0; i < routingTable.index("test1").shards().size(); i++) {
@@ -523,7 +523,7 @@ public class IndexBalanceTests extends ESAllocationTestCase {
 
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(prevRoutingTable != routingTable, equalTo(true));
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/NodeVersionAllocationDeciderTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/NodeVersionAllocationDeciderTests.java
index 94df9af..344d6b9 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/NodeVersionAllocationDeciderTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/NodeVersionAllocationDeciderTests.java
@@ -97,11 +97,11 @@ public class NodeVersionAllocationDeciderTests extends ESAllocationTestCase {
         }
 
         logger.info("start all the primary shards, replicas will start initializing");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (int i = 0; i < routingTable.index("test").shards().size(); i++) {
             assertThat(routingTable.index("test").shard(i).shards().size(), equalTo(3));
@@ -110,11 +110,11 @@ public class NodeVersionAllocationDeciderTests extends ESAllocationTestCase {
             assertThat(routingTable.index("test").shard(i).replicaShardsWithState(UNASSIGNED).size(), equalTo(1));
         }
 
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (int i = 0; i < routingTable.index("test").shards().size(); i++) {
             assertThat(routingTable.index("test").shard(i).shards().size(), equalTo(3));
@@ -129,7 +129,7 @@ public class NodeVersionAllocationDeciderTests extends ESAllocationTestCase {
         prevRoutingTable = routingTable;
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (int i = 0; i < routingTable.index("test").shards().size(); i++) {
             assertThat(routingTable.index("test").shard(i).shards().size(), equalTo(3));
@@ -145,7 +145,7 @@ public class NodeVersionAllocationDeciderTests extends ESAllocationTestCase {
         prevRoutingTable = routingTable;
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (int i = 0; i < routingTable.index("test").shards().size(); i++) {
             assertThat(routingTable.index("test").shard(i).shards().size(), equalTo(3));
@@ -154,11 +154,11 @@ public class NodeVersionAllocationDeciderTests extends ESAllocationTestCase {
             assertThat(routingTable.index("test").shard(i).replicaShardsWithState(INITIALIZING).size(), equalTo(1));
         }
 
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (int i = 0; i < routingTable.index("test").shards().size(); i++) {
             assertThat(routingTable.index("test").shard(i).shards().size(), equalTo(3));
@@ -287,11 +287,11 @@ public class NodeVersionAllocationDeciderTests extends ESAllocationTestCase {
     }
 
     private ClusterState stabilize(ClusterState clusterState, AllocationService service) {
-        logger.trace("RoutingNodes: {}", clusterState.routingNodes().prettyPrint());
+        logger.trace("RoutingNodes: {}", clusterState.getRoutingNodes().prettyPrint());
 
         RoutingTable routingTable = service.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         assertRecoveryNodeVersions(routingNodes);
 
         logger.info("complete rebalancing");
@@ -301,7 +301,7 @@ public class NodeVersionAllocationDeciderTests extends ESAllocationTestCase {
             logger.trace("RoutingNodes: {}", clusterState.getRoutingNodes().prettyPrint());
             routingTable = service.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
             clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-            routingNodes = clusterState.routingNodes();
+            routingNodes = clusterState.getRoutingNodes();
             if (stable = (routingTable == prev)) {
                 break;
             }
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferLocalPrimariesToRelocatingPrimariesTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferLocalPrimariesToRelocatingPrimariesTests.java
index f400513..f307b63 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferLocalPrimariesToRelocatingPrimariesTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferLocalPrimariesToRelocatingPrimariesTests.java
@@ -71,8 +71,8 @@ public class PreferLocalPrimariesToRelocatingPrimariesTests extends ESAllocation
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        while (!clusterState.routingNodes().shardsWithState(INITIALIZING).isEmpty()) {
-            routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        while (!clusterState.getRoutingNodes().shardsWithState(INITIALIZING).isEmpty()) {
+            routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
             clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
         }
 
@@ -95,8 +95,8 @@ public class PreferLocalPrimariesToRelocatingPrimariesTests extends ESAllocation
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("[{}] primaries should be still started but [{}] other primaries should be unassigned", numberOfShards, numberOfShards);
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(numberOfShards));
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(numberOfShards));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
         assertThat(clusterState.routingTable().shardsWithState(UNASSIGNED).size(), equalTo(numberOfShards));
 
         logger.info("start node back up");
@@ -105,17 +105,17 @@ public class PreferLocalPrimariesToRelocatingPrimariesTests extends ESAllocation
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        while (clusterState.routingNodes().shardsWithState(STARTED).size() < totalNumberOfShards) {
+        while (clusterState.getRoutingNodes().shardsWithState(STARTED).size() < totalNumberOfShards) {
             int localInitializations = 0;
             int relocatingInitializations = 0;
-            for (ShardRouting routing : clusterState.routingNodes().shardsWithState(INITIALIZING)) {
+            for (ShardRouting routing : clusterState.getRoutingNodes().shardsWithState(INITIALIZING)) {
                 if (routing.relocatingNodeId() == null) {
                     localInitializations++;
                 } else {
                     relocatingInitializations++;
                 }
             }
-            int needToInitialize = totalNumberOfShards - clusterState.routingNodes().shardsWithState(STARTED).size() - clusterState.routingNodes().shardsWithState(RELOCATING).size();
+            int needToInitialize = totalNumberOfShards - clusterState.getRoutingNodes().shardsWithState(STARTED).size() - clusterState.getRoutingNodes().shardsWithState(RELOCATING).size();
             logger.info("local initializations: [{}], relocating: [{}], need to initialize: {}", localInitializations, relocatingInitializations, needToInitialize);
             assertThat(localInitializations, equalTo(Math.min(primaryRecoveries, needToInitialize)));
             clusterState = startRandomInitializingShard(clusterState, strategy);
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferPrimaryAllocationTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferPrimaryAllocationTests.java
index be0dde8..108281e 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferPrimaryAllocationTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferPrimaryAllocationTests.java
@@ -67,8 +67,8 @@ public class PreferPrimaryAllocationTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        while (!clusterState.routingNodes().shardsWithState(INITIALIZING).isEmpty()) {
-            routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        while (!clusterState.getRoutingNodes().shardsWithState(INITIALIZING).isEmpty()) {
+            routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
             clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
         }
 
@@ -81,7 +81,7 @@ public class PreferPrimaryAllocationTests extends ESAllocationTestCase {
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("2 replicas should be initializing now for the existing indices (we throttle to 1)");
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(2));
 
         logger.info("create a new index");
         metaData = MetaData.builder(clusterState.metaData())
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryElectionRoutingTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryElectionRoutingTests.java
index 8e59326..69824f1 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryElectionRoutingTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryElectionRoutingTests.java
@@ -67,13 +67,13 @@ public class PrimaryElectionRoutingTests extends ESAllocationTestCase {
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("Start the primary shard (on node1)");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.node("node1").shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("Start the backup shard (on node2)");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.node("node2").shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -83,7 +83,7 @@ public class PrimaryElectionRoutingTests extends ESAllocationTestCase {
         prevRoutingTable = routingTable;
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(prevRoutingTable != routingTable, equalTo(true));
         assertThat(routingTable.index("test").shards().size(), equalTo(1));
@@ -117,10 +117,10 @@ public class PrimaryElectionRoutingTests extends ESAllocationTestCase {
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
 
         logger.info("Start the primary shards");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         rerouteResult = allocation.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING));
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(routingNodes.shardsWithState(STARTED).size(), equalTo(2));
         assertThat(routingNodes.shardsWithState(INITIALIZING).size(), equalTo(2));
@@ -134,7 +134,7 @@ public class PrimaryElectionRoutingTests extends ESAllocationTestCase {
         ).build();
         rerouteResult = allocation.reroute(clusterState);
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(routingNodes.shardsWithState(STARTED).size(), equalTo(1));
         assertThat(routingNodes.shardsWithState(INITIALIZING).size(), equalTo(1));
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryNotRelocatedWhileBeingRecoveredTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryNotRelocatedWhileBeingRecoveredTests.java
index 3202a82..c295b4f 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryNotRelocatedWhileBeingRecoveredTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryNotRelocatedWhileBeingRecoveredTests.java
@@ -69,7 +69,7 @@ public class PrimaryNotRelocatedWhileBeingRecoveredTests extends ESAllocationTes
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("Start the primary shard (on node1)");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.node("node1").shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/RandomAllocationDeciderTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/RandomAllocationDeciderTests.java
index 5ddb878..4c82860 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/RandomAllocationDeciderTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/RandomAllocationDeciderTests.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.cluster.routing.allocation;
 
 import org.elasticsearch.Version;
-import org.elasticsearch.cluster.ClusterInfoService;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.EmptyClusterInfoService;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
@@ -111,8 +110,8 @@ public class RandomAllocationDeciderTests extends ESAllocationTestCase {
             clusterState = stateBuilder.build();
             routingTable = strategy.reroute(clusterState).routingTable();
             clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-            if (clusterState.routingNodes().shardsWithState(INITIALIZING).size() > 0) {
-                routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING))
+            if (clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size() > 0) {
+                routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING))
                         .routingTable();
                 clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
             }
@@ -137,20 +136,20 @@ public class RandomAllocationDeciderTests extends ESAllocationTestCase {
             iterations++;
             routingTable = strategy.reroute(clusterState).routingTable();
             clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-            if (clusterState.routingNodes().shardsWithState(INITIALIZING).size() > 0) {
-                routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING))
+            if (clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size() > 0) {
+                routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING))
                         .routingTable();
                 clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
             }
 
-        } while (clusterState.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size() != 0 ||
-                clusterState.routingNodes().shardsWithState(ShardRoutingState.UNASSIGNED).size() != 0 && iterations < 200);
+        } while (clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size() != 0 ||
+                clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.UNASSIGNED).size() != 0 && iterations < 200);
         logger.info("Done Balancing after [{}] iterations", iterations);
         // we stop after 200 iterations if it didn't stabelize by then something is likely to be wrong
         assertThat("max num iteration exceeded", iterations, Matchers.lessThan(200));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(0));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.UNASSIGNED).size(), equalTo(0));
-        int shards = clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size();
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.UNASSIGNED).size(), equalTo(0));
+        int shards = clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size();
         assertThat(shards, equalTo(totalNumShards));
         final int numNodes = clusterState.nodes().size();
         final int upperBound = (int) Math.round(((shards / numNodes) * 1.10));
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/RebalanceAfterActiveTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/RebalanceAfterActiveTests.java
index 339f2e6..4b1b08e 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/RebalanceAfterActiveTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/RebalanceAfterActiveTests.java
@@ -87,11 +87,11 @@ public class RebalanceAfterActiveTests extends ESAllocationTestCase {
         }
 
         logger.info("start all the primary shards, replicas will start initializing");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (int i = 0; i < routingTable.index("test").shards().size(); i++) {
             assertThat(routingTable.index("test").shard(i).shards().size(), equalTo(2));
@@ -106,7 +106,7 @@ public class RebalanceAfterActiveTests extends ESAllocationTestCase {
         prevRoutingTable = routingTable;
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (int i = 0; i < routingTable.index("test").shards().size(); i++) {
             assertThat(routingTable.index("test").shard(i).shards().size(), equalTo(2));
@@ -115,33 +115,33 @@ public class RebalanceAfterActiveTests extends ESAllocationTestCase {
         }
 
         logger.info("start the replica shards, rebalancing should start");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         // we only allow one relocation at a time
         assertThat(routingTable.shardsWithState(STARTED).size(), equalTo(5));
         assertThat(routingTable.shardsWithState(RELOCATING).size(), equalTo(5));
 
         logger.info("complete relocation, other half of relocation should happen");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         // we now only relocate 3, since 2 remain where they are!
         assertThat(routingTable.shardsWithState(STARTED).size(), equalTo(7));
         assertThat(routingTable.shardsWithState(RELOCATING).size(), equalTo(3));
 
         logger.info("complete relocation, thats it!");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(routingTable.shardsWithState(STARTED).size(), equalTo(10));
         // make sure we have an even relocation
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ReplicaAllocatedAfterPrimaryTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ReplicaAllocatedAfterPrimaryTests.java
index e5b2302..5444058 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ReplicaAllocatedAfterPrimaryTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ReplicaAllocatedAfterPrimaryTests.java
@@ -86,7 +86,7 @@ public class ReplicaAllocatedAfterPrimaryTests extends ESAllocationTestCase {
         assertThat(routingTable.index("test").shard(0).replicaShards().get(0).currentNodeId(), nullValue());
 
         logger.info("Start all the primary shards");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.node(nodeHoldingPrimary).shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingNodesIntegrityTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingNodesIntegrityTests.java
index 885132b..9800966 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingNodesIntegrityTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingNodesIntegrityTests.java
@@ -61,12 +61,12 @@ public class RoutingNodesIntegrityTests extends ESAllocationTestCase {
         RoutingTable routingTable = RoutingTable.builder().addAsNew(metaData.index("test")).addAsNew(metaData.index("test1")).build();
 
         ClusterState clusterState = ClusterState.builder(org.elasticsearch.cluster.ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable).build();
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
 
         logger.info("Adding three node and performing rerouting");
         clusterState = ClusterState.builder(clusterState)
                 .nodes(DiscoveryNodes.builder().put(newNode("node1")).put(newNode("node2")).put(newNode("node3"))).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(assertShardStats(routingNodes), equalTo(true));
         // all shards are unassigned. so no inactive shards or primaries.
@@ -77,7 +77,7 @@ public class RoutingNodesIntegrityTests extends ESAllocationTestCase {
         RoutingTable prevRoutingTable = routingTable;
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(assertShardStats(routingNodes), equalTo(true));
         assertThat(routingNodes.hasInactiveShards(), equalTo(true));
@@ -90,22 +90,22 @@ public class RoutingNodesIntegrityTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         logger.info("Reroute, nothing should change");
         prevRoutingTable = routingTable;
         routingTable = strategy.reroute(clusterState).routingTable();
 
         logger.info("Start the more shards");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(assertShardStats(routingNodes), equalTo(true));
         assertThat(routingNodes.hasInactiveShards(), equalTo(false));
@@ -114,7 +114,7 @@ public class RoutingNodesIntegrityTests extends ESAllocationTestCase {
 
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
     }
 
@@ -150,7 +150,7 @@ public class RoutingNodesIntegrityTests extends ESAllocationTestCase {
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("Start the primary shard");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -160,15 +160,15 @@ public class RoutingNodesIntegrityTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
 
         logger.info("Start the backup shard");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         logger.info("Add another node and perform rerouting, nothing will happen since primary not started");
         clusterState = ClusterState.builder(clusterState)
@@ -182,18 +182,18 @@ public class RoutingNodesIntegrityTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
 
         logger.info("Start the backup shard");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(prevRoutingTable != routingTable, equalTo(true));
         assertThat(routingTable.index("test").shards().size(), equalTo(3));
 
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(prevRoutingTable != routingTable, equalTo(true));
         assertThat(routingTable.index("test1").shards().size(), equalTo(3));
@@ -231,7 +231,7 @@ public class RoutingNodesIntegrityTests extends ESAllocationTestCase {
         clusterState = ClusterState.builder(clusterState)
                 .nodes(DiscoveryNodes.builder().put(newNode("node1")).put(newNode("node2")).put(newNode("node3"))).build();
 
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         assertThat(assertShardStats(routingNodes), equalTo(true));
         assertThat(routingNodes.hasInactiveShards(), equalTo(false));
         assertThat(routingNodes.hasInactivePrimaries(), equalTo(false));
@@ -240,7 +240,7 @@ public class RoutingNodesIntegrityTests extends ESAllocationTestCase {
         RoutingTable prevRoutingTable = routingTable;
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(assertShardStats(routingNodes), equalTo(true));
         assertThat(routingNodes.hasInactiveShards(), equalTo(true));
@@ -255,7 +255,7 @@ public class RoutingNodesIntegrityTests extends ESAllocationTestCase {
 
         assertThat(prevRoutingTable == routingTable, equalTo(true));
 
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         assertThat(routingNodes.node("node1").numberOfShardsWithState(INITIALIZING), equalTo(1));
         assertThat(routingNodes.node("node2").numberOfShardsWithState(INITIALIZING), equalTo(1));
         assertThat(routingNodes.node("node3").numberOfShardsWithState(INITIALIZING), equalTo(1));
@@ -263,7 +263,7 @@ public class RoutingNodesIntegrityTests extends ESAllocationTestCase {
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(assertShardStats(routingNodes), equalTo(true));
         assertThat(routingNodes.hasInactiveShards(), equalTo(true));
@@ -279,11 +279,11 @@ public class RoutingNodesIntegrityTests extends ESAllocationTestCase {
         assertThat(prevRoutingTable == routingTable, equalTo(true));
 
         logger.info("Start the more shards");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(assertShardStats(routingNodes), equalTo(true));
         assertThat(routingNodes.hasInactiveShards(), equalTo(false));
@@ -311,7 +311,7 @@ public class RoutingNodesIntegrityTests extends ESAllocationTestCase {
                 .addAsNew(metaData.index("test1"))
                 .build();
         clusterState = ClusterState.builder(clusterState).metaData(metaData).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(assertShardStats(routingNodes), equalTo(true));
         assertThat(routingNodes.hasInactiveShards(), equalTo(false));
@@ -329,7 +329,7 @@ public class RoutingNodesIntegrityTests extends ESAllocationTestCase {
         prevRoutingTable = routingTable;
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(assertShardStats(routingNodes), equalTo(true));
         assertThat(routingNodes.hasInactiveShards(), equalTo(true));
@@ -339,11 +339,11 @@ public class RoutingNodesIntegrityTests extends ESAllocationTestCase {
         assertThat(prevRoutingTable == routingTable, equalTo(true));
 
         logger.info("Reroute, start the primaries");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(assertShardStats(routingNodes), equalTo(true));
         assertThat(routingNodes.hasInactiveShards(), equalTo(true));
@@ -351,11 +351,11 @@ public class RoutingNodesIntegrityTests extends ESAllocationTestCase {
         assertThat(routingNodes.hasUnassignedPrimaries(), equalTo(false));
 
         logger.info("Reroute, start the replicas");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(assertShardStats(routingNodes), equalTo(true));
         assertThat(routingNodes.hasInactiveShards(), equalTo(false));
@@ -376,7 +376,7 @@ public class RoutingNodesIntegrityTests extends ESAllocationTestCase {
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes()).remove(indexShardRoutingTable.primaryShard().currentNodeId())).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(assertShardStats(routingNodes), equalTo(true));
         assertThat(routingNodes.hasInactiveShards(), equalTo(true));
@@ -385,11 +385,11 @@ public class RoutingNodesIntegrityTests extends ESAllocationTestCase {
         assertThat(routingNodes.hasUnassignedPrimaries(), equalTo(false));
 
         logger.info("Start Recovering shards round 1");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(assertShardStats(routingNodes), equalTo(true));
         assertThat(routingNodes.hasInactiveShards(), equalTo(true));
@@ -397,11 +397,11 @@ public class RoutingNodesIntegrityTests extends ESAllocationTestCase {
         assertThat(routingNodes.hasUnassignedPrimaries(), equalTo(false));
 
         logger.info("Start Recovering shards round 2");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(assertShardStats(routingNodes), equalTo(true));
         assertThat(routingNodes.hasInactiveShards(), equalTo(false));
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/SameShardRoutingTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/SameShardRoutingTests.java
index 9896116..5213797 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/SameShardRoutingTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/SameShardRoutingTests.java
@@ -67,14 +67,14 @@ public class SameShardRoutingTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(numberOfShardsOfType(clusterState.readOnlyRoutingNodes(), ShardRoutingState.INITIALIZING), equalTo(2));
+        assertThat(numberOfShardsOfType(clusterState.getRoutingNodes(), ShardRoutingState.INITIALIZING), equalTo(2));
 
         logger.info("--> start all primary shards, no replica will be started since its on the same host");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.readOnlyRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(numberOfShardsOfType(clusterState.readOnlyRoutingNodes(), ShardRoutingState.STARTED), equalTo(2));
-        assertThat(numberOfShardsOfType(clusterState.readOnlyRoutingNodes(), ShardRoutingState.INITIALIZING), equalTo(0));
+        assertThat(numberOfShardsOfType(clusterState.getRoutingNodes(), ShardRoutingState.STARTED), equalTo(2));
+        assertThat(numberOfShardsOfType(clusterState.getRoutingNodes(), ShardRoutingState.INITIALIZING), equalTo(0));
 
         logger.info("--> add another node, with a different host, replicas will be allocating");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
@@ -82,9 +82,9 @@ public class SameShardRoutingTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(numberOfShardsOfType(clusterState.readOnlyRoutingNodes(), ShardRoutingState.STARTED), equalTo(2));
-        assertThat(numberOfShardsOfType(clusterState.readOnlyRoutingNodes(), ShardRoutingState.INITIALIZING), equalTo(2));
-        for (ShardRouting shardRouting : clusterState.readOnlyRoutingNodes().shardsWithState(INITIALIZING)) {
+        assertThat(numberOfShardsOfType(clusterState.getRoutingNodes(), ShardRoutingState.STARTED), equalTo(2));
+        assertThat(numberOfShardsOfType(clusterState.getRoutingNodes(), ShardRoutingState.INITIALIZING), equalTo(2));
+        for (ShardRouting shardRouting : clusterState.getRoutingNodes().shardsWithState(INITIALIZING)) {
             assertThat(shardRouting.currentNodeId(), equalTo("node3"));
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardVersioningTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardVersioningTests.java
index 8568948..1074f8b 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardVersioningTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardVersioningTests.java
@@ -78,11 +78,11 @@ public class ShardVersioningTests extends ESAllocationTestCase {
         }
 
         logger.info("start all the primary shards for test1, replicas will start initializing");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState("test1", INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         for (int i = 0; i < routingTable.index("test1").shards().size(); i++) {
             assertThat(routingTable.index("test1").shard(i).shards().size(), equalTo(2));
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardsLimitAllocationTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardsLimitAllocationTests.java
index cbd6233..c59e90f 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardsLimitAllocationTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardsLimitAllocationTests.java
@@ -68,22 +68,22 @@ public class ShardsLimitAllocationTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.readOnlyRoutingNodes().node("node1").numberOfShardsWithState(ShardRoutingState.INITIALIZING), equalTo(2));
-        assertThat(clusterState.readOnlyRoutingNodes().node("node2").numberOfShardsWithState(ShardRoutingState.INITIALIZING), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().node("node1").numberOfShardsWithState(ShardRoutingState.INITIALIZING), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().node("node2").numberOfShardsWithState(ShardRoutingState.INITIALIZING), equalTo(2));
 
         logger.info("Start the primary shards");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.readOnlyRoutingNodes().node("node1").numberOfShardsWithState(ShardRoutingState.STARTED), equalTo(2));
-        assertThat(clusterState.readOnlyRoutingNodes().node("node1").numberOfShardsWithState(ShardRoutingState.INITIALIZING), equalTo(0));
-        assertThat(clusterState.readOnlyRoutingNodes().node("node2").numberOfShardsWithState(ShardRoutingState.STARTED), equalTo(2));
-        assertThat(clusterState.readOnlyRoutingNodes().node("node2").numberOfShardsWithState(ShardRoutingState.INITIALIZING), equalTo(0));
-        assertThat(clusterState.readOnlyRoutingNodes().unassigned().size(), equalTo(4));
+        assertThat(clusterState.getRoutingNodes().node("node1").numberOfShardsWithState(ShardRoutingState.STARTED), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().node("node1").numberOfShardsWithState(ShardRoutingState.INITIALIZING), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().node("node2").numberOfShardsWithState(ShardRoutingState.STARTED), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().node("node2").numberOfShardsWithState(ShardRoutingState.INITIALIZING), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().unassigned().size(), equalTo(4));
 
         logger.info("Do another reroute, make sure its still not allocated");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
     }
@@ -119,11 +119,11 @@ public class ShardsLimitAllocationTests extends ESAllocationTestCase {
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("Start the primary shards");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(numberOfShardsOfType(clusterState.readOnlyRoutingNodes(), STARTED), equalTo(5));
+        assertThat(numberOfShardsOfType(clusterState.getRoutingNodes(), STARTED), equalTo(5));
 
         logger.info("add another index with 5 shards");
         metaData = MetaData.builder(metaData)
@@ -143,16 +143,16 @@ public class ShardsLimitAllocationTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(numberOfShardsOfType(clusterState.readOnlyRoutingNodes(), STARTED), equalTo(10));
+        assertThat(numberOfShardsOfType(clusterState.getRoutingNodes(), STARTED), equalTo(10));
 
-        for (ShardRouting shardRouting : clusterState.readOnlyRoutingNodes().node("node1")) {
+        for (ShardRouting shardRouting : clusterState.getRoutingNodes().node("node1")) {
             assertThat(shardRouting.index(), equalTo("test"));
         }
-        for (ShardRouting shardRouting : clusterState.readOnlyRoutingNodes().node("node2")) {
+        for (ShardRouting shardRouting : clusterState.getRoutingNodes().node("node2")) {
             assertThat(shardRouting.index(), equalTo("test1"));
         }
 
@@ -172,17 +172,17 @@ public class ShardsLimitAllocationTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.readOnlyRoutingNodes().node("node1").numberOfShardsWithState(STARTED), equalTo(3));
-        assertThat(clusterState.readOnlyRoutingNodes().node("node1").numberOfShardsWithState(RELOCATING), equalTo(2));
-        assertThat(clusterState.readOnlyRoutingNodes().node("node2").numberOfShardsWithState(RELOCATING), equalTo(2));
-        assertThat(clusterState.readOnlyRoutingNodes().node("node2").numberOfShardsWithState(STARTED), equalTo(3));
+        assertThat(clusterState.getRoutingNodes().node("node1").numberOfShardsWithState(STARTED), equalTo(3));
+        assertThat(clusterState.getRoutingNodes().node("node1").numberOfShardsWithState(RELOCATING), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().node("node2").numberOfShardsWithState(RELOCATING), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().node("node2").numberOfShardsWithState(STARTED), equalTo(3));
         // the first move will destroy the balance and the balancer will move 2 shards from node2 to node one right after
         // moving the nodes to node2 since we consider INITIALIZING nodes during rebalance
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
         // now we are done compared to EvenShardCountAllocator since the Balancer is not soely based on the average 
-        assertThat(clusterState.readOnlyRoutingNodes().node("node1").numberOfShardsWithState(STARTED), equalTo(5));
-        assertThat(clusterState.readOnlyRoutingNodes().node("node2").numberOfShardsWithState(STARTED), equalTo(5));
+        assertThat(clusterState.getRoutingNodes().node("node1").numberOfShardsWithState(STARTED), equalTo(5));
+        assertThat(clusterState.getRoutingNodes().node("node2").numberOfShardsWithState(STARTED), equalTo(5));
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardNoReplicasRoutingTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardNoReplicasRoutingTests.java
index 6f8027f..b2bd29d 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardNoReplicasRoutingTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardNoReplicasRoutingTests.java
@@ -94,7 +94,7 @@ public class SingleShardNoReplicasRoutingTests extends ESAllocationTestCase {
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("Marking the shard as started");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.node("node1").shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -141,7 +141,7 @@ public class SingleShardNoReplicasRoutingTests extends ESAllocationTestCase {
         assertThat(routingTable == prevRoutingTable, equalTo(true));
 
         logger.info("Start the shard on node 2");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.node("node2").shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -191,7 +191,7 @@ public class SingleShardNoReplicasRoutingTests extends ESAllocationTestCase {
         assertThat(routingTable.index("test").shard(0).shards().get(0).currentNodeId(), equalTo("node1"));
 
         logger.info("Marking the shard as failed");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyFailedShard(clusterState, routingNodes.node("node1").shardsWithState(INITIALIZING).get(0)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -261,7 +261,7 @@ public class SingleShardNoReplicasRoutingTests extends ESAllocationTestCase {
             int nodeIndex = Integer.parseInt(nodeId.substring("node".length()));
             assertThat(nodeIndex, lessThan(25));
         }
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         Set<String> encounteredIndices = newHashSet();
         for (RoutingNode routingNode : routingNodes) {
             assertThat(routingNode.numberOfShardsWithState(STARTED), equalTo(0));
@@ -359,7 +359,7 @@ public class SingleShardNoReplicasRoutingTests extends ESAllocationTestCase {
             assertThat(routingTable.index("test" + i).shard(0).shards().size(), equalTo(1));
             assertThat(routingTable.index("test" + i).shard(0).shards().get(0).state(), equalTo(INITIALIZING));
         }
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         assertThat(numberOfShardsOfType(routingNodes, INITIALIZING), equalTo(numberOfIndices));
         assertThat(routingNodes.node("node1").numberOfShardsWithState(INITIALIZING), anyOf(equalTo(3), equalTo(4)));
         assertThat(routingNodes.node("node2").numberOfShardsWithState(INITIALIZING), anyOf(equalTo(3), equalTo(4)));
@@ -376,7 +376,7 @@ public class SingleShardNoReplicasRoutingTests extends ESAllocationTestCase {
 
         assertThat(prevRoutingTable == routingTable, equalTo(true));
 
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -388,7 +388,7 @@ public class SingleShardNoReplicasRoutingTests extends ESAllocationTestCase {
             assertThat(routingTable.index("test" + i).shard(0).shards().size(), equalTo(1));
             assertThat(routingTable.index("test" + i).shard(0).shards().get(0).state(), anyOf(equalTo(RELOCATING), equalTo(STARTED)));
         }
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         assertThat("4 source shard routing are relocating", numberOfShardsOfType(routingNodes, RELOCATING), equalTo(4));
         assertThat("4 target shard routing are initializing", numberOfShardsOfType(routingNodes, INITIALIZING), equalTo(4));
 
@@ -405,7 +405,7 @@ public class SingleShardNoReplicasRoutingTests extends ESAllocationTestCase {
             assertThat(routingTable.index("test" + i).shard(0).shards().size(), equalTo(1));
             assertThat(routingTable.index("test" + i).shard(0).shards().get(0).state(), anyOf(equalTo(RELOCATING), equalTo(STARTED)));
         }
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         assertThat(numberOfShardsOfType(routingNodes, STARTED), equalTo(numberOfIndices));
         for (RoutingNode routingNode : routingNodes) {
             assertThat(routingNode.numberOfShardsWithState(STARTED), equalTo(2));
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardOneReplicaRoutingTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardOneReplicaRoutingTests.java
index de6a1c6..7ea7e1c 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardOneReplicaRoutingTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardOneReplicaRoutingTests.java
@@ -93,7 +93,7 @@ public class SingleShardOneReplicaRoutingTests extends ESAllocationTestCase {
         assertThat(prevRoutingTable == routingTable, equalTo(true));
 
         logger.info("Start the primary shard (on node1)");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.node("node1").shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -116,7 +116,7 @@ public class SingleShardOneReplicaRoutingTests extends ESAllocationTestCase {
         assertThat(prevRoutingTable == routingTable, equalTo(true));
 
         logger.info("Start the backup shard");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.node("node2").shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/TenShardsOneReplicaRoutingTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/TenShardsOneReplicaRoutingTests.java
index 2a80268..d6e7c86 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/TenShardsOneReplicaRoutingTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/TenShardsOneReplicaRoutingTests.java
@@ -105,7 +105,7 @@ public class TenShardsOneReplicaRoutingTests extends ESAllocationTestCase {
         assertThat(prevRoutingTable == routingTable, equalTo(true));
 
         logger.info("Start the primary shard (on node1)");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.node("node1").shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -129,11 +129,11 @@ public class TenShardsOneReplicaRoutingTests extends ESAllocationTestCase {
         assertThat(prevRoutingTable == routingTable, equalTo(true));
 
         logger.info("Start the backup shard");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.node("node2").shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(prevRoutingTable != routingTable, equalTo(true));
         assertThat(routingTable.index("test").shards().size(), equalTo(10));
@@ -154,7 +154,7 @@ public class TenShardsOneReplicaRoutingTests extends ESAllocationTestCase {
         prevRoutingTable = routingTable;
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(prevRoutingTable != routingTable, equalTo(true));
         assertThat(routingTable.index("test").shards().size(), equalTo(10));
@@ -165,11 +165,11 @@ public class TenShardsOneReplicaRoutingTests extends ESAllocationTestCase {
         assertThat(routingNodes.node("node3").numberOfShardsWithState(INITIALIZING), equalTo(6));
 
         logger.info("Start the shards on node 3");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.node("node3").shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
 
         assertThat(prevRoutingTable != routingTable, equalTo(true));
         assertThat(routingTable.index("test").shards().size(), equalTo(10));
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/UpdateNumberOfReplicasTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/UpdateNumberOfReplicasTests.java
index 0d2d513..67b81be 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/UpdateNumberOfReplicasTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/UpdateNumberOfReplicasTests.java
@@ -75,13 +75,13 @@ public class UpdateNumberOfReplicasTests extends ESAllocationTestCase {
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("Start all the primary shards");
-        RoutingNodes routingNodes = clusterState.routingNodes();
+        RoutingNodes routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("Start all the replica shards");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -100,7 +100,7 @@ public class UpdateNumberOfReplicasTests extends ESAllocationTestCase {
 
 
         logger.info("add another replica");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = RoutingTable.builder(routingTable).updateNumberOfReplicas(2).build();
         metaData = MetaData.builder(clusterState.metaData()).updateNumberOfReplicas(2).build();
@@ -135,7 +135,7 @@ public class UpdateNumberOfReplicasTests extends ESAllocationTestCase {
         assertThat(routingTable.index("test").shard(0).replicaShardsWithState(INITIALIZING).size(), equalTo(1));
         assertThat(routingTable.index("test").shard(0).replicaShardsWithState(INITIALIZING).get(0).currentNodeId(), equalTo("node3"));
 
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = strategy.applyStartedShards(clusterState, routingNodes.shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -151,7 +151,7 @@ public class UpdateNumberOfReplicasTests extends ESAllocationTestCase {
         assertThat(routingTable.index("test").shard(0).replicaShardsWithState(STARTED).get(1).currentNodeId(), anyOf(equalTo(nodeHoldingReplica), equalTo("node3")));
 
         logger.info("now remove a replica");
-        routingNodes = clusterState.routingNodes();
+        routingNodes = clusterState.getRoutingNodes();
         prevRoutingTable = routingTable;
         routingTable = RoutingTable.builder(routingTable).updateNumberOfReplicas(1).build();
         metaData = MetaData.builder(clusterState.metaData()).updateNumberOfReplicas(1).build();
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java
index 52280eb..41e995c 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java
@@ -119,25 +119,25 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
         logShardStates(clusterState);
 
         // Primary shard should be initializing, replica should not
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
 
         logger.info("--> start the shards (primaries)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logShardStates(clusterState);
         // Assert that we're able to start the primary
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(1));
         // Assert that node1 didn't get any shards because its disk usage is too high
         assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(0));
 
         logger.info("--> start the shards (replicas)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logShardStates(clusterState);
         // Assert that the replica couldn't be started since node1 doesn't have enough space
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(1));
 
         logger.info("--> adding node3");
 
@@ -149,16 +149,16 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
 
         logShardStates(clusterState);
         // Assert that the replica is initialized now that node3 is available with enough space
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(1));
 
         logger.info("--> start the shards (replicas)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logShardStates(clusterState);
         // Assert that the replica couldn't be started since node1 doesn't have enough space
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(2));
         assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(0));
         assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(1));
         assertThat(clusterState.getRoutingNodes().node("node3").size(), equalTo(1));
@@ -189,7 +189,7 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
         logShardStates(clusterState);
 
         // Shards remain started
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(2));
         assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(0));
         assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(1));
         assertThat(clusterState.getRoutingNodes().node("node3").size(), equalTo(1));
@@ -220,7 +220,7 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
 
         logShardStates(clusterState);
         // Shards remain started
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(2));
         assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(0));
         // Shard hasn't been moved off of node2 yet because there's nowhere for it to go
         assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(1));
@@ -236,11 +236,11 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
 
         logShardStates(clusterState);
         // Shards remain started
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
 
         logger.info("--> apply INITIALIZING shards");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logShardStates(clusterState);
@@ -315,7 +315,7 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
         logShardStates(clusterState);
 
         // Primary should initialize, even though both nodes are over the limit initialize
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
 
         String nodeWithPrimary, nodeWithoutPrimary;
         if (clusterState.getRoutingNodes().node("node1").size() == 1) {
@@ -354,15 +354,15 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
         logShardStates(clusterState);
 
         // Now the replica should be able to initialize
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(2));
 
         logger.info("--> start the shards (primaries)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logShardStates(clusterState);
         // Assert that we're able to start the primary and replica, since they were both initializing
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(2));
         // Assert that node1 got a single shard (the primary), even though its disk usage is too high
         assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
         // Assert that node2 got a single shard (a replica)
@@ -381,16 +381,16 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
 
         logShardStates(clusterState);
         // Assert that the replica is initialized now that node3 is available with enough space
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(2));
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(1));
 
         logger.info("--> start the shards (replicas)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logShardStates(clusterState);
         // Assert that all replicas could be started
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(3));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(3));
         assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
         assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(1));
         assertThat(clusterState.getRoutingNodes().node("node3").size(), equalTo(1));
@@ -421,7 +421,7 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
         logShardStates(clusterState);
 
         // Shards remain started
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(3));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(3));
         assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
         assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(1));
         assertThat(clusterState.getRoutingNodes().node("node3").size(), equalTo(1));
@@ -452,7 +452,7 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
 
         logShardStates(clusterState);
         // Shards remain started
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(3));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(3));
         assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
         // Shard hasn't been moved off of node2 yet because there's nowhere for it to go
         assertThat(clusterState.getRoutingNodes().node("node2").size(), equalTo(1));
@@ -468,13 +468,13 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
 
         logShardStates(clusterState);
         // Shards remain started
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(2));
         // One shard is relocating off of node1
-        assertThat(clusterState.routingNodes().shardsWithState(RELOCATING).size(), equalTo(1));
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(RELOCATING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
 
         logger.info("--> apply INITIALIZING shards");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logShardStates(clusterState);
@@ -495,14 +495,14 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
 
         logShardStates(clusterState);
         // Shards remain started on node3 and node4
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(2));
         // One shard is relocating off of node2 now
-        assertThat(clusterState.routingNodes().shardsWithState(RELOCATING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(RELOCATING).size(), equalTo(1));
         // Initializing on node5
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
 
         logger.info("--> apply INITIALIZING shards");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("--> final cluster state:");
@@ -572,14 +572,14 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
         logger.info("--> start the shards (primaries)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
         logShardStates(clusterState);
 
         // Shard can't be allocated to node1 (or node2) because it would cause too much usage
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
         // No shards are started, no nodes have enough disk for allocation
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(0));
     }
 
     @Test
@@ -641,17 +641,17 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
 
         // Shard can be allocated to node1, even though it only has 25% free,
         // because it's a primary that's never been allocated before
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
 
         logger.info("--> start the shards (primaries)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
         logShardStates(clusterState);
 
         // A single shard is started on node1, even though it normally would not
         // be allowed, because it's a primary that hasn't been allocated, and node1
         // is still below the high watermark (unlike node3)
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(1));
         assertThat(clusterState.getRoutingNodes().node("node1").size(), equalTo(1));
     }
 
@@ -748,15 +748,15 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
         logShardStates(clusterState);
 
         // shards should be initializing
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(4));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(4));
 
         logger.info("--> start the shards");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logShardStates(clusterState);
         // Assert that we're able to start the primary and replicas
-        assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(4));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size(), equalTo(4));
 
         logger.info("--> adding node3");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
@@ -898,7 +898,7 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
     }
 
     public void logShardStates(ClusterState state) {
-        RoutingNodes rn = state.routingNodes();
+        RoutingNodes rn = state.getRoutingNodes();
         logger.info("--> counts: total: {}, unassigned: {}, initializing: {}, relocating: {}, started: {}",
                 rn.shards(new Predicate<ShardRouting>() {
                     @Override
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationTests.java
index 4e2be68..3823893 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationTests.java
@@ -81,7 +81,7 @@ public class EnableAllocationTests extends ESAllocationTestCase {
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
 
     }
 
@@ -110,13 +110,13 @@ public class EnableAllocationTests extends ESAllocationTestCase {
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
 
         logger.info("--> start the shards (primaries)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
     }
 
     @Test
@@ -145,17 +145,17 @@ public class EnableAllocationTests extends ESAllocationTestCase {
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(1));
         logger.info("--> start the shards (primaries)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
         logger.info("--> start the shards (replicas)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         logger.info("--> verify only enabled index has been routed");
-        assertThat(clusterState.readOnlyRoutingNodes().shardsWithState("enabled", STARTED).size(), equalTo(2));
-        assertThat(clusterState.readOnlyRoutingNodes().shardsWithState("disabled", STARTED).size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().shardsWithState("enabled", STARTED).size(), equalTo(2));
+        assertThat(clusterState.getRoutingNodes().shardsWithState("disabled", STARTED).size(), equalTo(0));
     }
 
 
@@ -193,17 +193,17 @@ public class EnableAllocationTests extends ESAllocationTestCase {
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(4));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(4));
         logger.info("--> start the shards (primaries)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(4));
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(4));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(4));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(4));
 
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(8));
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(8));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
 
         logger.info("--> adding one nodes and do rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
@@ -214,8 +214,8 @@ public class EnableAllocationTests extends ESAllocationTestCase {
         ClusterState prevState = clusterState;
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(8));
-        assertThat(clusterState.routingNodes().shardsWithState(RELOCATING).size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(8));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(RELOCATING).size(), equalTo(0));
 
         if (useClusterSetting) {
             prevState = clusterState;
@@ -234,9 +234,9 @@ public class EnableAllocationTests extends ESAllocationTestCase {
         nodeSettingsService.clusterChanged(new ClusterChangedEvent("foo", clusterState, prevState));
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat("expected 6 shards to be started 2 to relocate useClusterSettings: " + useClusterSetting, clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(6));
-        assertThat("expected 2 shards to relocate useClusterSettings: " + useClusterSetting, clusterState.routingNodes().shardsWithState(RELOCATING).size(), equalTo(2));
-        List<ShardRouting> mutableShardRoutings = clusterState.routingNodes().shardsWithState(RELOCATING);
+        assertThat("expected 6 shards to be started 2 to relocate useClusterSettings: " + useClusterSetting, clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(6));
+        assertThat("expected 2 shards to relocate useClusterSettings: " + useClusterSetting, clusterState.getRoutingNodes().shardsWithState(RELOCATING).size(), equalTo(2));
+        List<ShardRouting> mutableShardRoutings = clusterState.getRoutingNodes().shardsWithState(RELOCATING);
         switch (allowedOnes) {
             case PRIMARIES:
                 for (ShardRouting routing : mutableShardRoutings) {
@@ -258,10 +258,10 @@ public class EnableAllocationTests extends ESAllocationTestCase {
             default:
                 fail("only replicas, primaries or all are allowed");
         }
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(8));
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(8));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
 
     }
 
@@ -294,12 +294,12 @@ public class EnableAllocationTests extends ESAllocationTestCase {
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(6));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(6));
         logger.info("--> start the shards (primaries)");
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(6));
-        assertThat(clusterState.routingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(6));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size(), equalTo(0));
 
         logger.info("--> adding one nodes and do rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
@@ -310,8 +310,8 @@ public class EnableAllocationTests extends ESAllocationTestCase {
         ClusterState prevState = clusterState;
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat(clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(6));
-        assertThat(clusterState.routingNodes().shardsWithState(RELOCATING).size(), equalTo(0));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(6));
+        assertThat(clusterState.getRoutingNodes().shardsWithState(RELOCATING).size(), equalTo(0));
         if (useClusterSetting) {
             prevState = clusterState;
             clusterState = ClusterState.builder(clusterState).metaData(MetaData.builder(metaData).transientSettings(settingsBuilder()
@@ -326,8 +326,8 @@ public class EnableAllocationTests extends ESAllocationTestCase {
         nodeSettingsService.clusterChanged(new ClusterChangedEvent("foo", clusterState, prevState));
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
-        assertThat("expected 4 primaries to be started and 2 to relocate useClusterSettings: " + useClusterSetting, clusterState.routingNodes().shardsWithState(STARTED).size(), equalTo(4));
-        assertThat("expected 2 primaries to relocate useClusterSettings: " + useClusterSetting, clusterState.routingNodes().shardsWithState(RELOCATING).size(), equalTo(2));
+        assertThat("expected 4 primaries to be started and 2 to relocate useClusterSettings: " + useClusterSetting, clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(4));
+        assertThat("expected 2 primaries to relocate useClusterSettings: " + useClusterSetting, clusterState.getRoutingNodes().shardsWithState(RELOCATING).size(), equalTo(2));
 
     }
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java b/core/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java
index 9e5f709..4e6eabc 100644
--- a/core/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java
@@ -238,10 +238,10 @@ public class RoutingIteratorTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         // after all are started, check routing iteration
@@ -288,7 +288,7 @@ public class RoutingIteratorTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         ShardsIterator shardsIterator = clusterState.routingTable().index("test").shard(0).onlyNodeSelectorActiveInitializingShardsIt("disk:ebs",clusterState.nodes());
@@ -344,10 +344,10 @@ public class RoutingIteratorTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         OperationRouting operationRouting = new OperationRouting(Settings.Builder.EMPTY_SETTINGS, new AwarenessAllocationDecider());
@@ -409,7 +409,7 @@ public class RoutingIteratorTests extends ESAllocationTestCase {
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
         // When replicas haven't initialized, it comes back with the primary first, then initializing replicas
@@ -431,10 +431,10 @@ public class RoutingIteratorTests extends ESAllocationTestCase {
         assertFalse(routing.primary());
         assertTrue(routing.initializing());
 
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
-        routingTable = strategy.applyStartedShards(clusterState, clusterState.routingNodes().shardsWithState(INITIALIZING)).routingTable();
+        routingTable = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING)).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
 
diff --git a/core/src/test/java/org/elasticsearch/common/io/stream/BytesStreamsTests.java b/core/src/test/java/org/elasticsearch/common/io/stream/BytesStreamsTests.java
index afc17ce..d313dd7 100644
--- a/core/src/test/java/org/elasticsearch/common/io/stream/BytesStreamsTests.java
+++ b/core/src/test/java/org/elasticsearch/common/io/stream/BytesStreamsTests.java
@@ -26,7 +26,6 @@ import org.elasticsearch.test.ESTestCase;
 import org.junit.Test;
 
 import java.io.IOException;
-
 import java.util.Objects;
 
 import static org.hamcrest.Matchers.closeTo;
diff --git a/core/src/test/java/org/elasticsearch/common/lucene/search/function/ScriptScoreFunctionTests.java b/core/src/test/java/org/elasticsearch/common/lucene/search/function/ScriptScoreFunctionTests.java
index 4b0c939..b6c6384 100644
--- a/core/src/test/java/org/elasticsearch/common/lucene/search/function/ScriptScoreFunctionTests.java
+++ b/core/src/test/java/org/elasticsearch/common/lucene/search/function/ScriptScoreFunctionTests.java
@@ -73,5 +73,10 @@ public class ScriptScoreFunctionTests extends ESTestCase {
                 }
             };
         }
+
+        @Override
+        public boolean needsScores() {
+            return false;
+        }
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/gateway/PrimaryShardAllocatorTests.java b/core/src/test/java/org/elasticsearch/gateway/PrimaryShardAllocatorTests.java
index be48fc7..ce6a8b0 100644
--- a/core/src/test/java/org/elasticsearch/gateway/PrimaryShardAllocatorTests.java
+++ b/core/src/test/java/org/elasticsearch/gateway/PrimaryShardAllocatorTests.java
@@ -187,7 +187,7 @@ public class PrimaryShardAllocatorTests extends ESAllocationTestCase {
                 .metaData(metaData)
                 .routingTable(routingTable)
                 .nodes(DiscoveryNodes.builder().put(node1).put(node2).put(node3)).build();
-        RoutingAllocation allocation = new RoutingAllocation(yesAllocationDeciders(), state.routingNodes(), state.nodes(), null);
+        RoutingAllocation allocation = new RoutingAllocation(yesAllocationDeciders(), state.getRoutingNodes(), state.nodes(), null);
 
         testAllocator.addData(node1, -1).addData(node2, -1);
         boolean changed = testAllocator.allocateUnassigned(allocation);
@@ -212,7 +212,7 @@ public class PrimaryShardAllocatorTests extends ESAllocationTestCase {
                 .routingTable(routingTable)
                 .nodes(DiscoveryNodes.builder().put(node1).put(node2).put(node3)).build();
 
-        RoutingAllocation allocation = new RoutingAllocation(yesAllocationDeciders(), state.routingNodes(), state.nodes(), null);
+        RoutingAllocation allocation = new RoutingAllocation(yesAllocationDeciders(), new RoutingNodes(state, false), state.nodes(), null);
         boolean changed = testAllocator.allocateUnassigned(allocation);
         assertThat(changed, equalTo(false));
         assertThat(allocation.routingNodes().unassigned().ignored().size(), equalTo(1));
@@ -220,7 +220,7 @@ public class PrimaryShardAllocatorTests extends ESAllocationTestCase {
         assertThat(allocation.routingNodes().shardsWithState(ShardRoutingState.UNASSIGNED).size(), equalTo(2)); // replicas
 
         testAllocator.addData(node1, 1);
-        allocation = new RoutingAllocation(yesAllocationDeciders(), state.routingNodes(), state.nodes(), null);
+        allocation = new RoutingAllocation(yesAllocationDeciders(), new RoutingNodes(state, false), state.nodes(), null);
         changed = testAllocator.allocateUnassigned(allocation);
         assertThat(changed, equalTo(false));
         assertThat(allocation.routingNodes().unassigned().ignored().size(), equalTo(1));
@@ -228,7 +228,7 @@ public class PrimaryShardAllocatorTests extends ESAllocationTestCase {
         assertThat(allocation.routingNodes().shardsWithState(ShardRoutingState.UNASSIGNED).size(), equalTo(2)); // replicas
 
         testAllocator.addData(node2, 1);
-        allocation = new RoutingAllocation(yesAllocationDeciders(), state.routingNodes(), state.nodes(), null);
+        allocation = new RoutingAllocation(yesAllocationDeciders(), new RoutingNodes(state, false), state.nodes(), null);
         changed = testAllocator.allocateUnassigned(allocation);
         assertThat(changed, equalTo(true));
         assertThat(allocation.routingNodes().unassigned().ignored().size(), equalTo(0));
@@ -254,7 +254,7 @@ public class PrimaryShardAllocatorTests extends ESAllocationTestCase {
                 .routingTable(routingTable)
                 .nodes(DiscoveryNodes.builder().put(node1).put(node2).put(node3)).build();
 
-        RoutingAllocation allocation = new RoutingAllocation(yesAllocationDeciders(), state.routingNodes(), state.nodes(), null);
+        RoutingAllocation allocation = new RoutingAllocation(yesAllocationDeciders(), new RoutingNodes(state, false), state.nodes(), null);
         boolean changed = testAllocator.allocateUnassigned(allocation);
         assertThat(changed, equalTo(false));
         assertThat(allocation.routingNodes().unassigned().ignored().size(), equalTo(1));
@@ -262,7 +262,7 @@ public class PrimaryShardAllocatorTests extends ESAllocationTestCase {
         assertThat(allocation.routingNodes().shardsWithState(ShardRoutingState.UNASSIGNED).size(), equalTo(2)); // replicas
 
         testAllocator.addData(node1, 1);
-        allocation = new RoutingAllocation(yesAllocationDeciders(), state.routingNodes(), state.nodes(), null);
+        allocation = new RoutingAllocation(yesAllocationDeciders(), new RoutingNodes(state, false), state.nodes(), null);
         changed = testAllocator.allocateUnassigned(allocation);
         assertThat(changed, equalTo(false));
         assertThat(allocation.routingNodes().unassigned().ignored().size(), equalTo(1));
@@ -270,7 +270,7 @@ public class PrimaryShardAllocatorTests extends ESAllocationTestCase {
         assertThat(allocation.routingNodes().shardsWithState(ShardRoutingState.UNASSIGNED).size(), equalTo(2)); // replicas
 
         testAllocator.addData(node2, 2);
-        allocation = new RoutingAllocation(yesAllocationDeciders(), state.routingNodes(), state.nodes(), null);
+        allocation = new RoutingAllocation(yesAllocationDeciders(), new RoutingNodes(state, false), state.nodes(), null);
         changed = testAllocator.allocateUnassigned(allocation);
         assertThat(changed, equalTo(true));
         assertThat(allocation.routingNodes().unassigned().ignored().size(), equalTo(0));
@@ -344,7 +344,7 @@ public class PrimaryShardAllocatorTests extends ESAllocationTestCase {
                 .metaData(metaData)
                 .routingTable(routingTable)
                 .nodes(DiscoveryNodes.builder().put(node1).put(node2).put(node3)).build();
-        return new RoutingAllocation(deciders, state.routingNodes(), state.nodes(), null);
+        return new RoutingAllocation(deciders, new RoutingNodes(state, false), state.nodes(), null);
     }
 
     class TestAllocator extends PrimaryShardAllocator {
diff --git a/core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java b/core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java
index be6be39..b19a552 100644
--- a/core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java
+++ b/core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java
@@ -302,7 +302,7 @@ public class ReplicaShardAllocatorTests extends ESAllocationTestCase {
                 .metaData(metaData)
                 .routingTable(routingTable)
                 .nodes(DiscoveryNodes.builder().put(node1).put(node2).put(node3)).build();
-        return new RoutingAllocation(deciders, state.routingNodes(), state.nodes(), null);
+        return new RoutingAllocation(deciders, new RoutingNodes(state, false), state.nodes(), null);
     }
 
     private RoutingAllocation onePrimaryOnNode1And1ReplicaRecovering(AllocationDeciders deciders) {
@@ -321,7 +321,7 @@ public class ReplicaShardAllocatorTests extends ESAllocationTestCase {
                 .metaData(metaData)
                 .routingTable(routingTable)
                 .nodes(DiscoveryNodes.builder().put(node1).put(node2).put(node3)).build();
-        return new RoutingAllocation(deciders, state.routingNodes(), state.nodes(), null);
+        return new RoutingAllocation(deciders, new RoutingNodes(state, false), state.nodes(), null);
     }
 
     class TestAllocator extends ReplicaShardAllocator {
diff --git a/core/src/test/java/org/elasticsearch/get/GetActionIT.java b/core/src/test/java/org/elasticsearch/get/GetActionIT.java
index 15242ab..b376590 100644
--- a/core/src/test/java/org/elasticsearch/get/GetActionIT.java
+++ b/core/src/test/java/org/elasticsearch/get/GetActionIT.java
@@ -1031,9 +1031,6 @@ public class GetActionIT extends ESIntegTestCase {
             "    \"doc\": {\n" +
             "      \"_timestamp\": {\n" +
             "        \"enabled\": true\n" +
-            "      },\n" +
-            "      \"_size\": {\n" +
-            "        \"enabled\": true\n" +
             "      }\n" +
             "    }\n" +
             "  }\n" +
@@ -1045,7 +1042,7 @@ public class GetActionIT extends ESIntegTestCase {
             "  \"text\": \"some text.\"\n" +
             "}\n";
         client().prepareIndex("test", "doc").setId("1").setSource(doc).setRouting("1").get();
-        String[] fieldsList = {"_timestamp", "_size", "_routing"};
+        String[] fieldsList = {"_timestamp", "_routing"};
         // before refresh - document is only in translog
         assertGetFieldsAlwaysWorks(indexOrAlias(), "doc", "1", fieldsList, "1");
         refresh();
diff --git a/core/src/test/java/org/elasticsearch/index/TransportIndexFailuresIT.java b/core/src/test/java/org/elasticsearch/index/TransportIndexFailuresIT.java
index 9426f39..ae2a979 100644
--- a/core/src/test/java/org/elasticsearch/index/TransportIndexFailuresIT.java
+++ b/core/src/test/java/org/elasticsearch/index/TransportIndexFailuresIT.java
@@ -133,7 +133,7 @@ public class TransportIndexFailuresIT extends ESIntegTestCase {
                 client().prepareGet(INDEX, "doc", resp.getId()).get().isExists(), equalTo(true));
 
         state = getNodeClusterState(randomFrom(nodes.toArray(Strings.EMPTY_ARRAY)));
-        RoutingNodes rn = state.routingNodes();
+        RoutingNodes rn = state.getRoutingNodes();
         logger.info("--> counts: total: {}, unassigned: {}, initializing: {}, relocating: {}, started: {}",
                 rn.shards(new Predicate<ShardRouting>() {
                     @Override
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java b/core/src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java
index b50f2da..009bb4c 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java
@@ -44,8 +44,8 @@ import org.elasticsearch.index.mapper.MapperParsingException;
 import org.elasticsearch.index.mapper.ParseContext.Document;
 import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.internal.AllFieldMapper;
-import org.elasticsearch.index.mapper.internal.SizeFieldMapper;
 import org.elasticsearch.test.ESSingleNodeTestCase;
+import org.elasticsearch.index.mapper.internal.TimestampFieldMapper;
 import org.hamcrest.Matchers;
 import org.junit.Test;
 
@@ -387,7 +387,7 @@ public class SimpleAllMapperTests extends ESSingleNodeTestCase {
         String mapping = "{";
         Map<String, String> rootTypes = new HashMap<>();
         //just pick some example from DocumentMapperParser.rootTypeParsers
-        rootTypes.put(SizeFieldMapper.NAME, "{\"enabled\" : true}");
+        rootTypes.put(TimestampFieldMapper.NAME, "{\"enabled\" : true}");
         rootTypes.put("include_in_all", "true");
         rootTypes.put("dynamic_date_formats", "[\"yyyy-MM-dd\", \"dd-MM-yyyy\"]");
         rootTypes.put("numeric_detection", "true");
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingIntegrationIT.java b/core/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingIntegrationIT.java
deleted file mode 100644
index 6e004c4..0000000
--- a/core/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingIntegrationIT.java
+++ /dev/null
@@ -1,86 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.mapper.size;
-
-import org.elasticsearch.action.admin.indices.mapping.get.GetMappingsResponse;
-import org.elasticsearch.action.admin.indices.mapping.put.PutMappingResponse;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.Locale;
-import java.util.Map;
-
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.hamcrest.Matchers.*;
-
-public class SizeMappingIntegrationIT extends ESIntegTestCase {
-
-    @Test // issue 5053
-    public void testThatUpdatingMappingShouldNotRemoveSizeMappingConfiguration() throws Exception {
-        String index = "foo";
-        String type = "mytype";
-
-        XContentBuilder builder = jsonBuilder().startObject().startObject("_size").field("enabled", true).endObject().endObject();
-        assertAcked(client().admin().indices().prepareCreate(index).addMapping(type, builder));
-
-        // check mapping again
-        assertSizeMappingEnabled(index, type, true);
-
-        // update some field in the mapping
-        XContentBuilder updateMappingBuilder = jsonBuilder().startObject().startObject("properties").startObject("otherField").field("type", "string").endObject().endObject();
-        PutMappingResponse putMappingResponse = client().admin().indices().preparePutMapping(index).setType(type).setSource(updateMappingBuilder).get();
-        assertAcked(putMappingResponse);
-
-        // make sure size field is still in mapping
-        assertSizeMappingEnabled(index, type, true);
-    }
-
-    @Test
-    public void testThatSizeCanBeSwitchedOnAndOff() throws Exception {
-        String index = "foo";
-        String type = "mytype";
-
-        XContentBuilder builder = jsonBuilder().startObject().startObject("_size").field("enabled", true).endObject().endObject();
-        assertAcked(client().admin().indices().prepareCreate(index).addMapping(type, builder));
-
-        // check mapping again
-        assertSizeMappingEnabled(index, type, true);
-
-        // update some field in the mapping
-        XContentBuilder updateMappingBuilder = jsonBuilder().startObject().startObject("_size").field("enabled", false).endObject().endObject();
-        PutMappingResponse putMappingResponse = client().admin().indices().preparePutMapping(index).setType(type).setSource(updateMappingBuilder).get();
-        assertAcked(putMappingResponse);
-
-        // make sure size field is still in mapping
-        assertSizeMappingEnabled(index, type, false);
-    }
-
-    private void assertSizeMappingEnabled(String index, String type, boolean enabled) throws IOException {
-        String errMsg = String.format(Locale.ROOT, "Expected size field mapping to be " + (enabled ? "enabled" : "disabled") + " for %s/%s", index, type);
-        GetMappingsResponse getMappingsResponse = client().admin().indices().prepareGetMappings(index).addTypes(type).get();
-        Map<String, Object> mappingSource = getMappingsResponse.getMappings().get(index).get(type).getSourceAsMap();
-        assertThat(errMsg, mappingSource, hasKey("_size"));
-        String sizeAsString = mappingSource.get("_size").toString();
-        assertThat(sizeAsString, is(notNullValue()));
-        assertThat(errMsg, sizeAsString, is("{enabled=" + (enabled) + "}"));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingTests.java
deleted file mode 100644
index 6f66c51..0000000
--- a/core/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingTests.java
+++ /dev/null
@@ -1,118 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.mapper.size;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.mapper.DocumentMapper;
-import org.elasticsearch.index.mapper.DocumentMapperParser;
-import org.elasticsearch.index.mapper.ParsedDocument;
-import org.elasticsearch.index.mapper.SourceToParse;
-import org.elasticsearch.test.ESSingleNodeTestCase;
-
-import static org.hamcrest.Matchers.*;
-
-public class SizeMappingTests extends ESSingleNodeTestCase {
-    
-    public void testSizeEnabled() throws Exception {
-        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
-                .startObject("_size").field("enabled", true).endObject()
-                .endObject().endObject().string();
-        DocumentMapper docMapper = createIndex("test").mapperService().documentMapperParser().parse(mapping);
-
-        BytesReference source = XContentFactory.jsonBuilder()
-                .startObject()
-                .field("field", "value")
-                .endObject()
-                .bytes();
-        ParsedDocument doc = docMapper.parse(SourceToParse.source(source).type("type").id("1"));
-
-        assertThat(doc.rootDoc().getField("_size").fieldType().stored(), equalTo(true));
-        assertThat(doc.rootDoc().getField("_size").tokenStream(docMapper.mappers().indexAnalyzer(), null), notNullValue());
-    }
-    
-    public void testSizeEnabledAndStoredBackcompat() throws Exception {
-        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
-                .startObject("_size").field("enabled", true).field("store", "yes").endObject()
-                .endObject().endObject().string();
-        Settings indexSettings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.V_1_4_2.id).build();
-        DocumentMapper docMapper = createIndex("test", indexSettings).mapperService().documentMapperParser().parse(mapping);
-
-        BytesReference source = XContentFactory.jsonBuilder()
-                .startObject()
-                .field("field", "value")
-                .endObject()
-                .bytes();
-        ParsedDocument doc = docMapper.parse(SourceToParse.source(source).type("type").id("1"));
-
-        assertThat(doc.rootDoc().getField("_size").fieldType().stored(), equalTo(true));
-        assertThat(doc.rootDoc().getField("_size").tokenStream(docMapper.mappers().indexAnalyzer(), null), notNullValue());
-    }
-    
-    public void testSizeDisabled() throws Exception {
-        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
-                .startObject("_size").field("enabled", false).endObject()
-                .endObject().endObject().string();
-        DocumentMapper docMapper = createIndex("test").mapperService().documentMapperParser().parse(mapping);
-
-        BytesReference source = XContentFactory.jsonBuilder()
-                .startObject()
-                .field("field", "value")
-                .endObject()
-                .bytes();
-        ParsedDocument doc = docMapper.parse(SourceToParse.source(source).type("type").id("1"));
-
-        assertThat(doc.rootDoc().getField("_size"), nullValue());
-    }
-    
-    public void testSizeNotSet() throws Exception {
-        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
-                .endObject().endObject().string();
-        DocumentMapper docMapper = createIndex("test").mapperService().documentMapperParser().parse(mapping);
-
-        BytesReference source = XContentFactory.jsonBuilder()
-                .startObject()
-                .field("field", "value")
-                .endObject()
-                .bytes();
-        ParsedDocument doc = docMapper.parse(SourceToParse.source(source).type("type").id("1"));
-
-        assertThat(doc.rootDoc().getField("_size"), nullValue());
-    }
-    
-    public void testThatDisablingWorksWhenMerging() throws Exception {
-        String enabledMapping = XContentFactory.jsonBuilder().startObject().startObject("type")
-                .startObject("_size").field("enabled", true).endObject()
-                .endObject().endObject().string();
-        DocumentMapperParser parser = createIndex("test").mapperService().documentMapperParser();
-        DocumentMapper enabledMapper = parser.parse(enabledMapping);
-
-        String disabledMapping = XContentFactory.jsonBuilder().startObject().startObject("type")
-                .startObject("_size").field("enabled", false).endObject()
-                .endObject().endObject().string();
-        DocumentMapper disabledMapper = parser.parse(disabledMapping);
-
-        enabledMapper.merge(disabledMapper.mapping(), false, false);
-        assertThat(enabledMapper.SizeFieldMapper().enabled(), is(false));
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/typelevels/ParseMappingTypeLevelTests.java b/core/src/test/java/org/elasticsearch/index/mapper/typelevels/ParseMappingTypeLevelTests.java
index 0dcbace..f675475 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/typelevels/ParseMappingTypeLevelTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/typelevels/ParseMappingTypeLevelTests.java
@@ -33,16 +33,16 @@ public class ParseMappingTypeLevelTests extends ESSingleNodeTestCase {
     @Test
     public void testTypeLevel() throws Exception {
         String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
-                .startObject("_size").field("enabled", true).endObject()
+                .startObject("_timestamp").field("enabled", true).endObject()
                 .endObject().endObject().string();
 
         DocumentMapperParser parser = createIndex("test").mapperService().documentMapperParser();
         DocumentMapper mapper = parser.parse("type", mapping);
         assertThat(mapper.type(), equalTo("type"));
-        assertThat(mapper.sizeFieldMapper().enabled(), equalTo(true));
+        assertThat(mapper.timestampFieldMapper().enabled(), equalTo(true));
 
         mapper = parser.parse(mapping);
         assertThat(mapper.type(), equalTo("type"));
-        assertThat(mapper.sizeFieldMapper().enabled(), equalTo(true));
+        assertThat(mapper.timestampFieldMapper().enabled(), equalTo(true));
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingTests.java
index 4b136a7..c10dc3b 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingTests.java
@@ -159,25 +159,6 @@ public class UpdateMappingTests extends ESSingleNodeTestCase {
     }
 
     @Test
-    public void testSizeParsing() throws IOException {
-        IndexService indexService = createIndex("test", Settings.settingsBuilder().build());
-        XContentBuilder indexMapping = XContentFactory.jsonBuilder();
-        boolean enabled = randomBoolean();
-        indexMapping.startObject()
-                .startObject("type")
-                .startObject("_size")
-                .field("enabled", enabled)
-                .endObject()
-                .endObject()
-                .endObject();
-        DocumentMapper documentMapper = indexService.mapperService().parse("type", new CompressedXContent(indexMapping.string()), true);
-        assertThat(documentMapper.sizeFieldMapper().enabled(), equalTo(enabled));
-        assertTrue(documentMapper.sizeFieldMapper().fieldType().stored());
-        documentMapper = indexService.mapperService().parse("type", new CompressedXContent(documentMapper.mappingSource().string()), true);
-        assertThat(documentMapper.sizeFieldMapper().enabled(), equalTo(enabled));
-    }
-
-    @Test
     public void testSizeTimestampIndexParsing() throws IOException {
         IndexService indexService = createIndex("test", Settings.settingsBuilder().build());
         String mapping = copyToStringFromClasspath("/org/elasticsearch/index/mapper/update/default_mapping_with_disabled_root_types.json");
@@ -192,7 +173,7 @@ public class UpdateMappingTests extends ESSingleNodeTestCase {
         createIndex("test1", Settings.settingsBuilder().build());
         createIndex("test2", Settings.settingsBuilder().build());
         XContentBuilder defaultMapping = XContentFactory.jsonBuilder().startObject()
-                .startObject(MapperService.DEFAULT_MAPPING).startObject("_size").field("enabled", true).endObject().endObject()
+                .startObject(MapperService.DEFAULT_MAPPING).startObject("_timestamp").field("enabled", true).endObject().endObject()
                 .endObject();
         client().admin().indices().preparePutMapping().setType(MapperService.DEFAULT_MAPPING).setSource(defaultMapping).get();
         XContentBuilder typeMapping = XContentFactory.jsonBuilder().startObject()
@@ -204,7 +185,7 @@ public class UpdateMappingTests extends ESSingleNodeTestCase {
         GetMappingsResponse response = client().admin().indices().prepareGetMappings("test2").get();
         assertNotNull(response.getMappings().get("test2").get("type").getSourceAsMap().get("_all"));
         assertFalse((Boolean) ((LinkedHashMap) response.getMappings().get("test2").get("type").getSourceAsMap().get("_all")).get("enabled"));
-        assertNotNull(response.getMappings().get("test2").get("type").getSourceAsMap().get("_size"));
-        assertTrue((Boolean)((LinkedHashMap)response.getMappings().get("test2").get("type").getSourceAsMap().get("_size")).get("enabled"));
+        assertNotNull(response.getMappings().get("test2").get("type").getSourceAsMap().get("_timestamp"));
+        assertTrue((Boolean)((LinkedHashMap)response.getMappings().get("test2").get("type").getSourceAsMap().get("_timestamp")).get("enabled"));
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/update/default_mapping_with_disabled_root_types.json b/core/src/test/java/org/elasticsearch/index/mapper/update/default_mapping_with_disabled_root_types.json
index 5011ec9..5f49a6f 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/update/default_mapping_with_disabled_root_types.json
+++ b/core/src/test/java/org/elasticsearch/index/mapper/update/default_mapping_with_disabled_root_types.json
@@ -1 +1 @@
-{"type":{"_size":{"enabled":false},"_timestamp":{"enabled":false}}}
\ No newline at end of file
+{"type":{"_timestamp":{"enabled":false}}}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/index/query/AndQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/AndQueryBuilderTest.java
deleted file mode 100644
index 3944ddc..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/AndQueryBuilderTest.java
+++ /dev/null
@@ -1,119 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-import static org.hamcrest.CoreMatchers.nullValue;
-
-@SuppressWarnings("deprecation")
-public class AndQueryBuilderTest extends BaseQueryTestCase<AndQueryBuilder> {
-
-    /**
-     * @return a AndQueryBuilder with random limit between 0 and 20
-     */
-    @Override
-    protected AndQueryBuilder doCreateTestQueryBuilder() {
-        AndQueryBuilder query = new AndQueryBuilder();
-        int subQueries = randomIntBetween(1, 5);
-        for (int i = 0; i < subQueries; i++ ) {
-            query.add(RandomQueryBuilder.createQuery(random()));
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(AndQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (queryBuilder.filters().isEmpty()) {
-            assertThat(query, nullValue());
-        } else {
-            List<Query> clauses = new ArrayList<>();
-            for (QueryBuilder innerFilter : queryBuilder.filters()) {
-                Query clause = innerFilter.toQuery(context);
-                if (clause != null) {
-                    clauses.add(clause);
-                }
-            }
-            if (clauses.isEmpty()) {
-                assertThat(query, nullValue());
-            } else {
-                assertThat(query, instanceOf(BooleanQuery.class));
-                BooleanQuery booleanQuery = (BooleanQuery) query;
-                assertThat(booleanQuery.clauses().size(), equalTo(clauses.size()));
-                Iterator<Query> queryIterator = clauses.iterator();
-                for (BooleanClause booleanClause : booleanQuery) {
-                    assertThat(booleanClause.getOccur(), equalTo(BooleanClause.Occur.MUST));
-                    assertThat(booleanClause.getQuery(), equalTo(queryIterator.next()));
-                }
-            }
-        }
-    }
-
-    /**
-     * test corner case where no inner queries exist
-     */
-    @Test
-    public void testNoInnerQueries() throws QueryShardException, IOException {
-        AndQueryBuilder andQuery = new AndQueryBuilder();
-        assertNull(andQuery.toQuery(createShardContext()));
-    }
-
-    @Test(expected=QueryParsingException.class)
-    public void testMissingFiltersSection() throws IOException {
-        QueryParseContext context = createParseContext();
-        String queryString = "{ \"and\" : {}";
-        XContentParser parser = XContentFactory.xContent(queryString).createParser(queryString);
-        context.reset(parser);
-        assertQueryHeader(parser, AndQueryBuilder.NAME);
-        context.queryParser(AndQueryBuilder.NAME).fromXContent(context);
-    }
-
-    @Test
-    public void testValidate() {
-        AndQueryBuilder andQuery = new AndQueryBuilder();
-        int iters = randomIntBetween(0, 5);
-        int totalExpectedErrors = 0;
-        for (int i = 0; i < iters; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    andQuery.add(RandomQueryBuilder.createInvalidQuery(random()));
-                } else {
-                    andQuery.add(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                andQuery.add(RandomQueryBuilder.createQuery(random()));
-            }
-        }
-        assertValidate(andQuery, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java b/core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java
deleted file mode 100644
index e04201f..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java
+++ /dev/null
@@ -1,399 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.elasticsearch.Version;
-import org.elasticsearch.action.admin.indices.mapping.put.PutMappingRequest;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.compress.CompressedXContent;
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Injector;
-import org.elasticsearch.common.inject.ModulesBuilder;
-import org.elasticsearch.common.inject.util.Providers;
-import org.elasticsearch.common.io.stream.*;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsModule;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.env.EnvironmentModule;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.IndexNameModule;
-import org.elasticsearch.index.analysis.AnalysisModule;
-import org.elasticsearch.index.cache.IndexCacheModule;
-import org.elasticsearch.index.mapper.MapperService;
-import org.elasticsearch.index.query.functionscore.FunctionScoreModule;
-import org.elasticsearch.index.query.support.QueryParsers;
-import org.elasticsearch.index.settings.IndexSettingsModule;
-import org.elasticsearch.index.similarity.SimilarityModule;
-import org.elasticsearch.indices.breaker.CircuitBreakerService;
-import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
-import org.elasticsearch.indices.query.IndicesQueriesModule;
-import org.elasticsearch.script.ScriptModule;
-import org.elasticsearch.search.internal.SearchContext;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.TestSearchContext;
-import org.elasticsearch.test.VersionUtils;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.threadpool.ThreadPoolModule;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.not;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
-
-public abstract class BaseQueryTestCase<QB extends AbstractQueryBuilder<QB>> extends ESTestCase {
-
-    protected static final String OBJECT_FIELD_NAME = "mapped_object";
-    protected static final String DATE_FIELD_NAME = "mapped_date";
-    protected static final String INT_FIELD_NAME = "mapped_int";
-    protected static final String STRING_FIELD_NAME = "mapped_string";
-    protected static final String DOUBLE_FIELD_NAME = "mapped_double";
-    protected static final String BOOLEAN_FIELD_NAME = "mapped_boolean";
-    protected static final String[] mappedFieldNames = new String[] { DATE_FIELD_NAME, INT_FIELD_NAME, STRING_FIELD_NAME,
-            DOUBLE_FIELD_NAME, BOOLEAN_FIELD_NAME, OBJECT_FIELD_NAME };
-
-    private static Injector injector;
-    private static IndexQueryParserService queryParserService;
-    private static Index index;
-
-    private static String[] currentTypes;
-
-    protected static String[] getCurrentTypes() {
-        return currentTypes;
-    }
-
-    private static NamedWriteableRegistry namedWriteableRegistry;
-
-    /**
-     * Setup for the whole base test class.
-     * @throws IOException
-     */
-    @BeforeClass
-    public static void init() throws IOException {
-        Settings settings = Settings.settingsBuilder()
-                .put("name", BaseQueryTestCase.class.toString())
-                .put("path.home", createTempDir())
-                .put(IndexMetaData.SETTING_VERSION_CREATED, VersionUtils.randomVersionBetween(random(),
-                        Version.V_1_0_0, Version.CURRENT))
-                .build();
-
-        index = new Index("test");
-        injector = new ModulesBuilder().add(
-                new EnvironmentModule(new Environment(settings)),
-                new SettingsModule(settings),
-                new ThreadPoolModule(new ThreadPool(settings)),
-                new IndicesQueriesModule(),
-                new ScriptModule(settings),
-                new IndexSettingsModule(index, settings),
-                new IndexCacheModule(settings),
-                new AnalysisModule(settings),
-                new SimilarityModule(settings),
-                new IndexNameModule(index),
-                new FunctionScoreModule(),
-                new AbstractModule() {
-                    @Override
-                    protected void configure() {
-                        bind(ClusterService.class).toProvider(Providers.of((ClusterService) null));
-                        bind(CircuitBreakerService.class).to(NoneCircuitBreakerService.class);
-                        bind(NamedWriteableRegistry.class).asEagerSingleton();
-                    }
-                }
-        ).createInjector();
-        queryParserService = injector.getInstance(IndexQueryParserService.class);
-        MapperService mapperService = queryParserService.mapperService;
-        //create some random type with some default field, those types will stick around for all of the subclasses
-        currentTypes = new String[randomIntBetween(0, 5)];
-        for (int i = 0; i < currentTypes.length; i++) {
-            String type = randomAsciiOfLengthBetween(1, 10);
-            mapperService.merge(type, new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef(type,
-                    DATE_FIELD_NAME, "type=date",
-                    INT_FIELD_NAME, "type=integer",
-                    DOUBLE_FIELD_NAME, "type=double",
-                    BOOLEAN_FIELD_NAME, "type=boolean",
-                    STRING_FIELD_NAME, "type=string",
-                    OBJECT_FIELD_NAME, "type=object"
-                    ).string()), false, false);
-            // also add mappings for two inner field in the object field
-            mapperService.merge(type, new CompressedXContent("{\"properties\":{\""+OBJECT_FIELD_NAME+"\":{\"type\":\"object\","
-                    + "\"properties\":{\""+DATE_FIELD_NAME+"\":{\"type\":\"date\"},\""+INT_FIELD_NAME+"\":{\"type\":\"integer\"}}}}}"), false, false);
-            currentTypes[i] = type;
-        }
-        namedWriteableRegistry = injector.getInstance(NamedWriteableRegistry.class);
-    }
-
-    @AfterClass
-    public static void afterClass() throws Exception {
-        terminate(injector.getInstance(ThreadPool.class));
-        injector = null;
-        index = null;
-        queryParserService = null;
-        currentTypes = null;
-        namedWriteableRegistry = null;
-    }
-
-    @Before
-    public void beforeTest() {
-        //set some random types to be queried as part the search request, before each test
-        String[] types = getRandomTypes();
-        //some query (e.g. range query) have a different behaviour depending on whether the current search context is set or not
-        //which is why we randomly set the search context, which will internally also do QueryParseContext.setTypes(types)
-        if (randomBoolean()) {
-            QueryShardContext.setTypes(types);
-        } else {
-            TestSearchContext testSearchContext = new TestSearchContext();
-            testSearchContext.setTypes(types);
-            SearchContext.setCurrent(testSearchContext);
-        }
-    }
-
-    @After
-    public void afterTest() {
-        QueryShardContext.removeTypes();
-        SearchContext.removeCurrent();
-    }
-
-    protected final QB createTestQueryBuilder() {
-        QB query = doCreateTestQueryBuilder();
-        if (supportsBoostAndQueryName()) {
-            if (randomBoolean()) {
-                query.boost(2.0f / randomIntBetween(1, 20));
-            }
-            if (randomBoolean()) {
-                query.queryName(randomAsciiOfLengthBetween(1, 10));
-            }
-        }
-        return query;
-    }
-
-    /**
-     * Create the query that is being tested
-     */
-    protected abstract QB doCreateTestQueryBuilder();
-
-    /**
-     * Generic test that creates new query from the test query and checks both for equality
-     * and asserts equality on the two queries.
-     */
-    @Test
-    public void testFromXContent() throws IOException {
-        QB testQuery = createTestQueryBuilder();
-        QueryParseContext context = createParseContext();
-        String contentString = testQuery.toString();
-        XContentParser parser = XContentFactory.xContent(contentString).createParser(contentString);
-        context.reset(parser);
-        assertQueryHeader(parser, testQuery.getName());
-
-        QueryBuilder newQuery = queryParserService.queryParser(testQuery.getName()).fromXContent(context);
-        assertNotSame(newQuery, testQuery);
-        assertEquals(testQuery, newQuery);
-        assertEquals(testQuery.hashCode(), newQuery.hashCode());
-    }
-
-    /**
-     * Test creates the {@link Query} from the {@link QueryBuilder} under test and delegates the
-     * assertions being made on the result to the implementing subclass.
-     */
-    @Test
-    public void testToQuery() throws IOException {
-        QueryShardContext context = createShardContext();
-        context.setAllowUnmappedFields(true);
-
-        QB firstQuery = createTestQueryBuilder();
-        Query firstLuceneQuery = firstQuery.toQuery(context);
-        assertLuceneQuery(firstQuery, firstLuceneQuery, context);
-
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            firstQuery.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                QueryBuilder<? extends QueryBuilder> prototype = queryParserService.queryParser(firstQuery.getWriteableName()).getBuilderPrototype();
-                @SuppressWarnings("unchecked")
-                QB secondQuery = (QB)prototype.readFrom(in);
-                //query _name never should affect the result of toQuery, we randomly set it to make sure
-                if (randomBoolean()) {
-                    secondQuery.queryName(secondQuery.queryName() == null ? randomAsciiOfLengthBetween(1, 30) : secondQuery.queryName() + randomAsciiOfLengthBetween(1, 10));
-                }
-                Query secondLuceneQuery = secondQuery.toQuery(context);
-                assertLuceneQuery(secondQuery, secondLuceneQuery, context);
-                assertThat("two equivalent query builders lead to different lucene queries", secondLuceneQuery, equalTo(firstLuceneQuery));
-
-                //if the initial lucene query is null, changing its boost won't have any effect, we shouldn't test that
-                //otherwise makes sure that boost is taken into account in toQuery
-                if (firstLuceneQuery != null) {
-                    secondQuery.boost(firstQuery.boost() + 1f + randomFloat());
-                    //some queries don't support boost, their setter is a no-op
-                    if (supportsBoostAndQueryName()) {
-                        Query thirdLuceneQuery = secondQuery.toQuery(context);
-                        assertThat("modifying the boost doesn't affect the corresponding lucene query", firstLuceneQuery, not(equalTo(thirdLuceneQuery)));
-                    }
-                }
-            }
-        }
-    }
-
-    /**
-     * Few queries allow you to set the boost and queryName but don't do anything with it. This method allows
-     * to disable boost and queryName related tests for those queries.
-     */
-    protected boolean supportsBoostAndQueryName() {
-        return true;
-    }
-
-    /**
-     * Checks the result of {@link QueryBuilder#toQuery(QueryShardContext)} given the original {@link QueryBuilder} and {@link QueryShardContext}.
-     * Verifies that named queries and boost are properly handled and delegates to {@link #doAssertLuceneQuery(AbstractQueryBuilder, Query, QueryShardContext)}
-     * for query specific checks.
-     */
-    protected final void assertLuceneQuery(QB queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (queryBuilder.queryName() != null) {
-            Query namedQuery = context.copyNamedQueries().get(queryBuilder.queryName());
-            assertThat(namedQuery, equalTo(query));
-        }
-        if (query != null) {
-            assertThat(query.getBoost(), equalTo(queryBuilder.boost()));
-        }
-        doAssertLuceneQuery(queryBuilder, query, context);
-    }
-
-    /**
-     * Checks the result of {@link QueryBuilder#toQuery(QueryShardContext)} given the original {@link QueryBuilder} and {@link QueryShardContext}.
-     * Contains the query specific checks to be implemented by subclasses.
-     */
-    protected abstract void doAssertLuceneQuery(QB queryBuilder, Query query, QueryShardContext context) throws IOException;
-
-    /**
-     * Test serialization and deserialization of the test query.
-     */
-    @Test
-    public void testSerialization() throws IOException {
-        QB testQuery = createTestQueryBuilder();
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            testQuery.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                QueryBuilder<? extends QueryBuilder> prototype = queryParserService.queryParser(testQuery.getWriteableName()).getBuilderPrototype();
-                QueryBuilder deserializedQuery = prototype.readFrom(in);
-                assertEquals(deserializedQuery, testQuery);
-                assertEquals(deserializedQuery.hashCode(), testQuery.hashCode());
-                assertNotSame(deserializedQuery, testQuery);
-            }
-        }
-    }
-
-    /**
-     * @return a new {@link QueryShardContext} based on the base test index and queryParserService
-     */
-    protected static QueryShardContext createShardContext() {
-        QueryShardContext queryCreationContext = new QueryShardContext(index, queryParserService);
-        queryCreationContext.parseFieldMatcher(ParseFieldMatcher.EMPTY);
-        return queryCreationContext;
-    }
-
-    /**
-     * @return a new {@link QueryParseContext} based on the base test index and queryParserService
-     */
-    protected static QueryParseContext createParseContext() {
-        QueryParseContext parseContext = createShardContext().parseContext();
-        return parseContext;
-    }
-
-    protected static void assertQueryHeader(XContentParser parser, String expectedParserName) throws IOException {
-        assertThat(parser.nextToken(), is(XContentParser.Token.START_OBJECT));
-        assertThat(parser.nextToken(), is(XContentParser.Token.FIELD_NAME));
-        assertThat(parser.currentName(), is(expectedParserName));
-        assertThat(parser.nextToken(), is(XContentParser.Token.START_OBJECT));
-    }
-
-    protected static void assertValidate(QueryBuilder queryBuilder, int totalExpectedErrors) {
-        QueryValidationException queryValidationException = queryBuilder.validate();
-        if (totalExpectedErrors > 0) {
-            assertThat(queryValidationException, notNullValue());
-            assertThat(queryValidationException.validationErrors().size(), equalTo(totalExpectedErrors));
-        } else {
-            assertThat(queryValidationException, nullValue());
-        }
-    }
-
-    /**
-     * create a random value for either {@link BaseQueryTestCase#BOOLEAN_FIELD_NAME}, {@link BaseQueryTestCase#INT_FIELD_NAME},
-     * {@link BaseQueryTestCase#DOUBLE_FIELD_NAME} or {@link BaseQueryTestCase#STRING_FIELD_NAME}, or a String value by default
-     */
-    protected static Object randomValueForField(String fieldName) {
-        Object value;
-        switch (fieldName) {
-            case BOOLEAN_FIELD_NAME: value = randomBoolean(); break;
-            case INT_FIELD_NAME: value = randomInt(); break;
-            case DOUBLE_FIELD_NAME: value = randomDouble(); break;
-            case STRING_FIELD_NAME: value = randomAsciiOfLengthBetween(1, 10); break;
-            default : value = randomAsciiOfLengthBetween(1, 10);
-        }
-        return value;
-    }
-
-    /**
-     * Helper method to return a random rewrite method
-     */
-    protected static String getRandomRewriteMethod() {
-        String rewrite;
-        if (randomBoolean()) {
-            rewrite = randomFrom(QueryParsers.CONSTANT_SCORE,
-                    QueryParsers.SCORING_BOOLEAN,
-                    QueryParsers.CONSTANT_SCORE_BOOLEAN).getPreferredName();
-        } else {
-            rewrite = randomFrom(QueryParsers.TOP_TERMS,
-                    QueryParsers.TOP_TERMS_BOOST,
-                    QueryParsers.TOP_TERMS_BLENDED_FREQS).getPreferredName() + "1";
-        }
-        return rewrite;
-    }
-
-    protected String[] getRandomTypes() {
-        String[] types;
-        if (currentTypes.length > 0 && randomBoolean()) {
-            int numberOfQueryTypes = randomIntBetween(1, currentTypes.length);
-            types = new String[numberOfQueryTypes];
-            for (int i = 0; i < numberOfQueryTypes; i++) {
-                types[i] = randomFrom(currentTypes);
-            }
-        } else {
-            if (randomBoolean()) {
-                types = new String[] { MetaData.ALL };
-            } else {
-                types = new String[0];
-            }
-        }
-        return types;
-    }
-
-    protected String getRandomType() {
-        return (currentTypes.length == 0) ? MetaData.ALL : randomFrom(currentTypes);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/BaseTermQueryTestCase.java b/core/src/test/java/org/elasticsearch/index/query/BaseTermQueryTestCase.java
deleted file mode 100644
index 6da2895..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/BaseTermQueryTestCase.java
+++ /dev/null
@@ -1,91 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.junit.Test;
-
-import static org.hamcrest.Matchers.is;
-
-public abstract class BaseTermQueryTestCase<QB extends BaseTermQueryBuilder<QB>> extends BaseQueryTestCase<QB> {
-
-    @Override
-    protected final QB doCreateTestQueryBuilder() {
-        String fieldName = null;
-        Object value;
-        switch (randomIntBetween(0, 3)) {
-            case 0:
-                if (randomBoolean()) {
-                    fieldName = BOOLEAN_FIELD_NAME;
-                }
-                value = randomBoolean();
-                break;
-            case 1:
-                if (randomBoolean()) {
-                    fieldName = STRING_FIELD_NAME;
-                }
-                if (frequently()) {
-                    value = randomAsciiOfLengthBetween(1, 10);
-                } else {
-                    // generate unicode string in 10% of cases
-                    value = randomUnicodeOfLength(10);
-                }
-                break;
-            case 2:
-                if (randomBoolean()) {
-                    fieldName = INT_FIELD_NAME;
-                }
-                value = randomInt(10000);
-                break;
-            case 3:
-                if (randomBoolean()) {
-                    fieldName = DOUBLE_FIELD_NAME;
-                }
-                value = randomDouble();
-                break;
-            default:
-                throw new UnsupportedOperationException();
-        }
-
-        if (fieldName == null) {
-            fieldName = randomAsciiOfLengthBetween(1, 10);
-        }
-        return createQueryBuilder(fieldName, value);
-    }
-
-    protected abstract QB createQueryBuilder(String fieldName, Object value);
-
-    @Test
-    public void testValidate() throws QueryShardException {
-        QB queryBuilder = createQueryBuilder(randomAsciiOfLengthBetween(1, 30), randomAsciiOfLengthBetween(1, 30));
-        assertNull(queryBuilder.validate());
-
-        queryBuilder = createQueryBuilder(null, randomAsciiOfLengthBetween(1, 30));
-        assertNotNull(queryBuilder.validate());
-        assertThat(queryBuilder.validate().validationErrors().size(), is(1));
-
-        queryBuilder = createQueryBuilder("", randomAsciiOfLengthBetween(1, 30));
-        assertNotNull(queryBuilder.validate());
-        assertThat(queryBuilder.validate().validationErrors().size(), is(1));
-
-        queryBuilder = createQueryBuilder("", null);
-        assertNotNull(queryBuilder.validate());
-        assertThat(queryBuilder.validate().validationErrors().size(), is(2));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTest.java
deleted file mode 100644
index 027ab36..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTest.java
+++ /dev/null
@@ -1,175 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class BoolQueryBuilderTest extends BaseQueryTestCase<BoolQueryBuilder> {
-
-    @Override
-    protected BoolQueryBuilder doCreateTestQueryBuilder() {
-        BoolQueryBuilder query = new BoolQueryBuilder();
-        if (randomBoolean()) {
-            query.adjustPureNegative(randomBoolean());
-        }
-        if (randomBoolean()) {
-            query.disableCoord(randomBoolean());
-        }
-        if (randomBoolean()) {
-            query.minimumNumberShouldMatch(randomIntBetween(1, 10));
-        }
-        int mustClauses = randomIntBetween(0, 3);
-        for (int i = 0; i < mustClauses; i++) {
-            query.must(RandomQueryBuilder.createQuery(random()));
-        }
-        int mustNotClauses = randomIntBetween(0, 3);
-        for (int i = 0; i < mustNotClauses; i++) {
-            query.mustNot(RandomQueryBuilder.createQuery(random()));
-        }
-        int shouldClauses = randomIntBetween(0, 3);
-        for (int i = 0; i < shouldClauses; i++) {
-            query.should(RandomQueryBuilder.createQuery(random()));
-        }
-        int filterClauses = randomIntBetween(0, 3);
-        for (int i = 0; i < filterClauses; i++) {
-            query.filter(RandomQueryBuilder.createQuery(random()));
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(BoolQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (!queryBuilder.hasClauses()) {
-            assertThat(query, instanceOf(MatchAllDocsQuery.class));
-        } else {
-            List<BooleanClause> clauses = new ArrayList<>();
-            clauses.addAll(getBooleanClauses(queryBuilder.must(), BooleanClause.Occur.MUST, context));
-            clauses.addAll(getBooleanClauses(queryBuilder.mustNot(), BooleanClause.Occur.MUST_NOT, context));
-            clauses.addAll(getBooleanClauses(queryBuilder.should(), BooleanClause.Occur.SHOULD, context));
-            clauses.addAll(getBooleanClauses(queryBuilder.filter(), BooleanClause.Occur.FILTER, context));
-
-            if (clauses.isEmpty()) {
-                assertThat(query, instanceOf(MatchAllDocsQuery.class));
-            } else {
-                assertThat(query, instanceOf(BooleanQuery.class));
-                BooleanQuery booleanQuery = (BooleanQuery) query;
-                if (queryBuilder.adjustPureNegative()) {
-                    boolean isNegative = true;
-                    for (BooleanClause clause : clauses) {
-                        if (clause.isProhibited() == false) {
-                            isNegative = false;
-                            break;
-                        }
-                    }
-                    if (isNegative) {
-                        clauses.add(new BooleanClause(new MatchAllDocsQuery(), BooleanClause.Occur.MUST));
-                    }
-                }
-                assertThat(booleanQuery.clauses().size(), equalTo(clauses.size()));
-                Iterator<BooleanClause> clauseIterator = clauses.iterator();
-                for (BooleanClause booleanClause : booleanQuery.getClauses()) {
-                    assertThat(booleanClause, equalTo(clauseIterator.next()));
-                }
-            }
-        }
-    }
-
-    private static List<BooleanClause> getBooleanClauses(List<QueryBuilder> queryBuilders, BooleanClause.Occur occur, QueryShardContext context) throws IOException {
-        List<BooleanClause> clauses = new ArrayList<>();
-        for (QueryBuilder query : queryBuilders) {
-            Query innerQuery = query.toQuery(context);
-            if (innerQuery != null) {
-                clauses.add(new BooleanClause(innerQuery, occur));
-            }
-        }
-        return clauses;
-    }
-
-    @Test
-    public void testValidate() {
-        BoolQueryBuilder booleanQuery = new BoolQueryBuilder();
-        int iters = randomIntBetween(0, 3);
-        int totalExpectedErrors = 0;
-        for (int i = 0; i < iters; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    booleanQuery.must(RandomQueryBuilder.createInvalidQuery(random()));
-                } else {
-                    booleanQuery.must(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                booleanQuery.must(RandomQueryBuilder.createQuery(random()));
-            }
-        }
-        iters = randomIntBetween(0, 3);
-        for (int i = 0; i < iters; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    booleanQuery.should(RandomQueryBuilder.createInvalidQuery(random()));
-                } else {
-                    booleanQuery.should(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                booleanQuery.should(RandomQueryBuilder.createQuery(random()));
-            }
-        }
-        iters = randomIntBetween(0, 3);
-        for (int i = 0; i < iters; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    booleanQuery.mustNot(RandomQueryBuilder.createInvalidQuery(random()));
-                } else {
-                    booleanQuery.mustNot(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                booleanQuery.mustNot(RandomQueryBuilder.createQuery(random()));
-            }
-        }
-        iters = randomIntBetween(0, 3);
-        for (int i = 0; i < iters; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    booleanQuery.filter(RandomQueryBuilder.createInvalidQuery(random()));
-                } else {
-                    booleanQuery.filter(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                booleanQuery.filter(RandomQueryBuilder.createQuery(random()));
-            }
-        }
-        assertValidate(booleanQuery, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/BoostingQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/BoostingQueryBuilderTest.java
deleted file mode 100644
index 72c157e..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/BoostingQueryBuilderTest.java
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.queries.BoostingQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.instanceOf;
-import static org.hamcrest.CoreMatchers.nullValue;
-
-public class BoostingQueryBuilderTest extends BaseQueryTestCase<BoostingQueryBuilder> {
-
-    @Override
-    protected BoostingQueryBuilder doCreateTestQueryBuilder() {
-        BoostingQueryBuilder query = new BoostingQueryBuilder(RandomQueryBuilder.createQuery(random()), RandomQueryBuilder.createQuery(random()));
-        query.negativeBoost(2.0f / randomIntBetween(1, 20));
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(BoostingQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Query positive = queryBuilder.positive().toQuery(context);
-        Query negative = queryBuilder.negative().toQuery(context);
-        if (positive == null || negative == null) {
-            assertThat(query, nullValue());
-        } else {
-            assertThat(query, instanceOf(BoostingQuery.class));
-        }
-    }
-
-    @Test
-    public void testValidate() {
-        int totalExpectedErrors = 0;
-        QueryBuilder positive = null;
-        QueryBuilder negative = null;
-        if (frequently()) {
-            if (randomBoolean()) {
-                negative = RandomQueryBuilder.createInvalidQuery(random());
-            }
-            totalExpectedErrors++;
-        } else {
-            negative = RandomQueryBuilder.createQuery(random());
-        }
-        if (frequently()) {
-            if (randomBoolean()) {
-                positive = RandomQueryBuilder.createInvalidQuery(random());
-            }
-            totalExpectedErrors++;
-        } else {
-            positive = RandomQueryBuilder.createQuery(random());
-        }
-        BoostingQueryBuilder boostingQuery = new BoostingQueryBuilder(positive, negative);
-        if (frequently()) {
-            boostingQuery.negativeBoost(0.5f);
-        } else {
-            boostingQuery.negativeBoost(-0.5f);
-            totalExpectedErrors++;
-        }
-        assertValidate(boostingQuery, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/CommonTermsQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/CommonTermsQueryBuilderTest.java
deleted file mode 100644
index ab5cb07..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/CommonTermsQueryBuilderTest.java
+++ /dev/null
@@ -1,105 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.queries.ExtendedCommonTermsQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-
-public class CommonTermsQueryBuilderTest extends BaseQueryTestCase<CommonTermsQueryBuilder> {
-
-    @Override
-    protected CommonTermsQueryBuilder doCreateTestQueryBuilder() {
-        CommonTermsQueryBuilder query;
-
-        // mapped or unmapped field
-        String text = randomAsciiOfLengthBetween(1, 10);
-        if (randomBoolean()) {
-            query = new CommonTermsQueryBuilder(STRING_FIELD_NAME, text);
-        } else {
-            query = new CommonTermsQueryBuilder(randomAsciiOfLengthBetween(1, 10), text);
-        }
-
-        if (randomBoolean()) {
-            query.cutoffFrequency((float) randomIntBetween(1, 10));
-        }
-
-        if (randomBoolean()) {
-            query.lowFreqOperator(randomFrom(Operator.values()));
-        }
-
-        // number of low frequency terms that must match
-        if (randomBoolean()) {
-            query.lowFreqMinimumShouldMatch("" + randomIntBetween(1, 5));
-        }
-
-        if (randomBoolean()) {
-            query.highFreqOperator(randomFrom(Operator.values()));
-        }
-
-        // number of high frequency terms that must match
-        if (randomBoolean()) {
-            query.highFreqMinimumShouldMatch("" + randomIntBetween(1, 5));
-        }
-
-        if (randomBoolean()) {
-            query.analyzer(randomFrom("simple", "keyword", "whitespace"));
-        }
-
-        if (randomBoolean()) {
-            query.disableCoord(randomBoolean());
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(CommonTermsQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(ExtendedCommonTermsQuery.class));
-        ExtendedCommonTermsQuery extendedCommonTermsQuery = (ExtendedCommonTermsQuery) query;
-        assertThat(extendedCommonTermsQuery.getHighFreqMinimumNumberShouldMatchSpec(), equalTo(queryBuilder.highFreqMinimumShouldMatch()));
-        assertThat(extendedCommonTermsQuery.getLowFreqMinimumNumberShouldMatchSpec(), equalTo(queryBuilder.lowFreqMinimumShouldMatch()));
-    }
-
-    @Test
-    public void testValidate() {
-        CommonTermsQueryBuilder commonTermsQueryBuilder = new CommonTermsQueryBuilder("", "text");
-        assertThat(commonTermsQueryBuilder.validate().validationErrors().size(), is(1));
-
-        commonTermsQueryBuilder = new CommonTermsQueryBuilder("field", null);
-        assertThat(commonTermsQueryBuilder.validate().validationErrors().size(), is(1));
-
-        commonTermsQueryBuilder = new CommonTermsQueryBuilder("field", "text");
-        assertNull(commonTermsQueryBuilder.validate());
-    }
-
-    @Test
-    public void testNoTermsFromQueryString() throws IOException {
-        CommonTermsQueryBuilder builder = new CommonTermsQueryBuilder(STRING_FIELD_NAME, "");
-        QueryShardContext context = createShardContext();
-        context.setAllowUnmappedFields(true);
-        assertNull(builder.toQuery(context));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/CommonTermsQueryParserTest.java b/core/src/test/java/org/elasticsearch/index/query/CommonTermsQueryParserTest.java
new file mode 100644
index 0000000..c95477e
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/query/CommonTermsQueryParserTest.java
@@ -0,0 +1,52 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.query;
+
+import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.test.ESSingleNodeTestCase;
+import org.junit.Test;
+
+import java.io.IOException;
+
+public class CommonTermsQueryParserTest extends ESSingleNodeTestCase {
+    @Test
+    public void testWhenParsedQueryIsNullNoNullPointerExceptionIsThrown() throws IOException {
+        final String index = "test-index";
+        final String type = "test-type";
+        client()
+                .admin()
+                .indices()
+                .prepareCreate(index)
+                .addMapping(type, "name", "type=string,analyzer=stop")
+                .execute()
+                .actionGet();
+        ensureGreen();
+
+        CommonTermsQueryBuilder commonTermsQueryBuilder =
+                new CommonTermsQueryBuilder("name", "the").queryName("query-name");
+
+        // the named query parses to null; we are testing this does not cause a NullPointerException
+        SearchResponse response =
+                client().prepareSearch(index).setTypes(type).setQuery(commonTermsQueryBuilder).execute().actionGet();
+
+        assertNotNull(response);
+        assertEquals(response.getHits().hits().length, 0);
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTest.java
deleted file mode 100644
index e05eb26..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTest.java
+++ /dev/null
@@ -1,85 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-import static org.hamcrest.CoreMatchers.nullValue;
-
-public class ConstantScoreQueryBuilderTest extends BaseQueryTestCase<ConstantScoreQueryBuilder> {
-
-    /**
-     * @return a {@link ConstantScoreQueryBuilder} with random boost between 0.1f and 2.0f
-     */
-    @Override
-    protected ConstantScoreQueryBuilder doCreateTestQueryBuilder() {
-        return new ConstantScoreQueryBuilder(RandomQueryBuilder.createQuery(random()));
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(ConstantScoreQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Query innerQuery = queryBuilder.query().toQuery(context);
-        if (innerQuery == null) {
-            assertThat(query, nullValue());
-        } else {
-            assertThat(query, instanceOf(ConstantScoreQuery.class));
-            ConstantScoreQuery constantScoreQuery = (ConstantScoreQuery) query;
-            assertThat(constantScoreQuery.getQuery(), equalTo(innerQuery));
-        }
-    }
-
-    /**
-     * test that missing "filter" element causes {@link QueryParsingException}
-     */
-    @Test(expected=QueryParsingException.class)
-    public void testFilterElement() throws IOException {
-        QueryParseContext context = createParseContext();
-        String queryId = ConstantScoreQueryBuilder.NAME;
-        String queryString = "{ \""+queryId+"\" : {}";
-        XContentParser parser = XContentFactory.xContent(queryString).createParser(queryString);
-        context.reset(parser);
-        assertQueryHeader(parser, queryId);
-        context.queryParser(queryId).fromXContent(context);
-    }
-
-    @Test
-    public void testValidate() {
-        QueryBuilder innerQuery = null;
-        int totalExpectedErrors = 0;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                innerQuery = RandomQueryBuilder.createInvalidQuery(random());
-            }
-            totalExpectedErrors++;
-        } else {
-            innerQuery = RandomQueryBuilder.createQuery(random());
-        }
-        ConstantScoreQueryBuilder constantScoreQuery = new ConstantScoreQueryBuilder(innerQuery);
-        assertValidate(constantScoreQuery, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/DisMaxQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/DisMaxQueryBuilderTest.java
deleted file mode 100644
index bde2910..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/DisMaxQueryBuilderTest.java
+++ /dev/null
@@ -1,121 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.DisjunctionMaxQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.Collection;
-import java.util.Iterator;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-import static org.hamcrest.CoreMatchers.nullValue;
-
-public class DisMaxQueryBuilderTest extends BaseQueryTestCase<DisMaxQueryBuilder> {
-
-    /**
-     * @return a {@link DisMaxQueryBuilder} with random inner queries
-     */
-    @Override
-    protected DisMaxQueryBuilder doCreateTestQueryBuilder() {
-        DisMaxQueryBuilder dismax = new DisMaxQueryBuilder();
-        int clauses = randomIntBetween(1, 5);
-        for (int i = 0; i < clauses; i++) {
-            dismax.add(RandomQueryBuilder.createQuery(random()));
-        }
-        if (randomBoolean()) {
-            dismax.tieBreaker(2.0f / randomIntBetween(1, 20));
-        }
-        return dismax;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(DisMaxQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Collection<Query> queries = AbstractQueryBuilder.toQueries(queryBuilder.queries(), context);
-        if (queries.isEmpty()) {
-            assertThat(query, nullValue());
-        } else {
-            assertThat(query, instanceOf(DisjunctionMaxQuery.class));
-            DisjunctionMaxQuery disjunctionMaxQuery = (DisjunctionMaxQuery) query;
-            assertThat(disjunctionMaxQuery.getTieBreakerMultiplier(), equalTo(queryBuilder.tieBreaker()));
-            assertThat(disjunctionMaxQuery.getDisjuncts().size(), equalTo(queries.size()));
-            Iterator<Query> queryIterator = queries.iterator();
-            for (int i = 0; i < disjunctionMaxQuery.getDisjuncts().size(); i++) {
-                assertThat(disjunctionMaxQuery.getDisjuncts().get(i), equalTo(queryIterator.next()));
-            }
-        }
-    }
-
-    /**
-     * test `null`return value for missing inner queries
-     * @throws IOException
-     * @throws QueryParsingException
-     */
-    @Test
-    public void testNoInnerQueries() throws QueryParsingException, IOException {
-        DisMaxQueryBuilder disMaxBuilder = new DisMaxQueryBuilder();
-        assertNull(disMaxBuilder.toQuery(createShardContext()));
-        assertNull(disMaxBuilder.validate());
-    }
-
-    /**
-     * Test inner query parsing to null. Current DSL allows inner filter element to parse to <tt>null</tt>.
-     * Those should be ignored upstream. To test this, we use inner {@link ConstantScoreQueryBuilder}
-     * with empty inner filter.
-     */
-    @Test
-    public void testInnerQueryReturnsNull() throws IOException {
-        QueryParseContext context = createParseContext();
-        String queryId = ConstantScoreQueryBuilder.NAME;
-        String queryString = "{ \""+queryId+"\" : { \"filter\" : { } }";
-        XContentParser parser = XContentFactory.xContent(queryString).createParser(queryString);
-        context.reset(parser);
-        assertQueryHeader(parser, queryId);
-        ConstantScoreQueryBuilder innerQueryBuilder = (ConstantScoreQueryBuilder) context.queryParser(queryId).fromXContent(context);
-
-        DisMaxQueryBuilder disMaxBuilder = new DisMaxQueryBuilder().add(innerQueryBuilder);
-        assertNull(disMaxBuilder.toQuery(createShardContext()));
-    }
-
-    @Test
-    public void testValidate() {
-        DisMaxQueryBuilder disMaxQuery = new DisMaxQueryBuilder();
-        int iters = randomIntBetween(0, 5);
-        int totalExpectedErrors = 0;
-        for (int i = 0; i < iters; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    disMaxQuery.add(RandomQueryBuilder.createInvalidQuery(random()));
-                } else {
-                    disMaxQuery.add(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                disMaxQuery.add(RandomQueryBuilder.createQuery(random()));
-            }
-        }
-        assertValidate(disMaxQuery, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/ExistsQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/ExistsQueryBuilderTest.java
deleted file mode 100644
index 752d292..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/ExistsQueryBuilderTest.java
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.index.mapper.object.ObjectMapper;
-
-import java.io.IOException;
-import java.util.Collection;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class ExistsQueryBuilderTest extends BaseQueryTestCase<ExistsQueryBuilder> {
-
-    @Override
-    protected ExistsQueryBuilder doCreateTestQueryBuilder() {
-        String fieldPattern;
-        if (randomBoolean()) {
-            fieldPattern = randomFrom(mappedFieldNames);
-        } else {
-            fieldPattern = randomAsciiOfLengthBetween(1, 10);
-        }
-        // also sometimes test wildcard patterns
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                fieldPattern = fieldPattern + "*";
-            } else {
-                fieldPattern = MetaData.ALL;
-            }
-        }
-        return new ExistsQueryBuilder(fieldPattern);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(ExistsQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        String fieldPattern = queryBuilder.name();
-        ObjectMapper objectMapper = context.getObjectMapper(fieldPattern);
-        if (objectMapper != null) {
-            // automatic make the object mapper pattern
-            fieldPattern = fieldPattern + ".*";
-        }
-        Collection<String> fields = context.simpleMatchToIndexNames(fieldPattern);
-        if (getCurrentTypes().length == 0 || fields.size() == 0) {
-            assertThat(query, instanceOf(BooleanQuery.class));
-            BooleanQuery booleanQuery = (BooleanQuery) query;
-            assertThat(booleanQuery.clauses().size(), equalTo(0));
-        } else {
-            assertThat(query, instanceOf(ConstantScoreQuery.class));
-            ConstantScoreQuery constantScoreQuery = (ConstantScoreQuery) query;
-            assertThat(constantScoreQuery.getQuery(), instanceOf(BooleanQuery.class));
-            BooleanQuery booleanQuery = (BooleanQuery) constantScoreQuery.getQuery();
-            assertThat(booleanQuery.clauses().size(), equalTo(fields.size()));
-            for (int i = 0; i < fields.size(); i++) {
-                BooleanClause booleanClause = booleanQuery.clauses().get(i);
-                assertThat(booleanClause.getOccur(), equalTo(BooleanClause.Occur.SHOULD));
-            }
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/FQueryFilterBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/FQueryFilterBuilderTest.java
deleted file mode 100644
index 133c6b1..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/FQueryFilterBuilderTest.java
+++ /dev/null
@@ -1,101 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-import static org.hamcrest.CoreMatchers.nullValue;
-
-@SuppressWarnings("deprecation")
-public class FQueryFilterBuilderTest extends BaseQueryTestCase<FQueryFilterBuilder> {
-
-    /**
-     * @return a FQueryFilterBuilder with random inner query
-     */
-    @Override
-    protected FQueryFilterBuilder doCreateTestQueryBuilder() {
-        QueryBuilder innerQuery = RandomQueryBuilder.createQuery(random());
-        return new FQueryFilterBuilder(innerQuery);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(FQueryFilterBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Query innerQuery = queryBuilder.innerQuery().toQuery(context);
-        if (innerQuery == null) {
-            assertThat(query, nullValue());
-        } else {
-            assertThat(query, instanceOf(ConstantScoreQuery.class));
-            ConstantScoreQuery constantScoreQuery = (ConstantScoreQuery) query;
-            assertThat(constantScoreQuery.getQuery(), equalTo(innerQuery));
-        }
-    }
-
-    /**
-     * test corner case where no inner query exist
-     */
-    @Test
-    public void testNoInnerQuery() throws QueryParsingException, IOException {
-        FQueryFilterBuilder queryFilterQuery = new FQueryFilterBuilder(EmptyQueryBuilder.PROTOTYPE);
-        assertNull(queryFilterQuery.toQuery(createShardContext()));
-    }
-
-    /**
-     * test wrapping an inner filter that returns null also returns <tt>null</null> to pass on upwards
-     */
-    @Test
-    public void testInnerQueryReturnsNull() throws IOException {
-        QueryParseContext context = createParseContext();
-
-        // create inner filter
-        String queryString = "{ \"constant_score\" : { \"filter\" : {} }";
-        XContentParser parser = XContentFactory.xContent(queryString).createParser(queryString);
-        context.reset(parser);
-        assertQueryHeader(parser, ConstantScoreQueryBuilder.NAME);
-        QueryBuilder innerQuery = context.queryParser(ConstantScoreQueryBuilder.NAME).fromXContent(context);
-
-        // check that when wrapping this filter, toQuery() returns null
-        FQueryFilterBuilder queryFilterQuery = new FQueryFilterBuilder(innerQuery);
-        assertNull(queryFilterQuery.toQuery(createShardContext()));
-    }
-
-    @Test
-    public void testValidate() {
-        QueryBuilder innerQuery = null;
-        int totalExpectedErrors = 0;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                innerQuery = RandomQueryBuilder.createInvalidQuery(random());
-            }
-            totalExpectedErrors++;
-        } else {
-            innerQuery = RandomQueryBuilder.createQuery(random());
-        }
-        FQueryFilterBuilder fQueryFilter = new FQueryFilterBuilder(innerQuery);
-        assertValidate(fQueryFilter, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilderTest.java
deleted file mode 100644
index 11291e0..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilderTest.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.FieldMaskingSpanQuery;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class FieldMaskingSpanQueryBuilderTest extends BaseQueryTestCase<FieldMaskingSpanQueryBuilder> {
-
-    @Override
-    protected FieldMaskingSpanQueryBuilder doCreateTestQueryBuilder() {
-        String fieldName;
-        if (randomBoolean()) {
-            fieldName = randomFrom(mappedFieldNames);
-        } else {
-            fieldName = randomAsciiOfLengthBetween(1, 10);
-        }
-        SpanTermQueryBuilder innerQuery = new SpanTermQueryBuilderTest().createTestQueryBuilder();
-        return new FieldMaskingSpanQueryBuilder(innerQuery, fieldName);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(FieldMaskingSpanQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        String fieldInQuery = queryBuilder.fieldName();
-        MappedFieldType fieldType = context.fieldMapper(fieldInQuery);
-        if (fieldType != null) {
-            fieldInQuery = fieldType.names().indexName();
-        }
-        assertThat(query, instanceOf(FieldMaskingSpanQuery.class));
-        FieldMaskingSpanQuery fieldMaskingSpanQuery = (FieldMaskingSpanQuery) query;
-        assertThat(fieldMaskingSpanQuery.getField(), equalTo(fieldInQuery));
-        assertThat(fieldMaskingSpanQuery.getMaskedQuery(), equalTo(queryBuilder.innerQuery().toQuery(context)));
-    }
-
-    @Test
-    public void testValidate() {
-        String fieldName = null;
-        SpanQueryBuilder spanQueryBuilder = null;
-        int totalExpectedErrors = 0;
-        if (randomBoolean()) {
-            fieldName = "fieldName";
-        } else {
-            if (randomBoolean()) {
-                fieldName = "";
-            }
-            totalExpectedErrors++;
-        }
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                spanQueryBuilder = new SpanTermQueryBuilder("", "test");
-            }
-            totalExpectedErrors++;
-        } else {
-            spanQueryBuilder = new SpanTermQueryBuilder("name", "value");
-        }
-        FieldMaskingSpanQueryBuilder queryBuilder = new FieldMaskingSpanQueryBuilder(spanQueryBuilder, fieldName);
-        assertValidate(queryBuilder, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/FilteredQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/FilteredQueryBuilderTest.java
deleted file mode 100644
index 4673e08..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/FilteredQueryBuilderTest.java
+++ /dev/null
@@ -1,113 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.lucene.search.Queries;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-import static org.hamcrest.CoreMatchers.nullValue;
-
-@SuppressWarnings("deprecation")
-public class FilteredQueryBuilderTest extends BaseQueryTestCase<FilteredQueryBuilder> {
-
-    @Override
-    protected FilteredQueryBuilder doCreateTestQueryBuilder() {
-        QueryBuilder queryBuilder = RandomQueryBuilder.createQuery(random());
-        QueryBuilder filterBuilder = RandomQueryBuilder.createQuery(random());
-        return new FilteredQueryBuilder(queryBuilder, filterBuilder);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(FilteredQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Query innerQuery = queryBuilder.query().toQuery(context);
-        if (innerQuery == null) {
-            assertThat(query, nullValue());
-        } else {
-            Query innerFilter = queryBuilder.filter().toQuery(context);
-            if (innerFilter == null || Queries.isConstantMatchAllQuery(innerFilter)) {
-                innerQuery.setBoost(queryBuilder.boost());
-                assertThat(query, equalTo(innerQuery));
-            } else if (Queries.isConstantMatchAllQuery(innerQuery)) {
-                assertThat(query, instanceOf(ConstantScoreQuery.class));
-                assertThat(((ConstantScoreQuery)query).getQuery(), equalTo(innerFilter));
-            } else {
-                assertThat(query, instanceOf(BooleanQuery.class));
-                BooleanQuery booleanQuery = (BooleanQuery) query;
-                assertThat(booleanQuery.clauses().size(), equalTo(2));
-                assertThat(booleanQuery.clauses().get(0).getOccur(), equalTo(BooleanClause.Occur.MUST));
-                assertThat(booleanQuery.clauses().get(0).getQuery(), equalTo(innerQuery));
-                assertThat(booleanQuery.clauses().get(1).getOccur(), equalTo(BooleanClause.Occur.FILTER));
-                assertThat(booleanQuery.clauses().get(1).getQuery(), equalTo(innerFilter));
-            }
-        }
-    }
-
-    @Test
-    public void testValidation() {
-        QueryBuilder valid = RandomQueryBuilder.createQuery(random());
-        QueryBuilder invalid = RandomQueryBuilder.createInvalidQuery(random());
-
-        // invalid cases
-        FilteredQueryBuilder qb = new FilteredQueryBuilder(invalid);
-        QueryValidationException result = qb.validate();
-        assertNotNull(result);
-        assertEquals(1, result.validationErrors().size());
-
-        qb = new FilteredQueryBuilder(valid, invalid);
-        result = qb.validate();
-        assertNotNull(result);
-        assertEquals(1, result.validationErrors().size());
-
-        qb = new FilteredQueryBuilder(invalid, valid);
-        result = qb.validate();
-        assertNotNull(result);
-        assertEquals(1, result.validationErrors().size());
-
-        qb = new FilteredQueryBuilder(invalid, invalid);
-        result = qb.validate();
-        assertNotNull(result);
-        assertEquals(2, result.validationErrors().size());
-
-        // valid cases
-        qb = new FilteredQueryBuilder(valid);
-        assertNull(qb.validate());
-
-        qb = new FilteredQueryBuilder(null);
-        assertNull(qb.validate());
-
-        qb = new FilteredQueryBuilder(null, valid);
-        assertNull(qb.validate());
-
-        qb = new FilteredQueryBuilder(valid, null);
-        assertNull(qb.validate());
-
-        qb = new FilteredQueryBuilder(valid, valid);
-        assertNull(qb.validate());
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTest.java
deleted file mode 100644
index ecdab89..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTest.java
+++ /dev/null
@@ -1,97 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-
-import org.apache.lucene.queries.TermsQuery;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class IdsQueryBuilderTest extends BaseQueryTestCase<IdsQueryBuilder> {
-
-    /**
-     * check that parser throws exception on missing values field
-     * @throws IOException
-     */
-    @Test(expected=QueryParsingException.class)
-    public void testIdsNotProvided() throws IOException {
-        String noIdsFieldQuery = "{\"ids\" : { \"type\" : \"my_type\"  }";
-        XContentParser parser = XContentFactory.xContent(noIdsFieldQuery).createParser(noIdsFieldQuery);
-        QueryParseContext context = createParseContext();
-        context.reset(parser);
-        assertQueryHeader(parser, "ids");
-        context.queryParser("ids").fromXContent(context);
-    }
-
-    @Override
-    protected IdsQueryBuilder doCreateTestQueryBuilder() {
-        String[] types;
-        if (getCurrentTypes().length > 0 && randomBoolean()) {
-            int numberOfTypes = randomIntBetween(1, getCurrentTypes().length);
-            types = new String[numberOfTypes];
-            for (int i = 0; i < numberOfTypes; i++) {
-                if (frequently()) {
-                    types[i] = randomFrom(getCurrentTypes());
-                } else {
-                    types[i] = randomAsciiOfLengthBetween(1, 10);
-                }
-            }
-        } else {
-            if (randomBoolean()) {
-                types = new String[]{MetaData.ALL};
-            } else {
-                types = new String[0];
-            }
-        }
-        int numberOfIds = randomIntBetween(0, 10);
-        String[] ids = new String[numberOfIds];
-        for (int i = 0; i < numberOfIds; i++) {
-            ids[i] = randomAsciiOfLengthBetween(1, 10);
-        }
-        IdsQueryBuilder query;
-        if (types.length > 0 || randomBoolean()) {
-            query = new IdsQueryBuilder(types);
-            query.addIds(ids);
-        } else {
-            query = new IdsQueryBuilder();
-            query.addIds(ids);
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(IdsQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (queryBuilder.ids().size() == 0) {
-            assertThat(query, instanceOf(BooleanQuery.class));
-            assertThat(((BooleanQuery)query).clauses().size(), equalTo(0));
-        } else {
-            assertThat(query, instanceOf(TermsQuery.class));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeTimezoneTests.java b/core/src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeTimezoneTests.java
index 6222f3b..d581aa6 100644
--- a/core/src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeTimezoneTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeTimezoneTests.java
@@ -83,7 +83,7 @@ public class IndexQueryParserFilterDateRangeTimezoneTests extends ESSingleNodeTe
             SearchContext.setCurrent(new TestSearchContext());
             queryParser.parse(query).query();
             fail("A Range Filter on a numeric field with a TimeZone should raise a QueryParsingException");
-        } catch (QueryShardException e) {
+        } catch (QueryParsingException e) {
             // We expect it
         } finally {
             SearchContext.removeCurrent();
@@ -120,7 +120,7 @@ public class IndexQueryParserFilterDateRangeTimezoneTests extends ESSingleNodeTe
             SearchContext.setCurrent(new TestSearchContext());
             queryParser.parse(query).query();
             fail("A Range Query on a numeric field with a TimeZone should raise a QueryParsingException");
-        } catch (QueryShardException e) {
+        } catch (QueryParsingException e) {
             // We expect it
         } finally {
             SearchContext.removeCurrent();
diff --git a/core/src/test/java/org/elasticsearch/index/query/LimitQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/LimitQueryBuilderTest.java
deleted file mode 100644
index 59bb644..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/LimitQueryBuilderTest.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class LimitQueryBuilderTest extends BaseQueryTestCase<LimitQueryBuilder> {
-
-    /**
-     * @return a LimitQueryBuilder with random limit between 0 and 20
-     */
-    @Override
-    protected LimitQueryBuilder doCreateTestQueryBuilder() {
-        return new LimitQueryBuilder(randomIntBetween(0, 20));
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(LimitQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(MatchAllDocsQuery.class));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/MatchAllQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/MatchAllQueryBuilderTest.java
deleted file mode 100644
index 277717c..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/MatchAllQueryBuilderTest.java
+++ /dev/null
@@ -1,40 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class MatchAllQueryBuilderTest extends BaseQueryTestCase<MatchAllQueryBuilder> {
-
-    @Override
-    protected MatchAllQueryBuilder doCreateTestQueryBuilder() {
-        return new MatchAllQueryBuilder();
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(MatchAllQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(MatchAllDocsQuery.class));
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/index/query/MissingQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/MissingQueryBuilderTest.java
deleted file mode 100644
index becba55..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/MissingQueryBuilderTest.java
+++ /dev/null
@@ -1,73 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.is;
-
-public class MissingQueryBuilderTest extends BaseQueryTestCase<MissingQueryBuilder> {
-
-    @Override
-    protected MissingQueryBuilder doCreateTestQueryBuilder() {
-        MissingQueryBuilder query  = new MissingQueryBuilder(randomBoolean() ? randomFrom(mappedFieldNames) : randomAsciiOfLengthBetween(1, 10));
-        if (randomBoolean()) {
-            query.nullValue(randomBoolean());
-        }
-        if (randomBoolean()) {
-            query.existence(randomBoolean());
-        }
-        // cannot set both to false
-        if ((query.nullValue() == false) && (query.existence() == false)) {
-            query.existence(!query.existence());
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(MissingQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        //too many mapping dependent cases to test, we don't want to end up duplication the toQuery method
-    }
-
-    @Test
-    public void testValidate() {
-        MissingQueryBuilder missingQueryBuilder = new MissingQueryBuilder("");
-        assertThat(missingQueryBuilder.validate().validationErrors().size(), is(1));
-
-        missingQueryBuilder = new MissingQueryBuilder(null);
-        assertThat(missingQueryBuilder.validate().validationErrors().size(), is(1));
-
-        missingQueryBuilder = new MissingQueryBuilder("field").existence(false).nullValue(false);
-        assertThat(missingQueryBuilder.validate().validationErrors().size(), is(1));
-
-        missingQueryBuilder = new MissingQueryBuilder("field");
-        assertNull(missingQueryBuilder.validate());
-    }
-
-    @Test(expected = QueryShardException.class)
-    public void testBothNullValueAndExistenceFalse() throws IOException {
-        QueryShardContext context = createShardContext();
-        context.setAllowUnmappedFields(true);
-        MissingQueryBuilder.newFilter(context, "field", false, false);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/NotQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/NotQueryBuilderTest.java
deleted file mode 100644
index 8a3a101..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/NotQueryBuilderTest.java
+++ /dev/null
@@ -1,90 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-import static org.hamcrest.CoreMatchers.nullValue;
-
-public class NotQueryBuilderTest extends BaseQueryTestCase<NotQueryBuilder> {
-
-    /**
-     * @return a NotQueryBuilder with random limit between 0 and 20
-     */
-    @Override
-    protected NotQueryBuilder doCreateTestQueryBuilder() {
-        return new NotQueryBuilder(RandomQueryBuilder.createQuery(random()));
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(NotQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Query filter = queryBuilder.filter().toQuery(context);
-        if (filter == null) {
-            assertThat(query, nullValue());
-        } else {
-            assertThat(query, instanceOf(BooleanQuery.class));
-            BooleanQuery booleanQuery = (BooleanQuery) query;
-            assertThat(booleanQuery.clauses().size(), equalTo(2));
-            assertThat(booleanQuery.clauses().get(0).getOccur(), equalTo(BooleanClause.Occur.MUST));
-            assertThat(booleanQuery.clauses().get(0).getQuery(), instanceOf(MatchAllDocsQuery.class));
-            assertThat(booleanQuery.clauses().get(1).getOccur(), equalTo(BooleanClause.Occur.MUST_NOT));
-            assertThat(booleanQuery.clauses().get(1).getQuery(), equalTo(filter));
-        }
-    }
-
-    /**
-     * @throws IOException
-     */
-    @Test(expected=QueryParsingException.class)
-    public void testMissingFilterSection() throws IOException {
-        QueryParseContext context = createParseContext();
-        String queryString = "{ \"not\" : {}";
-        XContentParser parser = XContentFactory.xContent(queryString).createParser(queryString);
-        context.reset(parser);
-        assertQueryHeader(parser, NotQueryBuilder.NAME);
-        context.queryParser(NotQueryBuilder.NAME).fromXContent(context);
-    }
-
-    @Test
-    public void testValidate() {
-        QueryBuilder innerQuery = null;
-        int totalExpectedErrors = 0;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                innerQuery = RandomQueryBuilder.createInvalidQuery(random());
-            }
-            totalExpectedErrors++;
-        } else {
-            innerQuery = RandomQueryBuilder.createQuery(random());
-        }
-        NotQueryBuilder notQuery = new NotQueryBuilder(innerQuery);
-        assertValidate(notQuery, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/OrQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/OrQueryBuilderTest.java
deleted file mode 100644
index efb9e64..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/OrQueryBuilderTest.java
+++ /dev/null
@@ -1,141 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-import static org.hamcrest.CoreMatchers.nullValue;
-
-@SuppressWarnings("deprecation")
-public class OrQueryBuilderTest extends BaseQueryTestCase<OrQueryBuilder> {
-
-/*
-    @Override
-    protected Query doCreateExpectedQuery(OrQueryBuilder queryBuilder, QueryCreationContext context) throws QueryCreationException, IOException {
-        if (queryBuilder.filters().isEmpty()) {
-            return null;
-        }
-        BooleanQuery query = new BooleanQuery();
-        for (QueryBuilder subQuery : queryBuilder.filters()) {
-            Query innerQuery = subQuery.toQuery(context);
-            // ignore queries that are null
-            if (innerQuery != null) {
-                query.add(innerQuery, Occur.SHOULD);
-            }
-        }
-        if (query.clauses().isEmpty()) {
-            return null;
-        }
-        return query;
-    }
-*/
-
-    /**
-     * @return an OrQueryBuilder with random limit between 0 and 20
-     */
-    @Override
-    protected OrQueryBuilder doCreateTestQueryBuilder() {
-        OrQueryBuilder query = new OrQueryBuilder();
-        int subQueries = randomIntBetween(1, 5);
-        for (int i = 0; i < subQueries; i++ ) {
-            query.add(RandomQueryBuilder.createQuery(random()));
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(OrQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (queryBuilder.filters().isEmpty()) {
-            assertThat(query, nullValue());
-        } else {
-            List<Query> innerQueries = new ArrayList<>();
-            for (QueryBuilder subQuery : queryBuilder.filters()) {
-                Query innerQuery = subQuery.toQuery(context);
-                // ignore queries that are null
-                if (innerQuery != null) {
-                    innerQueries.add(innerQuery);
-                }
-            }
-            if (innerQueries.isEmpty()) {
-                assertThat(query, nullValue());
-            } else {
-                assertThat(query, instanceOf(BooleanQuery.class));
-                BooleanQuery booleanQuery = (BooleanQuery) query;
-                assertThat(booleanQuery.clauses().size(), equalTo(innerQueries.size()));
-                Iterator<Query> queryIterator = innerQueries.iterator();
-                for (BooleanClause booleanClause : booleanQuery) {
-                    assertThat(booleanClause.getOccur(), equalTo(BooleanClause.Occur.SHOULD));
-                    assertThat(booleanClause.getQuery(), equalTo(queryIterator.next()));
-                }
-            }
-        }
-    }
-
-    /**
-     * test corner case where no inner queries exist
-     */
-    @Test
-    public void testNoInnerQueries() throws QueryShardException, IOException {
-        OrQueryBuilder orQuery = new OrQueryBuilder();
-        assertNull(orQuery.toQuery(createShardContext()));
-    }
-
-    @Test(expected=QueryParsingException.class)
-    public void testMissingFiltersSection() throws IOException {
-        QueryParseContext context = createParseContext();
-        String queryString = "{ \"or\" : {}";
-        XContentParser parser = XContentFactory.xContent(queryString).createParser(queryString);
-        context.reset(parser);
-        assertQueryHeader(parser, OrQueryBuilder.NAME);
-        context.queryParser(OrQueryBuilder.NAME).fromXContent(context);
-    }
-
-    @Test
-    public void testValidate() {
-        OrQueryBuilder orQuery = new OrQueryBuilder();
-        int iters = randomIntBetween(0, 5);
-        int totalExpectedErrors = 0;
-        for (int i = 0; i < iters; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    orQuery.add(RandomQueryBuilder.createInvalidQuery(random()));
-                } else {
-                    orQuery.add(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                orQuery.add(RandomQueryBuilder.createQuery(random()));
-            }
-        }
-        assertValidate(orQuery, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/PrefixQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/PrefixQueryBuilderTest.java
deleted file mode 100644
index 42fe5ab..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/PrefixQueryBuilderTest.java
+++ /dev/null
@@ -1,67 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.PrefixQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-
-public class PrefixQueryBuilderTest extends BaseQueryTestCase<PrefixQueryBuilder> {
-
-    @Override
-    protected PrefixQueryBuilder doCreateTestQueryBuilder() {
-        String fieldName = randomBoolean() ? STRING_FIELD_NAME : randomAsciiOfLengthBetween(1, 10);
-        String value = randomAsciiOfLengthBetween(1, 10);
-        PrefixQueryBuilder query = new PrefixQueryBuilder(fieldName, value);
-
-        if (randomBoolean()) {
-            query.rewrite(getRandomRewriteMethod());
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(PrefixQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(PrefixQuery.class));
-        PrefixQuery prefixQuery = (PrefixQuery) query;
-        assertThat(prefixQuery.getPrefix().field(), equalTo(queryBuilder.fieldName()));
-    }
-
-    @Test
-    public void testValidate() {
-        PrefixQueryBuilder prefixQueryBuilder = new PrefixQueryBuilder("", "prefix");
-        assertThat(prefixQueryBuilder.validate().validationErrors().size(), is(1));
-
-        prefixQueryBuilder = new PrefixQueryBuilder("field", null);
-        assertThat(prefixQueryBuilder.validate().validationErrors().size(), is(1));
-
-        prefixQueryBuilder = new PrefixQueryBuilder("field", "prefix");
-        assertNull(prefixQueryBuilder.validate());
-
-        prefixQueryBuilder = new PrefixQueryBuilder(null, null);
-        assertThat(prefixQueryBuilder.validate().validationErrors().size(), is(2));
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/index/query/QueryFilterBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/QueryFilterBuilderTest.java
deleted file mode 100644
index 6e3b7d8..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/QueryFilterBuilderTest.java
+++ /dev/null
@@ -1,94 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-import static org.hamcrest.CoreMatchers.nullValue;
-
-@SuppressWarnings("deprecation")
-public class QueryFilterBuilderTest extends BaseQueryTestCase<QueryFilterBuilder> {
-
-    @Override
-    protected QueryFilterBuilder doCreateTestQueryBuilder() {
-        QueryBuilder innerQuery = RandomQueryBuilder.createQuery(random());
-        return new QueryFilterBuilder(innerQuery);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(QueryFilterBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Query innerQuery = queryBuilder.innerQuery().toQuery(context);
-        if (innerQuery == null) {
-            assertThat(query, nullValue());
-        } else {
-            assertThat(query, instanceOf(ConstantScoreQuery.class));
-            ConstantScoreQuery constantScoreQuery = (ConstantScoreQuery) query;
-            assertThat(constantScoreQuery.getQuery(), equalTo(innerQuery));
-        }
-    }
-
-    @Override
-    protected boolean supportsBoostAndQueryName() {
-        return false;
-    }
-
-    /**
-     * test wrapping an inner filter that returns null also returns <tt>null</null> to pass on upwards
-     */
-    @Test
-    public void testInnerQueryReturnsNull() throws IOException {
-        QueryParseContext context = createParseContext();
-
-        // create inner filter
-        String queryString = "{ \"constant_score\" : { \"filter\" : {} }";
-        XContentParser parser = XContentFactory.xContent(queryString).createParser(queryString);
-        context.reset(parser);
-        assertQueryHeader(parser, ConstantScoreQueryBuilder.NAME);
-        QueryBuilder innerQuery = context.queryParser(ConstantScoreQueryBuilder.NAME).fromXContent(context);
-
-        // check that when wrapping this filter, toQuery() returns null
-        QueryFilterBuilder queryFilterQuery = new QueryFilterBuilder(innerQuery);
-        assertNull(queryFilterQuery.toQuery(createShardContext()));
-    }
-
-    @Test
-    public void testValidate() {
-        QueryBuilder innerQuery = null;
-        int totalExpectedErrors = 0;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                innerQuery = RandomQueryBuilder.createInvalidQuery(random());
-            }
-            totalExpectedErrors++;
-        } else {
-            innerQuery = RandomQueryBuilder.createQuery(random());
-        }
-        QueryFilterBuilder fQueryFilter = new QueryFilterBuilder(innerQuery);
-        assertValidate(fQueryFilter, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/RandomQueryBuilder.java b/core/src/test/java/org/elasticsearch/index/query/RandomQueryBuilder.java
deleted file mode 100644
index e86a0ec..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/RandomQueryBuilder.java
+++ /dev/null
@@ -1,91 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import com.carrotsearch.randomizedtesting.generators.RandomInts;
-import com.carrotsearch.randomizedtesting.generators.RandomStrings;
-
-import java.util.Random;
-
-/**
- * Utility class for creating random QueryBuilders.
- * So far only leaf queries like {@link MatchAllQueryBuilder}, {@link TermQueryBuilder} or
- * {@link IdsQueryBuilder} are returned.
- */
-public class RandomQueryBuilder {
-
-    /**
-     * Create a new query of a random type
-     * @param r random seed
-     * @return a random {@link QueryBuilder}
-     */
-    public static QueryBuilder createQuery(Random r) {
-        switch (RandomInts.randomIntBetween(r, 0, 4)) {
-            case 0:
-                return new MatchAllQueryBuilderTest().createTestQueryBuilder();
-            case 1:
-                return new TermQueryBuilderTest().createTestQueryBuilder();
-            case 2:
-                return new IdsQueryBuilderTest().createTestQueryBuilder();
-            case 3:
-                return createMultiTermQuery(r);
-            case 4:
-                return EmptyQueryBuilder.PROTOTYPE;
-            default:
-                throw new UnsupportedOperationException();
-        }
-    }
-
-    /**
-     * Create a new multi term query of a random type
-     * @param r random seed
-     * @return a random {@link MultiTermQueryBuilder}
-     */
-    public static MultiTermQueryBuilder createMultiTermQuery(Random r) {
-        // for now, only use String Rangequeries for MultiTerm test, numeric and date makes little sense
-        // see issue #12123 for discussion
-        // Prefix / Fuzzy / RegEx / Wildcard can go here later once refactored and they have random query generators
-        RangeQueryBuilder query = new RangeQueryBuilder(BaseQueryTestCase.STRING_FIELD_NAME);
-        query.from("a" + RandomStrings.randomAsciiOfLengthBetween(r, 1, 10));
-        query.to("z" + RandomStrings.randomAsciiOfLengthBetween(r, 1, 10));
-        return query;
-    }
-
-    /**
-     * Create a new invalid query of a random type
-     * @param r random seed
-     * @return a random {@link QueryBuilder} that is invalid, meaning that calling validate against it
-     * will return an error. We can rely on the fact that a single error will be returned per query.
-     */
-    public static QueryBuilder createInvalidQuery(Random r) {
-        switch (RandomInts.randomIntBetween(r, 0, 3)) {
-            case 0:
-                return new TermQueryBuilder("", "test");
-            case 1:
-                return new BoostingQueryBuilder(new MatchAllQueryBuilder(), new MatchAllQueryBuilder()).negativeBoost(-1f);
-            case 2:
-                return new CommonTermsQueryBuilder("", "text");
-            case 3:
-                return new SimpleQueryStringBuilder(null);
-            default:
-                throw new UnsupportedOperationException();
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTest.java
deleted file mode 100644
index 00753d8..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTest.java
+++ /dev/null
@@ -1,144 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.NumericRangeQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermRangeQuery;
-import org.joda.time.DateTime;
-import org.joda.time.DateTimeZone;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-
-public class RangeQueryBuilderTest extends BaseQueryTestCase<RangeQueryBuilder> {
-
-    private static final List<String> TIMEZONE_IDS = new ArrayList<>(DateTimeZone.getAvailableIDs());
-
-    @Override
-    protected RangeQueryBuilder doCreateTestQueryBuilder() {
-        RangeQueryBuilder query;
-        // switch between numeric and date ranges
-        switch (randomIntBetween(0, 2)) {
-            case 0:
-                if (randomBoolean()) {
-                    // use mapped integer field for numeric range queries
-                    query = new RangeQueryBuilder(INT_FIELD_NAME);
-                    query.from(randomIntBetween(1, 100));
-                    query.to(randomIntBetween(101, 200));
-                } else {
-                    // use unmapped field for numeric range queries
-                    query = new RangeQueryBuilder(randomAsciiOfLengthBetween(1, 10));
-                    query.from(0.0 - randomDouble());
-                    query.to(randomDouble());
-                }
-                break;
-            case 1:
-                // use mapped date field, using date string representation
-                query = new RangeQueryBuilder(DATE_FIELD_NAME);
-                query.from(new DateTime(System.currentTimeMillis() - randomIntBetween(0, 1000000), DateTimeZone.UTC).toString());
-                query.to(new DateTime(System.currentTimeMillis() + randomIntBetween(0, 1000000), DateTimeZone.UTC).toString());
-                // Create timestamp option only then we have a date mapper,
-                // otherwise we could trigger exception.
-                if (createShardContext().mapperService().smartNameFieldType(DATE_FIELD_NAME) != null) {
-                    if (randomBoolean()) {
-                        query.timeZone(TIMEZONE_IDS.get(randomIntBetween(0, TIMEZONE_IDS.size() - 1)));
-                    }
-                    if (randomBoolean()) {
-                        query.format("yyyy-MM-dd'T'HH:mm:ss.SSSZZ");
-                    }
-                }
-                break;
-            case 2:
-            default:
-                query = new RangeQueryBuilder(STRING_FIELD_NAME);
-                query.from("a" + randomAsciiOfLengthBetween(1, 10));
-                query.to("z" + randomAsciiOfLengthBetween(1, 10));
-                break;
-        }
-        query.includeLower(randomBoolean()).includeUpper(randomBoolean());
-        if (randomBoolean()) {
-            query.from(null);
-        }
-        if (randomBoolean()) {
-            query.to(null);
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(RangeQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (getCurrentTypes().length == 0 || (queryBuilder.fieldName().equals(DATE_FIELD_NAME) == false && queryBuilder.fieldName().equals(INT_FIELD_NAME) == false)) {
-            assertThat(query, instanceOf(TermRangeQuery.class));
-        } else if (queryBuilder.fieldName().equals(DATE_FIELD_NAME)) {
-            //we can't properly test unmapped dates because LateParsingQuery is package private
-        } else if (queryBuilder.fieldName().equals(INT_FIELD_NAME)) {
-            assertThat(query, instanceOf(NumericRangeQuery.class));
-        } else {
-            throw new UnsupportedOperationException();
-        }
-    }
-
-    @Test
-    public void testValidate() {
-        RangeQueryBuilder rangeQueryBuilder = new RangeQueryBuilder("");
-        assertThat(rangeQueryBuilder.validate().validationErrors().size(), is(1));
-
-        rangeQueryBuilder = new RangeQueryBuilder("okay").timeZone("UTC");
-        assertNull(rangeQueryBuilder.validate());
-
-        rangeQueryBuilder.timeZone("blab");
-        assertThat(rangeQueryBuilder.validate().validationErrors().size(), is(1));
-
-        rangeQueryBuilder.timeZone("UTC").format("basicDate");
-        assertNull(rangeQueryBuilder.validate());
-
-        rangeQueryBuilder.timeZone("UTC").format("broken_xx");
-        assertThat(rangeQueryBuilder.validate().validationErrors().size(), is(1));
-
-        rangeQueryBuilder.timeZone("xXx").format("broken_xx");
-        assertThat(rangeQueryBuilder.validate().validationErrors().size(), is(2));
-    }
-
-    /**
-     * Specifying a timezone together with a numeric range query should throw an exception.
-     */
-    @Test(expected=QueryShardException.class)
-    public void testToQueryNonDateWithTimezone() throws QueryShardException, IOException {
-        RangeQueryBuilder query = new RangeQueryBuilder(INT_FIELD_NAME);
-        query.from(1).to(10).timeZone("UTC");
-        query.toQuery(createShardContext());
-    }
-
-    /**
-     * Specifying a timezone together with an unmapped field should throw an exception.
-     */
-    @Test(expected=QueryShardException.class)
-    public void testToQueryUnmappedWithTimezone() throws QueryShardException, IOException {
-        RangeQueryBuilder query = new RangeQueryBuilder("bogus_field");
-        query.from(1).to(10).timeZone("UTC");
-        query.toQuery(createShardContext());
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/RegexpQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/RegexpQueryBuilderTest.java
deleted file mode 100644
index 9328609..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/RegexpQueryBuilderTest.java
+++ /dev/null
@@ -1,78 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.RegexpQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-
-public class RegexpQueryBuilderTest extends BaseQueryTestCase<RegexpQueryBuilder> {
-
-    @Override
-    protected RegexpQueryBuilder doCreateTestQueryBuilder() {
-        // mapped or unmapped fields
-        String fieldName = randomBoolean() ? STRING_FIELD_NAME : randomAsciiOfLengthBetween(1, 10);
-        String value = randomAsciiOfLengthBetween(1, 10);
-        RegexpQueryBuilder query = new RegexpQueryBuilder(fieldName, value);
-
-        if (randomBoolean()) {
-            List<RegexpFlag> flags = new ArrayList<>();
-            int iter = randomInt(5);
-            for (int i = 0; i < iter; i++) {
-                flags.add(randomFrom(RegexpFlag.values()));
-            }
-            query.flags(flags.toArray(new RegexpFlag[flags.size()]));
-        }
-        if (randomBoolean()) {
-            query.maxDeterminizedStates(randomInt(50000));
-        }
-        if (randomBoolean()) {
-            query.rewrite(randomFrom(getRandomRewriteMethod()));
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(RegexpQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(RegexpQuery.class));
-    }
-
-    @Test
-    public void testValidate() {
-        RegexpQueryBuilder regexQueryBuilder = new RegexpQueryBuilder("", "regex");
-        assertThat(regexQueryBuilder.validate().validationErrors().size(), is(1));
-
-        regexQueryBuilder = new RegexpQueryBuilder("field", null);
-        assertThat(regexQueryBuilder.validate().validationErrors().size(), is(1));
-
-        regexQueryBuilder = new RegexpQueryBuilder("field", "regex");
-        assertNull(regexQueryBuilder.validate());
-
-        regexQueryBuilder = new RegexpQueryBuilder(null, null);
-        assertThat(regexQueryBuilder.validate().validationErrors().size(), is(2));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/ScriptQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/ScriptQueryBuilderTest.java
deleted file mode 100644
index dcc74d1..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/ScriptQueryBuilderTest.java
+++ /dev/null
@@ -1,60 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-
-public class ScriptQueryBuilderTest extends BaseQueryTestCase<ScriptQueryBuilder> {
-
-    @Override
-    protected ScriptQueryBuilder doCreateTestQueryBuilder() {
-        String script;
-        Map<String, Object> params = null;
-        if (randomBoolean()) {
-            script = "5 * 2 > param";
-            params = new HashMap<>();
-            params.put("param", 1);
-        } else {
-            script = "5 * 2 > 2";
-        }
-        return new ScriptQueryBuilder(new Script(script, ScriptType.INLINE, "expression", params));
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(ScriptQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(ScriptQueryBuilder.ScriptQuery.class));
-    }
-
-    @Test
-    public void testValidate() {
-        ScriptQueryBuilder scriptQueryBuilder = new ScriptQueryBuilder(null);
-        assertThat(scriptQueryBuilder.validate().validationErrors().size(), is(1));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java b/core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java
index f441aa2..5381c92 100644
--- a/core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java
@@ -54,6 +54,7 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.IndexService;
+import org.elasticsearch.index.engine.Engine;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.core.NumberFieldMapper;
@@ -83,6 +84,7 @@ import static org.hamcrest.Matchers.*;
 public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
 
     private IndexQueryParserService queryParser;
+    private IndexService indexService;
 
     @Before
     public void setup() throws IOException {
@@ -99,6 +101,7 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
         assertNotNull(doc.dynamicMappingsUpdate());
         client().admin().indices().preparePutMapping("test").setType("person").setSource(doc.dynamicMappingsUpdate().toString()).get();
 
+        this.indexService = indexService;
         queryParser = indexService.queryParserService();
     }
 
@@ -967,7 +970,7 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
     @Test
     public void testBoostingQueryBuilder() throws IOException {
         IndexQueryParserService queryParser = queryParser();
-        Query parsedQuery = queryParser.parse(boostingQuery(termQuery("field1", "value1"), termQuery("field1", "value2")).negativeBoost(0.2f)).query();
+        Query parsedQuery = queryParser.parse(boostingQuery().positive(termQuery("field1", "value1")).negative(termQuery("field1", "value2")).negativeBoost(0.2f)).query();
         assertThat(parsedQuery, instanceOf(BoostingQuery.class));
     }
 
@@ -1322,7 +1325,7 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
     @Test
     public void testSpanNotQueryBuilder() throws IOException {
         IndexQueryParserService queryParser = queryParser();
-        Query parsedQuery = queryParser.parse(spanNotQuery(spanTermQuery("age", 34), spanTermQuery("age", 35))).query();
+        Query parsedQuery = queryParser.parse(spanNotQuery().include(spanTermQuery("age", 34)).exclude(spanTermQuery("age", 35))).query();
         assertThat(parsedQuery, instanceOf(SpanNotQuery.class));
         SpanNotQuery spanNotQuery = (SpanNotQuery) parsedQuery;
         // since age is automatically registered in data, we encode it as numeric
@@ -1347,7 +1350,9 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
         IndexQueryParserService queryParser = queryParser();
         Query expectedQuery = new SpanWithinQuery(new SpanTermQuery(new Term("age", longToPrefixCoded(34, 0))),
                                                   new SpanTermQuery(new Term("age", longToPrefixCoded(35, 0))));
-        Query actualQuery = queryParser.parse(spanWithinQuery(spanTermQuery("age", 34), spanTermQuery("age", 35)))
+        Query actualQuery = queryParser.parse(spanWithinQuery()
+                                              .big(spanTermQuery("age", 34))
+                                              .little(spanTermQuery("age", 35)))
                                               .query();
         assertEquals(expectedQuery, actualQuery);
     }
@@ -1367,7 +1372,10 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
         IndexQueryParserService queryParser = queryParser();
         Query expectedQuery = new SpanContainingQuery(new SpanTermQuery(new Term("age", longToPrefixCoded(34, 0))),
                                                       new SpanTermQuery(new Term("age", longToPrefixCoded(35, 0))));
-        Query actualQuery = queryParser.parse(spanContainingQuery(spanTermQuery("age", 34), spanTermQuery("age", 35))).query();
+        Query actualQuery = queryParser.parse(spanContainingQuery()
+                                              .big(spanTermQuery("age", 34))
+                                              .little(spanTermQuery("age", 35)))
+                                              .query();
         assertEquals(expectedQuery, actualQuery);
     }
 
@@ -1407,7 +1415,7 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
     @Test
     public void testSpanNearQueryBuilder() throws IOException {
         IndexQueryParserService queryParser = queryParser();
-        Query parsedQuery = queryParser.parse(spanNearQuery(12).clause(spanTermQuery("age", 34)).clause(spanTermQuery("age", 35)).clause(spanTermQuery("age", 36)).inOrder(false).collectPayloads(false)).query();
+        Query parsedQuery = queryParser.parse(spanNearQuery().clause(spanTermQuery("age", 34)).clause(spanTermQuery("age", 35)).clause(spanTermQuery("age", 36)).slop(12).inOrder(false).collectPayloads(false)).query();
         assertThat(parsedQuery, instanceOf(SpanNearQuery.class));
         SpanNearQuery spanNearQuery = (SpanNearQuery) parsedQuery;
         assertThat(spanNearQuery.getClauses().length, equalTo(3));
@@ -2264,6 +2272,23 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
         assertThat(parsedQuery, instanceOf(BooleanQuery.class));
     }
 
+    public void testCrossFieldMultiMatchQuery() throws IOException {
+        IndexQueryParserService queryParser = queryParser();
+        Query parsedQuery = queryParser.parse(multiMatchQuery("banon", "name.first^2", "name.last^3", "foobar").type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)).query();
+        try (Engine.Searcher searcher = indexService.shardSafe(0).acquireSearcher("test")) {
+            Query rewrittenQuery = searcher.searcher().rewrite(parsedQuery);
+
+            BooleanQuery expected = new BooleanQuery();
+            expected.add(new TermQuery(new Term("foobar", "banon")), Occur.SHOULD);
+            TermQuery tq1 = new TermQuery(new Term("name.first", "banon"));
+            tq1.setBoost(2);
+            TermQuery tq2 = new TermQuery(new Term("name.last", "banon"));
+            tq2.setBoost(3);
+            expected.add(new DisjunctionMaxQuery(Arrays.<Query>asList(tq1, tq2), 0f), Occur.SHOULD);
+            assertEquals(expected, rewrittenQuery);
+        }
+    }
+
     @Test
     public void testSimpleQueryString() throws Exception {
         IndexQueryParserService queryParser = queryParser();
@@ -2402,8 +2427,8 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
         assertThat(((ConstantScoreQuery) parsedQuery).getQuery().toString(), equalTo("ToParentBlockJoinQuery (+*:* #random_access(QueryWrapperFilter(_type:__nested)))"));
         SearchContext.removeCurrent();
     }
-
-    /**
+    
+    /** 
      * helper to extract term from TermQuery. */
     private Term getTerm(Query query) {
         while (query instanceof QueryWrapperFilter) {
diff --git a/core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTest.java
deleted file mode 100644
index 7a17ec7..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTest.java
+++ /dev/null
@@ -1,227 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.*;
-
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.notNullValue;
-
-public class SimpleQueryStringBuilderTest extends BaseQueryTestCase<SimpleQueryStringBuilder> {
-
-    private static final String[] MINIMUM_SHOULD_MATCH = new String[] { "1", "-1", "75%", "-25%", "2<75%", "2<-25%" };
-
-    @Override
-    protected SimpleQueryStringBuilder doCreateTestQueryBuilder() {
-        SimpleQueryStringBuilder result = new SimpleQueryStringBuilder(randomAsciiOfLengthBetween(1, 10));
-        if (randomBoolean()) {
-            result.analyzeWildcard(randomBoolean());
-        }
-        if (randomBoolean()) {
-            result.lenient(randomBoolean());
-        }
-        if (randomBoolean()) {
-            result.lowercaseExpandedTerms(randomBoolean());
-        }
-        if (randomBoolean()) {
-            result.locale(randomLocale(getRandom()));
-        }
-        if (randomBoolean()) {
-            result.minimumShouldMatch(randomFrom(MINIMUM_SHOULD_MATCH));
-        }
-        if (randomBoolean()) {
-            result.analyzer("simple");
-        }
-        if (randomBoolean()) {
-            result.defaultOperator(randomFrom(Operator.AND, Operator.OR));
-        }
-        if (randomBoolean()) {
-            Set<SimpleQueryStringFlag> flagSet = new HashSet<>();
-            int size = randomIntBetween(0, SimpleQueryStringFlag.values().length);
-            for (int i = 0; i < size; i++) {
-                flagSet.add(randomFrom(SimpleQueryStringFlag.values()));
-            }
-            if (flagSet.size() > 0) {
-                result.flags(flagSet.toArray(new SimpleQueryStringFlag[flagSet.size()]));
-            }
-        }
-
-        int fieldCount = randomIntBetween(0, 10);
-        Map<String, Float> fields = new TreeMap<>();
-        for (int i = 0; i < fieldCount; i++) {
-            if (randomBoolean()) {
-                fields.put(randomAsciiOfLengthBetween(1, 10), AbstractQueryBuilder.DEFAULT_BOOST);
-            } else {
-                fields.put(randomAsciiOfLengthBetween(1, 10), 2.0f / randomIntBetween(1, 20));
-            }
-        }
-        result.fields(fields);
-
-        return result;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SimpleQueryStringBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, notNullValue());
-    }
-
-    @Test
-    public void testDefaults() {
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder("The quick brown fox.");
-
-        assertEquals("Wrong default default boost.", AbstractQueryBuilder.DEFAULT_BOOST, qb.boost(), 0.001);
-        assertEquals("Wrong default default boost field.", AbstractQueryBuilder.DEFAULT_BOOST, SimpleQueryStringBuilder.DEFAULT_BOOST, 0.001);
-
-        assertEquals("Wrong default flags.", SimpleQueryStringFlag.ALL.value, qb.flags());
-        assertEquals("Wrong default flags field.", SimpleQueryStringFlag.ALL.value(), SimpleQueryStringBuilder.DEFAULT_FLAGS);
-
-        assertEquals("Wrong default default operator.", Operator.OR, qb.defaultOperator());
-        assertEquals("Wrong default default operator field.", Operator.OR, SimpleQueryStringBuilder.DEFAULT_OPERATOR);
-
-        assertEquals("Wrong default default locale.", Locale.ROOT, qb.locale());
-        assertEquals("Wrong default default locale field.", Locale.ROOT, SimpleQueryStringBuilder.DEFAULT_LOCALE);
-
-        assertEquals("Wrong default default analyze_wildcard.", false, qb.analyzeWildcard());
-        assertEquals("Wrong default default analyze_wildcard field.", false, SimpleQueryStringBuilder.DEFAULT_ANALYZE_WILDCARD);
-
-        assertEquals("Wrong default default lowercase_expanded_terms.", true, qb.lowercaseExpandedTerms());
-        assertEquals("Wrong default default lowercase_expanded_terms field.", true, SimpleQueryStringBuilder.DEFAULT_LOWERCASE_EXPANDED_TERMS);
-
-        assertEquals("Wrong default default lenient.", false, qb.lenient());
-        assertEquals("Wrong default default lenient field.", false, SimpleQueryStringBuilder.DEFAULT_LENIENT);
-
-        assertEquals("Wrong default default locale.", Locale.ROOT, qb.locale());
-        assertEquals("Wrong default default locale field.", Locale.ROOT, SimpleQueryStringBuilder.DEFAULT_LOCALE);
-    }
-
-    @Test
-    public void testDefaultNullLocale() {
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder("The quick brown fox.");
-        qb.locale(null);
-        assertEquals("Setting locale to null should result in returning to default value.",
-                SimpleQueryStringBuilder.DEFAULT_LOCALE, qb.locale());
-    }
-
-    @Test
-    public void testDefaultNullComplainFlags() {
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder("The quick brown fox.");
-        qb.flags((SimpleQueryStringFlag[]) null);
-        assertEquals("Setting flags to null should result in returning to default value.",
-                SimpleQueryStringBuilder.DEFAULT_FLAGS, qb.flags());
-    }
-
-    @Test
-    public void testDefaultEmptyComplainFlags() {
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder("The quick brown fox.");
-        qb.flags(new SimpleQueryStringFlag[]{});
-        assertEquals("Setting flags to empty should result in returning to default value.",
-                SimpleQueryStringBuilder.DEFAULT_FLAGS, qb.flags());
-    }
-
-    @Test
-    public void testDefaultNullComplainOp() {
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder("The quick brown fox.");
-        qb.defaultOperator(null);
-        assertEquals("Setting operator to null should result in returning to default value.",
-                SimpleQueryStringBuilder.DEFAULT_OPERATOR, qb.defaultOperator());
-    }
-
-    // Check operator handling, and default field handling.
-    @Test
-    public void testDefaultOperatorHandling() throws IOException {
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder("The quick brown fox.").field(STRING_FIELD_NAME);
-        QueryShardContext shardContext = createShardContext();
-        shardContext.setAllowUnmappedFields(true); // to avoid occasional cases in setup where we didn't add types but strict field resolution
-        BooleanQuery boolQuery = (BooleanQuery) qb.toQuery(shardContext);
-        assertThat(shouldClauses(boolQuery), is(4));
-
-        qb.defaultOperator(Operator.AND);
-        boolQuery = (BooleanQuery) qb.toQuery(shardContext);
-        assertThat(shouldClauses(boolQuery), is(0));
-
-        qb.defaultOperator(Operator.OR);
-        boolQuery = (BooleanQuery) qb.toQuery(shardContext);
-        assertThat(shouldClauses(boolQuery), is(4));
-    }
-
-    @Test
-    public void testValidation() {
-        SimpleQueryStringBuilder qb = createTestQueryBuilder();
-        assertNull(qb.validate());
-    }
-
-    @Test
-    public void testNullQueryTextGeneratesException() {
-        SimpleQueryStringBuilder builder = new SimpleQueryStringBuilder(null);
-        QueryValidationException exception = builder.validate();
-        assertThat(exception, notNullValue());
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testFieldCannotBeNull() {
-        SimpleQueryStringBuilder qb = createTestQueryBuilder();
-        qb.field(null);
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testFieldCannotBeNullAndWeighted() {
-        SimpleQueryStringBuilder qb = createTestQueryBuilder();
-        qb.field(null, AbstractQueryBuilder.DEFAULT_BOOST);
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testFieldCannotBeEmpty() {
-        SimpleQueryStringBuilder qb = createTestQueryBuilder();
-        qb.field("");
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testFieldCannotBeEmptyAndWeighted() {
-        SimpleQueryStringBuilder qb = createTestQueryBuilder();
-        qb.field("", AbstractQueryBuilder.DEFAULT_BOOST);
-    }
-
-    /**
-     * The following should fail fast - never silently set the map containing
-     * fields and weights to null but refuse to accept null instead.
-     * */
-    @Test(expected = NullPointerException.class)
-    public void testFieldsCannotBeSetToNull() {
-        SimpleQueryStringBuilder qb = createTestQueryBuilder();
-        qb.fields(null);
-    }
-
-    private int shouldClauses(BooleanQuery query) {
-        int result = 0;
-        for (BooleanClause c : query.clauses()) {
-            if (c.getOccur() == BooleanClause.Occur.SHOULD) {
-                result++;
-            }
-        }
-        return result;
-    }
-}
-
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanContainingQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SpanContainingQueryBuilderTest.java
deleted file mode 100644
index 7429023..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanContainingQueryBuilderTest.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanContainingQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SpanContainingQueryBuilderTest extends BaseQueryTestCase<SpanContainingQueryBuilder> {
-
-    @Override
-    protected SpanContainingQueryBuilder doCreateTestQueryBuilder() {
-        SpanTermQueryBuilder[] spanTermQueries = new SpanTermQueryBuilderTest().createSpanTermQueryBuilders(2);
-        return new SpanContainingQueryBuilder(spanTermQueries[0], spanTermQueries[1]);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanContainingQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanContainingQuery.class));
-    }
-
-    @Test
-    public void testValidate() {
-        int totalExpectedErrors = 0;
-        SpanQueryBuilder bigSpanQueryBuilder;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                bigSpanQueryBuilder = new SpanTermQueryBuilder("", "test");
-            } else {
-                bigSpanQueryBuilder = null;
-            }
-            totalExpectedErrors++;
-        } else {
-            bigSpanQueryBuilder = new SpanTermQueryBuilder("name", "value");
-        }
-        SpanQueryBuilder littleSpanQueryBuilder;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                littleSpanQueryBuilder = new SpanTermQueryBuilder("", "test");
-            } else {
-                littleSpanQueryBuilder = null;
-            }
-            totalExpectedErrors++;
-        } else {
-            littleSpanQueryBuilder = new SpanTermQueryBuilder("name", "value");
-        }
-        SpanContainingQueryBuilder queryBuilder = new SpanContainingQueryBuilder(bigSpanQueryBuilder, littleSpanQueryBuilder);
-        assertValidate(queryBuilder, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanFirstQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SpanFirstQueryBuilderTest.java
deleted file mode 100644
index d99010f..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanFirstQueryBuilderTest.java
+++ /dev/null
@@ -1,112 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanFirstQuery;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.elasticsearch.index.query.QueryBuilders.spanTermQuery;
-import static org.hamcrest.CoreMatchers.*;
-
-public class SpanFirstQueryBuilderTest extends BaseQueryTestCase<SpanFirstQueryBuilder> {
-
-    @Override
-    protected SpanFirstQueryBuilder doCreateTestQueryBuilder() {
-        SpanTermQueryBuilder[] spanTermQueries = new SpanTermQueryBuilderTest().createSpanTermQueryBuilders(1);
-        return new SpanFirstQueryBuilder(spanTermQueries[0], randomIntBetween(0, 1000));
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanFirstQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanFirstQuery.class));
-    }
-
-    @Test
-    public void testValidate() {
-        int totalExpectedErrors = 0;
-        SpanQueryBuilder innerSpanQueryBuilder;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                innerSpanQueryBuilder = new SpanTermQueryBuilder("", "test");
-            } else {
-                innerSpanQueryBuilder = null;
-            }
-            totalExpectedErrors++;
-        } else {
-            innerSpanQueryBuilder = new SpanTermQueryBuilder("name", "value");
-        }
-        int end = randomIntBetween(0, 10);
-        if (randomBoolean()) {
-            end = randomIntBetween(-10, -1);
-            totalExpectedErrors++;
-        }
-        SpanFirstQueryBuilder queryBuilder = new SpanFirstQueryBuilder(innerSpanQueryBuilder, end);
-        assertValidate(queryBuilder, totalExpectedErrors);
-    }
-
-    /**
-     * test exception on missing `end` and `match` parameter in parser
-     */
-    @Test
-    public void testParseEnd() throws IOException {
-        XContentBuilder builder = XContentFactory.jsonBuilder();
-        builder.startObject();
-        builder.startObject(SpanFirstQueryBuilder.NAME);
-        builder.field("match");
-        spanTermQuery("description", "jumped").toXContent(builder, null);
-        builder.endObject();
-        builder.endObject();
-
-        QueryParseContext context = createParseContext();
-        XContentParser parser = XContentFactory.xContent(builder.string()).createParser(builder.string());
-        context.reset(parser);
-        assertQueryHeader(parser, SpanFirstQueryBuilder.NAME);
-        try {
-            new SpanFirstQueryParser().fromXContent(context);
-            fail("missing [end] parameter should raise exception");
-        } catch (QueryParsingException e) {
-            assertTrue(e.getMessage().contains("spanFirst must have [end] set"));
-        }
-
-        builder = XContentFactory.jsonBuilder();
-        builder.startObject();
-        builder.startObject(SpanFirstQueryBuilder.NAME);
-        builder.field("end", 10);
-        builder.endObject();
-        builder.endObject();
-
-        context = createParseContext();
-        parser = XContentFactory.xContent(builder.string()).createParser(builder.string());
-        context.reset(parser);
-        assertQueryHeader(parser, SpanFirstQueryBuilder.NAME);
-        try {
-            new SpanFirstQueryParser().fromXContent(context);
-            fail("missing [match] parameter should raise exception");
-        } catch (QueryParsingException e) {
-            assertTrue(e.getMessage().contains("spanFirst must have [match] span query clause"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilderTest.java
deleted file mode 100644
index 1c75333..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilderTest.java
+++ /dev/null
@@ -1,87 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanMultiTermQueryWrapper;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SpanMultiTermQueryBuilderTest extends BaseQueryTestCase<SpanMultiTermQueryBuilder> {
-
-    @Override
-    protected SpanMultiTermQueryBuilder doCreateTestQueryBuilder() {
-        MultiTermQueryBuilder multiTermQueryBuilder = RandomQueryBuilder.createMultiTermQuery(random());
-        return new SpanMultiTermQueryBuilder(multiTermQueryBuilder);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanMultiTermQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanMultiTermQueryWrapper.class));
-        SpanMultiTermQueryWrapper spanMultiTermQueryWrapper = (SpanMultiTermQueryWrapper) query;
-        Query multiTermQuery = queryBuilder.multiTermQueryBuilder().toQuery(context);
-        assertThat(multiTermQuery, instanceOf(MultiTermQuery.class));
-        assertThat(spanMultiTermQueryWrapper.getWrappedQuery(), equalTo(new SpanMultiTermQueryWrapper<>((MultiTermQuery)multiTermQuery).getWrappedQuery()));
-    }
-
-    @Test
-    public void testValidate() {
-        int totalExpectedErrors = 0;
-        MultiTermQueryBuilder multiTermQueryBuilder;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                multiTermQueryBuilder = new RangeQueryBuilder("");
-            } else {
-                multiTermQueryBuilder = null;
-            }
-            totalExpectedErrors++;
-        } else {
-            multiTermQueryBuilder = new RangeQueryBuilder("field");
-        }
-        SpanMultiTermQueryBuilder queryBuilder = new SpanMultiTermQueryBuilder(multiTermQueryBuilder);
-        assertValidate(queryBuilder, totalExpectedErrors);
-    }
-
-    /**
-     * test checks that we throw an {@link UnsupportedOperationException} if the query wrapped
-     * by {@link SpanMultiTermQueryBuilder} does not generate a lucene {@link MultiTermQuery}.
-     * This is currently the case for {@link RangeQueryBuilder} when the target field is mapped
-     * to a date.
-     */
-    @Test
-    public void testUnsupportedInnerQueryType() throws IOException {
-        QueryShardContext context = createShardContext();
-        // test makes only sense if we have at least one type registered with date field mapping
-        if (getCurrentTypes().length > 0 && context.fieldMapper(DATE_FIELD_NAME) != null) {
-            try {
-                RangeQueryBuilder query = new RangeQueryBuilder(DATE_FIELD_NAME);
-                new SpanMultiTermQueryBuilder(query).toQuery(createShardContext());
-                fail("Exception expected, range query on date fields should not generate a lucene " + MultiTermQuery.class.getName());
-            } catch (UnsupportedOperationException e) {
-                assert(e.getMessage().contains("unsupported inner query, should be " + MultiTermQuery.class.getName()));
-            }
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanNearQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SpanNearQueryBuilderTest.java
deleted file mode 100644
index d2eb1a0..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanNearQueryBuilderTest.java
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanNearQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.Iterator;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SpanNearQueryBuilderTest extends BaseQueryTestCase<SpanNearQueryBuilder> {
-
-    @Override
-    protected SpanNearQueryBuilder doCreateTestQueryBuilder() {
-        SpanNearQueryBuilder queryBuilder = new SpanNearQueryBuilder(randomIntBetween(-10, 10));
-        SpanTermQueryBuilder[] spanTermQueries = new SpanTermQueryBuilderTest().createSpanTermQueryBuilders(randomIntBetween(1, 6));
-        for (SpanTermQueryBuilder clause : spanTermQueries) {
-            queryBuilder.clause(clause);
-        }
-        queryBuilder.inOrder(randomBoolean());
-        queryBuilder.collectPayloads(randomBoolean());
-        return queryBuilder;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanNearQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanNearQuery.class));
-        SpanNearQuery spanNearQuery = (SpanNearQuery) query;
-        assertThat(spanNearQuery.getSlop(), equalTo(queryBuilder.slop()));
-        assertThat(spanNearQuery.isInOrder(), equalTo(queryBuilder.inOrder()));
-        assertThat(spanNearQuery.getClauses().length, equalTo(queryBuilder.clauses().size()));
-        Iterator<SpanQueryBuilder> spanQueryBuilderIterator = queryBuilder.clauses().iterator();
-        for (SpanQuery spanQuery : spanNearQuery.getClauses()) {
-            assertThat(spanQuery, equalTo(spanQueryBuilderIterator.next().toQuery(context)));
-        }
-    }
-
-    @Test
-    public void testValidate() {
-        SpanNearQueryBuilder queryBuilder = new SpanNearQueryBuilder(1);
-        assertValidate(queryBuilder, 1); // empty clause list
-
-        int totalExpectedErrors = 0;
-        int clauses = randomIntBetween(1, 10);
-        for (int i = 0; i < clauses; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    queryBuilder.clause(new SpanTermQueryBuilder("", "test"));
-                } else {
-                    queryBuilder.clause(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                queryBuilder.clause(new SpanTermQueryBuilder("name", "value"));
-            }
-        }
-        assertValidate(queryBuilder, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanNotQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SpanNotQueryBuilderTest.java
deleted file mode 100644
index 33c7149..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanNotQueryBuilderTest.java
+++ /dev/null
@@ -1,218 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanNotQuery;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.elasticsearch.index.query.QueryBuilders.spanNearQuery;
-import static org.elasticsearch.index.query.QueryBuilders.spanTermQuery;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-
-public class SpanNotQueryBuilderTest extends BaseQueryTestCase<SpanNotQueryBuilder> {
-
-    @Override
-    protected SpanNotQueryBuilder doCreateTestQueryBuilder() {
-        SpanTermQueryBuilder[] spanTermQueries = new SpanTermQueryBuilderTest().createSpanTermQueryBuilders(2);
-        SpanNotQueryBuilder queryBuilder = new SpanNotQueryBuilder(spanTermQueries[0], spanTermQueries[1]);
-        if (randomBoolean()) {
-            // also test negative values, they should implicitly be changed to 0
-            queryBuilder.dist(randomIntBetween(-2, 10));
-        } else {
-            if (randomBoolean()) {
-                queryBuilder.pre(randomIntBetween(-2, 10));
-            }
-            if (randomBoolean()) {
-                queryBuilder.post(randomIntBetween(-2, 10));
-            }
-        }
-        return queryBuilder;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanNotQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanNotQuery.class));
-        SpanNotQuery spanNotQuery = (SpanNotQuery) query;
-        assertThat(spanNotQuery.getExclude(), equalTo(queryBuilder.exclude().toQuery(context)));
-        assertThat(spanNotQuery.getInclude(), equalTo(queryBuilder.include().toQuery(context)));
-    }
-
-    @Test
-    public void testValidate() {
-        int totalExpectedErrors = 0;
-        SpanQueryBuilder include;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                include = new SpanTermQueryBuilder("", "test");
-            } else {
-                include = null;
-            }
-            totalExpectedErrors++;
-        } else {
-            include = new SpanTermQueryBuilder("name", "value");
-        }
-        SpanQueryBuilder exclude;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                exclude = new SpanTermQueryBuilder("", "test");
-            } else {
-                exclude = null;
-            }
-            totalExpectedErrors++;
-        } else {
-            exclude = new SpanTermQueryBuilder("name", "value");
-        }
-        SpanNotQueryBuilder queryBuilder = new SpanNotQueryBuilder(include, exclude);
-        assertValidate(queryBuilder, totalExpectedErrors);
-    }
-
-    @Test
-    public void testDist() {
-        SpanNotQueryBuilder builder = new SpanNotQueryBuilder(new SpanTermQueryBuilder("name1", "value1"), new SpanTermQueryBuilder("name2", "value2"));
-        assertThat(builder.pre(), equalTo(0));
-        assertThat(builder.post(), equalTo(0));
-        builder.dist(-4);
-        assertThat(builder.pre(), equalTo(0));
-        assertThat(builder.post(), equalTo(0));
-        builder.dist(4);
-        assertThat(builder.pre(), equalTo(4));
-        assertThat(builder.post(), equalTo(4));
-    }
-
-    @Test
-    public void testPrePost() {
-        SpanNotQueryBuilder builder = new SpanNotQueryBuilder(new SpanTermQueryBuilder("name1", "value1"), new SpanTermQueryBuilder("name2", "value2"));
-        assertThat(builder.pre(), equalTo(0));
-        assertThat(builder.post(), equalTo(0));
-        builder.pre(-4).post(-4);
-        assertThat(builder.pre(), equalTo(0));
-        assertThat(builder.post(), equalTo(0));
-        builder.pre(1).post(2);
-        assertThat(builder.pre(), equalTo(1));
-        assertThat(builder.post(), equalTo(2));
-    }
-
-    /**
-     * test correct parsing of `dist` parameter, this should create builder with pre/post set to same value
-     */
-    @Test
-    public void testParseDist() throws IOException {
-        XContentBuilder builder = XContentFactory.jsonBuilder();
-        builder.startObject();
-        builder.startObject(SpanNotQueryBuilder.NAME);
-        builder.field("exclude");
-        spanTermQuery("description", "jumped").toXContent(builder, null);
-        builder.field("include");
-        spanNearQuery(1).clause(QueryBuilders.spanTermQuery("description", "quick"))
-                .clause(QueryBuilders.spanTermQuery("description", "fox")).toXContent(builder, null);
-        builder.field("dist", 3);
-        builder.endObject();
-        builder.endObject();
-
-        QueryParseContext context = createParseContext();
-        XContentParser parser = XContentFactory.xContent(builder.string()).createParser(builder.string());
-        context.reset(parser);
-        assertQueryHeader(parser, SpanNotQueryBuilder.NAME);
-        SpanNotQueryBuilder query = (SpanNotQueryBuilder) new SpanNotQueryParser().fromXContent(context);
-        assertThat(query.pre(), equalTo(3));
-        assertThat(query.post(), equalTo(3));
-        assertNotNull(query.include());
-        assertNotNull(query.exclude());
-    }
-
-    /**
-     * test exceptions for three types of broken json, missing include / exclude and both dist and pre/post specified
-     */
-    @Test
-    public void testParserExceptions() throws IOException {
-        try {
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            builder.startObject(SpanNotQueryBuilder.NAME);
-            builder.field("exclude");
-            spanTermQuery("description", "jumped").toXContent(builder, null);
-            builder.field("dist", 2);
-            builder.endObject();
-            builder.endObject();
-
-            QueryParseContext context = createParseContext();
-            XContentParser parser = XContentFactory.xContent(builder.string()).createParser(builder.string());
-            context.reset(parser);
-            assertQueryHeader(parser, SpanNotQueryBuilder.NAME);
-            new SpanNotQueryParser().fromXContent(context);
-            fail("QueryParsingException should have been caught");
-        } catch (QueryParsingException e) {
-            assertThat("QueryParsingException should have been caught", e.getDetailedMessage(), containsString("spanNot must have [include]"));
-        }
-
-        try {
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            builder.startObject(SpanNotQueryBuilder.NAME);
-            builder.field("include");
-            spanNearQuery(1).clause(QueryBuilders.spanTermQuery("description", "quick"))
-                .clause(QueryBuilders.spanTermQuery("description", "fox")).toXContent(builder, null);
-            builder.field("dist", 2);
-            builder.endObject();
-            builder.endObject();
-
-            QueryParseContext context = createParseContext();
-            XContentParser parser = XContentFactory.xContent(builder.string()).createParser(builder.string());
-            context.reset(parser);
-            assertQueryHeader(parser, SpanNotQueryBuilder.NAME);
-            new SpanNotQueryParser().fromXContent(context);
-            fail("QueryParsingException should have been caught");
-        } catch (QueryParsingException e) {
-            assertThat("QueryParsingException should have been caught", e.getDetailedMessage(), containsString("spanNot must have [exclude]"));
-        }
-
-        try {
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            builder.startObject(SpanNotQueryBuilder.NAME);
-            builder.field("include");
-            spanNearQuery(1).clause(QueryBuilders.spanTermQuery("description", "quick"))
-                .clause(QueryBuilders.spanTermQuery("description", "fox")).toXContent(builder, null);
-            builder.field("exclude");
-            spanTermQuery("description", "jumped").toXContent(builder, null);
-            builder.field("dist", 2);
-            builder.field("pre", 2);
-            builder.endObject();
-            builder.endObject();
-
-            QueryParseContext context = createParseContext();
-            XContentParser parser = XContentFactory.xContent(builder.string()).createParser(builder.string());
-            context.reset(parser);
-            assertQueryHeader(parser, SpanNotQueryBuilder.NAME);
-            new SpanNotQueryParser().fromXContent(context);
-            fail("QueryParsingException should have been caught");
-        } catch (QueryParsingException e) {
-            assertThat("QueryParsingException should have been caught", e.getDetailedMessage(), containsString("spanNot can either use [dist] or [pre] & [post] (or none)"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanOrQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SpanOrQueryBuilderTest.java
deleted file mode 100644
index 051e6fd..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanOrQueryBuilderTest.java
+++ /dev/null
@@ -1,77 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanOrQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.Iterator;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SpanOrQueryBuilderTest extends BaseQueryTestCase<SpanOrQueryBuilder> {
-
-    @Override
-    protected SpanOrQueryBuilder doCreateTestQueryBuilder() {
-        SpanOrQueryBuilder queryBuilder = new SpanOrQueryBuilder();
-        SpanTermQueryBuilder[] spanTermQueries = new SpanTermQueryBuilderTest().createSpanTermQueryBuilders(randomIntBetween(1, 6));
-        for (SpanTermQueryBuilder clause : spanTermQueries) {
-            queryBuilder.clause(clause);
-        }
-        return queryBuilder;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanOrQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanOrQuery.class));
-        SpanOrQuery spanOrQuery = (SpanOrQuery) query;
-        assertThat(spanOrQuery.getClauses().length, equalTo(queryBuilder.clauses().size()));
-        Iterator<SpanQueryBuilder> spanQueryBuilderIterator = queryBuilder.clauses().iterator();
-        for (SpanQuery spanQuery : spanOrQuery.getClauses()) {
-            assertThat(spanQuery, equalTo(spanQueryBuilderIterator.next().toQuery(context)));
-        }
-    }
-
-    @Test
-    public void testValidate() {
-        SpanOrQueryBuilder queryBuilder = new SpanOrQueryBuilder();
-        assertValidate(queryBuilder, 1); // empty clause list
-
-        int totalExpectedErrors = 0;
-        int clauses = randomIntBetween(1, 10);
-        for (int i = 0; i < clauses; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    queryBuilder.clause(new SpanTermQueryBuilder("", "test"));
-                } else {
-                    queryBuilder.clause(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                queryBuilder.clause(new SpanTermQueryBuilder("name", "value"));
-            }
-        }
-        assertValidate(queryBuilder, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanTermQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SpanTermQueryBuilderTest.java
deleted file mode 100644
index d8d5ef6..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanTermQueryBuilderTest.java
+++ /dev/null
@@ -1,75 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanTermQuery;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.lucene.BytesRefs;
-import org.elasticsearch.index.mapper.MappedFieldType;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SpanTermQueryBuilderTest extends BaseTermQueryTestCase<SpanTermQueryBuilder> {
-
-    @Override
-    protected SpanTermQueryBuilder createQueryBuilder(String fieldName, Object value) {
-        return new SpanTermQueryBuilder(fieldName, value);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanTermQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanTermQuery.class));
-        SpanTermQuery spanTermQuery = (SpanTermQuery) query;
-        assertThat(spanTermQuery.getTerm().field(), equalTo(queryBuilder.fieldName()));
-        MappedFieldType mapper = context.fieldMapper(queryBuilder.fieldName());
-        if (mapper != null) {
-            BytesRef bytesRef = mapper.indexedValueForSearch(queryBuilder.value());
-            assertThat(spanTermQuery.getTerm().bytes(), equalTo(bytesRef));
-        } else {
-            assertThat(spanTermQuery.getTerm().bytes(), equalTo(BytesRefs.toBytesRef(queryBuilder.value())));
-        }
-    }
-
-    /**
-     * @param amount the number of clauses that will be returned
-     * @return an array of random {@link SpanTermQueryBuilder} with same field name
-     */
-    public SpanTermQueryBuilder[] createSpanTermQueryBuilders(int amount) {
-        SpanTermQueryBuilder[] clauses = new SpanTermQueryBuilder[amount];
-        SpanTermQueryBuilder first = createTestQueryBuilder();
-        clauses[0] = first;
-        for (int i = 1; i < amount; i++) {
-            // we need same field name in all clauses, so we only randomize value
-            SpanTermQueryBuilder spanTermQuery = new SpanTermQueryBuilder(first.fieldName(), randomValueForField(first.fieldName()));
-            if (randomBoolean()) {
-                spanTermQuery.boost(2.0f / randomIntBetween(1, 20));
-            }
-            if (randomBoolean()) {
-                spanTermQuery.queryName(randomAsciiOfLengthBetween(1, 10));
-            }
-            clauses[i] = spanTermQuery;
-        }
-        return clauses;
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanWithinQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SpanWithinQueryBuilderTest.java
deleted file mode 100644
index ffc518d..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanWithinQueryBuilderTest.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanWithinQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SpanWithinQueryBuilderTest extends BaseQueryTestCase<SpanWithinQueryBuilder> {
-
-    @Override
-    protected SpanWithinQueryBuilder doCreateTestQueryBuilder() {
-        SpanTermQueryBuilder[] spanTermQueries = new SpanTermQueryBuilderTest().createSpanTermQueryBuilders(2);
-        return new SpanWithinQueryBuilder(spanTermQueries[0], spanTermQueries[1]);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanWithinQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanWithinQuery.class));
-    }
-
-    @Test
-    public void testValidate() {
-        int totalExpectedErrors = 0;
-        SpanQueryBuilder bigSpanQueryBuilder;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                bigSpanQueryBuilder = new SpanTermQueryBuilder("", "test");
-            } else {
-                bigSpanQueryBuilder = null;
-            }
-            totalExpectedErrors++;
-        } else {
-            bigSpanQueryBuilder = new SpanTermQueryBuilder("name", "value");
-        }
-        SpanQueryBuilder littleSpanQueryBuilder;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                littleSpanQueryBuilder = new SpanTermQueryBuilder("", "test");
-            } else {
-                littleSpanQueryBuilder = null;
-            }
-            totalExpectedErrors++;
-        } else {
-            littleSpanQueryBuilder = new SpanTermQueryBuilder("name", "value");
-        }
-        SpanWithinQueryBuilder queryBuilder = new SpanWithinQueryBuilder(bigSpanQueryBuilder, littleSpanQueryBuilder);
-        assertValidate(queryBuilder, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTest.java b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTest.java
index f687d88..ccb9cd1 100644
--- a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTest.java
+++ b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTest.java
@@ -60,7 +60,7 @@ import java.io.IOException;
 public class TemplateQueryParserTest extends ESTestCase {
 
     private Injector injector;
-    private QueryShardContext context;
+    private QueryParseContext context;
 
     @Before
     public void setup() throws IOException {
@@ -94,7 +94,7 @@ public class TemplateQueryParserTest extends ESTestCase {
         ).createInjector();
 
         IndexQueryParserService queryParserService = injector.getInstance(IndexQueryParserService.class);
-        context = new QueryShardContext(index, queryParserService);
+        context = new QueryParseContext(index, queryParserService);
     }
 
     @Override
diff --git a/core/src/test/java/org/elasticsearch/index/query/TermQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/TermQueryBuilderTest.java
deleted file mode 100644
index f84d1c0..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/TermQueryBuilderTest.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.lucene.BytesRefs;
-import org.elasticsearch.index.mapper.MappedFieldType;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class TermQueryBuilderTest extends BaseTermQueryTestCase<TermQueryBuilder> {
-
-    /**
-     * @return a TermQuery with random field name and value, optional random boost and queryname
-     */
-    @Override
-    protected TermQueryBuilder createQueryBuilder(String fieldName, Object value) {
-        return new TermQueryBuilder(fieldName, value);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(TermQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(TermQuery.class));
-        TermQuery termQuery = (TermQuery) query;
-        assertThat(termQuery.getTerm().field(), equalTo(queryBuilder.fieldName()));
-        MappedFieldType mapper = context.fieldMapper(queryBuilder.fieldName());
-        if (mapper != null) {
-            BytesRef bytesRef = mapper.indexedValueForSearch(queryBuilder.value());
-            assertThat(termQuery.getTerm().bytes(), equalTo(bytesRef));
-        } else {
-            assertThat(termQuery.getTerm().bytes(), equalTo(BytesRefs.toBytesRef(queryBuilder.value())));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/TypeQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/TypeQueryBuilderTest.java
deleted file mode 100644
index e1419d5..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/TypeQueryBuilderTest.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.*;
-
-public class TypeQueryBuilderTest extends BaseQueryTestCase<TypeQueryBuilder> {
-
-    @Override
-    protected TypeQueryBuilder doCreateTestQueryBuilder() {
-        return new TypeQueryBuilder(getRandomType());
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(TypeQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, either(instanceOf(TermQuery.class)).or(instanceOf(ConstantScoreQuery.class)));
-        if (query instanceof ConstantScoreQuery) {
-            query = ((ConstantScoreQuery) query).getQuery();
-            assertThat(query, instanceOf(TermQuery.class));
-        }
-        TermQuery termQuery = (TermQuery) query;
-        assertThat(termQuery.getTerm().field(), equalTo(TypeFieldMapper.NAME));
-        assertThat(termQuery.getTerm().bytes(), equalTo(queryBuilder.type()));
-    }
-
-    @Test
-    public void testValidate() {
-        TypeQueryBuilder typeQueryBuilder = new TypeQueryBuilder((String) null);
-        assertThat(typeQueryBuilder.validate().validationErrors().size(), is(1));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/WildcardQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/WildcardQueryBuilderTest.java
deleted file mode 100644
index ba23249..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/WildcardQueryBuilderTest.java
+++ /dev/null
@@ -1,78 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.WildcardQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-
-public class WildcardQueryBuilderTest extends BaseQueryTestCase<WildcardQueryBuilder> {
-
-    @Override
-    protected WildcardQueryBuilder doCreateTestQueryBuilder() {
-        WildcardQueryBuilder query;
-
-        // mapped or unmapped field
-        String text = randomAsciiOfLengthBetween(1, 10);
-        if (randomBoolean()) {
-            query = new WildcardQueryBuilder(STRING_FIELD_NAME, text);
-        } else {
-            query = new WildcardQueryBuilder(randomAsciiOfLengthBetween(1, 10), text);
-        }
-        if (randomBoolean()) {
-            query.rewrite(randomFrom(getRandomRewriteMethod()));
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(WildcardQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(WildcardQuery.class));
-    }
-
-    @Test
-    public void testValidate() {
-        WildcardQueryBuilder wildcardQueryBuilder = new WildcardQueryBuilder("", "text");
-        assertThat(wildcardQueryBuilder.validate().validationErrors().size(), is(1));
-
-        wildcardQueryBuilder = new WildcardQueryBuilder("field", null);
-        assertThat(wildcardQueryBuilder.validate().validationErrors().size(), is(1));
-
-        wildcardQueryBuilder = new WildcardQueryBuilder(null, null);
-        assertThat(wildcardQueryBuilder.validate().validationErrors().size(), is(2));
-
-        wildcardQueryBuilder = new WildcardQueryBuilder("field", "text");
-        assertNull(wildcardQueryBuilder.validate());
-    }
-
-    @Test
-    public void testEmptyValue() throws IOException {
-        QueryShardContext context = createShardContext();
-        context.setAllowUnmappedFields(true);
-
-        WildcardQueryBuilder wildcardQueryBuilder = new WildcardQueryBuilder(getRandomType(), "");
-        assertEquals(wildcardQueryBuilder.toQuery(context).getClass(), WildcardQuery.class);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java b/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java
index 7421a11..2e0356e 100644
--- a/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java
+++ b/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java
@@ -27,7 +27,10 @@ import org.elasticsearch.common.inject.Module;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.*;
+import org.elasticsearch.index.query.QueryBuilder;
+import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.query.QueryParser;
+import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.indices.query.IndicesQueriesModule;
 import org.elasticsearch.plugins.AbstractPlugin;
 
@@ -57,36 +60,24 @@ public class DummyQueryParserPlugin extends AbstractPlugin {
         return Settings.EMPTY;
     }
 
-    public static class DummyQueryBuilder extends AbstractQueryBuilder<DummyQueryBuilder> {
-        private static final String NAME = "dummy";
-
+    public static class DummyQueryBuilder extends QueryBuilder {
         @Override
         protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAME).endObject();
-        }
-
-        @Override
-        public String getWriteableName() {
-            return NAME;
+            builder.startObject("dummy").endObject();
         }
     }
 
-    public static class DummyQueryParser extends BaseQueryParserTemp {
+    public static class DummyQueryParser implements QueryParser {
         @Override
         public String[] names() {
-            return new String[]{DummyQueryBuilder.NAME};
+            return new String[]{"dummy"};
         }
 
         @Override
-        public Query parse(QueryShardContext context) throws IOException, QueryShardException {
-            XContentParser.Token token = context.parseContext().parser().nextToken();
+        public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
+            XContentParser.Token token = parseContext.parser().nextToken();
             assert token == XContentParser.Token.END_OBJECT;
-            return new DummyQuery(context.parseContext().isFilter());
-        }
-
-        @Override
-        public DummyQueryBuilder getBuilderPrototype() {
-            return new DummyQueryBuilder();
+            return new DummyQuery(parseContext.isFilter());
         }
     }
 
diff --git a/core/src/test/java/org/elasticsearch/index/search/child/AbstractChildTestCase.java b/core/src/test/java/org/elasticsearch/index/search/child/AbstractChildTestCase.java
index c69220f..c7dd274 100644
--- a/core/src/test/java/org/elasticsearch/index/search/child/AbstractChildTestCase.java
+++ b/core/src/test/java/org/elasticsearch/index/search/child/AbstractChildTestCase.java
@@ -19,19 +19,14 @@
 
 package org.elasticsearch.index.search.child;
 
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.Filter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.search.*;
 import org.apache.lucene.search.join.BitDocIdSetFilter;
 import org.apache.lucene.util.BitDocIdSet;
 import org.apache.lucene.util.BitSet;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.indices.mapping.put.PutMappingRequest;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.compress.CompressedXContent;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentParser;
@@ -40,7 +35,7 @@ import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.internal.UidFieldMapper;
 import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.test.ESSingleNodeTestCase;
 import org.hamcrest.Description;
@@ -71,7 +66,7 @@ public abstract class AbstractChildTestCase extends ESSingleNodeTestCase {
         mapperService.merge(childType, new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef(childType, "_parent", "type=" + parentType, CHILD_SCORE_NAME, "type=double,doc_values=false").string()), true, false);
         return createSearchContext(indexService);
     }
-
+    
     static void assertBitSet(BitSet actual, BitSet expected, IndexSearcher searcher) throws IOException {
         assertBitSet(new BitDocIdSet(actual), new BitDocIdSet(expected), searcher);
     }
@@ -88,7 +83,7 @@ public abstract class AbstractChildTestCase extends ESSingleNodeTestCase {
             throw new java.lang.AssertionError(description.toString());
         }
     }
-
+    
     static boolean equals(BitDocIdSet expected, BitDocIdSet actual) {
         if (actual == null && expected == null) {
             return true;
@@ -140,10 +135,10 @@ public abstract class AbstractChildTestCase extends ESSingleNodeTestCase {
     }
 
     static Query parseQuery(QueryBuilder queryBuilder) throws IOException {
-        QueryShardContext context = new QueryShardContext(new Index("test"), SearchContext.current().queryParserService());
+        QueryParseContext context = new QueryParseContext(new Index("test"), SearchContext.current().queryParserService());
         XContentParser parser = XContentHelper.createParser(queryBuilder.buildAsBytes());
         context.reset(parser);
-        return context.parseContext().parseInnerQuery();
+        return context.parseInnerQuery();
     }
 
 }
diff --git a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
index 3ff77a3..48e4e1b 100644
--- a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
+++ b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
@@ -52,9 +52,7 @@ import java.util.HashSet;
 import java.util.Set;
 import java.util.concurrent.ExecutionException;
 
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_VERSION_CREATED;
+import static org.elasticsearch.cluster.metadata.IndexMetaData.*;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
diff --git a/core/src/test/java/org/elasticsearch/index/store/CorruptedFileIT.java b/core/src/test/java/org/elasticsearch/index/store/CorruptedFileIT.java
index c80621c..1692eb5 100644
--- a/core/src/test/java/org/elasticsearch/index/store/CorruptedFileIT.java
+++ b/core/src/test/java/org/elasticsearch/index/store/CorruptedFileIT.java
@@ -431,7 +431,7 @@ public class CorruptedFileIT extends ESIntegTestCase {
         // we are green so primaries got not corrupted.
         // ensure that no shard is actually allocated on the unlucky node
         ClusterStateResponse clusterStateResponse = client().admin().cluster().prepareState().get();
-        for (IndexShardRoutingTable table : clusterStateResponse.getState().routingNodes().getRoutingTable().index("test")) {
+        for (IndexShardRoutingTable table : clusterStateResponse.getState().getRoutingNodes().getRoutingTable().index("test")) {
             for (ShardRouting routing : table) {
                 if (unluckyNode.getNode().getId().equals(routing.currentNodeId())) {
                     assertThat(routing.state(), not(equalTo(ShardRoutingState.STARTED)));
diff --git a/core/src/test/java/org/elasticsearch/indexlifecycle/IndexLifecycleActionIT.java b/core/src/test/java/org/elasticsearch/indexlifecycle/IndexLifecycleActionIT.java
index e299115..8ea074f 100644
--- a/core/src/test/java/org/elasticsearch/indexlifecycle/IndexLifecycleActionIT.java
+++ b/core/src/test/java/org/elasticsearch/indexlifecycle/IndexLifecycleActionIT.java
@@ -78,7 +78,7 @@ public class IndexLifecycleActionIT extends ESIntegTestCase {
         assertThat(clusterHealth.getStatus(), equalTo(ClusterHealthStatus.YELLOW));
 
         ClusterState clusterState = client().admin().cluster().prepareState().get().getState();
-        RoutingNode routingNodeEntry1 = clusterState.readOnlyRoutingNodes().node(node1);
+        RoutingNode routingNodeEntry1 = clusterState.getRoutingNodes().node(node1);
         assertThat(routingNodeEntry1.numberOfShardsWithState(STARTED), equalTo(11));
 
         logger.info("Starting server2");
@@ -108,11 +108,11 @@ public class IndexLifecycleActionIT extends ESIntegTestCase {
 
 
         clusterState = client().admin().cluster().prepareState().get().getState();
-        assertNodesPresent(clusterState.readOnlyRoutingNodes(), node1, node2);
-        routingNodeEntry1 = clusterState.readOnlyRoutingNodes().node(node1);
+        assertNodesPresent(clusterState.getRoutingNodes(), node1, node2);
+        routingNodeEntry1 = clusterState.getRoutingNodes().node(node1);
         assertThat(routingNodeEntry1.numberOfShardsWithState(RELOCATING), equalTo(0));
         assertThat(routingNodeEntry1.numberOfShardsWithState(STARTED), equalTo(11));
-        RoutingNode routingNodeEntry2 = clusterState.readOnlyRoutingNodes().node(node2);
+        RoutingNode routingNodeEntry2 = clusterState.getRoutingNodes().node(node2);
         assertThat(routingNodeEntry2.numberOfShardsWithState(INITIALIZING), equalTo(0));
         assertThat(routingNodeEntry2.numberOfShardsWithState(STARTED), equalTo(11));
 
@@ -144,11 +144,11 @@ public class IndexLifecycleActionIT extends ESIntegTestCase {
 
 
         clusterState = client().admin().cluster().prepareState().get().getState();
-        assertNodesPresent(clusterState.readOnlyRoutingNodes(), node1, node2, node3);
+        assertNodesPresent(clusterState.getRoutingNodes(), node1, node2, node3);
 
-        routingNodeEntry1 = clusterState.readOnlyRoutingNodes().node(node1);
-        routingNodeEntry2 = clusterState.readOnlyRoutingNodes().node(node2);
-        RoutingNode routingNodeEntry3 = clusterState.readOnlyRoutingNodes().node(node3);
+        routingNodeEntry1 = clusterState.getRoutingNodes().node(node1);
+        routingNodeEntry2 = clusterState.getRoutingNodes().node(node2);
+        RoutingNode routingNodeEntry3 = clusterState.getRoutingNodes().node(node3);
 
         assertThat(routingNodeEntry1.numberOfShardsWithState(STARTED) + routingNodeEntry2.numberOfShardsWithState(STARTED) + routingNodeEntry3.numberOfShardsWithState(STARTED), equalTo(22));
 
@@ -181,9 +181,9 @@ public class IndexLifecycleActionIT extends ESIntegTestCase {
         assertThat(clusterHealth.getActivePrimaryShards(), equalTo(11));
 
         clusterState = client().admin().cluster().prepareState().get().getState();
-        assertNodesPresent(clusterState.readOnlyRoutingNodes(), node3, node2);
-        routingNodeEntry2 = clusterState.readOnlyRoutingNodes().node(node2);
-        routingNodeEntry3 = clusterState.readOnlyRoutingNodes().node(node3);
+        assertNodesPresent(clusterState.getRoutingNodes(), node3, node2);
+        routingNodeEntry2 = clusterState.getRoutingNodes().node(node2);
+        routingNodeEntry3 = clusterState.getRoutingNodes().node(node3);
 
         assertThat(routingNodeEntry2.numberOfShardsWithState(STARTED) + routingNodeEntry3.numberOfShardsWithState(STARTED), equalTo(22));
 
@@ -200,11 +200,11 @@ public class IndexLifecycleActionIT extends ESIntegTestCase {
         assertThat(deleteIndexResponse.isAcknowledged(), equalTo(true));
 
         clusterState = client().admin().cluster().prepareState().get().getState();
-        assertNodesPresent(clusterState.readOnlyRoutingNodes(), node3, node2);
-        routingNodeEntry2 = clusterState.readOnlyRoutingNodes().node(node2);
+        assertNodesPresent(clusterState.getRoutingNodes(), node3, node2);
+        routingNodeEntry2 = clusterState.getRoutingNodes().node(node2);
         assertThat(routingNodeEntry2.isEmpty(), equalTo(true));
 
-        routingNodeEntry3 = clusterState.readOnlyRoutingNodes().node(node3);
+        routingNodeEntry3 = clusterState.getRoutingNodes().node(node3);
         assertThat(routingNodeEntry3.isEmpty(), equalTo(true));
     }
 
diff --git a/core/src/test/java/org/elasticsearch/indices/recovery/IndexRecoveryIT.java b/core/src/test/java/org/elasticsearch/indices/recovery/IndexRecoveryIT.java
index 084862a..c8e582a 100644
--- a/core/src/test/java/org/elasticsearch/indices/recovery/IndexRecoveryIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/recovery/IndexRecoveryIT.java
@@ -553,7 +553,7 @@ public class IndexRecoveryIT extends ESIntegTestCase {
         ClusterStateResponse stateResponse = client().admin().cluster().prepareState().get();
         final String blueNodeId = internalCluster().getInstance(DiscoveryService.class, blueNodeName).localNode().id();
 
-        assertFalse(stateResponse.getState().readOnlyRoutingNodes().node(blueNodeId).isEmpty());
+        assertFalse(stateResponse.getState().getRoutingNodes().node(blueNodeId).isEmpty());
 
         SearchResponse searchResponse = client().prepareSearch(indexName).get();
         assertHitCount(searchResponse, numDocs);
diff --git a/core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java b/core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java
index 6ea7e07..c75f2fa 100644
--- a/core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java
@@ -89,7 +89,7 @@ public class RareClusterStateIT extends ESIntegTestCase {
                 ClusterState.builder(current)
                         .routingTable(RoutingTable.builder(current.routingTable()).remove("a").addAsRecovery(current.metaData().index("a")))
                         .nodes(DiscoveryNodes.EMPTY_NODES)
-                        .build()
+                        .build(), false
         );
         RoutingAllocation routingAllocation = new RoutingAllocation(allocationDeciders, routingNodes, current.nodes(), ClusterInfo.EMPTY);
         allocator.allocateUnassigned(routingAllocation);
diff --git a/core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationIT.java b/core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationIT.java
index 5886e9b..db999cd 100644
--- a/core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationIT.java
@@ -327,7 +327,7 @@ public class IndicesStoreIntegrationIT extends ESIntegTestCase {
         waitNoPendingTasksOnAll();
         ClusterStateResponse stateResponse = client().admin().cluster().prepareState().get();
 
-        RoutingNode routingNode = stateResponse.getState().routingNodes().node(nonMasterId);
+        RoutingNode routingNode = stateResponse.getState().getRoutingNodes().node(nonMasterId);
         final int[] node2Shards = new int[routingNode.numberOfOwningShards()];
         int i = 0;
         for (ShardRouting shardRouting : routingNode) {
diff --git a/core/src/test/java/org/elasticsearch/percolator/MultiPercolatorIT.java b/core/src/test/java/org/elasticsearch/percolator/MultiPercolatorIT.java
index cb64437..77a4b63 100644
--- a/core/src/test/java/org/elasticsearch/percolator/MultiPercolatorIT.java
+++ b/core/src/test/java/org/elasticsearch/percolator/MultiPercolatorIT.java
@@ -26,7 +26,6 @@ import org.elasticsearch.client.Requests;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.MatchQueryBuilder;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
@@ -361,7 +360,7 @@ public class MultiPercolatorIT extends ESIntegTestCase {
         ensureGreen("nestedindex");
 
         client().prepareIndex("nestedindex", PercolatorService.TYPE_NAME, "Q").setSource(jsonBuilder().startObject()
-                .field("query", QueryBuilders.nestedQuery("employee", QueryBuilders.matchQuery("employee.name", "virginia potts").operator(Operator.AND)).scoreMode("avg")).endObject()).get();
+                .field("query", QueryBuilders.nestedQuery("employee", QueryBuilders.matchQuery("employee.name", "virginia potts").operator(MatchQueryBuilder.Operator.AND)).scoreMode("avg")).endObject()).get();
 
         refresh();
 
diff --git a/core/src/test/java/org/elasticsearch/percolator/PercolatorBackwardsCompatibilityIT.java b/core/src/test/java/org/elasticsearch/percolator/PercolatorBackwardsCompatibilityIT.java
index f250e92..ecee193 100644
--- a/core/src/test/java/org/elasticsearch/percolator/PercolatorBackwardsCompatibilityIT.java
+++ b/core/src/test/java/org/elasticsearch/percolator/PercolatorBackwardsCompatibilityIT.java
@@ -23,8 +23,8 @@ import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.percolate.PercolateResponse;
 import org.elasticsearch.action.percolate.PercolateSourceBuilder;
 import org.elasticsearch.index.percolator.PercolatorException;
+import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.test.ESIntegTestCase;
-import org.elasticsearch.index.query.QueryShardException;
 import org.junit.Test;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
@@ -67,7 +67,7 @@ public class PercolatorBackwardsCompatibilityIT extends ESIntegTestCase {
             fail();
         } catch (PercolatorException e) {
             e.printStackTrace();
-            assertThat(e.getRootCause(), instanceOf(QueryShardException.class));
+            assertThat(e.getRootCause(), instanceOf(QueryParsingException.class));
         }
     }
 
diff --git a/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java b/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java
index 28d605a..fb37a0d 100644
--- a/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java
+++ b/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java
@@ -42,9 +42,8 @@ import org.elasticsearch.index.engine.DocumentMissingException;
 import org.elasticsearch.index.engine.VersionConflictEngineException;
 import org.elasticsearch.index.percolator.PercolatorException;
 import org.elasticsearch.index.query.MatchQueryBuilder;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.query.functionscore.factor.FactorBuilder;
 import org.elasticsearch.index.query.support.QueryInnerHitBuilder;
 import org.elasticsearch.rest.RestStatus;
@@ -508,37 +507,6 @@ public class PercolatorIT extends ESIntegTestCase {
     }
 
     @Test
-    public void percolateWithSizeField() throws Exception {
-        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
-                .startObject("_size").field("enabled", true).endObject()
-                .startObject("properties").startObject("field1").field("type", "string").endObject().endObject()
-                .endObject().endObject().string();
-
-        assertAcked(prepareCreate("test").addMapping("type1", mapping));
-        ensureGreen();
-
-        logger.info("--> register a query");
-        client().prepareIndex("test", PercolatorService.TYPE_NAME, "kuku")
-                .setSource(jsonBuilder().startObject()
-                        .field("query", termQuery("field1", "value1"))
-                        .endObject())
-                .setRefresh(true)
-                .execute().actionGet();
-
-        logger.info("--> percolate a document");
-        PercolateResponse percolate = client().preparePercolate().setIndices("test").setDocumentType("type1")
-                .setSource(jsonBuilder().startObject()
-                        .startObject("doc")
-                        .field("field1", "value1")
-                        .endObject()
-                        .endObject())
-                .execute().actionGet();
-        assertMatchCount(percolate, 1l);
-        assertThat(percolate.getMatches(), arrayWithSize(1));
-        assertThat(convertFromTextArray(percolate.getMatches(), "test"), arrayContaining("kuku"));
-    }
-
-    @Test
     public void testPercolateStatistics() throws Exception {
         client().admin().indices().prepareCreate("test").execute().actionGet();
         ensureGreen();
@@ -1796,7 +1764,7 @@ public class PercolatorIT extends ESIntegTestCase {
                     .get();
             fail();
         } catch (PercolatorException e) {
-            assertThat(e.getRootCause(), instanceOf(QueryShardException.class));
+            assertThat(e.getRootCause(), instanceOf(QueryParsingException.class));
         }
 
         try {
@@ -1805,7 +1773,7 @@ public class PercolatorIT extends ESIntegTestCase {
                     .get();
             fail();
         } catch (PercolatorException e) {
-            assertThat(e.getRootCause(), instanceOf(QueryShardException.class));
+            assertThat(e.getRootCause(), instanceOf(QueryParsingException.class));
         }
     }
 
@@ -1844,7 +1812,7 @@ public class PercolatorIT extends ESIntegTestCase {
         ensureGreen("nestedindex");
 
         client().prepareIndex("nestedindex", PercolatorService.TYPE_NAME, "Q").setSource(jsonBuilder().startObject()
-                .field("query", QueryBuilders.nestedQuery("employee", QueryBuilders.matchQuery("employee.name", "virginia potts").operator(Operator.AND)).scoreMode("avg")).endObject()).get();
+                .field("query", QueryBuilders.nestedQuery("employee", QueryBuilders.matchQuery("employee.name", "virginia potts").operator(MatchQueryBuilder.Operator.AND)).scoreMode("avg")).endObject()).get();
 
         refresh();
 
@@ -2048,7 +2016,7 @@ public class PercolatorIT extends ESIntegTestCase {
                     .execute().actionGet();
             fail("Expected a parse error, because inner_hits isn't supported in the percolate api");
         } catch (Exception e) {
-            assertThat(e.getCause(), instanceOf(QueryShardException.class));
+            assertThat(e.getCause(), instanceOf(QueryParsingException.class));
             assertThat(e.getCause().getMessage(), containsString("inner_hits unsupported"));
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/recovery/RelocationIT.java b/core/src/test/java/org/elasticsearch/recovery/RelocationIT.java
index 54afe03..cc92e49 100644
--- a/core/src/test/java/org/elasticsearch/recovery/RelocationIT.java
+++ b/core/src/test/java/org/elasticsearch/recovery/RelocationIT.java
@@ -395,7 +395,7 @@ public class RelocationIT extends ESIntegTestCase {
         ClusterStateResponse stateResponse = client().admin().cluster().prepareState().get();
         String blueNodeId = internalCluster().getInstance(DiscoveryService.class, blueNodeName).localNode().id();
 
-        assertFalse(stateResponse.getState().readOnlyRoutingNodes().node(blueNodeId).isEmpty());
+        assertFalse(stateResponse.getState().getRoutingNodes().node(blueNodeId).isEmpty());
 
         SearchResponse searchResponse = client().prepareSearch(indexName).get();
         assertHitCount(searchResponse, numDocs);
@@ -445,7 +445,7 @@ public class RelocationIT extends ESIntegTestCase {
         assertHitCount(searchResponse, numDocs);
 
         stateResponse = client().admin().cluster().prepareState().get();
-        assertTrue(stateResponse.getState().readOnlyRoutingNodes().node(blueNodeId).isEmpty());
+        assertTrue(stateResponse.getState().getRoutingNodes().node(blueNodeId).isEmpty());
     }
 
     @Test
diff --git a/core/src/test/java/org/elasticsearch/script/NativeScriptTests.java b/core/src/test/java/org/elasticsearch/script/NativeScriptTests.java
index 6ac8f19..490bcef 100644
--- a/core/src/test/java/org/elasticsearch/script/NativeScriptTests.java
+++ b/core/src/test/java/org/elasticsearch/script/NativeScriptTests.java
@@ -97,6 +97,11 @@ public class NativeScriptTests extends ESTestCase {
         public ExecutableScript newScript(@Nullable Map<String, Object> params) {
             return new MyScript();
         }
+
+        @Override
+        public boolean needsScores() {
+            return false;
+        }
     }
 
     static class MyScript extends AbstractExecutableScript {
diff --git a/core/src/test/java/org/elasticsearch/script/ScriptFieldIT.java b/core/src/test/java/org/elasticsearch/script/ScriptFieldIT.java
index 2acfb55..471cbb4 100644
--- a/core/src/test/java/org/elasticsearch/script/ScriptFieldIT.java
+++ b/core/src/test/java/org/elasticsearch/script/ScriptFieldIT.java
@@ -81,6 +81,11 @@ public class ScriptFieldIT extends ESIntegTestCase {
         public ExecutableScript newScript(@Nullable Map<String, Object> params) {
             return new IntScript();
         }
+
+        @Override
+        public boolean needsScores() {
+            return false;
+        }
     }
 
     static class IntScript extends AbstractSearchScript {
@@ -95,6 +100,11 @@ public class ScriptFieldIT extends ESIntegTestCase {
         public ExecutableScript newScript(@Nullable Map<String, Object> params) {
             return new LongScript();
         }
+
+        @Override
+        public boolean needsScores() {
+            return false;
+        }
     }
 
     static class LongScript extends AbstractSearchScript {
@@ -109,6 +119,11 @@ public class ScriptFieldIT extends ESIntegTestCase {
         public ExecutableScript newScript(@Nullable Map<String, Object> params) {
             return new FloatScript();
         }
+
+        @Override
+        public boolean needsScores() {
+            return false;
+        }
     }
 
     static class FloatScript extends AbstractSearchScript {
@@ -123,6 +138,11 @@ public class ScriptFieldIT extends ESIntegTestCase {
         public ExecutableScript newScript(@Nullable Map<String, Object> params) {
             return new DoubleScript();
         }
+
+        @Override
+        public boolean needsScores() {
+            return false;
+        }
     }
 
     static class DoubleScript extends AbstractSearchScript {
diff --git a/core/src/test/java/org/elasticsearch/script/expression/ExpressionScriptTests.java b/core/src/test/java/org/elasticsearch/script/expression/ExpressionScriptTests.java
new file mode 100644
index 0000000..4b01435
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/script/expression/ExpressionScriptTests.java
@@ -0,0 +1,57 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script.expression;
+
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.IndexService;
+import org.elasticsearch.script.CompiledScript;
+import org.elasticsearch.script.ScriptService.ScriptType;
+import org.elasticsearch.script.SearchScript;
+import org.elasticsearch.search.lookup.SearchLookup;
+import org.elasticsearch.test.ESSingleNodeTestCase;
+
+import java.util.Collections;
+
+public class ExpressionScriptTests extends ESSingleNodeTestCase {
+
+    public void testNeedsScores() {
+        IndexService index = createIndex("test", Settings.EMPTY, "type", "d", "type=double");
+        
+        ExpressionScriptEngineService service = new ExpressionScriptEngineService(Settings.EMPTY);
+        SearchLookup lookup = new SearchLookup(index.mapperService(), index.fieldData(), null);
+
+        Object compiled = service.compile("1.2");
+        SearchScript ss = service.search(new CompiledScript(ScriptType.INLINE, "randomName", "expression", compiled), lookup, Collections.<String, Object>emptyMap());
+        assertFalse(ss.needsScores());
+
+        compiled = service.compile("doc['d'].value");
+        ss = service.search(new CompiledScript(ScriptType.INLINE, "randomName", "expression", compiled), lookup, Collections.<String, Object>emptyMap());
+        assertFalse(ss.needsScores());
+
+        compiled = service.compile("1/_score");
+        ss = service.search(new CompiledScript(ScriptType.INLINE, "randomName", "expression", compiled), lookup, Collections.<String, Object>emptyMap());
+        assertTrue(ss.needsScores());
+
+        compiled = service.compile("doc['d'].value * _score");
+        ss = service.search(new CompiledScript(ScriptType.INLINE, "randomName", "expression", compiled), lookup, Collections.<String, Object>emptyMap());
+        assertTrue(ss.needsScores());
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java
index 7269cd2..941fcb3 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java
@@ -29,7 +29,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.plugins.AbstractPlugin;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptModule;
@@ -218,7 +218,7 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         public static class SimpleHeuristicParser implements SignificanceHeuristicParser {
 
             @Override
-            public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException, QueryShardException {
+            public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException, QueryParsingException {
                 parser.nextToken();
                 return new SimpleHeuristic();
             }
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/script/NativeSignificanceScoreScriptNoParams.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/script/NativeSignificanceScoreScriptNoParams.java
index 816c845..313734c 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/script/NativeSignificanceScoreScriptNoParams.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/script/NativeSignificanceScoreScriptNoParams.java
@@ -35,6 +35,11 @@ public class NativeSignificanceScoreScriptNoParams extends TestScript {
         public ExecutableScript newScript(@Nullable Map<String, Object> params) {
             return new NativeSignificanceScoreScriptNoParams();
         }
+
+        @Override
+        public boolean needsScores() {
+            return false;
+        }
     }
 
     private NativeSignificanceScoreScriptNoParams() {
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/script/NativeSignificanceScoreScriptWithParams.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/script/NativeSignificanceScoreScriptWithParams.java
index 3568f8f..6b248fe 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/script/NativeSignificanceScoreScriptWithParams.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/script/NativeSignificanceScoreScriptWithParams.java
@@ -36,6 +36,11 @@ public class NativeSignificanceScoreScriptWithParams extends TestScript {
         public ExecutableScript newScript(@Nullable Map<String, Object> params) {
             return new NativeSignificanceScoreScriptWithParams(params);
         }
+
+        @Override
+        public boolean needsScores() {
+            return false;
+        }
     }
 
     private NativeSignificanceScoreScriptWithParams(Map<String, Object> params) {
diff --git a/core/src/test/java/org/elasticsearch/search/fields/SearchFieldsIT.java b/core/src/test/java/org/elasticsearch/search/fields/SearchFieldsIT.java
index 15abcc9..1075347 100644
--- a/core/src/test/java/org/elasticsearch/search/fields/SearchFieldsIT.java
+++ b/core/src/test/java/org/elasticsearch/search/fields/SearchFieldsIT.java
@@ -76,9 +76,8 @@ public class SearchFieldsIT extends ESIntegTestCase {
         client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForYellowStatus().execute().actionGet();
 
         String mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
-                // _timestamp and _size are randomly enabled via templates but we don't want it here to test stored fields behaviour
+                // _timestamp is randomly enabled via templates but we don't want it here to test stored fields behaviour
                 .startObject("_timestamp").field("enabled", false).endObject()
-                .startObject("_size").field("enabled", false).endObject()
                 .startObject("properties")
                 .startObject("field1").field("type", "string").field("store", "yes").endObject()
                 .startObject("field2").field("type", "string").field("store", "no").endObject()
diff --git a/core/src/test/java/org/elasticsearch/search/functionscore/ExplainableScriptIT.java b/core/src/test/java/org/elasticsearch/search/functionscore/ExplainableScriptIT.java
index 3fbc9b6..44b7ea7 100644
--- a/core/src/test/java/org/elasticsearch/search/functionscore/ExplainableScriptIT.java
+++ b/core/src/test/java/org/elasticsearch/search/functionscore/ExplainableScriptIT.java
@@ -102,6 +102,10 @@ public class ExplainableScriptIT extends ESIntegTestCase {
         public ExecutableScript newScript(@Nullable Map<String, Object> params) {
             return new MyScript();
         }
+        @Override
+        public boolean needsScores() {
+            return true;
+        }
     }
 
     static class MyScript extends AbstractDoubleSearchScript implements ExplainableSearchScript, ExecutableScript {
diff --git a/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java b/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
index 2509c32..d9f69de 100644
--- a/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
@@ -27,9 +27,10 @@ import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.settings.Settings.Builder;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.query.*;
+import org.elasticsearch.index.query.BoostableQueryBuilder;
 import org.elasticsearch.index.query.IdsQueryBuilder;
 import org.elasticsearch.index.query.MatchQueryBuilder;
+import org.elasticsearch.index.query.MatchQueryBuilder.Operator;
 import org.elasticsearch.index.query.MatchQueryBuilder.Type;
 import org.elasticsearch.index.query.MultiMatchQueryBuilder;
 import org.elasticsearch.index.query.QueryBuilder;
@@ -70,7 +71,12 @@ import static org.elasticsearch.index.query.QueryBuilders.typeQuery;
 import static org.elasticsearch.index.query.QueryBuilders.wildcardQuery;
 import static org.elasticsearch.search.builder.SearchSourceBuilder.highlight;
 import static org.elasticsearch.search.builder.SearchSourceBuilder.searchSource;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFailures;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHighlight;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNotHighlighted;
 import static org.elasticsearch.test.hamcrest.RegexMatcher.matches;
 import static org.hamcrest.Matchers.anyOf;
 import static org.hamcrest.Matchers.containsString;
@@ -1391,7 +1397,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
-                .query(boostingQuery(termQuery("field2", "brown"), termQuery("field2", "foobar")).negativeBoost(0.5f))
+                .query(boostingQuery().positive(termQuery("field2", "brown")).negative(termQuery("field2", "foobar")).negativeBoost(0.5f))
                 .highlight(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
 
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
@@ -1409,7 +1415,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
-                .query(boostingQuery(termQuery("field2", "brown"), termQuery("field2", "foobar")).negativeBoost(0.5f))
+                .query(boostingQuery().positive(termQuery("field2", "brown")).negative(termQuery("field2", "foobar")).negativeBoost(0.5f))
                 .highlight(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
 
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
@@ -2323,7 +2329,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
-                .query(boostingQuery(termQuery("field2", "brown"), termQuery("field2", "foobar")).negativeBoost(0.5f))
+                .query(boostingQuery().positive(termQuery("field2", "brown")).negative(termQuery("field2", "foobar")).negativeBoost(0.5f))
                 .highlight(highlight().field("field2").preTags("<x>").postTags("</x>"));
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -2611,7 +2617,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 queryStringQuery("\"highlight words together\"").field("field1^100").autoGeneratePhraseQueries(true));
     }
 
-    private <P extends AbstractQueryBuilder<P>> void
+    private <P extends QueryBuilder & BoostableQueryBuilder<?>> void
             phraseBoostTestCaseForClauses(String highlighterType, float boost, QueryBuilder terms, P phrase) {
         Matcher<String> highlightedMatcher = Matchers.either(containsString("<em>highlight words together</em>")).or(
                 containsString("<em>highlight</em> <em>words</em> <em>together</em>"));
@@ -2625,10 +2631,10 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         assertHighlight(response, 0, "field1", 0, 1, highlightedMatcher);
         phrase.boost(1);
         // Try with a boosting query
-        response = search.setQuery(boostingQuery(phrase, terms).boost(boost).negativeBoost(1)).get();
+        response = search.setQuery(boostingQuery().positive(phrase).negative(terms).boost(boost).negativeBoost(1)).get();
         assertHighlight(response, 0, "field1", 0, 1, highlightedMatcher);
         // Try with a boosting query using a negative boost
-        response = search.setQuery(boostingQuery(phrase, terms).boost(1).negativeBoost(1/boost)).get();
+        response = search.setQuery(boostingQuery().positive(phrase).negative(terms).boost(1).negativeBoost(1/boost)).get();
         assertHighlight(response, 0, "field1", 0, 1, highlightedMatcher);
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/search/query/MultiMatchQueryIT.java b/core/src/test/java/org/elasticsearch/search/query/MultiMatchQueryIT.java
index 793d365..b9099d0 100644
--- a/core/src/test/java/org/elasticsearch/search/query/MultiMatchQueryIT.java
+++ b/core/src/test/java/org/elasticsearch/search/query/MultiMatchQueryIT.java
@@ -28,7 +28,6 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.MatchQueryBuilder;
 import org.elasticsearch.index.query.MultiMatchQueryBuilder;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.search.SearchHits;
@@ -155,7 +154,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
         MatchQueryBuilder.Type type = randomBoolean() ? null : MatchQueryBuilder.Type.BOOLEAN;
         SearchResponse searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR))).get();
+                        .operator(MatchQueryBuilder.Operator.OR))).get();
         Set<String> topNIds = Sets.newHashSet("theone", "theother");
         for (int i = 0; i < searchResponse.getHits().hits().length; i++) {
             topNIds.remove(searchResponse.getHits().getAt(i).getId());
@@ -167,25 +166,25 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR).useDisMax(false).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).useDisMax(false).type(type))).get();
         assertFirstHit(searchResponse, anyOf(hasId("theone"), hasId("theother")));
         assertThat(searchResponse.getHits().hits()[0].getScore(), greaterThan(searchResponse.getHits().hits()[1].getScore()));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).type(type))).get();
         assertFirstHit(searchResponse, hasId("theother"));
 
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.AND).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.AND).type(type))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.AND).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.AND).type(type))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
     }
@@ -194,18 +193,18 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
     public void testPhraseType() {
         SearchResponse searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("Man the Ultimate", "full_name_phrase", "first_name_phrase", "last_name_phrase", "category_phrase")
-                        .operator(Operator.OR).type(MatchQueryBuilder.Type.PHRASE))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).type(MatchQueryBuilder.Type.PHRASE))).get();
         assertFirstHit(searchResponse, hasId("ultimate2"));
         assertHitCount(searchResponse, 1l);
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("Captain", "full_name_phrase", "first_name_phrase", "last_name_phrase", "category_phrase")
-                        .operator(Operator.OR).type(MatchQueryBuilder.Type.PHRASE))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).type(MatchQueryBuilder.Type.PHRASE))).get();
         assertThat(searchResponse.getHits().getTotalHits(), greaterThan(1l));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("the Ul", "full_name_phrase", "first_name_phrase", "last_name_phrase", "category_phrase")
-                        .operator(Operator.OR).type(MatchQueryBuilder.Type.PHRASE_PREFIX))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).type(MatchQueryBuilder.Type.PHRASE_PREFIX))).get();
         assertSearchHits(searchResponse, "ultimate2", "ultimate1");
         assertHitCount(searchResponse, 2l);
     }
@@ -264,7 +263,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
         Float cutoffFrequency = randomBoolean() ? Math.min(1, numDocs * 1.f / between(10, 20)) : 1.f / between(10, 20);
         SearchResponse searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR).cutoffFrequency(cutoffFrequency))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).cutoffFrequency(cutoffFrequency))).get();
         Set<String> topNIds = Sets.newHashSet("theone", "theother");
         for (int i = 0; i < searchResponse.getHits().hits().length; i++) {
             topNIds.remove(searchResponse.getHits().getAt(i).getId());
@@ -277,39 +276,39 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
         cutoffFrequency = randomBoolean() ? Math.min(1, numDocs * 1.f / between(10, 20)) : 1.f / between(10, 20);
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR).useDisMax(false).cutoffFrequency(cutoffFrequency).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).useDisMax(false).cutoffFrequency(cutoffFrequency).type(type))).get();
         assertFirstHit(searchResponse, anyOf(hasId("theone"), hasId("theother")));
         assertThat(searchResponse.getHits().hits()[0].getScore(), greaterThan(searchResponse.getHits().hits()[1].getScore()));
         long size = searchResponse.getHits().getTotalHits();
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR).useDisMax(false).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).useDisMax(false).type(type))).get();
         assertFirstHit(searchResponse, anyOf(hasId("theone"), hasId("theother")));
         assertThat("common terms expected to be a way smaller result set", size, lessThan(searchResponse.getHits().getTotalHits()));
 
         cutoffFrequency = randomBoolean() ? Math.min(1, numDocs * 1.f / between(10, 20)) : 1.f / between(10, 20);
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR).cutoffFrequency(cutoffFrequency).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).cutoffFrequency(cutoffFrequency).type(type))).get();
         assertFirstHit(searchResponse, hasId("theother"));
 
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.AND).cutoffFrequency(cutoffFrequency).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.AND).cutoffFrequency(cutoffFrequency).type(type))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.AND).cutoffFrequency(cutoffFrequency).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.AND).cutoffFrequency(cutoffFrequency).type(type))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero", "first_name", "last_name", "category")
-                        .operator(Operator.AND).cutoffFrequency(cutoffFrequency)
+                        .operator(MatchQueryBuilder.Operator.AND).cutoffFrequency(cutoffFrequency)
                         .analyzer("category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS))).get();
         assertHitCount(searchResponse, 1l);
@@ -331,7 +330,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
                 SearchResponse left = client().prepareSearch("test").setSize(numDocs)
                         .addSort(SortBuilders.scoreSort()).addSort(SortBuilders.fieldSort("_uid"))
                         .setQuery(randomizeType(multiMatchQueryBuilder
-                                .operator(Operator.OR).type(type))).get();
+                                .operator(MatchQueryBuilder.Operator.OR).type(type))).get();
 
                 SearchResponse right = client().prepareSearch("test").setSize(numDocs)
                         .addSort(SortBuilders.scoreSort()).addSort(SortBuilders.fieldSort("_uid"))
@@ -347,7 +346,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
             {
                 MatchQueryBuilder.Type type = randomBoolean() ? null : MatchQueryBuilder.Type.BOOLEAN;
                 String minShouldMatch = randomBoolean() ? null : "" + between(0, 1);
-                Operator op = randomBoolean() ? Operator.AND : Operator.OR;
+                MatchQueryBuilder.Operator op = randomBoolean() ? MatchQueryBuilder.Operator.AND : MatchQueryBuilder.Operator.OR;
                 MultiMatchQueryBuilder multiMatchQueryBuilder = randomBoolean() ? multiMatchQuery("captain america", "full_name", "first_name", "last_name", "category") :
                         multiMatchQuery("captain america", "*_name", randomBoolean() ? "category" : "categ*");
                 SearchResponse left = client().prepareSearch("test").setSize(numDocs)
@@ -368,7 +367,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
 
             {
                 String minShouldMatch = randomBoolean() ? null : "" + between(0, 1);
-                Operator op = randomBoolean() ? Operator.AND : Operator.OR;
+                MatchQueryBuilder.Operator op = randomBoolean() ? MatchQueryBuilder.Operator.AND : MatchQueryBuilder.Operator.OR;
                 SearchResponse left = client().prepareSearch("test").setSize(numDocs)
                         .addSort(SortBuilders.scoreSort()).addSort(SortBuilders.fieldSort("_uid"))
                         .setQuery(randomizeType(multiMatchQuery("capta", "full_name", "first_name", "last_name", "category")
@@ -386,7 +385,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
             }
             {
                 String minShouldMatch = randomBoolean() ? null : "" + between(0, 1);
-                Operator op = randomBoolean() ? Operator.AND : Operator.OR;
+                MatchQueryBuilder.Operator op = randomBoolean() ? MatchQueryBuilder.Operator.AND : MatchQueryBuilder.Operator.OR;
                 SearchResponse left;
                 if (randomBoolean()) {
                     left = client().prepareSearch("test").setSize(numDocs)
@@ -417,13 +416,13 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america", "full_name", "first_name", "last_name")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
-                        .operator(Operator.OR))).get();
+                        .operator(MatchQueryBuilder.Operator.OR))).get();
         assertFirstHit(searchResponse, hasId("theone"));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero captain america", "full_name", "first_name", "last_name", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
-                        .operator(Operator.OR))).get();
+                        .operator(MatchQueryBuilder.Operator.OR))).get();
         assertFirstHit(searchResponse, hasId("theone"));
         assertSecondHit(searchResponse, hasId("theother"));
         assertThat(searchResponse.getHits().hits()[0].getScore(), greaterThan(searchResponse.getHits().hits()[1].getScore()));
@@ -431,13 +430,13 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero", "full_name", "first_name", "last_name", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
-                        .operator(Operator.OR))).get();
+                        .operator(MatchQueryBuilder.Operator.OR))).get();
         assertFirstHit(searchResponse, hasId("theother"));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america", "full_name", "first_name", "last_name", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
-                        .operator(Operator.AND))).get();
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
 
@@ -445,7 +444,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
                 .setQuery(randomizeType(multiMatchQuery("captain america 15", "full_name", "first_name", "last_name", "category", "skill")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
                         .analyzer("category")
-                        .operator(Operator.AND))).get();
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
 
@@ -466,7 +465,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
                         .cutoffFrequency(0.1f)
                         .analyzer("category")
-                        .operator(Operator.OR))).get();
+                        .operator(MatchQueryBuilder.Operator.OR))).get();
         assertFirstHit(searchResponse, anyOf(hasId("theother"), hasId("theone")));
         long numResults = searchResponse.getHits().totalHits();
 
@@ -474,7 +473,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
                 .setQuery(randomizeType(multiMatchQuery("captain america marvel hero", "first_name", "last_name", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
                         .analyzer("category")
-                        .operator(Operator.OR))).get();
+                        .operator(MatchQueryBuilder.Operator.OR))).get();
         assertThat(numResults, lessThan(searchResponse.getHits().getTotalHits()));
         assertFirstHit(searchResponse, hasId("theone"));
 
@@ -484,28 +483,28 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
                 .setQuery(randomizeType(multiMatchQuery("captain america marvel hero", "first_name", "last_name", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
                         .analyzer("category")
-                        .operator(Operator.AND))).get();
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
         // counter example
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america marvel hero", "first_name", "last_name", "category")
                         .type(randomBoolean() ? MultiMatchQueryBuilder.Type.CROSS_FIELDS : null)
-                        .operator(Operator.AND))).get();
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertHitCount(searchResponse, 0l);
 
         // counter example
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america marvel hero", "first_name", "last_name", "category")
                         .type(randomBoolean() ? MultiMatchQueryBuilder.Type.CROSS_FIELDS : null)
-                        .operator(Operator.AND))).get();
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertHitCount(searchResponse, 0l);
 
         // test if boosts work
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("the ultimate", "full_name", "first_name", "last_name^2", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
-                        .operator(Operator.AND))).get();
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertFirstHit(searchResponse, hasId("ultimate1"));   // has ultimate in the last_name and that is boosted
         assertSecondHit(searchResponse, hasId("ultimate2"));
         assertThat(searchResponse.getHits().hits()[0].getScore(), greaterThan(searchResponse.getHits().hits()[1].getScore()));
@@ -515,7 +514,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("the ultimate", "full_name", "first_name", "last_name", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
-                        .operator(Operator.AND))).get();
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertFirstHit(searchResponse, hasId("ultimate2"));
         assertSecondHit(searchResponse, hasId("ultimate1"));
         assertThat(searchResponse.getHits().hits()[0].getScore(), greaterThan(searchResponse.getHits().hits()[1].getScore()));
diff --git a/core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java b/core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
index 4fbff91..f1ed9bb 100644
--- a/core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
+++ b/core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.search.query;
 
 import org.apache.lucene.util.English;
+import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
 import org.elasticsearch.action.index.IndexRequestBuilder;
@@ -31,8 +32,15 @@ import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.mapper.MapperParsingException;
-import org.elasticsearch.index.query.*;
+import org.elasticsearch.index.query.BoolQueryBuilder;
+import org.elasticsearch.index.query.CommonTermsQueryBuilder.Operator;
+import org.elasticsearch.index.query.MatchQueryBuilder;
 import org.elasticsearch.index.query.MatchQueryBuilder.Type;
+import org.elasticsearch.index.query.MultiMatchQueryBuilder;
+import org.elasticsearch.index.query.QueryBuilders;
+import org.elasticsearch.index.query.QueryStringQueryBuilder;
+import org.elasticsearch.index.query.TermQueryBuilder;
+import org.elasticsearch.index.query.WrapperQueryBuilder;
 import org.elasticsearch.rest.RestStatus;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.search.SearchHit;
@@ -56,8 +64,24 @@ import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.*;
 import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.scriptFunction;
 import static org.elasticsearch.test.VersionUtils.randomVersion;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
-import static org.hamcrest.Matchers.*;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFailures;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFirstHit;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHit;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHits;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSecondHit;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThirdHit;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasId;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasScore;
+import static org.hamcrest.Matchers.allOf;
+import static org.hamcrest.Matchers.closeTo;
+import static org.hamcrest.Matchers.containsString;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.greaterThan;
+import static org.hamcrest.Matchers.is;
 
 public class SearchQueryIT extends ESIntegTestCase {
 
@@ -327,18 +351,18 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertThirdHit(searchResponse, hasId("2"));
 
         // try the same with match query
-        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the quick brown").cutoffFrequency(3).operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the quick brown").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2l);
         assertFirstHit(searchResponse, hasId("1"));
         assertSecondHit(searchResponse, hasId("2"));
 
-        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the quick brown").cutoffFrequency(3).operator(Operator.OR)).get();
+        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the quick brown").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.OR)).get();
         assertHitCount(searchResponse, 3l);
         assertFirstHit(searchResponse, hasId("1"));
         assertSecondHit(searchResponse, hasId("2"));
         assertThirdHit(searchResponse, hasId("3"));
 
-        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the quick brown").cutoffFrequency(3).operator(Operator.AND).analyzer("stop")).get();
+        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the quick brown").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.AND).analyzer("stop")).get();
         assertHitCount(searchResponse, 3l);
         // stop drops "the" since its a stopword
         assertFirstHit(searchResponse, hasId("1"));
@@ -346,7 +370,7 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertThirdHit(searchResponse, hasId("2"));
 
         // try the same with multi match query
-        searchResponse = client().prepareSearch().setQuery(multiMatchQuery("the quick brown", "field1", "field2").cutoffFrequency(3).operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch().setQuery(multiMatchQuery("the quick brown", "field1", "field2").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 3l);
         assertFirstHit(searchResponse, hasId("3")); // better score due to different query stats
         assertSecondHit(searchResponse, hasId("1"));
@@ -419,18 +443,18 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertThirdHit(searchResponse, hasId("2"));
 
         // try the same with match query
-        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the fast brown").cutoffFrequency(3).operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the fast brown").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2l);
         assertFirstHit(searchResponse, hasId("1"));
         assertSecondHit(searchResponse, hasId("2"));
 
-        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the fast brown").cutoffFrequency(3).operator(Operator.OR)).get();
+        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the fast brown").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.OR)).get();
         assertHitCount(searchResponse, 3l);
         assertFirstHit(searchResponse, hasId("1"));
         assertSecondHit(searchResponse, hasId("2"));
         assertThirdHit(searchResponse, hasId("3"));
 
-        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the fast brown").cutoffFrequency(3).operator(Operator.AND).analyzer("stop")).get();
+        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the fast brown").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.AND).analyzer("stop")).get();
         assertHitCount(searchResponse, 3l);
         // stop drops "the" since its a stopword
         assertFirstHit(searchResponse, hasId("1"));
@@ -443,7 +467,7 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertSecondHit(searchResponse, hasId("2"));
 
         // try the same with multi match query
-        searchResponse = client().prepareSearch().setQuery(multiMatchQuery("the fast brown", "field1", "field2").cutoffFrequency(3).operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch().setQuery(multiMatchQuery("the fast brown", "field1", "field2").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 3l);
         assertFirstHit(searchResponse, hasId("3")); // better score due to different query stats
         assertSecondHit(searchResponse, hasId("1"));
@@ -914,7 +938,7 @@ public class SearchQueryIT extends ESIntegTestCase {
 
         client().admin().indices().prepareRefresh("test").get();
         builder = multiMatchQuery("value1", "field1", "field2")
-                .operator(Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together.
+                .operator(MatchQueryBuilder.Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together.
         searchResponse = client().prepareSearch()
                 .setQuery(builder)
                 .get();
@@ -923,14 +947,14 @@ public class SearchQueryIT extends ESIntegTestCase {
 
         refresh();
         builder = multiMatchQuery("value1", "field1", "field3^1.5")
-                .operator(Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together.
+                .operator(MatchQueryBuilder.Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together.
         searchResponse = client().prepareSearch().setQuery(builder).get();
         assertHitCount(searchResponse, 2l);
         assertSearchHits(searchResponse, "3", "1");
 
         client().admin().indices().prepareRefresh("test").get();
         builder = multiMatchQuery("value1").field("field1").field("field3", 1.5f)
-                .operator(Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together.
+                .operator(MatchQueryBuilder.Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together.
         searchResponse = client().prepareSearch().setQuery(builder).get();
         assertHitCount(searchResponse, 2l);
         assertSearchHits(searchResponse, "3", "1");
@@ -1573,9 +1597,10 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertHitCount(searchResponse, 1l);
 
         searchResponse = client().prepareSearch("test").setQuery(
-                spanNearQuery(3)
+                spanNearQuery()
                         .clause(spanTermQuery("description", "foo"))
-                        .clause(spanTermQuery("description", "other"))).get();
+                        .clause(spanTermQuery("description", "other"))
+                        .slop(3)).get();
         assertHitCount(searchResponse, 3l);
     }
 
@@ -1620,22 +1645,33 @@ public class SearchQueryIT extends ESIntegTestCase {
         refresh();
 
         SearchResponse searchResponse = client().prepareSearch("test")
-                .setQuery(spanNotQuery(spanNearQuery(1)
+                .setQuery(spanNotQuery().include(spanNearQuery()
                         .clause(QueryBuilders.spanTermQuery("description", "quick"))
-                        .clause(QueryBuilders.spanTermQuery("description", "fox")), spanTermQuery("description", "brown"))).get();
+                        .clause(QueryBuilders.spanTermQuery("description", "fox")).slop(1)).exclude(spanTermQuery("description", "brown"))).get();
         assertHitCount(searchResponse, 1l);
 
         searchResponse = client().prepareSearch("test")
-                .setQuery(spanNotQuery(spanNearQuery(1)
+                .setQuery(spanNotQuery().include(spanNearQuery()
                         .clause(QueryBuilders.spanTermQuery("description", "quick"))
-                        .clause(QueryBuilders.spanTermQuery("description", "fox")), spanTermQuery("description", "sleeping")).dist(5)).get();
+                        .clause(QueryBuilders.spanTermQuery("description", "fox")).slop(1)).exclude(spanTermQuery("description", "sleeping")).dist(5)).get();
         assertHitCount(searchResponse, 1l);
 
         searchResponse = client().prepareSearch("test")
-                .setQuery(spanNotQuery(spanNearQuery(1)
+                .setQuery(spanNotQuery().include(spanNearQuery()
                         .clause(QueryBuilders.spanTermQuery("description", "quick"))
-                        .clause(QueryBuilders.spanTermQuery("description", "fox")), spanTermQuery("description", "jumped")).pre(1).post(1)).get();
+                        .clause(QueryBuilders.spanTermQuery("description", "fox")).slop(1)).exclude(spanTermQuery("description", "jumped")).pre(1).post(1)).get();
         assertHitCount(searchResponse, 1l);
+
+        try {
+            client().prepareSearch("test")
+                    .setQuery(spanNotQuery().include(spanNearQuery()
+                            .clause(QueryBuilders.spanTermQuery("description", "quick"))
+                            .clause(QueryBuilders.spanTermQuery("description", "fox")).slop(1)).exclude(spanTermQuery("description", "jumped")).dist(2).pre(2)
+                    ).get();
+            fail("ElasticsearchIllegalArgumentException should have been caught");
+        } catch (ElasticsearchException e) {
+            assertThat("ElasticsearchIllegalArgumentException should have been caught", e.getDetailedMessage(), containsString("spanNot can either use [dist] or [pre] & [post] (or none)"));
+        }
     }
 
     @Test
@@ -1731,18 +1767,18 @@ public class SearchQueryIT extends ESIntegTestCase {
 
         client().prepareIndex("test", "test", "1").setSource("text", "quick brown fox").get();
         refresh();
-        SearchResponse searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick").operator(Operator.AND)).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
-        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick brown").operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick brown").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
-        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "fast").operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "fast").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
 
         client().prepareIndex("test", "test", "2").setSource("text", "fast brown fox").get();
         refresh();
-        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick").operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2);
-        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick brown").operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick brown").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2);
     }
 
@@ -1762,12 +1798,12 @@ public class SearchQueryIT extends ESIntegTestCase {
 
         client().prepareIndex("test", "test", "1").setSource("text", "the fox runs across the street").get();
         refresh();
-        SearchResponse searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "fox runs").operator(Operator.AND)).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "fox runs").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
 
         client().prepareIndex("test", "test", "2").setSource("text", "run fox run").get();
         refresh();
-        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "fox runs").operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "fox runs").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2);
     }
 
@@ -1788,19 +1824,19 @@ public class SearchQueryIT extends ESIntegTestCase {
         client().prepareIndex("test", "test", "1").setSource("text", "quick brown fox").get();
         refresh();
 
-        SearchResponse searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick").defaultField("text").defaultOperator(Operator.AND)).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick").defaultField("text").defaultOperator(QueryStringQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
-        searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick brown").defaultField("text").defaultOperator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick brown").defaultField("text").defaultOperator(QueryStringQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
-        searchResponse = client().prepareSearch().setQuery(queryStringQuery("fast").defaultField("text").defaultOperator(Operator.AND)).get();
+        searchResponse = client().prepareSearch().setQuery(queryStringQuery("fast").defaultField("text").defaultOperator(QueryStringQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
 
         client().prepareIndex("test", "test", "2").setSource("text", "fast brown fox").get();
         refresh();
 
-        searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick").defaultField("text").defaultOperator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick").defaultField("text").defaultOperator(QueryStringQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2);
-        searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick brown").defaultField("text").defaultOperator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick brown").defaultField("text").defaultOperator(QueryStringQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2);
     }
 
@@ -1826,7 +1862,7 @@ public class SearchQueryIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("test")
                 .setQuery(
-                        queryStringQuery("foo.baz").useDisMax(false).defaultOperator(Operator.AND)
+                        queryStringQuery("foo.baz").useDisMax(false).defaultOperator(QueryStringQueryBuilder.Operator.AND)
                                 .field("field1").field("field2")).get();
         assertHitCount(response, 1l);
     }
diff --git a/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java b/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java
index 2225a91..f660dec 100644
--- a/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java
+++ b/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java
@@ -23,7 +23,6 @@ import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.BoolQueryBuilder;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.SimpleQueryStringBuilder;
 import org.elasticsearch.index.query.SimpleQueryStringFlag;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -71,7 +70,7 @@ public class SimpleQueryStringIT extends ESIntegTestCase {
         assertFirstHit(searchResponse, hasId("3"));
 
         searchResponse = client().prepareSearch().setQuery(
-                simpleQueryStringQuery("foo bar").defaultOperator(Operator.AND)).get();
+                simpleQueryStringQuery("foo bar").defaultOperator(SimpleQueryStringBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("3"));
 
@@ -257,21 +256,21 @@ public class SimpleQueryStringIT extends ESIntegTestCase {
 
         searchResponse = client().prepareSearch().setQuery(
                 simpleQueryStringQuery("foo | bar")
-                        .defaultOperator(Operator.AND)
+                        .defaultOperator(SimpleQueryStringBuilder.Operator.AND)
                         .flags(SimpleQueryStringFlag.OR)).get();
         assertHitCount(searchResponse, 3l);
         assertSearchHits(searchResponse, "1", "2", "3");
 
         searchResponse = client().prepareSearch().setQuery(
                 simpleQueryStringQuery("foo | bar")
-                        .defaultOperator(Operator.AND)
+                        .defaultOperator(SimpleQueryStringBuilder.Operator.AND)
                         .flags(SimpleQueryStringFlag.NONE)).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("3"));
 
         searchResponse = client().prepareSearch().setQuery(
                 simpleQueryStringQuery("baz | egg*")
-                        .defaultOperator(Operator.AND)
+                        .defaultOperator(SimpleQueryStringBuilder.Operator.AND)
                         .flags(SimpleQueryStringFlag.NONE)).get();
         assertHitCount(searchResponse, 0l);
 
@@ -288,7 +287,7 @@ public class SimpleQueryStringIT extends ESIntegTestCase {
 
         searchResponse = client().prepareSearch().setQuery(
                 simpleQueryStringQuery("baz | egg*")
-                        .defaultOperator(Operator.AND)
+                        .defaultOperator(SimpleQueryStringBuilder.Operator.AND)
                         .flags(SimpleQueryStringFlag.WHITESPACE, SimpleQueryStringFlag.PREFIX)).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("4"));
diff --git a/core/src/test/java/org/elasticsearch/search/rescore/QueryRescorerIT.java b/core/src/test/java/org/elasticsearch/search/rescore/QueryRescorerIT.java
index 5b559da..6aa31ca 100644
--- a/core/src/test/java/org/elasticsearch/search/rescore/QueryRescorerIT.java
+++ b/core/src/test/java/org/elasticsearch/search/rescore/QueryRescorerIT.java
@@ -33,7 +33,6 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.MatchQueryBuilder;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders;
 import org.elasticsearch.script.Script;
@@ -117,7 +116,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         ensureYellow();
         refresh();
         SearchResponse searchResponse = client().prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(MatchQueryBuilder.Operator.OR))
                 .setRescorer(RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "quick brown").slop(2).boost(4.0f)).setRescoreQueryWeight(2))
                 .setRescoreWindow(5).execute().actionGet();
 
@@ -127,7 +126,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         assertThat(searchResponse.getHits().getHits()[2].getId(), equalTo("2"));
 
         searchResponse = client().prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(MatchQueryBuilder.Operator.OR))
                 .setRescorer(RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "the quick brown").slop(3)))
                 .setRescoreWindow(5).execute().actionGet();
 
@@ -137,7 +136,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         assertThirdHit(searchResponse, hasId("3"));
 
         searchResponse = client().prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(MatchQueryBuilder.Operator.OR))
                 .setRescorer(RescoreBuilder.queryRescorer((QueryBuilders.matchPhraseQuery("field1", "the quick brown"))))
                 .setRescoreWindow(5).execute().actionGet();
 
@@ -180,7 +179,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         client().admin().indices().prepareRefresh("test").execute().actionGet();
         SearchResponse searchResponse = client()
                 .prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "lexington avenue massachusetts").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "lexington avenue massachusetts").operator(MatchQueryBuilder.Operator.OR))
                 .setFrom(0)
                 .setSize(5)
                 .setRescorer(
@@ -195,7 +194,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
 
         searchResponse = client()
                 .prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "lexington avenue massachusetts").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "lexington avenue massachusetts").operator(MatchQueryBuilder.Operator.OR))
                 .setFrom(0)
                 .setSize(5)
                 .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
@@ -212,7 +211,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         // Make sure non-zero from works:
         searchResponse = client()
                 .prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "lexington avenue massachusetts").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "lexington avenue massachusetts").operator(MatchQueryBuilder.Operator.OR))
                 .setFrom(2)
                 .setSize(5)
                 .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
@@ -321,7 +320,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
 
         SearchResponse searchResponse = client()
                 .prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "massachusetts").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "massachusetts").operator(MatchQueryBuilder.Operator.OR))
                 .setFrom(0)
             .setSize(5).execute().actionGet();
         assertThat(searchResponse.getHits().hits().length, equalTo(4));
@@ -334,7 +333,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         // Now, penalizing rescore (nothing matches the rescore query):
         searchResponse = client()
                 .prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "massachusetts").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "massachusetts").operator(MatchQueryBuilder.Operator.OR))
                 .setFrom(0)
                 .setSize(5)
                 .setRescorer(
@@ -426,7 +425,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                     .prepareSearch()
                     .setSearchType(SearchType.QUERY_THEN_FETCH)
                     .setPreference("test") // ensure we hit the same shards for tie-breaking
-                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(Operator.OR))
+                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(MatchQueryBuilder.Operator.OR))
                     .setFrom(0)
                     .setSize(resultSize)
                     .setRescorer(
@@ -441,7 +440,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
             SearchResponse plain = client().prepareSearch()
                     .setSearchType(SearchType.QUERY_THEN_FETCH)
                     .setPreference("test") // ensure we hit the same shards for tie-breaking
-                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(Operator.OR)).setFrom(0).setSize(resultSize)
+                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(MatchQueryBuilder.Operator.OR)).setFrom(0).setSize(resultSize)
                     .execute().actionGet();
             
             // check equivalence
@@ -451,7 +450,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                     .prepareSearch()
                     .setSearchType(SearchType.QUERY_THEN_FETCH)
                     .setPreference("test") // ensure we hit the same shards for tie-breaking
-                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(Operator.OR))
+                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(MatchQueryBuilder.Operator.OR))
                     .setFrom(0)
                     .setSize(resultSize)
                     .setRescorer(
@@ -469,7 +468,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                     .prepareSearch()
                     .setSearchType(SearchType.QUERY_THEN_FETCH)
                     .setPreference("test") // ensure we hit the same shards for tie-breaking
-                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(Operator.OR))
+                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(MatchQueryBuilder.Operator.OR))
                     .setFrom(0)
                     .setSize(resultSize)
                     .setRescorer(
@@ -504,7 +503,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
             SearchResponse searchResponse = client()
                     .prepareSearch()
                     .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
-                    .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
+                    .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(MatchQueryBuilder.Operator.OR))
                     .setRescorer(
                             RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "the quick brown").slop(2).boost(4.0f))
                                     .setQueryWeight(0.5f).setRescoreQueryWeight(0.4f)).setRescoreWindow(5).setExplain(true).execute()
@@ -542,7 +541,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
             SearchResponse searchResponse = client()
                     .prepareSearch()
                     .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
-                    .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
+                    .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(MatchQueryBuilder.Operator.OR))
                     .setRescorer(innerRescoreQuery).setRescoreWindow(5).setExplain(true).execute()
                     .actionGet();
             assertHitCount(searchResponse, 3);
@@ -565,7 +564,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                 searchResponse = client()
                         .prepareSearch()
                         .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
-                        .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
+                        .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(MatchQueryBuilder.Operator.OR))
                         .addRescorer(innerRescoreQuery).setRescoreWindow(5)
                         .addRescorer(outerRescoreQuery).setRescoreWindow(10)
                         .setExplain(true).get();
diff --git a/core/src/test/java/org/elasticsearch/stresstest/rollingrestart/QuickRollingRestartStressTest.java b/core/src/test/java/org/elasticsearch/stresstest/rollingrestart/QuickRollingRestartStressTest.java
index be9281a..a2fd5d8 100644
--- a/core/src/test/java/org/elasticsearch/stresstest/rollingrestart/QuickRollingRestartStressTest.java
+++ b/core/src/test/java/org/elasticsearch/stresstest/rollingrestart/QuickRollingRestartStressTest.java
@@ -95,7 +95,7 @@ public class QuickRollingRestartStressTest {
                 ClusterState state = client.client().admin().cluster().prepareState().execute().actionGet().getState();
                 System.out.println(state.nodes().prettyPrint());
                 System.out.println(state.routingTable().prettyPrint());
-                System.out.println(state.routingNodes().prettyPrint());
+                System.out.println(state.getRoutingNodes().prettyPrint());
                 throw new ElasticsearchException("timed out waiting for green state");
             } else {
                 System.out.println("--> got green status");
diff --git a/core/src/test/java/org/elasticsearch/test/ESAllocationTestCase.java b/core/src/test/java/org/elasticsearch/test/ESAllocationTestCase.java
index 69c2db6..d2b73a9 100644
--- a/core/src/test/java/org/elasticsearch/test/ESAllocationTestCase.java
+++ b/core/src/test/java/org/elasticsearch/test/ESAllocationTestCase.java
@@ -20,7 +20,6 @@ package org.elasticsearch.test;
 
 import com.google.common.collect.ImmutableSet;
 import org.elasticsearch.Version;
-import org.elasticsearch.cluster.ClusterInfoService;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.EmptyClusterInfoService;
 import org.elasticsearch.cluster.node.DiscoveryNode;
@@ -118,7 +117,7 @@ public abstract class ESAllocationTestCase extends ESTestCase {
     }
 
     public static ClusterState startRandomInitializingShard(ClusterState clusterState, AllocationService strategy) {
-        List<ShardRouting> initializingShards = clusterState.routingNodes().shardsWithState(INITIALIZING);
+        List<ShardRouting> initializingShards = clusterState.getRoutingNodes().shardsWithState(INITIALIZING);
         if (initializingShards.isEmpty()) {
             return clusterState;
         }
diff --git a/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java b/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java
index b83d7a2..0784d2a 100644
--- a/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java
+++ b/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java
@@ -102,7 +102,6 @@ import org.elasticsearch.index.fielddata.FieldDataType;
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.MappedFieldType.Loading;
-import org.elasticsearch.index.mapper.internal.SizeFieldMapper;
 import org.elasticsearch.index.mapper.internal.TimestampFieldMapper;
 import org.elasticsearch.index.shard.MergePolicyConfig;
 import org.elasticsearch.index.translog.Translog;
@@ -357,11 +356,6 @@ public abstract class ESIntegTestCase extends ESTestCase {
                             .field("enabled", randomBoolean());
                     mappings.endObject();
                 }
-                if (randomBoolean()) {
-                    mappings.startObject(SizeFieldMapper.NAME)
-                            .field("enabled", randomBoolean())
-                            .endObject();
-                }
                 mappings.startArray("dynamic_templates")
                         .startObject()
                         .startObject("template-strings")
@@ -698,12 +692,15 @@ public abstract class ESIntegTestCase extends ESTestCase {
         if (numberOfReplicas >= 0) {
             builder.put(SETTING_NUMBER_OF_REPLICAS, numberOfReplicas).build();
         }
+        // norelease: disabled because custom data paths don't play well against
+        // an external test cluster: the security manager is not happy that random
+        // files are touched. See http://build-us-00.elastic.co/job/es_core_master_strong/4357/console
         // 30% of the time
-        if (randomInt(9) < 3) {
-            final Path dataPath = createTempDir();
-            logger.info("using custom data_path for index: [{}]", dataPath);
-            builder.put(IndexMetaData.SETTING_DATA_PATH, dataPath);
-        }
+        // if (randomInt(9) < 3) {
+        //     final Path dataPath = createTempDir();
+        //     logger.info("using custom data_path for index: [{}]", dataPath);
+        //    builder.put(IndexMetaData.SETTING_DATA_PATH, dataPath);
+        // }
         return builder.build();
     }
 
diff --git a/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java b/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
index f02c925..1229f9e 100644
--- a/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
+++ b/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
@@ -59,7 +59,7 @@ public class AssertingLocalTransport extends LocalTransport {
         ElasticsearchAssertions.assertVersionSerializable(VersionUtils.randomVersionBetween(random, minVersion, maxVersion), response);
         super.handleParsedResponse(response, handler);
     }
-
+    
     @Override
     public void sendRequest(final DiscoveryNode node, final long requestId, final String action, final TransportRequest request, TransportRequestOptions options) throws IOException, TransportException {
         ElasticsearchAssertions.assertVersionSerializable(VersionUtils.randomVersionBetween(random, minVersion, maxVersion), request);
diff --git a/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTests.java b/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTests.java
index 08af3fc..9041bca 100644
--- a/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTests.java
+++ b/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTests.java
@@ -52,7 +52,6 @@ public abstract class AbstractSimpleTransportTests extends ESTestCase {
 
     protected ThreadPool threadPool;
 
-    protected static final NamedWriteableRegistry namedWriteableRegistry = new NamedWriteableRegistry();
     protected static final Version version0 = Version.fromId(/*0*/99);
     protected DiscoveryNode nodeA;
     protected MockTransportService serviceA;
diff --git a/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java b/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java
index 0aac324..ff6a21b 100644
--- a/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java
+++ b/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java
@@ -122,7 +122,7 @@ public class ContextAndHeaderTransportIT extends ESIntegTestCase {
                 .setSource(jsonBuilder().startObject().field("username", "foo").endObject()).get();
         transportClient().admin().indices().prepareRefresh(queryIndex, lookupIndex).get();
 
-        TermsQueryBuilder termsLookupFilterBuilder = QueryBuilders.termsLookupQuery("username").lookupIndex(lookupIndex).lookupType("type").lookupId("1").lookupPath("followers");
+        TermsLookupQueryBuilder termsLookupFilterBuilder = QueryBuilders.termsLookupQuery("username").lookupIndex(lookupIndex).lookupType("type").lookupId("1").lookupPath("followers");
         BoolQueryBuilder queryBuilder = QueryBuilders.boolQuery().must(QueryBuilders.matchAllQuery()).must(termsLookupFilterBuilder);
 
         SearchResponse searchResponse = transportClient()
diff --git a/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportIT.java b/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportIT.java
index d440ae7..f53f3e0 100644
--- a/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportIT.java
+++ b/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportIT.java
@@ -93,15 +93,15 @@ public class NettyTransportIT extends ESIntegTestCase {
 
         @Override
         public ChannelPipelineFactory configureServerChannelPipelineFactory(String name, Settings groupSettings) {
-            return new ErrorPipelineFactory(this, name, groupSettings, namedWriteableRegistry);
+            return new ErrorPipelineFactory(this, name, groupSettings);
         }
 
         private static class ErrorPipelineFactory extends ServerChannelPipelineFactory {
 
             private final ESLogger logger;
 
-            public ErrorPipelineFactory(ExceptionThrowingNettyTransport exceptionThrowingNettyTransport, String name, Settings groupSettings, NamedWriteableRegistry namedWriteableRegistry) {
-                super(exceptionThrowingNettyTransport, name, groupSettings, namedWriteableRegistry);
+            public ErrorPipelineFactory(ExceptionThrowingNettyTransport exceptionThrowingNettyTransport, String name, Settings groupSettings) {
+                super(exceptionThrowingNettyTransport, name, groupSettings);
                 this.logger = exceptionThrowingNettyTransport.logger;
             }
 
diff --git a/core/src/test/java/org/elasticsearch/update/UpdateByNativeScriptIT.java b/core/src/test/java/org/elasticsearch/update/UpdateByNativeScriptIT.java
index 0c3e7de..d3284d8 100644
--- a/core/src/test/java/org/elasticsearch/update/UpdateByNativeScriptIT.java
+++ b/core/src/test/java/org/elasticsearch/update/UpdateByNativeScriptIT.java
@@ -74,6 +74,10 @@ public class UpdateByNativeScriptIT extends ESIntegTestCase {
         public ExecutableScript newScript(@Nullable Map<String, Object> params) {
             return new CustomScript(params);
         }
+        @Override
+        public boolean needsScores() {
+            return false;
+        }
     }
 
     static class CustomScript extends AbstractExecutableScript {
diff --git a/core/src/test/java/org/elasticsearch/validate/SimpleValidateQueryIT.java b/core/src/test/java/org/elasticsearch/validate/SimpleValidateQueryIT.java
index 6f70d36..be493cd 100644
--- a/core/src/test/java/org/elasticsearch/validate/SimpleValidateQueryIT.java
+++ b/core/src/test/java/org/elasticsearch/validate/SimpleValidateQueryIT.java
@@ -236,7 +236,7 @@ public class SimpleValidateQueryIT extends ESIntegTestCase {
                 containsString("(field:huge field:brown) +field:pidgin"), true);
         assertExplanation(QueryBuilders.commonTermsQuery("field", "the brown").analyzer("stop"),
                 containsString("field:brown"), true);
-
+        
         // match queries with cutoff frequency
         assertExplanation(QueryBuilders.matchQuery("field", "huge brown pidgin").cutoffFrequency(1),
                 containsString("(field:huge field:brown) +field:pidgin"), true);
@@ -276,7 +276,11 @@ public class SimpleValidateQueryIT extends ESIntegTestCase {
         assertThat(client().admin().indices().prepareValidateQuery("test").setSource(new BytesArray("{\"query\": {\"term\" : { \"user\" : \"kimchy\" }}, \"foo\": \"bar\"}")).get().isValid(), equalTo(false));
     }
 
-    private static void assertExplanation(QueryBuilder queryBuilder, Matcher<String> matcher, boolean withRewrite) {
+    private void assertExplanation(QueryBuilder queryBuilder, Matcher<String> matcher) {
+        assertExplanation(queryBuilder, matcher, false);
+    }
+
+    private void assertExplanation(QueryBuilder queryBuilder, Matcher<String> matcher, boolean withRewrite) {
         ValidateQueryResponse response = client().admin().indices().prepareValidateQuery("test")
                 .setTypes("type1")
                 .setQuery(queryBuilder)
diff --git a/dev-tools/src/main/resources/ant/integration-tests.xml b/dev-tools/src/main/resources/ant/integration-tests.xml
index 42e7cd9..4504dac 100644
--- a/dev-tools/src/main/resources/ant/integration-tests.xml
+++ b/dev-tools/src/main/resources/ant/integration-tests.xml
@@ -110,6 +110,20 @@
     </sequential>
   </macrodef>
 
+  <!-- waits for elasticsearch to start -->
+  <macrodef name="waitfor-elasticsearch">
+      <attribute name="port"/>
+      <attribute name="timeoutproperty"/>
+    <sequential>
+      <echo>Waiting for elasticsearch to become available on port @{port}...</echo>
+      <waitfor maxwait="30" maxwaitunit="second"
+               checkevery="500" checkeveryunit="millisecond"
+               timeoutproperty="@{timeoutproperty}">
+        <http url="http://127.0.0.1:@{port}"/>
+      </waitfor>
+    </sequential>
+  </macrodef>
+
   <!-- start elasticsearch and wait until its ready -->
   <macrodef name="startup-elasticsearch">
       <attribute name="home" default="${integ.scratch}/elasticsearch-${elasticsearch.version}"/>
@@ -120,7 +134,7 @@
       <attribute name="es.transport.tcp.port" default="${integ.transport.port}"/>
       <attribute name="es.pidfile" default="${integ.pidfile}"/>
       <attribute name="additional.args" default=""/>
-      <element name="nested.args" optional="true"/>
+      <attribute name="jvm.args" default="${tests.jvm.argline}"/>
     <sequential>
 
       <!-- build args to pass to es -->
@@ -140,20 +154,23 @@
 
       <!-- run bin/elasticsearch with args -->
       <echo>Starting up external cluster...</echo>
+      <echo>JAVA=${java.home}</echo>
+      <echo>ARGS=@{jvm.args}</echo>
+
       <run-script script="@{home}/bin/elasticsearch" 
                   spawn="@{spawn}"
                   args="${integ.args}">
         <nested>
-          <nested.args/>
+          <env key="JAVA_HOME" value="${java.home}"/>
+          <!-- we pass these as gc options, even if they arent, to avoid conflicting gc options -->
+          <env key="ES_GC_OPTS" value="@{jvm.args}"/>
         </nested>
       </run-script>
 
+      <!-- wait for startup -->
       <local name="failed.to.start"/>
-      <waitfor maxwait="30" maxwaitunit="second"
-               checkevery="500" checkeveryunit="millisecond"
-               timeoutproperty="failed.to.start">
-        <http url="http://127.0.0.1:@{es.http.port}"/>
-      </waitfor>
+      <waitfor-elasticsearch port="@{es.http.port}"
+                             timeoutproperty="failed.to.start"/>
 
       <!-- best effort, print console log. useful if it fails especially -->
       <local name="log.contents"/>
@@ -185,11 +202,8 @@
       <unzip src="${project.build.directory}/releases/${project.artifactId}-${project.version}.zip" dest="${integ.scratch}"/>
       <local name="home"/>
       <property name="home" location="${integ.scratch}/${project.artifactId}-${elasticsearch.version}"/>
-      <startup-elasticsearch spawn="false" home="${home}">
-        <nested.args>
-          <env key="JAVA_OPTS" value="-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=8000"/>
-        </nested.args>
-      </startup-elasticsearch>
+      <startup-elasticsearch spawn="false" home="${home}"
+                             jvm.args="${tests.jvm.argline} -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=8000"/>
   </target>
 
   <!-- unzip core release artifact, install plugin, then start ES -->
diff --git a/distribution/deb/pom.xml b/distribution/deb/pom.xml
index c944dcc..2567fa9 100644
--- a/distribution/deb/pom.xml
+++ b/distribution/deb/pom.xml
@@ -273,7 +273,9 @@
                         <configuration>
                             <skip>${skip.integ.tests}</skip>
                             <target>
-                                <ant antfile="${elasticsearch.integ.antfile}" target="start-external-cluster-deb"/>
+                                <ant antfile="${elasticsearch.integ.antfile}" target="start-external-cluster-deb">
+                                    <property name="tests.jvm.argline" value="${tests.jvm.argline}"/>
+                                </ant>
                             </target>
                         </configuration>
                     </execution>
diff --git a/distribution/pom.xml b/distribution/pom.xml
index 9a01416..80bbc98 100644
--- a/distribution/pom.xml
+++ b/distribution/pom.xml
@@ -111,7 +111,8 @@
                                         <available file="${basedir}/../licenses" type="dir"/>
                                     </condition>
                                     <echo taskName="license check">Running license check</echo>
-                                    <exec failonerror="${licenses.exists}" executable="perl"
+                                    <!-- don't run on windows, because everyone hates it -->
+                                    <exec failonerror="${licenses.exists}" executable="perl" osfamily="unix"
                                           dir="${elasticsearch.tools.directory}/license-check">
                                         <arg value="check_license_and_sha.pl"/>
                                         <arg value="--check"/>
diff --git a/distribution/rpm/pom.xml b/distribution/rpm/pom.xml
index e390c4a..f073561 100644
--- a/distribution/rpm/pom.xml
+++ b/distribution/rpm/pom.xml
@@ -327,7 +327,9 @@
                         <configuration>
                             <skip>${skip.integ.tests}</skip>
                             <target>
-                                <ant antfile="${elasticsearch.integ.antfile}" target="start-external-cluster-rpm"/>
+                                <ant antfile="${elasticsearch.integ.antfile}" target="start-external-cluster-rpm">
+                                    <property name="tests.jvm.argline" value="${tests.jvm.argline}"/>
+                                </ant>
                             </target>
                         </configuration>
                     </execution>
diff --git a/distribution/src/main/resources/bin/elasticsearch.in.bat b/distribution/src/main/resources/bin/elasticsearch.in.bat
index 6cdd131..68dc46d 100644
--- a/distribution/src/main/resources/bin/elasticsearch.in.bat
+++ b/distribution/src/main/resources/bin/elasticsearch.in.bat
@@ -50,14 +50,16 @@ if NOT "%ES_USE_IPV4%" == "" (
 set JAVA_OPTS=%JAVA_OPTS% -Djava.net.preferIPv4Stack=true
 )
 
-set JAVA_OPTS=%JAVA_OPTS% -XX:+UseParNewGC
-set JAVA_OPTS=%JAVA_OPTS% -XX:+UseConcMarkSweepGC
-
-set JAVA_OPTS=%JAVA_OPTS% -XX:CMSInitiatingOccupancyFraction=75
-set JAVA_OPTS=%JAVA_OPTS% -XX:+UseCMSInitiatingOccupancyOnly
-
+REM Add gc options. ES_GC_OPTS is unsupported, for internal testing
+if "%ES_GC_OPTS%" == "" (
+set ES_GC_OPTS=%ES_GC_OPTS% -XX:+UseParNewGC
+set ES_GC_OPTS=%ES_GC_OPTS% -XX:+UseConcMarkSweepGC
+set ES_GC_OPTS=%ES_GC_OPTS% -XX:CMSInitiatingOccupancyFraction=75
+set ES_GC_OPTS=%ES_GC_OPTS% -XX:+UseCMSInitiatingOccupancyOnly
 REM When running under Java 7
 REM JAVA_OPTS=%JAVA_OPTS% -XX:+UseCondCardMark
+)
+set JAVA_OPTS=%JAVA_OPTS% %ES_GC_OPTS%
 
 if "%ES_GC_LOG_FILE%" == "" goto nogclog
 
diff --git a/distribution/src/main/resources/bin/elasticsearch.in.sh b/distribution/src/main/resources/bin/elasticsearch.in.sh
index 46ab4b6..bb43061 100644
--- a/distribution/src/main/resources/bin/elasticsearch.in.sh
+++ b/distribution/src/main/resources/bin/elasticsearch.in.sh
@@ -44,11 +44,15 @@ if [ "x$ES_USE_IPV4" != "x" ]; then
   JAVA_OPTS="$JAVA_OPTS -Djava.net.preferIPv4Stack=true"
 fi
 
-JAVA_OPTS="$JAVA_OPTS -XX:+UseParNewGC"
-JAVA_OPTS="$JAVA_OPTS -XX:+UseConcMarkSweepGC"
+# Add gc options. ES_GC_OPTS is unsupported, for internal testing
+if [ "x$ES_GC_OPTS" = "x" ]; then
+  ES_GC_OPTS="$ES_GC_OPTS -XX:+UseParNewGC"
+  ES_GC_OPTS="$ES_GC_OPTS -XX:+UseConcMarkSweepGC"
+  ES_GC_OPTS="$ES_GC_OPTS -XX:CMSInitiatingOccupancyFraction=75"
+  ES_GC_OPTS="$ES_GC_OPTS -XX:+UseCMSInitiatingOccupancyOnly"
+fi
 
-JAVA_OPTS="$JAVA_OPTS -XX:CMSInitiatingOccupancyFraction=75"
-JAVA_OPTS="$JAVA_OPTS -XX:+UseCMSInitiatingOccupancyOnly"
+JAVA_OPTS="$JAVA_OPTS $ES_GC_OPTS"
 
 # GC logging options
 if [ -n "$ES_GC_LOG_FILE" ]; then
diff --git a/distribution/tar/pom.xml b/distribution/tar/pom.xml
index 3c41c5d..3c1ec9e 100644
--- a/distribution/tar/pom.xml
+++ b/distribution/tar/pom.xml
@@ -67,7 +67,9 @@
                         <configuration>
                             <skip>${skip.integ.tests}</skip>
                             <target>
-                                <ant antfile="${elasticsearch.integ.antfile}" target="start-external-cluster-tar"/>
+                                <ant antfile="${elasticsearch.integ.antfile}" target="start-external-cluster-tar">
+                                    <property name="tests.jvm.argline" value="${tests.jvm.argline}"/>
+                                </ant>
                             </target>
                         </configuration>
                     </execution>
diff --git a/distribution/zip/pom.xml b/distribution/zip/pom.xml
index afa3dc2..0546fb9 100644
--- a/distribution/zip/pom.xml
+++ b/distribution/zip/pom.xml
@@ -84,7 +84,9 @@
                         <configuration>
                             <skip>${skip.integ.tests}</skip>
                             <target>
-                                <ant antfile="${elasticsearch.integ.antfile}" target="start-external-cluster-zip"/>
+                                <ant antfile="${elasticsearch.integ.antfile}" target="start-external-cluster-zip">
+                                    <property name="tests.jvm.argline" value="${tests.jvm.argline}"/>
+                                </ant>
                             </target>
                         </configuration>
                     </execution>
diff --git a/docs/reference/aggregations/bucket/geodistance-aggregation.asciidoc b/docs/reference/aggregations/bucket/geodistance-aggregation.asciidoc
index 2120c0b..4890f9d 100644
--- a/docs/reference/aggregations/bucket/geodistance-aggregation.asciidoc
+++ b/docs/reference/aggregations/bucket/geodistance-aggregation.asciidoc
@@ -53,7 +53,7 @@ Response:
 }
 --------------------------------------------------
 
-The specified field must be of type `geo_point` (which can only be set explicitly in the mappings). And it can also hold an array of `geo_point` fields, in which case all will be taken into account during aggregation. The origin point can accept all formats supported by the `geo_point` <<mapping-geo-point-type,type>>:
+The specified field must be of type `geo_point` (which can only be set explicitly in the mappings). And it can also hold an array of `geo_point` fields, in which case all will be taken into account during aggregation. The origin point can accept all formats supported by the <<geo-point,`geo_point` type>>:
 
 * Object format: `{ "lat" : 52.3760, "lon" : 4.894 }` - this is the safest format as it is the most explicit about the `lat` & `lon` values
 * String format: `"52.3760, 4.894"` - where the first number is the `lat` and the second is the `lon`
diff --git a/docs/reference/aggregations/metrics/tophits-aggregation.asciidoc b/docs/reference/aggregations/metrics/tophits-aggregation.asciidoc
index b6e9c2c..30f403d 100644
--- a/docs/reference/aggregations/metrics/tophits-aggregation.asciidoc
+++ b/docs/reference/aggregations/metrics/tophits-aggregation.asciidoc
@@ -200,7 +200,7 @@ and therefore can't be used in the `order` option of the `terms` aggregator.
 If the `top_hits` aggregator is wrapped in a `nested` or `reverse_nested` aggregator then nested hits are being returned.
 Nested hits are in a sense hidden mini documents that are part of regular document where in the mapping a nested field type
 has been configured. The `top_hits` aggregator has the ability to un-hide these documents if it is wrapped in a `nested`
-or `reverse_nested` aggregator. Read more about nested in the <<mapping-nested-type,nested type mapping>>.
+or `reverse_nested` aggregator. Read more about nested in the <<nested,nested type mapping>>.
 
 If nested type has been configured a single document is actually indexed as multiple Lucene documents and they share
 the same id. In order to determine the identity of a nested hit there is more needed than just the id, so that is why
diff --git a/docs/reference/api-conventions.asciidoc b/docs/reference/api-conventions.asciidoc
index f7fbde4..98fbcae 100644
--- a/docs/reference/api-conventions.asciidoc
+++ b/docs/reference/api-conventions.asciidoc
@@ -152,6 +152,34 @@ being consumed by a monitoring tool, rather than intended for human
 consumption.  The default for the `human` flag is
 `false`.
 
+[[date-math]]
+[float]
+=== Date Math
+
+Most parameters which accept a formatted date value -- such as `gt` and `lt`
+in <<query-dsl-range-query,range queries>> `range` queries, or `from` and `to`
+in <<search-aggregations-bucket-daterange-aggregation,`daterange`
+aggregations>> -- understand date maths.
+
+The expression starts with an anchor date, which can either be `now`, or a
+date string ending with `||`. This anchor date can optionally be followed by
+one or more maths expressions:
+
+* `+1h` - add one hour
+* `-1d` - subtract one day
+* `/d`  - round down to the nearest day
+
+The supported <<time-units,time units>> are: `y` (year), `M` (month), `w` (week),
+`d` (day), `h` (hour), `m` (minute), and `s` (second).
+
+Some examples are:
+
+[horizontal]
+`now+1h`::              The current time plus one hour, with ms resolution.
+`now+1h+1m`::           The current time plus one hour plus one minute, with ms resolution.
+`now+1h/d`::            The current time plus one hour, rounded down to the nearest day.
+`2015-01-01||+1M/d`::   `2015-01-01` plus one month, rounded down to the nearest day.
+
 [float]
 === Response Filtering
 
@@ -237,10 +265,10 @@ curl 'localhost:9200/_segments?pretty&filter_path=indices.**.version'
 --------------------------------------------------
 
 Note that elasticsearch sometimes returns directly the raw value of a field,
-like the `_source` field. If you want to filter _source fields, you should
+like the `_source` field. If you want to filter `_source` fields, you should
 consider combining the already existing `_source` parameter (see
 <<get-source-filtering,Get API>> for more details) with the `filter_path`
- parameter like this:
+parameter like this:
 
 [source,sh]
 --------------------------------------------------
@@ -318,8 +346,9 @@ of supporting the native JSON number types.
 [float]
 === Time units
 
-Whenever durations need to be specified, eg for a `timeout` parameter, the duration
-can be specified as a whole number representing time in milliseconds, or as a time value like `2d` for 2 days.  The supported units are:
+Whenever durations need to be specified, eg for a `timeout` parameter, the
+duration must specify the unit, like `2d` for 2 days.  The supported units
+are:
 
 [horizontal]
 `y`::   Year
@@ -329,6 +358,7 @@ can be specified as a whole number representing time in milliseconds, or as a ti
 `h`::   Hour
 `m`::   Minute
 `s`::   Second
+`ms`::  Milli-second
 
 [[distance-units]]
 [float]
diff --git a/docs/reference/index-modules/mapper.asciidoc b/docs/reference/index-modules/mapper.asciidoc
index 4f82e2f..484aec1 100644
--- a/docs/reference/index-modules/mapper.asciidoc
+++ b/docs/reference/index-modules/mapper.asciidoc
@@ -6,53 +6,3 @@ added to an index either when creating it or by using the put mapping
 api. It also handles the dynamic mapping support for types that have no
 explicit mappings pre defined. For more information about mapping
 definitions, check out the <<mapping,mapping section>>.
-
-[float]
-=== Dynamic Mappings
-
-New types and new fields within types can be added dynamically just
-by indexing a document.  When Elasticsearch encounters a new type,
-it creates the type using the `_default_` mapping (see below).
-
-When it encounters a new field within a type, it autodetects the
-datatype that the field contains and adds it to the type mapping
-automatically.
-
-See <<mapping-dynamic-mapping>> for details of how to control and
-configure dynamic mapping.
-
-[float]
-=== Default Mapping
-
-When a new type is created (at <<indices-create-index,index creation>> time,
-using the  <<indices-put-mapping,`put-mapping` API>> or just by indexing a
-document into it), the type uses the `_default_` mapping as its basis.  Any
-mapping specified in the  <<indices-create-index,`create-index`>> or
-<<indices-put-mapping,`put-mapping`>> request override values set in the
-`_default_` mapping.
-
-The default mapping definition is a plain mapping definition that is
-embedded within Elasticsearch:
-
-[source,js]
---------------------------------------------------
-{
-    _default_ : {
-    }
-}
---------------------------------------------------
-
-Pretty short, isn't it? Basically, everything is `_default_`ed, including the
-dynamic nature of the root object mapping which allows new fields to be added
-automatically.
-
-The default mapping can be overridden by specifying the `_default_` type when
-creating a new index.
-
-[float]
-=== Mapper settings
-
-`index.mapper.dynamic` (_dynamic_)::
-
-    Dynamic creation of mappings for unmapped types can be completely
-    disabled by setting `index.mapper.dynamic` to `false`.
diff --git a/docs/reference/index-modules/similarity.asciidoc b/docs/reference/index-modules/similarity.asciidoc
index 0c95709..82399a8 100644
--- a/docs/reference/index-modules/similarity.asciidoc
+++ b/docs/reference/index-modules/similarity.asciidoc
@@ -6,8 +6,8 @@ are scored. Similarity is per field, meaning that via the mapping one
 can define a different similarity per field.
 
 Configuring a custom similarity is considered a expert feature and the
-builtin similarities are most likely sufficient as is described in the
-<<mapping-core-types,mapping section>>
+builtin similarities are most likely sufficient as is described in
+<<similarity>>.
 
 [float]
 [[configuration]]
@@ -41,7 +41,7 @@ Here we configure the DFRSimilarity so it can be referenced as
     "properties" : {
       "title" : { "type" : "string", "similarity" : "my_similarity" }
     }
-} 
+}
 --------------------------------------------------
 
 [float]
@@ -52,9 +52,9 @@ Here we configure the DFRSimilarity so it can be referenced as
 ==== Default similarity
 
 The default similarity that is based on the TF/IDF model. This
-similarity has the following option: 
+similarity has the following option:
 
-`discount_overlaps`:: 
+`discount_overlaps`::
     Determines whether overlap tokens (Tokens with
     0 position increment) are ignored when computing norm. By default this
     is true, meaning overlap tokens do not count when computing norms.
@@ -71,14 +71,14 @@ http://en.wikipedia.org/wiki/Okapi_BM25[Okapi_BM25] for more details.
 This similarity has the following options:
 
 [horizontal]
-`k1`:: 
+`k1`::
     Controls non-linear term frequency normalization
-    (saturation). 
+    (saturation).
 
-`b`:: 
-    Controls to what degree document length normalizes tf values. 
+`b`::
+    Controls to what degree document length normalizes tf values.
 
-`discount_overlaps`:: 
+`discount_overlaps`::
     Determines whether overlap tokens (Tokens with
     0 position increment) are ignored when computing norm. By default this
     is true, meaning overlap tokens do not count when computing norms.
@@ -90,17 +90,17 @@ Type name: `BM25`
 ==== DFR similarity
 
 Similarity that implements the
-http://lucene.apache.org/core/4_1_0/core/org/apache/lucene/search/similarities/DFRSimilarity.html[divergence
+http://lucene.apache.org/core/5_2_1/core/org/apache/lucene/search/similarities/DFRSimilarity.html[divergence
 from randomness] framework. This similarity has the following options:
 
 [horizontal]
-`basic_model`:: 
-    Possible values: `be`, `d`, `g`, `if`, `in`, `ine` and `p`. 
+`basic_model`::
+    Possible values: `be`, `d`, `g`, `if`, `in`, `ine` and `p`.
 
 `after_effect`::
-    Possible values: `no`, `b` and `l`. 
+    Possible values: `no`, `b` and `l`.
 
-`normalization`:: 
+`normalization`::
     Possible values: `no`, `h1`, `h2`, `h3` and `z`.
 
 All options but the first option need a normalization value.
@@ -111,12 +111,12 @@ Type name: `DFR`
 [[ib]]
 ==== IB similarity.
 
-http://lucene.apache.org/core/4_1_0/core/org/apache/lucene/search/similarities/IBSimilarity.html[Information
+http://lucene.apache.org/core/5_2_1/core/org/apache/lucene/search/similarities/IBSimilarity.html[Information
 based model] . This similarity has the following options:
 
 [horizontal]
-`distribution`::  Possible values: `ll` and `spl`. 
-`lambda`::        Possible values: `df` and `ttf`. 
+`distribution`::  Possible values: `ll` and `spl`.
+`lambda`::        Possible values: `df` and `ttf`.
 `normalization`:: Same as in `DFR` similarity.
 
 Type name: `IB`
@@ -125,7 +125,7 @@ Type name: `IB`
 [[lm_dirichlet]]
 ==== LM Dirichlet similarity.
 
-http://lucene.apache.org/core/4_7_1/core/org/apache/lucene/search/similarities/LMDirichletSimilarity.html[LM
+http://lucene.apache.org/core/5_2_1/core/org/apache/lucene/search/similarities/LMDirichletSimilarity.html[LM
 Dirichlet similarity] . This similarity has the following options:
 
 [horizontal]
@@ -137,7 +137,7 @@ Type name: `LMDirichlet`
 [[lm_jelinek_mercer]]
 ==== LM Jelinek Mercer similarity.
 
-http://lucene.apache.org/core/4_7_1/core/org/apache/lucene/search/similarities/LMJelinekMercerSimilarity.html[LM
+http://lucene.apache.org/core/5_2_1/core/org/apache/lucene/search/similarities/LMJelinekMercerSimilarity.html[LM
 Jelinek Mercer similarity] . This similarity has the following options:
 
 [horizontal]
diff --git a/docs/reference/mapping.asciidoc b/docs/reference/mapping.asciidoc
index ce514e5..407f436 100644
--- a/docs/reference/mapping.asciidoc
+++ b/docs/reference/mapping.asciidoc
@@ -3,76 +3,173 @@
 
 [partintro]
 --
-Mapping is the process of defining how a document should be mapped to
-the Search Engine, including its searchable characteristics such as
-which fields are searchable and if/how they are tokenized. In
-Elasticsearch, an index may store documents of different "mapping
-types". Elasticsearch allows one to associate multiple mapping
-definitions for each mapping type.
-
-Explicit mapping is defined on an index/type level. By default, there
-isn't a need to define an explicit mapping, since one is automatically
-created and registered when a new type or new field is introduced (with
-no performance overhead) and have sensible defaults. Only when the
-defaults need to be overridden must a mapping definition be provided.
+
+Mapping is the process of defining how a document, and the fields it contains,
+are stored and indexed.  For instance, use mappings to define:
+
+* which string fields should be treated as full text fields.
+* which fields contain numbers, dates, or geolocations.
+* whether the values of all fields in the document should be
+  indexed into the catch-all <<mapping-all-field,`_all`>> field.
+* the <<mapping-date-format,format>> of date values.
+* custom rules to control the mapping for
+  <<dynamic-mapping,dynamically added fields>>.
 
 [float]
-[[all-mapping-types]]
-=== Mapping Types
-
-Mapping types are a way to divide the documents in an index into logical
-groups. Think of it as tables in a database. Though there is separation
-between types, it's not a full separation (all end up as a document
-within the same Lucene index).
-
-Field names with the same name across types are highly recommended to
-have the same type and same mapping characteristics (analysis settings
-for example). There is an effort to allow to explicitly "choose" which
-field to use by using type prefix (`my_type.my_field`), but it's not
-complete, and there are places where it will never work (like
-aggregations on the field).
-
-In practice though, this restriction is almost never an issue. The field
-name usually ends up being a good indication to its "typeness" (e.g.
-"first_name" will always be a string). Note also, that this does not
-apply to the cross index case.
+[[mapping-type]]
+== Mapping Types
+
+Each index has one or more _mapping types_, which are used to divide the
+documents in an index into logical groups. User documents might be stored in a
+`user` type, and blog posts in a `blogpost` type.
+
+Each mapping type has:
+
+<<mapping-fields,Meta-fields>>::
+
+Meta-fields are used to customize how a document's metadata associated is
+treated. Examples of meta-fields include the document's
+<<mapping-index-field,`_index`>>, <<mapping-type-field,`_type`>>,
+<<mapping-id-field,`_id`>>,  and <<mapping-source-field,`_source`>> fields.
+
+<<mapping-types,Fields>> or _properties_::
+
+Each mapping type contains a list of fields or `properties` pertinent to that
+type.  A `user` type might contain `title`, `name`, and `age` fields, while a
+`blogpost` type might contain `title`, `body`, `user_id` and `created` fields.
+Fields with the same name in different mapping types in the same index
+<<field-conflicts,must have the same mapping>>.
+
 
 [float]
-[[mapping-api]]
-=== Mapping API
+== Field datatypes
 
-To create a mapping, you will need the <<indices-put-mapping,Put Mapping
-API>>, or you can add multiple mappings when you <<indices-create-index,create an
-index>>.
+Each field has a data `type` which can be:
+
+* a simple type like <<string,`string`>>, <<date,`date`>>, <<number,`long`>>,
+  <<number,`double`>>, <<boolean,`boolean`>> or <<ip,`ip`>>.
+* a type which supports the hierarchical nature of JSON such as
+  <<object,`object`>> or <<nested,`nested`>>.
+* or a specialised type like <<geo-point,`geo_point`>>,
+  <<geo-shape,`geo_shape`>>, or <<search-suggesters-completion,`completion`>>.
+
+It is often useful to index the same field in different ways for different
+purposes. For instance, a `string` field could be <<mapping-index,indexed>> as
+an `analyzed` field for full-text search, and as a `not_analyzed` field for
+sorting or aggregations.  Alternatively, you could index a string field with
+the <<analysis-standard-analyzer,`standard` analyzer>>, the
+<<english-analyzer,`english`>> analyzer, and the
+<<french-analyzer,`french` analyzer>>.
+
+This is the purpose of _multi-fields_.  Most datatypes support multi-fields
+via the <<multi-fields>> parameter.
 
 [float]
-[[mapping-settings]]
-=== Global Settings
-
-The `index.mapping.ignore_malformed` global setting can be set on the
-index level to allow to ignore malformed content globally across all
-mapping types (malformed content example is trying to index a text string
-value as a numeric type).
-
-The `index.mapping.coerce` global setting can be set on the
-index level to coerce numeric content globally across all
-mapping types (The default setting is true and coercions attempted are
-to convert strings with numbers into numeric types and also numeric values
-with fractions to any integer/short/long values minus the fraction part).
-When the permitted conversions fail in their attempts, the value is considered
-malformed and the ignore_malformed setting dictates what will happen next.
---
+== Dynamic mapping
 
-include::mapping/fields.asciidoc[]
+Fields and mapping types do not need to be defined before being used. Thanks
+to _dynamic mapping_, new mapping types and new field names will be added
+automatically, just by indexing a document. New fields can be added both to
+the top-level mapping type, and to inner <<object,`object`>>  and
+<<nested,`nested`>> fields.
 
-include::mapping/types.asciidoc[]
+The
+<<dynamic-mapping,dynamic mapping>> rules can be configured to
+customise the mapping that is used for new types and new fields.
+
+[float]
+== Explicit mappings
 
-include::mapping/date-format.asciidoc[]
+You know more about your data than Elasticsearch can guess, so while dynamic
+mapping can be useful to get started, at some point you will want to specify
+your own explicit mappings.
 
-include::mapping/fielddata_formats.asciidoc[]
+You can create mapping types and field mappings when you
+<<indices-create-index,create an index>>, and you can add mapping types and
+fields to an existing index with the <<indices-put-mapping,PUT mapping API>>.
+
+[float]
+== Updating existing mappings
+
+Other than where documented, *existing type and field mappings cannot be
+updated*. Changing the mapping would mean invalidating already indexed
+documents.  Instead, you should create a new index with the correct mappings
+and reindex your data into that index.
+
+[[field-conflicts]]
+[float]
+== Fields are shared across mapping types
+
+Mapping types are used to group fields, but the fields in each mapping type
+are not independent of each other. Fields with:
+
+* the _same name_
+* in the _same index_
+* in _different mapping types_
+* map to the _same field_ internally,
+* and *must have the same mapping*.
+
+If a `title` field exists in both the `user` and `blogpost` mapping types, the
+`title` fields must have exactly the same mapping in each type.  The only
+exceptions to this rule are the <<copy-to>>, <<dynamic>>, <<enabled>>,
+<<ignore-above>>, <<include-in-all>>, and <<properties>> parameters, which may
+have different settings per field.
+
+Usually, fields with the same name also contain the same type of data, so
+having the same mapping is not a problem.  When conflicts do arise, these can
+be solved by choosing more descriptive names, such as `user_title` and
+`blog_title`.
+
+[float]
+== Example mapping
+
+A mapping for the example described above could be specified when creating the
+index, as follows:
+
+[source,js]
+---------------------------------------
+PUT my_index <1>
+{
+  "mappings": {
+    "user": { <2>
+      "_all":       { "enabled": false  }, <3>
+      "properties": { <4>
+        "title":    { "type": "string"  }, <5>
+        "name":     { "type": "string"  }, <5>
+        "age":      { "type": "integer" }  <5>
+      }
+    },
+    "blogpost": { <2>
+      "properties": { <4>
+        "title":    { "type": "string"  }, <5>
+        "body":     { "type": "string"  }, <5>
+        "user_id":  {
+          "type":   "string", <5>
+          "index":  "not_analyzed"
+        },
+        "created":  {
+          "type":   "date", <5>
+          "format": "strict_date_optional_time||epoch_millis"
+        }
+      }
+    }
+  }
+}
+---------------------------------------
+// AUTOSENSE
+<1> Create an index called `my_index`.
+<2> Add mapping types called `user` and `blogpost`.
+<3> Disable the `_all` <<mapping-fields,meta field>> for the `user` mapping type.
+<4> Specify fields or _properties_ in each mapping type.
+<5> Specify the data `type` and mapping for each field.
 
-include::mapping/dynamic-mapping.asciidoc[]
 
-include::mapping/meta.asciidoc[]
+--
+
+include::mapping/types.asciidoc[]
 
-include::mapping/transform.asciidoc[]
+include::mapping/fields.asciidoc[]
+
+include::mapping/params.asciidoc[]
+
+include::mapping/dynamic-mapping.asciidoc[]
diff --git a/docs/reference/mapping/date-format.asciidoc b/docs/reference/mapping/date-format.asciidoc
deleted file mode 100644
index 7f12616..0000000
--- a/docs/reference/mapping/date-format.asciidoc
+++ /dev/null
@@ -1,238 +0,0 @@
-[[mapping-date-format]]
-== Date Format
-
-In JSON documents, dates are represented as strings. Elasticsearch uses a set
-of pre-configured format to recognize and convert those, but you can change the
-defaults by specifying the `format` option when defining a `date` type, or by
-specifying `dynamic_date_formats` in the `root object` mapping (which will
-be used unless explicitly overridden by a `date` type). There are built in
-formats supported, as well as complete custom one.
-
-The parsing of dates uses http://www.joda.org/joda-time/[Joda]. The
-default date parsing used if no format is specified is
-http://www.joda.org/joda-time/apidocs/org/joda/time/format/ISODateTimeFormat.html#dateOptionalTimeParser--[ISODateTimeFormat.dateOptionalTimeParser].
-
-An extension to the format allow to define several formats using `||`
-separator. This allows to define less strict formats that can be used,
-for example, the `yyyy/MM/dd HH:mm:ss||yyyy/MM/dd` format will parse
-both `yyyy/MM/dd HH:mm:ss` and `yyyy/MM/dd`. The first format will also
-act as the one that converts back from milliseconds to a string
-representation.
-
-[float]
-[[date-math]]
-=== Date Math
-
-The `date` type supports using date math expression when using it in a
-query/filter (mainly makes sense in `range` query/filter).
-
-The expression starts with an "anchor" date, which can be either `now`
-or a date string (in the applicable format) ending with `||`. It can
-then follow by a math expression, supporting `+`, `-` and `/`
-(rounding). The units supported are `y` (year), `M` (month), `w` (week),
-`d` (day), `h` (hour), `m` (minute), and `s` (second).
-
-Here are some samples: `now+1h`, `now+1h+1m`, `now+1h/d`,
-`2012-01-01||+1M/d`.
-
-When doing `range` type searches with rounding, the value parsed
-depends on whether the end of the range is inclusive or exclusive, and
-whether the beginning or end of the range.  Rounding up moves to the
-last millisecond of the rounding scope, and rounding down to the
-first millisecond of the rounding scope. The semantics work as follows:
-* `gt` - round up, and use > that value (`2014-11-18||/M` becomes `2014-11-30T23:59:59.999`, ie excluding the entire month)
-* `gte` - round D down, and use >= that value (`2014-11-18||/M` becomes `2014-11-01`, ie including the entire month)
-* `lt` - round D down, and use < that value (`2014-11-18||/M` becomes `2014-11-01`, ie excluding the entire month)
-* `lte` - round D up, and use <= that value(`2014-11-18||/M` becomes `2014-11-30T23:59:59.999`, ie including the entire month)
-
-[float]
-[[built-in]]
-=== Built In Formats
-
-Most of the below dates have a `strict` companion dates, which means, that
-year, month and day parts of the week must have prepending zeros in order
-to be valid. This means, that a date like `5/11/1` would not be valid, but
-you would need to specify the full date, which would be `2005/11/01` in this
-example. So instead of `date_optional_time` you would need to specify
-`strict_date_optional_time`.
-
-The following tables lists all the defaults ISO formats supported:
-
-[cols="<,<",options="header",]
-|=======================================================================
-|Name |Description
-|`basic_date`|A basic formatter for a full date as four digit year, two
-digit month of year, and two digit day of month (yyyyMMdd).
-
-|`basic_date_time`|A basic formatter that combines a basic date and time,
-separated by a 'T' (yyyyMMdd'T'HHmmss.SSSZ).
-
-|`basic_date_time_no_millis`|A basic formatter that combines a basic date
-and time without millis, separated by a 'T' (yyyyMMdd'T'HHmmssZ).
-
-|`basic_ordinal_date`|A formatter for a full ordinal date, using a four
-digit year and three digit dayOfYear (yyyyDDD).
-
-|`basic_ordinal_date_time`|A formatter for a full ordinal date and time,
-using a four digit year and three digit dayOfYear
-(yyyyDDD'T'HHmmss.SSSZ).
-
-|`basic_ordinal_date_time_no_millis`|A formatter for a full ordinal date
-and time without millis, using a four digit year and three digit
-dayOfYear (yyyyDDD'T'HHmmssZ).
-
-|`basic_time`|A basic formatter for a two digit hour of day, two digit
-minute of hour, two digit second of minute, three digit millis, and time
-zone offset (HHmmss.SSSZ).
-
-|`basic_time_no_millis`|A basic formatter for a two digit hour of day,
-two digit minute of hour, two digit second of minute, and time zone
-offset (HHmmssZ).
-
-|`basic_t_time`|A basic formatter for a two digit hour of day, two digit
-minute of hour, two digit second of minute, three digit millis, and time
-zone off set prefixed by 'T' ('T'HHmmss.SSSZ).
-
-|`basic_t_time_no_millis`|A basic formatter for a two digit hour of day,
-two digit minute of hour, two digit second of minute, and time zone
-offset prefixed by 'T' ('T'HHmmssZ).
-
-|`basic_week_date`|A basic formatter for a full date as four digit
-weekyear, two digit week of weekyear, and one digit day of week
-(xxxx'W'wwe). `strict_basic_week_date` is supported.
-
-|`basic_week_date_time`|A basic formatter that combines a basic weekyear
-date and time, separated by a 'T' (xxxx'W'wwe'T'HHmmss.SSSZ).
-`strict_basic_week_date_time` is supported.
-
-|`basic_week_date_time_no_millis`|A basic formatter that combines a basic
-weekyear date and time without millis, separated by a 'T'
-(xxxx'W'wwe'T'HHmmssZ). `strict_week_date_time` is supported.
-
-|`date`|A formatter for a full date as four digit year, two digit month
-of year, and two digit day of month (yyyy-MM-dd). `strict_date` is supported.
-_
-|`date_hour`|A formatter that combines a full date and two digit hour of
-day. strict_date_hour` is supported.
-
-
-|`date_hour_minute`|A formatter that combines a full date, two digit hour
-of day, and two digit minute of hour. strict_date_hour_minute` is supported.
-
-|`date_hour_minute_second`|A formatter that combines a full date, two
-digit hour of day, two digit minute of hour, and two digit second of
-minute. `strict_date_hour_minute_second` is supported.
-
-|`date_hour_minute_second_fraction`|A formatter that combines a full
-date, two digit hour of day, two digit minute of hour, two digit second
-of minute, and three digit fraction of second
-(yyyy-MM-dd'T'HH:mm:ss.SSS). `strict_date_hour_minute_second_fraction` is supported.
-
-|`date_hour_minute_second_millis`|A formatter that combines a full date,
-two digit hour of day, two digit minute of hour, two digit second of
-minute, and three digit fraction of second (yyyy-MM-dd'T'HH:mm:ss.SSS).
-`strict_date_hour_minute_second_millis` is supported.
-
-|`date_optional_time`|a generic ISO datetime parser where the date is
-mandatory and the time is optional. `strict_date_optional_time` is supported.
-
-|`date_time`|A formatter that combines a full date and time, separated by
-a 'T' (yyyy-MM-dd'T'HH:mm:ss.SSSZZ). `strict_date_time` is supported.
-
-|`date_time_no_millis`|A formatter that combines a full date and time
-without millis, separated by a 'T' (yyyy-MM-dd'T'HH:mm:ssZZ).
-`strict_date_time_no_millis` is supported.
-
-|`hour`|A formatter for a two digit hour of day. `strict_hour` is supported.
-
-|`hour_minute`|A formatter for a two digit hour of day and two digit
-minute of hour. `strict_hour_minute` is supported.
-
-|`hour_minute_second`|A formatter for a two digit hour of day, two digit
-minute of hour, and two digit second of minute.
-`strict_hour_minute_second` is supported.
-
-|`hour_minute_second_fraction`|A formatter for a two digit hour of day,
-two digit minute of hour, two digit second of minute, and three digit
-fraction of second (HH:mm:ss.SSS).
-`strict_hour_minute_second_fraction` is supported.
-
-|`hour_minute_second_millis`|A formatter for a two digit hour of day, two
-digit minute of hour, two digit second of minute, and three digit
-fraction of second (HH:mm:ss.SSS).
-`strict_hour_minute_second_millis` is supported.
-
-|`ordinal_date`|A formatter for a full ordinal date, using a four digit
-year and three digit dayOfYear (yyyy-DDD). `strict_ordinal_date` is supported.
-
-|`ordinal_date_time`|A formatter for a full ordinal date and time, using
-a four digit year and three digit dayOfYear (yyyy-DDD'T'HH:mm:ss.SSSZZ).
-`strict_ordinal_date_time` is supported.
-
-|`ordinal_date_time_no_millis`|A formatter for a full ordinal date and
-time without millis, using a four digit year and three digit dayOfYear
-(yyyy-DDD'T'HH:mm:ssZZ).
-`strict_ordinal_date_time_no_millis` is supported.
-
-|`time`|A formatter for a two digit hour of day, two digit minute of
-hour, two digit second of minute, three digit fraction of second, and
-time zone offset (HH:mm:ss.SSSZZ). `strict_time` is supported.
-
-|`time_no_millis`|A formatter for a two digit hour of day, two digit
-minute of hour, two digit second of minute, and time zone offset
-(HH:mm:ssZZ). `strict_time_no_millis` is supported.
-
-|`t_time`|A formatter for a two digit hour of day, two digit minute of
-hour, two digit second of minute, three digit fraction of second, and
-time zone offset prefixed by 'T' ('T'HH:mm:ss.SSSZZ).
-`strict_t_time` is supported.
-
-|`t_time_no_millis`|A formatter for a two digit hour of day, two digit
-minute of hour, two digit second of minute, and time zone offset
-prefixed by 'T' ('T'HH:mm:ssZZ). `strict_t_time_no_millis` is supported.
-
-|`week_date`|A formatter for a full date as four digit weekyear, two
-digit week of weekyear, and one digit day of week (xxxx-'W'ww-e).
-`strict_week_date` is supported.
-
-|`week_date_time`|A formatter that combines a full weekyear date and
-time, separated by a 'T' (xxxx-'W'ww-e'T'HH:mm:ss.SSSZZ).
-`strict_week_date_time` is supported.
-
-|`week_date_time_no_millis`|A formatter that combines a full weekyear date
-and time without millis, separated by a 'T' (xxxx-'W'ww-e'T'HH:mm:ssZZ).
-`strict_week_date_time` is supported.
-
-|`weekyear`|A formatter for a four digit weekyear. `strict_week_year` is supported.
-
-|`weekyear_week`|A formatter for a four digit weekyear and two digit week
-of weekyear. `strict_weekyear_week` is supported.
-
-|`weekyear_week_day`|A formatter for a four digit weekyear, two digit week
-of weekyear, and one digit day of week. `strict_weekyear_week_day` is supported.
-
-|`year`|A formatter for a four digit year. `strict_year` is supported.
-
-|`year_month`|A formatter for a four digit year and two digit month of
-year. `strict_year_month` is supported.
-
-|`year_month_day`|A formatter for a four digit year, two digit month of
-year, and two digit day of month. `strict_year_month_day` is supported.
-
-|`epoch_second`|A formatter for the number of seconds since the epoch.
-Note, that this timestamp allows a max length of 10 chars, so dates
-older than 1653 and 2286 are not supported. You should use a different
-date formatter in that case.
-
-|`epoch_millis`|A formatter for the number of milliseconds since the epoch.
-Note, that this timestamp allows a max length of 13 chars, so dates
-older than 1653 and 2286 are not supported. You should use a different
-date formatter in that case.
-|=======================================================================
-
-[float]
-[[custom]]
-=== Custom Format
-
-Allows for a completely customizable date format explained
-http://www.joda.org/joda-time/apidocs/org/joda/time/format/DateTimeFormat.html[here].
diff --git a/docs/reference/mapping/dynamic-mapping.asciidoc b/docs/reference/mapping/dynamic-mapping.asciidoc
index 91ecd6b..0f445ac 100644
--- a/docs/reference/mapping/dynamic-mapping.asciidoc
+++ b/docs/reference/mapping/dynamic-mapping.asciidoc
@@ -1,73 +1,67 @@
-[[mapping-dynamic-mapping]]
+[[dynamic-mapping]]
 == Dynamic Mapping
 
-Default mappings allow generic mapping definitions to be automatically applied
-to types that do not have mappings predefined. This is mainly done
-thanks to the fact that the
-<<mapping-object-type,object mapping>> and
-namely the <<mapping-root-object-type,root
-object mapping>> allow for schema-less dynamic addition of unmapped
-fields.
-
-The default mapping definition is a plain mapping definition that is
-embedded within the distribution:
+One of the most important features of Elasticsearch is that it tries to get
+out of your way and let you start exploring your data as quickly as possible.
+To index a document, you don't have to first create an index, define a mapping
+type, and define your fields -- you can just index a document and the index,
+type, and fields will spring to life automatically:
 
 [source,js]
 --------------------------------------------------
-{
-    "_default_" : {
-    }
-}
+PUT data/counters/1 <1>
+{ "count": 5 }
 --------------------------------------------------
+// AUTOSENSE
+<1> Creates the `data` index, the `counters` mapping type, and a field
+    called `count` with datatype `long`.
 
-Pretty short, isn't it? Basically, everything is defaulted, especially the
-dynamic nature of the root object mapping. The default mapping can be
-overridden by specifying the `_default_` type when creating a new index.
+The automatic detection and addition of new types and fields is called
+_dynamic mapping_. The dynamic mapping rules can be customised to suit your
+purposes with:
 
-The dynamic creation of mappings for unmapped types can be completely
-disabled by setting `index.mapper.dynamic` to `false`.
+<<default-mapping,`_default_` mapping>>::
 
-The dynamic creation of fields within a type can be completely
-disabled by setting the `dynamic` property of the type to `strict`.
+    Configure the base mapping to be used for new mapping types.
 
-Here is a <<indices-put-mapping,Put Mapping>> example that
-disables dynamic field creation for a `tweet`:
+<<dynamic-field-mapping,Dynamic field mappings>>::
 
-[source,js]
---------------------------------------------------
-$ curl -XPUT 'http://localhost:9200/twitter/_mapping/tweet' -d '
-{
-    "tweet" : {
-        "dynamic": "strict",
-        "properties" : {
-            "message" : {"type" : "string", "store" : true }
-        }
-    }
-}
-'
---------------------------------------------------
+    The rules governing dynamic field detection.
+
+<<dynamic-templates,Dynamic templates>>::
 
-Here is how we can change the default
-<<mapping-date-format,date_formats>> used in the
-root and inner object types:
+    Custom rules to configure the mapping for dynamically added fields.
+
+TIP: <<indices-templates,Index templates>> allow you to configure the default
+mappings, settings, aliases, and warmers for new indices, whether created
+automatically or explicitly.
+
+
+[float]
+=== Disabling automatic type creation
+
+Automatic type creation can be disabled by setting the `index.mapper.dynamic`
+setting to `false`, either by setting the default value in the
+`config/elasticsearch.yml` file, or per-index as an index setting:
 
 [source,js]
 --------------------------------------------------
+PUT /_settings <1>
 {
-    "_default_" : {
-        "dynamic_date_formats" : ["yyyy-MM-dd", "dd-MM-yyyy", "date_optional_time"]
-    }
+  "index.mapper.dynamic":false
 }
 --------------------------------------------------
+// AUTOSENSE
+<1> Disable automatic type creation for all indices.
 
-[float]
-=== Unmapped fields in queries
+Regardless of the value of this setting, types can still be added explicitly
+when <<indices-create-index,creating an index>> or with the
+<<indices-put-mapping,PUT mapping>> API.
+
+
+include::dynamic/default-mapping.asciidoc[]
+
+include::dynamic/field-mapping.asciidoc[]
 
-Queries and filters can refer to fields that don't exist in a mapping. Whether this
-is allowed is controlled by the `index.query.parse.allow_unmapped_fields` setting.
-This setting defaults to `true`. Setting it to `false` will disallow the usage of
-unmapped fields in queries.
+include::dynamic/templates.asciidoc[]
 
-When registering a new <<search-percolate,percolator query>> or creating
-a <<filtered,filtered alias>> then the `index.query.parse.allow_unmapped_fields` setting
-is forcefully overwritten to disallowed unmapped fields.
\ No newline at end of file
diff --git a/docs/reference/mapping/dynamic/default-mapping.asciidoc b/docs/reference/mapping/dynamic/default-mapping.asciidoc
new file mode 100644
index 0000000..c1e1f8d
--- /dev/null
+++ b/docs/reference/mapping/dynamic/default-mapping.asciidoc
@@ -0,0 +1,82 @@
+[[default-mapping]]
+=== `_default_` mapping
+
+The default mapping, which will be used as the base mapping for any new
+mapping types, can be customised by adding a mapping type with the name
+`_default_` to an index, either when
+<<indices-create-index,creating the index>> or later on with the
+<<indices-put-mapping,PUT mapping>> API.
+
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "_default_": { <1>
+      "_all": {
+        "enabled": false
+      }
+    },
+    "user": {}, <2>
+    "blogpost": { <3>
+      "_all": {
+        "enabled": true
+      }
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> The `_default_` mapping defaults the <<mapping-all-field,`_all`>> field to disabled.
+<2> The `user` type inherits the settings from `_default_`.
+<3> The `blogpost` type overrides the defaults and enables the <<mapping-all-field,`_all`>> field.
+
+While the `_default_` mapping can be updated after an index has been created,
+the new defaults will only affect mapping types that are created afterwards.
+
+The `_default_` mapping can be used in conjunction with
+<<indices-templates,Index templates>> to control dynamically created types
+within automatically created indices:
+
+
+[source,js]
+--------------------------------------------------
+PUT _template/logging
+{
+  "template":   "logs-*", <1>
+  "settings": { "number_of_shards": 1 }, <2>
+  "mappings": {
+    "_default_": {
+      "_all": { <3>
+        "enabled": false
+      },
+      "dynamic_templates": [
+        {
+          "strings": { <4>
+            "match_mapping_type": "string",
+            "mapping": {
+              "type": "string",
+              "fields": {
+                "raw": {
+                  "type":  "string",
+                  "index": "not_analyzed",
+                  "ignore_above": 256
+                }
+              }
+            }
+          }
+        }
+      ]
+    }
+  }
+}
+
+PUT logs-2015.10.01/event/1
+{ "message": "error:16" }
+--------------------------------------------------
+// AUTOSENSE
+<1> The `logging` template will match any indices beginning with `logs-`.
+<2> Matching indices will be created with a single primary shard.
+<3> The `_all` field will be disabled by default for new type mappings.
+<4> String fields will be created with an `analyzed` main field, and a `not_analyzed` `.raw` field.
diff --git a/docs/reference/mapping/dynamic/field-mapping.asciidoc b/docs/reference/mapping/dynamic/field-mapping.asciidoc
new file mode 100644
index 0000000..585931d
--- /dev/null
+++ b/docs/reference/mapping/dynamic/field-mapping.asciidoc
@@ -0,0 +1,139 @@
+[[dynamic-field-mapping]]
+=== Dynamic field mapping
+
+By default, when a previously unseen field is found in a document,
+Elasticsearch will add the new field to the type mapping.   This behaviour can
+be disabled, both at the document and at the <<object,`object`>>  level, by
+setting the <<dynamic,`dynamic`>> parameter to `false` or to `strict`.
+
+Assuming `dynamic` field mapping is enabled, some simple rules are used to
+determine which datatype the field should have:
+
+[horizontal]
+*JSON datatype*::                   *Elasticsearch datatype*
+
+`null`::                            No field is added.
+`true` or `false`::                 <<boolean,`boolean`>> field
+floating{nbsp}point{nbsp}number::   <<number,`double`>> field
+integer::                           <<number,`long`>> field
+object::                            <<object,`object`>> field
+array::                             Depends on the first non-`null` value in the array.
+string::                            Either a <<date,`date`>> field
+                                        (if the value passes <<date-detection,date detection>>),
+                                    a <<number,`double`>> or <<number,`long`>> field
+                                        (if the value passes <<numeric-detection,numeric detection>>)
+                                    or an <<mapping-index,`analyzed`>> <<string,`string`>> field.
+
+These are the only <<mapping-types,field datatypes>> that are dynamically
+detected.  All other datatypes must be mapped explicitly.
+
+Besides the options listed below, dynamic field mapping rules can be further
+customised with <<dynamic-templates,`dynamic_templates`>>.
+
+[[date-detection]]
+==== Date detection
+
+If `date_detection` is enabled (default), then new string fields are checked
+to see whether their contents match any of the date patterns specified in
+`dynamic_date_formats`.  If a match is found, a new <<date,`date`>> field is
+added with the corresponding format.
+
+The default value for `dynamic_date_formats` is:
+
+&#91; <<strict-date-time,`"strict_date_optional_time"`>>,`"yyyy/MM/dd HH:mm:ss Z||yyyy/MM/dd Z"`]
+
+For example:
+
+
+[source,js]
+--------------------------------------------------
+PUT my_index/my_type/1
+{
+  "create_date": "2015/09/02"
+}
+
+GET my_index/_mapping <1>
+--------------------------------------------------
+// AUTOSENSE
+<1> The `create_date` field has been added as a <<date,`date`>>
+    field with the <<mapping-date-format,`format`>>: +
+    `"yyyy/MM/dd HH:mm:ss Z||yyyy/MM/dd Z"`.
+
+===== Disabling date detection
+
+Dynamic date dection can be disabled by setting `date_detection` to `false`:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "date_detection": false
+    }
+  }
+}
+
+PUT my_index/my_type/1 <1>
+{
+  "create": "2015/09/02"
+}
+--------------------------------------------------
+// AUTOSENSE
+
+<1> The `create_date` field has been added as a <<string,`string`>> field.
+
+===== Customising detected date formats
+
+Alternatively, the `dynamic_date_formats` can be customised to support your
+own <<mapping-date-format,date formats>>:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "dynamic_date_formats": ["MM/dd/yyyy"]
+    }
+  }
+}
+
+PUT my_index/my_type/1
+{
+  "create_date": "09/25/2015"
+}
+--------------------------------------------------
+// AUTOSENSE
+
+
+[[numeric-detection]]
+==== Numeric detection
+
+While JSON has support for native floating point and integer datatypes, some
+applications or languages may sometimes render numbers as strings. Usually the
+correct solution is to map these fields explicitly, but numeric detection
+(which is disabled by default) can be enabled to do this automatically:
+
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "numeric_detection": true
+    }
+  }
+}
+
+PUT my_index/my_type/1
+{
+  "my_float":   "1.0", <1>
+  "my_integer": "1" <2>
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> The `my_float` field is added as a <<number,`double`>> field.
+<2> The `my_integer` field is added as a <<number,`long`>> field.
+
diff --git a/docs/reference/mapping/dynamic/templates.asciidoc b/docs/reference/mapping/dynamic/templates.asciidoc
new file mode 100644
index 0000000..e38fc31
--- /dev/null
+++ b/docs/reference/mapping/dynamic/templates.asciidoc
@@ -0,0 +1,251 @@
+[[dynamic-templates]]
+=== Dynamic templates
+
+Dynamic templates allow you to define custom mappings that can be applied to
+dynamically added fields based on:
+
+* the <<dynamic-mapping,datatype>> detected by Elasticsearch, with <<match-mapping-type,`match_mapping_type`>>.
+* the name of the field, with <<match-unmatch,`match` and `unmatch`>> or <<match-pattern,`match_pattern`>>.
+* the full dotted path to the field, with <<path-match-unmatch,`path_match` and `path_unmatch`>>.
+
+The original field name `{name}` and the detected datatype
+`{dynamic_type`} <<template-variables,template variables>> can be used in
+the mapping specification as placeholders.
+
+IMPORTANT: Dynamic field mappings are only added when a field contains a
+concrete value -- not `null` or an empty array. This means that if the
+`null_value` option  is used in a `dynamic_template`, it will only be applied
+after the first document  with a concrete value for the field has been
+indexed.
+
+Dynamic templates are specified as an array of named objects:
+
+[source,js]
+--------------------------------------------------
+  "dynamic_templates": [
+    {
+      "my_template_name": { <1>
+        ...  match conditions ... <2>
+        "mapping": { ... } <3>
+      }
+    },
+    ...
+  ]
+--------------------------------------------------
+<1> The template name can be any string value.
+<2> The match conditions can include any of : `match_mapping_type`, `match`, `match_pattern`, `unmatch`, `match_path`, `unmatch_path`.
+<3> The mapping that the matched field should use.
+
+
+Templates are processed in order -- the first matching template wins. New
+templates can be appended to the end of the list with the
+<<indices-put-mapping,PUT mapping>> API.  If a new template has the same
+name as an existing template, it will replace the old version.
+
+[[match-mapping-type]]
+==== `match_mapping_type`
+
+The `match_mapping_type` matches on the datatype detected by
+<<dynamic-field-mapping,dynamic field mapping>>, in other words, the datatype
+that Elasticsearch thinks the field should have.  Only the following datatypes
+can be automatically detected: `boolean`, `date`, `double`, `long`, `object`,
+`string`.  It also accepts `*` to match all datatypes.
+
+For example, if we wanted to map all integer fields as `integer` instead of
+`long`, and all `string` fields as both `analyzed` and `not_analyzed`, we
+could use the following template:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "dynamic_templates": [
+        {
+          "integers": {
+            "match_mapping_type": "long",
+            "mapping": {
+              "type": "integer"
+            }
+          }
+        },
+        {
+          "strings": {
+            "match_mapping_type": "string",
+            "mapping": {
+              "type": "string",
+              "fields": {
+                "raw": {
+                  "type":  "string",
+                  "index": "not_analyzed",
+                  "ignore_above": 256
+                }
+              }
+            }
+          }
+        }
+      ]
+    }
+  }
+}
+
+PUT my_index/my_type/1
+{
+  "my_integer": 5, <1>
+  "my_string": "Some string" <2>
+}
+
+--------------------------------------------------
+// AUTOSENSE
+<1> The `my_integer` field is mapped as an `integer`.
+<2> The `my_string` field is mapped as an analyzed `string`, with a `not_analyzed` <<multi-fields,multi field>>.
+
+
+[[match-unmatch]]
+==== `match` and `unmatch`
+
+The `match` parameter uses a pattern to match on the fieldname, while
+`unmatch` uses a pattern to exclude fields matched by `match`.
+
+The following example matches all `string` fields whose name starts with
+`long_` (except for those which end with `_text`) and maps them as `long`
+fields:
+
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "dynamic_templates": [
+        {
+          "longs_as_strings": {
+            "match_mapping_type": "string",
+            "match":   "long_*",
+            "unmatch": "*_text",
+            "mapping": {
+              "type": "long"
+            }
+          }
+        }
+      ]
+    }
+  }
+}
+
+PUT my_index/my_type/1
+{
+  "long_num": "5", <1>
+  "long_text": "foo" <2>
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> The `long_num` field is mapped as a `long`.
+<2> The `long_text` field uses the default `string` mapping.
+
+[[match-pattern]]
+==== `match_pattern`
+
+The `match_pattern` parameter behaves just like the `match` parameter, but
+supports full Java regular expression matching on the field name instead of
+simple wildcards, for instance:
+
+[source,js]
+--------------------------------------------------
+  "match_pattern": "^profit_\d+$"
+--------------------------------------------------
+
+[[path-match-unmatch]]
+==== `path_match` and `path_unmatch`
+
+The `path_match` and `path_unmatch` parameters work in the same way as `match`
+and `unmatch`, but operate on the full dotted path to the field, not just the
+final name, e.g. `some_object.*.some_field`.
+
+This example copies the values of any fields in the `name` object to the
+top-level `full_name` field, except for the `middle` field:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "dynamic_templates": [
+        {
+          "full_name": {
+            "path_match":   "name.*",
+            "path_unmatch": "*.middle",
+            "mapping": {
+              "type":       "string",
+              "copy_to":    "full_name"
+            }
+          }
+        }
+      ]
+    }
+  }
+}
+
+PUT my_index/my_type/1
+{
+  "name": {
+    "first":  "Alice",
+    "middle": "Mary",
+    "last":   "White"
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+
+[[template-variables]]
+==== `{name}` and `{dynamic_type}`
+
+The `{name}` and `{dynamic_type}` placeholders are replaced in the `mapping`
+with the field name and detected dynamic type.  The following example sets all
+string fields to use an <<analyzer,`analyzer`>> with the same name as the
+field, and disables <<doc-values,`doc_values`>> for all non-string fields:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "dynamic_templates": [
+        {
+          "named_analyzers": {
+            "match_mapping_type": "string",
+            "match": "*",
+            "mapping": {
+              "type": "string",
+              "analyzer": "{name}"
+            }
+          }
+        },
+        {
+          "no_doc_values": {
+            "match_mapping_type":"*",
+            "mapping": {
+              "type": "{dynamic_type}",
+              "doc_values": false
+            }
+          }
+        }
+      ]
+    }
+  }
+}
+
+PUT my_index/my_type/1
+{
+  "english": "Some English text", <1>
+  "count":   5 <2>
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> The `english` field is mapped as a `string` field with the `english` analyzer.
+<2> The `count` field is mapped as a `long` field with `doc_values` disabled
+
diff --git a/docs/reference/mapping/fielddata_formats.asciidoc b/docs/reference/mapping/fielddata_formats.asciidoc
deleted file mode 100644
index eda3566..0000000
--- a/docs/reference/mapping/fielddata_formats.asciidoc
+++ /dev/null
@@ -1,257 +0,0 @@
-[[fielddata-formats]]
-== Fielddata formats
-
-The field data format controls how field data should be stored.
-
-Depending on the field type, there might be several field data types
-available. In particular, string, geo-point and numeric types support the `doc_values`
-format which allows for computing the field data data-structures at indexing
-time and storing them on disk. Although it will make the index larger and may
-be slightly slower, this implementation will be more near-realtime-friendly
-and will require much less memory from the JVM than other implementations.
-
-Here is an example of how to configure the `tag` field to use the `paged_bytes` field
-data format.
-
-[source,js]
---------------------------------------------------
-{
-    "tag": {
-        "type":      "string",
-        "fielddata": {
-            "format": "paged_bytes"
-        }
-    }
-}
---------------------------------------------------
-
-It is possible to change the field data format (and the field data settings
-in general) on a live index by using the update mapping API.
-
-[float]
-=== String field data types
-
-`paged_bytes` (default on analyzed string fields)::
-    Stores unique terms sequentially in a large buffer and maps documents to
-    the indices of the terms they contain in this large buffer.
-
-`doc_values` (default when index is set to `not_analyzed`)::
-    Computes and stores field data data-structures on disk at indexing time.
-    Lowers memory usage but only works on non-analyzed strings (`index`: `no` or
-    `not_analyzed`).
-
-[float]
-=== Numeric field data types
-
-`array`::
-    Stores field values in memory using arrays.
-
-`doc_values` (default unless doc values are disabled)::
-    Computes and stores field data data-structures on disk at indexing time.
-
-[float]
-=== Geo point field data types
-
-`array`::
-    Stores latitudes and longitudes in arrays.
-
-`doc_values` (default unless doc values are disabled)::
-    Computes and stores field data data-structures on disk at indexing time.
-
-[float]
-[[global-ordinals]]
-=== Global ordinals
-
-Global ordinals is a data-structure on top of field data, that maintains an
-incremental numbering for all the terms in field data in a lexicographic order.
-Each term has a unique number and the number of term 'A' is lower than the number
-of term 'B'. Global ordinals are only supported on string fields.
-
-Field data on string also has ordinals, which is a unique numbering for all terms
-in a particular segment and field. Global ordinals just build on top of this,
-by providing a mapping between the segment ordinals and the global ordinals.
-The latter being unique across the entire shard.
-
-Global ordinals can be beneficial in search features that use segment ordinals already
-such as the terms aggregator to improve the execution time. Often these search features
-need to merge the segment ordinal results to a cross segment terms result. With
-global ordinals this mapping happens during field data load time instead of during each
-query execution. With global ordinals search features only need to resolve the actual
-term when building the (shard) response, but during the execution there is no need
-at all to use the actual terms and the unique numbering global ordinals provided is
-sufficient and improves the execution time.
-
-Global ordinals for a specified field are tied to all the segments of a shard (Lucene index),
-which is different than for field data for a specific field which is tied to a single segment.
-For this reason global ordinals need to be rebuilt in its entirety once new segments
-become visible. This one time cost would happen anyway without global ordinals, but
-then it would happen for each search execution instead!
-
-The loading time of global ordinals depends on the number of terms in a field, but in general
-it is low, since it source field data has already been loaded. The memory overhead of global
-ordinals is a small because it is very efficiently compressed. Eager loading of global ordinals
-can move the loading time from the first search request, to the refresh itself.
-
-[float]
-[[fielddata-loading]]
-=== Fielddata loading
-
-By default, field data is loaded lazily, ie. the first time that a query that
-requires them is executed. However, this can make the first requests that
-follow a merge operation quite slow since fielddata loading is a heavy
-operation.
-
-It is possible to force field data to be loaded and cached eagerly through the
-`loading` setting of fielddata:
-
-[source,js]
---------------------------------------------------
-{
-    "category": {
-        "type":      "string",
-        "fielddata": {
-            "loading": "eager"
-        }
-    }
-}
---------------------------------------------------
-
-Global ordinals can also be eagerly loaded:
-
-[source,js]
---------------------------------------------------
-{
-    "category": {
-        "type":      "string",
-        "fielddata": {
-            "loading": "eager_global_ordinals"
-        }
-    }
-}
---------------------------------------------------
-
-With the above setting both field data and global ordinals for a specific field
-are eagerly loaded.
-
-[float]
-==== Disabling field data loading
-
-Field data can take a lot of RAM so it makes sense to disable field data
-loading on the fields that don't need field data, for example those that are
-used for full-text search only. In order to disable field data loading, just
-change the field data format to `disabled`. When disabled, all requests that
-will try to load field data, e.g. when they include aggregations and/or sorting,
-will return an error.
-
-[source,js]
---------------------------------------------------
-{
-    "text": {
-        "type":      "string",
-        "fielddata": {
-            "format": "disabled"
-        }
-    }
-}
---------------------------------------------------
-
-The `disabled` format is supported by all field types.
-
-[float]
-[[field-data-filtering]]
-=== Filtering fielddata
-
-It is possible to control which field values are loaded into memory,
-which is particularly useful for string fields. When specifying the
-<<mapping-core-types,mapping>> for a field, you
-can also specify a fielddata filter.
-
-Fielddata filters can be changed using the
-<<indices-put-mapping,PUT mapping>>
-API. After changing the filters, use the
-<<indices-clearcache,Clear Cache>> API
-to reload the fielddata using the new filters.
-
-[float]
-==== Filtering by frequency:
-
-The frequency filter allows you to only load terms whose frequency falls
-between a `min` and `max` value, which can be expressed an absolute
-number (when the number is bigger than 1.0) or as a percentage
-(eg `0.01` is `1%` and `1.0` is `100%`). Frequency is calculated
-*per segment*. Percentages are based on the number of docs which have a
-value for the field, as opposed to all docs in the segment.
-
-Small segments can be excluded completely by specifying the minimum
-number of docs that the segment should contain with `min_segment_size`:
-
-[source,js]
---------------------------------------------------
-{
-    "tag": {
-        "type":      "string",
-        "fielddata": {
-            "filter": {
-                "frequency": {
-                    "min":              0.001,
-                    "max":              0.1,
-                    "min_segment_size": 500
-                }
-            }
-        }
-    }
-}
---------------------------------------------------
-
-[float]
-==== Filtering by regex
-
-Terms can also be filtered by regular expression - only values which
-match the regular expression are loaded. Note: the regular expression is
-applied to each term in the field, not to the whole field value. For
-instance, to only load hashtags from a tweet, we can use a regular
-expression which matches terms beginning with `#`:
-
-[source,js]
---------------------------------------------------
-{
-    "tweet": {
-        "type":      "string",
-        "analyzer":  "whitespace"
-        "fielddata": {
-            "filter": {
-                "regex": {
-                    "pattern": "^#.*"
-                }
-            }
-        }
-    }
-}
---------------------------------------------------
-
-[float]
-==== Combining filters
-
-The `frequency` and `regex` filters can be combined:
-
-[source,js]
---------------------------------------------------
-{
-    "tweet": {
-        "type":      "string",
-        "analyzer":  "whitespace"
-        "fielddata": {
-            "filter": {
-                "regex": {
-                    "pattern":          "^#.*",
-                },
-                "frequency": {
-                    "min":              0.001,
-                    "max":              0.1,
-                    "min_segment_size": 500
-                }
-            }
-        }
-    }
-}
---------------------------------------------------
diff --git a/docs/reference/mapping/fields.asciidoc b/docs/reference/mapping/fields.asciidoc
index e2d42a9..dd267a8 100644
--- a/docs/reference/mapping/fields.asciidoc
+++ b/docs/reference/mapping/fields.asciidoc
@@ -5,7 +5,8 @@ Each document has metadata associated with it, such as the `_index`, mapping
 <<mapping-type-field,`_type`>>, and `_id` meta-fields.  The behaviour of some of these meta-fields
 can be customised when a mapping type is created.
 
-The meta-fields are:
+[float]
+=== Identity meta-fields
 
 [horizontal]
 <<mapping-index-field,`_index`>>::
@@ -18,16 +19,26 @@ The meta-fields are:
 
 <<mapping-type-field,`_type`>>::
 
-    The document's <<all-mapping-types,mapping type>>.
+    The document's <<mapping-type,mapping type>>.
 
 <<mapping-id-field,`_id`>>::
 
     The document's ID.
 
+[float]
+=== Document source meta-fields
+
 <<mapping-source-field,`_source`>>::
 
     The original JSON representing the body of the document.
 
+<<mapping-size-field,`_size`>>::
+
+    The size of the `_source` field in bytes.
+
+[float]
+=== Indexing meta-fields
+
 <<mapping-all-field,`_all`>>::
 
     A _catch-all_  field that indexes the values of all other fields.
@@ -36,6 +47,17 @@ The meta-fields are:
 
     All fields in the document which contain non-null values.
 
+<<mapping-timestamp-field,`_timestamp`>>::
+
+    A timestamp associated with the document, either specified manually or auto-generated.
+
+<<mapping-ttl-field,`_ttl`>>::
+
+    How long a document should live before it is automatically deleted.
+
+[float]
+=== Routing meta-fields
+
 <<mapping-parent-field,`_parent`>>::
 
     Used to create a parent-child relationship between two mapping types.
@@ -44,31 +66,24 @@ The meta-fields are:
 
     A custom routing value which routes a document to a particular shard.
 
-<<mapping-size-field,`_size`>>::
+[float]
+=== Other meta-field
 
-    The size of the `_source` field in bytes.
+<<mapping-meta-field,`_meta`>>::
 
-<<mapping-timestamp-field,`_timestamp`>>::
+    Application specific metadata.
 
-    A timestamp associated with the document, either specified manually or auto-generated.
 
-<<mapping-ttl-field,`_ttl`>>::
 
-    How long a document should live before it is automatically deleted.
-
-include::fields/index-field.asciidoc[]
-
-include::fields/uid-field.asciidoc[]
+include::fields/all-field.asciidoc[]
 
-include::fields/type-field.asciidoc[]
+include::fields/field-names-field.asciidoc[]
 
 include::fields/id-field.asciidoc[]
 
-include::fields/source-field.asciidoc[]
-
-include::fields/all-field.asciidoc[]
+include::fields/index-field.asciidoc[]
 
-include::fields/field-names-field.asciidoc[]
+include::fields/meta-field.asciidoc[]
 
 include::fields/parent-field.asciidoc[]
 
@@ -76,7 +91,13 @@ include::fields/routing-field.asciidoc[]
 
 include::fields/size-field.asciidoc[]
 
+include::fields/source-field.asciidoc[]
+
 include::fields/timestamp-field.asciidoc[]
 
 include::fields/ttl-field.asciidoc[]
 
+include::fields/type-field.asciidoc[]
+
+include::fields/uid-field.asciidoc[]
+
diff --git a/docs/reference/mapping/fields/all-field.asciidoc b/docs/reference/mapping/fields/all-field.asciidoc
index d6037f4..00c8d3b 100644
--- a/docs/reference/mapping/fields/all-field.asciidoc
+++ b/docs/reference/mapping/fields/all-field.asciidoc
@@ -151,82 +151,18 @@ PUT my_index
 <1> The `_all` field is disabled for the `my_type` type.
 <2> The `query_string` query will default to querying the `content` field in this index.
 
-[[include-in-all]]
-==== Including specific fields in `_all`
+[[excluding-from-all]]
+==== Excluding fields from `_all`
 
 Individual fields can be included or excluded from the `_all` field with the
-`include_in_all` setting, which defaults to `true`:
+<<include-in-all,`include_in_all`>> setting.
 
-[source,js]
---------------------------------
-PUT my_index
-{
-  "mappings": {
-    "my_type": {
-      "properties": {
-        "title": { <1>
-          "type": "string"
-        }
-        "content": { <1>
-          "type": "string"
-        },
-        "date": { <2>
-          "type": "date",
-          "include_in_all": false
-        }
-      }
-    }
-  }
-}
---------------------------------
-// AUTOSENSE
-
-<1> The `title` and `content` fields with be included in the `_all` field.
-<2> The `date` field will not be included in the `_all` field.
-
-The `include_in_all` parameter can also be set at the type level and on
-<<mapping-object-type,`object`>> or <<mapping-nested-type,`nested`>> fields,
-in which case all sub-fields inherit that setting.  For instance:
-
-[source,js]
---------------------------------
-PUT my_index
-{
-  "mappings": {
-    "my_type": {
-      "include_in_all": false, <1>
-      "properties": {
-        "title":          { "type": "string" },
-        "author": {
-          "include_in_all": true, <2>
-          "properties": {
-            "first_name": { "type": "string" },
-            "last_name":  { "type": "string" }
-          }
-        },
-        "editor": {
-          "properties": {
-            "first_name": { "type": "string" }, <3>
-            "last_name":  { "type": "string", "include_in_all": true } <3>
-          }
-        }
-      }
-    }
-  }
-}
---------------------------------
-// AUTOSENSE
-
-<1> All fields in `my_type` are excluded from `_all`.
-<2> The `author.first_name` and `author.last_name` fields are included in `_all`.
-<3> Only the `editor.last_name` field is included in `_all`.
-    The `editor.first_name` inherits the type-level setting and is excluded.
 
 [[all-field-and-boosting]]
 ==== Index boosting and the `_all` field
 
-Individual fields can be _boosted_  at index time, with the `boost` parameter.
-The `_all` field takes these boosts into account:
+Individual fields can be _boosted_ at index time, with the <<index-boost,`boost`>>
+parameter. The `_all` field takes these boosts into account:
 
 [source,js]
 --------------------------------
diff --git a/docs/reference/mapping/fields/id-field.asciidoc b/docs/reference/mapping/fields/id-field.asciidoc
index 3aa6b92..994ff2e 100644
--- a/docs/reference/mapping/fields/id-field.asciidoc
+++ b/docs/reference/mapping/fields/id-field.asciidoc
@@ -2,8 +2,8 @@
 === `_id` field
 
 Each document indexed is associated with a <<mapping-type-field,`_type`>> (see
-<<all-mapping-types,Mapping Types>>) and an <<mapping-id-field,`_id`>>.  The
-`_id` field is not indexed as its value can be derived automatically from the
+<<mapping-type>>) and an <<mapping-id-field,`_id`>>.  The `_id` field is not
+indexed as its value can be derived automatically from the
 <<mapping-uid-field,`_uid`>> field.
 
 The value of the `_id` field is accessible in queries and scripts, but _not_
diff --git a/docs/reference/mapping/fields/meta-field.asciidoc b/docs/reference/mapping/fields/meta-field.asciidoc
new file mode 100644
index 0000000..4bcab15
--- /dev/null
+++ b/docs/reference/mapping/fields/meta-field.asciidoc
@@ -0,0 +1,30 @@
+[[mapping-meta-field]]
+=== `_meta` field
+
+Each mapping type can have custom meta data associated with it. These are not
+used at all by Elasticsearch, but can be used to store application-specific
+metadata, such as the class that a document belongs to:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "user": {
+      "_meta": { <1>
+        "class": "MyApp::User",
+        "version": {
+          "min": "1.0",
+          "max": "1.3"
+        }
+      }
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> This `_meta` info can be retrieved with the
+    <<indices-get-mapping,GET mapping>> API.
+
+The `_meta` field can be updated on an existing type using the
+<<indices-put-mapping,PUT mapping>> API.
diff --git a/docs/reference/mapping/fields/source-field.asciidoc b/docs/reference/mapping/fields/source-field.asciidoc
index f33b4a7..767b6af 100644
--- a/docs/reference/mapping/fields/source-field.asciidoc
+++ b/docs/reference/mapping/fields/source-field.asciidoc
@@ -78,8 +78,7 @@ stored.
 WARNING: Removing fields from the `_source` has similar downsides to disabling
 `_source`, especially the fact that you cannot reindex documents from one
 Elasticsearch index to another. Consider using
-<<search-request-source-filtering,source filtering>> or a
-<<mapping-transform,transform script>> instead.
+<<search-request-source-filtering,source filtering>> instead.
 
 The `includes`/`excludes` parameters (which also accept wildcards) can be used
 as follows:
diff --git a/docs/reference/mapping/fields/ttl-field.asciidoc b/docs/reference/mapping/fields/ttl-field.asciidoc
index 446b1d9..26bc3e7 100644
--- a/docs/reference/mapping/fields/ttl-field.asciidoc
+++ b/docs/reference/mapping/fields/ttl-field.asciidoc
@@ -1,5 +1,5 @@
 [[mapping-ttl-field]]
-=== `_ttl`
+=== `_ttl` field
 
 Some types of documents, such as session data or special offers, come with an
 expiration date. The `_ttl` field allows you to specify the minimum time a
diff --git a/docs/reference/mapping/fields/type-field.asciidoc b/docs/reference/mapping/fields/type-field.asciidoc
index bc6c578..c8f3817 100644
--- a/docs/reference/mapping/fields/type-field.asciidoc
+++ b/docs/reference/mapping/fields/type-field.asciidoc
@@ -2,8 +2,8 @@
 === `_type` field
 
 Each document indexed is associated with a <<mapping-type-field,`_type`>> (see
-<<all-mapping-types,Mapping Types>>) and an <<mapping-id-field,`_id`>>.  The
-`_type` field is indexed in order to make searching by type name fast.
+<<mapping-type>>) and an <<mapping-id-field,`_id`>>.  The `_type` field is
+indexed in order to make searching by type name fast.
 
 The value of the `_type` field is accessible in queries, aggregations,
 scripts, and when sorting:
diff --git a/docs/reference/mapping/fields/uid-field.asciidoc b/docs/reference/mapping/fields/uid-field.asciidoc
index a6dc6a9..cab5f4f 100644
--- a/docs/reference/mapping/fields/uid-field.asciidoc
+++ b/docs/reference/mapping/fields/uid-field.asciidoc
@@ -2,8 +2,8 @@
 === `_uid` field
 
 Each document indexed is associated with a <<mapping-type-field,`_type`>> (see
-<<all-mapping-types,Mapping Types>>) and an <<mapping-id-field,`_id`>>.  These
-values are combined as `{type}#{id}` and indexed as the `_uid` field.
+<<mapping-type>>) and an <<mapping-id-field,`_id`>>.  These values are
+combined as `{type}#{id}` and indexed as the `_uid` field.
 
 The value of the `_uid` field is accessible in queries, aggregations, scripts,
 and when sorting:
diff --git a/docs/reference/mapping/meta.asciidoc b/docs/reference/mapping/meta.asciidoc
deleted file mode 100644
index 5cb0c14..0000000
--- a/docs/reference/mapping/meta.asciidoc
+++ /dev/null
@@ -1,25 +0,0 @@
-[[mapping-meta]]
-== Meta
-
-Each mapping can have custom meta data associated with it. These are
-simple storage elements that are simply persisted along with the mapping
-and can be retrieved when fetching the mapping definition. The meta is
-defined under the `_meta` element, for example:
-
-[source,js]
---------------------------------------------------
-{
-    "tweet" : {
-        "_meta" : {
-            "attr1" : "value1",
-            "attr2" : {
-                "attr3" : "value3"
-            }
-        }
-    }
-}
---------------------------------------------------
-
-Meta can be handy for example for client libraries that perform
-serialization and deserialization to store its meta model (for example,
-the class the document maps to).
diff --git a/docs/reference/mapping/params.asciidoc b/docs/reference/mapping/params.asciidoc
new file mode 100644
index 0000000..119ce82
--- /dev/null
+++ b/docs/reference/mapping/params.asciidoc
@@ -0,0 +1,100 @@
+[[mapping-params]]
+== Mapping parameters
+
+The following pages provide detailed explanations of the various mapping
+parameters that are used by <<mapping-types,field mappings>>:
+
+
+The following mapping parameters are common to some or all field datatypes:
+
+* <<analyzer,`analyzer`>>
+* <<index-boost,`boost`>>
+* <<coerce,`coerce`>>
+* <<copy-to,`copy_to`>>
+* <<doc-values,`doc_values`>>
+* <<dynamic,`dynamic`>>
+* <<enabled,`enabled`>>
+* <<fielddata,`fielddata`>>
+* <<geohash,`geohash`>>
+* <<geohash-precision,`geohash_precision`>>
+* <<geohash-prefix,`geohash_prefix`>>
+* <<mapping-date-format,`format`>>
+* <<ignore-above,`ignore_above`>>
+* <<ignore-malformed,`ignore_malformed`>>
+* <<include-in-all,`include_in_all`>>
+* <<index-options,`index_options`>>
+* <<lat-lon,`lat_lon`>>
+* <<mapping-index,`index`>>
+* <<multi-fields,`fields`>>
+* <<norms,`norms`>>
+* <<null-value,`null_value`>>
+* <<position-offset-gap,`position_offset_gap`>>
+* <<properties,`properties`>>
+* <<search-analyzer,`search_analyzer`>>
+* <<similarity,`similarity`>>
+* <<mapping-store,`store`>>
+* <<term-vector,`term_vector`>>
+
+
+include::params/analyzer.asciidoc[]
+
+include::params/boost.asciidoc[]
+
+include::params/coerce.asciidoc[]
+
+include::params/copy-to.asciidoc[]
+
+include::params/doc-values.asciidoc[]
+
+include::params/dynamic.asciidoc[]
+
+include::params/enabled.asciidoc[]
+
+include::params/fielddata.asciidoc[]
+
+include::params/format.asciidoc[]
+
+include::params/geohash.asciidoc[]
+
+include::params/geohash-precision.asciidoc[]
+
+include::params/geohash-prefix.asciidoc[]
+
+include::params/ignore-above.asciidoc[]
+
+include::params/ignore-malformed.asciidoc[]
+
+include::params/include-in-all.asciidoc[]
+
+include::params/index.asciidoc[]
+
+include::params/index-options.asciidoc[]
+
+include::params/lat-lon.asciidoc[]
+
+include::params/multi-fields.asciidoc[]
+
+include::params/norms.asciidoc[]
+
+include::params/null-value.asciidoc[]
+
+include::params/position-offset-gap.asciidoc[]
+
+include::params/precision-step.asciidoc[]
+
+include::params/properties.asciidoc[]
+
+include::params/search-analyzer.asciidoc[]
+
+include::params/similarity.asciidoc[]
+
+include::params/store.asciidoc[]
+
+include::params/term-vector.asciidoc[]
+
+
+[source,js]
+--------------------------------------------------
+--------------------------------------------------
+// AUTOSENSE
+
diff --git a/docs/reference/mapping/params/analyzer.asciidoc b/docs/reference/mapping/params/analyzer.asciidoc
new file mode 100644
index 0000000..6c48ebd
--- /dev/null
+++ b/docs/reference/mapping/params/analyzer.asciidoc
@@ -0,0 +1,80 @@
+[[analyzer]]
+=== `analyzer`
+
+The values of <<mapping-index,`analyzed`>> string fields are passed through an
+<<analysis,analyzer>> to convert the string into a stream of _tokens_ or
+_terms_.  For instance, the string `"The quick Brown Foxes."` may, depending
+on which analyzer is used,  be analyzed to the tokens: `quick`, `brown`,
+`fox`.  These are the actual terms that are indexed for the field, which makes
+it possible to search efficiently for individual words _within_  big blobs of
+text.
+
+This analysis process needs to happen not just at index time, but also at
+query time: the query string needs to be passed through the same (or a
+similar) analyzer so that the terms that it tries to find are in the same
+format as those that exist in the index.
+
+Elasticsearch ships with a number of <<analysis-analyzers,pre-defined analyzers>>,
+which can be used without further configuration.  It also ships with many
+<<analysis-charfilters,character filters>>, <<analysis-tokenizers,tokenizers>>,
+and <<analysis-tokenfilters>> which can be combined to configure
+custom analyzers per index.
+
+Analyzers can be specified per-query, per-field or per-index. At index time,
+Elasticsearch will look for an analyzer in this order:
+
+* The `analyzer` defined in the field mapping.
+* An analyzer named `default` in the index settings.
+* The <<analysis-standard-analyzer,`standard`>> analyzer.
+
+At query time, there are a few more layers:
+
+* The `analyzer` defined in a <<full-text-queries,full-text query>>.
+* The `search_analyzer` defined in the field mapping.
+* The `analyzer` defined in the field mapping.
+* An analyzer named `default_search` in the index settings.
+* An analyzer named `default` in the index settings.
+* The <<analysis-standard-analyzer,`standard`>> analyzer.
+
+The easiest way to specify an analyzer for a particular field is to define it
+in the field mapping, as follows:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "text": { <1>
+          "type": "string",
+          "fields": {
+            "english": { <2>
+              "type":     "string",
+              "analyzer": "english"
+            }
+          }
+        }
+      }
+    }
+  }
+}
+
+GET my_index/_analyze?field=text <3>
+{
+  "text": "The quick Brown Foxes."
+}
+
+GET my_index/_analyze?field=text.english <4>
+{
+  "text": "The quick Brown Foxes."
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> The `text` field uses the default `standard` analyzer`.
+<2> The `text.english` <<multi-fields,multi-field>> uses the `english` analyzer, which removes stop words and applies stemming.
+<3> This returns the tokens: [ `the`, `quick`, `brown`, `foxes` ].
+<4> This returns the tokens: [ `quick`, `brown`, `fox` ].
+
+
+
diff --git a/docs/reference/mapping/params/boost.asciidoc b/docs/reference/mapping/params/boost.asciidoc
new file mode 100644
index 0000000..b92e081
--- /dev/null
+++ b/docs/reference/mapping/params/boost.asciidoc
@@ -0,0 +1,59 @@
+[[index-boost]]
+=== `boost`
+
+Individual fields can be _boosted_ -- count more towards the relevance score
+-- at index time, with the `boost` parameter as follows:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "title": {
+          "type": "string",
+          "boost": 2 <1>
+        },
+        "content": {
+          "type": "string"
+        }
+      }
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+
+<1> Matches on the `title` field will have twice the weight as those on the
+    `content` field, which has the default `boost` of `1.0`.
+
+Note that a `title` field will usually be shorter than a `content` field.  The
+default relevance calculation takes field length into account, so a short
+`title` field will have a higher natural boost than a long `content` field.
+
+[WARNING]
+.Why index time boosting is a bad idea
+==================================================
+
+We advise against using index time boosting for the following reasons:
+
+* You cannot change index-time `boost` values without reindexing all of your
+  documents.
+
+* Every query supports query-time boosting which achieves the same effect. The
+  difference is that you can tweak the `boost` value without having to reindex.
+
+* Index-time boosts are stored as part of the <<norms,`norm`>>, which is only one
+  byte.  This reduces the resolution of the field length normalization factor
+  which can lead to lower quality relevance calculations.
+
+==================================================
+
+The only advantage that index time boosting has is that it is copied with the
+value into the <<mapping-all-field,`_all`>> field. This means that, when
+querying the `_all` field, words that originated from the `title` field will
+have a higher score than words that originated in the `content` field.
+This functionality comes at a cost: queries on the `_all` field are slower
+when index-time boosting is used.
+
diff --git a/docs/reference/mapping/params/coerce.asciidoc b/docs/reference/mapping/params/coerce.asciidoc
new file mode 100644
index 0000000..66add2a
--- /dev/null
+++ b/docs/reference/mapping/params/coerce.asciidoc
@@ -0,0 +1,89 @@
+[[coerce]]
+=== `coerce`
+
+Data is not always clean.  Depending on how it is produced a number might be
+rendered in the JSON body as a true JSON number, e.g. `5`, but it might also
+be rendered as a string, e.g. `"5"`.  Alternatively, a number that should be
+an integer might instead be rendered as a floating point, e.g. `5.0`, or even
+`"5.0"`.
+
+Coercion attempts to clean up dirty values to fit the datatype of a field.
+For instance:
+
+* Strings will be coerced to numbers.
+* Floating points will be truncated for integer values.
+* Lon/lat geo-points will be normalized to a standard -180:180 / -90:90 coordinate system.
+
+For instance:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "number_one": {
+          "type": "integer"
+        },
+        "number_two": {
+          "type": "integer",
+          "coerce": false
+        }
+      }
+    }
+  }
+}
+
+PUT my_index/my_type/1
+{
+  "number_one": "10" <1>
+}
+
+PUT my_index/my_type/2
+{
+  "number_two": "10" <2>
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> The `number_one` field will contain the integer `10`.
+<2> This document will be rejected because coercion is disabled.
+
+[[coerce-setting]]
+==== Index-level default
+
+The `index.mapping.coerce` setting can be set on the index level to disable
+coercion globally across all mapping types:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "settings": {
+    "index.mapping.coerce": false
+  },
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "number_one": {
+          "type": "integer"
+        },
+        "number_two": {
+          "type": "integer",
+          "coerce": true
+        }
+      }
+    }
+  }
+}
+
+PUT my_index/my_type/1
+{ "number_one": "10" } <1>
+
+PUT my_index/my_type/2
+{ "number_two": "10" } <2>
+--------------------------------------------------
+// AUTOSENSE
+<1> This document will be rejected because the `number_one` field inherits the index-level coercion setting.
+<2> The `number_two` field overrides the index level setting to enable coercion.
+
diff --git a/docs/reference/mapping/params/copy-to.asciidoc b/docs/reference/mapping/params/copy-to.asciidoc
new file mode 100644
index 0000000..b437a87
--- /dev/null
+++ b/docs/reference/mapping/params/copy-to.asciidoc
@@ -0,0 +1,64 @@
+[[copy-to]]
+=== `copy_to`
+
+The `copy_to` parameter allows you to create custom
+<<mapping-all-field,`_all`>> fields.  In other words, the values of multiple
+fields can be copied into a group field, which can then be queried as a single
+field.  For instance, the `first_name` and `last_name` fields can be copied to
+the `full_name` field as follows:
+
+[source,js]
+--------------------------------------------------
+PUT /my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "first_name": {
+          "type": "string",
+          "copy_to": "full_name" <1>
+        },
+        "last_name": {
+          "type": "string",
+          "copy_to": "full_name" <1>
+        },
+        "full_name": {
+          "type": "string"
+        }
+      }
+    }
+  }
+}
+
+PUT /my_index/my_type/1
+{
+  "first_name": "John",
+  "last_name": "Smith"
+}
+
+GET /my_index/_search
+{
+  "query": {
+    "match": {
+      "full_name": { <2>
+        "query": "John Smith",
+        "operator": "and"
+      }
+    }
+  }
+}
+
+--------------------------------------------------
+// AUTOSENSE
+<1>  The values of the `first_name` and `last_name` fields are copied to the
+     `full_name` field.
+
+<2>  The `first_name` and `last_name` fields can still be queried for the
+     first name and last name respectively, but the `full_name` field can be
+     queried for both first and last names.
+
+Some important points:
+
+* It is the field _value_ which is copied, not the terms (which result from the analysis process).
+* The original <<mapping-source-field,`_source`>> field will not be modified to show the copied values.
+* The same value can be copied to multiple fields, with `"copy_to": [ "field_1", "field_2" ]`
diff --git a/docs/reference/mapping/params/doc-values.asciidoc b/docs/reference/mapping/params/doc-values.asciidoc
new file mode 100644
index 0000000..4a495e6
--- /dev/null
+++ b/docs/reference/mapping/params/doc-values.asciidoc
@@ -0,0 +1,46 @@
+[[doc-values]]
+=== `doc_values`
+
+Most fields are <<mapping-index,indexed>> by default, which makes them
+searchable. The inverted index allows queries to look up the search term in
+unique sorted list of terms, and from that immediately have access to the list
+of documents that contain the term.
+
+Sorting, aggregations, and access to field values in scripts requires a
+different data access pattern.  Instead of lookup up the term and finding
+documents, we need to be able to look up the document and find the terms that
+is has in a field.
+
+Doc values are the on-disk data structure, built at document index time, which
+makes this data access pattern possible. Doc values are supported on almost
+all field types, with the __notable exception of `analyzed` string fields__.
+
+All fields which support doc values have them enabled by default. If you are
+sure that you don't need to sort or aggregate on a field, or access the field
+value from a script, you can disable doc values in order to save disk space:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "status_code": { <1>
+          "type":       "string",
+          "index":      "not_analyzed"
+        },
+        "session_id": { <2>
+          "type":       "string",
+          "index":      "not_analyzed",
+          "doc_values": false
+        }
+      }
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> The `status_code` field has `doc_values` enabled by default.
+<2> The `session_id` has `doc_values` disabled, but can still be queried.
+
diff --git a/docs/reference/mapping/params/dynamic.asciidoc b/docs/reference/mapping/params/dynamic.asciidoc
new file mode 100644
index 0000000..3ea3c9c
--- /dev/null
+++ b/docs/reference/mapping/params/dynamic.asciidoc
@@ -0,0 +1,87 @@
+[[dynamic]]
+=== `dynamic`
+
+By default, fields can be added _dynamically_ to a document, or to
+<<object,inner objects>> within a document, just by indexing a document
+containing the new field.  For instance:
+
+[source,js]
+--------------------------------------------------
+DELETE my_index <1>
+
+PUT my_index/my_type/1 <2>
+{
+  "username": "johnsmith",
+  "name": {
+    "first": "John",
+    "last": "Smith"
+  }
+}
+
+GET my_index/_mapping <3>
+
+PUT my_index/my_type/2 <4>
+{
+  "username": "marywhite",
+  "email": "mary@white.com",
+  "name": {
+    "first": "Mary",
+    "middle": "Alice",
+    "last": "White"
+  }
+}
+
+GET my_index/_mapping <5>
+--------------------------------------------------
+// AUTOSENSE
+<1> First delete the index, in case it already exists.
+<2> This document introduces the string field `username`, the object field
+    `name`, and two string fields under the `name` object which can be
+    referred to as `name.first` and `name.last`.
+<3> Check the mapping to verify the above.
+<4> This document adds two string fields: `email` and `name.middle`.
+<5> Check the mapping to verify the changes.
+
+The details of how new fields are detected and added to the mapping is explained in <<dynamic-mapping>>.
+
+The `dynamic` setting controls whether new fields can be added dynamically or
+not.  It accepts three settings:
+
+[horizontal]
+`true`::    Newly detected fields are added to the mapping. (default)
+`false`::   Newly detected fields are ignored.  New fields must be added explicitly.
+`strict`::  If new fields are detected, an exception is thrown and the document is rejected.
+
+The `dynamic` setting may be set at the mapping type level, and on each
+<<object,inner object>>.  Inner objects inherit the setting from their parent
+object or from the mapping type.  For instance:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "dynamic": false, <1>
+      "properties": {
+        "user": { <2>
+          "properties": {
+            "name": {
+              "type": "string"
+            },
+            "social_networks": { <3>
+              "dynamic": true,
+              "properties": {}
+            }
+          }
+        }
+      }
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> Dynamic mapping is disabled at the type level, so no new top-level fields will be added dynamically.
+<2> The `user` object inherits the type-level setting.
+<3> The `user.social_networks` object enables dynamic mapping, so new fields may be added to this inner object.
+
diff --git a/docs/reference/mapping/params/enabled.asciidoc b/docs/reference/mapping/params/enabled.asciidoc
new file mode 100644
index 0000000..d9d162e
--- /dev/null
+++ b/docs/reference/mapping/params/enabled.asciidoc
@@ -0,0 +1,94 @@
+[[enabled]]
+=== `enabled`
+
+Elasticsearch tries to index all of the fields you give it, but sometimes you
+want to just store the field without indexing it.  For instance, imagine that
+you are using Elasticsearch as a web session store.  You may want to index the
+session ID and last update time, but you don't need to query or run
+aggregations on the session data itself.
+
+The `enabled` setting, which can be applied only to the mapping type and to
+<<object,`object`>> fields, causes Elasticsearch to skip parsing of the
+contents of the field entirely.  The JSON can still be retrieved from the
+<<mapping-source-field,`_source`>> field, but it is not searchable or stored
+in any other way:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "session": {
+      "properties": {
+        "user_id": {
+          "type":  "string",
+          "index": "not_analyzed"
+        },
+        "last_updated": {
+          "type": "date"
+        },
+        "session_data": { <1>
+          "enabled": false
+        }
+      }
+    }
+  }
+}
+
+PUT my_index/session/session_1
+{
+  "user_id": "kimchy",
+  "session_data": { <2>
+    "arbitrary_object": {
+      "some_array": [ "foo", "bar", { "baz": 2 } ]
+    }
+  },
+  "last_updated": "2015-12-06T18:20:22"
+}
+
+PUT my_index/session/session_2
+{
+  "user_id": "jpountz",
+  "session_data": "none", <3>
+  "last_updated": "2015-12-06T18:22:13"
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> The `session_data` field is disabled.
+<2> Any arbitrary data can be passed to the `session_data` field as it will be entirely ignored.
+<3> The `session_data` will also ignore values that are not JSON objects.
+
+The entire mapping type may be disabled as well, in which case the document is
+stored in the <<mapping-source-field,`_source`>> field, which means it can be
+retrieved, but none of its contents are indexed in any way:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "session": { <1>
+      "enabled": false
+    }
+  }
+}
+
+PUT my_index/session/session_1
+{
+  "user_id": "kimchy",
+  "session_data": {
+    "arbitrary_object": {
+      "some_array": [ "foo", "bar", { "baz": 2 } ]
+    }
+  },
+  "last_updated": "2015-12-06T18:20:22"
+}
+
+GET my_index/session/session_1 <2>
+
+GET my_index/_mapping <3>
+--------------------------------------------------
+// AUTOSENSE
+<1> The entire `session` mapping type is disabled.
+<2> The document can be retrieved.
+<3> Checking the mapping reveals that no fields have been added.
diff --git a/docs/reference/mapping/params/fielddata.asciidoc b/docs/reference/mapping/params/fielddata.asciidoc
new file mode 100644
index 0000000..33dd715
--- /dev/null
+++ b/docs/reference/mapping/params/fielddata.asciidoc
@@ -0,0 +1,225 @@
+[[fielddata]]
+=== `fielddata`
+
+Most fields are <<mapping-index,indexed>> by default, which makes them
+searchable. The inverted index allows queries to look up the search term in
+unique sorted list of terms, and from that immediately have access to the list
+of documents that contain the term.
+
+Sorting, aggregations, and access to field values in scripts requires a
+different data access pattern.  Instead of lookup up the term and finding
+documents, we need to be able to look up the document and find the terms that
+is has in a field.
+
+Most fields can use index-time, on-disk <<doc-values,`doc_values`>> to support
+this type of data access pattern, but `analyzed` string fields do not support
+`doc_values`.
+
+Instead, `analyzed` strings use a query-time data structure called
+`fielddata`.  This data structure is built on demand the first time that a
+field is used for aggregations, sorting, or is accessed in a script.  It is built
+by reading the entire inverted index for each segment from disk, inverting the
+term ↔︎ document relationship, and storing the result in memory, in the
+JVM heap.
+
+
+Loading fielddata is an expensive process so, once it has been loaded, it
+remains in memory for the lifetime of the segment.
+
+[WARNING]
+.Fielddata can fill up your heap space
+==============================================================================
+Fielddata can consume a lot of heap space, especially when loading high
+cardinality `analyzed` string fields.  Most of the time, it doesn't make sense
+to sort or aggregate on `analyzed` string fields (with the notable exception
+of the
+<<search-aggregations-bucket-significantterms-aggregation,`significant_terms`>>
+aggregation).  Always think about whether a `not_analyzed` field (which can
+use `doc_values`) would be  a better fit for your use case.
+==============================================================================
+
+[[fielddata-format]]
+==== `fielddata.format`
+
+For `analyzed` string fields, the fielddata `format` controls whether
+fielddata should be enabled or not.  It accepts: `disabled` and `paged_bytes`
+(enabled, which is the default).  To disable fielddata loading, you can use
+the following mapping:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "text": {
+          "type": "string",
+          "fielddata": {
+            "format": "disabled" <1>
+          }
+        }
+      }
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> The `text` field cannot be used for sorting, aggregations, or in scripts.
+
+.Fielddata and other datatypes
+[NOTE]
+==================================================
+
+Historically, other field datatypes also used fielddata, but this has been replaced
+by index-time, disk-based <<doc-values,`doc_values`>>.
+
+==================================================
+
+
+[[fielddata-loading]]
+==== `fielddata.loading`
+
+This per-field setting controls when fielddata is loaded into memory. It
+accepts three options:
+
+[horizontal]
+`lazy`::
+
+    Fielddata is only loaded into memory when it is needed. (default)
+
+`eager`::
+
+    Fielddata is loaded into memory before a new search segment becomes
+    visible to search.  This can reduce the latency that a user may experience
+    if their search request has to trigger lazy loading from a big segment.
+
+`eager_global_ordinals`::
+
+    Loading fielddata into memory is only part of the work that is required.
+    After loading the fielddata for each segment, Elasticsearch builds the
+    <<global-ordinals>> data structure to make a list of all unique terms
+    across all the segments in a shard.  By default, global ordinals are built
+    lazily.  If the field has a very high cardinality, global ordinals may
+    take some time to build, in which case you can use eager loading instead.
+
+[[global-ordinals]]
+.Global ordinals
+*****************************************
+
+Global ordinals is a data-structure on top of fielddata and doc values, that
+maintains an incremental numbering for each unique term in a lexicographic
+order. Each term has a unique number and the number of term 'A' is lower than
+the number of term 'B'. Global ordinals are only supported on string fields.
+
+Fielddata and doc values also have ordinals, which is a unique numbering for all terms
+in a particular segment and field. Global ordinals just build on top of this,
+by providing a mapping between the segment ordinals and the global ordinals,
+the latter being unique across the entire shard.
+
+Global ordinals are used for features that use segment ordinals, such as
+sorting and the terms aggregation, to improve the execution time. A terms
+aggregation relies purely on global ordinals to perform the aggregation at the
+shard level, then converts global ordinals to the real term only for the final
+reduce phase, which combines results from different shards.
+
+Global ordinals for a specified field are tied to _all the segments of a
+shard_, while fielddata and doc values ordinals are tied to a single segment.
+which is different than for field data for a specific field which is tied to a
+single segment. For this reason global ordinals need to be entirely rebuilt
+whenever a once new segment becomes visible.
+
+The loading time of global ordinals depends on the number of terms in a field, but in general
+it is low, since it source field data has already been loaded. The memory overhead of global
+ordinals is a small because it is very efficiently compressed. Eager loading of global ordinals
+can move the loading time from the first search request, to the refresh itself.
+
+*****************************************
+
+[[field-data-filtering]]
+==== `fielddata.filter`
+
+Fielddata filtering can be used to reduce the number of terms loaded into
+memory, and thus reduce memory usage. Terms can be filtered by _frequency_ or
+by _regular expression_, or a combination of the two:
+
+Filtering by frequency::
++
+--
+
+The frequency filter allows you to only load terms whose term frequency falls
+between a `min` and `max` value, which can be expressed an absolute
+number (when the number is bigger than 1.0) or as a percentage
+(eg `0.01` is `1%` and `1.0` is `100%`). Frequency is calculated
+*per segment*. Percentages are based on the number of docs which have a
+value for the field, as opposed to all docs in the segment.
+
+Small segments can be excluded completely by specifying the minimum
+number of docs that the segment should contain with `min_segment_size`:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "tag": {
+          "type": "string",
+          "fielddata": {
+            "filter": {
+              "frequency": {
+                "min": 0.001,
+                "max": 0.1,
+                "min_segment_size": 500
+              }
+            }
+          }
+        }
+      }
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+--
+
+Filtering by regex::
++
+--
+Terms can also be filtered by regular expression - only values which
+match the regular expression are loaded. Note: the regular expression is
+applied to each term in the field, not to the whole field value. For
+instance, to only load hashtags from a tweet, we can use a regular
+expression which matches terms beginning with `#`:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "tweet": {
+          "type": "string",
+          "analyzer": "whitespace",
+          "fielddata": {
+            "filter": {
+              "regex": {
+                "pattern": "^#.*"
+              }
+            }
+          }
+        }
+      }
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+--
+
+These filters can be updated on an existing field mapping and will take
+effect the next time the fielddata for a segment is loaded. Use the
+<<indices-clearcache,Clear Cache>> API
+to reload the fielddata using the new filters.
diff --git a/docs/reference/mapping/params/format.asciidoc b/docs/reference/mapping/params/format.asciidoc
new file mode 100644
index 0000000..a740858
--- /dev/null
+++ b/docs/reference/mapping/params/format.asciidoc
@@ -0,0 +1,281 @@
+[[mapping-date-format]]
+=== `format`
+
+In JSON documents, dates are represented as strings. Elasticsearch uses a set
+of preconfigured formats to recognize and parse these strings into a long
+value representing _milliseconds-since-the-epoch_ in UTC.
+
+Besides the <<built-in-date-formats,built-in formats>>, your own
+<<custom-date-formats,custom formats>> can be specified using the familiar
+`yyyy/MM/dd` syntax:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "date": {
+          "type":   "date",
+          "format": "yyyy-MM-dd"
+        }
+      }
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+
+Many APIs which support date values also support <<date-math,date math>>
+expressions, such as `now-1m/d` -- the current time, minus one month, rounded
+down to the nearest day.
+
+[[custom-date-formats]]
+==== Custom date formats
+
+Completely customizable date formats are supported.  The syntax for these is explained
+http://www.joda.org/joda-time/apidocs/org/joda/time/format/DateTimeFormat.html[in the Joda docs].
+
+[[built-in-date-formats]]
+==== Built In Formats
+
+Most of the below dates have a `strict` companion dates, which means, that
+year, month and day parts of the week must have prepending zeros in order
+to be valid. This means, that a date like `5/11/1` would not be valid, but
+you would need to specify the full date, which would be `2005/11/01` in this
+example. So instead of `date_optional_time` you would need to specify
+`strict_date_optional_time`.
+
+The following tables lists all the defaults ISO formats supported:
+
+`epoch_millis`::
+
+    A formatter for the number of milliseconds since the epoch. Note, that
+    this timestamp allows a max length of 13 chars, so dates older than 1653
+    and 2286 are not supported. You should use a different date formatter in
+    that case.
+
+`epoch_second`::
+
+    A formatter for the number of seconds since the epoch. Note, that this
+    timestamp allows a max length of 10 chars, so dates older than 1653 and
+    2286 are not supported. You should use a different date formatter in that
+    case.
+
+[[strict-date-time]]`date_optional_time` or `strict_date_optional_time`::
+
+    A generic ISO datetime parser where the date is mandatory and the time is
+    optional.
+    http://www.joda.org/joda-time/apidocs/org/joda/time/format/ISODateTimeFormat.html#dateOptionalTimeParser--[Full details here].
+
+`basic_date`::
+
+    A basic formatter for a full date as four digit year, two digit month of
+    year, and two digit day of month: `yyyyMMdd`.
+
+`basic_date_time`::
+
+    A basic formatter that combines a basic date and time, separated by a 'T':
+    `yyyyMMdd'T'HHmmss.SSSZ`.
+
+`basic_date_time_no_millis`::
+
+    A basic formatter that combines a basic date and time without millis,
+    separated by a 'T': `yyyyMMdd'T'HHmmssZ`.
+
+`basic_ordinal_date`::
+
+    A formatter for a full ordinal date, using a four digit year and three
+    digit dayOfYear: `yyyyDDD`.
+
+`basic_ordinal_date_time`::
+
+    A formatter for a full ordinal date and time, using a four digit year and
+    three digit dayOfYear: `yyyyDDD'T'HHmmss.SSSZ`.
+
+`basic_ordinal_date_time_no_millis`::
+
+    A formatter for a full ordinal date and time without millis, using a four
+    digit year and three digit dayOfYear: `yyyyDDD'T'HHmmssZ`.
+
+`basic_time`::
+
+    A basic formatter for a two digit hour of day, two digit minute of hour,
+    two digit second of minute, three digit millis, and time zone offset:
+    `HHmmss.SSSZ`.
+
+`basic_time_no_millis`::
+
+    A basic formatter for a two digit hour of day, two digit minute of hour,
+    two digit second of minute, and time zone offset: `HHmmssZ`.
+
+`basic_t_time`::
+
+    A basic formatter for a two digit hour of day, two digit minute of hour,
+    two digit second of minute, three digit millis, and time zone off set
+    prefixed by 'T': `'T'HHmmss.SSSZ`.
+
+`basic_t_time_no_millis`::
+
+    A basic formatter for a two digit hour of day, two digit minute of hour,
+    two digit second of minute, and time zone offset prefixed by 'T':
+    `'T'HHmmssZ`.
+
+`basic_week_date` or `strict_basic_week_date`::
+
+    A basic formatter for a full date as four digit weekyear, two digit week
+    of weekyear, and one digit day of week: `xxxx'W'wwe`.
+
+`basic_week_date_time` or `strict_basic_week_date_time`::
+
+    A basic formatter that combines a basic weekyear date and time, separated
+    by a 'T': `xxxx'W'wwe'T'HHmmss.SSSZ`.
+
+`basic_week_date_time_no_millis` or `strict_basic_week_date_time_no_millis`::
+
+    A basic formatter that combines a basic weekyear date and time without
+    millis, separated by a 'T': `xxxx'W'wwe'T'HHmmssZ`.
+
+`date` or `strict_date`::
+
+    A formatter for a full date as four digit year, two digit month of year,
+    and two digit day of month: `yyyy-MM-dd`.
+
+`date_hour` or `strict_date_hour`::
+
+    A formatter that combines a full date and two digit hour of day.
+
+`date_hour_minute` or `strict_date_hour_minute`::
+
+    A formatter that combines a full date, two digit hour of day, and two
+    digit minute of hour.
+
+`date_hour_minute_second` or `strict_date_hour_minute_second`::
+
+    A formatter that combines a full date, two digit hour of day, two digit
+    minute of hour, and two digit second of minute.
+
+`date_hour_minute_second_fraction` or `strict_date_hour_minute_second_fraction`::
+
+    A formatter that combines a full date, two digit hour of day, two digit
+    minute of hour, two digit second of minute, and three digit fraction of
+    second: `yyyy-MM-dd'T'HH:mm:ss.SSS`.
+
+`date_hour_minute_second_millis` or `strict_date_hour_minute_second_millis`::
+
+    A formatter that combines a full date, two digit hour of day, two digit
+    minute of hour, two digit second of minute, and three digit fraction of
+    second: `yyyy-MM-dd'T'HH:mm:ss.SSS`.
+
+`date_time` or `strict_date_time`::
+
+    A formatter that combines a full date and time, separated by a 'T': `yyyy-
+    MM-dd'T'HH:mm:ss.SSSZZ`.
+
+`date_time_no_millis` or `strict_date_time_no_millis`::
+
+    A formatter that combines a full date and time without millis, separated
+    by a 'T': `yyyy-MM-dd'T'HH:mm:ssZZ`.
+
+`hour` or `strict_hour`::
+
+    A formatter for a two digit hour of day.
+
+`hour_minute` or `strict_hour_minute`::
+
+    A formatter for a two digit hour of day and two digit minute of hour.
+
+`hour_minute_second` or `strict_hour_minute_second`::
+
+    A formatter for a two digit hour of day, two digit minute of hour, and two
+    digit second of minute.
+
+`hour_minute_second_fraction` or `strict_hour_minute_second_fraction`::
+
+    A formatter for a two digit hour of day, two digit minute of hour, two
+    digit second of minute, and three digit fraction of second: `HH:mm:ss.SSS`.
+
+`hour_minute_second_millis` or `strict_hour_minute_second_millis`::
+
+    A formatter for a two digit hour of day, two digit minute of hour, two
+    digit second of minute, and three digit fraction of second: `HH:mm:ss.SSS`.
+
+`ordinal_date` or `strict_ordinal_date`::
+
+    A formatter for a full ordinal date, using a four digit year and three
+    digit dayOfYear: `yyyy-DDD`.
+
+`ordinal_date_time` or `strict_ordinal_date_time`::
+
+    A formatter for a full ordinal date and time, using a four digit year and
+    three digit dayOfYear: `yyyy-DDD'T'HH:mm:ss.SSSZZ`.
+
+`ordinal_date_time_no_millis` or `strict_ordinal_date_time_no_millis`::
+
+    A formatter for a full ordinal date and time without millis, using a four
+    digit year and three digit dayOfYear: `yyyy-DDD'T'HH:mm:ssZZ`.
+
+`time` or `strict_time`::
+
+    A formatter for a two digit hour of day, two digit minute of hour, two
+    digit second of minute, three digit fraction of second, and time zone
+    offset: `HH:mm:ss.SSSZZ`.
+
+`time_no_millis` or `strict_time_no_millis`::
+
+    A formatter for a two digit hour of day, two digit minute of hour, two
+    digit second of minute, and time zone offset: `HH:mm:ssZZ`.
+
+`t_time` or `strict_t_time`::
+
+    A formatter for a two digit hour of day, two digit minute of hour, two
+    digit second of minute, three digit fraction of second, and time zone
+    offset prefixed by 'T': `'T'HH:mm:ss.SSSZZ`.
+
+`t_time_no_millis` or `strict_t_time_no_millis`::
+
+    A formatter for a two digit hour of day, two digit minute of hour, two
+    digit second of minute, and time zone offset prefixed by 'T': `'T'HH:mm:ssZZ`.
+
+`week_date` or `strict_week_date`::
+
+    A formatter for a full date as four digit weekyear, two digit week of
+    weekyear, and one digit day of week: `xxxx-'W'ww-e`.
+
+`week_date_time` or `strict_week_date_time`::
+
+    A formatter that combines a full weekyear date and time, separated by a
+    'T': `xxxx-'W'ww-e'T'HH:mm:ss.SSSZZ`.
+
+`week_date_time_no_millis` or `strict_week_date_time_no_millis`::
+
+    A formatter that combines a full weekyear date and time without millis,
+    separated by a 'T': `xxxx-'W'ww-e'T'HH:mm:ssZZ`.
+
+`weekyear` or `strict_weekyear`::
+
+    A formatter for a four digit weekyear.
+
+`weekyear_week` or `strict_weekyear_week`::
+
+    A formatter for a four digit weekyear and two digit week of weekyear.
+
+`weekyear_week_day` or `strict_weekyear_week_day`::
+
+    A formatter for a four digit weekyear, two digit week of weekyear, and one
+    digit day of week.
+
+`year` or `strict_year`::
+
+    A formatter for a four digit year.
+
+`year_month` or `strict_year_month`::
+
+    A formatter for a four digit year and two digit month of year.
+
+`year_month_day` or `strict_year_month_day`::
+
+    A formatter for a four digit year, two digit month of year, and two digit
+    day of month.
+
diff --git a/docs/reference/mapping/params/geohash-precision.asciidoc b/docs/reference/mapping/params/geohash-precision.asciidoc
new file mode 100644
index 0000000..c571a70
--- /dev/null
+++ b/docs/reference/mapping/params/geohash-precision.asciidoc
@@ -0,0 +1,60 @@
+[[geohash-precision]]
+=== `geohash_precision`
+
+Geohashes are a form of lat/lon encoding which divides the earth up into
+a grid. Each cell in this grid is represented by a geohash string. Each
+cell in turn can be further subdivided into smaller cells which are
+represented by a longer string. So the longer the geohash, the smaller
+(and thus more accurate) the cell is.
+
+The `geohash_precision` setting controls the length of the geohash that is
+indexed when the <<geohash,`geohash`>> option is enabled, and the maximum
+geohash length when the <<geohash-prefix,`geohash_prefix`>> option is enabled.
+
+It accepts:
+
+* a number between 1 and 12 (default), which represents the length of the geohash.
+* a <<distance-units,distance>>, e.g. `1km`.
+
+If a distance is specified, it will be translated to the smallest
+geohash-length that will provide the requested resolution.
+
+For example:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "location": {
+          "type": "geo_point",
+          "geohash_prefix": true,
+          "geohash_precision": 6 <1>
+        }
+      }
+    }
+  }
+}
+
+PUT my_index/my_type/1
+{
+  "location": {
+    "lat": 41.12,
+    "lon": -71.34
+  }
+}
+
+GET my_index/_search?fielddata_fields=location.geohash
+{
+  "query": {
+    "term": {
+      "location.geohash": "drm3bt"
+    }
+  }
+}
+
+--------------------------------------------------
+// AUTOSENSE
+<1> A `geohash_precision` of 6 equates to geohash cells of approximately 1.26km x 0.6km
diff --git a/docs/reference/mapping/params/geohash-prefix.asciidoc b/docs/reference/mapping/params/geohash-prefix.asciidoc
new file mode 100644
index 0000000..da4708f
--- /dev/null
+++ b/docs/reference/mapping/params/geohash-prefix.asciidoc
@@ -0,0 +1,64 @@
+[[geohash-prefix]]
+=== `geohash_prefix`
+
+Geohashes are a form of lat/lon encoding which divides the earth up into
+a grid. Each cell in this grid is represented by a geohash string. Each
+cell in turn can be further subdivided into smaller cells which are
+represented by a longer string. So the longer the geohash, the smaller
+(and thus more accurate) the cell is.
+
+While the <<geohash,`geohash`>> option enables indexing the geohash that
+corresponds to the lat/lon point, at the specified
+<<geohash-precision,precision>>, the `geohash_prefix` option will also
+index all the enclosing cells as well.
+
+For instance, a geohash of `drm3btev3e86` will index all of the following
+terms: [ `d`, `dr`, `drm`, `drm3`, `drm3b`, `drm3bt`, `drm3bte`, `drm3btev`,
+`drm3btev3`, `drm3btev3e`, `drm3btev3e8`, `drm3btev3e86` ].
+
+The geohash prefixes can be used with the
+<<query-dsl-geohash-cell-query,`geohash_cell` query>> to find points within a
+particular geohash, or its neighbours:
+
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "location": {
+          "type": "geo_point",
+          "geohash_prefix": true,
+          "geohash_precision": 6
+        }
+      }
+    }
+  }
+}
+
+PUT my_index/my_type/1
+{
+  "location": {
+    "lat": 41.12,
+    "lon": -71.34
+  }
+}
+
+GET my_index/_search?fielddata_fields=location.geohash
+{
+  "query": {
+    "geohash_cell": {
+      "location": {
+        "lat": 41.02,
+        "lon": -71.48
+      },
+      "precision": 4, <1>
+      "neighbors": true <1>
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+
diff --git a/docs/reference/mapping/params/geohash.asciidoc b/docs/reference/mapping/params/geohash.asciidoc
new file mode 100644
index 0000000..a1319c0
--- /dev/null
+++ b/docs/reference/mapping/params/geohash.asciidoc
@@ -0,0 +1,70 @@
+[[geohash]]
+=== `geohash`
+
+Geohashes are a form of lat/lon encoding which divides the earth up into
+a grid. Each cell in this grid is represented by a geohash string. Each
+cell in turn can be further subdivided into smaller cells which are
+represented by a longer string. So the longer the geohash, the smaller
+(and thus more accurate) the cell is.
+
+Because geohashes are just strings, they can be stored in an inverted
+index like any other string, which makes querying them very efficient.
+
+If you enable the `geohash` option, a `geohash` ``sub-field'' will be indexed
+as, eg `.geohash`. The length of the geohash is controlled by the
+<<geohash-precision,`geohash_precision`>> parameter.
+
+If the <<geohash-prefix,`geohash_prefix`>> option is enabled, the `geohash`
+option will be enabled automatically.
+
+For example:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "location": {
+          "type": "geo_point", <1>
+          "geohash": true
+        }
+      }
+    }
+  }
+}
+
+
+PUT my_index/my_type/1
+{
+  "location": {
+    "lat": 41.12,
+    "lon": -71.34
+  }
+}
+
+GET my_index/_search?fielddata_fields=location.geohash <2>
+{
+  "query": {
+    "prefix": {
+      "location.geohash": "drm3b" <3>
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> A `location.geohash` field will be indexed for each geo-point.
+<2> The geohash can be retrieved with <<doc-values,`doc_values`>>.
+<3> A <<query-dsl-prefix-query,`prefix`>> query can find all geohashes which start with a particular prefix.
+
+[WARNING]
+============================================
+
+A `prefix` query on geohashes is expensive.  Instead, consider using the
+<<geohash-prefix,`geohash_prefix`>> to pay the expense once at index time
+instead of on every query.
+
+============================================
+
+
diff --git a/docs/reference/mapping/params/ignore-above.asciidoc b/docs/reference/mapping/params/ignore-above.asciidoc
new file mode 100644
index 0000000..61d6e8f
--- /dev/null
+++ b/docs/reference/mapping/params/ignore-above.asciidoc
@@ -0,0 +1,61 @@
+[[ignore-above]]
+=== `ignore_above`
+
+Strings longer than the `ignore_above` setting will not be processed by the
+<<analyzer,analyzer>> and will not be indexed. This is mainly useful for
+<<mapping-index,`not_analyzed`>> string fields, which are typically used for
+filtering, aggregations, and sorting.  These are structured fields and it
+doesn't usually make sense to allow very long terms to be indexed in these
+fields.
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "message": {
+          "type": "string",
+          "index": "not_analyzed",
+          "ignore_above": 20 <1>
+        }
+      }
+    }
+  }
+}
+
+PUT my_index/my_type/1 <2>
+{
+  "message": "Syntax error"
+}
+
+PUT my_index/my_type/2 <3>
+{
+  "message": "Syntax error with some long stacktrace"
+}
+
+GET _search <4>
+{
+  "aggs": {
+    "messages": {
+      "terms": {
+        "field": "message"
+      }
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> This field will ignore any string longer than 20 characters.
+<2> This document is indexed successfully.
+<3> This document will be indexed, but without indexing the `message` field.
+<4> Search returns both documents, but only the first is present in the terms aggregation.
+
+This option is also useful for protecting against Lucene's term byte-length
+limit of `32766`.
+
+NOTE: The value for `ignore_above` is the _character count_, but Lucene counts
+bytes. If you use UTF-8 text with many non-ASCII characters, you may want to
+set the limit to `32766 / 3 = 10922` since UTF-8 characters may occupy at most
+3 bytes.
diff --git a/docs/reference/mapping/params/ignore-malformed.asciidoc b/docs/reference/mapping/params/ignore-malformed.asciidoc
new file mode 100644
index 0000000..359c51e
--- /dev/null
+++ b/docs/reference/mapping/params/ignore-malformed.asciidoc
@@ -0,0 +1,83 @@
+[[ignore-malformed]]
+=== `ignore_malformed`
+
+Sometimes you don't have much control over the data that you receive.  One
+user may send a `login` field that is a <<date,`date`>>, and another sends a
+`login` field that is an email address.
+
+Trying to index the wrong datatype into a field throws an exception by
+default, and rejects the whole document.  The `ignore_malformed` parameter, if
+set to `true`, allows the exception to be ignored.  The malformed field is not
+indexed, but other fields in the document are processed normally.
+
+For example:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "number_one": {
+          "type": "integer"
+        },
+        "number_two": {
+          "type": "integer",
+          "ignore_malformed": true
+        }
+      }
+    }
+  }
+}
+
+PUT my_index/my_type/1
+{
+  "text":       "Some text value",
+  "number_one": "foo" <1>
+}
+
+PUT my_index/my_type/2
+{
+  "text":       "Some text value",
+  "number_two": "foo" <2>
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> This document will be rejected because `number_one` does not allow malformed values.
+<2> This document will have the `text` field indexed, but not the `number_two` field.
+
+
+[[ignore-malformed-setting]]
+==== Index-level default
+
+The `index.mapping.ignore_malformed` setting can be set on the index level to
+allow to ignore malformed content globally across all mapping types.
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "settings": {
+    "index.mapping.ignore_malformed": true <1>
+  },
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "number_one": { <1>
+          "type": "byte"
+        },
+        "number_two": {
+          "type": "integer",
+          "ignore_malformed": false <2>
+        }
+      }
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+
+<1> The `number_one` field inherits the index-level setting.
+<2> The `number_two` field overrides the index-level setting to turn off `ignore_malformed`.
+
diff --git a/docs/reference/mapping/params/include-in-all.asciidoc b/docs/reference/mapping/params/include-in-all.asciidoc
new file mode 100644
index 0000000..09e4266
--- /dev/null
+++ b/docs/reference/mapping/params/include-in-all.asciidoc
@@ -0,0 +1,83 @@
+[[include-in-all]]
+=== `include_in_all`
+
+The `include_in_all` parameter provides per-field control over which fields
+are included in the <<mapping-all-field,`_all`>> field.  It defaults to `true`, unless <<mapping-index,`index`>> is set to `no`.
+
+This example demonstrates how to exclude the `date` field from the `_all` field:
+
+[source,js]
+--------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "title": { <1>
+          "type": "string"
+        }
+        "content": { <1>
+          "type": "string"
+        },
+        "date": { <2>
+          "type": "date",
+          "include_in_all": false
+        }
+      }
+    }
+  }
+}
+--------------------------------
+// AUTOSENSE
+
+<1> The `title` and `content` fields with be included in the `_all` field.
+<2> The `date` field will not be included in the `_all` field.
+
+The `include_in_all` parameter can also be set at the type level and on
+<<object,`object`>> or <<nested,`nested`>> fields, in which case all sub-
+fields inherit that setting.  For instance:
+
+[source,js]
+--------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "include_in_all": false, <1>
+      "properties": {
+        "title":          { "type": "string" },
+        "author": {
+          "include_in_all": true, <2>
+          "properties": {
+            "first_name": { "type": "string" },
+            "last_name":  { "type": "string" }
+          }
+        },
+        "editor": {
+          "properties": {
+            "first_name": { "type": "string" }, <3>
+            "last_name":  { "type": "string", "include_in_all": true } <3>
+          }
+        }
+      }
+    }
+  }
+}
+--------------------------------
+// AUTOSENSE
+
+<1> All fields in `my_type` are excluded from `_all`.
+<2> The `author.first_name` and `author.last_name` fields are included in `_all`.
+<3> Only the `editor.last_name` field is included in `_all`.
+    The `editor.first_name` inherits the type-level setting and is excluded.
+
+[NOTE]
+.Multi-fields and `include_in_all`
+=================================
+
+The original field value is added to the `_all` field, not the terms produced
+by a field's analyzer.  For this reason, it makes no sense to set
+`include_in_all` to `true` on <<multi-fields,multi-fields>>, as each
+multi-field has exactly the same value as its parent.
+
+=================================
diff --git a/docs/reference/mapping/params/index-options.asciidoc b/docs/reference/mapping/params/index-options.asciidoc
new file mode 100644
index 0000000..9e09660
--- /dev/null
+++ b/docs/reference/mapping/params/index-options.asciidoc
@@ -0,0 +1,70 @@
+[[index-options]]
+=== `index_options`
+
+The `index_options` parameter controls what information is added to the
+inverted index, for search and highlighting purposes.  It accepts the
+following settings:
+
+[horizontal]
+`docs`::
+
+    Only the doc number is indexed.  Can answer the question _Does this term
+    exist in this field?_
+
+`freqs`::
+
+    Doc number and term frequencies are indexed.  Term frequencies are used to
+    score repeated terms higher than single terms.
+
+`positions`::
+
+    Doc number, term frequencies, and term positions (or order) are indexed.
+    Positions can be used for
+    <<query-dsl-match-query-phrase,proximity or phrase queries>>.
+
+`offsets`::
+
+    Doc number, term frequencies, positions, and start and end character
+    offsets (which map the term back to the original string) are indexed.
+    Offsets are used by the <<postings-highlighter,postings highlighter>>.
+
+<<mapping-index,Analyzed>> string fields use `positions` as the default, and
+<<all other fields use `docs` as the default.
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "text": {
+          "type": "string",
+          "index_options": "offsets"
+        }
+      }
+    }
+  }
+}
+
+PUT my_index/my_type/1
+{
+  "text": "Quick brown fox"
+}
+
+GET my_index/_search
+{
+  "query": {
+    "match": {
+      "text": "brown fox"
+    }
+  },
+  "highlight": {
+    "fields": {
+      "text": {} <1>
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> The `text` field will use the postings highlighter by default because `offsets` are indexed.
diff --git a/docs/reference/mapping/params/index.asciidoc b/docs/reference/mapping/params/index.asciidoc
new file mode 100644
index 0000000..979ead0
--- /dev/null
+++ b/docs/reference/mapping/params/index.asciidoc
@@ -0,0 +1,48 @@
+[[mapping-index]]
+=== `index`
+
+The `index` option controls how field values are indexed and, thus, how they
+are searchable.  It accepts three values:
+
+[horizontal]
+`no`::
+
+    Do not add this field value to the index. With this setting, the field
+    will not be queryable.
+
+`not_analyzed`::
+
+    Add the field value to the index unchanged, as a single term.  This is the
+    default for all fields that support this option except for
+    <<string,`string`>> fields.  `not_analyzed` fields are usually used with
+    <<term-level-queries,term-level queries>> for structured search.
+
+`analyzed`::
+
+    This option applies only to `string` fields, for which it is the default.
+    The string field value is first <<analysis,analyzed>> to convert the
+    string into terms (e.g. a list of individual words), which are then
+    indexed.  At search time, the the query string is passed through
+    (<<search-analyzer,usually>>) the same analyzer to generate terms
+    in the same format as those in the index.  It is this process that enables
+    <<full-text-queries,full text search>>.
+
+For example, you can create a `not_analyzed` string field with the following:
+
+[source,js]
+--------------------------------------------------
+PUT /my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "status_code": {
+          "type": "string",
+          "index": "not_analyzed"
+        }
+      }
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
\ No newline at end of file
diff --git a/docs/reference/mapping/params/lat-lon.asciidoc b/docs/reference/mapping/params/lat-lon.asciidoc
new file mode 100644
index 0000000..0a5cce4
--- /dev/null
+++ b/docs/reference/mapping/params/lat-lon.asciidoc
@@ -0,0 +1,63 @@
+[[lat-lon]]
+=== `lat_lon`
+
+<<geo-queries,Geo-queries>> are usually performed by plugging the value of
+each <<geo-point,`geo_point`>> field into a formula to determine whether it
+falls into the required area or not. Unlike most queries, the inverted index
+is not involved.
+
+Setting `lat_lon` to `true` causes the latitude and longitude values to be
+indexed as numeric fields (called `.lat` and `.lon`). These fields can be used
+by the <<query-dsl-geo-bounding-box-query,`geo_bounding_box`>> and
+<<query-dsl-geo-distance-query,`geo_distance`>> queries instead of
+performing in-memory calculations.
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "location": {
+          "type": "geo_point",
+          "lat_lon": true <1>
+        }
+      }
+    }
+  }
+}
+
+PUT my_index/my_type/1
+{
+  "location": {
+    "lat": 41.12,
+    "lon": -71.34
+  }
+}
+
+
+GET my_index/_search
+{
+  "query": {
+    "geo_distance": {
+      "location": {
+        "lat": 41,
+        "lon": -71
+      },
+      "distance": "50km",
+      "optimize_bbox": "indexed" <2>
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> Setting `lat_lon` to true indexes the geo-point in the `location.lat` and `location.lon` fields.
+<2> The `indexed` option tells the geo-distance query to use the inverted index instead of the in-memory calculation.
+
+Whether the in-memory or indexed operation performs better depends both on
+your dataset and on the types of queries that you are running.
+
+NOTE: The `lat_lon` option only makes sense for single-value `geo_point`
+fields. It will not work with arrays of geo-points.
+
diff --git a/docs/reference/mapping/params/multi-fields.asciidoc b/docs/reference/mapping/params/multi-fields.asciidoc
new file mode 100644
index 0000000..1287fa3
--- /dev/null
+++ b/docs/reference/mapping/params/multi-fields.asciidoc
@@ -0,0 +1,132 @@
+[[multi-fields]]
+=== `fields`
+
+It is often useful to index the same field in different ways for different
+purposes.  This is the purpose of _multi-fields_. For instance, a `string`
+field could be <<mapping-index,indexed>> as an `analyzed` field for full-text
+search, and as a `not_analyzed` field for sorting or aggregations:
+
+[source,js]
+--------------------------------------------------
+PUT /my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "city": {
+          "type": "string",
+          "fields": {
+            "raw": { <1>
+              "type":  "string",
+              "index": "not_analyzed"
+            }
+          }
+        }
+      }
+    }
+  }
+}
+
+PUT /my_index/my_type/1
+{
+  "city": "New York"
+}
+
+PUT /my_index/my_type/2
+{
+  "city": "York"
+}
+
+GET /my_index/_search
+{
+  "query": {
+    "match": {
+      "city": "york" <2>
+    }
+  },
+  "sort": {
+    "city.raw": "asc" <3>
+  },
+  "aggs": {
+    "Cities": {
+      "terms": {
+        "field": "city.raw" <3>
+      }
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> The `city.raw` field is a `not_analyzed` version of the `city` field.
+<2> The analyzed `city` field can be used for full text search.
+<3> The `city.raw` field can be used for sorting and aggregations
+
+NOTE: Multi-fields do not change the original `_source` field.
+
+==== Multi-fields with multiple analyzers
+
+Another use case of multi-fields is to analyze the same field in different
+ways for better relevance. For instance we could index a field with the
+<<analysis-standard-analyzer,`standard` analyzer>> which breaks text up into
+words, and again with the <<english-analyzer,`english` analyzer>>
+which stems words into their root form:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "text": { <1>
+          "type": "string"
+        },
+        "fields": {
+          "english": { <2>
+            "type":     "string",
+            "analyzer": "english"
+          }
+        }
+      }
+    }
+  }
+}
+
+PUT my_index/my_type/1
+{ "text": "quick brown fox" } <3>
+
+PUT my_index/my_type/2
+{ "text": "quick brown foxes" } <3>
+
+GET my_index/_search
+{
+  "query": {
+    "multi_match": {
+      "query": "quick brown foxes",
+      "fields": [ <4>
+        "text",
+        "text.english"
+      ],
+      "type": "most_fields" <4>
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+
+<1> The `text` field uses the `standard` analyzer.
+<2> The `text.english` field uses the `english` analyzer.
+<3> Index two documents, one with `fox` and the other with `foxes`.
+<4> Query both the `text` and `text.english` fields and combine the scores.
+
+The `text` field contains the term `fox` in the first document and `foxes` in
+the second document.  The `text.english` field contains `fox` for both
+documents, because `foxes` is stemmed to `fox`.
+
+The query string is also analyzed by the `standard` analyzer for the `text`
+field, and by the `english` analyzer` for the `text.english` field.  The
+stemmed field allows a query for `foxes` to also match the document containing
+just `fox`.  This allows us to match as many documents as possible.  By also
+querying the unstemmed `text` field, we improve the relevance score of the
+document which matches `foxes` exactly.
+
diff --git a/docs/reference/mapping/params/norms.asciidoc b/docs/reference/mapping/params/norms.asciidoc
new file mode 100644
index 0000000..0a28f5a
--- /dev/null
+++ b/docs/reference/mapping/params/norms.asciidoc
@@ -0,0 +1,64 @@
+[[norms]]
+=== `norms`
+
+Norms store various normalization factors -- a number to represent the
+relative field length and the <<index-boost,index time `boost`>> setting --
+that are later used at query time in order to compute the score of a document
+relatively to a query.
+
+Although useful for scoring, norms also require quite a lot of memory
+(typically in the order of one byte per document per field in your index, even
+for documents that don't have this specific field). As a consequence, if you
+don't need scoring on a specific field, you should disable norms on that
+field. In  particular, this is the case for fields that are used solely for
+filtering or aggregations.
+
+Norms can be disabled (but not reenabled) after the fact, using the
+<<indices-put-mapping,PUT mapping API>> like so:
+
+[source,js]
+------------
+PUT my_index/_mapping/my_type
+{
+  "properties": {
+    "title": {
+      "type": "string",
+      "norms": {
+        "enabled": false
+      }
+    }
+  }
+}
+------------
+// AUTOSENSE
+
+NOTE: Norms will not be removed instantly, but will be removed as old segments
+are merged into new segments as you continue indexing new documents. Any score
+computation on a field that has had norms removed might return inconsistent
+results since some documents won't have norms anymore while other documents
+might still have norms.
+
+==== Lazy loading of norms
+
+Norms can be loaded into memory eagerly (`eager`), whenever a new segment
+comes online, or they can loaded lazily (`lazy`, default), only when the field
+is queried.
+
+Eager loading can be configured as follows:
+
+[source,js]
+------------
+PUT my_index/_mapping/my_type
+{
+  "properties": {
+    "title": {
+      "type": "string",
+      "norms": {
+        "loading": "eager"
+      }
+    }
+  }
+}
+------------
+// AUTOSENSE
+
diff --git a/docs/reference/mapping/params/null-value.asciidoc b/docs/reference/mapping/params/null-value.asciidoc
new file mode 100644
index 0000000..09584ed
--- /dev/null
+++ b/docs/reference/mapping/params/null-value.asciidoc
@@ -0,0 +1,58 @@
+[[null-value]]
+=== `null_value`
+
+A `null` value cannot be indexed or searched.  When a field is set to `null`,
+(or an empty array or an array of `null` values)  it is treated as though that
+field has no values.
+
+The `null_value` parameter allows you to replace explicit `null` values with
+the specified value so that it can be indexed and searched.  For instance:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "status_code": {
+          "type":       "string",
+          "index":      "not_analyzed",
+          "null_value": "NULL" <1>
+        }
+      }
+    }
+  }
+}
+
+PUT my_index/my_type/1
+{
+  "status_code": null
+}
+
+PUT my_index/my_type/2
+{
+  "status_code": [] <2>
+}
+
+GET my_index/_search
+{
+  "query": {
+    "term": {
+      "status_code": "NULL" <3>
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> Replace explicit `null` values with the term `NULL`.
+<2> An empty array does not contain an explicit `null`, and so won't be replaced with the `null_value`.
+<3> A query for `NULL` returns document 1, but not document 2.
+
+IMPORTANT: The `null_value` needs to be the same datatype as the field.  For
+instance, a `long` field cannot have a string `null_value`.  String fields
+which are `analyzed` will also pass the `null_value` through the configured
+analyzer.
+
+Also see the <<query-dsl-missing-query,`missing` query>> for its `null_value` support.
+
diff --git a/docs/reference/mapping/params/position-offset-gap.asciidoc b/docs/reference/mapping/params/position-offset-gap.asciidoc
new file mode 100644
index 0000000..0e908dd
--- /dev/null
+++ b/docs/reference/mapping/params/position-offset-gap.asciidoc
@@ -0,0 +1,68 @@
+[[position-offset-gap]]
+=== `position_offset_gap`
+
+<<mapping-index,Analyzed>> string fields take term <<index-options,positions>>
+into account, in order to be able to support
+<<query-dsl-match-query-phrase,proximity or phrase queries>>.
+When indexing an array of strings, each string of the array is indexed
+directly after the previous one, almost as though all the strings in the array
+had been concatenated into one big string.
+
+This can result in matches from phrase queries spanning two array elements.
+For instance:
+
+[source,js]
+--------------------------------------------------
+PUT /my_index/groups/1
+{
+    "names": [ "John Abraham", "Lincoln Smith"]
+}
+
+GET /my_index/groups/_search
+{
+    "query": {
+        "match_phrase": {
+            "names": "Abraham Lincoln" <1>
+        }
+    }
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> This phrase query matches our document, even though `Abraham` and `Lincoln` are in separate strings.
+
+The `position_offset_gap` can introduce a fake gap between each array element.  For instance:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "names": {
+          "type": "string",
+          "position_offset_gap": 50 <1>
+        }
+      }
+    }
+  }
+}
+
+PUT /my_index/groups/1
+{
+    "names": [ "John Abraham", "Lincoln Smith"]
+}
+
+GET /my_index/groups/_search
+{
+    "query": {
+        "match_phrase": {
+            "names": "Abraham Lincoln" <2>
+        }
+    }
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> The first term in the next array element will be 50 terms apart from the
+    last term in the previous array element.
+<2> The phrase query no longer matches our document.
\ No newline at end of file
diff --git a/docs/reference/mapping/params/precision-step.asciidoc b/docs/reference/mapping/params/precision-step.asciidoc
new file mode 100644
index 0000000..a7ea4d3
--- /dev/null
+++ b/docs/reference/mapping/params/precision-step.asciidoc
@@ -0,0 +1,56 @@
+[[precision-step]]
+=== `precision_step`
+
+Most <<number,numeric>> datatypes index extra terms representing numeric
+ranges for each number to make <<query-dsl-range-query,`range` queries>>
+faster.  For instance, this `range` query:
+
+[source,js]
+--------------------------------------------------
+  "range": {
+    "number": {
+      "gte": 0
+      "lte": 321
+    }
+  }
+--------------------------------------------------
+
+might be executed internally as a <<query-dsl-terms-query,`terms` query>> that
+looks something like this:
+
+[source,js]
+--------------------------------------------------
+  "terms": {
+    "number": [
+      "0-255",
+      "256-319"
+      "320",
+      "321"
+    ]
+  }
+--------------------------------------------------
+
+These extra terms greatly reduce the number of terms that have to be examined,
+at the cost of increased disk space.
+
+The default value for `precision_step` depends on the `type` of the numeric field:
+
+[horizontal]
+`long`, `double`, `date`, `ip`::  `16` (3 extra terms)
+`integer`, `float`, `short`::     `8` (3 extra terms)
+`byte`::                          `2147483647` (0 extra terms)
+`token_count`::                   `32` (0 extra terms)
+
+The value of the `precision_step` setting indicates the number of bits that
+should be compressed into an extra term.  A `long` value consists of 64 bits,
+so a `precision_step` of 16 results in the following terms:
+
+[horizontal]
+Bits 0-15:: `value & 1111111111111111 0000000000000000 0000000000000000 0000000000000000`
+Bits 0-31:: `value & 1111111111111111 1111111111111111 0000000000000000 0000000000000000`
+Bits 0-47:: `value & 1111111111111111 1111111111111111 1111111111111111 0000000000000000`
+Bits 0-63:: `value`
+
+
+
+
diff --git a/docs/reference/mapping/params/properties.asciidoc b/docs/reference/mapping/params/properties.asciidoc
new file mode 100644
index 0000000..c340ce4
--- /dev/null
+++ b/docs/reference/mapping/params/properties.asciidoc
@@ -0,0 +1,101 @@
+[[properties]]
+=== `properties`
+
+Type mappings, <<object,`object` fields>> and <<nested,`nested` fields>>
+contain sub-fields, called `properties`. These properties may be of any
+<<mapping-types,datatype>>, including `object` and `nested`.  Properties can
+be added:
+
+* explicitly by defining them when <<indices-create-index,creating an index>>.
+* explicitily by defining them when adding or updating a mapping type with the <<indices-put-mapping,PUT mapping>> API.
+* <<dynamic-mapping,dynamically>> just by indexing documents containing new fields.
+
+Below is an example of adding `properties` to a mapping type, an `object`
+field, and a `nested` field:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": { <1>
+      "properties": {
+        "manager": { <2>
+          "properties": {
+            "age":  { "type": "integer" },
+            "name": { "type": "string"  }
+          }
+        },
+        "employees": { <3>
+          "type": "nested",
+          "properties": {
+            "age":  { "type": "integer" },
+            "name": { "type": "string"  }
+          }
+        }
+      }
+    }
+  }
+}
+
+PUT my_index/my_type/1 <4>
+{
+  "region": "US",
+  "manager": {
+    "name": "Alice White",
+    "age": 30
+  },
+  "employees": [
+    {
+      "name": "John Smith",
+      "age": 34
+    },
+    {
+      "name": "Peter Brown",
+      "age": 26
+    }
+  ]
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> Properties under the `my_type` mapping type.
+<2> Properties under the `manager` object field.
+<3> Properties under the `employees` nested field.
+<4> An example document which corresponds to the above mapping.
+
+==== Dot notation
+
+Inner fields can be referred to in queries, aggregations, etc., using _dot
+notation_:
+
+[source,js]
+--------------------------------------------------
+GET my_index/_search
+{
+  "query": {
+    "match": {
+      "manager.name": "Alice White" <1>
+    }
+  },
+  "aggs": {
+    "Employees": {
+      "nested": {
+        "path": "employees"
+      },
+      "aggs": {
+        "Employee Ages": {
+          "histogram": {
+            "field": "employees.age", <2>
+            "interval": 5
+          }
+        }
+      }
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+
+IMPORTANT: The full path to the inner field must be specified.
+
+
diff --git a/docs/reference/mapping/params/search-analyzer.asciidoc b/docs/reference/mapping/params/search-analyzer.asciidoc
new file mode 100644
index 0000000..2ebf3f8
--- /dev/null
+++ b/docs/reference/mapping/params/search-analyzer.asciidoc
@@ -0,0 +1,79 @@
+[[search-analyzer]]
+=== `search_analyzer`
+
+Usually, the same <<analyzer,analyzer>> should be applied at index time and at
+search time, to ensure that the terms in the query are in the same format as
+the terms in the inverted index.
+
+Sometimes, though, it can make sense to use a different analyzer at search
+time, such as when using the  <<analysis-edgengram-tokenizer,`edge_ngram`>>
+tokenizer for autocomplete.
+
+By default, queries will use the `analyzer` defined in the field mapping, but
+this can be overridden with the `search_analyzer` setting:
+
+[source,js]
+--------------------------------------------------
+PUT /my_index
+{
+  "settings": {
+    "analysis": {
+      "filter": {
+        "autocomplete_filter": {
+          "type": "edge_ngram",
+          "min_gram": 1,
+          "max_gram": 20
+        }
+      },
+      "analyzer": {
+        "autocomplete": { <1>
+          "type": "custom",
+          "tokenizer": "standard",
+          "filter": [
+            "lowercase",
+            "autocomplete_filter"
+          ]
+        }
+      }
+    }
+  },
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "text": {
+          "type": "string",
+          "analyzer": "autocomplete", <2>
+          "search_analyzer": "standard" <2>
+        }
+      }
+    }
+  }
+}
+
+PUT my_index/my_type/1
+{
+  "text": "Quick Brown Fox" <3>
+}
+
+GET my_index/_search
+{
+  "query": {
+    "match": {
+      "text": {
+        "query": "Quick Br", <4>
+        "operator": "and"
+      }
+    }
+  }
+}
+
+--------------------------------------------------
+// AUTOSENSE
+
+<1> Analysis settings to define the custom `autocomplete` analyzer.
+<2> The `text` field uses the `autocomplete` analyzer at index time, but the `standard` analyzer at search time.
+<3> This field is indexed as the terms: [ `q`, `qu`, `qui`, `quic`, `quick`, `b`, `br`, `bro`, `brow`, `brown`, `f`, `fo`, `fox` ]
+<4> The query searches for both of these terms: [ `quick`, `br` ]
+
+See {defguide}/_index_time_search_as_you_type.html[Index time search-as-you-
+type] for a full explanation of this example.
diff --git a/docs/reference/mapping/params/similarity.asciidoc b/docs/reference/mapping/params/similarity.asciidoc
new file mode 100644
index 0000000..393f654
--- /dev/null
+++ b/docs/reference/mapping/params/similarity.asciidoc
@@ -0,0 +1,54 @@
+[[similarity]]
+=== `similarity`
+
+Elasticsearch allows you to configure a scoring algorithm or _similarity_ per
+field. The `similarity` setting provides a simple way of choosing a similarity
+algorithm other than the default TF/IDF, such as `BM25`.
+
+Similarities are mostly useful for <<string,`string`>> fields, especially
+`analyzed` string fields, but can also apply to other field types.
+
+Custom similarites can be configured by tuning the parameters of the built-in
+similarities. For more details about this expert options, see the
+<<index-modules-similarity,similarity module>>.
+
+The only similarities which can be used out of the box, without any further
+configuration are:
+
+`default`::
+        The Default TF/IDF algorithm used by Elasticsearch and
+        Lucene. See {defguide}/practical-scoring-function.html[Lucene’s Practical Scoring Function]
+        for more information.
+
+`BM25`::
+        The Okapi BM25 algorithm.
+        See {defguide}/pluggable-similarites.html[Plugggable Similarity Algorithms]
+        for more information.
+
+
+The `similarity` can be set on the field level when a field is first created,
+as follows:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "default_field": { <1>
+          "type": "string"
+        },
+        "bm25_field": {
+          "type": "string",
+          "similarity": "BM25" <2>
+        }
+      }
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> The `default_field` uses the `default` similarity (ie TF/IDF).
+<2> The `bm25_field` uses the `BM25` similarity.
+
diff --git a/docs/reference/mapping/params/store.asciidoc b/docs/reference/mapping/params/store.asciidoc
new file mode 100644
index 0000000..b81208a
--- /dev/null
+++ b/docs/reference/mapping/params/store.asciidoc
@@ -0,0 +1,73 @@
+[[mapping-store]]
+=== `store`
+
+By default, field values <<mapping-index,indexed>> to make them searchable,
+but they are not _stored_.  This means that the field can be queried, but the
+original field value cannot be retrieved.
+
+Usually this doesn't matter.  The field value is already part of the
+<<mapping-source-field,`_source` field>>, which is stored by default. If you
+only want to retrieve the value of a single field or of a few fields, instead
+of the whole `_source`, then this can be achieved with
+<<search-request-source-filtering,source filtering>>.
+
+In certain situations it can make sense to `store` a field.  For instance, if
+you have a document with a `title`, a `date`, and a very large `content`
+field, you may want to retrieve just the `title` and the `date` without having
+to extract those fields from a large `_source` field:
+
+[source,js]
+--------------------------------------------------
+PUT /my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "title": {
+          "type": "string",
+          "store": true <1>
+        },
+        "date": {
+          "type": "date",
+          "store": true <1>
+        },
+        "content": {
+          "type": "string"
+        }
+      }
+    }
+  }
+}
+
+PUT /my_index/my_type/1
+{
+  "title":   "Some short title",
+  "date":    "2015-01-01",
+  "content": "A very long content field..."
+}
+
+GET my_index/_search
+{
+  "fields": [ "title", "date" ] <2>
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> The `title` and `date` fields are stored.
+<2> This request will retrieve the values of the `title` and `date` fields.
+
+[NOTE]
+.Stored fields returned as arrays
+======================================
+
+For consistency, stored fields are always returned as an _array_ because there
+is no way of knowing if the original field value was a single value, multiple
+values, or an empty array.
+
+If you need the original value, you should retrieve it from the `_source`
+field instead.
+
+======================================
+
+Another situation where it can make sense to make a field stored is for those
+that don't appear in the `_source` field (such as <<copy-to,`copy_to` fields>>).
+
diff --git a/docs/reference/mapping/params/term-vector.asciidoc b/docs/reference/mapping/params/term-vector.asciidoc
new file mode 100644
index 0000000..74c4c41
--- /dev/null
+++ b/docs/reference/mapping/params/term-vector.asciidoc
@@ -0,0 +1,68 @@
+[[term-vector]]
+=== `term_vector`
+
+Term vectors contain information about the terms produced by the
+<<analysis,analysis>> process, including:
+
+* a list of terms.
+* the position (or order) of each term.
+* the start and end character offsets mapping the term to its
+  origin in the original string.
+
+These term vectors can be stored so that they can be retrieved for a
+particular document.
+
+The `term_vector` setting accepts:
+
+[horizontal]
+`no`::                      No term vectors are stored. (default)
+`yes`::                     Just the terms in the field are stored.
+`with_positions`::          Terms and positions are stored.
+`with_offsets`::            Terms and character offsets are stored.
+`with_positions_offsets`::  Terms, positions, and character offsets are stored.
+
+The fast vector highlighter requires `with_positions_offsets`.  The term
+vectors API can retrieve whatever is stored.
+
+WARNING:  Setting `with_positions_offsets` will double the size of a field's
+index.
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "text": {
+          "type":        "string",
+          "term_vector": "with_positions_offsets"
+        }
+      }
+    }
+  }
+}
+
+PUT my_index/my_type/1
+{
+  "text": "Quick brown fox"
+}
+
+GET my_index/_search
+{
+  "query": {
+    "match": {
+      "text": "brown fox"
+    }
+  },
+  "highlight": {
+    "fields": {
+      "text": {} <1>
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> The fast vector highlighter will be used by default for the `text` field
+    because term vectors are enabled.
+
diff --git a/docs/reference/mapping/transform.asciidoc b/docs/reference/mapping/transform.asciidoc
deleted file mode 100644
index 0fc8aab..0000000
--- a/docs/reference/mapping/transform.asciidoc
+++ /dev/null
@@ -1,61 +0,0 @@
-[[mapping-transform]]
-== Transform
-The document can be transformed before it is indexed by registering a
-script in the `transform` element of the mapping.  The result of the
-transform is indexed but the original source is stored in the `_source`
-field.  Example:
-
-[source,js]
---------------------------------------------------
-{
-    "example" : {
-        "transform" : {
-            "script" : {
-                "inline": "if (ctx._source['title']?.startsWith('t')) ctx._source['suggest'] = ctx._source['content']",
-                "params" : {
-                    "variable" : "not used but an example anyway"
-                },
-                "lang": "groovy"
-            }
-        },
-        "properties": {
-           "title": { "type": "string" },
-           "content": { "type": "string" },
-           "suggest": { "type": "string" }
-        }
-    }
-}
---------------------------------------------------
-
-Its also possible to specify multiple transforms:
-[source,js]
---------------------------------------------------
-{
-    "example" : {
-        "transform" : [
-            {"script": "ctx._source['suggest'] = ctx._source['content']"}
-            {"script": "ctx._source['foo'] = ctx._source['bar'];"}
-        ]
-    }
-}
---------------------------------------------------
-
-Because the result isn't stored in the source it can't normally be fetched by
-source filtering.  It can be highlighted if it is marked as stored.
-
-=== Get Transformed
-The get endpoint will retransform the source if the `_source_transform`
-parameter is set.  Example:
-
-[source,sh]
---------------------------------------------------
-curl -XGET "http://localhost:9200/test/example/3?pretty&_source_transform"
---------------------------------------------------
-
-The transform is performed before any source filtering but it is mostly
-designed to make it easy to see what was passed to the index for debugging.
-
-=== Immutable Transformation
-Once configured the transform script cannot be modified.  This is not
-because that is technically impossible but instead because madness lies
-down that road.
diff --git a/docs/reference/mapping/types.asciidoc b/docs/reference/mapping/types.asciidoc
index 0cc967e..abefda4 100644
--- a/docs/reference/mapping/types.asciidoc
+++ b/docs/reference/mapping/types.asciidoc
@@ -1,24 +1,85 @@
 [[mapping-types]]
-== Types
+== Field datatypes
 
-The datatype for each field in a document (eg strings, numbers, 
-objects etc) can be controlled via the type mapping.
+Elasticsearch supports a number of different datatypes for the fields in a
+document:
 
-include::types/core-types.asciidoc[]
+[float]
+=== Core datatypes
 
-include::types/array-type.asciidoc[]
+<<string>>::    `string`
+<<number>>::    `long`, `integer`, `short`, `byte`, `double`, `float`
+<<date>>::      `date`
+<<boolean>>::   `boolean`
+<<binary>>::    `binary`
 
-include::types/object-type.asciidoc[]
+[float]
+=== Complex datatypes
 
-include::types/root-object-type.asciidoc[]
+<<array>>::     Array support does not require a dedicated `type`
+<<object>>::    `object` for single JSON objects
+<<nested>>::    `nested` for arrays of JSON objects
+
+[float]
+=== Geo dataypes
+
+<<geo-point>>::     `geo_point` for lat/lon points
+<<geo-shape>>::     `geo_shape` for complex shapes like polygons
+
+[float]
+=== Specialised datatypes
+
+<<ip>>::            `ip` for IPv4 addresses
+<<search-suggesters-completion,Completion datatype>>::
+                    `completion` to provide auto-complete suggestions
+<<token-count>>::   `token_count` to count the number of tokens in a string
+
+Attachment datatype::
+
+    See the https://github.com/elastic/elasticsearch-mapper-attachments[mapper attachment plugin]
+    which supports indexing ``attachments'' like Microsoft Office formats, Open
+    Document formats, ePub, HTML, etc. into an `attachment` datatype.
+
+[float]
+=== Multi-fields
+
+It is often useful to index the same field in different ways for different
+purposes. For instance, a `string` field could be <<mapping-index,indexed>> as
+an `analyzed` field for full-text search, and as a `not_analyzed` field for
+sorting or aggregations.  Alternatively, you could index a string field with
+the <<analysis-standard-analyzer,`standard` analyzer>>, the
+<<english-analyzer,`english`>> analyzer, and the
+<<french-analyzer,`french` analyzer>>.
+
+This is the purpose of _multi-fields_.  Most datatypes support multi-fields
+via the <<multi-fields>> parameter.
+
+include::types/array.asciidoc[]
+
+include::types/binary.asciidoc[]
+
+include::types/boolean.asciidoc[]
+
+include::types/date.asciidoc[]
+
+include::types/geo-point.asciidoc[]
+
+include::types/geo-shape.asciidoc[]
+
+include::types/ip.asciidoc[]
+
+include::types/nested.asciidoc[]
+
+include::types/numeric.asciidoc[]
+
+include::types/object.asciidoc[]
+
+include::types/string.asciidoc[]
+
+include::types/token-count.asciidoc[]
 
-include::types/nested-type.asciidoc[]
 
-include::types/ip-type.asciidoc[]
 
-include::types/geo-point-type.asciidoc[]
 
-include::types/geo-shape-type.asciidoc[]
 
-include::types/attachment-type.asciidoc[]
 
diff --git a/docs/reference/mapping/types/array-type.asciidoc b/docs/reference/mapping/types/array-type.asciidoc
deleted file mode 100644
index f2dc40e..0000000
--- a/docs/reference/mapping/types/array-type.asciidoc
+++ /dev/null
@@ -1,69 +0,0 @@
-[[mapping-array-type]]
-=== Array Type
-
-JSON documents allow to define an array (list) of fields or objects.
-Mapping array types could not be simpler since arrays gets automatically
-detected and mapping them can be done either with
-<<mapping-core-types,Core Types>> or
-<<mapping-object-type,Object Type>> mappings.
-For example, the following JSON defines several arrays:
-
-[source,js]
---------------------------------------------------
-{
-    "tweet" : {
-        "message" : "some arrays in this tweet...",
-        "tags" : ["elasticsearch", "wow"],
-        "lists" : [
-            {
-                "name" : "prog_list",
-                "description" : "programming list"
-            },
-            {
-                "name" : "cool_list",
-                "description" : "cool stuff list"
-            }
-        ]
-    }
-}
---------------------------------------------------
-
-The above JSON has the `tags` property defining a list of a simple
-`string` type, and the `lists` property is an `object` type array. Here
-is a sample explicit mapping:
-
-[source,js]
---------------------------------------------------
-{
-    "tweet" : {
-        "properties" : {
-            "message" : {"type" : "string"},
-            "tags" : {"type" : "string"},
-            "lists" : {
-                "properties" : {
-                    "name" : {"type" : "string"},
-                    "description" : {"type" : "string"}
-                }
-            }
-        }
-    }
-}
---------------------------------------------------
-
-The fact that array types are automatically supported can be shown by
-the fact that the following JSON document is perfectly fine:
-
-[source,js]
---------------------------------------------------
-{
-    "tweet" : {
-        "message" : "some arrays in this tweet...",
-        "tags" : "elasticsearch",
-        "lists" : {
-            "name" : "prog_list",
-            "description" : "programming list"
-        }
-    }
-}
---------------------------------------------------
-
diff --git a/docs/reference/mapping/types/array.asciidoc b/docs/reference/mapping/types/array.asciidoc
new file mode 100644
index 0000000..2422e48
--- /dev/null
+++ b/docs/reference/mapping/types/array.asciidoc
@@ -0,0 +1,99 @@
+[[array]]
+=== Array datatype
+
+In Elasticsearch, there is no dedicated `array` type.  Any field can contain
+zero or more values by default, however, all values in the array must be of
+the same datatype. For instance:
+
+* an array of strings: [ `"one"`, `"two"` ]
+* an array of integers: [ `1`, `2` ]
+* an array of arrays: [ `1`, [ `2`, `3` ]] which is the equivalent of [ `1`, `2`, `3` ]
+* an array of objects: [ `{ "name": "Mary", "age": 12 }`, `{ "name": "John", "age": 10 }`]
+
+.Arrays of objects
+[NOTE]
+====================================================
+
+Arrays of objects do not work as you would expect: you cannot query each
+object independently of the other objects in the array.  If you need to be
+able to do this then you should use the <<nested,`nested`>> datatype instead
+of the <<object,`object`>> datatype.
+
+This is explained in more detail in <<nested>>.
+====================================================
+
+
+When adding a field dynamically, the first value in the array determines the
+field `type`.  All subsequent values must be of the same datatype or it must
+at least be possible to <<coerce,coerce>> subsequent values to the same
+datatype.
+
+Arrays with a mixture of datatypes are _not_ supported: [ `10`, `"some string"` ]
+
+An array may contain `null` values, which are either replaced by the
+configured <<null-value,`null_value`>> or skipped entirely.  An empty array
+`[]` is treated as a missing field -- a field with no values.
+
+Nothing needs to be pre-configured in order to use arrays in documents, they
+are supported out of the box:
+
+
+[source,js]
+--------------------------------------------------
+PUT my_index/my_type/1
+{
+  "message": "some arrays in this document...",
+  "tags":  [ "elasticsearch", "wow" ], <1>
+  "lists": [ <2>
+    {
+      "name": "prog_list",
+      "description": "programming list"
+    },
+    {
+      "name": "cool_list",
+      "description": "cool stuff list"
+    }
+  ]
+}
+
+PUT my_index/my_type/2 <3>
+{
+  "message": "no arrays in this document...",
+  "tags":  "elasticsearch",
+  "lists": {
+    "name": "prog_list",
+    "description": "programming list"
+  }
+}
+
+GET my_index/_search
+{
+  "query": {
+    "match": {
+      "tags": "elasticsearch" <4>
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> The `tags` field is dynamically added as a `string` field.
+<2> The `lists` field is dynamically added as an `object` field.
+<3> The second document contains no arrays, but can be indexed into the same fields.
+<4> The query looks for `elasticsearch` in the `tags` field, and matches both documents.
+
+.Multi-value fields and the inverted index
+****************************************************
+
+The fact that all field types support multi-value fields out of the box is a
+consequence of the origins of Lucene.  Lucene was designed to be a full text
+search engine.  In order to be able to search for individual words within a
+big block of text, Lucene tokenizes the text into individual terms, and
+adds each term to the inverted index separately.
+
+This means that even a simple text field must be able to support multiple
+values by default.  When other datatypes were added, such as numbers and
+dates, they used the same data structure as strings, and so got multi-values
+for free.
+
+****************************************************
+
diff --git a/docs/reference/mapping/types/attachment-type.asciidoc b/docs/reference/mapping/types/attachment-type.asciidoc
deleted file mode 100644
index a8e49b4..0000000
--- a/docs/reference/mapping/types/attachment-type.asciidoc
+++ /dev/null
@@ -1,13 +0,0 @@
-[[mapping-attachment-type]]
-=== Attachment Type
-
-The `attachment` type allows to index different "attachment" type field
-(encoded as `base64`), for example, Microsoft Office formats, open
-document formats, ePub, HTML, and so on.
-
-The `attachment` type is provided as a
-https://github.com/elasticsearch/elasticsearch-mapper-attachments[plugin
-extension]. It uses http://tika.apache.org/[Apache Tika] behind the scene.
-
-See https://github.com/elasticsearch/elasticsearch-mapper-attachments#mapper-attachments-type-for-elasticsearch[README file]
-for details.
diff --git a/docs/reference/mapping/types/binary.asciidoc b/docs/reference/mapping/types/binary.asciidoc
new file mode 100644
index 0000000..ff76fbe
--- /dev/null
+++ b/docs/reference/mapping/types/binary.asciidoc
@@ -0,0 +1,52 @@
+[[binary]]
+=== Binary datatype
+
+The `binary` type accepts a binary value as a
+https://en.wikipedia.org/wiki/Base64[Base64] encoded string. The field is not
+stored by default and is not searchable:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "name": {
+          "type": "string"
+        },
+        "blob": {
+          "type": "binary"
+        }
+      }
+    }
+  }
+}
+
+PUT my_index/my_type/1
+{
+  "name": "Some binary blob",
+  "blob": "U29tZSBiaW5hcnkgYmxvYg==" <1>
+}
+--------------------------------------------------
+<1> The Base64 encoded binary value must not have embedded newlines `\n`.
+
+[[binary-params]]
+==== Parameters for `binary` fields
+
+The following parameters are accepted by `binary` fields:
+
+[horizontal]
+
+<<doc-values,`doc_values`>>::
+
+    Can the field value be used for sorting, aggregations, or scripting?
+    Accepts `true` or `false` (default).
+
+<<mapping-store,`store`>>::
+
+    Whether the field value should be stored and retrievable separately from
+    the <<mapping-source-field,`_source`>> field. Accepts `true` or `false`
+    (default).
+
+
diff --git a/docs/reference/mapping/types/boolean.asciidoc b/docs/reference/mapping/types/boolean.asciidoc
new file mode 100644
index 0000000..5ebcc65
--- /dev/null
+++ b/docs/reference/mapping/types/boolean.asciidoc
@@ -0,0 +1,119 @@
+[[boolean]]
+=== Boolean datatype
+
+Boolean fields accept JSON `true` and `false` values, but can also accept
+strings and numbers which are interpreted as either true or false:
+
+[horizontal]
+False values::
+
+    `false`, `"false"`, `"off"`, `"no"`, `"0"`, `""` (empty string), `0`, `0.0`
+
+True values::
+
+    Anything that isn't false.
+
+For example:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "is_published": {
+          "type": "boolean"
+        }
+      }
+    }
+  }
+}
+
+POST my_index/my_type/1
+{
+  "is_published": true <1>
+}
+
+GET my_index/_search
+{
+  "query": {
+    "term": {
+      "is_published": 1 <2>
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> Indexing a document with a JSON `true`.
+<2> Querying for the document with `1`, which is interpreted as `true`.
+
+Aggregations like the <<search-aggregations-bucket-terms-aggregation,`terms`
+aggregation>>  use `1` and `0` for the `key`, and the strings `"true"` and
+`"false"` for the `key_as_string`. Boolean fields  when used in scripts,
+return `1` and `0`:
+
+[source,js]
+--------------------------------------------------
+POST my_index/my_type/1
+{
+  "is_published": true
+}
+
+POST my_index/my_type/2
+{
+  "is_published": false
+}
+
+GET my_index/_search
+{
+  "aggs": {
+    "publish_state": {
+      "terms": {
+        "field": "is_published"
+      }
+    }
+  },
+  "script_fields": {
+    "is_published": {
+      "script": "doc['is_published'].value" <1>
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> Inline scripts must be <<enable-dynamic-scripting,enabled>> for this example to work.
+
+[[boolean-params]]
+==== Parameters for `boolean` fields
+
+The following parameters are accepted by `boolean` fields:
+
+[horizontal]
+
+<<index-boost,`boost`>>::
+
+    Field-level index time boosting. Accepts a floating point number, defaults
+    to `1.0`.
+
+<<doc-values,`doc_values`>>::
+
+    Can the field value be used for sorting, aggregations, or scripting?
+    Accepts `true` (default) or `false`.
+
+<<mapping-index,`index`>>::
+
+    Should the field be searchable? Accepts `not_analyzed` (default) and `no`.
+
+<<null-value,`null_value`>>::
+
+    Accepts any of the true or false values listed above. The value is
+    substituted for any explicit `null` values.  Defaults to `null`, which
+    means the field is treated as missing.
+
+<<mapping-store,`store`>>::
+
+    Whether the field value should be stored and retrievable separately from
+    the <<mapping-source-field,`_source`>> field. Accepts `true` or `false`
+    (default).
+
diff --git a/docs/reference/mapping/types/core-types.asciidoc b/docs/reference/mapping/types/core-types.asciidoc
deleted file mode 100644
index f848fbd..0000000
--- a/docs/reference/mapping/types/core-types.asciidoc
+++ /dev/null
@@ -1,649 +0,0 @@
-[[mapping-core-types]]
-=== Core Types
-
-Each JSON field can be mapped to a specific core type. JSON itself
-already provides us with some typing, with its support for `string`,
-`integer`/`long`, `float`/`double`, `boolean`, and `null`.
-
-The following sample tweet JSON document will be used to explain the
-core types:
-
-[source,js]
---------------------------------------------------
-{
-    "tweet" {
-        "user" : "kimchy",
-        "message" : "This is a tweet!",
-        "postDate" : "2009-11-15T14:12:12",
-        "priority" : 4,
-        "rank" : 12.3
-    }
-}
---------------------------------------------------
-
-Explicit mapping for the above JSON tweet can be:
-
-[source,js]
---------------------------------------------------
-{
-    "tweet" : {
-        "properties" : {
-            "user" : {"type" : "string", "index" : "not_analyzed"},
-            "message" : {"type" : "string", "null_value" : "na"},
-            "postDate" : {"type" : "date"},
-            "priority" : {"type" : "integer"},
-            "rank" : {"type" : "float"}
-        }
-    }
-}
---------------------------------------------------
-
-[float]
-[[string]]
-==== String
-
-The text based string type is the most basic type, and contains one or
-more characters. An example mapping can be:
-
-[source,js]
---------------------------------------------------
-{
-    "tweet" : {
-        "properties" : {
-            "message" : {
-                "type" : "string",
-                "store" : true,
-                "index" : "analyzed",
-                "null_value" : "na"
-            },
-            "user" : {
-                "type" : "string",
-                "index" : "not_analyzed",
-                "norms" : {
-                    "enabled" : false
-                }
-            }
-        }
-    }
-}
---------------------------------------------------
-
-The above mapping defines a `string` `message` property/field within the
-`tweet` type. The field is stored in the index (so it can later be
-retrieved using selective loading when searching), and it gets analyzed
-(broken down into searchable terms). If the message has a `null` value,
-then the value that will be stored is `na`. There is also a `string` `user`
-which is indexed as-is (not broken down into tokens) and has norms
-disabled (so that matching this field is a binary decision, no match is
-better than another one).
-
-The following table lists all the attributes that can be used with the
-`string` type:
-
-[cols="<,<",options="header",]
-|=======================================================================
-|Attribute |Description
-|`index_name` |The name of the field that will be stored in the index.
-Defaults to the property/field name.
-
-|`store` |Set to `true` to actually store the field in the index, `false` to not
-store it. Since by default Elasticsearch stores all fields of the source
-document in the special `_source` field, this option is primarily useful when
-the `_source` field has been disabled in the type definition. Defaults to
-`false`.
-
-|`index` |Set to `analyzed` for the field to be indexed and searchable
-after being broken down into token using an analyzer. `not_analyzed`
-means that its still searchable, but does not go through any analysis
-process or broken down into tokens. `no` means that it won't be
-searchable at all (as an individual field; it may still be included in
-`_all`). Setting to `no` disables `include_in_all`. Defaults to
-`analyzed`.
-
-|`doc_values` |Set to `true` to store field values in a column-stride fashion.
-Automatically set to `true` when the <<fielddata-formats,`fielddata` format>> is `doc_values`.
-
-|`term_vector` |Possible values are `no`, `yes`, `with_offsets`,
-`with_positions`, `with_positions_offsets`. Defaults to `no`.
-
-|`boost` |The boost value. Defaults to `1.0`.
-
-|`null_value` |When there is a (JSON) null value for the field, use the
-`null_value` as the field value. Defaults to not adding the field at
-all.
-
-|`norms: {enabled: <value>}` |Boolean value if norms should be enabled or
-not. Defaults to `true` for `analyzed` fields, and to `false` for
-`not_analyzed` fields. See the <<norms,section about norms>>.
-
-|`norms: {loading: <value>}` |Describes how norms should be loaded, possible values are
-`eager` and `lazy` (default). It is possible to change the default value to
-eager for all fields by configuring the index setting `index.norms.loading`
-to `eager`.
-
-|`index_options` | Allows to set the indexing
-options, possible values are `docs` (only doc numbers are indexed),
-`freqs` (doc numbers and term frequencies), and `positions` (doc
-numbers, term frequencies and positions). Defaults to `positions` for
-`analyzed` fields, and to `docs` for `not_analyzed` fields. It
-is also possible to set it to `offsets` (doc numbers, term
-frequencies, positions and offsets).
-
-|`analyzer` |The analyzer used to analyze the text contents when
-`analyzed` during indexing and searching.
-Defaults to the globally configured analyzer.
-
-|`search_analyzer` |The analyzer used to analyze the field when searching, which
-overrides the value of `analyzer`. Can be updated on an existing field.
-
-|`include_in_all` |Should the field be included in the `_all` field (if
-enabled). If `index` is set to `no` this defaults to `false`, otherwise,
-defaults to `true` or to the parent `object` type setting.
-
-|`ignore_above` |The analyzer will ignore strings larger than this size.
-Useful for generic `not_analyzed` fields that should ignore long text.
-
-This option is also useful for protecting against Lucene's term byte-length
-limit of `32766`. Note: the value for `ignore_above` is the _character count_,
-but Lucene counts bytes, so if you have UTF-8 text, you may want to set the
-limit to `32766 / 3 = 10922` since UTF-8 characters may occupy at most 3
-bytes.
-
-|`position_offset_gap` |Position increment gap between field instances
-with the same field name. Defaults to 0.
-|=======================================================================
-
-The `string` type also support custom indexing parameters associated
-with the indexed value. For example:
-
-[source,js]
---------------------------------------------------
-{
-    "message" : {
-        "_value":  "boosted value",
-        "_boost":  2.0
-    }
-}
---------------------------------------------------
-
-The mapping is required to disambiguate the meaning of the document.
-Otherwise, the structure would interpret "message" as a value of type
-"object". The key `_value` (or `value`) in the inner document specifies
-the real string content that should eventually be indexed. The `_boost`
-(or `boost`) key specifies the per field document boost (here 2.0).
-
-[float]
-[[norms]]
-===== Norms
-
-Norms store various normalization factors that are later used (at query time)
-in order to compute the score of a document relatively to a query.
-
-Although useful for scoring, norms also require quite a lot of memory
-(typically in the order of one byte per document per field in your index,
-even for documents that don't have this specific field). As a consequence, if
-you don't need scoring on a specific field, it is highly recommended to disable
-norms on it. In  particular, this is the case for fields that are used solely
-for filtering or aggregations.
-
-In case you would like to disable norms after the fact, it is possible to do so
-by using the <<indices-put-mapping,PUT mapping API>>, like this:
-
-[source,js]
-------------
-PUT my_index/_mapping/my_type
-{
-  "properties": {
-    "title": {
-      "type": "string",
-      "norms": {
-        "enabled": false
-      }
-    }
-  }
-}
-------------
-
-Please however note that norms won't be removed instantly, but will be removed
-as old segments are merged into new segments as you continue indexing new documents.
-Any score computation on a field that has had
-norms removed might return inconsistent results since some documents won't have
-norms anymore while other documents might still have norms.
-
-[float]
-[[number]]
-==== Number
-
-A number based type supporting `float`, `double`, `byte`, `short`,
-`integer`, and `long`. It uses specific constructs within Lucene in
-order to support numeric values. The number types have the same ranges
-as corresponding
-http://docs.oracle.com/javase/tutorial/java/nutsandbolts/datatypes.html[Java
-types]. An example mapping can be:
-
-[source,js]
---------------------------------------------------
-{
-    "tweet" : {
-        "properties" : {
-            "rank" : {
-                "type" : "float",
-                "null_value" : 1.0
-            }
-        }
-    }
-}
---------------------------------------------------
-
-The following table lists all the attributes that can be used with a
-numbered type:
-
-[cols="<,<",options="header",]
-|=======================================================================
-|Attribute |Description
-|`type` |The type of the number. Can be `float`, `double`, `integer`,
-`long`, `short`, `byte`. Required.
-
-|`index_name` |The name of the field that will be stored in the index.
-Defaults to the property/field name.
-
-|`store` |Set to `true` to store actual field in the index, `false` to not
-store it. Defaults to `false` (note, the JSON document itself is stored,
-and it can be retrieved from it).
-
-|`index` |Set to `no` if the value should not be indexed. Setting to
-`no` disables `include_in_all`. If set to `no` the field should be either stored
-in `_source`, have `include_in_all` enabled, or `store` be set to
-`true` for this to be useful.
-
-|`doc_values` |Set to `true` to store field values in a column-stride fashion.
-Automatically set to `true` when the fielddata format is `doc_values`.
-
-|`precision_step` |The precision step (influences the number of terms
-generated for each number value). Defaults to `16` for `long`, `double`,
-`8` for `short`, `integer`, `float`, and `2147483647` for `byte`.
-
-|`boost` |The boost value. Defaults to `1.0`.
-
-|`null_value` |When there is a (JSON) null value for the field, use the
-`null_value` as the field value. Defaults to not adding the field at
-all.
-
-|`include_in_all` |Should the field be included in the `_all` field (if
-enabled). If `index` is set to `no` this defaults to `false`, otherwise,
-defaults to `true` or to the parent `object` type setting.
-
-|`ignore_malformed` |Ignored a malformed number. Defaults to `false`.
-
-|`coerce` |Try convert strings to numbers and truncate fractions for integers. Defaults to `true`.
-
-|=======================================================================
-
-[float]
-[[token_count]]
-==== Token Count
-The `token_count` type maps to the JSON string type but indexes and stores
-the number of tokens in the string rather than the string itself.  For
-example:
-
-[source,js]
---------------------------------------------------
-{
-    "tweet" : {
-        "properties" : {
-            "name" : {
-                "type" : "string",
-                "fields" : {
-                    "word_count": {
-                        "type" : "token_count",
-                        "store" : "yes",
-                        "analyzer" : "standard"
-                    }
-                }
-            }
-        }
-    }
-}
---------------------------------------------------
-
-All the configuration that can be specified for a number can be specified
-for a token_count.  The only extra configuration is the required
-`analyzer` field which specifies which analyzer to use to break the string
-into tokens.  For best performance, use an analyzer with no token filters.
-
-[NOTE]
-===================================================================
-Technically the `token_count` type sums position increments rather than
-counting tokens. This means that even if the analyzer filters out stop
-words they are included in the count.
-===================================================================
-
-[float]
-[[date]]
-==== Date
-
-The date type is a special type which maps to JSON string type. It
-follows a specific format that can be explicitly set. All dates are
-`UTC`. Internally, a date maps to a number type `long`, with the added
-parsing stage from string to long and from long to string. An example
-mapping:
-
-[source,js]
---------------------------------------------------
-{
-    "tweet" : {
-        "properties" : {
-            "postDate" : {
-                "type" : "date",
-                "format" : "YYYY-MM-dd"
-            }
-        }
-    }
-}
---------------------------------------------------
-
-The date type will also accept a long number representing UTC
-milliseconds since the epoch, regardless of the format it can handle.
-
-The following table lists all the attributes that can be used with a
-date type:
-
-[cols="<,<",options="header",]
-|=======================================================================
-|Attribute |Description
-|`index_name` |The name of the field that will be stored in the index.
-Defaults to the property/field name.
-
-|`format` |The <<mapping-date-format,date
-format>>. Defaults to `epoch_millis||strictDateOptionalTime`.
-
-|`store` |Set to `true` to store actual field in the index, `false` to not
-store it. Defaults to `false` (note, the JSON document itself is stored,
-and it can be retrieved from it).
-
-|`index` |Set to `no` if the value should not be indexed. Setting to
-`no` disables `include_in_all`. If set to `no` the field should be either stored
-in `_source`, have `include_in_all` enabled, or `store` be set to
-`true` for this to be useful.
-
-|`doc_values` |Set to `true` to store field values in a column-stride fashion.
-Automatically set to `true` when the fielddata format is `doc_values`.
-
-|`precision_step` |The precision step (influences the number of terms
-generated for each number value). Defaults to `16`.
-
-|`boost` |The boost value. Defaults to `1.0`.
-
-|`null_value` |When there is a (JSON) null value for the field, use the
-`null_value` as the field value. Defaults to not adding the field at
-all.
-
-|`include_in_all` |Should the field be included in the `_all` field (if
-enabled). If `index` is set to `no` this defaults to `false`, otherwise,
-defaults to `true` or to the parent `object` type setting.
-
-|`ignore_malformed` |Ignored a malformed number. Defaults to `false`.
-
-|=======================================================================
-
-[float]
-[[boolean]]
-==== Boolean
-
-The boolean type Maps to the JSON boolean type. It ends up storing
-within the index either `T` or `F`, with automatic translation to `true`
-and `false` respectively.
-
-[source,js]
---------------------------------------------------
-{
-    "tweet" : {
-        "properties" : {
-            "hes_my_special_tweet" : {
-                "type" : "boolean"
-            }
-        }
-    }
-}
---------------------------------------------------
-
-The boolean type also supports passing the value as a number or a string
-(in this case `0`, an empty string, `false`, `off` and `no` are
-`false`, all other values are `true`).
-
-The following table lists all the attributes that can be used with the
-boolean type:
-
-[cols="<,<",options="header",]
-|=======================================================================
-|Attribute |Description
-|`index_name` |The name of the field that will be stored in the index.
-Defaults to the property/field name.
-
-|`store` |Set to `true` to store actual field in the index, `false` to not
-store it. Defaults to `false` (note, the JSON document itself is stored,
-and it can be retrieved from it).
-
-|`index` |Set to `no` if the value should not be indexed. Setting to
-`no` disables `include_in_all`. If set to `no` the field should be either stored
-in `_source`, have `include_in_all` enabled, or `store` be set to
-`true` for this to be useful.
-
-|`doc_values` |Set to `true` to store field values in a column-stride fashion.
-Automatically set to `true` when the fielddata format is `doc_values`.
-
-|`boost` |The boost value. Defaults to `1.0`.
-
-|`null_value` |When there is a (JSON) null value for the field, use the
-`null_value` as the field value. Defaults to not adding the field at
-all.
-|=======================================================================
-
-[float]
-[[binary]]
-==== Binary
-
-The binary type is a base64 representation of binary data that can be
-stored in the index. The field is not stored by default and not indexed at
-all.
-
-[source,js]
---------------------------------------------------
-{
-    "tweet" : {
-        "properties" : {
-            "image" : {
-                "type" : "binary"
-            }
-        }
-    }
-}
---------------------------------------------------
-
-The following table lists all the attributes that can be used with the
-binary type:
-
-[horizontal]
-
-`index_name`::
-
-    The name of the field that will be stored in the index. Defaults to the
-    property/field name.
-
-`store`::
-
-    Set to `true` to store actual field in the index, `false` to not store it.
-    Defaults to `false` (note, the JSON document itself is already stored, so
-    the binary field can be retrieved from there).
-
-`doc_values`::
-
-    Set to `true` to store field values in a column-stride fashion.
-
-[float]
-[[fielddata-filters]]
-==== Fielddata filters
-
-It is possible to control which field values are loaded into memory,
-which is particularly useful for aggregations on string fields, using
-fielddata filters, which are explained in detail in the
-<<modules-fielddata,Fielddata>> section.
-
-Fielddata filters can exclude terms which do not match a regex, or which
-don't fall between a `min` and `max` frequency range:
-
-[source,js]
---------------------------------------------------
-{
-    tweet: {
-        type:      "string",
-        analyzer:  "whitespace"
-        fielddata: {
-            filter: {
-                regex: {
-                    "pattern":        "^#.*"
-                },
-                frequency: {
-                    min:              0.001,
-                    max:              0.1,
-                    min_segment_size: 500
-                }
-            }
-        }
-    }
-}
---------------------------------------------------
-
-These filters can be updated on an existing field mapping and will take
-effect the next time the fielddata for a segment is loaded. Use the
-<<indices-clearcache,Clear Cache>> API
-to reload the fielddata using the new filters.
-
-[float]
-==== Similarity
-
-Elasticsearch allows you to configure a similarity (scoring algorithm) per field.
-The `similarity` setting provides a simple way of choosing a similarity algorithm
-other than the default TF/IDF, such as `BM25`.
-
-You can configure similarities via the
-<<index-modules-similarity,similarity module>>
-
-[float]
-===== Configuring Similarity per Field
-
-Defining the Similarity for a field is done via the `similarity` mapping
-property, as this example shows:
-
-[source,js]
---------------------------------------------------
-{
-   "book":{
-      "properties":{
-         "title":{
-            "type":"string", "similarity":"BM25"
-         }
-      }
-   }
-}
---------------------------------------------------
-
-The following Similarities are configured out-of-box:
-
-`default`::
-        The Default TF/IDF algorithm used by Elasticsearch and
-        Lucene in previous versions.
-
-`BM25`::
-        The BM25 algorithm.
-        http://en.wikipedia.org/wiki/Okapi_BM25[See Okapi_BM25] for more
-        details.
-
-
-[[copy-to]]
-[float]
-===== Copy to field
-
-Adding `copy_to` parameter to any field mapping will cause all values of this field to be copied to fields specified in
-the parameter. In the following example all values from fields `title` and `abstract` will be copied to the field
-`meta_data`. The field which is being copied to will be indexed (i.e. searchable, and available through `fielddata_field`) but the original source will not be modified.
-
-
-[source,js]
---------------------------------------------------
-{
-  "book" : {
-    "properties" : {
-      "title" : { "type" : "string", "copy_to" : "meta_data" },
-      "abstract" : { "type" : "string", "copy_to" : "meta_data" },
-      "meta_data" : { "type" : "string" }
-    }
-}
---------------------------------------------------
-
-Multiple fields are also supported:
-
-[source,js]
---------------------------------------------------
-{
-  "book" : {
-    "properties" : {
-      "title" : { "type" : "string", "copy_to" : ["meta_data", "article_info"] }
-    }
-}
---------------------------------------------------
-
-[float]
-[[multi-fields]]
-===== Multi fields
-
-The `fields` options allows to map several core types fields into a single
-json source field. This can be useful if a single field need to be
-used in different ways. For example a single field is to be used for both
-free text search and sorting.
-
-[source,js]
---------------------------------------------------
-{
-  "tweet" : {
-    "properties" : {
-      "name" : {
-        "type" : "string",
-        "index" : "analyzed",
-        "fields" : {
-          "raw" : {"type" : "string", "index" : "not_analyzed"}
-        }
-      }
-    }
-  }
-}
---------------------------------------------------
-
-In the above example the field `name` gets processed twice. The first time it gets
-processed as an analyzed string and this version is accessible under the field name
-`name`, this is the main field and is in fact just like any other field. The second time
-it gets processed as a not analyzed string and is accessible under the name `name.raw`.
-
-[float]
-==== Include in All
-
-The `include_in_all` setting is ignored on any field that is defined in
-the `fields` options. Setting the `include_in_all` only makes sense on
-the main field, since the raw field value is copied to the `_all` field,
-the tokens aren't copied.
-
-[float]
-==== Updating a field
-
-In essence a field cannot be updated. However multi fields can be
-added to existing fields. This allows for example to have a different
-`analyzer` configuration in addition to the already configured
-`analyzer` configuration specified in the main and other multi fields.
-
-Also the new multi field will only be applied on document that have been
-added after the multi field has been added and in fact the new multi field
-doesn't exist in existing documents.
-
-Another important note is that new multi fields will be merged into the
-list of existing multi fields, so when adding new multi fields for a field
-previous added multi fields don't need to be specified.
diff --git a/docs/reference/mapping/types/date.asciidoc b/docs/reference/mapping/types/date.asciidoc
new file mode 100644
index 0000000..c8067a8
--- /dev/null
+++ b/docs/reference/mapping/types/date.asciidoc
@@ -0,0 +1,142 @@
+[[date]]
+=== Date datatype
+
+JSON doesn't have a date datatype, so dates in Elasticsearch can either be:
+
+* strings containing formatted dates, e.g. `"2015-01-01"` or `"2015/01/01 12:10:30"`.
+* a long number representing _milliseconds-since-the-epoch_.
+* an integer representing _seconds-since-the-epoch_.
+
+Internally, dates are converted to UTC (if the time-zone is specified) and
+stored as a long number representing milliseconds-since-the-epoch.
+
+Date formats can be customised, but if no `format` is specified then it uses
+the default:
+
+    "strict_date_optional_time||epoch_millis"
+
+This means that it will accept dates with optional timestamps, which conform
+to the formats supported by <<strict-date-time,`strict_date_optional_time`>>
+or milliseconds-since-the-epoch.
+
+For instance:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "date": {
+          "type": "date" <1>
+        }
+      }
+    }
+  }
+}
+
+PUT my_index/my_type/1
+{ "date": "2015-01-01" } <2>
+
+PUT my_index/my_type/2
+{ "date": "2015-01-01T12:10:30Z" } <3>
+
+PUT my_index/my_type/3
+{ "date": 1420070400001 } <4>
+
+GET my_index/_search
+{
+  "sort": { "date": "asc"} <5>
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> The `date` field uses the default `format`.
+<2> This document uses a plain date.
+<3> This document includes a time.
+<4> This document uses milliseconds-since-the-epoch.
+<5> Note that the `sort` values that are returned are all in milliseconds-since-the-epoch.
+
+[[multiple-date-formats]]
+==== Multiple date formats
+
+Multiple formats can be specified by separating them with `||` as a separator.
+Each format will be tried in turn until a matching format is found.  The first
+format will be used to convert the _milliseconds-since-the-epoch_ value back
+into a string.
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "date": {
+          "type":   "date",
+          "format": "yyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis"
+        }
+      }
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+
+[[date-params]]
+==== Parameters for `date` fields
+
+The following parameters are accepted by `date` fields:
+
+[horizontal]
+
+<<index-boost,`boost`>>::
+
+    Field-level index time boosting. Accepts a floating point number, defaults
+    to `1.0`.
+
+<<doc-values,`doc_values`>>::
+
+    Can the field value be used for sorting, aggregations, or scripting?
+    Accepts `true` (default) or `false`.
+
+<<mapping-date-format,`format`>>::
+
+    The date format(s) that can be parsed.  Defaults to
+    `strict_date_optional_time||epoch_millis`.
+
+<<ignore-malformed,`ignore_malformed`>>::
+
+    If `true`, malformed numbers are ignored. If `false` (default), malformed
+    numbers throw an exception and reject the whole document.
+
+<<include-in-all,`include_in_all`>>::
+
+    Whether or not the field value should be included in the
+    <<mapping-all-field,`_all`>> field? Accepts `true` or `false`.  Defaults
+    to `false` if <<mapping-index,`index`>> is set to `no`, or if a parent
+    <<object,`object`>> field sets `include_in_all` to `false`.
+    Otherwise defaults to `true`.
+
+<<mapping-index,`index`>>::
+
+    Should the field be searchable? Accepts `not_analyzed` (default) and `no`.
+
+<<null-value,`null_value`>>::
+
+    Accepts a date value in one of the configured +format+'s as the field
+    which is substituted for any explicit `null` values.  Defaults to `null`,
+    which means the field is treated as missing.
+
+<<precision-step,`precision_step`>>::
+
+    Controls the number of extra terms that are indexed to make
+    <<query-dsl-range-query,`range` queries>> faster. Defaults to `16`.
+
+<<mapping-store,`store`>>::
+
+    Whether the field value should be stored and retrievable separately from
+    the <<mapping-source-field,`_source`>> field. Accepts `true` or `false`
+    (default).
+
+
diff --git a/docs/reference/mapping/types/geo-point-type.asciidoc b/docs/reference/mapping/types/geo-point-type.asciidoc
deleted file mode 100644
index 35f3600..0000000
--- a/docs/reference/mapping/types/geo-point-type.asciidoc
+++ /dev/null
@@ -1,215 +0,0 @@
-[[mapping-geo-point-type]]
-=== Geo Point Type
-
-Mapper type called `geo_point` to support geo based points. The
-declaration looks as follows:
-
-[source,js]
---------------------------------------------------
-{
-    "pin" : {
-        "properties" : {
-            "location" : {
-                "type" : "geo_point"
-            }
-        }
-    }
-}
---------------------------------------------------
-
-[float]
-==== Indexed Fields
-
-The `geo_point` mapping will index a single field with the format of
-`lat,lon`. The `lat_lon` option can be set to also index the `.lat` and
-`.lon` as numeric fields, and `geohash` can be set to `true` to also
-index `.geohash` value.
-
-A good practice is to enable indexing `lat_lon` as well, since both the
-geo distance and bounding box filters can either be executed using in
-memory checks, or using the indexed lat lon values, and it really
-depends on the data set which one performs better. Note though, that
-indexed lat lon only make sense when there is a single geo point value
-for the field, and not multi values.
-
-[float]
-==== Geohashes
-
-Geohashes are a form of lat/lon encoding which divides the earth up into
-a grid. Each cell in this grid is represented by a geohash string. Each
-cell in turn can be further subdivided into smaller cells which are
-represented by a longer string. So the longer the geohash, the smaller
-(and thus more accurate) the cell is.
-
-Because geohashes are just strings, they can be stored in an inverted
-index like any other string, which makes querying them very efficient.
-
-If you enable the `geohash` option, a `geohash` ``sub-field'' will be
-indexed as, eg `pin.geohash`. The length of the geohash is controlled by
-the `geohash_precision` parameter, which can either be set to an absolute
-length (eg `12`, the default) or to a distance (eg `1km`).
-
-More usefully, set the `geohash_prefix` option to `true` to not only index
-the geohash value, but all the enclosing cells as well.  For instance, a
-geohash of `u30` will be indexed as `[u,u3,u30]`. This option can be used
-by the <<query-dsl-geohash-cell-query>> to find geopoints within a
-particular cell very efficiently.
-
-[float]
-==== Input Structure
-
-The above mapping defines a `geo_point`, which accepts different
-formats. The following formats are supported:
-
-[float]
-===== Lat Lon as Properties
-
-[source,js]
---------------------------------------------------
-{
-    "pin" : {
-        "location" : {
-            "lat" : 41.12,
-            "lon" : -71.34
-        }
-    }
-}
---------------------------------------------------
-
-[float]
-===== Lat Lon as String
-
-Format in `lat,lon`.
-
-[source,js]
---------------------------------------------------
-{
-    "pin" : {
-        "location" : "41.12,-71.34"
-    }
-}
---------------------------------------------------
-
-[float]
-===== Geohash
-
-[source,js]
---------------------------------------------------
-{
-    "pin" : {
-        "location" : "drm3btev3e86"
-    }
-}
---------------------------------------------------
-
-[float]
-===== Lat Lon as Array
-
-Format in `[lon, lat]`, note, the order of lon/lat here in order to
-conform with http://geojson.org/[GeoJSON].
-
-[source,js]
---------------------------------------------------
-{
-    "pin" : {
-        "location" : [-71.34, 41.12]
-    }
-}
---------------------------------------------------
-
-[float]
-==== Mapping Options
-
-[cols="<,<",options="header",]
-|=======================================================================
-|Option |Description
-|`lat_lon` |Set to `true` to also index the `.lat` and `.lon` as fields.
-Defaults to `false`.
-
-|`geohash` |Set to `true` to also index the `.geohash` as a field.
-Defaults to `false`.
-
-|`geohash_precision` |Sets the geohash precision. It can be set to an
-absolute geohash length or a distance value (eg 1km, 1m, 1ml) defining
-the size of the smallest cell. Defaults to an absolute length of 12.
-
-|`geohash_prefix` |If this option is set to `true`, not only the geohash
-but also all its parent cells (true prefixes) will be indexed as well. The
-number of terms that will be indexed depends on the `geohash_precision`.
-Defaults to `false`. *Note*: This option implicitly enables `geohash`.
-
-|`validate` |Set to `false` to accept geo points with invalid latitude or
-longitude (default is `true`). *Note*: Validation only works when
-normalization has been disabled. This option will be deprecated and removed
-in upcoming releases.
-
-|`validate_lat` |Set to `false` to accept geo points with an invalid
-latitude (default is `true`). This option will be deprecated and removed
-in upcoming releases.
-
-|`validate_lon` |Set to `false` to accept geo points with an invalid
-longitude (default is `true`). This option will be deprecated and removed
-in upcoming releases.
-
-|`normalize` |Set to `true` to normalize latitude and longitude (default
-is `true`).
-
-|`normalize_lat` |Set to `true` to normalize latitude.
-
-|`normalize_lon` |Set to `true` to normalize longitude.
-
-|`precision_step` |The precision step (influences the number of terms 
-generated for each number value) for `.lat` and `.lon` fields 
-if `lat_lon` is set to `true`.
-Defaults to `16`.
-|=======================================================================
-
-[float]
-==== Field data
-
-By default, geo points use the `array` format which loads geo points into two
-parallel double arrays, making sure there is no precision loss. However, this
-can require a non-negligible amount of memory (16 bytes per document) which is
-why Elasticsearch also provides a field data implementation with lossy
-compression called `compressed`:
-
-[source,js]
---------------------------------------------------
-{
-    "pin" : {
-        "properties" : {
-            "location" : {
-                "type" : "geo_point",
-                "fielddata" : {
-                    "format" : "compressed",
-                    "precision" : "1cm"
-                }
-            }
-        }
-    }
-}
---------------------------------------------------
-
-This field data format comes with a `precision` option which allows to
-configure how much precision can be traded for memory. The default value is
-`1cm`. The following table presents values of the memory savings given various
-precisions:
-
-|=============================================
-| Precision | Bytes per point | Size reduction
-|       1km |               4 |            75%
-|        3m |               6 |          62.5%
-|       1cm |               8 |            50%
-|       1mm |              10 |          37.5%
-|=============================================
-
-Precision can be changed on a live index by using the update mapping API.
-
-[float]
-==== Usage in Scripts
-
-When using `doc[geo_field_name]` (in the above mapping,
-`doc['location']`), the `doc[...].value` returns a `GeoPoint`, which
-then allows access to `lat` and `lon` (for example,
-`doc[...].value.lat`). For performance, it is better to access the `lat`
-and `lon` directly using `doc[...].lat` and `doc[...].lon`.
diff --git a/docs/reference/mapping/types/geo-point.asciidoc b/docs/reference/mapping/types/geo-point.asciidoc
new file mode 100644
index 0000000..0049d8f
--- /dev/null
+++ b/docs/reference/mapping/types/geo-point.asciidoc
@@ -0,0 +1,167 @@
+[[geo-point]]
+=== Geo-point datatype
+
+Fields of type `geo_point` accept latitude-longitude pairs, which can be used:
+
+* to find geo-points within a <<query-dsl-geo-bounding-box-query,bounding box>>,
+  within a certain <<query-dsl-geo-distance-query,distance>> of a central point,
+  within a <<query-dsl-geo-polygon-query,polygon>>, or within a
+  <<query-dsl-geohash-cell-query,geohash>> cell.
+* to aggregate documents by <<search-aggregations-bucket-geohashgrid-aggregation,geographically>>
+  or by <<search-aggregations-bucket-geodistance-aggregation,distance>> from a central point.
+* to integerate distance into a document's <<query-dsl-function-score-query,relevance score>>.
+* to <<geo-sorting,sort>> documents by distance.
+
+There are four ways that a geo-point may be specified, as demonstrated below:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "location": {
+          "type": "geo_point"
+        }
+      }
+    }
+  }
+}
+
+PUT my_index/my_type/1
+{
+  "text": "Geo-point as an object",
+  "location": { <1>
+    "lat": 41.12,
+    "lon": -71.34
+  }
+}
+
+PUT my_index/my_type/2
+{
+  "text": "Geo-point as a string",
+  "location": "41.12,-71.34" <2>
+}
+
+PUT my_index/my_type/3
+{
+  "text": "Geo-point as a geohash",
+  "location": "drm3btev3e86" <3>
+}
+
+PUT my_index/my_type/4
+{
+  "text": "Geo-point as an array",
+  "location": [ -71.34, 41.12 ] <4>
+}
+
+GET my_index/_search
+{
+  "query": {
+    "geo_bounding_box": { <5>
+      "location": {
+        "top_left": {
+          "lat": 42,
+          "lon": -72
+        },
+        "bottom_right": {
+          "lat": 40,
+          "lon": -74
+        }
+      }
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> Geo-point expressed as an object, with `lat` and `lon` keys.
+<2> Geo-point expressed as a string with the format: `"lat,lon"`.
+<3> Geo-point expressed as a geohash.
+<4> Geo-point expressed as an array with the format: [ `lon`, `lat`]
+<5> A geo-bounding box query which finds all geo-points that fall inside the box.
+
+[IMPORTANT]
+.Geo-points expressed as an array or string
+==================================================
+
+Please note that string geo-points are ordered as `lat,lon`, while array
+geo-points are ordered as the reverse: `lon,lat`.
+
+Originally, `lat,lon` was used for both array and string, but the array
+format was changed early on to conform to the format used by GeoJSON.
+
+==================================================
+
+
+[[geo-point-params]]
+==== Parameters for `geo_point` fields
+
+The following parameters are accepted by `geo_point` fields:
+
+[horizontal]
+
+<<coerce,`coerce`>>::
+
+    Normalize longitude and latitude values to a standard -180:180 / -90:90
+    coordinate system. Accepts `true` and `false` (default).
+
+<<doc-values,`doc_values`>>::
+
+    Can the field value be used for sorting, aggregations, or scripting?
+    Accepts `true` (default) or `false`.
+
+<<geohash,`geohash`>>::
+
+    Should the geo-point also be indexed as a geohash in the `.geohash`
+    sub-field? Defaults to `false`, unless `geohash_prefix` is `true`.
+
+<<geohash-precision,`geohash_precision`>>::
+
+    The maximum length of the geohash to use for the `geohash` and
+    `geohash_prefix` options.
+
+<<geohash-prefix,`geohash_prefix`>>::
+
+    Should the geo-point also be indexed as a geohash plus all its prefixes?
+    Defaults to `false`.
+
+<<ignore-malformed,`ignore_malformed`>>::
+
+    If `true`, malformed geo-points are ignored. If `false` (default),
+    malformed geo-points throw an exception and reject the whole document.
+
+<<lat-lon,`lat_lon`>>::
+
+    Should the geo-point also be indexed as `.lat` and `.lon` sub-fields?
+    Accepts `true` and `false` (default).
+
+<<precision-step,`precision_step`>>::
+
+    Controls the number of extra terms that are indexed for each lat/lon point.
+    Defaults to `16`.  Ignored if `lat_lon` is `false`.
+
+
+==== Using geo-points in scripts
+
+When accessing the value of a geo-point in a script, the value is returned as
+a `GeoPoint` object, which allows access to the `.lat` and `.lon` values
+respectively:
+
+
+[source,js]
+--------------------------------------------------
+geopoint = doc['location'].value;
+lat      = geopoint.lat;
+lon      = geopoint.lon;
+--------------------------------------------------
+
+For performance reasons, it is better to access the lat/lon values directly:
+
+[source,js]
+--------------------------------------------------
+lat      = doc['location'].lat;
+lon      = doc['location'].lon;
+--------------------------------------------------
+
+
diff --git a/docs/reference/mapping/types/geo-shape-type.asciidoc b/docs/reference/mapping/types/geo-shape-type.asciidoc
deleted file mode 100644
index 0e20743..0000000
--- a/docs/reference/mapping/types/geo-shape-type.asciidoc
+++ /dev/null
@@ -1,426 +0,0 @@
-[[mapping-geo-shape-type]]
-=== Geo Shape Type
-
-The `geo_shape` mapping type facilitates the indexing of and searching
-with arbitrary geo shapes such as rectangles and polygons. It should be
-used when either the data being indexed or the queries being executed
-contain shapes other than just points.
-
-You can query documents using this type using
-<<query-dsl-geo-shape-query,geo_shape Query>>.
-
-[[geo-shape-mapping-options]]
-[float]
-==== Mapping Options
-
-The geo_shape mapping maps geo_json geometry objects to the geo_shape
-type. To enable it, users must explicitly map fields to the geo_shape
-type.
-
-[cols="<,<",options="header",]
-|=======================================================================
-|Option |Description
-
-|`tree` |Name of the PrefixTree implementation to be used: `geohash` for
-GeohashPrefixTree and `quadtree` for QuadPrefixTree. Defaults to
-`geohash`.
-
-|`precision` |This parameter may be used instead of `tree_levels` to set
-an appropriate value for the `tree_levels` parameter. The value
-specifies the desired precision and Elasticsearch will calculate the
-best tree_levels value to honor this precision. The value should be a
-number followed by an optional distance unit. Valid distance units
-include: `in`, `inch`, `yd`, `yard`, `mi`, `miles`, `km`, `kilometers`,
-`m`,`meters` (default), `cm`,`centimeters`, `mm`, `millimeters`.
-
-|`tree_levels` |Maximum number of layers to be used by the PrefixTree.
-This can be used to control the precision of shape representations and
-therefore how many terms are indexed. Defaults to the default value of
-the chosen PrefixTree implementation. Since this parameter requires a
-certain level of understanding of the underlying implementation, users
-may use the `precision` parameter instead. However, Elasticsearch only
-uses the tree_levels parameter internally and this is what is returned
-via the mapping API even if you use the precision parameter.
-
-|`distance_error_pct` |Used as a hint to the PrefixTree about how
-precise it should be. Defaults to 0.025 (2.5%) with 0.5 as the maximum
-supported value. PERFORMANCE NOTE: This value will be default to 0 if a `precision` or
-`tree_level` definition is explicitly defined. This guarantees spatial precision
-at the level defined in the mapping. This can lead to significant memory usage
-for high resolution shapes with low error (e.g., large shapes at 1m with < 0.001 error).
-To improve indexing performance (at the cost of query accuracy) explicitly define
-`tree_level` or `precision` along with a reasonable `distance_error_pct`, noting
-that large shapes will have greater false positives.
-
-|`orientation` |Optionally define how to interpret vertex order for
-polygons / multipolygons.  This parameter defines one of two coordinate
-system rules (Right-hand or Left-hand) each of which can be specified in three
-different ways. 1. Right-hand rule (default): `right`, `ccw`, `counterclockwise`,
-2. Left-hand rule: `left`, `cw`, `clockwise`. The default orientation
-(`counterclockwise`) complies with the OGC standard which defines
-outer ring vertices in counterclockwise order with inner ring(s) vertices (holes)
-in clockwise order. Setting this parameter in the geo_shape mapping explicitly
-sets vertex order for the coordinate list of a geo_shape field but can be
-overridden in each individual GeoJSON document.
-|=======================================================================
-
-[float]
-==== Prefix trees
-
-To efficiently represent shapes in the index, Shapes are converted into
-a series of hashes representing grid squares using implementations of a
-PrefixTree. The tree notion comes from the fact that the PrefixTree uses
-multiple grid layers, each with an increasing level of precision to
-represent the Earth.
-
-Multiple PrefixTree implementations are provided:
-
-* GeohashPrefixTree - Uses
-http://en.wikipedia.org/wiki/Geohash[geohashes] for grid squares.
-Geohashes are base32 encoded strings of the bits of the latitude and
-longitude interleaved. So the longer the hash, the more precise it is.
-Each character added to the geohash represents another tree level and
-adds 5 bits of precision to the geohash. A geohash represents a
-rectangular area and has 32 sub rectangles. The maximum amount of levels
-in Elasticsearch is 24.
-* QuadPrefixTree - Uses a
-http://en.wikipedia.org/wiki/Quadtree[quadtree] for grid squares.
-Similar to geohash, quad trees interleave the bits of the latitude and
-longitude the resulting hash is a bit set. A tree level in a quad tree
-represents 2 bits in this bit set, one for each coordinate. The maximum
-amount of levels for the quad trees in Elasticsearch is 50.
-
-[float]
-===== Accuracy
-
-Geo_shape does not provide 100% accuracy and depending on how it is
-configured it may return some false positives or false negatives for
-certain queries. To mitigate this, it is important to select an
-appropriate value for the tree_levels parameter and to adjust
-expectations accordingly. For example, a point may be near the border of
-a particular grid cell and may thus not match a query that only matches the
-cell right next to it -- even though the shape is very close to the point.
-
-[float]
-===== Example
-
-[source,js]
---------------------------------------------------
-{
-    "properties": {
-        "location": {
-            "type": "geo_shape",
-            "tree": "quadtree",
-            "precision": "1m"
-        }
-    }
-}
---------------------------------------------------
-
-This mapping maps the location field to the geo_shape type using the
-quad_tree implementation and a precision of 1m. Elasticsearch translates
-this into a tree_levels setting of 26.
-
-[float]
-===== Performance considerations
-
-Elasticsearch uses the paths in the prefix tree as terms in the index
-and in queries. The higher the levels is (and thus the precision), the
-more terms are generated. Of course, calculating the terms, keeping them in
-memory, and storing them on disk all have a price. Especially with higher
-tree levels, indices can become extremely large even with a modest
-amount of data. Additionally, the size of the features also matters.
-Big, complex polygons can take up a lot of space at higher tree levels.
-Which setting is right depends on the use case. Generally one trades off
-accuracy against index size and query performance.
-
-The defaults in Elasticsearch for both implementations are a compromise
-between index size and a reasonable level of precision of 50m at the
-equator. This allows for indexing tens of millions of shapes without
-overly bloating the resulting index too much relative to the input size.
-
-[float]
-==== Input Structure
-
-The http://www.geojson.org[GeoJSON] format is used to represent
-http://geojson.org/geojson-spec.html#geometry-objects[shapes] as input
-as follows:
-
-[cols="<,<,<",options="header",]
-|=======================================================================
-|GeoJSON Type |Elasticsearch Type |Description
-
-|`Point` |`point` |A single geographic coordinate.
-|`LineString` |`linestring` |An arbitrary line given two or more points.
-|`Polygon` |`polygon` |A _closed_ polygon whose first and last point
-must match, thus requiring `n + 1` vertices to create an `n`-sided
-polygon and a minimum of `4` vertices.
-|`MultiPoint` |`multipoint` |An array of unconnected, but likely related
-points.
-|`MultiLineString` |`multilinestring` |An array of separate linestrings.
-|`MultiPolygon` |`multipolygon` |An array of separate polygons.
-|`GeometryCollection` |`geometrycollection` | A GeoJSON shape similar to the
-`multi*` shapes except that multiple types can coexist (e.g., a Point
-and a LineString).
-|`N/A` |`envelope` |A bounding rectangle, or envelope, specified by
-specifying only the top left and bottom right points.
-|`N/A` |`circle` |A circle specified by a center point and radius with
-units, which default to `METERS`.
-|=======================================================================
-
-[NOTE]
-=============================================
-For all types, both the inner `type` and `coordinates` fields are
-required.
-
-In GeoJSON, and therefore Elasticsearch, the correct *coordinate
-order is longitude, latitude (X, Y)* within coordinate arrays. This
-differs from many Geospatial APIs (e.g., Google Maps) that generally
-use the colloquial latitude, longitude (Y, X).
-=============================================
-
-[float]
-===== http://geojson.org/geojson-spec.html#id2[Point]
-
-A point is a single geographic coordinate, such as the location of a
-building or the current position given by a smartphone's Geolocation
-API.
-
-[source,js]
---------------------------------------------------
-{
-    "location" : {
-        "type" : "point",
-        "coordinates" : [-77.03653, 38.897676]
-    }
-}
---------------------------------------------------
-
-[float]
-===== http://geojson.org/geojson-spec.html#id3[LineString]
-
-A `linestring` defined by an array of two or more positions. By
-specifying only two points, the `linestring` will represent a straight
-line.  Specifying more than two points creates an arbitrary path.
-
-[source,js]
---------------------------------------------------
-{
-    "location" : {
-        "type" : "linestring",
-        "coordinates" : [[-77.03653, 38.897676], [-77.009051, 38.889939]]
-    }
-}
---------------------------------------------------
-
-The above `linestring` would draw a straight line starting at the White
-House to the US Capitol Building.
-
-[float]
-===== http://www.geojson.org/geojson-spec.html#id4[Polygon]
-
-A polygon is defined by a list of a list of points. The first and last
-points in each (outer) list must be the same (the polygon must be
-closed).
-
-[source,js]
---------------------------------------------------
-{
-    "location" : {
-        "type" : "polygon",
-        "coordinates" : [
-            [ [100.0, 0.0], [101.0, 0.0], [101.0, 1.0], [100.0, 1.0], [100.0, 0.0] ]
-        ]
-    }
-}
---------------------------------------------------
-
-The first array represents the outer boundary of the polygon, the other
-arrays represent the interior shapes ("holes"):
-
-[source,js]
---------------------------------------------------
-{
-    "location" : {
-        "type" : "polygon",
-        "coordinates" : [
-            [ [100.0, 0.0], [101.0, 0.0], [101.0, 1.0], [100.0, 1.0], [100.0, 0.0] ],
-            [ [100.2, 0.2], [100.8, 0.2], [100.8, 0.8], [100.2, 0.8], [100.2, 0.2] ]
-        ]
-    }
-}
---------------------------------------------------
-
-*IMPORTANT NOTE:* GeoJSON does not mandate a specific order for vertices thus ambiguous
-polygons around the dateline and poles are possible. To alleviate ambiguity
-the Open Geospatial Consortium (OGC)
-http://www.opengeospatial.org/standards/sfa[Simple Feature Access] specification
-defines the following vertex ordering:
-
-* Outer Ring - Counterclockwise
-* Inner Ring(s) / Holes - Clockwise
-
-For polygons that do not cross the dateline, vertex order will not matter in
-Elasticsearch. For polygons that do cross the dateline, Elasticsearch requires
-vertex ordering to comply with the OGC specification. Otherwise, an unintended polygon
-may be created and unexpected query/filter results will be returned.
-
-The following provides an example of an ambiguous polygon.  Elasticsearch will apply
-OGC standards to eliminate ambiguity resulting in a polygon that crosses the dateline.
-
-[source,js]
---------------------------------------------------
-{
-    "location" : {
-        "type" : "polygon",
-        "coordinates" : [
-            [ [-177.0, 10.0], [176.0, 15.0], [172.0, 0.0], [176.0, -15.0], [-177.0, -10.0], [-177.0, 10.0] ],
-            [ [178.2, 8.2], [-178.8, 8.2], [-180.8, -8.8], [178.2, 8.8] ]
-        ]
-    }
-}
---------------------------------------------------
-
-An `orientation` parameter can be defined when setting the geo_shape mapping (see <<geo-shape-mapping-options>>). This will define vertex
-order for the coordinate list on the mapped geo_shape field. It can also be overridden on each document.  The following is an example for
-overriding the orientation on a document:
-
-[source,js]
---------------------------------------------------
-{
-    "location" : {
-        "type" : "polygon",
-        "orientation" : "clockwise",
-        "coordinates" : [
-            [ [-177.0, 10.0], [176.0, 15.0], [172.0, 0.0], [176.0, -15.0], [-177.0, -10.0], [-177.0, 10.0] ],
-            [ [178.2, 8.2], [-178.8, 8.2], [-180.8, -8.8], [178.2, 8.8] ]
-        ]
-    }
-}
---------------------------------------------------
-
-[float]
-===== http://www.geojson.org/geojson-spec.html#id5[MultiPoint]
-
-A list of geojson points.
-
-[source,js]
---------------------------------------------------
-{
-    "location" : {
-        "type" : "multipoint",
-        "coordinates" : [
-            [102.0, 2.0], [103.0, 2.0]
-        ]
-    }
-}
---------------------------------------------------
-
-[float]
-===== http://www.geojson.org/geojson-spec.html#id6[MultiLineString]
-
-A list of geojson linestrings.
-
-[source,js]
---------------------------------------------------
-{
-    "location" : {
-        "type" : "multilinestring",
-        "coordinates" : [
-            [ [102.0, 2.0], [103.0, 2.0], [103.0, 3.0], [102.0, 3.0] ],
-            [ [100.0, 0.0], [101.0, 0.0], [101.0, 1.0], [100.0, 1.0] ],
-            [ [100.2, 0.2], [100.8, 0.2], [100.8, 0.8], [100.2, 0.8] ]
-        ]
-    }
-}
---------------------------------------------------
-
-[float]
-===== http://www.geojson.org/geojson-spec.html#id7[MultiPolygon]
-
-A list of geojson polygons.
-
-[source,js]
---------------------------------------------------
-{
-    "location" : {
-        "type" : "multipolygon",
-        "coordinates" : [
-            [ [[102.0, 2.0], [103.0, 2.0], [103.0, 3.0], [102.0, 3.0], [102.0, 2.0]] ],
-
-            [ [[100.0, 0.0], [101.0, 0.0], [101.0, 1.0], [100.0, 1.0], [100.0, 0.0]],
-              [[100.2, 0.2], [100.8, 0.2], [100.8, 0.8], [100.2, 0.8], [100.2, 0.2]] ]
-        ]
-    }
-}
---------------------------------------------------
-
-[float]
-===== http://geojson.org/geojson-spec.html#geometrycollection[Geometry Collection]
-
-A collection of geojson geometry objects.
-
-[source,js]
---------------------------------------------------
-{
-    "location" : {
-        "type": "geometrycollection",
-        "geometries": [
-            {
-                "type": "point",
-                "coordinates": [100.0, 0.0]
-            },
-            {
-                "type": "linestring",
-                "coordinates": [ [101.0, 0.0], [102.0, 1.0] ]
-            }
-        ]
-    }
-}
---------------------------------------------------
-
-
-[float]
-===== Envelope
-
-Elasticsearch supports an `envelope` type, which consists of coordinates
-for upper left and lower right points of the shape to represent a
-bounding rectangle:
-
-[source,js]
---------------------------------------------------
-{
-    "location" : {
-        "type" : "envelope",
-        "coordinates" : [ [-45.0, 45.0], [45.0, -45.0] ]
-    }
-}
---------------------------------------------------
-
-[float]
-===== Circle
-
-Elasticsearch supports a `circle` type, which consists of a center
-point with a radius:
-
-[source,js]
---------------------------------------------------
-{
-    "location" : {
-        "type" : "circle",
-        "coordinates" : [-45.0, 45.0],
-        "radius" : "100m"
-    }
-}
---------------------------------------------------
-
-Note: The inner `radius` field is required. If not specified, then
-the units of the `radius` will default to `METERS`.
-
-[float]
-==== Sorting and Retrieving index Shapes
-
-Due to the complex input structure and index representation of shapes,
-it is not currently possible to sort shapes or retrieve their fields
-directly. The geo_shape value is only retrievable through the `_source`
-field.
diff --git a/docs/reference/mapping/types/geo-shape.asciidoc b/docs/reference/mapping/types/geo-shape.asciidoc
new file mode 100644
index 0000000..563d04d
--- /dev/null
+++ b/docs/reference/mapping/types/geo-shape.asciidoc
@@ -0,0 +1,426 @@
+[[geo-shape]]
+=== Geo-Shape datatype
+
+The `geo_shape` datatype facilitates the indexing of and searching
+with arbitrary geo shapes such as rectangles and polygons. It should be
+used when either the data being indexed or the queries being executed
+contain shapes other than just points.
+
+You can query documents using this type using
+<<query-dsl-geo-shape-query,geo_shape Query>>.
+
+[[geo-shape-mapping-options]]
+[float]
+==== Mapping Options
+
+The geo_shape mapping maps geo_json geometry objects to the geo_shape
+type. To enable it, users must explicitly map fields to the geo_shape
+type.
+
+[cols="<,<",options="header",]
+|=======================================================================
+|Option |Description
+
+|`tree` |Name of the PrefixTree implementation to be used: `geohash` for
+GeohashPrefixTree and `quadtree` for QuadPrefixTree. Defaults to
+`geohash`.
+
+|`precision` |This parameter may be used instead of `tree_levels` to set
+an appropriate value for the `tree_levels` parameter. The value
+specifies the desired precision and Elasticsearch will calculate the
+best tree_levels value to honor this precision. The value should be a
+number followed by an optional distance unit. Valid distance units
+include: `in`, `inch`, `yd`, `yard`, `mi`, `miles`, `km`, `kilometers`,
+`m`,`meters` (default), `cm`,`centimeters`, `mm`, `millimeters`.
+
+|`tree_levels` |Maximum number of layers to be used by the PrefixTree.
+This can be used to control the precision of shape representations and
+therefore how many terms are indexed. Defaults to the default value of
+the chosen PrefixTree implementation. Since this parameter requires a
+certain level of understanding of the underlying implementation, users
+may use the `precision` parameter instead. However, Elasticsearch only
+uses the tree_levels parameter internally and this is what is returned
+via the mapping API even if you use the precision parameter.
+
+|`distance_error_pct` |Used as a hint to the PrefixTree about how
+precise it should be. Defaults to 0.025 (2.5%) with 0.5 as the maximum
+supported value. PERFORMANCE NOTE: This value will be default to 0 if a `precision` or
+`tree_level` definition is explicitly defined. This guarantees spatial precision
+at the level defined in the mapping. This can lead to significant memory usage
+for high resolution shapes with low error (e.g., large shapes at 1m with < 0.001 error).
+To improve indexing performance (at the cost of query accuracy) explicitly define
+`tree_level` or `precision` along with a reasonable `distance_error_pct`, noting
+that large shapes will have greater false positives.
+
+|`orientation` |Optionally define how to interpret vertex order for
+polygons / multipolygons.  This parameter defines one of two coordinate
+system rules (Right-hand or Left-hand) each of which can be specified in three
+different ways. 1. Right-hand rule (default): `right`, `ccw`, `counterclockwise`,
+2. Left-hand rule: `left`, `cw`, `clockwise`. The default orientation
+(`counterclockwise`) complies with the OGC standard which defines
+outer ring vertices in counterclockwise order with inner ring(s) vertices (holes)
+in clockwise order. Setting this parameter in the geo_shape mapping explicitly
+sets vertex order for the coordinate list of a geo_shape field but can be
+overridden in each individual GeoJSON document.
+|=======================================================================
+
+[float]
+==== Prefix trees
+
+To efficiently represent shapes in the index, Shapes are converted into
+a series of hashes representing grid squares using implementations of a
+PrefixTree. The tree notion comes from the fact that the PrefixTree uses
+multiple grid layers, each with an increasing level of precision to
+represent the Earth.
+
+Multiple PrefixTree implementations are provided:
+
+* GeohashPrefixTree - Uses
+http://en.wikipedia.org/wiki/Geohash[geohashes] for grid squares.
+Geohashes are base32 encoded strings of the bits of the latitude and
+longitude interleaved. So the longer the hash, the more precise it is.
+Each character added to the geohash represents another tree level and
+adds 5 bits of precision to the geohash. A geohash represents a
+rectangular area and has 32 sub rectangles. The maximum amount of levels
+in Elasticsearch is 24.
+* QuadPrefixTree - Uses a
+http://en.wikipedia.org/wiki/Quadtree[quadtree] for grid squares.
+Similar to geohash, quad trees interleave the bits of the latitude and
+longitude the resulting hash is a bit set. A tree level in a quad tree
+represents 2 bits in this bit set, one for each coordinate. The maximum
+amount of levels for the quad trees in Elasticsearch is 50.
+
+[float]
+===== Accuracy
+
+Geo_shape does not provide 100% accuracy and depending on how it is
+configured it may return some false positives or false negatives for
+certain queries. To mitigate this, it is important to select an
+appropriate value for the tree_levels parameter and to adjust
+expectations accordingly. For example, a point may be near the border of
+a particular grid cell and may thus not match a query that only matches the
+cell right next to it -- even though the shape is very close to the point.
+
+[float]
+===== Example
+
+[source,js]
+--------------------------------------------------
+{
+    "properties": {
+        "location": {
+            "type": "geo_shape",
+            "tree": "quadtree",
+            "precision": "1m"
+        }
+    }
+}
+--------------------------------------------------
+
+This mapping maps the location field to the geo_shape type using the
+quad_tree implementation and a precision of 1m. Elasticsearch translates
+this into a tree_levels setting of 26.
+
+[float]
+===== Performance considerations
+
+Elasticsearch uses the paths in the prefix tree as terms in the index
+and in queries. The higher the levels is (and thus the precision), the
+more terms are generated. Of course, calculating the terms, keeping them in
+memory, and storing them on disk all have a price. Especially with higher
+tree levels, indices can become extremely large even with a modest
+amount of data. Additionally, the size of the features also matters.
+Big, complex polygons can take up a lot of space at higher tree levels.
+Which setting is right depends on the use case. Generally one trades off
+accuracy against index size and query performance.
+
+The defaults in Elasticsearch for both implementations are a compromise
+between index size and a reasonable level of precision of 50m at the
+equator. This allows for indexing tens of millions of shapes without
+overly bloating the resulting index too much relative to the input size.
+
+[float]
+==== Input Structure
+
+The http://www.geojson.org[GeoJSON] format is used to represent
+http://geojson.org/geojson-spec.html#geometry-objects[shapes] as input
+as follows:
+
+[cols="<,<,<",options="header",]
+|=======================================================================
+|GeoJSON Type |Elasticsearch Type |Description
+
+|`Point` |`point` |A single geographic coordinate.
+|`LineString` |`linestring` |An arbitrary line given two or more points.
+|`Polygon` |`polygon` |A _closed_ polygon whose first and last point
+must match, thus requiring `n + 1` vertices to create an `n`-sided
+polygon and a minimum of `4` vertices.
+|`MultiPoint` |`multipoint` |An array of unconnected, but likely related
+points.
+|`MultiLineString` |`multilinestring` |An array of separate linestrings.
+|`MultiPolygon` |`multipolygon` |An array of separate polygons.
+|`GeometryCollection` |`geometrycollection` | A GeoJSON shape similar to the
+`multi*` shapes except that multiple types can coexist (e.g., a Point
+and a LineString).
+|`N/A` |`envelope` |A bounding rectangle, or envelope, specified by
+specifying only the top left and bottom right points.
+|`N/A` |`circle` |A circle specified by a center point and radius with
+units, which default to `METERS`.
+|=======================================================================
+
+[NOTE]
+=============================================
+For all types, both the inner `type` and `coordinates` fields are
+required.
+
+In GeoJSON, and therefore Elasticsearch, the correct *coordinate
+order is longitude, latitude (X, Y)* within coordinate arrays. This
+differs from many Geospatial APIs (e.g., Google Maps) that generally
+use the colloquial latitude, longitude (Y, X).
+=============================================
+
+[float]
+===== http://geojson.org/geojson-spec.html#id2[Point]
+
+A point is a single geographic coordinate, such as the location of a
+building or the current position given by a smartphone's Geolocation
+API.
+
+[source,js]
+--------------------------------------------------
+{
+    "location" : {
+        "type" : "point",
+        "coordinates" : [-77.03653, 38.897676]
+    }
+}
+--------------------------------------------------
+
+[float]
+===== http://geojson.org/geojson-spec.html#id3[LineString]
+
+A `linestring` defined by an array of two or more positions. By
+specifying only two points, the `linestring` will represent a straight
+line.  Specifying more than two points creates an arbitrary path.
+
+[source,js]
+--------------------------------------------------
+{
+    "location" : {
+        "type" : "linestring",
+        "coordinates" : [[-77.03653, 38.897676], [-77.009051, 38.889939]]
+    }
+}
+--------------------------------------------------
+
+The above `linestring` would draw a straight line starting at the White
+House to the US Capitol Building.
+
+[float]
+===== http://www.geojson.org/geojson-spec.html#id4[Polygon]
+
+A polygon is defined by a list of a list of points. The first and last
+points in each (outer) list must be the same (the polygon must be
+closed).
+
+[source,js]
+--------------------------------------------------
+{
+    "location" : {
+        "type" : "polygon",
+        "coordinates" : [
+            [ [100.0, 0.0], [101.0, 0.0], [101.0, 1.0], [100.0, 1.0], [100.0, 0.0] ]
+        ]
+    }
+}
+--------------------------------------------------
+
+The first array represents the outer boundary of the polygon, the other
+arrays represent the interior shapes ("holes"):
+
+[source,js]
+--------------------------------------------------
+{
+    "location" : {
+        "type" : "polygon",
+        "coordinates" : [
+            [ [100.0, 0.0], [101.0, 0.0], [101.0, 1.0], [100.0, 1.0], [100.0, 0.0] ],
+            [ [100.2, 0.2], [100.8, 0.2], [100.8, 0.8], [100.2, 0.8], [100.2, 0.2] ]
+        ]
+    }
+}
+--------------------------------------------------
+
+*IMPORTANT NOTE:* GeoJSON does not mandate a specific order for vertices thus ambiguous
+polygons around the dateline and poles are possible. To alleviate ambiguity
+the Open Geospatial Consortium (OGC)
+http://www.opengeospatial.org/standards/sfa[Simple Feature Access] specification
+defines the following vertex ordering:
+
+* Outer Ring - Counterclockwise
+* Inner Ring(s) / Holes - Clockwise
+
+For polygons that do not cross the dateline, vertex order will not matter in
+Elasticsearch. For polygons that do cross the dateline, Elasticsearch requires
+vertex ordering to comply with the OGC specification. Otherwise, an unintended polygon
+may be created and unexpected query/filter results will be returned.
+
+The following provides an example of an ambiguous polygon.  Elasticsearch will apply
+OGC standards to eliminate ambiguity resulting in a polygon that crosses the dateline.
+
+[source,js]
+--------------------------------------------------
+{
+    "location" : {
+        "type" : "polygon",
+        "coordinates" : [
+            [ [-177.0, 10.0], [176.0, 15.0], [172.0, 0.0], [176.0, -15.0], [-177.0, -10.0], [-177.0, 10.0] ],
+            [ [178.2, 8.2], [-178.8, 8.2], [-180.8, -8.8], [178.2, 8.8] ]
+        ]
+    }
+}
+--------------------------------------------------
+
+An `orientation` parameter can be defined when setting the geo_shape mapping (see <<geo-shape-mapping-options>>). This will define vertex
+order for the coordinate list on the mapped geo_shape field. It can also be overridden on each document.  The following is an example for
+overriding the orientation on a document:
+
+[source,js]
+--------------------------------------------------
+{
+    "location" : {
+        "type" : "polygon",
+        "orientation" : "clockwise",
+        "coordinates" : [
+            [ [-177.0, 10.0], [176.0, 15.0], [172.0, 0.0], [176.0, -15.0], [-177.0, -10.0], [-177.0, 10.0] ],
+            [ [178.2, 8.2], [-178.8, 8.2], [-180.8, -8.8], [178.2, 8.8] ]
+        ]
+    }
+}
+--------------------------------------------------
+
+[float]
+===== http://www.geojson.org/geojson-spec.html#id5[MultiPoint]
+
+A list of geojson points.
+
+[source,js]
+--------------------------------------------------
+{
+    "location" : {
+        "type" : "multipoint",
+        "coordinates" : [
+            [102.0, 2.0], [103.0, 2.0]
+        ]
+    }
+}
+--------------------------------------------------
+
+[float]
+===== http://www.geojson.org/geojson-spec.html#id6[MultiLineString]
+
+A list of geojson linestrings.
+
+[source,js]
+--------------------------------------------------
+{
+    "location" : {
+        "type" : "multilinestring",
+        "coordinates" : [
+            [ [102.0, 2.0], [103.0, 2.0], [103.0, 3.0], [102.0, 3.0] ],
+            [ [100.0, 0.0], [101.0, 0.0], [101.0, 1.0], [100.0, 1.0] ],
+            [ [100.2, 0.2], [100.8, 0.2], [100.8, 0.8], [100.2, 0.8] ]
+        ]
+    }
+}
+--------------------------------------------------
+
+[float]
+===== http://www.geojson.org/geojson-spec.html#id7[MultiPolygon]
+
+A list of geojson polygons.
+
+[source,js]
+--------------------------------------------------
+{
+    "location" : {
+        "type" : "multipolygon",
+        "coordinates" : [
+            [ [[102.0, 2.0], [103.0, 2.0], [103.0, 3.0], [102.0, 3.0], [102.0, 2.0]] ],
+
+            [ [[100.0, 0.0], [101.0, 0.0], [101.0, 1.0], [100.0, 1.0], [100.0, 0.0]],
+              [[100.2, 0.2], [100.8, 0.2], [100.8, 0.8], [100.2, 0.8], [100.2, 0.2]] ]
+        ]
+    }
+}
+--------------------------------------------------
+
+[float]
+===== http://geojson.org/geojson-spec.html#geometrycollection[Geometry Collection]
+
+A collection of geojson geometry objects.
+
+[source,js]
+--------------------------------------------------
+{
+    "location" : {
+        "type": "geometrycollection",
+        "geometries": [
+            {
+                "type": "point",
+                "coordinates": [100.0, 0.0]
+            },
+            {
+                "type": "linestring",
+                "coordinates": [ [101.0, 0.0], [102.0, 1.0] ]
+            }
+        ]
+    }
+}
+--------------------------------------------------
+
+
+[float]
+===== Envelope
+
+Elasticsearch supports an `envelope` type, which consists of coordinates
+for upper left and lower right points of the shape to represent a
+bounding rectangle:
+
+[source,js]
+--------------------------------------------------
+{
+    "location" : {
+        "type" : "envelope",
+        "coordinates" : [ [-45.0, 45.0], [45.0, -45.0] ]
+    }
+}
+--------------------------------------------------
+
+[float]
+===== Circle
+
+Elasticsearch supports a `circle` type, which consists of a center
+point with a radius:
+
+[source,js]
+--------------------------------------------------
+{
+    "location" : {
+        "type" : "circle",
+        "coordinates" : [-45.0, 45.0],
+        "radius" : "100m"
+    }
+}
+--------------------------------------------------
+
+Note: The inner `radius` field is required. If not specified, then
+the units of the `radius` will default to `METERS`.
+
+[float]
+==== Sorting and Retrieving index Shapes
+
+Due to the complex input structure and index representation of shapes,
+it is not currently possible to sort shapes or retrieve their fields
+directly. The geo_shape value is only retrievable through the `_source`
+field.
diff --git a/docs/reference/mapping/types/ip-type.asciidoc b/docs/reference/mapping/types/ip-type.asciidoc
deleted file mode 100644
index e257534..0000000
--- a/docs/reference/mapping/types/ip-type.asciidoc
+++ /dev/null
@@ -1,40 +0,0 @@
-[[mapping-ip-type]]
-=== IP Type
-
-An `ip` mapping type allows to store _ipv4_ addresses in a numeric form
-allowing to easily sort, and range query it (using ip values).
-
-The following table lists all the attributes that can be used with an ip
-type:
-
-[cols="<,<",options="header",]
-|=======================================================================
-|Attribute |Description
-|`index_name` |The name of the field that will be stored in the index.
-Defaults to the property/field name.
-
-|`store` |Set to `true` to store actual field in the index, `false` to not
-store it. Defaults to `false` (note, the JSON document itself is stored,
-and it can be retrieved from it).
-
-|`index` |Set to `no` if the value should not be indexed. In this case,
-`store` should be set to `true`, since if it's not indexed and not
-stored, there is nothing to do with it.
-
-|`precision_step` |The precision step (influences the number of terms 
-generated for each number value). Defaults to `16`.
-
-|`boost` |The boost value. Defaults to `1.0`.
-
-|`null_value` |When there is a (JSON) null value for the field, use the
-`null_value` as the field value. Defaults to not adding the field at
-all.
-
-|`include_in_all` |Should the field be included in the `_all` field (if
-enabled). Defaults to `true` or to the parent `object` type setting.
-
-|`doc_values` |Set to `true` to store field values in a column-stride fashion.
-Automatically set to `true` when the <<fielddata-formats,`fielddata` format>> is `doc_values`.
-
-|=======================================================================
-
diff --git a/docs/reference/mapping/types/ip.asciidoc b/docs/reference/mapping/types/ip.asciidoc
new file mode 100644
index 0000000..9610466
--- /dev/null
+++ b/docs/reference/mapping/types/ip.asciidoc
@@ -0,0 +1,89 @@
+[[ip]]
+=== IPv4 datatype
+
+An `ip` field is really a <<number,`long`>> field which accepts
+https://en.wikipedia.org/wiki/IPv4[IPv4] addresses and indexes them as long
+values:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "ip_addr": {
+          "type": "ip"
+        }
+      }
+    }
+  }
+}
+
+PUT my_index/my_type/1
+{
+  "ip_addr": "192.168.1.1"
+}
+
+GET my_index/_search
+{
+  "query": {
+    "range": {
+      "ip_addr": {
+        "gte": "192.168.1.0",
+        "lt":  "192.168.2.0"
+      }
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+
+
+[[ip-params]]
+==== Parameters for `ip` fields
+
+The following parameters are accepted by `ip` fields:
+
+[horizontal]
+
+<<index-boost,`boost`>>::
+
+    Field-level index time boosting. Accepts a floating point number, defaults
+    to `1.0`.
+
+<<doc-values,`doc_values`>>::
+
+    Can the field value be used for sorting, aggregations, or scripting?
+    Accepts `true` (default) or `false`.
+
+<<include-in-all,`include_in_all`>>::
+
+    Whether or not the field value should be included in the
+    <<mapping-all-field,`_all`>> field? Accepts `true` or `false`.  Defaults
+    to `false` if <<mapping-index,`index`>> is set to `no`, or if a parent
+    <<object,`object`>> field sets `include_in_all` to `false`.
+    Otherwise defaults to `true`.
+
+<<mapping-index,`index`>>::
+
+    Should the field be searchable? Accepts `not_analyzed` (default) and `no`.
+
+<<null-value,`null_value`>>::
+
+    Accepts an IPv4 value which is substituted for any explicit `null` values.
+    Defaults to `null`, which means the field is treated as missing.
+
+<<precision-step,`precision_step`>>::
+
+    Controls the number of extra terms that are indexed to make
+    <<query-dsl-range-query,`range` queries>> faster. Defaults to `16`.
+
+<<mapping-store,`store`>>::
+
+    Whether the field value should be stored and retrievable separately from
+    the <<mapping-source-field,`_source`>> field. Accepts `true` or `false`
+    (default).
+
+
+NOTE: IPv6 addresses are not supported yet.
diff --git a/docs/reference/mapping/types/nested-type.asciidoc b/docs/reference/mapping/types/nested-type.asciidoc
deleted file mode 100644
index 1427c93..0000000
--- a/docs/reference/mapping/types/nested-type.asciidoc
+++ /dev/null
@@ -1,165 +0,0 @@
-[[mapping-nested-type]]
-=== Nested Type
-
-The `nested` type works like the <<mapping-object-type,`object` type>> except
-that an array of `objects` is flattened, while an array of `nested`  objects
-allows each object to be queried independently.  To explain, consider this
-document:
-
-[source,js]
---------------------------------------------------
-{
-    "group" : "fans",
-    "user" : [
-        {
-            "first" : "John",
-            "last" :  "Smith"
-        },
-        {
-            "first" : "Alice",
-            "last" :  "White"
-        },
-    ]
-}
---------------------------------------------------
-
-If the `user` field is of type `object`, this document would be indexed
-internally something like this:
-
-[source,js]
---------------------------------------------------
-{
-    "group" :        "fans",
-    "user.first" : [ "alice", "john" ],
-    "user.last" :  [ "smith", "white" ]
-}
---------------------------------------------------
-
-The `first` and `last` fields are flattened, and the association between
-`alice` and `white` is lost.  This document would incorrectly match a query
-for `alice AND smith`.
-
-If the `user` field is of type `nested`, each object is indexed as a separate
-document, something like this:
-
-[source,js]
---------------------------------------------------
-{ <1>
-    "user.first" : "alice",
-    "user.last" :  "white"
-}
-{ <1>
-    "user.first" : "john",
-    "user.last" :  "smith"
-}
-{ <2>
-    "group" :       "fans"
-}
---------------------------------------------------
-<1> Hidden nested documents.
-<2> Visible ``parent'' document.
-
-By keeping each nested object separate, the association between the
-`user.first` and `user.last` fields is maintained. The query for `alice AND
-smith` would *not* match this document.
-
-Searching on nested docs can be done using either the
-<<query-dsl-nested-query,nested query>>.
-
-==== Mapping
-
-The mapping for `nested` fields is the same as `object` fields, except that it
-uses type `nested`:
-
-[source,js]
---------------------------------------------------
-{
-    "type1" : {
-        "properties" : {
-            "user" : {
-                "type" : "nested",
-                "properties": {
-                    "first" : {"type": "string" },
-                    "last"  : {"type": "string" }
-                }
-            }
-        }
-    }
-}
---------------------------------------------------
-
-NOTE: changing an `object` type to `nested` type requires reindexing.
-
-You may want to index inner objects both as `nested` fields *and*  as flattened
-`object` fields, eg for highlighting.  This can be achieved by setting
-`include_in_parent` to `true`:
-
-[source,js]
---------------------------------------------------
-{
-    "type1" : {
-        "properties" : {
-            "user" : {
-                "type" : "nested",
-                "include_in_parent": true,
-                "properties": {
-                    "first" : {"type": "string" },
-                    "last"  : {"type": "string" }
-                }
-            }
-        }
-    }
-}
---------------------------------------------------
-
-The result of indexing our example document would be something like this:
-
-[source,js]
---------------------------------------------------
-{ <1>
-    "user.first" : "alice",
-    "user.last" :  "white"
-}
-{ <1>
-    "user.first" : "john",
-    "user.last" :  "smith"
-}
-{ <2>
-    "group" :        "fans",
-    "user.first" : [ "alice", "john" ],
-    "user.last" :  [ "smith", "white" ]
-}
---------------------------------------------------
-<1> Hidden nested documents.
-<2> Visible ``parent'' document.
-
-
-Nested fields may contain other nested fields.  The `include_in_parent` object
-refers to the direct parent of the field, while the `include_in_root`
-parameter refers only to the topmost ``root'' object or document.
-
-NOTE: The `include_in_parent` and `include_in_root` options do not apply
-to <<mapping-geo-shape-type,`geo_shape` fields>>, which are only ever
-indexed inside the nested document.
-
-Nested docs will automatically use the root doc `_all` field only.
-
-.Internal Implementation
-*********************************************
-Internally, nested objects are indexed as additional documents, but,
-since they can be guaranteed to be indexed within the same "block", it
-allows for extremely fast joining with parent docs.
-
-Those internal nested documents are automatically masked away when doing
-operations against the index (like searching with a match_all query),
-and they bubble out when using the nested query.
-
-Because nested docs are always masked to the parent doc, the nested docs
-can never be accessed outside the scope of the `nested` query. For example
-stored fields can be enabled on fields inside nested objects, but there is
-no way of retrieving them, since stored fields are fetched outside of
-the `nested` query scope.
-
-The `_source` field is always associated with the parent document and
-because of that field values via the source can be fetched for nested object.
-*********************************************
diff --git a/docs/reference/mapping/types/nested.asciidoc b/docs/reference/mapping/types/nested.asciidoc
new file mode 100644
index 0000000..07f8703
--- /dev/null
+++ b/docs/reference/mapping/types/nested.asciidoc
@@ -0,0 +1,201 @@
+[[nested]]
+=== Nested datatype
+
+The `nested` type is a specialised version of the <<object,`object`>> datatype
+that allows arrays of objects to be indexed and queried independently of each
+other.
+
+==== How arrays of objects are flattened
+
+Arrays of inner <<object,`object` fields>> do not work the way you may expect.
+Lucene has no concept of inner objects, so Elasticsearch flattens object
+hierarchies into a simple list of field names and values. For instance, the
+following document:
+
+[source,js]
+--------------------------------------------------
+PUT my_index/my_type/1
+{
+  "group" : "fans",
+  "user" : [ <1>
+    {
+      "first" : "John",
+      "last" :  "Smith"
+    },
+    {
+      "first" : "Alice",
+      "last" :  "White"
+    }
+  ]
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> The `user` field is dynamically added as a field of type `object`.
+
+would be transformed internally into a document that looks more like this:
+
+[source,js]
+--------------------------------------------------
+{
+  "group" :        "fans",
+  "user.first" : [ "alice", "john" ],
+  "user.last" :  [ "smith", "white" ]
+}
+--------------------------------------------------
+
+The `user.first` and `user.last` fields are flattened into multi-value fields,
+and the association between `alice` and `white` is lost.  This document would
+incorrectly match a query for `alice AND smith`:
+
+[source,js]
+--------------------------------------------------
+GET my_index/_search
+{
+  "query": {
+    "bool": {
+      "must": [
+        { "match": { "user.first": "Alice" }},
+        { "match": { "user.last":  "White" }}
+      ]
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+
+==== Using `nested` fields for arrays of objects
+
+If you need to index arrays of objects and to maintain the independence of
+each object in the array, you should used the `nested` datatype instead of the
+<<object,`object`>> datatype.  Internally, nested objects index each object in
+the array as a separate hidden document, meaning that each nested object can be
+queried independently of the others, with the <<query-dsl-nested-query,`nested` query>>:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "user": {
+          "type": "nested" <1>
+        }
+      }
+    }
+  }
+}
+
+PUT my_index/my_type/1
+{
+  "group" : "fans",
+  "user" : [
+    {
+      "first" : "John",
+      "last" :  "Smith"
+    },
+    {
+      "first" : "Alice",
+      "last" :  "White"
+    }
+  ]
+}
+
+GET my_index/_search
+{
+  "query": {
+    "nested": {
+      "path": "user",
+      "query": {
+        "bool": {
+          "must": [
+            { "match": { "user.first": "Alice" }},
+            { "match": { "user.last":  "White" }} <2>
+          ]
+        }
+      }
+    }
+  }
+}
+
+GET my_index/_search
+{
+  "query": {
+    "nested": {
+      "path": "user",
+      "query": {
+        "bool": {
+          "must": [
+            { "match": { "user.first": "Alice" }},
+            { "match": { "user.last":  "Smith" }} <3>
+          ]
+        }
+      },
+      "inner_hits": { <4>
+        "highlight": {
+          "fields": {
+            "user.first": {}
+          }
+        }
+      }
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> The `user` field is mapped as type `nested` instead of type `object`.
+<2> This query doesn't match because `Alice` and `White` are not in the same nested object.
+<3> This query matches because `Alice` and `White` are in the same nested object.
+<4> `inner_hits` allow us to highlight the matching nested documents.
+
+
+Nested documents can be:
+
+* queried with the <<query-dsl-nested-query,`nested`>> query.
+* analyzed with the <<search-aggregations-bucket-nested-aggregation,`nested`>>
+  and <<search-aggregations-bucket-reverse-nested-aggregation, `reverse_nested`>>
+  aggregations.
+* sorted with <<nested-sorting,nested sorting>>.
+* retrieved and highlighted with <<nested-inner-hits,nested inner hits>>.
+
+
+[[nested-params]]
+==== Parameters for `nested` fields
+
+The following parameters are accepted by `nested` fields:
+
+[horizontal]
+<<dynamic,`dynamic`>>::
+
+    Whether or not new `properties` should be added dynamically to an existing
+    nested object.  Accepts `true` (default), `false` and `strict`.
+
+<<include-in-all,`include_in_all`>>::
+
+    Sets the default `include_in_all` value for all the `properties` within
+    the nested object. Nested documents do not have their own `_all` field.
+    Instead, values are added to the `_all` field of the main ``root''
+    document.
+
+<<properties,`properties`>>::
+
+    The fields within the nested object, which can be of any
+    <<mapping-types,datatype>>, including `nested`. New properties
+    may be added to an existing nested object.
+
+
+[IMPORTANT]
+=============================================
+
+Because nested documents are indexed as separate documents, they can only be
+accessed  within the scope of the `nested` query, the
+`nested`/`reverse_nested`, or <<nested-inner-hits,nested inner hits>>.
+
+For instance, if a string field within a nested document has
+<<index-options,`index_options`>> set to `offsets` to allow use of the postings
+highlighter, these offsets will not be available during the main highlighting
+phase.  Instead, highlighting needs to be performed via
+<<nested-inner-hits,nested inner hits>>.
+
+=============================================
+
diff --git a/docs/reference/mapping/types/numeric.asciidoc b/docs/reference/mapping/types/numeric.asciidoc
new file mode 100644
index 0000000..f04efa1
--- /dev/null
+++ b/docs/reference/mapping/types/numeric.asciidoc
@@ -0,0 +1,93 @@
+[[number]]
+=== Numeric datatypes
+
+The following numeric types are supported:
+
+[horizontal]
+`long`::    A signed 64-bit integer with a minimum value of +-2^63^+ and a maximum value of +2^63^-1+.
+`integer`:: A signed 32-bit integer with a minimum value of +-2^31^+ and a maximum value of +2^31^-1+.
+`short`::   A signed 16-bit integer with a minimum value of +-32,768+ and a maximum value of +32,767+.
+`byte`::    A signed 8-bit integer with a minimum value of +-128+ and a maximum value of +127+.
+`double`::  A double-precision 64-bit IEEE 754 floating point.
+`float`::   A single-precision 32-bit IEEE 754 floating point.
+
+Below is an example of configuring a mapping with numeric fields:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "number_of_bytes": {
+          "type": "integer"
+        },
+        "time_in_seconds": {
+          "type": "float"
+        }
+      }
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+
+[[number-params]]
+==== Parameters for numeric fields
+
+The following parameters are accepted by numeric types:
+
+[horizontal]
+
+<<coerce,`coerce`>>::
+
+    Try to convert strings to numbers and truncate fractions for integers.
+    Accepts `true` (default) and `false`.
+
+<<index-boost,`boost`>>::
+
+    Field-level index time boosting. Accepts a floating point number, defaults
+    to `1.0`.
+
+<<doc-values,`doc_values`>>::
+
+    Can the field value be used for sorting, aggregations, or scripting?
+    Accepts `true` (default) or `false`.
+
+<<ignore-malformed,`ignore_malformed`>>::
+
+    If `true`, malformed numbers are ignored. If `false` (default), malformed
+    numbers throw an exception and reject the whole document.
+
+<<include-in-all,`include_in_all`>>::
+
+    Whether or not the field value should be included in the
+    <<mapping-all-field,`_all`>> field? Accepts `true` or `false`.  Defaults
+    to `false` if <<mapping-index,`index`>> is set to `no`, or if a parent
+    <<object,`object`>> field sets `include_in_all` to `false`.
+    Otherwise defaults to `true`.
+
+<<mapping-index,`index`>>::
+
+    Should the field be searchable? Accepts `not_analyzed` (default) and `no`.
+
+<<null-value,`null_value`>>::
+
+    Accepts a numeric value of the same `type` as the field which is
+    substituted for any explicit `null` values.  Defaults to `null`, which
+    means the field is treated as missing.
+
+<<precision-step,`precision_step`>>::
+
+    Controls the number of extra terms that are indexed to make
+    <<query-dsl-range-query,`range` queries>> faster. The default depends on the
+    numeric `type`.
+
+<<mapping-store,`store`>>::
+
+    Whether the field value should be stored and retrievable separately from
+    the <<mapping-source-field,`_source`>> field. Accepts `true` or `false`
+    (default).
+
+
diff --git a/docs/reference/mapping/types/object-type.asciidoc b/docs/reference/mapping/types/object-type.asciidoc
deleted file mode 100644
index b1ef623..0000000
--- a/docs/reference/mapping/types/object-type.asciidoc
+++ /dev/null
@@ -1,179 +0,0 @@
-[[mapping-object-type]]
-=== Object Type
-
-JSON documents are hierarchical in nature, allowing them to define inner
-"objects" within the actual JSON. Elasticsearch completely understands
-the nature of these inner objects and can map them easily, providing
-query support for their inner fields. Because each document can have
-objects with different fields each time, objects mapped this way are
-known as "dynamic". Dynamic mapping is enabled by default. Let's take
-the following JSON as an example:
-
-[source,js]
---------------------------------------------------
-{
-    "tweet" : {
-        "person" : {
-            "name" : {
-                "first_name" : "Shay",
-                "last_name" : "Banon"
-            },
-            "sid" : "12345"
-        },
-        "message" : "This is a tweet!"
-    }
-}
---------------------------------------------------
-
-The above shows an example where a tweet includes the actual `person`
-details. A `person` is an object, with a `sid`, and a `name` object
-which has `first_name` and `last_name`. It's important to note that
-`tweet` is also an object, although it is a special
-<<mapping-root-object-type,root object type>>
-which allows for additional mapping definitions.
-
-The following is an example of explicit mapping for the above JSON:
-
-[source,js]
---------------------------------------------------
-{
-    "tweet" : {
-        "properties" : {
-            "person" : {
-                "type" : "object",
-                "properties" : {
-                    "name" : {
-                        "type" : "object",
-                        "properties" : {
-                            "first_name" : {"type" : "string"},
-                            "last_name" : {"type" : "string"}
-                        }
-                    },
-                    "sid" : {"type" : "string", "index" : "not_analyzed"}
-                }
-            },
-            "message" : {"type" : "string"}
-        }
-    }
-}
---------------------------------------------------
-
-In order to mark a mapping of type `object`, set the `type` to object.
-This is an optional step, since if there are `properties` defined for
-it, it will automatically be identified as an `object` mapping.
-
-[float]
-==== properties
-
-An object mapping can optionally define one or more properties using the
-`properties` tag for a field. Each property can be either another
-`object`, or one of the
-<<mapping-core-types,core_types>>.
-
-[float]
-==== dynamic
-
-One of the most important features of Elasticsearch is its ability to be
-schema-less. This means that, in our example above, the `person` object
-can be indexed later with a new property -- `age`, for example -- and it
-will automatically be added to the mapping definitions. Same goes for
-the `tweet` root object.
-
-This feature is by default turned on, and it's the `dynamic` nature of
-each object mapped. Each object mapped is automatically dynamic, though
-it can be explicitly turned off:
-
-[source,js]
---------------------------------------------------
-{
-    "tweet" : {
-        "properties" : {
-            "person" : {
-                "type" : "object",
-                "properties" : {
-                    "name" : {
-                        "dynamic" : false,
-                        "properties" : {
-                            "first_name" : {"type" : "string"},
-                            "last_name" : {"type" : "string"}
-                        }
-                    },
-                    "sid" : {"type" : "string", "index" : "not_analyzed"}
-                }
-            },
-            "message" : {"type" : "string"}
-        }
-    }
-}
---------------------------------------------------
-
-In the above example, the `name` object mapped is not dynamic, meaning
-that if, in the future, we try to index JSON with a `middle_name` within
-the `name` object, it will get discarded and not added.
-
-There is no performance overhead if an `object` is dynamic, the ability
-to turn it off is provided as a safety mechanism so "malformed" objects
-won't, by mistake, index data that we do not wish to be indexed.
-
-If a dynamic object contains yet another inner `object`, it will be
-automatically added to the index and mapped as well.
-
-When processing dynamic new fields, their type is automatically derived.
-For example, if it is a `number`, it will automatically be treated as
-number <<mapping-core-types,core_type>>. Dynamic
-fields default to their default attributes, for example, they are not
-stored and they are always indexed.
-
-Date fields are special since they are represented as a `string`. Date
-fields are detected if they can be parsed as a date when they are first
-introduced into the system. The set of date formats that are tested
-against can be configured using the `dynamic_date_formats` on the root object,
-which is explained later.
-
-Note, once a field has been added, *its type can not change*. For
-example, if we added age and its value is a number, then it can't be
-treated as a string.
-
-The `dynamic` parameter can also be set to `strict`, meaning that not
-only will new fields not be introduced into the mapping, but also that parsing
-(indexing) docs with such new fields will fail.
-
-[float]
-==== enabled
-
-The `enabled` flag allows to disable parsing and indexing a named object
-completely. This is handy when a portion of the JSON document contains
-arbitrary JSON which should not be indexed, nor added to the mapping.
-For example:
-
-[source,js]
---------------------------------------------------
-{
-    "tweet" : {
-        "properties" : {
-            "person" : {
-                "type" : "object",
-                "properties" : {
-                    "name" : {
-                        "type" : "object",
-                        "enabled" : false
-                    },
-                    "sid" : {"type" : "string", "index" : "not_analyzed"}
-                }
-            },
-            "message" : {"type" : "string"}
-        }
-    }
-}
---------------------------------------------------
-
-In the above, `name` and its content will not be indexed at all.
-
-
-[float]
-==== include_in_all
-
-`include_in_all` can be set on the `object` type level. When set, it
-propagates down to all the inner mappings defined within the `object`
-that do not explicitly set it.
-
diff --git a/docs/reference/mapping/types/object.asciidoc b/docs/reference/mapping/types/object.asciidoc
new file mode 100644
index 0000000..0d159d7
--- /dev/null
+++ b/docs/reference/mapping/types/object.asciidoc
@@ -0,0 +1,105 @@
+[[object]]
+=== Object datatype
+
+JSON documents are hierarchical in nature: the document may contain inner
+objects which, in turn, may contain inner objects themselves:
+
+[source,js]
+--------------------------------------------------
+PUT my_index/my_type/1
+{ <1>
+  "region": "US",
+  "manager": { <2>
+    "age":     30,
+    "name": { <3>
+      "first": "John",
+      "last":  "Smith"
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> The outer document is also a JSON object.
+<2> It contains an inner object called `manager`.
+<3> Which in turn contains an inner object called `name`.
+
+Internally, this document is indexed as a simple, flat list of key-value
+pairs, something like this:
+
+[source,js]
+--------------------------------------------------
+{
+  "region":             "US",
+  "manager.age":        30,
+  "manager.name.first": "John",
+  "manager.name.last":  "Smith"
+}
+--------------------------------------------------
+
+An explicit mapping for the above document could look like this:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": { <1>
+      "properties": {
+        "region": {
+          "type": "string",
+          "index": "not_analyzed"
+        },
+        "manager": { <2>
+          "properties": {
+            "age":  { "type": "integer" },
+            "name": { <3>
+              "properties": {
+                "first": { "type": "string" },
+                "last":  { "type": "string" }
+              }
+            }
+          }
+        }
+      }
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> The mapping type is a type of object, and has a `properties` field.
+<2> The `manager` field is an inner `object` field.
+<3> The `manager.name` field is an inner `object` field within the `manager` field.
+
+You are not required to set the field `type` to `object` explicitly, as this is the default value.
+
+[[object-params]]
+==== Parameters for `object` fields
+
+The following parameters are accepted by `object` fields:
+
+[horizontal]
+<<dynamic,`dynamic`>>::
+
+    Whether or not new `properties` should be added dynamically
+    to an existing object.  Accepts `true` (default), `false`
+    and `strict`.
+
+<<enabled,`enabled`>>::
+
+    Whether the JSON value given for the object field should be
+    parsed and indexed (`true`, default) or completely ignored (`false`).
+
+<<include-in-all,`include_in_all`>>::
+
+    Sets the default `include_in_all` value for all the `properties` within
+    the object. The object itself is not added to the `_all` field.
+
+<<properties,`properties`>>::
+
+    The fields within the object, which can be of any
+    <<mapping-types,datatype>>, including `object`. New properties
+    may be added to an existing object.
+
+IMPORTANT: If you need to index arrays of objects instead of single objects,
+read <<nested>> first.
+
diff --git a/docs/reference/mapping/types/root-object-type.asciidoc b/docs/reference/mapping/types/root-object-type.asciidoc
deleted file mode 100644
index 090f88b..0000000
--- a/docs/reference/mapping/types/root-object-type.asciidoc
+++ /dev/null
@@ -1,190 +0,0 @@
-[[mapping-root-object-type]]
-=== Root Object Type
-
-The root object mapping is an <<mapping-object-type,object type mapping>> that
-maps the root object (the type itself). It supports all of the different
-mappings that can be set using the <<mapping-object-type,object type mapping>>.
-
-The root object mapping allows to index a JSON document that only contains its
-fields. For example, the following `tweet` JSON can be indexed without
-specifying the `tweet` type in the document itself:
-
-[source,js]
---------------------------------------------------
-{
-    "message" : "This is a tweet!"
-}
---------------------------------------------------
-
-[float]
-==== dynamic_date_formats
-
-`dynamic_date_formats` (old setting called `date_formats` still works)
-is the ability to set one or more date formats that will be used to
-detect `date` fields. For example:
-
-[source,js]
---------------------------------------------------
-{
-    "tweet" : {
-        "dynamic_date_formats" : ["yyyy-MM-dd", "dd-MM-yyyy"],
-        "properties" : {
-            "message" : {"type" : "string"}
-        }
-    }
-}
---------------------------------------------------
-
-In the above mapping, if a new JSON field of type string is detected,
-the date formats specified will be used in order to check if its a date.
-If it passes parsing, then the field will be declared with `date` type,
-and will use the matching format as its format attribute. The date
-format itself is explained
-<<mapping-date-format,here>>.
-
-The default formats are: `strictDateOptionalTime` (ISO) and
-`yyyy/MM/dd HH:mm:ss Z||yyyy/MM/dd Z` and `epoch_millis`.
-
-*Note:* `dynamic_date_formats` are used *only* for dynamically added
-date fields, not for `date` fields that you specify in your mapping.
-
-[float]
-==== date_detection
-
-Allows to disable automatic date type detection (if a new field is introduced
-and matches the provided format), for example:
-
-[source,js]
---------------------------------------------------
-{
-    "tweet" : {
-        "date_detection" : false,
-        "properties" : {
-            "message" : {"type" : "string"}
-        }
-    }
-}
---------------------------------------------------
-
-[float]
-==== numeric_detection
-
-Sometimes, even though json has support for native numeric types,
-numeric values are still provided as strings. In order to try and
-automatically detect numeric values from string, the `numeric_detection`
-can be set to `true`. For example:
-
-[source,js]
---------------------------------------------------
-{
-    "tweet" : {
-        "numeric_detection" : true,
-        "properties" : {
-            "message" : {"type" : "string"}
-        }
-    }
-}
---------------------------------------------------
-
-[float]
-==== dynamic_templates
-
-Dynamic templates allow to define mapping templates that will be applied
-when dynamic introduction of fields / objects happens.
-
-IMPORTANT: Dynamic field mappings are only added when a field contains
-a concrete value -- not `null` or an empty array. This means that if the `null_value` option 
-is used in a `dynamic_template`, it will only be applied after the first document 
-with a concrete value for the field has been indexed.
-
-For example, we might want to have all fields to be stored by default,
-or all `string` fields to be stored, or have `string` fields to always
-be indexed with multi fields syntax, once analyzed and once not_analyzed.
-Here is a simple example:
-
-[source,js]
---------------------------------------------------
-{
-    "person" : {
-        "dynamic_templates" : [
-            {
-                "template_1" : {
-                    "match" : "multi*",
-                    "mapping" : {
-                        "type" : "{dynamic_type}",
-                        "index" : "analyzed",
-                        "fields" : {
-                            "org" : {"type": "{dynamic_type}", "index" : "not_analyzed"}
-                        }
-                    }
-                }
-            },
-            {
-                "template_2" : {
-                    "match" : "*",
-                    "match_mapping_type" : "string",
-                    "mapping" : {
-                        "type" : "string",
-                        "index" : "not_analyzed"
-                    }
-                }
-            }
-        ]
-    }
-}
---------------------------------------------------
-
-The above mapping will create a field with multi fields for all field
-names starting with multi, and will map all `string` types to be
-`not_analyzed`.
-
-Dynamic templates are named to allow for simple merge behavior. A new
-mapping, just with a new template can be "put" and that template will be
-added, or if it has the same name, the template will be replaced.
-
-The `match` allow to define matching on the field name. An `unmatch`
-option is also available to exclude fields if they do match on `match`.
-The `match_mapping_type` controls if this template will be applied only
-for dynamic fields of the specified type (as guessed by the json
-format).
-
-Another option is to use `path_match`, which allows to match the dynamic
-template against the "full" dot notation name of the field (for example
-`obj1.*.value` or `obj1.obj2.*`), with the respective `path_unmatch`.
-
-The format of all the matching is simple format, allowing to use * as a
-matching element supporting simple patterns such as xxx*, *xxx, xxx*yyy
-(with arbitrary number of pattern types), as well as direct equality.
-The `match_pattern` can be set to `regex` to allow for regular
-expression based matching.
-
-The `mapping` element provides the actual mapping definition. The
-`{name}` keyword can be used and will be replaced with the actual
-dynamic field name being introduced. The `{dynamic_type}` (or
-`{dynamicType}`) can be used and will be replaced with the mapping
-derived based on the field type (or the derived type, like `date`).
-
-Complete generic settings can also be applied, for example, to have all
-mappings be stored, just set:
-
-[source,js]
---------------------------------------------------
-{
-    "person" : {
-        "dynamic_templates" : [
-            {
-                "store_generic" : {
-                    "match" : "*",
-                    "mapping" : {
-                        "store" : true
-                    }
-                }
-            }
-        ]
-    }
-}
---------------------------------------------------
-
-Such generic templates should be placed at the end of the
-`dynamic_templates` list because when two or more dynamic templates
-match a field, only the first matching one from the list is used.
diff --git a/docs/reference/mapping/types/string.asciidoc b/docs/reference/mapping/types/string.asciidoc
new file mode 100644
index 0000000..61041ad
--- /dev/null
+++ b/docs/reference/mapping/types/string.asciidoc
@@ -0,0 +1,170 @@
+[[string]]
+=== String datatype
+
+Fields of type `string` accept text values.  Strings may be sub-divided into:
+
+Full text::
++
+--
+
+Full text values, like the body of an email, are typically used for text based
+relevance searches, such as: _Find the most relevant documents that match a
+query for "quick brown fox"_.
+
+These fields are `analyzed`, that is they are passed through an
+<<analysis,analyzer>> to convert the string into a list of individual terms
+before being indexed. The analysis process allows Elasticsearch to search for
+individual words _within_  each full text field.  Full text fields are not
+used for sorting and seldom used for aggregations (although the
+<<search-aggregations-bucket-significantterms-aggregation,significant terms aggregation>> is a notable exception).
+
+--
+
+Keywords::
+
+Keywords are exact values like email addresses, hostnames, status codes, or
+tags.  They are typically used for filtering (_Find me all blog posts where
+++status++ is ++published++_), for sorting, and for aggregations. Keyword
+fields are `not_analyzed`.  Instead, the exact string value is added to the
+index as a single term.
+
+Below is an example of a mapping for a full text (`analyzed`) and a keyword
+(`not_analyzed`) string field:
+
+[source,js]
+--------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "full_name": { <1>
+          "type":  "string"
+        },
+        "status": {
+          "type":  "string", <2>
+          "index": "not_analyzed"
+        }
+      }
+    }
+  }
+}
+--------------------------------
+// AUTOSENSE
+<1> The `full_name` field is an `analyzed` full text field -- `index:analyzed` is the default.
+<2> The `status` field is a `not_analyzed` keyword field.
+
+Sometimes it is useful to have both a full text (`analyzed`) and a keyword
+(`not_analyzed`) version of the same field: one for full text search and the
+other for aggregations and sorting. This can be achieved with
+<<multi-fields,multi-fields>>.
+
+
+[[string-params]]
+==== Parameters for string fields
+
+The following parameters are accepted by `string` fields:
+
+[horizontal]
+
+<<analyzer,`analyzer`>>::
+
+    The <<analysis,analyzer>> which should be used for
+    <<mapping-index,`analyzed`>> string fields, both at index-time and at
+    search-time (unless overridden by the  <<search-analyzer,`search_analyzer`>>).
+    Defaults to the default index analyzer, or the
+    <<analysis-standard-analyzer,`standard` analyzer>>.
+
+<<index-boost,`boost`>>::
+
+    Field-level index time boosting. Accepts a floating point number, defaults
+    to `1.0`.
+
+<<doc-values,`doc_values`>>::
+
+    Can the field use on-disk index-time doc values for sorting, aggregations,
+    or scripting? Accepts `true` or `false`. Defaults to `true` for
+    `not_analyzed` fields. Analyzed fields do not support doc values.
+
+<<fielddata,`fielddata`>>::
+
+    Can the field use in-memory fielddata for sorting, aggregations,
+    or scripting? Accepts `disabled` or `paged_bytes` (default).
+    Not analyzed fields will use <<doc-values,doc values>> in preference
+    to fielddata.
+
+<<multi-fields,`fields`>>::
+
+    Multi-fields allow the same string value to be indexed in multiple ways for
+    different purposes, such as one field for search and a multi-field for
+    sorting and aggregations, or the same string value analyzed by different
+    analyzers.
+
+<<ignore-above,`ignore_above`>>::
+
+    Do not index or analyze any string longer than this value.  Defaults to `0` (disabled).
+
+<<include-in-all,`include_in_all`>>::
+
+    Whether or not the field value should be included in the
+    <<mapping-all-field,`_all`>> field? Accepts `true` or `false`.  Defaults
+    to `false` if <<mapping-index,`index`>> is set to `no`, or if a parent
+    <<object,`object`>> field sets `include_in_all` to `false`.
+    Otherwise defaults to `true`.
+
+<<mapping-index,`index`>>::
+
+    Should the field be searchable? Accepts `analyzed` (default, treat as full-text field),
+    `not_analyzed` (treat as keyword field) and `no`.
+
+<<index-options,`index_options`>>::
+
+    What information should be stored in the index, for search and highlighting purposes.
+    Defaults to `positions` for <<mapping-index,`analyzed`>> fields, and to `docs` for
+    `not_analyzed` fields.
+
+
+<<norms,`norms`>>::
++
+--
+
+Whether field-length should be taken into account when scoring queries.
+Defaults depend on the <<mapping-index,`index`>> setting:
+
+* `analyzed` fields default to `{ "enabled": true, "loading": "lazy" }`.
+* `not_analyzed` fields default to `{ "enabled": false }`.
+--
+
+<<null-value,`null_value`>>::
+
+    Accepts a string value which is substituted for any explicit `null`
+    values.  Defaults to `null`, which means the field is treated as missing.
+    If the field is `analyzed`, the `null_value` will also be analyzed.
+
+<<position-offset-gap,`position_offset_gap`>>::
+
+    The number of fake term positions which should be inserted between
+    each element of an array of strings. Defaults to 0.
+
+<<mapping-store,`store`>>::
+
+    Whether the field value should be stored and retrievable separately from
+    the <<mapping-source-field,`_source`>> field. Accepts `true` or `false`
+    (default).
+
+<<search-analyzer,`search_analyzer`>>::
+
+    The <<analyzer,`analyzer`>> that should be used at search time on
+    <<mapping-index,`analyzed`>> fields. Defaults to the `analyzer` setting.
+
+<<similarity,`similarity`>>::
+
+    Which scoring algorithm or _similarity_ should be used. Defaults
+    to `default`, which uses TF/IDF.
+
+<<term-vector,`term_vector`>>::
+
+    Whether term vectors should be stored for an <<mapping-index,`analyzed`>>
+    field. Defaults to `no`.
+
+
diff --git a/docs/reference/mapping/types/token-count.asciidoc b/docs/reference/mapping/types/token-count.asciidoc
new file mode 100644
index 0000000..6c1b93c
--- /dev/null
+++ b/docs/reference/mapping/types/token-count.asciidoc
@@ -0,0 +1,107 @@
+[[token-count]]
+=== Token count datatype
+
+A field of type `token_count` is really an <<number,`integer>> field which
+accepts string values, analyzes them, then indexes the number of tokens in the
+string.
+
+For instance:
+
+[source,js]
+--------------------------------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "name": { <1>
+          "type": "string",
+          "fields": {
+            "length": { <2>
+              "type":     "token_count",
+              "analyzer": "standard"
+            }
+          }
+        }
+      }
+    }
+  }
+}
+
+PUT my_index/my_type/1
+{ "name": "John Smith" }
+
+PUT my_index/my_type/2
+{ "name": "Rachel Alice Williams" }
+
+GET my_index/_search
+{
+  "query": {
+    "term": {
+      "name.length": 3 <3>
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> The `name` field is an analyzed string field which uses the default `standard` analyzer.
+<2> The `name.length` field is a `token_count` <<multi-fields,multi-field>> which will index the number of tokens in the `name` field.
+<3> This query matches only the document containing `Rachel Alice Williams`, as it contains three tokens.
+
+[NOTE]
+===================================================================
+Technically the `token_count` type sums position increments rather than
+counting tokens. This means that even if the analyzer filters out stop
+words they are included in the count.
+===================================================================
+
+[[token-count-params]]
+==== Parameters for `token_count` fields
+
+The following parameters are accepted by `token_count` fields:
+
+[horizontal]
+
+<<analyzer,`analyzer`>>::
+
+    The <<analysis,analyzer>> which should be used to analyze the string
+    value. Required. For best performance, use an analyzer without token
+    filters.
+
+<<index-boost,`boost`>>::
+
+    Field-level index time boosting. Accepts a floating point number, defaults
+    to `1.0`.
+
+<<doc-values,`doc_values`>>::
+
+    Can the field value be used for sorting, aggregations, or scripting?
+    Accepts `true` (default) or `false`.
+
+<<mapping-index,`index`>>::
+
+    Should the field be searchable? Accepts `not_analyzed` (default) and `no`.
+
+<<include-in-all,`include_in_all`>>::
+
+    Whether or not the field value should be included in the
+    <<mapping-all-field,`_all`>> field? Accepts `true` or `false`.  Defaults
+    to `false`. Note: if `true`, it is the string value that is added to `_all`,
+    not the calculated token count.
+
+<<null-value,`null_value`>>::
+
+    Accepts a numeric value of the same `type` as the field which is
+    substituted for any explicit `null` values.  Defaults to `null`, which
+    means the field is treated as missing.
+
+<<precision-step,`precision_step`>>::
+
+    Controls the number of extra terms that are indexed to make
+    <<query-dsl-range-query,`range` queries>> faster. Defaults to `32`.
+
+<<mapping-store,`store`>>::
+
+    Whether the field value should be stored and retrievable separately from
+    the <<mapping-source-field,`_source`>> field. Accepts `true` or `false`
+    (default).
diff --git a/docs/reference/migration/migrate_query_refactoring.asciidoc b/docs/reference/migration/migrate_query_refactoring.asciidoc
deleted file mode 100644
index 1f066db..0000000
--- a/docs/reference/migration/migrate_query_refactoring.asciidoc
+++ /dev/null
@@ -1,55 +0,0 @@
-[[breaking-changes query-refactoring]]
-== Breaking changes on the query-refactoring branch
-
-This section discusses changes that are breaking to the current rest or java-api
-on the query-refactoring feature branch.
-
-=== Java-API
-
-==== BoostingQueryBuilder
-
-Removed setters for mandatory positive/negative query. Both arguments now have
-to be supplied at construction time already and have to be non-null.
-
-==== SpanContainingQueryBuilder
-
-Removed setters for mandatory big/little inner span queries. Both arguments now have
-to be supplied at construction time already and have to be non-null. Updated
-static factory methods in QueryBuilders accordingly.
-
-==== SpanNearQueryBuilder
-
-Removed setter for mandatory slop parameter, needs to be set in constructor now.
-Updated the static factory methods in QueryBuilders accordingly.
-
-==== SpanNotQueryBuilder
-
-Removed setter for mandatory include/exclude span query clause, needs to be set in constructor now.
-Updated the static factory methods in QueryBuilders and tests accordingly.
-
-==== SpanWithinQueryBuilder
-
-Removed setters for mandatory big/little inner span queries. Both arguments now have
-to be supplied at construction time already and have to be non-null. Updated
-static factory methods in QueryBuilders accordingly.
->>>>>>> Query refactoring: SpanWithinQueryBuilder and Parser
-
-==== QueryFilterBuilder
-
-Removed the setter `queryName(String queryName)` since this field is not supported
-in this type of query. Use `FQueryFilterBuilder.queryName(String queryName)` instead 
-when in need to wrap a named query as a filter.
-
-==== Operator
-
-Removed the enums called `Operator` from `MatchQueryBuilder`, `QueryStringQueryBuilder`,
-`SimpleQueryStringBuilder`, and `CommonTermsQueryBuilder` in favour of using the enum
-defined in `org.elasticsearch.index.query.Operator` in an effort to consolidate the
-codebase and avoid duplication.
-
-==== queryName and boost support
-
-Support for `queryName` and `boost` has been streamlined to all of the queries. That is
-a breaking change till queries get sent over the network as serialized json rather
-than in `Streamable` format. In fact whenever additional fields are added to the json
-representation of the query, older nodes might throw error when they find unknown fields.pd
diff --git a/docs/reference/modules/advanced-scripting.asciidoc b/docs/reference/modules/advanced-scripting.asciidoc
index b4e907c..206a2fe 100644
--- a/docs/reference/modules/advanced-scripting.asciidoc
+++ b/docs/reference/modules/advanced-scripting.asciidoc
@@ -71,7 +71,7 @@ Field statistics can be accessed with a subscript operator like this:
 
 Field statistics are computed per shard and therefore these numbers can vary
 depending on the shard the current document resides in.
-The number of terms in a field cannot be accessed using the `_index` variable. See <<mapping-core-types, word count mapping type>> on how to do that.
+The number of terms in a field cannot be accessed using the `_index` variable. See <<token-count>> for how to do that.
 
 [float]
 ==== Term statistics:
@@ -80,7 +80,7 @@ Term statistics for a field can be accessed with a subscript operator like
 this: `_index['FIELD']['TERM']`. This will never return null, even if term or field does not exist.
 If you do not need the term frequency, call `_index['FIELD'].get('TERM', 0)`
 to avoid unnecessary initialization of the frequencies. The flag will have only
-affect is your set the `index_options` to `docs` (see <<mapping-core-types, mapping documentation>>).
+affect is your set the <<index-options,`index_options`>> to `docs`.
 
 
 `_index['FIELD']['TERM'].df()`::
@@ -176,7 +176,7 @@ return score;
 [float]
 ==== Term vectors:
 
-The `_index` variable can only be used to gather statistics for single terms. If you want to use information on all terms in a field, you must store the term vectors (set `term_vector` in the mapping as described in the <<mapping-core-types,mapping documentation>>). To access them, call
+The `_index` variable can only be used to gather statistics for single terms. If you want to use information on all terms in a field, you must store the term vectors (see <<term-vector>>). To access them, call
 `_index.termVectors()` to get a
 https://lucene.apache.org/core/4_0_0/core/org/apache/lucene/index/Fields.html[Fields]
 instance. This object can then be used as described in https://lucene.apache.org/core/4_0_0/core/org/apache/lucene/index/Fields.html[lucene doc] to iterate over fields and then for each field iterate over each term in the field.
diff --git a/docs/reference/modules/scripting.asciidoc b/docs/reference/modules/scripting.asciidoc
index ea31b2d..7729ce2 100644
--- a/docs/reference/modules/scripting.asciidoc
+++ b/docs/reference/modules/scripting.asciidoc
@@ -284,7 +284,6 @@ supported operations are:
 |=======================================================================
 |Value |Description
 | `aggs`    |Aggregations (wherever they may be used)
-| `mapping` |Mappings (script transform feature)
 | `search`  |Search api, Percolator api and Suggester api (e.g filters, script_fields)
 | `update`  |Update api
 | `plugin`  |Any plugin that makes use of scripts under the generic `plugin` category
diff --git a/docs/reference/query-dsl/exists-query.asciidoc b/docs/reference/query-dsl/exists-query.asciidoc
index d25ebde..ca72887 100644
--- a/docs/reference/query-dsl/exists-query.asciidoc
+++ b/docs/reference/query-dsl/exists-query.asciidoc
@@ -44,7 +44,7 @@ These documents would *not* match the above query:
 [float]
 ===== `null_value` mapping
 
-If the field mapping includes the `null_value` setting (see <<mapping-core-types>>)
+If the field mapping includes the <<null-value,`null_value`>> setting
 then explicit `null` values are replaced with the specified `null_value`.  For
 instance, if the `user` field were mapped as follows:
 
diff --git a/docs/reference/query-dsl/function-score-query.asciidoc b/docs/reference/query-dsl/function-score-query.asciidoc
index ab1d7d7..08e5e57 100644
--- a/docs/reference/query-dsl/function-score-query.asciidoc
+++ b/docs/reference/query-dsl/function-score-query.asciidoc
@@ -254,7 +254,7 @@ decay function is specified as
 <1> The `DECAY_FUNCTION` should be one of `linear`, `exp`, or `gauss`.
 <2> The specified field must be a numeric, date, or geo-point field.
 
-In the above example, the field is a <<mapping-geo-point-type>> and origin can be provided in geo format. `scale` and `offset` must be given with a unit in this case. If your field is a date field, you can set `scale` and `offset` as days, weeks, and so on. Example:
+In the above example, the field is a <<geo-point,`geo_point`>> and origin can be provided in geo format. `scale` and `offset` must be given with a unit in this case. If your field is a date field, you can set `scale` and `offset` as days, weeks, and so on. Example:
 
 
 [source,js]
@@ -268,7 +268,7 @@ In the above example, the field is a <<mapping-geo-point-type>> and origin can b
         }
     }
 --------------------------------------------------
-<1> The date format of the origin depends on the <<mapping-date-format>> defined in
+<1> The date format of the origin depends on the <<mapping-date-format,`format`>> defined in
     your mapping. If you do not define the origin, the current time is used.
 <2> The `offset` and `decay` parameters are optional.
 
diff --git a/docs/reference/query-dsl/geo-polygon-query.asciidoc b/docs/reference/query-dsl/geo-polygon-query.asciidoc
index b9b624b..6c28a05 100644
--- a/docs/reference/query-dsl/geo-polygon-query.asciidoc
+++ b/docs/reference/query-dsl/geo-polygon-query.asciidoc
@@ -112,7 +112,6 @@ Format in `lat,lon`.
 [float]
 ==== geo_point Type
 
-The filter *requires* the
-<<mapping-geo-point-type,geo_point>> type to be
-set on the relevant field.
+The query *requires* the <<geo-point,`geo_point`>> type to be set on the
+relevant field.
 
diff --git a/docs/reference/query-dsl/geo-queries.asciidoc b/docs/reference/query-dsl/geo-queries.asciidoc
index 01f4283..ca7064f 100644
--- a/docs/reference/query-dsl/geo-queries.asciidoc
+++ b/docs/reference/query-dsl/geo-queries.asciidoc
@@ -2,8 +2,8 @@
 == Geo queries
 
 Elasticsearch supports two types of geo data:
-<<mapping-geo-point-type,`geo_point`>> fields which support lat/lon pairs, and
-<<mapping-geo-shape-type,`geo_shape`>> fields, which support points,
+<<geo-point,`geo_point`>> fields which support lat/lon pairs, and
+<<geo-shape,`geo_shape`>> fields, which support points,
 lines, circles, polygons, multi-polygons etc.
 
 The queries in this group are:
diff --git a/docs/reference/query-dsl/geo-shape-query.asciidoc b/docs/reference/query-dsl/geo-shape-query.asciidoc
index 7a11677..8ab2f13 100644
--- a/docs/reference/query-dsl/geo-shape-query.asciidoc
+++ b/docs/reference/query-dsl/geo-shape-query.asciidoc
@@ -3,7 +3,7 @@
 
 Filter documents indexed using the `geo_shape` type.
 
-Requires the <<mapping-geo-shape-type,geo_shape Mapping>>.
+Requires the <<geo-shape,`geo_shape` Mapping>>.
 
 The `geo_shape` query uses the same grid square representation as the
 geo_shape mapping to find documents that have a shape that intersects
diff --git a/docs/reference/query-dsl/geohash-cell-query.asciidoc b/docs/reference/query-dsl/geohash-cell-query.asciidoc
index 8b75f3c..c3c83de 100644
--- a/docs/reference/query-dsl/geohash-cell-query.asciidoc
+++ b/docs/reference/query-dsl/geohash-cell-query.asciidoc
@@ -2,13 +2,13 @@
 === Geohash Cell Query
 
 The `geohash_cell` query provides access to a hierarchy of geohashes.
-By defining a geohash cell, only <<mapping-geo-point-type,geopoints>>
+By defining a geohash cell, only <<geo-point,geopoints>>
 within this cell will match this filter.
 
 To get this filter work all prefixes of a geohash need to be indexed. In
 example a geohash `u30` needs to be decomposed into three terms: `u30`,
 `u3` and `u`. This decomposition must be enabled in the mapping of the
-<<mapping-geo-point-type,geopoint>> field that's going to be filtered by
+<<geo-point,geopoint>> field that's going to be filtered by
 setting the `geohash_prefix` option:
 
 [source,js]
diff --git a/docs/reference/query-dsl/joining-queries.asciidoc b/docs/reference/query-dsl/joining-queries.asciidoc
index a230ded..ae66db8 100644
--- a/docs/reference/query-dsl/joining-queries.asciidoc
+++ b/docs/reference/query-dsl/joining-queries.asciidoc
@@ -7,7 +7,7 @@ which are designed to scale horizontally.
 
 <<query-dsl-nested-query,`nested` query>>::
 
-Documents may contains fields of type <<mapping-nested-type,`nested`>>. These
+Documents may contains fields of type <<nested,`nested`>>. These
 fields are used to index arrays of objects, where each object can be queried
 (with the `nested` query) as an independent document.
 
diff --git a/docs/reference/query-dsl/missing-query.asciidoc b/docs/reference/query-dsl/missing-query.asciidoc
index 276722b..648da06 100644
--- a/docs/reference/query-dsl/missing-query.asciidoc
+++ b/docs/reference/query-dsl/missing-query.asciidoc
@@ -44,7 +44,7 @@ These documents would *not* match the above filter:
 [float]
 ==== `null_value` mapping
 
-If the field mapping includes a `null_value` (see <<mapping-core-types>>) then explicit `null` values
+If the field mapping includes a <<null-value,`null_value`>> then explicit `null` values
 are replaced with the specified `null_value`.  For instance, if the `user` field were mapped
 as follows:
 
diff --git a/docs/reference/query-dsl/multi-match-query.asciidoc b/docs/reference/query-dsl/multi-match-query.asciidoc
index ecfaad7..fe1a5f6 100644
--- a/docs/reference/query-dsl/multi-match-query.asciidoc
+++ b/docs/reference/query-dsl/multi-match-query.asciidoc
@@ -302,10 +302,17 @@ document to match.  (Compare this to
 
 That solves one of the two problems. The problem of differing term frequencies
 is solved by _blending_ the term frequencies for all fields in order to even
-out the differences.  In other words, `first_name:smith` will be treated as
-though it has the same weight as `last_name:smith`. (Actually,
-`last_name:smith` is given a tiny advantage over `first_name:smith`, just to
-make the order of results more stable.)
+out the differences.
+
+In practice, `first_name:smith` will be treated as though it has the same
+frequencies as `last_name:smith`, plus one. This will make matches on
+`first_name` and `last_name` have comparable scores, with a tiny advantage
+for `last_name` since it is the most likely field that contains `smith`.
+
+Note that `cross_fields` is usually only useful on short string fields
+that all have a `boost` of `1`. Otherwise boosts, term freqs and length
+normalization contribute to the score in such a way that the blending of term
+statistics is not meaningful anymore.
 
 If you run the above query through the <<search-validate>>, it returns this
 explanation:
diff --git a/docs/reference/query-dsl/nested-query.asciidoc b/docs/reference/query-dsl/nested-query.asciidoc
index 0d8f5ce..d32705a 100644
--- a/docs/reference/query-dsl/nested-query.asciidoc
+++ b/docs/reference/query-dsl/nested-query.asciidoc
@@ -2,7 +2,7 @@
 === Nested Query
 
 Nested query allows to query nested objects / docs (see
-<<mapping-nested-type,nested mapping>>). The
+<<nested,nested mapping>>). The
 query is executed against the nested objects / docs as if they were
 indexed as separate docs (they are, internally) and resulting in the
 root parent doc (or parent nested mapping). Here is a sample mapping we
@@ -48,7 +48,7 @@ And here is a sample nested query usage:
 The query `path` points to the nested object path, and the `query` (or
 `filter`) includes the query that will run on the nested docs matching
 the direct path, and joining with the root parent docs. Note that any
-fields referenced inside the query must use the complete path (fully 
+fields referenced inside the query must use the complete path (fully
 qualified).
 
 The `score_mode` allows to set how inner children matching affects
diff --git a/docs/reference/query-dsl/range-query.asciidoc b/docs/reference/query-dsl/range-query.asciidoc
index d5b2712..3c216ca 100644
--- a/docs/reference/query-dsl/range-query.asciidoc
+++ b/docs/reference/query-dsl/range-query.asciidoc
@@ -29,33 +29,60 @@ The `range` query accepts the following parameters:
 `lt`::  	Less-than
 `boost`:: 	Sets the boost value of the query, defaults to `1.0`
 
-[float]
-==== Date options
 
-When applied on `date` fields the `range` filter accepts also a `time_zone` parameter.
-The `time_zone` parameter will be applied to your input lower and upper bounds and will
-move them to UTC time based date:
+[[ranges-on-dates]]
+==== Ranges on date fields
+
+When running `range` queries on fields of type <<date,`date`>>, ranges can be
+specified using <<date-math>>::
 
 [source,js]
 --------------------------------------------------
 {
     "range" : {
-        "born" : {
-            "gte": "2012-01-01",
-            "lte": "now",
-            "time_zone": "+01:00"
+        "date" : {
+            "gte" : "now-1d/d",
+            "lt" :  "now/d"
         }
     }
 }
 --------------------------------------------------
 
-In the above example, `gte` will be actually moved to `2011-12-31T23:00:00` UTC date.
+===== Date math and rounding
+
+When using <<date-math,date math>> to round dates to the nearest day, month,
+hour, etc, the rounded dates depend on whether the ends of the ranges are
+inclusive or exclusive.
+
+Rounding up moves to the last millisecond of the rounding scope, and rounding
+down to the first millisecond of the rounding scope. For example:
+
+[horizontal]
+`gt`::
+
+    Greater than the date rounded up: `2014-11-18||/M` becomes
+    `2014-11-30T23:59:59.999`, ie excluding the entire month.
+
+`gte`::
+
+    Greater than or equal to the date rounded down: `2014-11-18||/M` becomes
+    `2014-11-01`, ie including the entire month.
+
+`lt`::
+
+    Less than the date rounded down: `2014-11-18||/M` becomes `2014-11-01`, ie
+    excluding the entire month.
+
+`lte`::
+
+    Less than or equal to the date rounded up: `2014-11-18||/M` becomes
+    `2014-11-30T23:59:59.999`, ie including the entire month.
 
-NOTE: if you give a date with a timezone explicitly defined and use the `time_zone` parameter, `time_zone` will be
-ignored. For example, setting `gte` to `2012-01-01T00:00:00+01:00` with `"time_zone":"+10:00"` will still use `+01:00` time zone.
+===== Date format in range queries
 
-When applied on `date` fields the `range` query accepts also a `format` parameter.
-The `format` parameter will help support another date format than the one defined in mapping:
+Formatted dates will be parsed using the <<mapping-date-format,`format`>>
+specified on the <<date,`date`>> field by default, but it can be overridden by
+passing the `format` parameter to the `range` query:
 
 [source,js]
 --------------------------------------------------
@@ -69,3 +96,25 @@ The `format` parameter will help support another date format than the one define
     }
 }
 --------------------------------------------------
+
+===== Time zone in range queries
+
+Dates can be converted from another timezone to UTC either by specifying the
+time zone in the date value itself (if the <<mapping-date-format, `format`>>
+accepts it), or it can be specified as the `time_zone` parameter:
+
+[source,js]
+--------------------------------------------------
+{
+    "range" : {
+        "timestamp" : {
+            "gte": "2015-01-01 00:00:00", <1>
+            "lte": "now",
+            "time_zone": "+01:00"
+        }
+    }
+}
+--------------------------------------------------
+<1> This date will be converted to `2014-12-31T23:00:00 UTC`.
+
+
diff --git a/docs/reference/search/request/inner-hits.asciidoc b/docs/reference/search/request/inner-hits.asciidoc
index 5e38ac0..bf7d184 100644
--- a/docs/reference/search/request/inner-hits.asciidoc
+++ b/docs/reference/search/request/inner-hits.asciidoc
@@ -3,7 +3,7 @@
 
 experimental[]
 
-The <<mapping-parent-field, parent/child>> and <<mapping-nested-type, nested>> features allow the return of documents that
+The <<mapping-parent-field, parent/child>> and <<nested, nested>> features allow the return of documents that
 have matches in a different scope. In the parent/child case, parent document are returned based on matches in child
 documents or child document are returned based on matches in parent documents. In the nested case, documents are returned
 based on matches in nested inner objects.
diff --git a/docs/reference/search/request/sort.asciidoc b/docs/reference/search/request/sort.asciidoc
index c529955..0a8f368 100644
--- a/docs/reference/search/request/sort.asciidoc
+++ b/docs/reference/search/request/sort.asciidoc
@@ -71,6 +71,7 @@ curl -XPOST 'localhost:9200/_search' -d '{
 }'
 --------------------------------------------------
 
+[[nested-sorting]]
 ==== Sorting within nested objects.
 
 Elasticsearch also supports sorting by
@@ -166,6 +167,7 @@ If any of the indices that are queried doesn't have a mapping for `price`
 then Elasticsearch will handle it as if there was a mapping of type
 `long`, with all documents in this index having no value for this field.
 
+[[geo-sorting]]
 ==== Geo Distance Sorting
 
 Allow to sort by `_geo_distance`. Here is an example:
diff --git a/plugins/lang-javascript/src/main/java/org/elasticsearch/script/javascript/JavaScriptScriptEngineService.java b/plugins/lang-javascript/src/main/java/org/elasticsearch/script/javascript/JavaScriptScriptEngineService.java
index 71d5197..9ce83f3 100644
--- a/plugins/lang-javascript/src/main/java/org/elasticsearch/script/javascript/JavaScriptScriptEngineService.java
+++ b/plugins/lang-javascript/src/main/java/org/elasticsearch/script/javascript/JavaScriptScriptEngineService.java
@@ -150,6 +150,12 @@ public class JavaScriptScriptEngineService extends AbstractComponent implements
 
                 return new JavaScriptSearchScript((Script) compiledScript.compiled(), scope, leafLookup);
               }
+
+              @Override
+              public boolean needsScores() {
+                  // TODO: can we reliably know if a javascript script makes use of _score
+                  return true;
+              }
             };
         } finally {
             Context.exit();
diff --git a/plugins/lang-python/src/main/java/org/elasticsearch/script/python/PythonScriptEngineService.java b/plugins/lang-python/src/main/java/org/elasticsearch/script/python/PythonScriptEngineService.java
index 3e3fb7f..4bdf985 100644
--- a/plugins/lang-python/src/main/java/org/elasticsearch/script/python/PythonScriptEngineService.java
+++ b/plugins/lang-python/src/main/java/org/elasticsearch/script/python/PythonScriptEngineService.java
@@ -90,6 +90,11 @@ public class PythonScriptEngineService extends AbstractComponent implements Scri
                 final LeafSearchLookup leafLookup = lookup.getLeafSearchLookup(context);
                 return new PythonSearchScript((PyCode) compiledScript.compiled(), vars, leafLookup);
             }
+            @Override
+            public boolean needsScores() {
+                // TODO: can we reliably know if a python script makes use of _score
+                return true;
+            }
         };
     }
 
diff --git a/plugins/mapper-size/pom.xml b/plugins/mapper-size/pom.xml
new file mode 100644
index 0000000..cdbb869
--- /dev/null
+++ b/plugins/mapper-size/pom.xml
@@ -0,0 +1,43 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!-- Licensed to Elasticsearch under one or more contributor
+license agreements. See the NOTICE file distributed with this work for additional
+information regarding copyright ownership. ElasticSearch licenses this file to you
+under the Apache License, Version 2.0 (the "License"); you may not use this
+file except in compliance with the License. You may obtain a copy of the
+License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by
+applicable law or agreed to in writing, software distributed under the License
+is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied. See the License for the specific language
+governing permissions and limitations under the License. -->
+
+<project xmlns="http://maven.apache.org/POM/4.0.0"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+
+    <parent>
+        <groupId>org.elasticsearch.plugin</groupId>
+        <artifactId>elasticsearch-plugin</artifactId>
+        <version>2.0.0-beta1-SNAPSHOT</version>
+    </parent>
+
+    <artifactId>elasticsearch-mapper-size</artifactId>
+    <name>Elasticsearch Mapper size plugin</name>
+    <description>The Mapper Size plugin allows document to record their uncompressed size at index time.</description>
+
+    <properties>
+        <elasticsearch.plugin.classname>org.elasticsearch.plugin.mapper.MapperSizePlugin</elasticsearch.plugin.classname>
+        <tests.rest.suite>mapper_size</tests.rest.suite>
+        <tests.rest.load_packaged>false</tests.rest.load_packaged>
+    </properties>
+
+    <build>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
diff --git a/plugins/mapper-size/rest-api-spec/test/mapper_size/10_basic.yaml b/plugins/mapper-size/rest-api-spec/test/mapper_size/10_basic.yaml
new file mode 100644
index 0000000..9e76645
--- /dev/null
+++ b/plugins/mapper-size/rest-api-spec/test/mapper_size/10_basic.yaml
@@ -0,0 +1,32 @@
+# Integration tests for Mapper Size components
+#
+
+---
+"Mapper Size":
+
+    - do:
+        indices.create:
+            index: test
+            body:
+                mappings:
+                    type1: { "_size": { "enabled": true } }
+
+    - do:
+        index:
+            index: test
+            type: type1
+            id: 1
+            body: { "foo": "bar" }
+
+    - do:
+        indices.refresh: {}
+
+    - do:
+        get:
+            index: test
+            type: type1
+            id: 1
+            fields: "_size"
+
+    - match: { _size: 13 }
+
diff --git a/plugins/mapper-size/src/main/java/org/elasticsearch/index/mapper/size/RegisterSizeFieldMapper.java b/plugins/mapper-size/src/main/java/org/elasticsearch/index/mapper/size/RegisterSizeFieldMapper.java
new file mode 100644
index 0000000..e71018f
--- /dev/null
+++ b/plugins/mapper-size/src/main/java/org/elasticsearch/index/mapper/size/RegisterSizeFieldMapper.java
@@ -0,0 +1,36 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.mapper.size;
+
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.AbstractIndexComponent;
+import org.elasticsearch.index.Index;
+import org.elasticsearch.index.mapper.MapperService;
+
+public class RegisterSizeFieldMapper extends AbstractIndexComponent {
+
+    @Inject
+    public RegisterSizeFieldMapper(Index index, Settings indexSettings, MapperService mapperService) {
+        super(index, indexSettings);
+        mapperService.documentMapperParser().putRootTypeParser(SizeFieldMapper.NAME, new SizeFieldMapper.TypeParser());
+    }
+
+}
diff --git a/plugins/mapper-size/src/main/java/org/elasticsearch/index/mapper/size/SizeFieldMapper.java b/plugins/mapper-size/src/main/java/org/elasticsearch/index/mapper/size/SizeFieldMapper.java
new file mode 100644
index 0000000..6c80bcd
--- /dev/null
+++ b/plugins/mapper-size/src/main/java/org/elasticsearch/index/mapper/size/SizeFieldMapper.java
@@ -0,0 +1,180 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.mapper.size;
+
+import org.apache.lucene.document.Field;
+import org.elasticsearch.Version;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.index.analysis.NumericIntegerAnalyzer;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.Mapper;
+import org.elasticsearch.index.mapper.MapperParsingException;
+import org.elasticsearch.index.mapper.MergeMappingException;
+import org.elasticsearch.index.mapper.MergeResult;
+import org.elasticsearch.index.mapper.MetadataFieldMapper;
+import org.elasticsearch.index.mapper.ParseContext;
+import org.elasticsearch.index.mapper.core.IntegerFieldMapper;
+import org.elasticsearch.index.mapper.internal.EnabledAttributeMapper;
+
+import java.io.IOException;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+
+import static org.elasticsearch.common.xcontent.support.XContentMapValues.nodeBooleanValue;
+import static org.elasticsearch.index.mapper.core.TypeParsers.parseStore;
+
+public class SizeFieldMapper extends MetadataFieldMapper {
+
+    public static final String NAME = "_size";
+    public static final String CONTENT_TYPE = "_size";
+
+    public static class Defaults extends IntegerFieldMapper.Defaults {
+        public static final EnabledAttributeMapper ENABLED_STATE = EnabledAttributeMapper.UNSET_DISABLED;
+
+        public static final MappedFieldType SIZE_FIELD_TYPE = IntegerFieldMapper.Defaults.FIELD_TYPE.clone();
+
+        static {
+            SIZE_FIELD_TYPE.setStored(true);
+            SIZE_FIELD_TYPE.setNumericPrecisionStep(Defaults.PRECISION_STEP_32_BIT);
+            SIZE_FIELD_TYPE.setNames(new MappedFieldType.Names(NAME));
+            SIZE_FIELD_TYPE.setIndexAnalyzer(NumericIntegerAnalyzer.buildNamedAnalyzer(Defaults.PRECISION_STEP_32_BIT));
+            SIZE_FIELD_TYPE.setSearchAnalyzer(NumericIntegerAnalyzer.buildNamedAnalyzer(Integer.MAX_VALUE));
+            SIZE_FIELD_TYPE.freeze();
+        }
+    }
+
+    public static class Builder extends MetadataFieldMapper.Builder<Builder, SizeFieldMapper> {
+
+        protected EnabledAttributeMapper enabledState = EnabledAttributeMapper.UNSET_DISABLED;
+
+        public Builder(MappedFieldType existing) {
+            super(NAME, existing == null ? Defaults.SIZE_FIELD_TYPE : existing);
+            builder = this;
+        }
+
+        public Builder enabled(EnabledAttributeMapper enabled) {
+            this.enabledState = enabled;
+            return builder;
+        }
+
+        @Override
+        public SizeFieldMapper build(BuilderContext context) {
+            setupFieldType(context);
+            fieldType.setHasDocValues(false);
+            return new SizeFieldMapper(enabledState, fieldType, context.indexSettings());
+        }
+    }
+
+    public static class TypeParser implements Mapper.TypeParser {
+        @Override
+        public Mapper.Builder<?, ?> parse(String name, Map<String, Object> node, ParserContext parserContext) throws MapperParsingException {
+            Builder builder = new Builder(parserContext.mapperService().fullName(NAME));
+            for (Iterator<Map.Entry<String, Object>> iterator = node.entrySet().iterator(); iterator.hasNext();) {
+                Map.Entry<String, Object> entry = iterator.next();
+                String fieldName = Strings.toUnderscoreCase(entry.getKey());
+                Object fieldNode = entry.getValue();
+                if (fieldName.equals("enabled")) {
+                    builder.enabled(nodeBooleanValue(fieldNode) ? EnabledAttributeMapper.ENABLED : EnabledAttributeMapper.DISABLED);
+                    iterator.remove();
+                } else if (fieldName.equals("store") && parserContext.indexVersionCreated().before(Version.V_2_0_0_beta1)) {
+                    builder.store(parseStore(fieldName, fieldNode.toString()));
+                    iterator.remove();
+                }
+            }
+            return builder;
+        }
+    }
+
+    private EnabledAttributeMapper enabledState;
+
+    private SizeFieldMapper(EnabledAttributeMapper enabled, MappedFieldType fieldType, Settings indexSettings) {
+        super(NAME, fieldType, Defaults.SIZE_FIELD_TYPE, indexSettings);
+        this.enabledState = enabled;
+
+    }
+
+    @Override
+    protected String contentType() {
+        return NAME;
+    }
+
+    public boolean enabled() {
+        return this.enabledState.enabled;
+    }
+
+    @Override
+    public void preParse(ParseContext context) throws IOException {
+    }
+
+    @Override
+    public void postParse(ParseContext context) throws IOException {
+        // we post parse it so we get the size stored, possibly compressed (source will be preParse)
+        super.parse(context);
+    }
+
+    @Override
+    public Mapper parse(ParseContext context) throws IOException {
+        // nothing to do here, we call the parent in postParse
+        return null;
+    }
+
+    @Override
+    protected void parseCreateField(ParseContext context, List<Field> fields) throws IOException {
+        if (!enabledState.enabled) {
+            return;
+        }
+        if (context.flyweight()) {
+            return;
+        }
+        fields.add(new IntegerFieldMapper.CustomIntegerNumericField(context.source().length(), fieldType()));
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        boolean includeDefaults = params.paramAsBoolean("include_defaults", false);
+
+        // all are defaults, no need to write it at all
+        if (!includeDefaults && enabledState == Defaults.ENABLED_STATE && (indexCreatedBefore2x == false || fieldType().stored() == false)) {
+            return builder;
+        }
+        builder.startObject(contentType());
+        if (includeDefaults || enabledState != Defaults.ENABLED_STATE) {
+            builder.field("enabled", enabledState.enabled);
+        }
+        if (indexCreatedBefore2x && (includeDefaults || fieldType().stored() == true)) {
+            builder.field("store", fieldType().stored());
+        }
+        builder.endObject();
+        return builder;
+    }
+
+    @Override
+    public void merge(Mapper mergeWith, MergeResult mergeResult) throws MergeMappingException {
+        SizeFieldMapper sizeFieldMapperMergeWith = (SizeFieldMapper) mergeWith;
+        if (!mergeResult.simulate()) {
+            if (sizeFieldMapperMergeWith.enabledState != enabledState && !sizeFieldMapperMergeWith.enabledState.unset()) {
+                this.enabledState = sizeFieldMapperMergeWith.enabledState;
+            }
+        }
+    }
+}
diff --git a/plugins/mapper-size/src/main/java/org/elasticsearch/plugin/mapper/MapperSizeIndexModule.java b/plugins/mapper-size/src/main/java/org/elasticsearch/plugin/mapper/MapperSizeIndexModule.java
new file mode 100644
index 0000000..df9585b
--- /dev/null
+++ b/plugins/mapper-size/src/main/java/org/elasticsearch/plugin/mapper/MapperSizeIndexModule.java
@@ -0,0 +1,32 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plugin.mapper;
+
+import org.elasticsearch.common.inject.AbstractModule;
+import org.elasticsearch.index.mapper.size.RegisterSizeFieldMapper;
+
+public class MapperSizeIndexModule extends AbstractModule {
+
+    @Override
+    protected void configure() {
+        bind(RegisterSizeFieldMapper.class).asEagerSingleton();
+    }
+
+}
diff --git a/plugins/mapper-size/src/main/java/org/elasticsearch/plugin/mapper/MapperSizePlugin.java b/plugins/mapper-size/src/main/java/org/elasticsearch/plugin/mapper/MapperSizePlugin.java
new file mode 100644
index 0000000..20f5254
--- /dev/null
+++ b/plugins/mapper-size/src/main/java/org/elasticsearch/plugin/mapper/MapperSizePlugin.java
@@ -0,0 +1,45 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plugin.mapper;
+
+import org.elasticsearch.common.inject.Module;
+import org.elasticsearch.plugins.AbstractPlugin;
+
+import java.util.Collection;
+import java.util.Collections;
+
+public class MapperSizePlugin extends AbstractPlugin {
+
+    @Override
+    public String name() {
+        return "mapper-size";
+    }
+
+    @Override
+    public String description() {
+        return "A mapper that allows document to record their uncompressed size";
+    }
+
+    @Override
+    public Collection<Class<? extends Module>> indexModules() {
+        return Collections.<Class<? extends Module>>singleton(MapperSizeIndexModule.class);
+    }
+
+}
diff --git a/plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/MapperSizeRestIT.java b/plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/MapperSizeRestIT.java
new file mode 100644
index 0000000..ec7e000
--- /dev/null
+++ b/plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/MapperSizeRestIT.java
@@ -0,0 +1,42 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.mapper.size;
+
+import com.carrotsearch.randomizedtesting.annotations.Name;
+import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
+
+import org.elasticsearch.test.rest.ESRestTestCase;
+import org.elasticsearch.test.rest.RestTestCandidate;
+import org.elasticsearch.test.rest.parser.RestTestParseException;
+
+import java.io.IOException;
+
+public class MapperSizeRestIT extends ESRestTestCase {
+
+    public MapperSizeRestIT(@Name("yaml") RestTestCandidate testCandidate) {
+        super(testCandidate);
+    }
+
+    @ParametersFactory
+    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
+        return createParameters(0, 1);
+    }
+}
+
diff --git a/plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingIT.java b/plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingIT.java
new file mode 100644
index 0000000..aa82c5a
--- /dev/null
+++ b/plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingIT.java
@@ -0,0 +1,109 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.index.mapper.size;
+
+import org.elasticsearch.action.admin.indices.mapping.get.GetMappingsResponse;
+import org.elasticsearch.action.admin.indices.mapping.put.PutMappingResponse;
+import org.elasticsearch.action.get.GetResponse;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.plugin.mapper.MapperSizePlugin;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.Locale;
+import java.util.Map;
+
+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
+import static org.hamcrest.Matchers.hasKey;
+import static org.hamcrest.Matchers.is;
+import static org.hamcrest.Matchers.notNullValue;
+
+public class SizeMappingIT extends ESIntegTestCase {
+
+    @Override
+    protected Settings nodeSettings(int nodeOrdinal) {
+        return Settings.builder()
+                .put(super.nodeSettings(nodeOrdinal))
+                .put("plugin.types", MapperSizePlugin.class.getName())
+                .build();
+    }
+
+    // issue 5053
+    public void testThatUpdatingMappingShouldNotRemoveSizeMappingConfiguration() throws Exception {
+        String index = "foo";
+        String type = "mytype";
+
+        XContentBuilder builder = jsonBuilder().startObject().startObject("_size").field("enabled", true).endObject().endObject();
+        assertAcked(client().admin().indices().prepareCreate(index).addMapping(type, builder));
+
+        // check mapping again
+        assertSizeMappingEnabled(index, type, true);
+
+        // update some field in the mapping
+        XContentBuilder updateMappingBuilder = jsonBuilder().startObject().startObject("properties").startObject("otherField").field("type", "string").endObject().endObject();
+        PutMappingResponse putMappingResponse = client().admin().indices().preparePutMapping(index).setType(type).setSource(updateMappingBuilder).get();
+        assertAcked(putMappingResponse);
+
+        // make sure size field is still in mapping
+        assertSizeMappingEnabled(index, type, true);
+    }
+
+    public void testThatSizeCanBeSwitchedOnAndOff() throws Exception {
+        String index = "foo";
+        String type = "mytype";
+
+        XContentBuilder builder = jsonBuilder().startObject().startObject("_size").field("enabled", true).endObject().endObject();
+        assertAcked(client().admin().indices().prepareCreate(index).addMapping(type, builder));
+
+        // check mapping again
+        assertSizeMappingEnabled(index, type, true);
+
+        // update some field in the mapping
+        XContentBuilder updateMappingBuilder = jsonBuilder().startObject().startObject("_size").field("enabled", false).endObject().endObject();
+        PutMappingResponse putMappingResponse = client().admin().indices().preparePutMapping(index).setType(type).setSource(updateMappingBuilder).get();
+        assertAcked(putMappingResponse);
+
+        // make sure size field is still in mapping
+        assertSizeMappingEnabled(index, type, false);
+    }
+
+    private void assertSizeMappingEnabled(String index, String type, boolean enabled) throws IOException {
+        String errMsg = String.format(Locale.ROOT, "Expected size field mapping to be " + (enabled ? "enabled" : "disabled") + " for %s/%s", index, type);
+        GetMappingsResponse getMappingsResponse = client().admin().indices().prepareGetMappings(index).addTypes(type).get();
+        Map<String, Object> mappingSource = getMappingsResponse.getMappings().get(index).get(type).getSourceAsMap();
+        assertThat(errMsg, mappingSource, hasKey("_size"));
+        String sizeAsString = mappingSource.get("_size").toString();
+        assertThat(sizeAsString, is(notNullValue()));
+        assertThat(errMsg, sizeAsString, is("{enabled=" + (enabled) + "}"));
+    }
+
+    public void testBasic() throws Exception {
+        assertAcked(prepareCreate("test").addMapping("type", "_size", "enabled=true"));
+        final String source = "{\"f\":10}";
+        indexRandom(true,
+                client().prepareIndex("test", "type", "1").setSource(source));
+        GetResponse getResponse = client().prepareGet("test", "type", "1").setFields("_size").get();
+        assertNotNull(getResponse.getField("_size"));
+        assertEquals(source.length(), getResponse.getField("_size").getValue());
+    }
+}
diff --git a/plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingTests.java b/plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingTests.java
new file mode 100644
index 0000000..df2f060
--- /dev/null
+++ b/plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingTests.java
@@ -0,0 +1,126 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.mapper.size;
+
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.index.mapper.DocumentMapper;
+import org.elasticsearch.index.mapper.DocumentMapperParser;
+import org.elasticsearch.index.mapper.ParsedDocument;
+import org.elasticsearch.index.mapper.SourceToParse;
+import org.elasticsearch.test.ESSingleNodeTestCase;
+
+import static org.hamcrest.Matchers.*;
+
+public class SizeMappingTests extends ESSingleNodeTestCase {
+    
+    public void testSizeEnabled() throws Exception {
+        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
+                .startObject("_size").field("enabled", true).endObject()
+                .endObject().endObject().string();
+        DocumentMapperParser parser = createIndex("test").mapperService().documentMapperParser();
+        parser.putRootTypeParser(SizeFieldMapper.NAME, new SizeFieldMapper.TypeParser());
+        DocumentMapper docMapper = parser.parse(mapping);
+
+        BytesReference source = XContentFactory.jsonBuilder()
+                .startObject()
+                .field("field", "value")
+                .endObject()
+                .bytes();
+        ParsedDocument doc = docMapper.parse(SourceToParse.source(source).type("type").id("1"));
+
+        assertThat(doc.rootDoc().getField("_size").fieldType().stored(), equalTo(true));
+        assertThat(doc.rootDoc().getField("_size").tokenStream(docMapper.mappers().indexAnalyzer(), null), notNullValue());
+    }
+    
+    public void testSizeEnabledAndStoredBackcompat() throws Exception {
+        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
+                .startObject("_size").field("enabled", true).field("store", "yes").endObject()
+                .endObject().endObject().string();
+        Settings indexSettings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.V_1_4_2.id).build();
+
+        DocumentMapperParser parser = createIndex("test", indexSettings).mapperService().documentMapperParser();
+        parser.putRootTypeParser(SizeFieldMapper.NAME, new SizeFieldMapper.TypeParser());
+        DocumentMapper docMapper = parser.parse(mapping);
+
+        BytesReference source = XContentFactory.jsonBuilder()
+                .startObject()
+                .field("field", "value")
+                .endObject()
+                .bytes();
+        ParsedDocument doc = docMapper.parse(SourceToParse.source(source).type("type").id("1"));
+
+        assertThat(doc.rootDoc().getField("_size").fieldType().stored(), equalTo(true));
+        assertThat(doc.rootDoc().getField("_size").tokenStream(docMapper.mappers().indexAnalyzer(), null), notNullValue());
+    }
+    
+    public void testSizeDisabled() throws Exception {
+        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
+                .startObject("_size").field("enabled", false).endObject()
+                .endObject().endObject().string();
+        DocumentMapperParser parser = createIndex("test").mapperService().documentMapperParser();
+        parser.putRootTypeParser(SizeFieldMapper.NAME, new SizeFieldMapper.TypeParser());
+        DocumentMapper docMapper = parser.parse(mapping);
+
+        BytesReference source = XContentFactory.jsonBuilder()
+                .startObject()
+                .field("field", "value")
+                .endObject()
+                .bytes();
+        ParsedDocument doc = docMapper.parse(SourceToParse.source(source).type("type").id("1"));
+
+        assertThat(doc.rootDoc().getField("_size"), nullValue());
+    }
+    
+    public void testSizeNotSet() throws Exception {
+        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
+                .endObject().endObject().string();
+        DocumentMapper docMapper = createIndex("test").mapperService().documentMapperParser().parse(mapping);
+
+        BytesReference source = XContentFactory.jsonBuilder()
+                .startObject()
+                .field("field", "value")
+                .endObject()
+                .bytes();
+        ParsedDocument doc = docMapper.parse(SourceToParse.source(source).type("type").id("1"));
+
+        assertThat(doc.rootDoc().getField("_size"), nullValue());
+    }
+    
+    public void testThatDisablingWorksWhenMerging() throws Exception {
+        String enabledMapping = XContentFactory.jsonBuilder().startObject().startObject("type")
+                .startObject("_size").field("enabled", true).endObject()
+                .endObject().endObject().string();
+        DocumentMapperParser parser = createIndex("test").mapperService().documentMapperParser();
+        parser.putRootTypeParser(SizeFieldMapper.NAME, new SizeFieldMapper.TypeParser());
+        DocumentMapper enabledMapper = parser.parse(enabledMapping);
+
+        String disabledMapping = XContentFactory.jsonBuilder().startObject().startObject("type")
+                .startObject("_size").field("enabled", false).endObject()
+                .endObject().endObject().string();
+        DocumentMapper disabledMapper = parser.parse(disabledMapping);
+
+        enabledMapper.merge(disabledMapper.mapping(), false, false);
+        assertThat(enabledMapper.rootMapper(SizeFieldMapper.class).enabled(), is(false));
+    }
+}
\ No newline at end of file
diff --git a/plugins/pom.xml b/plugins/pom.xml
index 2f0b987..5ec47e7 100644
--- a/plugins/pom.xml
+++ b/plugins/pom.xml
@@ -311,7 +311,9 @@
                             <configuration>
                                 <skip>${skip.integ.tests}</skip>
                                 <target>
-                                    <ant antfile="${elasticsearch.integ.antfile}" target="start-external-cluster-with-plugin"/>
+                                    <ant antfile="${elasticsearch.integ.antfile}" target="start-external-cluster-with-plugin">
+                                        <property name="tests.jvm.argline" value="${tests.jvm.argline}"/>
+                                    </ant>
                                 </target>
                             </configuration>
                         </execution>
@@ -430,9 +432,10 @@
         <module>cloud-gce</module>
         <module>cloud-azure</module>
         <module>cloud-aws</module>
-        <module>site-example</module>
+        <module>delete-by-query</module>
         <module>lang-python</module>
         <module>lang-javascript</module>
-        <module>delete-by-query</module>
+        <module>mapper-size</module>
+        <module>site-example</module>
     </modules>
 </project>
diff --git a/qa/smoke-test-plugins/pom.xml b/qa/smoke-test-plugins/pom.xml
index b3cf1b6..489608b 100644
--- a/qa/smoke-test-plugins/pom.xml
+++ b/qa/smoke-test-plugins/pom.xml
@@ -127,7 +127,7 @@
 
                  <artifactItem>
                    <groupId>org.elasticsearch.plugin</groupId>
-                   <artifactId>elasticsearch-site-example</artifactId>
+                   <artifactId>elasticsearch-delete-by-query</artifactId>
                    <version>${elasticsearch.version}</version>
                    <type>zip</type>
                    <overWrite>true</overWrite>
@@ -151,11 +151,20 @@
 
                  <artifactItem>
                    <groupId>org.elasticsearch.plugin</groupId>
-                   <artifactId>elasticsearch-delete-by-query</artifactId>
+                   <artifactId>elasticsearch-mapper-size</artifactId>
+                   <version>${elasticsearch.version}</version>
+                   <type>zip</type>
+                   <overWrite>true</overWrite>
+                 </artifactItem>
+
+                 <artifactItem>
+                   <groupId>org.elasticsearch.plugin</groupId>
+                   <artifactId>elasticsearch-site-example</artifactId>
                    <version>${elasticsearch.version}</version>
                    <type>zip</type>
                    <overWrite>true</overWrite>
                  </artifactItem>
+
                </artifactItems>
              </configuration>
            </execution>
@@ -193,6 +202,7 @@
             <configuration>
               <target>
                 <ant antfile="${elasticsearch.integ.antfile}" target="start-external-cluster-with-plugins">
+                  <property name="tests.jvm.argline" value="${tests.jvm.argline}"/>
                   <property name="plugins.dir" value="${plugins.dir}"/>
                   <property name="expected.plugin.count" value="${expected.plugin.count}"/>
                 </ant>
